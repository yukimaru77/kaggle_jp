{
    "main_topic": {
        "author": "Bao Loc Pham",
        "title": "[Need help] How to custom Head for AutoModelForSequenceClassification with LoRA?",
        "content": "I've a model look like this\n\n```\nGemma2ForSequenceClassification(\n  (model): Gemma2Model(\n    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n    (layers): ModuleList(\n      (0-41): 42 x Gemma2DecoderLayer(\n        (self_attn): Gemma2SdpaAttention(\n          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)\n          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)\n          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)\n          (rotary_emb): Gemma2RotaryEmbedding()\n        )\n        (mlp): Gemma2MLP(\n          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)\n          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)\n          (act_fn): PytorchGELUTanh()\n        )\n        (input_layernorm): Gemma2RMSNorm()\n        (post_attention_layernorm): Gemma2RMSNorm()\n        (pre_feedforward_layernorm): Gemma2RMSNorm()\n        (post_feedforward_layernorm): Gemma2RMSNorm()\n      )\n    )\n    (norm): Gemma2RMSNorm()\n  )\n  (score): Linear(in_features=3584, out_features=3, bias=False)\n)\n\n```\n\nAfter applied LoRA, the model look like this\n\n```\nPeftModelForSequenceClassification(\n  (base_model): LoraModel(\n    (model): Gemma2ForSequenceClassification(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n        (layers): ModuleList(\n         ....\n            )\n            (mlp): Gemma2MLP(\n              ....\n            )\n            ...\n          )\n        )\n        (norm): Gemma2RMSNorm()\n      )\n      (score): ModulesToSaveWrapper(\n        (original_module): Linear(in_features=3584, out_features=3, bias=False)\n        (modules_to_save): ModuleDict(\n          (default): Linear(in_features=3584, out_features=3, bias=False)\n        )\n      )\n    )\n  )\n)\n\n```\n\nBecause of score module is just a simple fully connected layer, I want to make it more complex,\n\nI tried to replace the head like this\n\n```\nCustomGemmaForSequenceClassification(\n  (model): GemmaModel(\n    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n    (layers): ModuleList(\n      (0-41): 42 x GemmaDecoderLayer(\n        ...\n        )\n        (mlp): GemmaMLP(\n        ...\n        )\n        (input_layernorm): GemmaRMSNorm()\n        (post_attention_layernorm): GemmaRMSNorm()\n      )\n    )\n    (norm): GemmaRMSNorm()\n  )\n  **(score): Sequential(\n    (0): Linear4bit(in_features=7168, out_features=3584, bias=True)\n    (1): Linear(in_features=3584, out_features=3, bias=False)\n  )**\n)\n\n```\n\nBut after applied peft and LoRA, there is an error \n\n```\np.requires_grad_(requires_grad)\nRuntimeError: only Tensors of floating point dtype can require gradients\n\n```\n\nI already put lora module_to_save=[\"score\"] like this [huggingface tutorial](https://huggingface.co/docs/peft/en/developer_guides/custom_models) but seem not working yet\n\n",
        "date": "Sat Jul 20 2024 13:52:51 GMT+0900 (日本標準時)",
        "votes": "2"
    },
    "comments": [
        {
            "author": "CPMP",
            "content": "Why use LORA on a matrix of rank 3?\n\nThe classification head linear layer has a rank of at most 3 because its dimension is 3584x3. \n\nTL;DR it does not make sense to apply LORA to the classification head.\n\n",
            "date": "Posted 15 days ago  ·  27th in this Competition",
            "votes": "3",
            "reply": [
                {
                    "author": "Bao Loc PhamTopic Author",
                    "content": "[@cpmpml](https://www.kaggle.com/cpmpml) \n\nthank for your comment, the rank 3 you mean is the number of class.\n\n- Yes, I don't want to apply LORA to the classification head.\n\nI just apply these code like the huggingface tutorial\n\n```\nlora_config = LoraConfig(\n    r=config.lora_r,\n    lora_alpha=config.lora_alpha,\n    # only target self-attention\n    target_modules=config.target_modules,\n    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n    lora_dropout=config.lora_dropout,\n    bias=config.lora_bias,\n    task_type=TaskType.SEQ_CLS,\n)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    config.checkpoint,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\nmodel.config.use_cache = False\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)\nprint(model)\n\nprint(model.print_trainable_parameters())\n\n```\n\nthe AutoModelForSequenceClassification class will create a simple neural network with 3 output.\n\nbut I want to replace with CustomModelForSequenceClassification with my custom head.\n\n",
                    "date": "Posted 15 days ago  ·  244th in this Competition",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "Ashwani",
                            "content": "you can specify the target_modules in which you want to apply LoRA. target_modules=[\"query\", \"key\", \"value\"] specifies that LoRA should only be applied to the attention modules, effectively excluding the classification head.\n\n",
                            "date": "Posted 15 days ago  ·  136th in this Competition",
                            "votes": "2",
                            "reply": []
                        },
                        {
                            "author": "CPMP",
                            "content": "\nthe rank 3 you mean is the number of class.\n\nThe rank is  the rank of the matrix. LoRA is about approximating a high rank matrix with a low rank matrix. That's the lora_r parameter of LoRA. \n\nWhat I am saying is that applying LoRA to the classification head only makes sense with lora_r smaller than 3.\n\n",
                            "date": "Posted 14 days ago  ·  27th in this Competition",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}