{
    "main_topic": {
        "author": "superferg",
        "title": "LLM Models that can be used",
        "content": "May I ask which models everyone has tried? I tried the following modelÔºåRandomly select 20% of the samples as the validation set.Ôºö\n\n| Model | Local Validation | Public Leaderboard |\n| --- | --- | --- |\n| Llama3-8B-instruct | 0.9419 | 0.954 |\n| Llama3-8B | 0.9818 | 0.987 |\n| Gemma2-9B-instruct | 0.9262 | 1.206 |\n| Gemma2-9B | 0.9499 | 1.299 |\n\nGemma2-9B has obtained abnormal results, I guess it might be a problem with the inference. Does anyone have similar problems?\n\nUPDATE:\n\nWith the [new public notebook](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora), the correct results were obtained.\n\n| Model | Local Validation | Public Leaderboard |\n| --- | --- | --- |\n| Llama3-8B-instruct | 0.9419 | 0.954 |\n| Llama3-8B | 0.9818 | 0.987 |\n| Gemma2-9B-instruct | 0.9262 | 0.930 |\n| Gemma2-9B | 0.9499 | TODO‚Ä¶ |\n",
        "date": "Sat Jul 06 2024 21:28:26 GMT+0900 (Êó•Êú¨Ê®ôÊ∫ñÊôÇ)",
        "votes": "26"
    },
    "comments": [
        {
            "author": "Valentin Werner",
            "content": "gonna leave this one here üòâ\n\n",
            "date": "Posted a month ago  ¬∑  38th in this Competition",
            "votes": "5",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "The current local validation set is 0.91X, I still can't migrate to LB. LoL\n\n",
                    "date": "Posted 25 days ago  ¬∑  249th in this Competition",
                    "votes": "0",
                    "reply": []
                },
                {
                    "author": "SAY WHAT",
                    "content": "so funnyÔºÅÔºÅÔºÅ\n\n",
                    "date": "Posted 25 days ago  ¬∑  71st in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "Valentin Werner",
            "content": "Gemma2-9B came out recently. The 9B makes it even harder to train, but it tops the performance benchmarks among these models\n\n",
            "date": "Posted a month ago  ¬∑  38th in this Competition",
            "votes": "4",
            "reply": [
                {
                    "author": "Cody_Null",
                    "content": "Were you able to pull the gemma2-9B into kaggle from huggingface or are you using the Gemma 2 ¬∑ gemma-2-9b-pt ¬∑ V1 on kaggle models? \n\n",
                    "date": "Posted a month ago  ¬∑  30th in this Competition",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "Valentin Werner",
                            "content": "We pulled gemma2-9b from huggingface into kaggle.\n\n",
                            "date": "Posted a month ago  ¬∑  38th in this Competition",
                            "votes": "2",
                            "reply": []
                        }
                    ]
                },
                {
                    "author": "s111mple",
                    "content": "Finetuned model donnot get fine results~ Have you tried it?\n\n",
                    "date": "Posted 18 days ago  ¬∑  224th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "xiaotingting",
            "content": "It seems that the validation set index is positively correlated with the public score, and there is still room for further improvement of the index.\n\n",
            "date": "Posted 5 days ago  ¬∑  19th in this Competition",
            "votes": "0",
            "reply": []
        },
        {
            "author": "Xiot1206",
            "content": "thanks for providing these key information\n\n",
            "date": "Posted 19 days ago  ¬∑  70th in this Competition",
            "votes": "0",
            "reply": []
        },
        {
            "author": "lllleeeo",
            "content": "As an nlp newbie, I'd like to ask a possibly stupid question, how did you determine how many parameters you needed to use to participate in the fine-tuning, did you try them one by one? How much is generally best based on experience, is it different for different models, I observed that the public laptop fine-tuning in liama 8b only used 0.02% of the parameters is this too little?\n\n",
            "date": "Posted 21 days ago  ¬∑  126th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "If there is not enough computing power, using the Lora fine-tuning method may be the only choice.\n\n",
                    "date": "Posted 20 days ago  ¬∑  249th in this Competition",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "lllleeeo",
                            "content": "Thanks for your reply! I've rented an A100 and a 4090 and want to do some experiments in parallel, I'm wondering if I can try more parameters based on that computing power, but I'm not sure how much I should start trying.\n\n",
                            "date": "Posted 20 days ago  ¬∑  126th in this Competition",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "superfergTopic Author",
                            "content": "The first step can try the top-level public notebook.\n\n",
                            "date": "Posted 20 days ago  ¬∑  249th in this Competition",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "lllleeeo",
                            "content": "Thank you it worksÔºÅ\n\n",
                            "date": "Posted 18 days ago  ¬∑  126th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Mr.T",
            "content": "How do you load gemma 2-9b during inference?\n\n",
            "date": "Posted 21 days ago  ¬∑  21st in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "Please refer to the notebook belowÔºö\n\n[https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)\n\n",
                    "date": "Posted 20 days ago  ¬∑  249th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "EISLab_hwlee",
            "content": "Can the Gemma2-27B-instruct model perform better?\n\n",
            "date": "Posted 21 days ago  ¬∑  154th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "EISLab_hwlee",
                    "content": "As a result of the experiment, it was observed that the performance was poor.\n\n",
                    "date": "Posted 19 days ago  ¬∑  154th in this Competition",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "superfergTopic Author",
                            "content": "I still can't complete the reasoning of 27B within 9 hours, theoretically, 27B should achieve better results.\n\n",
                            "date": "Posted 18 days ago  ¬∑  249th in this Competition",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "EISLab_hwlee",
                            "content": "I also failed to submit it.\n\nHowever, in training, the loss did not fall below 1.0, and the evaluation loss did not fall below 1.0.\n\n",
                            "date": "Posted 18 days ago  ¬∑  154th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "hn",
            "content": "Just curious, what was the missing piece that lead to your poor inference results from Gemma2? I see that you mentioned it‚Äôs fixed with the public notebook \n\n",
            "date": "Posted 22 days ago  ¬∑  17th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "I don't have enough time to figure out the reason, but you can analyze the reason by comparing the following two notebooks.\n\n[https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference](https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference)\n\n[https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)\n\n",
                    "date": "Posted 20 days ago  ¬∑  249th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "Mukatai",
            "content": "In a recent public notebook, a score of 0.941 was recorded with fine-tuning of Gemma2, but this table shows a score of 0.930 with Gemma2-9B-instruct. Is there any difference?\n\n",
            "date": "Posted 23 days ago  ¬∑  367th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "I am using my own training script, so there should be some differences, I can make it public after the competition ends.\n\n",
                    "date": "Posted 22 days ago  ¬∑  249th in this Competition",
                    "votes": "3",
                    "reply": [
                        {
                            "author": "Mukatai",
                            "content": "Thank you. Is Gemma's training conducted on Kaggle? With a public notebook, training on a single dataset exceeds the 30-hour weekly limit\n\n",
                            "date": "Posted 19 days ago  ¬∑  367th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Femca7",
            "content": "May I ask the results you get is from pre-trained or finetuned model ?\n\n",
            "date": "Posted 25 days ago  ¬∑  155th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "You can see the details in the table I provided, those with an 'instruct' suffix are fine-tuned models.\n\n",
                    "date": "Posted 25 days ago  ¬∑  249th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "yechenzhi1",
            "content": "May I ask if Instruct model is better than the base model? I have only tried Instruct model.\n\n",
            "date": "Posted a month ago  ¬∑  25th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "superfergTopic Author",
                    "content": "According to my local testing, Llama3-8B instruct is better than Llama3-8B. But perhaps the appropriate hyperparameters for  Llama3-8B have not been found.\n\n",
                    "date": "Posted a month ago  ¬∑  249th in this Competition",
                    "votes": "1",
                    "reply": []
                },
                {
                    "author": "ducnh279",
                    "content": "I also had a similar question in the early days when I started with fine-tuning decoder-only models for text classification! \n\nI asked [@rasbtn](https://www.kaggle.com/rasbtn) (a prominent researcher/educator) on Twitter! He replied:\n\nI also conducted some experiments, and the results indicate that using instruction-tuned versions often gives better performance and faster convergence compared to the base model.\n\n",
                    "date": "Posted a month ago  ¬∑  765th in this Competition",
                    "votes": "4",
                    "reply": [
                        {
                            "author": "yechenzhi1",
                            "content": "Thanks! That's really helpful!\n\n",
                            "date": "Posted a month ago  ¬∑  25th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}