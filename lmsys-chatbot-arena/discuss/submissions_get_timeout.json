{
    "main_topic": {
        "author": "yechenzhi1",
        "title": "submissions get timeout?",
        "content": "Hi, I'm new to kaggle. I've submitted several times, all all my submissions failed due to timeout, but when I run it in my local Kaggle environment with T4*2 the inference time is as follows:\n\nAnd I got a warning like this:\n\n2024-05-19 01:36:52.192095: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n\n  2024-05-19 01:36:52.192192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n\n  2024-05-19 01:36:52.309490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\nBut I'm sure GPU is used during the inference.\n\nAny help would be appreciated. \n\n",
        "date": "Sun May 19 2024 11:28:12 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)",
        "votes": "6"
    },
    "comments": [
        {
            "author": "yechenzhi1Topic Author",
            "content": "thanks everyone helped! Setting batch size=1 solved my problemðŸ˜ƒ\n\n",
            "date": "Posted 3 months ago  Â·  25th in this Competition",
            "votes": "3",
            "reply": []
        },
        {
            "author": "yechenzhi1Topic Author",
            "content": "Another question is that, when we score in the public leaderboard, is the test dataset about 25000 * 0.3 rows? And when tested in the private leaderboard, it's about 25000 * 0.7 rows?\n\n",
            "date": "Posted 3 months ago  Â·  25th in this Competition",
            "votes": "1",
            "reply": [
                {
                    "author": "Kishan Vavdara",
                    "content": "Yes, that's correct! \n\n",
                    "date": "Posted 3 months ago  Â·  38th in this Competition",
                    "votes": "1",
                    "reply": []
                },
                {
                    "author": "Rich Olson",
                    "content": "I'll add that assuming this is like most contests - you're notebook is always run for the entire private / public test set.  It's just the scores for the private data portion of the scores is revealed at the end of the contest.\n\n",
                    "date": "Posted 3 months ago  Â·  1199th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "lijiang3859",
            "content": "Hey, [@yechenzhi1](https://www.kaggle.com/yechenzhi1). Thanks for your sharing! I also got this warning:\n\n```\n  warnings.warn(\n2024-07-06 05:05:32.818151: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-06 05:05:32.818272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-06 05:05:32.956771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n```\n\nHowever, my program does not raise bugs. Is there any influence on it?  By setting batch_size=1, the warning is gone?\n\n",
            "date": "Posted a month ago  Â·  1205th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "yechenzhi1Topic Author",
                    "content": "We can ignore this warning.\n\n",
                    "date": "Posted a month ago  Â·  25th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "lijiang3859",
            "content": "I think I also have the same issue with model=llama3-8B. Here is my script:\n\n```\nresults = []\ndf = pd.read_csv(args.test_file, dtype={'prompt': str, \"response_a\":str, \"response_b\":str})\ndf.fillna(\"''\", inplace=True)\ndf.replace('null', \"'null'\", inplace=True)\n\neval_dataset = Dataset.from_pandas(df)\nlength =  len(eval_dataset)\nfor i in tqdm(range(length)): # batch_size = 1\n    data = eval_dataset[i]\n    idx = data[\"id\"]\n    resp_a = template.format(data['prompt'], data['response_a'])\n    resp_b = template.format(data['prompt'], data['response_b'])\n    resp_tokens = tokenizer(\n        [resp_a, resp_b],\n        max_length=args.max_total_length,\n        padding=True,\n        truncation=True,\n        return_tensors=\"pt\",\n    )\n    # concated responses to save inference time -> batch_size =2\n    output = model(resp_tokens)\n\n```\n\nHere is some other settings to speed up the inference process:\n\nuse bf116=True for model initialization.\nuse autocast() and\n\nIs there any other process to speed up the inference? I have tested it with 25000 samples, it is very risky to excel the total training budget with 9hrs.\n\n",
            "date": "Posted a month ago  Â·  1205th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "yechenzhi1Topic Author",
                    "content": "[https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference](https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference)  you can check this notebook to see if it can help.\n\n",
                    "date": "Posted a month ago  Â·  25th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "Valentin Werner",
            "content": "One Remark, the test data has 25000 samples, so this will 10x your runtime. Technically that is still less than 540 minutes, but it is a lot slower\n\n",
            "date": "Posted 3 months ago  Â·  38th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "yechenzhi1Topic Author",
                    "content": "yes, so the prediction time should be 40x10 minutes, that's about 7 hours,  so it shouldn't be timeout.\n\n",
                    "date": "Posted 3 months ago  Â·  25th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "Rich Olson",
            "content": "how many rows are you testing prediction with?\n\n(when you score - it scores against 25,000)\n\n",
            "date": "Posted 3 months ago  Â·  1199th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "yechenzhi1Topic Author",
                    "content": "I tested 2500 rows, it was about 40 minutes.\n\n",
                    "date": "Posted 3 months ago  Â·  25th in this Competition",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "Rich Olson",
                            "content": "well - I can't think of anything obvious.  just assuming you aren't doing anything that takes a bunch of time before inference? (training / pre-processing / generating embeddings)?\n\nif you run out of ideas - I would try to test the workflow as close as possible to submission.\n\nI would load 25k rows from \"train\" into your \"test\" dataframe (and drop columns / etc to make it look like test).\n\nThen - I would save a version of your notebook.  That will run it like it was getting submitted.\n\nYou should then be able to look at the logs (even if it times out before finishing).\n\nMight want to try adding some logging / debug statements before doing.\n\n",
                            "date": "Posted 3 months ago  Â·  1199th in this Competition",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}