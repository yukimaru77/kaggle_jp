{
    "main_topic": {
        "author": "Lorry Zou",
        "title": "Are We Really on the Right Track?",
        "content": "From the competition description:\n\n\"This challenge aligns with the concept of \"reward models\" or \"preference models\" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias).\"\n\nLooks like the competition host encourage us to try reinforcement learning but everyone is still fine-tuning existing LLMs.ðŸ™‚ðŸ™ƒ\n\n",
        "date": "Sun Jul 21 2024 23:33:24 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)",
        "votes": "2"
    },
    "comments": [
        {
            "author": "CPMP",
            "content": "RLHF is a supervised learning method.  Labels are provided by humans, and are quite similar to the labels we have in this competition.\n\nNot sure what you suggest we do differently.\n\n",
            "date": "Posted 14 days ago  Â·  27th in this Competition",
            "votes": "3",
            "reply": []
        },
        {
            "author": "Dlond Mike",
            "content": "yepâ€¦.cause it's really perform great.it's a game for rich.(GPU :))\n\n",
            "date": "Posted 14 days ago  Â·  173rd in this Competition",
            "votes": "4",
            "reply": []
        },
        {
            "author": "chan peter",
            "content": "I tried out rlhf model and use the reward score as input and build a simple classifier, it work out great, but running the rlhf model is too time comsuing and I joined comp a bit late, don't have enough time to optimize it to pass the time limit.\n\n",
            "date": "Posted 3 days ago  Â·  1418th in this Competition",
            "votes": "0",
            "reply": []
        }
    ]
}