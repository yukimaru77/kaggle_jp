{
    "main_topic": {
        "author": "Valentin Werner",
        "title": "Danube3 (0.5 B & 4B) just dropped!",
        "content": "I have used Danube2 for several experiments and even with QLoRA on Kaggle GPU it seems to be a way better alternative to DeBERTa Large. \n\nDanube3 just came out as 0.5B and 4B and also comes with a Chat model, which might be an upside for this competition. The 4B model outperforms its 1.5B predecessor on all benchmarks by a lot (also a bit of a size difference), while the 0.5B outperforms Qwen2 0.5B on most benchmarks. However, to me it will be particularly interesting, how the 4B model compares to Phi3-Mini, as this is the only other model I know in its weight class. Maybe this is team Danube's secret? üòâ\n\nFrom my experience smaller models, like 0.5B will still not fit on Kaggle GPUs (it should work on a 4090), so I will focus on the 4B model.\n\nI also want to applaud the H2O Team, which is quite active on Kaggle, on this new release! It is always amazing, when talented researchers and Data Scientists contribute towards the Open LLM efforts (also the sheer speed of new releases). Looking forward to see how good this model is!\n\nLinks: \n\nModel card: [https://huggingface.co/h2oai/h2o-danube3-4b-chat](https://huggingface.co/h2oai/h2o-danube3-4b-chat)\n\nTechnical Report: [https://arxiv.org/abs/2407.09276](https://arxiv.org/abs/2407.09276)\n\nBenchmarks:\n\nSome benchmarks I aggrated from the [old open LLM leaderboard](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard). Danube3 is not included in the leaderboard yet, but reports these values on their model card. I think it is very interesting to see how close Danube3 comes to Gemma-7B and Mistral-7B.\n\n| Category | Benchmark | Danube3-4b-Chat | Danube2-1.8B-Chat | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7B Ins 0.2 |\n| --- | --- | --- | --- | --- | --- | --- |\n| Popular aggregated | MMLU  (5-shot) | 54.74 | 37.77 | 69.08 | 64.56 | 60.78 |\n| Language Understanding | HellaSwag (5-shot) | 80.36 | 73.54 | 80.60 | 82.20 | 84.88 |\n| Reasoning | ARC Challenge (5-shot) | 58.96 | 43.43 | 62.97 | 61.09 | 63.14 |\n|  | TruthfulQA (0-shot) | 47.79 | 39.96 | 59.88 | 44.79 | 68.26 |\n|  | WinoGrande (5-shot) | 76.48 | 69.77 | 71.6 | 79.01 | 77.19 |\n| Math | GSM8K CoT   (5-shot) | 50.18 | 26.16 | 85.7 | 50.87 | 40.03 |\n| Average |  | 61.42 | 48.44 | 69.91 | 63.75 | 63.14 |\n\nModels were chosen based on the models microsoft phi3-mini is reporting against on their model card.\n\n",
        "date": "Mon Jul 15 2024 15:57:26 GMT+0900 (Êó•Êú¨Ê®ôÊ∫ñÊôÇ)",
        "votes": "32"
    },
    "comments": [
        {
            "author": "chaneyMA",
            "content": "nice work!!!!\n\n",
            "date": "Posted 10 days ago  ¬∑  116th in this Competition",
            "votes": "1",
            "reply": []
        },
        {
            "author": "madarshbb",
            "content": "Just for curiosity,\n\nFrom my experience smaller models, like 0.5B will still not fit on Kaggle GPUs (it should work on a 4090), so I will focus on the 4B model.\n\nWhat do you mean by this? Shouldn't 0.5B model be easier to fit than 4B?\n\n",
            "date": "Posted 16 days ago  ¬∑  343rd in this Competition",
            "votes": "1",
            "reply": [
                {
                    "author": "Valentin WernerTopic Author",
                    "content": "I mean 0.5 is just big enough so you can't train it on Kaggle without quantization. This is basically similar size as DeBERTa Large\n\n",
                    "date": "Posted 16 days ago  ¬∑  38th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "Abhay Ayare",
            "content": "Fantastic guide! Thank you for sharing these valuable resources and insights on becoming a data scientist. Your passion for data science is inspiring. Looking forward to exploring your book \"Kaggle for Beginners.\"\n\n",
            "date": "Posted 20 days ago",
            "votes": "1",
            "reply": [
                {
                    "author": "Valentin WernerTopic Author",
                    "content": "There are plenty of kaggle books, but I certainly have not written one of them üòâ\n\n",
                    "date": "Posted 19 days ago  ¬∑  38th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "sayoulala",
            "content": "Thanks for you share, May I ask that the scores of this competition by the model ?\n\n",
            "date": "Posted 20 days ago  ¬∑  3rd in this Competition",
            "votes": "1",
            "reply": [
                {
                    "author": "Valentin WernerTopic Author",
                    "content": "I have no trained it yet. Some experiments (did not try super hard) got danube2-1.8B to .98x for me\n\n",
                    "date": "Posted 20 days ago  ¬∑  38th in this Competition",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "The-Hai Nguyen",
            "content": "You are always shedding light on my learning progress all the way back from the PII-detection competition. Really appreciate and thanks for your sharing, it helps me and the others learn a lot throughout the journey üôè.\n\n",
            "date": "Posted 20 days ago  ¬∑  829th in this Competition",
            "votes": "2",
            "reply": []
        }
    ]
}