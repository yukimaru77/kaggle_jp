{
    "main_topic": {
        "author": "Siddhantoon",
        "title": "Data Understanding: Why prompt is list of strings?",
        "content": "| prompt examples |\n| --- |\n| [\"Is it morally right to try to have a certain percentage of females on managerial positions?\",\"OK, does pineapple belong on a pizza? Relax and give me fun answer.\"] |\n| [\"hey\",\"write \\\"lollipop\\\" reversed\"] |\n| [\"What's the difference between a sine and a line?\",\"can you explain it even simpilier to me using examples?\",\"how does a sine keep going or whats an analogy using sine the expresses a continuation?\",\"What if AI becomes better than humans at everything. Maybe come up with an analogy involving geometry, thanks\"] |\n\nFor some the output of model a and b is also list of 2 strings for some it is single string.\n\n",
        "date": "Mon May 06 2024 20:36:28 GMT+0900 (日本標準時)",
        "votes": "19"
    },
    "comments": [
        {
            "author": "steubk",
            "content": "87% of train samples are chats with single prompt, while others have more prompts and responses\n\n",
            "date": "Posted 3 months ago",
            "votes": "16",
            "reply": []
        },
        {
            "author": "namtran",
            "content": "Thank you for your finding. I will try to extract individual conversations and see if it improves the model.\n\n",
            "date": "Posted 2 months ago  ·  1005th in this Competition",
            "votes": "1",
            "reply": []
        },
        {
            "author": "Valentin Werner",
            "content": "I recommend playing around with the tool in general. This might also gives you a better feeling for the data and competition in general!\n\nThe answer is pretty simple: You are not evaluating individual prompts, but full chats.\n\nWhile this opens a new question of \"What happens if you evaluate a chat after every prompt\" (which is possible) - I don't think it matters for the competition and assume that the data provided is always until the first evaluation.\n\n",
            "date": "Posted 3 months ago  ·  38th in this Competition",
            "votes": "4",
            "reply": [
                {
                    "author": "SiddhantoonTopic Author",
                    "content": "So actually we aren't evaluating a \"prompt and response\", technically we are evaluating a \"chat\". This increases the complexity on how long the chat is in the data\n\n",
                    "date": "Posted 3 months ago",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "Rich Olson",
            "content": "great find.  looking through the data - it seems like this is very common.  \n\noften the prompts seem disconnected from each-other - but sometimes they are clearly a continuing conversation.\n\n",
            "date": "Posted 3 months ago  ·  1199th in this Competition",
            "votes": "1",
            "reply": []
        },
        {
            "author": "Sparsh Tewatia",
            "content": "Even data is corrupt for around 200 rows, some null values , syntax errors. Will have to check for it in the test data\n\n",
            "date": "Posted 3 months ago  ·  512th in this Competition",
            "votes": "2",
            "reply": []
        }
    ]
}