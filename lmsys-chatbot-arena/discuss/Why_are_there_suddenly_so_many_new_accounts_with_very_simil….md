# Why are there suddenly so many new accounts with very similar scores?

**Robert0921** *Fri Jul 12 2024 11:58:20 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-1 votes)

Looking at PB, nearly 100 new accounts suddenly appeared in the past day, and their scores were all 0.941. Is there any problem?



---

 # Comments from other users

> ## Valentin Werner
> 
> Dropping silver solutions for free!
> 
> 
> 
> > ## Cody_Null
> > 
> > Thankfully I think it will be outside of the medal range at competition end but always wild to see a medal winning notebook in the last month of every competition haha
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > I think the most wild thing is just the copy & submit. I understand it If you joined early and never did another submission. However, what will happen next is 5 Notebooks that Ensemble these to get .940, .939, â€¦ without any novel ideas
> > > 
> > > This is not the kaggle way in my opinion.
> > > 
> > > I think it is great that this approach was shared because I read a lot of people are struggling with GPU resources to get something to work, the notebook that was shared just "works a little too well"
> > > 
> > > 
> > > 


---

> ## Xinyuan Qiao
> 
> there is a new notebook provide Gemma 2 inference code with 0.941 on PB, my original score was 0.942ðŸ˜‚
> 
> 
> 
> > ## yechenzhi1
> > 
> > try to ensemble itðŸ˜‚
> > 
> > 
> > 
> > > ## Lorry Zou
> > > 
> > > I tried to ensemble it with my 0.957 Llama3 inference, and I got 0.959ðŸ˜…ðŸ˜…
> > > 
> > > 
> > > 
> > > ## Allen Wang
> > > 
> > > [@lorryzouzelun](https://www.kaggle.com/lorryzouzelun) Is the integration of gemma and llama3 effective? I tried it and the lb dropped a lot
> > > 
> > > 
> > > 
> > ## Robert0921Topic Author
> > 
> > 
> > there is a new notebook provide Gemma 2 inference code with 0.941 on PB, my original score was 0.942ðŸ˜‚
> > 
> > Under the same conditions, which one is betterï¼ŸGemma 2 and Llama3
> > 
> > 
> > 
> > > ## Xinyuan Qiao
> > > 
> > > Not sure, the gemma notebook provider use A6000 for training and I don't know his hyper-parameters. I will use same parameters to train both and reply you later.
> > > 
> > > 
> > > 


---

