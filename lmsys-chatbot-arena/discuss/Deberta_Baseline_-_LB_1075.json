{
    "main_topic": {
        "author": "Fritz Cremer",
        "title": "Deberta Baseline - LB 1.075",
        "content": "I made a very quick deberta-v3-base baseline:\n\n[https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline/notebook](https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline/notebook)\n\nCurrently, it only uses a small fraction of the train data and doesn't get a great score. But this is how the code for a deberta submission could look like.\n\nPossible improvements:\n\n- Utilize all data\n\n- K Fold cross-validation\n\n- Swap the responses for more data\n\n- Formulate the loss differently\n\nEspecially the last one. I think it could make sense to have a two stage model. In the first stage, just predict if a response won a duel or not (without providing the other response), in the second stage, using two such predictions + hand crafted features to predict the better response. I think this looks like a very interesting competition, with not one straight forward path.\n\nLet me know what you think!\n\n",
        "date": "Fri May 03 2024 21:45:56 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)",
        "votes": "5"
    },
    "comments": [
        {
            "author": "Nicholas Broad",
            "content": "Just so you know, this is basically just random guessing.\n\n```\nfrom sklearn.metrics import log_loss\nlog_loss([1], [[1/3, 1/3, 1/3]], labels=[0,1,2])\n\n# 1.0986122886681098\n\n```\n\n",
            "date": "Posted 3 months ago  Â·  18th in this Competition",
            "votes": "2",
            "reply": [
                {
                    "author": "Valentin Werner",
                    "content": "The Notebook exactly replicates the label distribution. It seems that a basic starter with huggingface Trainer is not able to learn from the data.\n\n",
                    "date": "Posted 3 months ago  Â·  38th in this Competition",
                    "votes": "0",
                    "reply": []
                },
                {
                    "author": "Fritz CremerTopic Author",
                    "content": "[@nbroad](https://www.kaggle.com/nbroad) Yes, I know. It was more like a general setup to fit a model for this task with huggingface. I found that the training was very unstable, on some runs the model learned more than just the label distribution (e.g. on LB the notebook has 1.075 with the submitted version), and on others it failed completely. But then again, it is not a well tuned approach at all, just a quick first day approach ðŸ˜„\n\n",
                    "date": "Posted 3 months ago  Â·  1594th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        }
    ]
}