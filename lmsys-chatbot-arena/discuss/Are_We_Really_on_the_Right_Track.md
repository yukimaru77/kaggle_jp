# Are We Really on the Right Track?

**Lorry Zou** *Sun Jul 21 2024 23:33:24 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

From the competition description:

"This challenge aligns with the concept of "reward models" or "preference models" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias)."

Looks like the competition host encourage us to try reinforcement learning but everyone is still fine-tuning existing LLMs.ðŸ™‚ðŸ™ƒ



---

 # Comments from other users

> ## CPMP
> 
> RLHF is a supervised learning method.  Labels are provided by humans, and are quite similar to the labels we have in this competition.
> 
> Not sure what you suggest we do differently.
> 
> 
> 


---

> ## Dlond Mike
> 
> yepâ€¦.cause it's really perform great.it's a game for rich.(GPU :))
> 
> 
> 


---

> ## chan peter
> 
> I tried out rlhf model and use the reward score as input and build a simple classifier, it work out great, but running the rlhf model is too time comsuing and I joined comp a bit late, don't have enough time to optimize it to pass the time limit.
> 
> 
> 


---

