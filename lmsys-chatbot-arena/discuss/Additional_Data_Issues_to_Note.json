{
    "main_topic": {
        "author": "AbaoJiang",
        "title": "Additional Data Issues to Note",
        "content": "Hi everyone,\n\nAfter more EDA in my [LMSYS - Detailed EDA](https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda?scriptVersionId=180273328), I find additional data issues to report and discuss with you.\n\n### Not Only null But Also Empty Responses\n\nAs pointed out in [this forum](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303), we know that there exists null response issue. Here, I want to share another similar issue that responses from models are empty strings. I currently check if there's at least one model response equal to \"\" during one conversation, and the answer is yes. \n\n[](https://postimg.cc/75R2GD0h)\n\nAs illustrated above, we can observe that,\n\nThere exists no samples with None/empty prompts.\nThere exist 120+ rows with None responses for both A and B.\nThere exist 30+ rows with empty responses for both A and B.\n\nYou can also test strings like space only (e.g., \" \", \"    \"), new line only (e.g., \\n), etc.\n\n### Unintentional Mixed Prompts and Responses?\n\n[](https://postimg.cc/F1xDBP2p)\n\nWhen exploring missing values, we find another interesting chat example shown above. As can be seen, model A gpt-4-0613 responds an empty string, but model B responds normally. Also, the ending of the prompt is the same as the response.\n\nOut of curiosity, we feed the same prompt to Chatbot Arena in two forms,\n\n#### a. Feed this Prompt As Is\n\n[](https://postimg.cc/7fzm9sPK)\n\n#### b. Feed this Prompt without QAs at the End of Prompt\n\n[](https://postimg.cc/qg9pnynr)\n\nAs can be seen, model A in the first case might take QA information at the end of the prompt as a response. In the second case, model B provides a similar answer without QA information in the prompt.\n\nSo, our questions are\n\nWhat's the difference between null responses and empty responses?\nIs it possible that there exist samples which unintentionally mixes responses into prompts?\n\nPlease feel free to share your thoughts on these topics, thanks!\n\n",
        "date": "Wed May 29 2024 01:05:17 GMT+0900 (日本標準時)",
        "votes": "7"
    },
    "comments": [
        {
            "author": "dragon zhang",
            "content": "thanks for sharing.  The score is changing. Is metric changed or test data?\n\n",
            "date": "Posted 2 months ago  ·  957th in this Competition",
            "votes": "1",
            "reply": [
                {
                    "author": "AbaoJiangTopic Author",
                    "content": "Hi, \n\nThere exists data leakage issue, so the solution file is modified and notebooks are re-scored. You can refer to [this forum](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/506137). Hope this helps.\n\n",
                    "date": "Posted 2 months ago  ·  127th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "Dr. Gopalashivabalasubramanium Chandrashekaran",
            "content": "Interesting! I was avoiding checking the text data columns because of this. \n\nQualitative response analysis will be tricky. What if the user asked a question the model could not answer such as outside of the regulations that the model is put under? \n\nHowever, since there are 50k+ rows, I would think dropping these null values wouldn't affect training a model too intensely.\n\nIt gives me idea that some type of ensembled model where user prompt is filtered for potential to generate a null response.\n\n",
            "date": "Posted 2 months ago",
            "votes": "0",
            "reply": []
        }
    ]
}