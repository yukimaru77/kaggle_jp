# Llama3.1 works not as good as expect

**justin1357** *Wed Jul 24 2024 13:44:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (10 votes)

After first 1000 steps, i found its performence is worse than gemma-2 slightly.



---

 # Comments from other users

> ## Rise_Hand
> 
> I got a very bad result according to llama3.1 while using the ft method of QLoRA+SFT. CV = 0.914
> 
> 
> 


---

> ## Nicholas Broad
> 
> Make sure your tokenizer uses the [correct bos token](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/discussions/29)
> 
> 
> 
> > ## justin1357Topic Author
> > 
> > still bad performance
> > 
> > 
> > 


---

> ## Xinyuan Qiao
> 
> I tested it in [@emiz6413](https://www.kaggle.com/emiz6413) notebook with exact same parameter, the evaluation log loss is 0.958, the gemma2 version was 0.927.
> 
> 
> 


---

> ## sayoulala
> 
> Thanks for share it. That's great, it looks like I'll be able to save a lot on my electricity bill, hahaha!
> 
> 
> 
> > ## william.wu
> > 
> > You're safe for the 1st place. It's tough to make improvementsðŸ˜­
> > 
> > 
> > 


---

> ## justin1357Topic Author
> 
> After 4000 steps, its significantly worse than gemma-2 haha
> 
> 
> 


---

> ## Lorry Zou
> 
> Thank you for saving our time and TPU quotaðŸ˜
> 
> 
> 


---

> ## Yixiao Yuan
> 
> Same here.
> 
> 
> 


---

