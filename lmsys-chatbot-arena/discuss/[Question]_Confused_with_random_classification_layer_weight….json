{
    "main_topic": {
        "author": "Xuhang_CN",
        "title": "[Question] Confused with random classification layer weight(score.weight) for LlamaForSequenceClassification",
        "content": "Hi , I am a freshman for llm. I want to SFT llama3 in my GPU.\n\nWhen I check for the same train environment, I find classification layer weight is initialized randomly(maybe?):\n\nWhen I run below code first time and second time, I get different score.weight.\n\nmodel_raw = LlamaForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=3,\n    device_map='auto'\nmodel = prepare_model_for_kbit_training(model)\nconfig = PeftConfig.from_pretrained(finetune_model_name)\nmodel = PeftModel.from_pretrained(model, finetune_model_name,is_trainable=False)\n)\n\nEven if I load saved adapter weight, weight is still not the same.\n\nSo I want to know how to make sure I train llama3 in my GPU and upload adapter in kaggle to make sure I get the same model.\n\nThanks!\n\n",
        "date": "Sat Jul 20 2024 19:07:52 GMT+0900 (日本標準時)",
        "votes": "0"
    },
    "comments": [
        {
            "author": "Valentin Werner",
            "content": "You could set the seed for the run. That way you always get the same initial values. Make sure versions are similar / identical between kaggle notebook and local. There are plenty of \"seed all\" functions on kaggle and google that you can use\n\n",
            "date": "Posted 15 days ago  ·  38th in this Competition",
            "votes": "2",
            "reply": []
        },
        {
            "author": "hn",
            "content": "You may need to do a custom save LoRA as some combinations may render the LoRA saving the wrong weights.\n\n",
            "date": "Posted 15 days ago  ·  17th in this Competition",
            "votes": "0",
            "reply": []
        }
    ]
}