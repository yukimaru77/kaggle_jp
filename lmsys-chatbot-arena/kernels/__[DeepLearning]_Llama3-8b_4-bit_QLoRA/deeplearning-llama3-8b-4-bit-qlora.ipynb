{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330de5f2",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:01.882737Z",
     "iopub.status.busy": "2024-07-28T14:10:01.882391Z",
     "iopub.status.idle": "2024-07-28T14:10:32.192327Z",
     "shell.execute_reply": "2024-07-28T14:10:32.191393Z",
     "shell.execute_reply.started": "2024-07-28T14:10:01.882707Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:32.19456Z",
     "iopub.status.busy": "2024-07-28T14:10:32.194259Z",
     "iopub.status.idle": "2024-07-28T14:10:50.243769Z",
     "shell.execute_reply": "2024-07-28T14:10:50.243013Z",
     "shell.execute_reply.started": "2024-07-28T14:10:32.194534Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95686997",
   "metadata": {},
   "source": [
    "## Model Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:50.24555Z",
     "iopub.status.busy": "2024-07-28T14:10:50.24483Z",
     "iopub.status.idle": "2024-07-28T14:10:50.283887Z",
     "shell.execute_reply": "2024-07-28T14:10:50.282869Z",
     "shell.execute_reply.started": "2024-07-28T14:10:50.245513Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3f0e9",
   "metadata": {},
   "source": [
    "## Load sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:50.287241Z",
     "iopub.status.busy": "2024-07-28T14:10:50.286357Z",
     "iopub.status.idle": "2024-07-28T14:10:53.662602Z",
     "shell.execute_reply": "2024-07-28T14:10:53.661834Z",
     "shell.execute_reply.started": "2024-07-28T14:10:50.287216Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(20)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8952dc",
   "metadata": {},
   "source": [
    "## Tokenizer and Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:53.663955Z",
     "iopub.status.busy": "2024-07-28T14:10:53.663655Z",
     "iopub.status.idle": "2024-07-28T14:10:56.889319Z",
     "shell.execute_reply": "2024-07-28T14:10:56.888515Z",
     "shell.execute_reply.started": "2024-07-28T14:10:53.663917Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:56.890762Z",
     "iopub.status.busy": "2024-07-28T14:10:56.890455Z",
     "iopub.status.idle": "2024-07-28T14:10:57.212413Z",
     "shell.execute_reply": "2024-07-28T14:10:57.211643Z",
     "shell.execute_reply.started": "2024-07-28T14:10:56.890737Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5c4a4",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:02:43.326732Z",
     "iopub.status.busy": "2024-07-28T14:02:43.326452Z",
     "iopub.status.idle": "2024-07-28T14:02:43.334273Z",
     "shell.execute_reply": "2024-07-28T14:02:43.333179Z",
     "shell.execute_reply.started": "2024-07-28T14:02:43.326708Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8f48a",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomLlama3(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLlama3.from_pretrained(\n",
    "    model_path, \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/kaggle/working/model\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118f323",
   "metadata": {},
   "source": [
    "### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:10:47.631227Z",
     "iopub.status.busy": "2024-07-28T13:10:47.630555Z",
     "iopub.status.idle": "2024-07-28T13:10:47.660935Z",
     "shell.execute_reply": "2024-07-28T13:10:47.659985Z",
     "shell.execute_reply.started": "2024-07-28T13:10:47.63119Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=5,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2, ##GPU 16GB o\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b248c",
   "metadata": {},
   "source": [
    "# Training (Only Run one of Training or Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:10:47.662368Z",
     "iopub.status.busy": "2024-07-28T13:10:47.662033Z",
     "iopub.status.idle": "2024-07-28T13:18:14.683109Z",
     "shell.execute_reply": "2024-07-28T13:18:14.682131Z",
     "shell.execute_reply.started": "2024-07-28T13:10:47.662344Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:18:14.685291Z",
     "iopub.status.busy": "2024-07-28T13:18:14.684401Z",
     "iopub.status.idle": "2024-07-28T13:18:15.204301Z",
     "shell.execute_reply": "2024-07-28T13:18:15.20348Z",
     "shell.execute_reply.started": "2024-07-28T13:18:14.685255Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('pretrained_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa7ea9",
   "metadata": {},
   "source": [
    "# Inference (Only Run one of Training or Inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:57.213717Z",
     "iopub.status.busy": "2024-07-28T14:10:57.213437Z",
     "iopub.status.idle": "2024-07-28T14:10:57.218427Z",
     "shell.execute_reply": "2024-07-28T14:10:57.217578Z",
     "shell.execute_reply.started": "2024-07-28T14:10:57.213693Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:57.220185Z",
     "iopub.status.busy": "2024-07-28T14:10:57.219623Z",
     "iopub.status.idle": "2024-07-28T14:12:37.907647Z",
     "shell.execute_reply": "2024-07-28T14:12:37.906698Z",
     "shell.execute_reply.started": "2024-07-28T14:10:57.220153Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Đường dẫn tới mô hình đã được huấn luyện trước và file Lora adapter\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "lora_adapter_path = \"/kaggle/working/pretrained_model\"\n",
    "\n",
    "# Tải mô hình gốc\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# Tải tokenizer tương ứng\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Cấu hình Lora\n",
    "lora_config = peft_config\n",
    "\n",
    "# Chuẩn bị mô hình cho k-bit training nếu cần\n",
    "model_1 = prepare_model_for_kbit_training(model_1)\n",
    "\n",
    "# Áp dụng Lora Adapter vào mô hình\n",
    "model_1 = get_peft_model(model_1, lora_config)\n",
    "\n",
    "# Tải các tham số của Lora Adapter đã lưu trước đó\n",
    "model_1.load_adapter(lora_adapter_path, adapter_name=\"test\")\n",
    "\n",
    "# Mô hình hoàn chỉnh đã sẵn sàng sử dụng\n",
    "model_1.eval()  # Đặt mô hình vào chế độ đánh giá nếu cần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:37:05.617083Z",
     "iopub.status.busy": "2024-07-28T14:37:05.616197Z",
     "iopub.status.idle": "2024-07-28T14:37:05.622623Z",
     "shell.execute_reply": "2024-07-28T14:37:05.621685Z",
     "shell.execute_reply.started": "2024-07-28T14:37:05.617048Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def softmax(row):\n",
    "    e_row = np.exp(row - np.max(row))\n",
    "    return e_row / e_row.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:12:37.917502Z",
     "iopub.status.busy": "2024-07-28T14:12:37.917246Z",
     "iopub.status.idle": "2024-07-28T14:12:37.954873Z",
     "shell.execute_reply": "2024-07-28T14:12:37.954057Z",
     "shell.execute_reply.started": "2024-07-28T14:12:37.917481Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ds.to_pandas()[0:10]\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)\n",
    "display(data[:3])\n",
    "print()\n",
    "\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:12:37.956701Z",
     "iopub.status.busy": "2024-07-28T14:12:37.956134Z",
     "iopub.status.idle": "2024-07-28T14:12:37.967859Z",
     "shell.execute_reply": "2024-07-28T14:12:37.966691Z",
     "shell.execute_reply.started": "2024-07-28T14:12:37.956669Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=2, max_length=1024):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        labels = tmp[\"labels\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        pad_labels=[]\n",
    "        for label in labels:\n",
    "            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "        labels = torch.tensor(pad_labels).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    df['winner_model_a'] = a_win\n",
    "    df['winner_model_b'] = b_win\n",
    "    df['winner_tie'] = tie\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:38:08.823708Z",
     "iopub.status.busy": "2024-07-28T14:38:08.822854Z",
     "iopub.status.idle": "2024-07-28T14:38:17.783376Z",
     "shell.execute_reply": "2024-07-28T14:38:17.782458Z",
     "shell.execute_reply.started": "2024-07-28T14:38:08.823675Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df  = inference(data[0:4], model_1, device, batch_size=2, max_length=1024)\n",
    "\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "        \n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "\n",
    "result_df['winner_model_a'] = result_df['winner_model_a'].apply(lambda x: x[0])\n",
    "result_df['winner_model_b'] = result_df['winner_model_b'].apply(lambda x: x[0])\n",
    "result_df['winner_tie'] = result_df['winner_tie'].apply(lambda x: x[0])\n",
    "\n",
    "result_df"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
