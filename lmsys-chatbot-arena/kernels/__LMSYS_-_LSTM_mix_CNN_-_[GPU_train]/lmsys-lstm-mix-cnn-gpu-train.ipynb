{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abde0009",
   "metadata": {},
   "source": [
    "# Import libs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:06.171683Z",
     "iopub.status.busy": "2024-07-14T01:07:06.171259Z",
     "iopub.status.idle": "2024-07-14T01:07:07.336959Z",
     "shell.execute_reply": "2024-07-14T01:07:07.335917Z",
     "shell.execute_reply.started": "2024-07-14T01:07:06.171637Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fcbff",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:07.339244Z",
     "iopub.status.busy": "2024-07-14T01:07:07.338802Z",
     "iopub.status.idle": "2024-07-14T01:07:08.623098Z",
     "shell.execute_reply": "2024-07-14T01:07:08.622046Z",
     "shell.execute_reply.started": "2024-07-14T01:07:07.339209Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2')\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "# save tokenizer to load offline during inference\n",
    "tokenizer.save_pretrained('tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:08.624648Z",
     "iopub.status.busy": "2024-07-14T01:07:08.624328Z",
     "iopub.status.idle": "2024-07-14T01:07:08.629717Z",
     "shell.execute_reply": "2024-07-14T01:07:08.628857Z",
     "shell.execute_reply.started": "2024-07-14T01:07:08.624622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Utility function giving token length\n",
    "def get_token_lengths(texts):\n",
    "    # tokenize and receive input_ids for reach text\n",
    "    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "    # return length of inputs_ids for each text\n",
    "    return [len(t) for t in input_ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9447cd",
   "metadata": {},
   "source": [
    "# Prepare train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:08.632508Z",
     "iopub.status.busy": "2024-07-14T01:07:08.632241Z",
     "iopub.status.idle": "2024-07-14T01:07:12.59289Z",
     "shell.execute_reply": "2024-07-14T01:07:12.591942Z",
     "shell.execute_reply.started": "2024-07-14T01:07:08.632485Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "# Drop 'Null' for training\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")\n",
    "print('Total train samples: ', len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:12.59434Z",
     "iopub.status.busy": "2024-07-14T01:07:12.594039Z",
     "iopub.status.idle": "2024-07-14T01:07:12.611151Z",
     "shell.execute_reply": "2024-07-14T01:07:12.610173Z",
     "shell.execute_reply.started": "2024-07-14T01:07:12.594316Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:12.613447Z",
     "iopub.status.busy": "2024-07-14T01:07:12.612751Z",
     "iopub.status.idle": "2024-07-14T01:07:12.890655Z",
     "shell.execute_reply": "2024-07-14T01:07:12.889697Z",
     "shell.execute_reply.started": "2024-07-14T01:07:12.613401Z"
    }
   },
   "outputs": [],
   "source": [
    "train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\n",
    "print(train['text'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T01:07:12.892091Z",
     "iopub.status.busy": "2024-07-14T01:07:12.89181Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train with only take 50% train dataset\n",
    "train = train[:int(len(train) * 1)]\n",
    "\n",
    "train.loc[:, 'token_count'] = get_token_lengths(train['text'])\n",
    "\n",
    "# prepare label for model\n",
    "train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n",
    "\n",
    "# Display data\n",
    "display(train.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# token Count\n",
    "display(train['token_count'].describe().to_frame().astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get length of tokens which covers 90% of data, we'll still take 1024 length!\n",
    "np.percentile(train['token_count'], 90)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d49594c",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(), \n",
    "    max_length=1024, \n",
    "    truncation=True, \n",
    "    return_tensors='np')\n",
    "\n",
    "# Input IDs are the token IDs\n",
    "INPUT_IDS = tokens['input_ids']\n",
    "# Attention Masks to Ignore Padding Tokens\n",
    "ATTENTION_MASKS = tokens['attention_mask']\n",
    "# Label of Texts\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n",
    "print(f'LABELS shape: {LABELS.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 21540#14300\n",
    "maxlen = 1024\n",
    "batch_size = 16\n",
    "embedding_dims = 200\n",
    "nb_filter = 150\n",
    "filter_length = 3\n",
    "hidden_dims = 100\n",
    "nb_epoch = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Convolution1D, LSTM\n",
    "from keras.datasets import imdb\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adadelta\n",
    "from keras.preprocessing import sequence as sq\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.python.keras.backend import set_session as K\n",
    "X_train = sq.pad_sequences(INPUT_IDS, maxlen=maxlen)\n",
    "y_train = LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7499a983",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This example demonstrates the use of Convolution1D for text classification.\n",
    "Gets to 0.88 test accuracy after 2 epochs.\n",
    "90s/epoch on Intel i5 2.4Ghz CPU.\n",
    "10s/epoch on Tesla K40 GPU.\n",
    "'''\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import  GlobalMaxPooling1D\n",
    "\n",
    "#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \\\n",
    "#                        allow_soft_placement=True, device_count = {'CPU': 1})\n",
    "\n",
    "\n",
    "# tf_config = K.tf.ConfigProto()\n",
    "# tf_config.gpu_options.allow_growth = True\n",
    "# session = K.tf.Session(config=tf_config)\n",
    "# K.set_session(session)\n",
    "\n",
    "# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \\\n",
    "#                         allow_soft_placement=True, device_count = {'CPU': 4})\n",
    "# session = K.tf.Session(config=config)\n",
    "# K.set_session(session)\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "input_layer = Input(shape=(maxlen,),dtype='int64', name='main_input')\n",
    "emb_layer = Embedding(max_features,\n",
    "                      embedding_dims,\n",
    "                      input_length=maxlen\n",
    "                      )(input_layer)\n",
    "def max_1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 3:\n",
    "\n",
    "con3_layer = Convolution1D(filters=nb_filter,\n",
    "                    padding='valid',\n",
    "                    activation='relu',\n",
    "                    kernel_size =3,\n",
    "                    strides=1)(emb_layer)\n",
    "\n",
    "pool_con3_layer = GlobalMaxPooling1D()(con3_layer)\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 4:\n",
    "\n",
    "con4_layer = Convolution1D(filters=nb_filter,\n",
    "                    kernel_size=5,\n",
    "                    padding='valid',\n",
    "                    activation='relu',\n",
    "                    strides=1)(emb_layer)\n",
    "\n",
    "pool_con4_layer = GlobalMaxPooling1D()(con4_layer)\n",
    "\n",
    "\n",
    "# we add a Convolution1D, which will learn nb_filter\n",
    "# word group filters of size 5:\n",
    "\n",
    "con5_layer = Convolution1D(filters=nb_filter,\n",
    "                    kernel_size=7,\n",
    "                    padding='valid',\n",
    "                    activation='relu',\n",
    "                    strides=1)(emb_layer)\n",
    "\n",
    "pool_con5_layer = GlobalMaxPooling1D()(con5_layer)\n",
    "\n",
    "\n",
    "cnn_layer =Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])\n",
    "\n",
    "\n",
    "#LSTM\n",
    "\n",
    "\n",
    "x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)\n",
    "lstm_layer = LSTM(128)(x)\n",
    "\n",
    "cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])\n",
    "\n",
    "dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)\n",
    "output_layer= Dropout(0.2)(dense_layer)\n",
    "output_layer = Dense(3, trainable=True,activation='softmax')(output_layer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "adadelta = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-06)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adamax\",\n",
    "              metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e017ff",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',\n",
    "                                 monitor='val_acc', verbose=0, save_best_only=True,\n",
    "                                 mode='max')\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=16,\n",
    "          epochs=nb_epoch,\n",
    "          callbacks=[checkpoint])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=\"adamax\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_LSTM_mix_CNN.keras')  # Lưu toàn bộ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c6193",
   "metadata": {},
   "source": [
    "# Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# Drop 'Null' for training\n",
    "indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index\n",
    "test.drop(indexes, inplace=True)\n",
    "test.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")\n",
    "print('Total train samples: ', len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\n",
    "print(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize Data\n",
    "tokens_test = tokenizer(\n",
    "    test['text'].tolist(), \n",
    "    max_length=1024, \n",
    "    truncation=True, \n",
    "    return_tensors='np')\n",
    "\n",
    "# Input IDs are the token IDs\n",
    "INPUT_test = tokens_test['input_ids']\n",
    "# Attention Masks to Ignore Padding Tokens\n",
    "ATTENTION_MASKS2 = tokens_test['attention_mask']\n",
    "\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "result_df = pd.concat([test['id'], winner_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ea3ab2",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 1899,
     "sourceId": 2622,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
