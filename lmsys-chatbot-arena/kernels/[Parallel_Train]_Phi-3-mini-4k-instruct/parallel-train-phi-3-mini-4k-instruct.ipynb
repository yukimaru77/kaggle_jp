{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2338cbe",
   "metadata": {},
   "source": [
    "# No Installation Required\n",
    "\n",
    "microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training\n",
    "\n",
    "The max sequence length has a significant impact on model performance, \n",
    "but due to insufficient memory, it was set to a maximum length of 768.\n",
    "\n",
    "\n",
    "## Load Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:40.963732Z",
     "iopub.status.busy": "2024-07-27T22:50:40.962925Z",
     "iopub.status.idle": "2024-07-27T22:50:40.978385Z",
     "shell.execute_reply": "2024-07-27T22:50:40.977577Z",
     "shell.execute_reply.started": "2024-07-27T22:50:40.963692Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:41.221379Z",
     "iopub.status.busy": "2024-07-27T22:50:41.221089Z",
     "iopub.status.idle": "2024-07-27T22:50:56.767411Z",
     "shell.execute_reply": "2024-07-27T22:50:56.766638Z",
     "shell.execute_reply.started": "2024-07-27T22:50:41.221356Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss\n",
    "\n",
    "from accelerate import notebook_launcher, Accelerator, PartialState\n",
    "from accelerate.utils import write_basic_config\n",
    "from accelerate.inference import prepare_pippy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.769675Z",
     "iopub.status.busy": "2024-07-27T22:50:56.769113Z",
     "iopub.status.idle": "2024-07-27T22:50:56.775287Z",
     "shell.execute_reply": "2024-07-27T22:50:56.774322Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.769648Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "model_name = \"/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1\"\n",
    "model_path = \"model_checkpoint.pth\"\n",
    "seed = 42\n",
    "lora_r = 2\n",
    "quantize_bit = 16\n",
    "learning_rate = 5e-4\n",
    "weight_decay = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "eps = 1e-9\n",
    "l1_rate = 1e-10\n",
    "batch_size = 1\n",
    "max_len = 256\n",
    "n_sample = 0.10\n",
    "n_epoch = 2\n",
    "device = \"cuda\"\n",
    "file_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08cba3",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.776658Z",
     "iopub.status.busy": "2024-07-27T22:50:56.776375Z",
     "iopub.status.idle": "2024-07-27T22:50:56.78264Z",
     "shell.execute_reply": "2024-07-27T22:50:56.781862Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.776635Z"
    }
   },
   "outputs": [],
   "source": [
    "def cl(x):\n",
    "  if x == [1,0,0]:\n",
    "    return 0\n",
    "  elif x == [0,1,0]:\n",
    "    return 1\n",
    "  else :\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.785091Z",
     "iopub.status.busy": "2024-07-27T22:50:56.784682Z",
     "iopub.status.idle": "2024-07-27T22:50:56.798035Z",
     "shell.execute_reply": "2024-07-27T22:50:56.797073Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.785068Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, sample = False):\n",
    "    train = pd.read_csv(file_path)\n",
    "    clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]\n",
    "\n",
    "    clf_train.loc[:, \"prompt\"] = clf_train[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
    "    clf_train.loc[:, \"response_a\"] = clf_train[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
    "    clf_train.loc[:, \"response_b\"] = clf_train[\"response_b\"].apply(lambda x: json.loads(x)[0])\n",
    "\n",
    "    clf_train = clf_train.dropna()\n",
    "    clf_train = clf_train.reset_index(drop = True)\n",
    "\n",
    "    clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train)) ]\n",
    "\n",
    "    clf_train = clf_train[['prompt','response_a','response_b','target']]\n",
    "\n",
    "    clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))\n",
    "\n",
    "    clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))\n",
    "    clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))\n",
    "    clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))\n",
    "\n",
    "    clf_train['len'] = clf_train['p_len'] + clf_train['a_len']+ clf_train['b_len']\n",
    "    \n",
    "    if sample:\n",
    "        clf_train = clf_train.sample(int(len(clf_train)*n_sample), weights = \"len\", random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    t_dat, v_dat = train_test_split(clf_train, test_size=0.2, random_state=42, stratify = clf_train['labels'])\n",
    "\n",
    "    t_dat = t_dat.reset_index(drop=True)\n",
    "    v_dat = v_dat.reset_index(drop=True)\n",
    "\n",
    "    t_dat = t_dat.drop( labels= 'target' , axis = 1)\n",
    "    v_dat = v_dat.drop( labels= 'target' , axis = 1)\n",
    "    return t_dat, v_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.800082Z",
     "iopub.status.busy": "2024-07-27T22:50:56.799429Z",
     "iopub.status.idle": "2024-07-27T22:50:56.813998Z",
     "shell.execute_reply": "2024-07-27T22:50:56.81315Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.800051Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = df['prompt']\n",
    "        self.response_a = df['response_a']\n",
    "        self.response_b = df['response_b']\n",
    "        self.max_len = max_len\n",
    "        self.targets = df.get('labels', None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prompt = str(self.prompt[index])\n",
    "        response_a = str(self.response_a[index])\n",
    "        response_b = str(self.response_b[index])\n",
    "\n",
    "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])\n",
    "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])\n",
    "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])\n",
    "\n",
    "        final_prompt_len = min(self.max_len, prompt_len)\n",
    "        final_a_len = min(self.max_len, response_a_len)\n",
    "        final_b_len = min(self.max_len, response_b_len)\n",
    "\n",
    "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
    "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            labels = torch.LongTensor([self.targets[index]])\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}\n",
    "        else:\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.815829Z",
     "iopub.status.busy": "2024-07-27T22:50:56.815186Z",
     "iopub.status.idle": "2024-07-27T22:50:56.825043Z",
     "shell.execute_reply": "2024-07-27T22:50:56.824259Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.815797Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer):\n",
    "\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None\n",
    "\n",
    "    # Find the maximum length of the sequences in the batch\n",
    "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
    "\n",
    "    # Re-tokenize with the new max length\n",
    "    new_input_ids = []\n",
    "    new_attention_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids'][:max_len]\n",
    "        attention_mask = item['attention_mask'][:max_len]\n",
    "\n",
    "        new_input_ids.append(input_ids)\n",
    "        new_attention_masks.append(attention_mask)\n",
    "\n",
    "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    output = {\n",
    "    'input_ids': new_input_ids,\n",
    "    'attention_mask': new_attention_masks}\n",
    "\n",
    "    if labels is not None:\n",
    "        output['labels'] = labels\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.826275Z",
     "iopub.status.busy": "2024-07-27T22:50:56.826003Z",
     "iopub.status.idle": "2024-07-27T22:50:56.835718Z",
     "shell.execute_reply": "2024-07-27T22:50:56.834859Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.826254Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):\n",
    "    dataloader = DataLoader(\n",
    "        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff4b22",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.838013Z",
     "iopub.status.busy": "2024-07-27T22:50:56.837391Z",
     "iopub.status.idle": "2024-07-27T22:50:56.846877Z",
     "shell.execute_reply": "2024-07-27T22:50:56.845932Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.837983Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, num_bits=quantize_bit):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "\n",
    "    quantized_tensor = torch.round(tensor / scale + zero_point)\n",
    "    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)\n",
    "    quantized_tensor = (quantized_tensor - zero_point) * scale\n",
    "\n",
    "    return quantized_tensor\n",
    "\n",
    "def quantize_model(model, num_bits=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# import torch.quantization\n",
    "\n",
    "# def quantize_model_dynamic(model):\n",
    "#     model.qconfig = torch.quantization.default_dynamic_qconfig\n",
    "#     torch.quantization.prepare(model, inplace=True)\n",
    "#     torch.quantization.convert(model, inplace=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90398fff",
   "metadata": {},
   "source": [
    "## Add Adepter Layers\n",
    "Copied from transformers.models.phi3.modeling_phi3.Phi3Attention\n",
    "\n",
    "[github url](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.848239Z",
     "iopub.status.busy": "2024-07-27T22:50:56.847937Z",
     "iopub.status.idle": "2024-07-27T22:50:56.856779Z",
     "shell.execute_reply": "2024-07-27T22:50:56.85601Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.848216Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)\n",
    "        self.dropout = nn.Dropout(lora_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_out =  self.alpha * self.lora_b(self.lora_a(x))\n",
    "        lora_out = self.dropout(lora_out)\n",
    "        return lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.86011Z",
     "iopub.status.busy": "2024-07-27T22:50:56.859835Z",
     "iopub.status.idle": "2024-07-27T22:50:56.915738Z",
     "shell.execute_reply": "2024-07-27T22:50:56.914895Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.860088Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.phi3.modeling_phi3 import (\n",
    "Phi3RotaryEmbedding, \n",
    "# Phi3LongRoPEScaledRotaryEmbedding,\n",
    "apply_rotary_pos_emb,\n",
    "repeat_kv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.917012Z",
     "iopub.status.busy": "2024-07-27T22:50:56.916766Z",
     "iopub.status.idle": "2024-07-27T22:50:56.943771Z",
     "shell.execute_reply": "2024-07-27T22:50:56.942785Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.916991Z"
    }
   },
   "outputs": [],
   "source": [
    "class Phi3Attention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.original_max_position_embeddings = config.original_max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.rope_scaling = config.rope_scaling\n",
    "        self.is_causal = True\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)\n",
    "        self._init_rope()\n",
    "        \n",
    "        ########################## LoRA adapters ##########################\n",
    "        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)\n",
    "        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        \n",
    "    def _init_rope(self):\n",
    "        if self.rope_scaling is None:\n",
    "            self.rotary_emb = Phi3RotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            if scaling_type == \"longrope\":\n",
    "                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "#         logger.warning_once(\"You are not running the flash-attention implementation, expect numerical differences.\")\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        ########################## LoRA adapters ##########################\n",
    "        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        query_pos = self.num_heads * self.head_dim\n",
    "        query_states = qkv[..., :query_pos]\n",
    "        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n",
    "        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            if self.layer_idx is None:\n",
    "                raise ValueError(\n",
    "                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n",
    "                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n",
    "                    \"with a layer index.\"\n",
    "                )\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights += causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.945391Z",
     "iopub.status.busy": "2024-07-27T22:50:56.945023Z",
     "iopub.status.idle": "2024-07-27T22:50:56.953323Z",
     "shell.execute_reply": "2024-07-27T22:50:56.952431Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.945338Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_attention_module(config,layer,layer_idx):\n",
    "    if hasattr(layer, 'self_attn') and layer_idx > 12:\n",
    "\n",
    "        new_attention = Phi3Attention(config,layer_idx)\n",
    "\n",
    "        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)\n",
    "        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)\n",
    "\n",
    "        layer.self_attn = new_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.955186Z",
     "iopub.status.busy": "2024-07-27T22:50:56.954567Z",
     "iopub.status.idle": "2024-07-27T22:50:56.962953Z",
     "shell.execute_reply": "2024-07-27T22:50:56.962104Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.955157Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class LoraModelForClassification(nn.Module):\n",
    "    def __init__(self, lora_model):  # config 추가\n",
    "        super(LoraModelForClassification, self).__init__()\n",
    "        self.config = lora_model.config  # config 설정\n",
    "        self.peft_model = lora_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 3)\n",
    "#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)\n",
    "#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.peft_model(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim =1)\n",
    "        output_dropout = self.dropout(pooled_output)\n",
    "        logits = self.classifier(output_dropout)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          labels = labels\n",
    "          loss = loss_fn(logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a43a6",
   "metadata": {},
   "source": [
    "## Parallel Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.964449Z",
     "iopub.status.busy": "2024-07-27T22:50:56.964151Z",
     "iopub.status.idle": "2024-07-27T22:50:56.984294Z",
     "shell.execute_reply": "2024-07-27T22:50:56.983454Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.964426Z"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_function(model_name,attention_name,file_path):\n",
    "    mp.set_start_method('spawn', force=True)\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "    if accelerator.is_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    set_seed(seed)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "    \n",
    "    model = AutoModel.from_pretrained(model_name,torch_dtype=torch.float16)\n",
    "    model = quantize_model(model)\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        replace_attention_module(model.config,layer,idx)\n",
    "    model = LoraModelForClassification(model)\n",
    "\n",
    "    for param in model.peft_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, attention_name):\n",
    "            module.qkv_lora.lora_a.weight.requires_grad = True\n",
    "            module.qkv_lora.lora_b.weight.requires_grad = True\n",
    "            module.o_lora.lora_a.weight.requires_grad = True\n",
    "            module.o_lora.lora_b.weight.requires_grad = True\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)\n",
    "    print(f\"Total trainable parameters: {total_params}\")\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    \n",
    "    t_dat, v_dat = preprocess_data(file_path,sample = True)\n",
    "    train_dataloader = create_dataloaders(t_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)\n",
    "    eval_dataloader = create_dataloaders(v_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)\n",
    "\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_dataloader) * n_epoch,\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(n_epoch * len(train_dataloader)), disable=not accelerator.is_main_process)\n",
    "\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            loss, _ = model(**batch)\n",
    "            accelerator.backward(loss)\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        model.eval()\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(**batch)\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            all_predictions.append(accelerator.gather(predictions))\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))\n",
    "            \n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()\n",
    "        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()\n",
    "\n",
    "        acc_metric = accuracy_score(all_labels, all_predictions)\n",
    "        eval_metric = f1_score(all_labels, all_predictions, average=\"macro\")\n",
    "        train_loss_avg = train_loss / len(train_dataloader)\n",
    "        valid_loss_avg = valid_loss / len(eval_dataloader)\n",
    "        \n",
    "        accelerator.print(f\"epoch: {epoch} \\n accuracy: {acc_metric:.3f} \\n f1 score: {eval_metric:.3f} \\n train loss: {train_loss_avg:.3f} \\n valid loss: {valid_loss_avg:.3f}\")\n",
    "\n",
    "    model = accelerator.unwrap_model(model)\n",
    "    accelerator.wait_for_everyone()\n",
    "\n",
    "    # 모델 상태 사전 저장\n",
    "    if accelerator.is_main_process:\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "    # 동기화 완료 메시지\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.985884Z",
     "iopub.status.busy": "2024-07-27T22:50:56.985531Z",
     "iopub.status.idle": "2024-07-27T22:55:49.631987Z",
     "shell.execute_reply": "2024-07-27T22:55:49.63091Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.985855Z"
    }
   },
   "outputs": [],
   "source": [
    "notebook_launcher(parallel_function, args=(model_name,Phi3Attention,file_path,), num_processes=2)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 95068,
     "modelInstanceId": 69944,
     "sourceId": 83272,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
