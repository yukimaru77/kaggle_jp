{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-30T10:56:16.328454Z",
     "iopub.status.busy": "2024-06-30T10:56:16.32813Z",
     "iopub.status.idle": "2024-06-30T10:56:16.733353Z",
     "shell.execute_reply": "2024-06-30T10:56:16.732434Z",
     "shell.execute_reply.started": "2024-06-30T10:56:16.32843Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:56:16.891816Z",
     "iopub.status.busy": "2024-06-30T10:56:16.891352Z",
     "iopub.status.idle": "2024-06-30T10:56:31.055106Z",
     "shell.execute_reply": "2024-06-30T10:56:31.053847Z",
     "shell.execute_reply.started": "2024-06-30T10:56:16.891786Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install cupy-cuda11x>=12.0.0\n",
    "# !pip install packaging>=22\n",
    "# !pip install shapely>=2.0.1\n",
    "# !pip install numpy<1.26\n",
    "# !pip install scipy<1.12\n",
    "!pip install transformers datasets accelerate peft\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:56:31.057594Z",
     "iopub.status.busy": "2024-06-30T10:56:31.057299Z",
     "iopub.status.idle": "2024-06-30T10:57:00.598862Z",
     "shell.execute_reply": "2024-06-30T10:57:00.597749Z",
     "shell.execute_reply.started": "2024-06-30T10:56:31.057568Z"
    }
   },
   "outputs": [],
   "source": [
    "# You only need to run this once per machine\n",
    "!pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:57:00.600629Z",
     "iopub.status.busy": "2024-06-30T10:57:00.600293Z",
     "iopub.status.idle": "2024-06-30T10:57:02.118465Z",
     "shell.execute_reply": "2024-06-30T10:57:02.117732Z",
     "shell.execute_reply.started": "2024-06-30T10:57:00.600598Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('csv', data_files='/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:57:02.120522Z",
     "iopub.status.busy": "2024-06-30T10:57:02.120048Z",
     "iopub.status.idle": "2024-06-30T10:57:02.143838Z",
     "shell.execute_reply": "2024-06-30T10:57:02.143188Z",
     "shell.execute_reply.started": "2024-06-30T10:57:02.120496Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:57:25.093855Z",
     "iopub.status.busy": "2024-06-30T10:57:25.092858Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('../input/lmsys-chatbot-arena/train.csv')\n",
    "test_data = pd.read_csv('../input/lmsys-chatbot-arena/test.csv')\n",
    "submission_data = pd.read_csv('../input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:05:32.797047Z",
     "iopub.status.busy": "2024-06-30T10:05:32.79666Z",
     "iopub.status.idle": "2024-06-30T10:05:32.804768Z",
     "shell.execute_reply": "2024-06-30T10:05:32.803647Z",
     "shell.execute_reply.started": "2024-06-30T10:05:32.797016Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data.shape,test_data.shape,submission_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f6ac5f",
   "metadata": {},
   "source": [
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "def trim_endings(custom_string):\n",
    "    return custom_string[:-2][2:]\n",
    "\n",
    "def count_newlines(custom_string):\n",
    "    return custom_string.count('\\\\n')\n",
    "\n",
    "def word_counts(custom_string):\n",
    "    return len(custom_string.split())\n",
    "\n",
    "\n",
    "def apply_transformations(df):\n",
    "    \n",
    "    df['prompt'] =  df['prompt'].map(process)\n",
    "    df['response_a'] =  df['response_a'].map(process)\n",
    "    df['response_b'] =  df['response_b'].map(process)\n",
    "    \n",
    "    df['prompt'] =  df['prompt'].map(trim_endings)\n",
    "    df['response_a'] =  df['response_a'].map(trim_endings)\n",
    "    df['response_b'] =  df['response_b'].map(trim_endings)\n",
    "    \n",
    "    df['res_a_line_count'] = df['response_a'].map(count_newlines).astype(str)\n",
    "    df['res_b_line_count'] = df['response_b'].map(count_newlines).astype(str)\n",
    "    \n",
    "    df['res_a_word_count'] = df['response_a'].map(word_counts).astype(str)\n",
    "    df['res_b_word_count'] = df['response_b'].map(word_counts).astype(str)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def create_summary(df):\n",
    "    df['summary'] = 'User prompt: ' + df['prompt'] +  '\\n\\n Model A :\\n' + df['response_a'] +'\\n\\n Model A length:\\n' + df['res_a_word_count'] +'\\n\\n Model A Line Count:\\n' + df['res_a_line_count'] +'\\n\\nModel B:\\n'  + df['response_b'] +'\\n\\n \\n\\nModel B length:\\n'  + df['res_b_word_count'] +'\\n\\n \\n\\nModel B Line Count:\\n'  + df['res_b_line_count']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:05:40.281316Z",
     "iopub.status.busy": "2024-06-30T10:05:40.2804Z",
     "iopub.status.idle": "2024-06-30T10:05:43.420432Z",
     "shell.execute_reply": "2024-06-30T10:05:43.419516Z",
     "shell.execute_reply.started": "2024-06-30T10:05:40.28127Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = apply_transformations(train_data)\n",
    "train_data = create_summary(train_data)\n",
    "test_data = apply_transformations(test_data)    \n",
    "test_data = create_summary(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:05:44.365731Z",
     "iopub.status.busy": "2024-06-30T10:05:44.364877Z",
     "iopub.status.idle": "2024-06-30T10:05:44.371369Z",
     "shell.execute_reply": "2024-06-30T10:05:44.370456Z",
     "shell.execute_reply.started": "2024-06-30T10:05:44.365694Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['summary'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:04.487272Z",
     "iopub.status.busy": "2024-06-30T10:24:04.486415Z",
     "iopub.status.idle": "2024-06-30T10:24:04.513266Z",
     "shell.execute_reply": "2024-06-30T10:24:04.512224Z",
     "shell.execute_reply.started": "2024-06-30T10:24:04.487236Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data['labels'] = train_data[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1)\n",
    "train_data['labels']=train_data['label'].astype('category')\n",
    "train_data['target']=train_data['label'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:06.693799Z",
     "iopub.status.busy": "2024-06-30T10:24:06.692976Z",
     "iopub.status.idle": "2024-06-30T10:24:06.70254Z",
     "shell.execute_reply": "2024-06-30T10:24:06.701465Z",
     "shell.execute_reply.started": "2024-06-30T10:24:06.693764Z"
    }
   },
   "outputs": [],
   "source": [
    "category_map = {code: category for code, category in enumerate(train_data['labels'].cat.categories)}\n",
    "category_map"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:22:08.804678Z",
     "iopub.status.busy": "2024-06-30T10:22:08.803825Z",
     "iopub.status.idle": "2024-06-30T10:22:08.81028Z",
     "shell.execute_reply": "2024-06-30T10:22:08.808883Z",
     "shell.execute_reply.started": "2024-06-30T10:22:08.804647Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 7920"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:10.554189Z",
     "iopub.status.busy": "2024-06-30T10:24:10.553265Z",
     "iopub.status.idle": "2024-06-30T10:24:10.604219Z",
     "shell.execute_reply": "2024-06-30T10:24:10.603135Z",
     "shell.execute_reply.started": "2024-06-30T10:24:10.554152Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
    "train_df = train_data[['id','summary','target']]\n",
    "test_df = train_data[['id','summary']]\n",
    "\n",
    "df_train,df_val = train_test_split(train_df,test_size=0.2,random_state=SEED,stratify=train_df['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:12.178932Z",
     "iopub.status.busy": "2024-06-30T10:24:12.178234Z",
     "iopub.status.idle": "2024-06-30T10:24:13.791462Z",
     "shell.execute_reply": "2024-06-30T10:24:13.790313Z",
     "shell.execute_reply.started": "2024-06-30T10:24:12.178896Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "dataset_train = Dataset.from_pandas(df_train,preserve_index=False)\n",
    "dataset_val = Dataset.from_pandas(df_val,preserve_index=False)\n",
    "dataset_test = Dataset.from_pandas(test_df,preserve_index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:14.245859Z",
     "iopub.status.busy": "2024-06-30T10:24:14.24494Z",
     "iopub.status.idle": "2024-06-30T10:24:14.276496Z",
     "shell.execute_reply": "2024-06-30T10:24:14.275549Z",
     "shell.execute_reply.started": "2024-06-30T10:24:14.245823Z"
    }
   },
   "outputs": [],
   "source": [
    " dataset_train_shuffled = dataset_train.shuffle(seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:14.74363Z",
     "iopub.status.busy": "2024-06-30T10:24:14.7427Z",
     "iopub.status.idle": "2024-06-30T10:24:14.752724Z",
     "shell.execute_reply": "2024-06-30T10:24:14.751038Z",
     "shell.execute_reply.started": "2024-06-30T10:24:14.743596Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': dataset_train_shuffled,\n",
    "#     'val': dataset_val,\n",
    "#     'test': dataset_test\n",
    "})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:16.289475Z",
     "iopub.status.busy": "2024-06-30T10:24:16.288626Z",
     "iopub.status.idle": "2024-06-30T10:24:16.300414Z",
     "shell.execute_reply": "2024-06-30T10:24:16.299274Z",
     "shell.execute_reply.started": "2024-06-30T10:24:16.289439Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train.target.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:18.146546Z",
     "iopub.status.busy": "2024-06-30T10:24:18.145661Z",
     "iopub.status.idle": "2024-06-30T10:24:18.157909Z",
     "shell.execute_reply": "2024-06-30T10:24:18.156933Z",
     "shell.execute_reply.started": "2024-06-30T10:24:18.146512Z"
    }
   },
   "outputs": [],
   "source": [
    "class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()\n",
    "class_weights=torch.tensor(class_weights)\n",
    "class_weights=class_weights/class_weights.sum()\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:16:12.398089Z",
     "iopub.status.busy": "2024-06-30T10:16:12.397697Z",
     "iopub.status.idle": "2024-06-30T10:16:12.582641Z",
     "shell.execute_reply": "2024-06-30T10:16:12.581589Z",
     "shell.execute_reply.started": "2024-06-30T10:16:12.39806Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:24:22.647924Z",
     "iopub.status.busy": "2024-06-30T10:24:22.647127Z",
     "iopub.status.idle": "2024-06-30T10:25:06.432728Z",
     "shell.execute_reply": "2024-06-30T10:25:06.431801Z",
     "shell.execute_reply.started": "2024-06-30T10:24:22.647888Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "col_to_delete = ['summary']\n",
    "\n",
    "def llama_preprocessing_function(examples):\n",
    "    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)\n",
    "\n",
    "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:12.033837Z",
     "iopub.status.busy": "2024-06-30T10:18:12.033061Z",
     "iopub.status.idle": "2024-06-30T10:18:12.092712Z",
     "shell.execute_reply": "2024-06-30T10:18:12.091596Z",
     "shell.execute_reply.started": "2024-06-30T10:18:12.033806Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:12.740827Z",
     "iopub.status.busy": "2024-06-30T10:18:12.740447Z",
     "iopub.status.idle": "2024-06-30T10:18:12.750671Z",
     "shell.execute_reply": "2024-06-30T10:18:12.749196Z",
     "shell.execute_reply.started": "2024-06-30T10:18:12.740796Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:17.312999Z",
     "iopub.status.busy": "2024-06-30T10:18:17.312075Z",
     "iopub.status.idle": "2024-06-30T10:18:17.336991Z",
     "shell.execute_reply": "2024-06-30T10:18:17.336114Z",
     "shell.execute_reply.started": "2024-06-30T10:18:17.312964Z"
    }
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:21.348412Z",
     "iopub.status.busy": "2024-06-30T10:18:21.3477Z",
     "iopub.status.idle": "2024-06-30T10:18:21.359511Z",
     "shell.execute_reply": "2024-06-30T10:18:21.358311Z",
     "shell.execute_reply.started": "2024-06-30T10:18:21.348375Z"
    }
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:21.751883Z",
     "iopub.status.busy": "2024-06-30T10:18:21.751248Z",
     "iopub.status.idle": "2024-06-30T10:18:22.37163Z",
     "shell.execute_reply": "2024-06-30T10:18:22.37053Z",
     "shell.execute_reply.started": "2024-06-30T10:18:21.751849Z"
    }
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:18:59.252077Z",
     "iopub.status.busy": "2024-06-30T10:18:59.251685Z",
     "iopub.status.idle": "2024-06-30T10:19:09.470832Z",
     "shell.execute_reply": "2024-06-30T10:19:09.469719Z",
     "shell.execute_reply.started": "2024-06-30T10:18:59.252046Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q wandb -U\n",
    "\n",
    "import wandb, os\n",
    "wandb.login()\n",
    "\n",
    "wandb_project = \"mistral-finetune\"\n",
    "if len(wandb_project) > 0:\n",
    "    os.environ[\"WANDB_PROJECT\"] = wandb_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:19:12.502929Z",
     "iopub.status.busy": "2024-06-30T10:19:12.502066Z",
     "iopub.status.idle": "2024-06-30T10:19:12.512449Z",
     "shell.execute_reply": "2024-06-30T10:19:12.511432Z",
     "shell.execute_reply.started": "2024-06-30T10:19:12.502887Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:28:25.804874Z",
     "iopub.status.busy": "2024-06-30T10:28:25.803904Z",
     "iopub.status.idle": "2024-06-30T10:28:50.744939Z",
     "shell.execute_reply": "2024-06-30T10:28:50.743611Z",
     "shell.execute_reply.started": "2024-06-30T10:28:25.804832Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvcc --version\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "!pip install torch==2.0.0+cu121 torchvision==0.15.0+cu121 torchaudio==2.0.0+cu121 -f https://download.pytorch.org/whl/nightly/cu121/torch_nightly.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:29:18.230437Z",
     "iopub.status.busy": "2024-06-30T10:29:18.230016Z",
     "iopub.status.idle": "2024-06-30T10:31:31.423457Z",
     "shell.execute_reply": "2024-06-30T10:31:31.422192Z",
     "shell.execute_reply.started": "2024-06-30T10:29:18.230402Z"
    }
   },
   "outputs": [],
   "source": [
    "# Uninstall existing versions of torch, torchvision, and torchaudio\n",
    "!pip uninstall torch torchvision torchaudio -y\n",
    "\n",
    "# Install compatible versions with CUDA 11.2 support\n",
    "!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu112/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:37:10.710967Z",
     "iopub.status.busy": "2024-06-30T10:37:10.710194Z",
     "iopub.status.idle": "2024-06-30T10:37:10.719709Z",
     "shell.execute_reply": "2024-06-30T10:37:10.718053Z",
     "shell.execute_reply.started": "2024-06-30T10:37:10.71093Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '0'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:56:06.912496Z",
     "iopub.status.busy": "2024-06-30T10:56:06.911774Z",
     "iopub.status.idle": "2024-06-30T10:56:06.917517Z",
     "shell.execute_reply": "2024-06-30T10:56:06.916595Z",
     "shell.execute_reply.started": "2024-06-30T10:56:06.912463Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "seed = 7920  # Replace with your desired seed\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = torch.Generator(device='cuda').manual_seed(seed)\n",
    "else:\n",
    "    generator = torch.Generator().manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:37:12.02127Z",
     "iopub.status.busy": "2024-06-30T10:37:12.020896Z",
     "iopub.status.idle": "2024-06-30T10:37:12.074962Z",
     "shell.execute_reply": "2024-06-30T10:37:12.073402Z",
     "shell.execute_reply.started": "2024-06-30T10:37:12.021241Z"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "  generator = torch.Generator('cuda').manual_seed(seed)\n",
    "else:\n",
    "  generator = torch.Generator().manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:38:52.877824Z",
     "iopub.status.busy": "2024-06-30T10:38:52.877466Z",
     "iopub.status.idle": "2024-06-30T10:38:52.888061Z",
     "shell.execute_reply": "2024-06-30T10:38:52.886773Z",
     "shell.execute_reply.started": "2024-06-30T10:38:52.877799Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "class CustomTrainer(transformers.Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Ensure label_weights is a tensor\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
    "        else:\n",
    "            self.class_weights = None\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Extract labels and convert them to long type for cross_entropy\n",
    "        labels = inputs.pop(\"labels\").long()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "        # Extract logits assuming they are directly outputted by the model\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Compute custom loss with class weights for imbalanced data handling\n",
    "        if self.class_weights is not None:\n",
    "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
    "        else:\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:40:42.940778Z",
     "iopub.status.busy": "2024-06-30T10:40:42.939862Z",
     "iopub.status.idle": "2024-06-30T10:40:42.947877Z",
     "shell.execute_reply": "2024-06-30T10:40:42.946591Z",
     "shell.execute_reply.started": "2024-06-30T10:40:42.940728Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:40:47.846323Z",
     "iopub.status.busy": "2024-06-30T10:40:47.845447Z",
     "iopub.status.idle": "2024-06-30T10:40:47.88476Z",
     "shell.execute_reply": "2024-06-30T10:40:47.883933Z",
     "shell.execute_reply.started": "2024-06-30T10:40:47.846288Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = transformers.TrainingArguments(\n",
    "    output_dir = 'model_classification',\n",
    "    learning_rate = 1e-4,\n",
    "    per_device_train_batch_size = 8,\n",
    "    per_device_eval_batch_size = 8,\n",
    "    num_train_epochs = 1,\n",
    "    weight_decay = 0.01,\n",
    "    evaluation_strategy = 'epoch',\n",
    "    save_strategy = 'epoch',\n",
    "    load_best_model_at_end = True,\n",
    "\tlogging_steps=10,\n",
    "\tmax_steps=100,\n",
    "    fp16=True,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:40:48.844753Z",
     "iopub.status.busy": "2024-06-30T10:40:48.844362Z",
     "iopub.status.idle": "2024-06-30T10:40:48.852048Z",
     "shell.execute_reply": "2024-06-30T10:40:48.850644Z",
     "shell.execute_reply.started": "2024-06-30T10:40:48.84471Z"
    }
   },
   "outputs": [],
   "source": [
    "collate_fn = transformers.DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:42:16.536193Z",
     "iopub.status.busy": "2024-06-30T10:42:16.535812Z",
     "iopub.status.idle": "2024-06-30T10:42:16.72858Z",
     "shell.execute_reply": "2024-06-30T10:42:16.727087Z",
     "shell.execute_reply.started": "2024-06-30T10:42:16.536164Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = CustomTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = tokenized_datasets['train'],\n",
    "    tokenizer = tokenizer,\n",
    "    data_collator = collate_fn,\n",
    "    compute_metrics = compute_metrics,\n",
    "    class_weights=class_weights,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:42:34.528039Z",
     "iopub.status.busy": "2024-06-30T10:42:34.527638Z",
     "iopub.status.idle": "2024-06-30T10:42:47.820535Z",
     "shell.execute_reply": "2024-06-30T10:42:47.81924Z",
     "shell.execute_reply.started": "2024-06-30T10:42:34.528008Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T10:42:47.823897Z",
     "iopub.status.busy": "2024-06-30T10:42:47.823352Z",
     "iopub.status.idle": "2024-06-30T10:42:48.063002Z",
     "shell.execute_reply": "2024-06-30T10:42:48.060733Z",
     "shell.execute_reply.started": "2024-06-30T10:42:47.823843Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Check if CUDA is available\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "project = \"mistral-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=50,\n",
    "        fp16=False,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "        report_to=\"wandb\",           # Comment this out if you don't want to use weights & baises\n",
    "        run_name=f\"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}\"          # Name of the W&B run (optional)\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-viggo-finetune/checkpoint-1000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_prompt = \"\"\"Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.\n",
    "This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].\n",
    "The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']\n",
    "\n",
    "### Target sentence:\n",
    "Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?\n",
    "\n",
    "### Meaning representation:\n",
    "\"\"\"\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
