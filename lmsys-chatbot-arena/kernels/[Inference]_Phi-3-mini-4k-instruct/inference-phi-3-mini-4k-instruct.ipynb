{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad8d8d27",
   "metadata": {},
   "source": [
    "# No Installation RequiredÂ¶\n",
    "microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training\n",
    "\n",
    "The max sequence length has a significant impact on model performance, but due to insufficient memory, it was set to a maximum length of 768.\n",
    "\n",
    "[training code](https://www.kaggle.com/code/argozero01/parallel-train-phi-3-mini-4k-instruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:42:52.52789Z",
     "iopub.status.busy": "2024-07-28T12:42:52.527573Z",
     "iopub.status.idle": "2024-07-28T12:43:01.446177Z",
     "shell.execute_reply": "2024-07-28T12:43:01.445408Z",
     "shell.execute_reply.started": "2024-07-28T12:42:52.527862Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss\n",
    "\n",
    "from accelerate import notebook_launcher, Accelerator, PartialState\n",
    "from accelerate.utils import write_basic_config\n",
    "from accelerate.inference import prepare_pippy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.452034Z",
     "iopub.status.busy": "2024-07-28T12:43:01.451479Z",
     "iopub.status.idle": "2024-07-28T12:43:01.456896Z",
     "shell.execute_reply": "2024-07-28T12:43:01.456053Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.452001Z"
    }
   },
   "outputs": [],
   "source": [
    "# params\n",
    "model_name = \"/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1\"\n",
    "model_path = \"/kaggle/input/checkpoint-phi3/model_checkpoint.pth\"\n",
    "seed = 42\n",
    "lora_r = 2\n",
    "quantize_bit = 16\n",
    "test_batch_size = 1\n",
    "test_max_len = 256\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.458241Z",
     "iopub.status.busy": "2024-07-28T12:43:01.457984Z",
     "iopub.status.idle": "2024-07-28T12:43:01.47395Z",
     "shell.execute_reply": "2024-07-28T12:43:01.473229Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.458218Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = df['prompt']\n",
    "        self.response_a = df['response_a']\n",
    "        self.response_b = df['response_b']\n",
    "        self.max_len = max_len\n",
    "        self.targets = df.get('labels', None)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        prompt = str(self.prompt[index])\n",
    "        response_a = str(self.response_a[index])\n",
    "        response_b = str(self.response_b[index])\n",
    "\n",
    "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])\n",
    "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])\n",
    "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])\n",
    "\n",
    "        final_prompt_len = min(self.max_len, prompt_len)\n",
    "        final_a_len = min(self.max_len, response_a_len)\n",
    "        final_b_len = min(self.max_len, response_b_len)\n",
    "\n",
    "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
    "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            labels = torch.LongTensor([self.targets[index]])\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}\n",
    "        else:\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.476599Z",
     "iopub.status.busy": "2024-07-28T12:43:01.476247Z",
     "iopub.status.idle": "2024-07-28T12:43:01.48831Z",
     "shell.execute_reply": "2024-07-28T12:43:01.487415Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.476576Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer):\n",
    "\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None\n",
    "\n",
    "    # Find the maximum length of the sequences in the batch\n",
    "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
    "\n",
    "    # Re-tokenize with the new max length\n",
    "    new_input_ids = []\n",
    "    new_attention_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids'][:max_len]\n",
    "        attention_mask = item['attention_mask'][:max_len]\n",
    "\n",
    "        new_input_ids.append(input_ids)\n",
    "        new_attention_masks.append(attention_mask)\n",
    "\n",
    "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    output = {\n",
    "    'input_ids': new_input_ids,\n",
    "    'attention_mask': new_attention_masks}\n",
    "\n",
    "    if labels is not None:\n",
    "        output['labels'] = labels\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.489657Z",
     "iopub.status.busy": "2024-07-28T12:43:01.489398Z",
     "iopub.status.idle": "2024-07-28T12:43:01.499196Z",
     "shell.execute_reply": "2024-07-28T12:43:01.498418Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.489635Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):\n",
    "    dataloader = DataLoader(\n",
    "        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.500877Z",
     "iopub.status.busy": "2024-07-28T12:43:01.500164Z",
     "iopub.status.idle": "2024-07-28T12:43:01.510489Z",
     "shell.execute_reply": "2024-07-28T12:43:01.509774Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.500848Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, num_bits=quantize_bit):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()\n",
    "    scale = (max_val - min_val) / (qmax - qmin)\n",
    "    zero_point = qmin - min_val / scale\n",
    "\n",
    "    quantized_tensor = torch.round(tensor / scale + zero_point)\n",
    "    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)\n",
    "    quantized_tensor = (quantized_tensor - zero_point) * scale\n",
    "\n",
    "    return quantized_tensor\n",
    "\n",
    "def quantize_model(model, num_bits=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# import torch.quantization\n",
    "\n",
    "# def quantize_model_dynamic(model):\n",
    "#     model.qconfig = torch.quantization.default_dynamic_qconfig\n",
    "#     torch.quantization.prepare(model, inplace=True)\n",
    "#     torch.quantization.convert(model, inplace=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.512227Z",
     "iopub.status.busy": "2024-07-28T12:43:01.511688Z",
     "iopub.status.idle": "2024-07-28T12:43:01.524661Z",
     "shell.execute_reply": "2024-07-28T12:43:01.523844Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.512189Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.rank = rank\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)\n",
    "        self.dropout = nn.Dropout(lora_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_out =  self.alpha * self.lora_b(self.lora_a(x))\n",
    "        lora_out = self.dropout(lora_out)\n",
    "        return lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.525997Z",
     "iopub.status.busy": "2024-07-28T12:43:01.525684Z",
     "iopub.status.idle": "2024-07-28T12:43:01.594784Z",
     "shell.execute_reply": "2024-07-28T12:43:01.594077Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.525967Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.phi3.modeling_phi3 import (\n",
    "Phi3RotaryEmbedding, \n",
    "# Phi3LongRoPEScaledRotaryEmbedding,\n",
    "apply_rotary_pos_emb,\n",
    "repeat_kv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.596455Z",
     "iopub.status.busy": "2024-07-28T12:43:01.596066Z",
     "iopub.status.idle": "2024-07-28T12:43:01.623393Z",
     "shell.execute_reply": "2024-07-28T12:43:01.622631Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.596423Z"
    }
   },
   "outputs": [],
   "source": [
    "class Phi3Attention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n",
    "                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n",
    "                \"when creating this class.\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.original_max_position_embeddings = config.original_max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.rope_scaling = config.rope_scaling\n",
    "        self.is_causal = True\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "\n",
    "        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)\n",
    "        self._init_rope()\n",
    "        \n",
    "        ########################## LoRA adapters ##########################\n",
    "        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)\n",
    "        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        \n",
    "    def _init_rope(self):\n",
    "        if self.rope_scaling is None:\n",
    "            self.rotary_emb = Phi3RotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            if scaling_type == \"longrope\":\n",
    "                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "#         logger.warning_once(\"You are not running the flash-attention implementation, expect numerical differences.\")\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        ########################## LoRA adapters ##########################\n",
    "        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        query_pos = self.num_heads * self.head_dim\n",
    "        query_states = qkv[..., :query_pos]\n",
    "        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n",
    "        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n",
    "\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            if self.layer_idx is None:\n",
    "                raise ValueError(\n",
    "                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n",
    "                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n",
    "                    \"with a layer index.\"\n",
    "                )\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights += causal_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)\n",
    "        ########################## LoRA adapters ##########################\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.625081Z",
     "iopub.status.busy": "2024-07-28T12:43:01.624642Z",
     "iopub.status.idle": "2024-07-28T12:43:01.638084Z",
     "shell.execute_reply": "2024-07-28T12:43:01.637254Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.625025Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_attention_module(config,layer,layer_idx):\n",
    "    if hasattr(layer, 'self_attn') and layer_idx > 12:\n",
    "\n",
    "        new_attention = Phi3Attention(config,layer_idx)\n",
    "\n",
    "        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)\n",
    "        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)\n",
    "\n",
    "        layer.self_attn = new_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.640016Z",
     "iopub.status.busy": "2024-07-28T12:43:01.639281Z",
     "iopub.status.idle": "2024-07-28T12:43:01.648891Z",
     "shell.execute_reply": "2024-07-28T12:43:01.648021Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.639992Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "class LoraModelForClassification(nn.Module):\n",
    "    def __init__(self, lora_model):  # config ì¶ê°\n",
    "        super(LoraModelForClassification, self).__init__()\n",
    "        self.config = lora_model.config  # config ì¤ì \n",
    "        self.peft_model = lora_model\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 3)\n",
    "#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)\n",
    "#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.peft_model(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim =1)\n",
    "        output_dropout = self.dropout(pooled_output)\n",
    "        logits = self.classifier(output_dropout)\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "          labels = labels\n",
    "          loss = loss_fn(logits, labels)\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.650213Z",
     "iopub.status.busy": "2024-07-28T12:43:01.649876Z",
     "iopub.status.idle": "2024-07-28T12:43:01.672271Z",
     "shell.execute_reply": "2024-07-28T12:43:01.671442Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.650188Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.675399Z",
     "iopub.status.busy": "2024-07-28T12:43:01.675065Z",
     "iopub.status.idle": "2024-07-28T12:43:01.681955Z",
     "shell.execute_reply": "2024-07-28T12:43:01.680916Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.675365Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(lambda x: json.loads(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.683609Z",
     "iopub.status.busy": "2024-07-28T12:43:01.683057Z",
     "iopub.status.idle": "2024-07-28T12:43:01.692052Z",
     "shell.execute_reply": "2024-07-28T12:43:01.691221Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.683579Z"
    }
   },
   "outputs": [],
   "source": [
    "test_0 = test[:len(test)//2].reset_index(drop=True)\n",
    "test_1 = test[len(test)//2:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.693824Z",
     "iopub.status.busy": "2024-07-28T12:43:01.693506Z",
     "iopub.status.idle": "2024-07-28T12:43:01.70146Z",
     "shell.execute_reply": "2024-07-28T12:43:01.700419Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.693795Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def infer(model, dataloader, device):\n",
    "#     model = nn.DataParallel(model)  # Wrap the model with DataParallel\n",
    "#     model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    target_list = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                _,logits = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                softmax_logits = torch.nn.functional.softmax(logits, dim=1)\n",
    "                target_list.append(softmax_logits)\n",
    "\n",
    "    return target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.703165Z",
     "iopub.status.busy": "2024-07-28T12:43:01.702729Z",
     "iopub.status.idle": "2024-07-28T12:43:01.71508Z",
     "shell.execute_reply": "2024-07-28T12:43:01.714342Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.703135Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "gpu0 = \"cuda:0\"\n",
    "gpu1 = \"cuda:1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.716349Z",
     "iopub.status.busy": "2024-07-28T12:43:01.716089Z",
     "iopub.status.idle": "2024-07-28T12:43:14.445907Z",
     "shell.execute_reply": "2024-07-28T12:43:14.444988Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.716327Z"
    }
   },
   "outputs": [],
   "source": [
    "model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16\n",
    "                                  ,device_map=\"cpu\")\n",
    "model0 = quantize_model(model0)\n",
    "for idx, layer in enumerate(model0.layers):\n",
    "    replace_attention_module(model0.config,layer,idx)\n",
    "model0 = LoraModelForClassification(model0)\n",
    "\n",
    "model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16\n",
    "                                  ,device_map=\"cpu\")\n",
    "model1 = quantize_model(model1)\n",
    "for idx, layer in enumerate(model1.layers):\n",
    "    replace_attention_module(model1.config,layer,idx)\n",
    "model1 = LoraModelForClassification(model1)\n",
    "\n",
    "\n",
    "model0.load_state_dict(torch.load(model_path))\n",
    "model1.load_state_dict(torch.load(model_path))\n",
    "model0.to(gpu0)\n",
    "model1.to(gpu1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer0 = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer0.pad_token is None:\n",
    "    tokenizer0.pad_token = tokenizer0.eos_token\n",
    "tokenizer0.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "if tokenizer1.pad_token is None:\n",
    "    tokenizer1.pad_token = tokenizer1.eos_token\n",
    "tokenizer1.padding_side = \"right\"  # Fix weird overflow issue with fp16 training\n",
    "\n",
    "test_dataloader0 = create_dataloaders(test_0,tokenizer0,test_max_len,test_batch_size, shuffle = False)\n",
    "test_dataloader1 = create_dataloaders(test_1,tokenizer1,test_max_len,test_batch_size, shuffle = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataloader, device, results, index):\n",
    "    results[index] = infer(model, dataloader, device)\n",
    "\n",
    "results = {}\n",
    "\n",
    "process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results,0))\n",
    "process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results,1))\n",
    "\n",
    "# Start the processes\n",
    "process0.start()\n",
    "process1.start()\n",
    "\n",
    "# Wait for both processes to finish\n",
    "process0.join()\n",
    "process1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # ì´ëí  ì¥ì¹ ì í\n",
    "for k, v in results.items():\n",
    "    for i in range(len(v)):\n",
    "        results[k][i] = v[i].to(device)\n",
    "\n",
    "# ëìëë¦¬ì ê°ì íëë¡ í©ì¹ê¸°\n",
    "target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for tensor in target_list:\n",
    "    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "sub = sub.set_index(pd.Index(combined_df.index))\n",
    "\n",
    "final_df = pd.concat([sub[['id']], combined_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_and_folders(path):\n",
    "    # ê²½ë¡ê° ì¡´ì¬íëì§ íì¸\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Error: {path} does not exist.\")\n",
    "        return\n",
    "\n",
    "    # ê²½ë¡ ë´ì ëª¨ë  íì¼ ë° í´ëë¥¼ íì\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        # íì¼ ì­ì \n",
    "        for name in files:\n",
    "            if name == \"submission.csv\":\n",
    "                print(f\"Skipping file: {os.path.join(root, name)}\")\n",
    "                continue\n",
    "            file_path = os.path.join(root, name)\n",
    "            print(f\"Deleting file: {file_path}\")\n",
    "            os.remove(file_path)\n",
    "\n",
    "#         # í´ë ì­ì \n",
    "#         for name in dirs:\n",
    "#             folder_path = os.path.join(root, name)\n",
    "#             print(f\"Deleting folder: {folder_path}\")\n",
    "#             shutil.rmtree(folder_path)\n",
    "\n",
    "    print(f\"All files and folders in {path} have been deleted.\")\n",
    "\n",
    "# ìì  ê²½ë¡\n",
    "path_to_delete = \"/kaggle/working/\"\n",
    "\n",
    "# íì¼ ë° í´ë ì­ì  í¨ì í¸ì¶\n",
    "delete_files_and_folders(path_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU ë©ëª¨ë¦¬ ë¹ì°ê¸°\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# íìµ í GPU ë©ëª¨ë¦¬ ë¹ì°ê¸°\n",
    "clear_gpu_memory()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5456005,
     "sourceId": 9055296,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 95068,
     "modelInstanceId": 69944,
     "sourceId": 83272,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
