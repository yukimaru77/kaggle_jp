{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":69788,"sourceType":"modelInstanceVersion","modelInstanceId":58238,"modelId":76277}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n This notebook provides a comprehensive guide for loading, preprocessing, and training a text classification model using the Hugging Face Transformers library. We will use the Roberta tokenizer and a smaller Roberta model to manage computational resources efficiently. The process includes data loading, sampling, tokenization, and model preparation.\n\n# Step 1: Load and Sample Data\n First, we load the dataset from a CSV file and take a random sample to reduce the dataset size for quicker processing.\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\n# Load data\ndf = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n\n# Sample 10% of the data\ndf_sample = df.sample(frac=0.1, random_state=42)\n\n# Check sample data\nprint(df_sample.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:20:15.59644Z","iopub.execute_input":"2024-07-28T22:20:15.596812Z","iopub.status.idle":"2024-07-28T22:20:17.317719Z","shell.execute_reply.started":"2024-07-28T22:20:15.596784Z","shell.execute_reply":"2024-07-28T22:20:17.316693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 2: Split Data into Training and Validation Sets\nWe split the data into training and validation sets to evaluate the model's performance.","metadata":{}},{"cell_type":"code","source":"# Split data into training and validation sets\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    df_sample['prompt'].tolist(), \n    df_sample['winner_model_a'], \n    test_size=0.1, \n    random_state=42\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:20:17.319291Z","iopub.execute_input":"2024-07-28T22:20:17.319584Z","iopub.status.idle":"2024-07-28T22:20:17.329557Z","shell.execute_reply.started":"2024-07-28T22:20:17.31956Z","shell.execute_reply":"2024-07-28T22:20:17.328559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 3: Load and Prepare the Tokenizer\nWe use a smaller Roberta model tokenizer for efficiency and tokenize the text data.","metadata":{}},{"cell_type":"code","source":"from transformers import RobertaTokenizer\n\n# Load tokenizer\ntokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n\n# Tokenize function\ndef tokenize_function(texts):\n    return tokenizer(\n        texts,\n        padding=\"max_length\",\n        truncation=True,\n        max_length=128,\n        return_tensors='pt'\n    )\n\n# Tokenize texts\ntrain_encodings = tokenize_function(train_texts)\nval_encodings = tokenize_function(val_texts)\n\n# Check tokenized data\nprint(train_encodings)\nprint(val_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:20:17.330852Z","iopub.execute_input":"2024-07-28T22:20:17.331562Z","iopub.status.idle":"2024-07-28T22:20:24.153345Z","shell.execute_reply.started":"2024-07-28T22:20:17.331527Z","shell.execute_reply":"2024-07-28T22:20:24.152411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 4: Prepare the Model Configuration\n Set up the configuration for the model, ensuring it uses the appropriate device (GPU if available).","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\nfrom torch.utils.data import Dataset\nimport torch\nimport numpy as np\n\n# Config class\nclass Config:\n    def __init__(self):\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.batch_size = 16\n\ncfg = Config()\n\n# Load model\nmodel = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(cfg.device)\n\n# CustomDataset class\nclass CustomDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = np.array(labels, dtype=int)\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n        return item\n    \n    def __len__(self):\n        return len(self.labels)\n\n# Create datasets\ntrain_dataset = CustomDataset(train_encodings, train_labels)\nval_dataset = CustomDataset(val_encodings, val_labels)\n\n# TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=cfg.batch_size,\n    per_device_eval_batch_size=cfg.batch_size,\n    warmup_steps=500,\n    weight_decay=0.01,\n    logging_dir='./logs',\n    logging_steps=10,\n)\n\n# Initialize Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset\n)\n\n# Train model\ntrainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:20:24.15514Z","iopub.execute_input":"2024-07-28T22:20:24.155417Z","iopub.status.idle":"2024-07-28T22:24:09.126503Z","shell.execute_reply.started":"2024-07-28T22:20:24.155391Z","shell.execute_reply":"2024-07-28T22:24:09.125607Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 5: Inference Function\nDefine a function to perform inference on the tokenized data using the trained model.","metadata":{}},{"cell_type":"code","source":"def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):\n    model.eval()\n    results = []\n    with torch.no_grad():\n        for i in range(0, len(input_ids), batch_size):\n            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)\n            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)\n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n            results.extend(outputs.logits.cpu().numpy())\n    return np.array(results)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:24:09.128045Z","iopub.execute_input":"2024-07-28T22:24:09.128408Z","iopub.status.idle":"2024-07-28T22:24:09.13874Z","shell.execute_reply.started":"2024-07-28T22:24:09.128367Z","shell.execute_reply":"2024-07-28T22:24:09.137869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Step 6: Evaluate and Save Results\nTokenize the test data, perform inference, and save the results to a CSV file.","metadata":{}},{"cell_type":"code","source":"# Tokenize the test data\ntest_texts = df_sample['prompt'].tolist()\ntest_encodings = tokenize_function(test_texts)\n\n# Perform inference\nresults = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])\n\n# Convert results to DataFrame and save as CSV\nresults_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])\nsubmission = pd.concat([df_sample[['id']], results_df], axis=1)\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T22:24:09.139927Z","iopub.execute_input":"2024-07-28T22:24:09.140244Z","iopub.status.idle":"2024-07-28T22:24:52.526467Z","shell.execute_reply.started":"2024-07-28T22:24:09.140214Z","shell.execute_reply":"2024-07-28T22:24:52.52548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion\nThis notebook provided a structured approach to loading, preprocessing, and training a text classification model using the Transformers library. The steps included data sampling, tokenization, model configuration, and inference. The final results were saved for further analysis. This methodology ensures efficient use of computational resources while maintaining model performance.","metadata":{}}]}