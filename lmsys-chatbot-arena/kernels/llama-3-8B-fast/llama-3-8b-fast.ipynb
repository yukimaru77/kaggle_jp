{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":206.513308,"end_time":"2024-07-01T03:00:01.146998","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-07-01T02:56:34.63369","version":"2.5.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"03b341b06afc40599e50c9c1ce88be20":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"07273d2112d649ffbea2991a6a79df98":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"142be9e5949c44fabd0370c6df1203d6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15d31276fcd44350a50d1c561c13e3a4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c","placeholder":"â€‹","style":"IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e","value":"Loading checkpoint shards: 100%"}},"1df41ee456cd4fa78a23f7ea2fded110":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"28cb0aaaf9d24d3d857d224850f62f5b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd","value":4}},"3974a6c7f61d4f4582a8e4fdf4c9976c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3daebd55184a4a9982813dc1ac948f2e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"411187a29c544ebbb425a06b0dfae7a4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"475dd481f05a46c1908b9f781ae1afa8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075","IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b","IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"],"layout":"IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"}},"74d46aef6d8949c584024a6e7bb4f06c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9c5128ae1d114334b30f2e1013062b26","placeholder":"â€‹","style":"IPY_MODEL_03b341b06afc40599e50c9c1ce88be20","value":" 4/4 [01:30&lt;00:00, 18.30s/it]"}},"846776c5dc1f44bc9cf2e3d394e8ba48":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e8e3620620b445eb1d0286befa13278":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4","IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe","IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"],"layout":"IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"}},"9c5128ae1d114334b30f2e1013062b26":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7c6588ad13549ae958237ca8e3af9db":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c23e125e5c3e4cee844bd057453c7aca":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b","placeholder":"â€‹","style":"IPY_MODEL_07273d2112d649ffbea2991a6a79df98","value":" 4/4 [00:13&lt;00:00,  2.76s/it]"}},"cb2874dbe9904c07a254eb33d6f0ecfe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d0043cb27ac54061b85f9b3886954314","value":4}},"d0043cb27ac54061b85f9b3886954314":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dd4aa3639e4a46e7a3861fa3dbd5a31b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7361623d5e1491c888080ee4fb8bfdd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f0cc9ad10c3d4a63bd03716995531022":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f43e003b1f2e4367800ba2bad74c7075":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db","placeholder":"â€‹","style":"IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022","value":"Loading checkpoint shards: 100%"}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is\nThis notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.\nI made a few improvements upon @kishanvavdara's work:\n\n### 38% faster inference\nInference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:\n\n#### 1. Dynamic padding\nInstead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.\n\n#### 2. Sort the test data by input length\nTo take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.\n\n### Longer input sequence\nAlthough 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.\nChanging `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.\n\n## Things I have tried but didn't work\n\n### Test Time Augmentation (TTA)\nI tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.\nWe can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.\nTTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.\n\n### Truncate each input\nThe original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.\nI tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.007271,"end_time":"2024-07-01T02:56:37.823686","exception":false,"start_time":"2024-07-01T02:56:37.816415","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"# Import libs","metadata":{"papermill":{"duration":0.006571,"end_time":"2024-07-01T02:56:37.837206","exception":false,"start_time":"2024-07-01T02:56:37.830635","status":"completed"},"tags":[]}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"papermill":{"duration":53.686843,"end_time":"2024-07-01T02:57:31.530755","exception":false,"start_time":"2024-07-01T02:56:37.843912","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-03T04:41:47.90305Z","iopub.execute_input":"2024-07-03T04:41:47.903328Z","iopub.status.idle":"2024-07-03T04:42:39.631135Z","shell.execute_reply.started":"2024-07-03T04:41:47.903302Z","shell.execute_reply":"2024-07-03T04:42:39.629914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import time\nfrom dataclasses import dataclass\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nfrom transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\nfrom transformers.data.data_collator import pad_without_fast_tokenizer_warning\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType","metadata":{"papermill":{"duration":21.138547,"end_time":"2024-07-01T02:57:52.676238","exception":false,"start_time":"2024-07-01T02:57:31.537691","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:11.62089Z","iopub.execute_input":"2024-07-02T03:14:11.621254Z","iopub.status.idle":"2024-07-02T03:14:30.392808Z","shell.execute_reply.started":"2024-07-02T03:14:11.621219Z","shell.execute_reply":"2024-07-02T03:14:30.391997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"papermill":{"duration":0.014661,"end_time":"2024-07-01T02:57:52.698559","exception":false,"start_time":"2024-07-01T02:57:52.683898","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:30.393853Z","iopub.execute_input":"2024-07-02T03:14:30.394366Z","iopub.status.idle":"2024-07-02T03:14:30.401305Z","shell.execute_reply.started":"2024-07-02T03:14:30.39434Z","shell.execute_reply":"2024-07-02T03:14:30.400241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@dataclass\nclass Config:\n    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    weights_path = '/kaggle/input/lmsys-model/model'\n    max_length = 1280\n    batch_size = 8\n    device = torch.device(\"cuda\")    \n    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n\ncfg = Config()","metadata":{"papermill":{"duration":0.014817,"end_time":"2024-07-01T02:57:52.720706","exception":false,"start_time":"2024-07-01T02:57:52.705889","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:30.403058Z","iopub.execute_input":"2024-07-02T03:14:30.403467Z","iopub.status.idle":"2024-07-02T03:14:30.429871Z","shell.execute_reply.started":"2024-07-02T03:14:30.403417Z","shell.execute_reply":"2024-07-02T03:14:30.429026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Data ","metadata":{"papermill":{"duration":0.006613,"end_time":"2024-07-01T02:57:52.734069","exception":false,"start_time":"2024-07-01T02:57:52.727456","status":"completed"},"tags":[]}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n\n# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(test.head(5))","metadata":{"papermill":{"duration":0.040947,"end_time":"2024-07-01T02:57:52.781962","exception":false,"start_time":"2024-07-01T02:57:52.741015","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:30.432475Z","iopub.execute_input":"2024-07-02T03:14:30.432769Z","iopub.status.idle":"2024-07-02T03:14:30.471168Z","shell.execute_reply.started":"2024-07-02T03:14:30.432745Z","shell.execute_reply":"2024-07-02T03:14:30.470159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{"papermill":{"duration":0.006899,"end_time":"2024-07-01T02:57:52.796232","exception":false,"start_time":"2024-07-01T02:57:52.789333","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def tokenize(\n    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n):\n    prompt = [\"User prompt: \" + p for p in prompt]\n    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n    if spread_max_length:\n        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        attention_mask = [[1]* len(i) for i in input_ids]\n    else:\n        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n        input_ids = tokenized.input_ids\n        attention_mask = tokenized.attention_mask\n    return input_ids, attention_mask","metadata":{"papermill":{"duration":0.017982,"end_time":"2024-07-01T02:57:52.821282","exception":false,"start_time":"2024-07-01T02:57:52.8033","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:30.472522Z","iopub.execute_input":"2024-07-02T03:14:30.472869Z","iopub.status.idle":"2024-07-02T03:14:30.483247Z","shell.execute_reply.started":"2024-07-02T03:14:30.472842Z","shell.execute_reply":"2024-07-02T03:14:30.482262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\ntokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ndata = pd.DataFrame()\ndata[\"id\"] = test[\"id\"]\ndata[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\ndata[\"length\"] = data[\"input_ids\"].apply(len)\n\naug_data = pd.DataFrame()\naug_data[\"id\"] = test[\"id\"]\n# swap response_a & response_b\naug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\naug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)","metadata":{"papermill":{"duration":0.596117,"end_time":"2024-07-01T02:57:53.425111","exception":false,"start_time":"2024-07-01T02:57:52.828994","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:30.484646Z","iopub.execute_input":"2024-07-02T03:14:30.485344Z","iopub.status.idle":"2024-07-02T03:14:31.052319Z","shell.execute_reply.started":"2024-07-02T03:14:30.485309Z","shell.execute_reply":"2024-07-02T03:14:31.051301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(data[\"input_ids\"][0]))","metadata":{"papermill":{"duration":0.015987,"end_time":"2024-07-01T02:57:53.448552","exception":false,"start_time":"2024-07-01T02:57:53.432565","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:31.053586Z","iopub.execute_input":"2024-07-02T03:14:31.053896Z","iopub.status.idle":"2024-07-02T03:14:31.060126Z","shell.execute_reply.started":"2024-07-02T03:14:31.053869Z","shell.execute_reply":"2024-07-02T03:14:31.059158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(tokenizer.decode(aug_data[\"input_ids\"][0]))","metadata":{"papermill":{"duration":0.015964,"end_time":"2024-07-01T02:57:53.472007","exception":false,"start_time":"2024-07-01T02:57:53.456043","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:31.061348Z","iopub.execute_input":"2024-07-02T03:14:31.061655Z","iopub.status.idle":"2024-07-02T03:14:31.069545Z","shell.execute_reply.started":"2024-07-02T03:14:31.061631Z","shell.execute_reply":"2024-07-02T03:14:31.068581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model \nWe load 1 model on each gpu.  ","metadata":{"papermill":{"duration":0.007246,"end_time":"2024-07-01T02:57:53.48653","exception":false,"start_time":"2024-07-01T02:57:53.479284","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# BitsAndBytes configuration\nbnb_config =  BitsAndBytesConfig(\n    load_in_8bit=True,\n    bnb_8bit_compute_dtype=torch.float16,\n    bnb_8bit_use_double_quant=False,\n)\n\n# Load base model on GPU 0\ndevice_0 = torch.device('cuda:0')\nbase_model_0 = LlamaForSequenceClassification.from_pretrained(\n    cfg.model_name,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id\n\n# Load base model on GPU 1\ndevice_1 = torch.device('cuda:1')\nbase_model_1 = LlamaForSequenceClassification.from_pretrained(\n    cfg.model_name,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id","metadata":{"papermill":{"duration":105.076557,"end_time":"2024-07-01T02:59:38.570536","exception":false,"start_time":"2024-07-01T02:57:53.493979","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:14:31.070767Z","iopub.execute_input":"2024-07-02T03:14:31.071051Z","iopub.status.idle":"2024-07-02T03:16:20.333489Z","shell.execute_reply.started":"2024-07-02T03:14:31.071027Z","shell.execute_reply":"2024-07-02T03:16:20.332491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load weights ","metadata":{"papermill":{"duration":0.007888,"end_time":"2024-07-01T02:59:38.586895","exception":false,"start_time":"2024-07-01T02:59:38.579007","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# LoRA configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.10,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj']\n)","metadata":{"papermill":{"duration":0.0162,"end_time":"2024-07-01T02:59:38.610906","exception":false,"start_time":"2024-07-01T02:59:38.594706","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:20.334929Z","iopub.execute_input":"2024-07-02T03:16:20.335814Z","iopub.status.idle":"2024-07-02T03:16:20.342069Z","shell.execute_reply.started":"2024-07-02T03:16:20.335778Z","shell.execute_reply":"2024-07-02T03:16:20.340677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device_0) \n# Load weights\nmodel_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device_1)\nmodel_1.load_state_dict(torch.load(cfg.weights_path), strict=False)\nmodel_1.eval()","metadata":{"papermill":{"duration":13.701042,"end_time":"2024-07-01T02:59:52.320278","exception":false,"start_time":"2024-07-01T02:59:38.619236","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:20.343599Z","iopub.execute_input":"2024-07-02T03:16:20.344016Z","iopub.status.idle":"2024-07-02T03:16:34.706184Z","shell.execute_reply.started":"2024-07-02T03:16:20.343979Z","shell.execute_reply":"2024-07-02T03:16:34.705179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Trainable Parameters\nmodel_0.print_trainable_parameters()\nmodel_1.print_trainable_parameters()","metadata":{"papermill":{"duration":0.026729,"end_time":"2024-07-01T02:59:52.356012","exception":false,"start_time":"2024-07-01T02:59:52.329283","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:34.707509Z","iopub.execute_input":"2024-07-02T03:16:34.70787Z","iopub.status.idle":"2024-07-02T03:16:34.721779Z","shell.execute_reply.started":"2024-07-02T03:16:34.707843Z","shell.execute_reply":"2024-07-02T03:16:34.720734Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference\n","metadata":{"papermill":{"duration":0.008337,"end_time":"2024-07-01T02:59:52.373215","exception":false,"start_time":"2024-07-01T02:59:52.364878","status":"completed"},"tags":[]}},{"cell_type":"code","source":"@torch.no_grad()\n@torch.cuda.amp.autocast()\ndef inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n    a_win, b_win, tie = [], [], []\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        tmp = df.iloc[start_idx:end_idx]\n        input_ids = tmp[\"input_ids\"].to_list()\n        attention_mask = tmp[\"attention_mask\"].to_list()\n        inputs = pad_without_fast_tokenizer_warning(\n            tokenizer,\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n            padding=True,\n            max_length=max_length,\n            pad_to_multiple_of=None,\n            return_tensors=\"pt\",\n        )\n        outputs = model(**inputs.to(device))\n        proba = outputs.logits.softmax(-1).cpu()\n        \n        a_win.extend(proba[:, 0].tolist())\n        b_win.extend(proba[:, 1].tolist())\n        tie.extend(proba[:, 2].tolist())\n    \n    df[\"winner_model_a\"] = a_win\n    df[\"winner_model_b\"] = b_win\n    df[\"winner_tie\"] = tie\n    \n    return df","metadata":{"papermill":{"duration":0.021078,"end_time":"2024-07-01T02:59:52.402973","exception":false,"start_time":"2024-07-01T02:59:52.381895","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:34.725601Z","iopub.execute_input":"2024-07-02T03:16:34.725899Z","iopub.status.idle":"2024-07-02T03:16:34.735894Z","shell.execute_reply.started":"2024-07-02T03:16:34.725874Z","shell.execute_reply":"2024-07-02T03:16:34.734863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\n# sort by input length to fully leverage dynaminc padding\ndata = data.sort_values(\"length\", ascending=False)\n# the total #tokens in sub_1 and sub_2 should be more or less the same\nsub_1 = data.iloc[0::2].copy()\nsub_2 = data.iloc[1::2].copy()\n\nwith ThreadPoolExecutor(max_workers=2) as executor:\n    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\nresult_df = pd.concat(list(results), axis=0)\nproba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"papermill":{"duration":3.316613,"end_time":"2024-07-01T02:59:55.727834","exception":false,"start_time":"2024-07-01T02:59:52.411221","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:34.737213Z","iopub.execute_input":"2024-07-02T03:16:34.737581Z","iopub.status.idle":"2024-07-02T03:16:38.026999Z","shell.execute_reply.started":"2024-07-02T03:16:34.737553Z","shell.execute_reply":"2024-07-02T03:16:38.02593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nif cfg.tta:\n    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n    sub_1 = data.iloc[0::2].copy()\n    sub_2 = data.iloc[1::2].copy()\n\n    with ThreadPoolExecutor(max_workers=2) as executor:\n        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n\n    tta_result_df = pd.concat(list(results), axis=0)\n    # recall TTA's order is flipped\n    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n    # average original result and TTA result.\n    proba = (proba + tta_proba) / 2\n\nprint(f\"elapsed time: {time.time() - st}\")","metadata":{"papermill":{"duration":1.755381,"end_time":"2024-07-01T02:59:57.492377","exception":false,"start_time":"2024-07-01T02:59:55.736996","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:38.028453Z","iopub.execute_input":"2024-07-02T03:16:38.028866Z","iopub.status.idle":"2024-07-02T03:16:39.822255Z","shell.execute_reply.started":"2024-07-02T03:16:38.028828Z","shell.execute_reply":"2024-07-02T03:16:39.821134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\nresult_df.loc[:, \"winner_model_b\"] = proba[:, 1]\nresult_df.loc[:, \"winner_tie\"] = proba[:, 2]\nsubmission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\nsubmission_df.to_csv('submission.csv', index=False)\ndisplay(submission_df)","metadata":{"papermill":{"duration":0.03061,"end_time":"2024-07-01T02:59:57.532498","exception":false,"start_time":"2024-07-01T02:59:57.501888","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-07-02T03:16:39.823604Z","iopub.execute_input":"2024-07-02T03:16:39.823929Z","iopub.status.idle":"2024-07-02T03:16:39.842742Z","shell.execute_reply.started":"2024-07-02T03:16:39.823902Z","shell.execute_reply":"2024-07-02T03:16:39.841784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.008889,"end_time":"2024-07-01T02:59:57.551154","exception":false,"start_time":"2024-07-01T02:59:57.542265","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}