{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534231d0",
   "metadata": {},
   "source": [
    "# Inference - llama-3 8b Super Fast ðŸš€\n",
    "In this notebook we do inference using fine-tuned llama-3 8b model using T4 * 2 Gpu parallel, motivation behind to create this is the huge test size (25k samples). \n",
    "\n",
    "Prerequisite: Access to Llama-3, if you have not filled consent form, go [here](https://www.kaggle.com/models/metaresearch/llama-3), fill the form to access llama-3.  \n",
    "\n",
    "Training notebook can be found [here](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)\n",
    "\n",
    "Please upvote if you find this helpful!\n",
    "\n",
    "# Import libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:02.957881Z",
     "iopub.status.busy": "2024-07-08T02:27:02.957535Z",
     "iopub.status.idle": "2024-07-08T02:27:55.475584Z",
     "shell.execute_reply": "2024-07-08T02:27:55.474311Z",
     "shell.execute_reply.started": "2024-07-08T02:27:02.957851Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:55.477832Z",
     "iopub.status.busy": "2024-07-08T02:27:55.477514Z",
     "iopub.status.idle": "2024-07-08T02:28:01.180228Z",
     "shell.execute_reply": "2024-07-08T02:28:01.179278Z",
     "shell.execute_reply.started": "2024-07-08T02:27:55.477802Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType,prepare_model_for_kbit_training\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.181789Z",
     "iopub.status.busy": "2024-07-08T02:28:01.181357Z",
     "iopub.status.idle": "2024-07-08T02:28:01.186354Z",
     "shell.execute_reply": "2024-07-08T02:28:01.185425Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.181764Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aed9ee",
   "metadata": {},
   "source": [
    "# Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.189204Z",
     "iopub.status.busy": "2024-07-08T02:28:01.188861Z",
     "iopub.status.idle": "2024-07-08T02:28:01.23147Z",
     "shell.execute_reply": "2024-07-08T02:28:01.230662Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.189175Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.232988Z",
     "iopub.status.busy": "2024-07-08T02:28:01.232654Z",
     "iopub.status.idle": "2024-07-08T02:28:01.240511Z",
     "shell.execute_reply": "2024-07-08T02:28:01.239621Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.232956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6c04b",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.24295Z",
     "iopub.status.busy": "2024-07-08T02:28:01.242086Z",
     "iopub.status.idle": "2024-07-08T02:28:02.030199Z",
     "shell.execute_reply": "2024-07-08T02:28:02.029332Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.242902Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041df94",
   "metadata": {},
   "source": [
    "# Load model \n",
    "We load 1 model on each gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:02.03184Z",
     "iopub.status.busy": "2024-07-08T02:28:02.03147Z",
     "iopub.status.idle": "2024-07-08T02:29:45.864969Z",
     "shell.execute_reply": "2024-07-08T02:29:45.863989Z",
     "shell.execute_reply.started": "2024-07-08T02:28:02.031806Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytes configuration\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:0',\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:1',\n",
    "    use_cache=False,\n",
    ")\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a0973",
   "metadata": {},
   "source": [
    "Now, we have sucessfully loaded one model on each GPU!\n",
    "\n",
    "# Load weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:45.866326Z",
     "iopub.status.busy": "2024-07-08T02:29:45.866042Z",
     "iopub.status.idle": "2024-07-08T02:29:45.871399Z",
     "shell.execute_reply": "2024-07-08T02:29:45.870364Z",
     "shell.execute_reply.started": "2024-07-08T02:29:45.8663Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRa configuration\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    inference_mode=True,\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:58.204581Z",
     "iopub.status.busy": "2024-07-08T02:29:58.203873Z",
     "iopub.status.idle": "2024-07-08T02:30:24.990536Z",
     "shell.execute_reply": "2024-07-08T02:30:24.989591Z",
     "shell.execute_reply.started": "2024-07-08T02:29:58.204542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get peft\n",
    "# model_0 = get_peft_model(base_model_0, lora_config).to(device0) \n",
    "model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)\n",
    "# model_0 = model_0.merge_and_unload()\n",
    "# model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_0.config.pretraining_tp = 1\n",
    "model_0.eval()\n",
    "\n",
    "# model_1 = get_peft_model(base_model_1, lora_config).to(device1) \n",
    "model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)\n",
    "# model_1 = model_1.merge_and_unload()\n",
    "# model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_0.config.pretraining_tp = 1\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.487776Z",
     "iopub.status.busy": "2024-07-08T02:30:48.487408Z",
     "iopub.status.idle": "2024-07-08T02:30:48.506346Z",
     "shell.execute_reply": "2024-07-08T02:30:48.505415Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.487748Z"
    }
   },
   "outputs": [],
   "source": [
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043d12e",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.831515Z",
     "iopub.status.busy": "2024-07-08T02:30:48.830751Z",
     "iopub.status.idle": "2024-07-08T02:30:49.013585Z",
     "shell.execute_reply": "2024-07-08T02:30:49.012589Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.831485Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.015875Z",
     "iopub.status.busy": "2024-07-08T02:30:49.015444Z",
     "iopub.status.idle": "2024-07-08T02:30:49.025481Z",
     "shell.execute_reply": "2024-07-08T02:30:49.024531Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.015843Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.168703Z",
     "iopub.status.busy": "2024-07-08T02:30:49.168372Z",
     "iopub.status.idle": "2024-07-08T02:31:00.511295Z",
     "shell.execute_reply": "2024-07-08T02:31:00.510349Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.168678Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}\n",
    "\n",
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.513694Z",
     "iopub.status.busy": "2024-07-08T02:31:00.513202Z",
     "iopub.status.idle": "2024-07-08T02:31:00.532115Z",
     "shell.execute_reply": "2024-07-08T02:31:00.530886Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.513657Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "display(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.534677Z",
     "iopub.status.busy": "2024-07-08T02:31:00.533529Z",
     "iopub.status.idle": "2024-07-08T02:31:00.551799Z",
     "shell.execute_reply": "2024-07-08T02:31:00.550761Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.534638Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46876b6",
   "metadata": {},
   "source": [
    "Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351301,
     "sourceId": 8917867,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186059439,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
