{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8811121,"sourceType":"datasetVersion","datasetId":5299906},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":186083288,"sourceType":"kernelVersion"},{"sourceId":186270439,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Gemma 2 - 9b\nWe train a simple classifier in this using computed embeddings as input from [here](https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1?scriptVersionId=186083288) and compute embeddings for test and use trained classifier for inference. Let's get started!\n\nUpvote if you found this helpful!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import Libs","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip\n!pip install -q -U transformers --no-index --find-links ../input/libs-install","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:44:19.939541Z","iopub.execute_input":"2024-06-30T19:44:19.939828Z","iopub.status.idle":"2024-06-30T19:44:58.916383Z","shell.execute_reply.started":"2024-06-30T19:44:19.939804Z","shell.execute_reply":"2024-06-30T19:44:58.915213Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\n\nimport torch\nimport transformers\nimport sklearn\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig\n\nimport time\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, log_loss\n\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:45:17.48338Z","iopub.execute_input":"2024-06-30T19:45:17.484133Z","iopub.status.idle":"2024-06-30T19:45:24.216568Z","shell.execute_reply.started":"2024-06-30T19:45:17.484097Z","shell.execute_reply":"2024-06-30T19:45:24.215788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Classifier ","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('/kaggle/input/gemma-2-9b-part-1/train_embed.csv')\ntrain_embed = np.load('/kaggle/input/gemma-2-9b-part-1/gemma2_train_embed.npy')\n\ntrain_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:38:03.532342Z","iopub.execute_input":"2024-06-30T19:38:03.532707Z","iopub.status.idle":"2024-06-30T19:38:09.162491Z","shell.execute_reply.started":"2024-06-30T19:38:03.532676Z","shell.execute_reply":"2024-06-30T19:38:09.161636Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splits\nTargets = ['winner_model_a','winner_model_b','winner_tie']\n\ny = train_df['label'].values\ntrain_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)\n\nX_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values\nX_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values\n\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:38:09.16452Z","iopub.execute_input":"2024-06-30T19:38:09.16517Z","iopub.status.idle":"2024-06-30T19:38:09.295013Z","shell.execute_reply.started":"2024-06-30T19:38:09.165118Z","shell.execute_reply":"2024-06-30T19:38:09.293963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Here we just use classifier with default settings, try out tuning the params) \n\nmodel_cb = CatBoostClassifier(\n    iterations=1000,\n    learning_rate=0.03,\n    loss_function='MultiClass',\n    eval_metric='MultiClass',\n    early_stopping_rounds=10,\n    task_type='GPU',\n    devices='0:1',\n    verbose=100)\n\nmodel_cb.fit(X_train, y_train, \n          eval_set=(X_test, y_test),\n          early_stopping_rounds=50)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:38:09.29607Z","iopub.execute_input":"2024-06-30T19:38:09.296384Z","iopub.status.idle":"2024-06-30T19:40:25.215201Z","shell.execute_reply.started":"2024-06-30T19:38:09.296357Z","shell.execute_reply":"2024-06-30T19:40:25.214402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_proba = model_cb.predict_proba(X_test)\ny_pred = model_cb.predict(X_test)\n\n# Evaluate the model\nlogloss = log_loss(y_test, y_pred_proba)\naccuracy = accuracy_score(y_test, y_pred)\ngc.collect()\n\nprint(f'Log Loss: {logloss:.3f}')\nprint(f'Accuracy: {accuracy:.3f}')","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:40:25.21705Z","iopub.execute_input":"2024-06-30T19:40:25.217867Z","iopub.status.idle":"2024-06-30T19:40:39.524493Z","shell.execute_reply.started":"2024-06-30T19:40:25.217834Z","shell.execute_reply":"2024-06-30T19:40:39.523612Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We'll use this classifer for inference.","metadata":{}},{"cell_type":"markdown","source":"# Load Gemma 2","metadata":{}},{"cell_type":"code","source":"MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'\nMAX_LENGTH = 1024\nBATCH_SIZE = 2\n    \ndevice0 = torch.device('cuda:0')\ndevice1 = torch.device('cuda:1')\n\ntokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)\n\nbnb_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False)\n\nmodel_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,\n                                        revision=\"float16\",\n                                        device_map='cuda:0',\n                                        quantization_config=bnb_config_4bit)        \n\nmodel_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,\n                                        revision=\"float16\",\n                                        device_map='cuda:1',\n                                        quantization_config=bnb_config_4bit)     ","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:45:34.255007Z","iopub.execute_input":"2024-06-30T19:45:34.255517Z","iopub.status.idle":"2024-06-30T19:48:09.97744Z","shell.execute_reply.started":"2024-06-30T19:45:34.255485Z","shell.execute_reply":"2024-06-30T19:48:09.976588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return sentences[-1] if sentences else ''\n  \ntest = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ntest['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n----\\n\\nModel B:\\n'  + test['response_b'] + '<end_of_turn><eos>'\nprint(test['text'][0])","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:40:41.08643Z","iopub.status.idle":"2024-06-30T19:40:41.086888Z","shell.execute_reply.started":"2024-06-30T19:40:41.086656Z","shell.execute_reply":"2024-06-30T19:40:41.086675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize ","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer(test['text'].tolist(),\n                   padding='max_length',\n                   max_length=MAX_LENGTH,\n                   truncation=True,\n                   return_tensors='pt')\n\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]\ndata['ATTENTION_MASKS'] = [tensor.tolist() for tensor in  tokens['attention_mask']]\ndata[:2]","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:06:53.302703Z","iopub.execute_input":"2024-06-30T19:06:53.303056Z","iopub.status.idle":"2024-06-30T19:06:53.350283Z","shell.execute_reply.started":"2024-06-30T19:06:53.303027Z","shell.execute_reply":"2024-06-30T19:06:53.349471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Get embeddings","metadata":{}},{"cell_type":"code","source":"def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  \n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n\n    embed_list = []\n\n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        gc.collect()\n        torch.cuda.empty_cache()\n        with torch.no_grad():\n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)\n            embed = outputs.hidden_states[-1]\n            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool\n            embed_list.append(embed_mean) \n            \n            torch.cuda.empty_cache()\n        \n    embeddings = torch.cat(embed_list, dim=0)\n    return embeddings\n\ndef compute_embed(df, model, device, results, index):\n    results[index] = get_embeddings(df, model, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:07:31.838907Z","iopub.execute_input":"2024-06-30T19:07:31.83978Z","iopub.status.idle":"2024-06-30T19:07:31.84909Z","shell.execute_reply.started":"2024-06-30T19:07:31.839747Z","shell.execute_reply":"2024-06-30T19:07:31.848135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\nresults = {}\n\nt0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\nt0.join()\nt1.join()\n\nprint(f\"Processing complete. Total time: {time.time() - st:.2f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:07:54.564396Z","iopub.execute_input":"2024-06-30T19:07:54.565046Z","iopub.status.idle":"2024-06-30T19:08:10.694665Z","shell.execute_reply.started":"2024-06-30T19:07:54.565016Z","shell.execute_reply":"2024-06-30T19:08:10.693693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_embeddings = torch.cat([results[0], results[1]], dim=0)\ntest_embeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:09:36.313867Z","iopub.execute_input":"2024-06-30T19:09:36.314841Z","iopub.status.idle":"2024-06-30T19:09:36.320831Z","shell.execute_reply.started":"2024-06-30T19:09:36.314808Z","shell.execute_reply":"2024-06-30T19:09:36.319932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ndel model_1\ndel  model_0\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:08:52.875516Z","iopub.execute_input":"2024-06-30T19:08:52.876211Z","iopub.status.idle":"2024-06-30T19:08:53.470247Z","shell.execute_reply.started":"2024-06-30T19:08:52.87618Z","shell.execute_reply":"2024-06-30T19:08:53.469297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"preds = model_cb.predict_proba(test_embeddings.numpy())\npreds","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:09:53.812362Z","iopub.execute_input":"2024-06-30T19:09:53.812765Z","iopub.status.idle":"2024-06-30T19:09:53.843767Z","shell.execute_reply.started":"2024-06-30T19:09:53.812734Z","shell.execute_reply":"2024-06-30T19:09:53.842674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Submission","metadata":{}},{"cell_type":"code","source":"sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\nsample_sub[Targets] =  preds\n\ndisplay(sample_sub)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:11:43.391752Z","iopub.execute_input":"2024-06-30T19:11:43.392109Z","iopub.status.idle":"2024-06-30T19:11:43.407772Z","shell.execute_reply.started":"2024-06-30T19:11:43.392082Z","shell.execute_reply":"2024-06-30T19:11:43.406801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T19:11:58.588251Z","iopub.execute_input":"2024-06-30T19:11:58.588605Z","iopub.status.idle":"2024-06-30T19:11:58.599109Z","shell.execute_reply.started":"2024-06-30T19:11:58.58858Z","shell.execute_reply":"2024-06-30T19:11:58.598312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \n\nThat's it! Just wanted to share the idea! Try out tuning the classifier or using other classifier. Thanks!\n\nIf you learned something, Please upvote:)","metadata":{}}]}