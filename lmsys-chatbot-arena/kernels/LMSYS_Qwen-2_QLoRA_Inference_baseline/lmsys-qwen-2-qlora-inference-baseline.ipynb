{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "404f56ec",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook is forked from [Here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)\n",
    "\n",
    "Qwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.\n",
    "\n",
    "The notebook training Qwen2 1.5b version with batch size of 4 and 1 epoch. The training time is around 1 hr on A100. I expect that Qwen2 7b could have better performance on this task. The training code for Colab is attached at the end of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-20T00:21:28.054679Z",
     "iopub.status.busy": "2024-07-20T00:21:28.053903Z"
    },
    "papermill": {
     "duration": 31.479497,
     "end_time": "2024-07-10T01:13:41.690971",
     "exception": false,
     "start_time": "2024-07-10T01:13:10.211474",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 19.200405,
     "end_time": "2024-07-10T01:14:00.90474",
     "exception": false,
     "start_time": "2024-07-10T01:13:41.704335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Qwen2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.225826Z",
     "iopub.status.idle": "2024-07-20T00:16:19.226184Z",
     "shell.execute_reply": "2024-07-20T00:16:19.226021Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.226008Z"
    },
    "papermill": {
     "duration": 0.047799,
     "end_time": "2024-07-10T01:14:00.965921",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.918122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41514b",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.228103Z",
     "iopub.status.idle": "2024-07-20T00:16:19.228495Z",
     "shell.execute_reply": "2024-07-20T00:16:19.228332Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.228317Z"
    },
    "papermill": {
     "duration": 0.021338,
     "end_time": "2024-07-10T01:14:01.000606",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.979268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dir = '/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1'\n",
    "    lora_dir = '/kaggle/input/lmsys-qwen2-1-5b-checkpoint/checkpoint-5748'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484f74b",
   "metadata": {},
   "source": [
    "# Load & pre-process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.22986Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23022Z",
     "shell.execute_reply": "2024-07-20T00:16:19.230068Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.230053Z"
    },
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-07-10T01:14:01.06946",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.03979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.231625Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23215Z",
     "shell.execute_reply": "2024-07-20T00:16:19.231915Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.231895Z"
    },
    "papermill": {
     "duration": 0.040127,
     "end_time": "2024-07-10T01:14:01.12241",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.082283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2b1f8",
   "metadata": {},
   "source": [
    "# Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.23378Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23424Z",
     "shell.execute_reply": "2024-07-20T00:16:19.234021Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.234003Z"
    },
    "papermill": {
     "duration": 0.030237,
     "end_time": "2024-07-10T01:14:01.194318",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.164081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"<User prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.235624Z",
     "iopub.status.idle": "2024-07-20T00:16:19.236104Z",
     "shell.execute_reply": "2024-07-20T00:16:19.23588Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.235861Z"
    },
    "papermill": {
     "duration": 1.169844,
     "end_time": "2024-07-10T01:14:02.377579",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.207735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.237697Z",
     "iopub.status.idle": "2024-07-20T00:16:19.238075Z",
     "shell.execute_reply": "2024-07-20T00:16:19.237906Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.23789Z"
    },
    "papermill": {
     "duration": 0.024759,
     "end_time": "2024-07-10T01:14:02.419091",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.394332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.23967Z",
     "iopub.status.idle": "2024-07-20T00:16:19.240033Z",
     "shell.execute_reply": "2024-07-20T00:16:19.239858Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.239844Z"
    },
    "papermill": {
     "duration": 0.021982,
     "end_time": "2024-07-10T01:14:02.454045",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.432063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d22ca2",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.242975Z",
     "iopub.status.idle": "2024-07-20T00:16:19.243396Z",
     "shell.execute_reply": "2024-07-20T00:16:19.24319Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.243174Z"
    },
    "papermill": {
     "duration": 83.919146,
     "end_time": "2024-07-10T01:15:26.412583",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.493437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    cfg.model_dir,\n",
    "    num_labels=3,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    cfg.model_dir,\n",
    "    num_labels=3,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209288d9",
   "metadata": {},
   "source": [
    "#### Load LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.244803Z",
     "iopub.status.idle": "2024-07-20T00:16:19.245324Z",
     "shell.execute_reply": "2024-07-20T00:16:19.245117Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.245101Z"
    },
    "papermill": {
     "duration": 1.265087,
     "end_time": "2024-07-10T01:15:27.719297",
     "exception": false,
     "start_time": "2024-07-10T01:15:26.45421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_0.config.pad_token_id = model_0.config.eos_token_id\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)\n",
    "model_1.config.pad_token_id = model_1.config.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500f21e",
   "metadata": {},
   "source": [
    "# Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.247098Z",
     "iopub.status.idle": "2024-07-20T00:16:19.247484Z",
     "shell.execute_reply": "2024-07-20T00:16:19.247312Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.247296Z"
    },
    "papermill": {
     "duration": 0.026726,
     "end_time": "2024-07-10T01:15:27.838497",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.811771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.248879Z",
     "iopub.status.idle": "2024-07-20T00:16:19.24943Z",
     "shell.execute_reply": "2024-07-20T00:16:19.249062Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.249048Z"
    },
    "papermill": {
     "duration": 4.598663,
     "end_time": "2024-07-10T01:15:32.45234",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.853677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.250756Z",
     "iopub.status.idle": "2024-07-20T00:16:19.251103Z",
     "shell.execute_reply": "2024-07-20T00:16:19.25094Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.250926Z"
    },
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-07-10T01:15:32.491283",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.466724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.25235Z",
     "iopub.status.idle": "2024-07-20T00:16:19.252721Z",
     "shell.execute_reply": "2024-07-20T00:16:19.252556Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.252541Z"
    },
    "papermill": {
     "duration": 0.034664,
     "end_time": "2024-07-10T01:15:32.539974",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.50531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d409a4",
   "metadata": {},
   "source": [
    "# Training Code\n",
    "\n",
    "This training code is forked and inspired from https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning\n",
    "\n",
    "\n",
    "## Transfer Kaggle Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.254501Z",
     "iopub.status.idle": "2024-07-20T00:16:19.254894Z",
     "shell.execute_reply": "2024-07-20T00:16:19.254724Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.254708Z"
    }
   },
   "outputs": [],
   "source": [
    "# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
    "# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
    "# # THEN FEEL FREE TO DELETE THIS CELL.\n",
    "# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
    "# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
    "# # NOTEBOOK.\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# from tempfile import NamedTemporaryFile\n",
    "# from urllib.request import urlopen\n",
    "# from urllib.parse import unquote, urlparse\n",
    "# from urllib.error import HTTPError\n",
    "# from zipfile import ZipFile\n",
    "# import tarfile\n",
    "# import shutil\n",
    "\n",
    "# CHUNK_SIZE = 40960\n",
    "# DATA_SOURCE_MAPPING = 'lmsys-chatbot-arena:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66631%2F8346466%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ae2cd10c5a4750deca45551519904c5858980d5cf8cd8ade09b2299d926c86c895b50ba333acf0db5210d0dd29197c9a9c5a525c8afd0186b88f17d3ca756f0562ad5acfa2e856e8b159f554e61f72102865abd60add751dd59bed5126536d977d6fe54d2e85f8e5baa8d3d75337d0a222a89f0f30fa6dd7c360e4a192363dc417e9e4a9c9c23368991db65b4994c2200bee494d8d5e2684f754ab1b1a511f7db3652e01ab658b04d26cc1321e783fa5509f67d4c438808adc7932a0e79a21849375023b36e90cbe288cf68a6185b2ce950464b71c9b6133d49769c67e77a5298809fb63da23c0655165e80661623bd9bb908bc9a486dbc9e09caebf2a01392,qwen2/transformers/qwen2-1.5b-instruct/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F52038%2F62308%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80f6a1f964073d960129f611693afca0666128c1a319b7ab93769a1993d5910e4995cf3c6f2f87323452999b069eafb9f7b01be97dbb80a3441cd0b4871d9d35379750a3b5614397253624b097961c886c48df889ac7b1100231715e2b60bf4f3ffccd5e7fc68b3d7d0668f350a8fefbddb90275770e75aaa7f74fae68b2f5314f610ec2f1abf0436156e9426e6173e229172ca0c4ee91eb2d768de3190c9f07e6c28b73bc8c5553e2dac6320842103524591b663021a41801bb2274b5fd91dd62f174ba8976c74995012ad3ed34ecf554a9e2cb08f91813e9cacc9b8d554c6a7a037414635f30e506ea39f63fb4db01f5cf3322dca02097f9550b1a5454ae99'\n",
    "\n",
    "# KAGGLE_INPUT_PATH='/kaggle/input'\n",
    "# KAGGLE_WORKING_PATH='/kaggle/working'\n",
    "# KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "# !umount /kaggle/input/ 2> /dev/null\n",
    "# shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "\n",
    "# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "#     directory, download_url_encoded = data_source_mapping.split(':')\n",
    "#     download_url = unquote(download_url_encoded)\n",
    "#     filename = urlparse(download_url).path\n",
    "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "#     try:\n",
    "#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "#             total_length = fileres.headers['content-length']\n",
    "#             print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "#             dl = 0\n",
    "#             data = fileres.read(CHUNK_SIZE)\n",
    "#             while len(data) > 0:\n",
    "#                 dl += len(data)\n",
    "#                 tfile.write(data)\n",
    "#                 done = int(50 * dl / int(total_length))\n",
    "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "#                 sys.stdout.flush()\n",
    "#                 data = fileres.read(CHUNK_SIZE)\n",
    "#             if filename.endswith('.zip'):\n",
    "#               with ZipFile(tfile) as zfile:\n",
    "#                 zfile.extractall(destination_path)\n",
    "#             else:\n",
    "#               with tarfile.open(tfile.name) as tarfile:\n",
    "#                 tarfile.extractall(destination_path)\n",
    "#             print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "#     except HTTPError as e:\n",
    "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "#     except OSError as e:\n",
    "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "\n",
    "# print('Data source import complete.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f863364",
   "metadata": {},
   "source": [
    "## Install and Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.255983Z",
     "iopub.status.idle": "2024-07-20T00:16:19.256372Z",
     "shell.execute_reply": "2024-07-20T00:16:19.256174Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.256159Z"
    }
   },
   "outputs": [],
   "source": [
    "## gemma-2 is available from transformers>=4.42.3\n",
    "# !pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.25779Z",
     "iopub.status.idle": "2024-07-20T00:16:19.258299Z",
     "shell.execute_reply": "2024-07-20T00:16:19.258051Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.258031Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import copy\n",
    "# from dataclasses import dataclass\n",
    "\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from datasets import Dataset\n",
    "# from transformers import (\n",
    "#     BitsAndBytesConfig,\n",
    "#     AutoTokenizer,\n",
    "#     Qwen2ForSequenceClassification,\n",
    "#     PreTrainedTokenizerBase,\n",
    "#     EvalPrediction,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorWithPadding,\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "# from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e49717",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.260099Z",
     "iopub.status.idle": "2024-07-20T00:16:19.260609Z",
     "shell.execute_reply": "2024-07-20T00:16:19.260369Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.260348Z"
    }
   },
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Config:\n",
    "#     output_dir: str = \"output\"\n",
    "#     model_name: str = \"/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1\"\n",
    "#     checkpoint: str = \"/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1\" # local directory of qwen model\n",
    "#     max_length: int = 2048\n",
    "#     n_splits: int = 5\n",
    "#     fold_idx: int = 0\n",
    "#     optim_type: str = \"adamw_8bit\"\n",
    "#     per_device_train_batch_size: int = 4\n",
    "#     gradient_accumulation_steps: int = 2  # global batch size is 8\n",
    "#     per_device_eval_batch_size: int = 8\n",
    "#     n_epochs: int = 1\n",
    "#     freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "#     lr: float = 2e-4\n",
    "#     warmup_steps: int = 20\n",
    "#     lora_r: int = 16\n",
    "#     lora_alpha: float = lora_r * 2\n",
    "#     lora_dropout: float = 0.05\n",
    "#     lora_bias: str = \"none\"\n",
    "\n",
    "# config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14927cb",
   "metadata": {},
   "source": [
    "## Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.263021Z",
     "iopub.status.idle": "2024-07-20T00:16:19.263439Z",
     "shell.execute_reply": "2024-07-20T00:16:19.263227Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.263212Z"
    }
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"output\",\n",
    "#     overwrite_output_dir=True,\n",
    "#     report_to=\"none\",\n",
    "#     num_train_epochs=config.n_epochs,\n",
    "#     per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "#     gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "#     per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "#     logging_steps=10,\n",
    "#     eval_strategy=\"epoch\",\n",
    "#     save_strategy=\"steps\",\n",
    "#     save_steps=200,\n",
    "#     optim=config.optim_type,\n",
    "#     fp16=True,\n",
    "#     learning_rate=config.lr,\n",
    "#     warmup_steps=config.warmup_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9c7f5",
   "metadata": {},
   "source": [
    "## LoRA Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.264398Z",
     "iopub.status.idle": "2024-07-20T00:16:19.264765Z",
     "shell.execute_reply": "2024-07-20T00:16:19.264599Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.264584Z"
    }
   },
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=config.lora_r,\n",
    "#     lora_alpha=config.lora_alpha,\n",
    "#     # only target self-attention\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "#     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "#     lora_dropout=config.lora_dropout,\n",
    "#     bias=config.lora_bias,\n",
    "#     task_type=TaskType.SEQ_CLS,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a19b4b",
   "metadata": {},
   "source": [
    "## Initialize and Tokenike model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.26592Z",
     "iopub.status.idle": "2024-07-20T00:16:19.266307Z",
     "shell.execute_reply": "2024-07-20T00:16:19.266116Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.266102Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
    "# tokenizer.add_eos_token = True\n",
    "# tokenizer.padding_side = \"right\"\n",
    "# model = Qwen2ForSequenceClassification.from_pretrained(\n",
    "#     config.model_name,\n",
    "#     num_labels=3,\n",
    "#     torch_dtype=torch.float16,\n",
    "#     device_map=\"auto\",\n",
    "# )\n",
    "# model.config.use_cache = False\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "# model = get_peft_model(model, lora_config)\n",
    "# model.config.pad_token_id = model.config.eos_token_id\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.267566Z",
     "iopub.status.idle": "2024-07-20T00:16:19.267923Z",
     "shell.execute_reply": "2024-07-20T00:16:19.267762Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.267747Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce0f31",
   "metadata": {},
   "source": [
    "# Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.269677Z",
     "iopub.status.idle": "2024-07-20T00:16:19.270024Z",
     "shell.execute_reply": "2024-07-20T00:16:19.269866Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.269851Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "# class CustomTokenizer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         tokenizer: PreTrainedTokenizerBase,\n",
    "#         max_length: int\n",
    "#     ) -> None:\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __call__(self, batch: dict) -> dict:\n",
    "#         prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
    "#         response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
    "#         response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
    "#         texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "#         tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "#         labels=[]\n",
    "#         for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "#             if a_win:\n",
    "#                 label = 0\n",
    "#             elif b_win:\n",
    "#                 label = 1\n",
    "#             else:\n",
    "#                 label = 2\n",
    "#             labels.append(label)\n",
    "#         return {**tokenized, \"labels\": labels}\n",
    "\n",
    "#     @staticmethod\n",
    "#     def process_text(text: str) -> str:\n",
    "#         return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "# encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "# ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be123a58",
   "metadata": {},
   "source": [
    "## Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.27132Z",
     "iopub.status.idle": "2024-07-20T00:16:19.27167Z",
     "shell.execute_reply": "2024-07-20T00:16:19.271506Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.271492Z"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "#     preds = eval_preds.predictions\n",
    "#     labels = eval_preds.label_ids\n",
    "#     probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "#     loss = log_loss(y_true=labels, y_pred=probs)\n",
    "#     acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "#     return {\"acc\": acc, \"log_loss\": loss}\n",
    "\n",
    "# folds = [\n",
    "#     (\n",
    "#         [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "#         [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "#     )\n",
    "#     for fold_idx in range(config.n_splits)\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.272998Z",
     "iopub.status.idle": "2024-07-20T00:16:19.273371Z",
     "shell.execute_reply": "2024-07-20T00:16:19.273181Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.273167Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "# trainer = Trainer(\n",
    "#     args=training_args,\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     train_dataset=ds.select(train_idx),\n",
    "#     eval_dataset=ds.select(eval_idx),\n",
    "#     compute_metrics=compute_metrics,\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    "# )\n",
    "# trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5395179,
     "sourceId": 8963385,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 71342,
     "modelInstanceId": 52038,
     "sourceId": 62308,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.347272,
   "end_time": "2024-07-10T01:15:35.655682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T01:13:07.30841",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f59addf0d2f40309e025976c382cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_268e3946106b4e41849bf11c5a375dac",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb130c471af4927a6644f932ae47523",
       "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
      }
     },
     "19ef2d43bafa44a8b20dc5230aea5ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca042ffa14e4dbebdc66435f7b1f07f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d03719518b8423099b8b68a92e449d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64ef70cfa9c04764868fa52963323322",
       "placeholder": "​",
       "style": "IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d89705c74d34016bbc1e0601ead825c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d409334237014614bf9ae742597d98ea",
       "placeholder": "​",
       "style": "IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6",
       "value": " 2/2 [01:18&lt;00:00, 34.68s/it]"
      }
     },
     "1ef6d64d40d8461d9e6adddd513089b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268e3946106b4e41849bf11c5a375dac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "324c2396f44f45c89d9ec264007ef9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5bb130c471af4927a6644f932ae47523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d243712a1a545e99fa858b0cf19831d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e81b324ca1b46a2a96d02bb2acadc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64ef70cfa9c04764868fa52963323322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ae4b02a13f4570ad729c437ebd28ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f",
        "IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef",
        "IPY_MODEL_0f59addf0d2f40309e025976c382cad8"
       ],
       "layout": "IPY_MODEL_9422a87b93ab4473913e601da3a18689"
      }
     },
     "81b4a9a7cde64b17856b89dbd238c0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff",
       "value": 2
      }
     },
     "85d217d6b45847869cea506db59e8b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d243712a1a545e99fa858b0cf19831d",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96385fd98f304649ab5c4ae81333fb63",
       "value": 2
      }
     },
     "875758123e4f41f0b5c3fa0cd4fb47c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9422a87b93ab4473913e601da3a18689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96385fd98f304649ab5c4ae81333fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d409334237014614bf9ae742597d98ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d576a283e6424206ab4c25d809241c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8a7714cd80d479e859b6ae31ebce7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d98918fae8174629b4819a1114f21202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d03719518b8423099b8b68a92e449d7",
        "IPY_MODEL_85d217d6b45847869cea506db59e8b42",
        "IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"
       ],
       "layout": "IPY_MODEL_d576a283e6424206ab4c25d809241c21"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
