{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":87350,"sourceType":"modelInstanceVersion","modelInstanceId":73373,"modelId":98230}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from tqdm import tqdm\nimport pandas as pd\nimport json\nimport torch\nfrom transformers import AutoModel\nfrom numpy.linalg import norm\nimport torch.nn as nn\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\nimport re","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-02T08:49:33.158285Z","iopub.execute_input":"2024-08-02T08:49:33.158666Z","iopub.status.idle":"2024-08-02T08:49:33.16425Z","shell.execute_reply.started":"2024-08-02T08:49:33.158636Z","shell.execute_reply":"2024-08-02T08:49:33.163288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available else \"cpu\"\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-08-02T08:49:33.169448Z","iopub.execute_input":"2024-08-02T08:49:33.170388Z","iopub.status.idle":"2024-08-02T08:49:33.187453Z","shell.execute_reply.started":"2024-08-02T08:49:33.170353Z","shell.execute_reply":"2024-08-02T08:49:33.186421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoModel.from_pretrained('/kaggle/input/jinaai/pytorch/default/4')","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:21:58.339099Z","iopub.execute_input":"2024-08-02T09:21:58.340116Z","iopub.status.idle":"2024-08-02T09:21:58.733531Z","shell.execute_reply.started":"2024-08-02T09:21:58.340082Z","shell.execute_reply":"2024-08-02T09:21:58.732695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class EmbeddingModel(nn.Module):\n    def __init__(self, embedding_model, max_sequences):\n        super(EmbeddingModel, self).__init__()\n        self.embedding = embedding_model\n        self.max_seq_length = max_sequences\n        self.device = device\n\n    def forward(self, prompts, responses_a, responses_b):\n        batch_features_a = []\n        batch_features_b = []\n\n        for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):\n            prompt = json.loads(prompt)\n            response_a = json.loads(response_a)\n            response_b = json.loads(response_b)\n            \n            prompt = [\"\" if p is None else p for p in prompt]\n            response_a = [\"\" if r is None else r for r in response_a]\n            response_b = [\"\" if r is None else r for r in response_b]\n            \n            \n            embedded_prompt = torch.from_numpy(self.embedding.encode(prompt)).to(self.device)\n           \n            embedded_response_a = torch.from_numpy(self.embedding.encode(response_a)).to(self.device)\n            embedded_response_b = torch.from_numpy(self.embedding.encode(response_b)).to(self.device)\n\n            features_a = []\n            features_b = []\n            for i in range(len(embedded_prompt)):\n                combined_a = torch.cat((embedded_prompt[i], embedded_response_a[i]), dim=0)\n                combined_b = torch.cat((embedded_prompt[i], embedded_response_b[i]), dim=0)\n\n                features_a.append(combined_a)\n                features_b.append(combined_b)\n\n            features_a = torch.stack(features_a) if features_a else torch.tensor([]).to(self.device)\n            features_b = torch.stack(features_b) if features_b else torch.tensor([]).to(self.device)\n\n            features_a = self.pad_to_shape(features_a, (self.max_seq_length, 768 * 2))\n            features_b = self.pad_to_shape(features_b, (self.max_seq_length, 768 * 2))\n\n            batch_features_a.append(features_a)\n            batch_features_b.append(features_b)\n\n        return torch.stack(batch_features_a).to(self.device), torch.stack(batch_features_b).to(self.device)\n\n    def pad_to_shape(self, tensor, shape):\n        current_shape = tensor.shape\n        padding = [(0, max(s - cs, 0)) for cs, s in zip(current_shape, shape)]\n        padded_tensor = F.pad(tensor, pad=[p for pair in reversed(padding) for p in pair], mode='constant', value=0)\n        return padded_tensor[:shape[0], :shape[1]]\n\nclass Model(nn.Module):\n    def __init__(self, embedding_model, max_sequences=64, hidden_dim=512, dropout=0.3):\n        super(Model, self).__init__()\n        self.device = device\n        self.embedding = EmbeddingModel(embedding_model, max_sequences)\n        self.lstm_input_a = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)\n        self.lstm_input_b = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)\n\n        self.conv_input_a = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)\n        self.conv_input_b = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)\n\n        self.fc = nn.Sequential(\n            nn.Linear(hidden_dim * 2 + hidden_dim * 2, 256),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Dropout(dropout),\n            nn.Linear(128, 3),\n            nn.Softmax()\n        ).to(self.device)\n\n    def forward(self, prompts, responses_a, responses_b):\n        batch_a, batch_b = self.embedding(prompts, responses_a, responses_b)\n\n        batch_a_lstm, _ = self.lstm_input_a(batch_a)  # (batch, 64, hidden_dim)\n        batch_b_lstm, _ = self.lstm_input_b(batch_b)  # (batch, 64, hidden_dim)\n\n        batch_a_cnn = self.conv_input_a(batch_a.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)\n        batch_b_cnn = self.conv_input_b(batch_b.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)\n\n        batch_a_lstm = batch_a_lstm[:, -1, :] \n        batch_b_lstm = batch_b_lstm[:, -1, :]  \n        batch_a_cnn = batch_a_cnn[:, -1, :]    \n        batch_b_cnn = batch_b_cnn[:, -1, :]\n        \n        combined = torch.cat([batch_a_lstm, batch_a_cnn, batch_b_lstm, batch_b_cnn], dim=1)\n        flattened = combined.view(combined.size(0), -1)\n\n        output = self.fc(flattened)\n        return output\n    \nclass DatasetLMSYS(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        prompt = sample['prompt']\n        response_a = sample['response_a']\n        response_b = sample['response_b']\n        label = sample['model_result']\n        return prompt, response_a, response_b, label\n    \nclass DatasetLMSYSTest(Dataset):\n    def __init__(self, data):\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data.iloc[idx]\n        _id = sample['id']\n        _prompt = sample['prompt']\n        _response_a = sample['response_a']\n        _response_b = sample['response_b']\n        return _id, _prompt, _response_a, _response_b","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:20:17.038348Z","iopub.execute_input":"2024-08-02T09:20:17.03872Z","iopub.status.idle":"2024-08-02T09:20:17.067718Z","shell.execute_reply.started":"2024-08-02T09:20:17.038692Z","shell.execute_reply":"2024-08-02T09:20:17.066641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(tokenizer).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:08:33.3659Z","iopub.execute_input":"2024-08-02T09:08:33.366619Z","iopub.status.idle":"2024-08-02T09:08:33.499952Z","shell.execute_reply.started":"2024-08-02T09:08:33.366588Z","shell.execute_reply":"2024-08-02T09:08:33.499165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 128\nlearning_rate = 0.001\nepochs = 5","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:08:34.029641Z","iopub.execute_input":"2024-08-02T09:08:34.030007Z","iopub.status.idle":"2024-08-02T09:08:34.034188Z","shell.execute_reply.started":"2024-08-02T09:08:34.029978Z","shell.execute_reply":"2024-08-02T09:08:34.033273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\nfile_data['model_result'] = file_data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)\nfile_data = file_data[['prompt', 'response_a', 'response_b', 'model_result']]\ntrain_loader = DataLoader(\n    dataset=DatasetLMSYS(file_data),\n    batch_size=batch_size,\n    shuffle=True\n)\n\nfile_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\ntest_loader = DataLoader(\n    dataset=DatasetLMSYSTest(file_test),\n    batch_size=batch_size,\n    shuffle=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:08:34.644017Z","iopub.execute_input":"2024-08-02T09:08:34.645007Z","iopub.status.idle":"2024-08-02T09:08:37.197375Z","shell.execute_reply.started":"2024-08-02T09:08:34.64496Z","shell.execute_reply":"2024-08-02T09:08:37.196582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:08:37.199256Z","iopub.execute_input":"2024-08-02T09:08:37.19986Z","iopub.status.idle":"2024-08-02T09:08:37.205323Z","shell.execute_reply.started":"2024-08-02T09:08:37.199825Z","shell.execute_reply":"2024-08-02T09:08:37.204358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(epochs):\n    print(f\"Epoch {epoch + 1}/{epochs}\")\n    running_loss = 0\n    total_train = 0\n    correct_train = 0\n    # Train\n    model.train()\n    for batch in tqdm(train_loader):\n        prompts, responses_a, responses_b, labels = batch\n        labels = labels.to(device)\n\n        outputs = model(prompts, responses_a, responses_b)\n        _, predicted_idx = torch.max(outputs.data, 1)\n\n        loss = criterion(outputs, labels)\n        optimizer.zero_grad()\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        total_train += labels.size(0)\n        correct_train += (predicted_idx == labels).sum().item()\n\n        del labels, outputs\n        \n    train_accuracy = 100 * correct_train / total_train\n    print(f\"\\nTraining Loss: {running_loss/len(train_loader):.4f} | Training Accuracy: {train_accuracy:.2f}%\")\nprint(\"\\n==> Training finished!\")","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-08-02T09:08:37.206464Z","iopub.execute_input":"2024-08-02T09:08:37.206796Z","iopub.status.idle":"2024-08-02T09:09:12.449928Z","shell.execute_reply.started":"2024-08-02T09:08:37.206766Z","shell.execute_reply":"2024-08-02T09:09:12.448103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test(model, test_loader, device):\n    model.eval() \n    results = [] \n    with torch.no_grad(): \n        for batch in tqdm(test_loader):\n            ids, prompts, responses_a, responses_b = batch\n            outputs = model(prompts, responses_a, responses_b)\n            _, predicted_idx = torch.max(outputs.data, 1)\n            \n            for idx, output, prediction in zip(ids, outputs, predicted_idx):\n                results.append({\n                    'id': idx.item(),\n                    'winner_model_a': output[0].item(),\n                    'winner_model_b': output[1].item(),\n                    'winner_tie': output[2].item()\n                })\n    df_results = pd.DataFrame(results)\n    return df_results\n\ndf_results = test(model, test_loader, device)\ndf_results","metadata":{"execution":{"iopub.status.busy":"2024-08-02T09:08:05.639993Z","iopub.execute_input":"2024-08-02T09:08:05.640383Z","iopub.status.idle":"2024-08-02T09:08:05.854033Z","shell.execute_reply.started":"2024-08-02T09:08:05.640353Z","shell.execute_reply":"2024-08-02T09:08:05.853063Z"},"trusted":true},"execution_count":null,"outputs":[]}]}