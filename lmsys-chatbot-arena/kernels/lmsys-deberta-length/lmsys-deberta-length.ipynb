{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ac4669",
   "metadata": {},
   "source": [
    "# Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:24.696323Z",
     "iopub.status.busy": "2024-07-06T08:18:24.695656Z",
     "iopub.status.idle": "2024-07-06T08:18:38.603387Z",
     "shell.execute_reply": "2024-07-06T08:18:38.602609Z",
     "shell.execute_reply.started": "2024-07-06T08:18:24.696269Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"  # or \"jax\" or \"torch\"\n",
    "import re\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b5bdad",
   "metadata": {},
   "source": [
    "# Num GPUs Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:38.605512Z",
     "iopub.status.busy": "2024-07-06T08:18:38.604965Z",
     "iopub.status.idle": "2024-07-06T08:18:39.274199Z",
     "shell.execute_reply": "2024-07-06T08:18:39.272847Z",
     "shell.execute_reply.started": "2024-07-06T08:18:38.605485Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432d3312",
   "metadata": {},
   "source": [
    "# TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.275981Z",
     "iopub.status.busy": "2024-07-06T08:18:39.275537Z",
     "iopub.status.idle": "2024-07-06T08:18:39.296311Z",
     "shell.execute_reply": "2024-07-06T08:18:39.295509Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.275939Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Detect hardware, return appropriate distribution strategy\n",
    "# try:\n",
    "#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "#     print('Running on TPU ', tpu.master())\n",
    "# except ValueError:\n",
    "#     tpu = None\n",
    "\n",
    "# if tpu:\n",
    "#     tf.config.experimental_connect_to_cluster(tpu)\n",
    "#     tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "#     strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "# else:\n",
    "#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "# print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa5920",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.299068Z",
     "iopub.status.busy": "2024-07-06T08:18:39.298649Z",
     "iopub.status.idle": "2024-07-06T08:18:39.307362Z",
     "shell.execute_reply": "2024-07-06T08:18:39.306565Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.299039Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"deberta_v3_extra_small_en\"\n",
    "    sequence_length = 512\n",
    "    epochs = 6\n",
    "    batch_size = 16\n",
    "#     batch_size = 16 * strategy.num_replicas_in_sync\n",
    "    scheduler = 'cosine'  # Learning rate scheduler\n",
    "    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n",
    "    name2label = {v:k for k, v in label2name.items()}\n",
    "    class_labels = list(label2name.keys())\n",
    "    class_names = list(label2name.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64909c3b",
   "metadata": {},
   "source": [
    "# Reproducibility \n",
    "设置随机种子的值以在每次运行中产生类似的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.308477Z",
     "iopub.status.busy": "2024-07-06T08:18:39.308223Z",
     "iopub.status.idle": "2024-07-06T08:18:39.317829Z",
     "shell.execute_reply": "2024-07-06T08:18:39.316936Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.308456Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e75a189",
   "metadata": {},
   "source": [
    "#  Mixed Precision\n",
    "\n",
    "在本笔记中，我们将使用混合精度而不是 float32 精度进行训练和推理，以减少 GPU 内存使用量。这最终将使我们能够使用更大的批量大小，从而减少我们的训练和推理时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.319115Z",
     "iopub.status.busy": "2024-07-06T08:18:39.318845Z",
     "iopub.status.idle": "2024-07-06T08:18:39.328914Z",
     "shell.execute_reply": "2024-07-06T08:18:39.328119Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.319092Z"
    }
   },
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")\n",
    "#在mixed_float16策略下，模型的某些部分会自动使用float16进行计算，而其他部分（如损失函数的计算）则可能仍然使用float32以保持稳定性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e1f29",
   "metadata": {},
   "source": [
    "# Dataset Path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.330581Z",
     "iopub.status.busy": "2024-07-06T08:18:39.329996Z",
     "iopub.status.idle": "2024-07-06T08:18:39.339076Z",
     "shell.execute_reply": "2024-07-06T08:18:39.338163Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.330549Z"
    }
   },
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19fd39b4",
   "metadata": {},
   "source": [
    "# Meta Data \n",
    "## Files\n",
    "\n",
    "### `train.csv`\n",
    "- `id`: Unique identifier for each row.\n",
    "- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.\n",
    "- `prompt`: Input prompt given to both models.\n",
    "- `response_[a/b]`: Model_[a/b]'s response to the prompt.\n",
    "- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).\n",
    "\n",
    "### `test.csv`\n",
    "- `id`: Unique identifier for each row.\n",
    "- `prompt`: Input prompt given to both models.\n",
    "- `response_[a/b]`: Model_[a/b]'s response to the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:39.340799Z",
     "iopub.status.busy": "2024-07-06T08:18:39.340245Z",
     "iopub.status.idle": "2024-07-06T08:18:46.525334Z",
     "shell.execute_reply": "2024-07-06T08:18:46.524439Z",
     "shell.execute_reply.started": "2024-07-06T08:18:39.340768Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Train Data\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv') \n",
    "ultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')\n",
    "df = pd.concat([df, ultrachat_df], axis=0)\n",
    "lmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')\n",
    "df = pd.concat([df, lmsys_33k_deduplicated], axis=0)\n",
    "# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')\n",
    "# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'[\"{x}\"]')\n",
    "# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)\n",
    "\n",
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "\n",
    "# display(ultrafeedback_lmsysformat.head())\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:46.526829Z",
     "iopub.status.busy": "2024-07-06T08:18:46.526529Z",
     "iopub.status.idle": "2024-07-06T08:18:55.668662Z",
     "shell.execute_reply": "2024-07-06T08:18:55.667798Z",
     "shell.execute_reply.started": "2024-07-06T08:18:46.526804Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.drop(\"id\", axis=1)\n",
    "df = df.drop_duplicates(keep=\"first\", ignore_index=True)\n",
    "\n",
    "for col in [\"prompt\"]:\n",
    "    df[col] = df[col].apply(lambda x: eval(x))\n",
    "    test_df[col] = test_df[col].apply(lambda x: eval(x))\n",
    "for col in [\"response_a\", \"response_b\"]:\n",
    "    df[col] = df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n",
    "    test_df[col] = test_df[col].apply(lambda x: eval(x.replace(\"null\", \"None\")))\n",
    "    \n",
    "# Sample data\n",
    "# df = df.sample(frac=0.01)\n",
    "\n",
    "# Label conversion\n",
    "df[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\n",
    "df[\"class_label\"] = df.class_name.map(CFG.name2label)\n",
    "\n",
    "# Show Sample\n",
    "display(df.head())\n",
    "# Show Sample\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a208bef",
   "metadata": {},
   "source": [
    "## Contextualize Response with Prompt\n",
    "\n",
    "在我们的方法中，我们将根据提示对每个回答进行情境化，而不是对所有回答使用单一提示。这意味着，对于每个回答，我们将为模型提供同一组提示及其各自的回答（例如，“(P + R_A)”，“(P + R_B)”等）。\n",
    "\n",
    "> 某些提示和响应可能未使用 `utf-8` 编码，导致创建数据加载器时出错。在这种情况下，我们将用空字符串替换它们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:55.67367Z",
     "iopub.status.busy": "2024-07-06T08:18:55.673373Z",
     "iopub.status.idle": "2024-07-06T08:18:55.683287Z",
     "shell.execute_reply": "2024-07-06T08:18:55.682351Z",
     "shell.execute_reply.started": "2024-07-06T08:18:55.673644Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_pairs(row):\n",
    "    row['options'] = []\n",
    "    row[\"encode_fail\"] = False\n",
    "\n",
    "    try:\n",
    "        # 确保所有需要的键都存在于row字典中\n",
    "        prompts = row['prompt']\n",
    "        responses_a = row['response_a']\n",
    "        responses_b = row['response_b']\n",
    "        \n",
    "        # 检查列表长度是否匹配\n",
    "        if not (len(prompts) == len(responses_a) == len(responses_b)):\n",
    "            raise ValueError(\"The lists 'prompt', 'response_a', and 'response_b' must be of the same length.\")\n",
    "            \n",
    "        response_a_str = ''\n",
    "        response_b_str = ''\n",
    "        \n",
    "        for idx in range(len(prompts)):\n",
    "            response_a_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_a[idx]}\"\n",
    "            response_b_str += f\"Prompt: {prompts[idx]}\\n\\nResponse: {responses_b[idx]}\"\n",
    "        \n",
    "        # 文本清洗，例如去除无法识别的Unicode字符或替换它们\n",
    "        clean_response_a_str = \"\".join(filter(lambda x: ord(x) < 128, response_a_str))\n",
    "        clean_response_b_str = \"\".join(filter(lambda x: ord(x) < 128, response_b_str))\n",
    "        \n",
    "        row['options'].append(clean_response_a_str)\n",
    "        row['options'].append(clean_response_b_str)\n",
    "        \n",
    "    except KeyError as e:\n",
    "        print(f\"Missing key in row: {e}\")\n",
    "        row[\"encode_fail\"] = True\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        row[\"encode_fail\"] = True\n",
    "    except Exception as e:\n",
    "        # 捕获其他所有异常\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "        row[\"encode_fail\"] = True\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T08:18:55.68493Z",
     "iopub.status.busy": "2024-07-06T08:18:55.684564Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.apply(make_pairs, axis=1)\n",
    "display(df.head(2))\n",
    "\n",
    "test_df = test_df.apply(make_pairs, axis=1)\n",
    "display(test_df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075d9084",
   "metadata": {},
   "source": [
    "## Encoding Fail Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.encode_fail.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea074f8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataFrameStatsProcessor:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def _is_empty(self, string: str) -> bool:\n",
    "        return bool(re.match(\"^\\s*$\", string))\n",
    "\n",
    "    def _len(self, string: str) -> int:\n",
    "        if string is None:\n",
    "            return 0\n",
    "        return len(string)\n",
    "\n",
    "    def _add_len_stats(self, col: str) -> pd.DataFrame:\n",
    "        if col == \"prompt\":\n",
    "            col_prefix = \"p_len\"\n",
    "        elif col == \"response_a\":\n",
    "            col_prefix = \"res_a_len\"\n",
    "        elif col == \"response_b\":\n",
    "            col_prefix = \"res_b_len\"\n",
    "        \n",
    "        self.df[f\"{col_prefix}_sum\"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))\n",
    "        self.df[f\"{col_prefix}_mean\"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))\n",
    "        self.df[f\"{col_prefix}_max\"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))\n",
    "        self.df[f\"{col_prefix}_sum_log\"] = np.log1p(self.df[f\"{col_prefix}_sum\"])\n",
    "        self.df[f\"{col_prefix}_mean_log\"] =  np.log1p(self.df[f\"{col_prefix}_mean\"])\n",
    "        self.df[f\"{col_prefix}_max_log\"] = np.log1p(self.df[f\"{col_prefix}_max\"])\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def z_score_normalize(self, columns):\n",
    "        \"\"\"\n",
    "        对指定的列进行Z得分归一化。\n",
    "        参数:\n",
    "            columns (list): 需要进行Z得分归一化的列名列表。\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()\n",
    "    \n",
    "    def process_dataframe(self):\n",
    "        self.df[\"n_prompts\"] = self.df[\"prompt\"].apply(lambda x: len(x))\n",
    "        self.df[\"n_res_a\"] = self.df[\"response_a\"].apply(lambda x: len(x))\n",
    "        self.df[\"n_res_b\"] = self.df[\"response_b\"].apply(lambda x: len(x))\n",
    "        assert ((self.df[\"n_prompts\"] == self.df[\"n_res_a\"]) & (self.df[\"n_prompts\"] == self.df[\"n_res_b\"])).all()\n",
    "\n",
    "        self.df[\"n_na_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n",
    "        self.df[\"n_empty_prompts\"] = self.df[\"prompt\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n",
    "        self.df[\"n_na_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n",
    "        self.df[\"n_empty_res_a\"] = self.df[\"response_a\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n",
    "        self.df[\"n_na_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))\n",
    "        self.df[\"n_empty_res_b\"] = self.df[\"response_b\"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))\n",
    "\n",
    "        self.df[\"n_miss_res_a\"] = self.df[\"n_na_res_a\"] + self.df[\"n_empty_res_a\"]\n",
    "        self.df[\"n_miss_res_b\"] = self.df[\"n_na_res_b\"] + self.df[\"n_empty_res_b\"]\n",
    "\n",
    "        self.df[\"n_eff_res_a\"] = self.df[\"n_res_a\"] - self.df[\"n_miss_res_a\"]\n",
    "        self.df[\"n_eff_res_b\"] = self.df[\"n_res_b\"] - self.df[\"n_miss_res_b\"]\n",
    "\n",
    "        self._add_len_stats(\"prompt\")\n",
    "        self._add_len_stats(\"response_a\")\n",
    "        self._add_len_stats(\"response_b\")\n",
    "\n",
    "        self.df[\"res_len_mean_diff\"] = self.df[\"res_a_len_mean\"] - self.df[\"res_b_len_mean\"]\n",
    "        self.df[\"res_len_mean_diff_clip\"] = self.df[\"res_len_mean_diff\"].clip(-6000, 6000)\n",
    "\n",
    "        self.df[\"n_miss_prompts\"] = self.df[\"n_na_prompts\"] + self.df[\"n_empty_prompts\"]\n",
    "        self.df[\"n_eff_prompts\"] = self.df[\"n_prompts\"] - self.df[\"n_miss_prompts\"]\n",
    "\n",
    "        self.df[\"na_prompt_ratio\"] = self.df[\"n_na_prompts\"] / self.df[\"n_prompts\"]\n",
    "        self.df[\"empty_prompt_ratio\"] = self.df[\"n_empty_prompts\"] / self.df[\"n_prompts\"]\n",
    "        self.df[\"miss_prompt_ratio\"] = self.df[\"n_miss_prompts\"] / self.df[\"n_prompts\"]\n",
    "\n",
    "        self.df[\"na_res_a_ratio\"] = self.df[\"n_na_res_a\"] / self.df[\"n_res_a\"]\n",
    "        self.df[\"empty_res_a_ratio\"] = self.df[\"n_empty_res_a\"] / self.df[\"n_res_a\"]\n",
    "        self.df[\"miss_res_a_ratio\"] = self.df[\"n_miss_res_a\"] / self.df[\"n_res_a\"]\n",
    "        self.df[\"na_res_b_ratio\"] = self.df[\"n_na_res_b\"] / self.df[\"n_res_b\"]\n",
    "        self.df[\"empty_res_b_ratio\"] = self.df[\"n_empty_res_b\"] / self.df[\"n_res_b\"]\n",
    "        self.df[\"miss_res_b_ratio\"] = self.df[\"n_miss_res_b\"] / self.df[\"n_res_b\"]\n",
    "\n",
    "        for col, col_prefix in zip([\"prompt\", \"response_a\", \"response_b\"], [\"p_len\", \"res_a_len\", \"res_b_len\"]):\n",
    "            self.df[f\"{col_prefix}_med\"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))\n",
    "            self.df[f\"{col_prefix}_std\"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))\n",
    "\n",
    "        self.df[\"p_len_eff_mean\"] = self.df[\"p_len_sum\"] / self.df[\"n_eff_prompts\"]\n",
    "        self.df[\"res_a_len_eff_mean\"] = self.df[\"res_a_len_sum\"] / self.df[\"n_eff_res_a\"]\n",
    "        self.df[\"res_b_len_eff_mean\"] = self.df[\"res_b_len_sum\"] / self.df[\"n_eff_res_b\"]\n",
    "\n",
    "        for stats in [\"sum\", \"mean\", \"max\", \"med\", \"eff_mean\"]:\n",
    "            self.df[f\"p_a_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_a_len_{stats}\"]\n",
    "            self.df[f\"p_b_{stats}_diff\"] = self.df[f\"p_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n",
    "            self.df[f\"a_b_{stats}_diff\"] = self.df[f\"res_a_len_{stats}\"] - self.df[f\"res_b_len_{stats}\"]\n",
    "            \n",
    "        len_feature_a_col = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n",
    "                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n",
    "                     \"p_a_eff_mean_diff\"]\n",
    "        \n",
    "        len_feature_b_col = [\"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n",
    "                             \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n",
    "                             \"p_b_eff_mean_diff\"]\n",
    "        \n",
    "        numerical_feature_columns = [\"res_a_len_sum\",\"res_a_len_mean\",\"res_a_len_max\",\"res_a_len_sum_log\",\"res_a_len_mean_log\",\"res_a_len_max_log\",\n",
    "                                     \"res_a_len_med\",\"res_a_len_std\",\"res_a_len_eff_mean\",\"p_a_sum_diff\",\"p_a_mean_diff\",\"p_a_max_diff\",\"p_a_med_diff\",\n",
    "                                     \"p_a_eff_mean_diff\", \"res_b_len_sum\",\"res_b_len_mean\",\"res_b_len_max\",\"res_b_len_sum_log\",\"res_b_len_mean_log\",\"res_b_len_max_log\",\n",
    "                                     \"res_b_len_med\",\"res_b_len_std\",\"res_b_len_eff_mean\",\"p_b_sum_diff\",\"p_b_mean_diff\",\"p_b_max_diff\",\"p_b_med_diff\",\n",
    "                                     \"p_b_eff_mean_diff\"]\n",
    "        # 确保不除以零进行归一化\n",
    "        for col in numerical_feature_columns:\n",
    "            if self.df[col].std() == 0:\n",
    "                print(f\"Warning: Standard deviation is zero for column {col}. Skipping normalization.\")\n",
    "            else:\n",
    "                self.z_score_normalize([col])\n",
    "                \n",
    "        self.df = self.df.fillna(0)\n",
    "        \n",
    "        # 选择这些列并将它们转换为列表\n",
    "        len_features_a = self.df[len_feature_a_col].values.tolist()\n",
    "        len_features_b = self.df[len_feature_b_col].values.tolist()\n",
    "\n",
    "        return len_features_a, len_features_b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e001db",
   "metadata": {},
   "source": [
    "# Data Split\n",
    "\n",
    "在下面提供的代码片段中，我们将使用class_label列的分层将现有数据分为训练和验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import package\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed54a2",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset, \n",
    "    sequence_length=CFG.sequence_length, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(text, label=None, features_a=None, features_b=None):\n",
    "    text = preprocessor(text)\n",
    "    if features_a is not None:\n",
    "        text['features_a'] = features_a\n",
    "    if features_b is not None:\n",
    "         text['features_b'] = features_b\n",
    "    return (text, label) if label is not None else text  # Return processed text and label if available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd68c79",
   "metadata": {},
   "source": [
    "# FGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 添加 FGM 扰动函数\n",
    "# def fgm_perturb(features, epsilon=1.0):\n",
    "#     # 计算扰动量，epsilon 为扰动比例\n",
    "#     perturbation = np.random.uniform(-1, 1, features.shape) * epsilon\n",
    "#     # 应用扰动\n",
    "#     return features + perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 修改数据预处理函数以包含 FGM 扰动\n",
    "# def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):\n",
    "#     # 预处理文本\n",
    "#     text = preprocessor(text)\n",
    "#     if features_a is not None:\n",
    "#         if is_fgm:\n",
    "#             # 如果是 FGM，应用扰动\n",
    "#             features_a = fgm_perturb(features_a, epsilon)\n",
    "#         text['features_a'] = features_a\n",
    "#     if features_b is not None:\n",
    "#         if is_fgm:\n",
    "#             # 如果是 FGM，应用扰动\n",
    "#             features_b = fgm_perturb(features_b, epsilon)\n",
    "#         text['features_b'] = features_b\n",
    "#     return (text, label) if label is not None else text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9a7dbc",
   "metadata": {},
   "source": [
    "# AWP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义 AWP 扰动函数\n",
    "def awp_perturb(model, epsilon=1e-4):\n",
    "    for layer in model.layers:\n",
    "        if hasattr(layer, 'kernel'):\n",
    "            # 获取权重\n",
    "            weights = layer.kernel\n",
    "            # 计算扰动\n",
    "            perturbation = tf.random.normal(weights.shape, stddev=epsilon)\n",
    "            # 应用扰动\n",
    "            layer.kernel.assign_add(perturbation)\n",
    "\n",
    "#创建 AWP 回调函数\n",
    "class AWPCallback(keras.callbacks.Callback):\n",
    "    def __init__(self, epsilon):\n",
    "        super(AWPCallback, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        # 在每个批次开始时应用 AWP 扰动\n",
    "        awp_perturb(self.model, self.epsilon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b03ad44",
   "metadata": {},
   "source": [
    "# DataLoader\n",
    "\n",
    "下面的代码使用tf.data.Dataset为数据处理设置了一个健壮的数据流管道。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,\n",
    "                                cache=True, shuffle=1024):\n",
    "    AUTO = tf.data.AUTOTUNE\n",
    "    if (features_a is not None) and (features_b is not None):\n",
    "        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # Create slices\n",
    "    else:\n",
    "        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)\n",
    "    ds = ds.cache() if cache else ds\n",
    "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)\n",
    "#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),\n",
    "#                 num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    opt = tf.data.Options()\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)\n",
    "        opt.experimental_deterministic = False\n",
    "    ds = ds.with_options(opt)\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)\n",
    "    ds = ds.prefetch(AUTO)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa4066b",
   "metadata": {},
   "source": [
    "## Build Train/Valid Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_processor = DataFrameStatsProcessor(train_df.copy())\n",
    "train_features_a, train_features_b = train_features_processor.process_dataframe()\n",
    "valid_features_processor = DataFrameStatsProcessor(valid_df.copy())\n",
    "valid_features_a, valid_features_b = valid_features_processor.process_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false
   },
   "outputs": [],
   "source": [
    "# # Train\n",
    "train_texts = train_df.options.tolist()  \n",
    "train_labels = train_df.class_label.tolist() \n",
    "train_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, \n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=True)\n",
    "# # Valid\n",
    "valid_texts = valid_df.options.tolist()  \n",
    "valid_labels = valid_df.class_label.tolist() \n",
    "valid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, \n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False)\n",
    "print(train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893b7e0",
   "metadata": {},
   "source": [
    "# LR Schedule\n",
    "\n",
    "实施学习率调度程序对于迁移学习至关重要。\n",
    "\n",
    "学习率从 lr_start 开始，然后使用各种技术逐渐减小到 lr_min，包括：\n",
    "\n",
    "- step：以类似楼梯的方式逐步降低学习率。\n",
    "- cos：利用余弦曲线逐渐降低学习率。\n",
    "- exp：以指数方式降低学习率。\n",
    "\n",
    "**重要性**：结构良好的学习率调度对于有效的模型训练至关重要，可确保最佳收敛并避免诸如过冲或停滞等问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10):\n",
    "    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "    \n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80186ba6",
   "metadata": {},
   "source": [
    "# Model Checkpointing\n",
    "\n",
    "下面的代码将创建一个回调，在训练期间保存模型的最佳检查点，我们将在提交时使用它进行推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',\n",
    "                                          monitor='val_log_loss',\n",
    "                                          save_best_only=True,\n",
    "                                          save_weights_only=True,\n",
    "                                          mode='min')  # Get Model checkpoint callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbef402a",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "这次比赛的指标是对数损失。这个度量可以用数学表示为：\n",
    "\n",
    "$$\n",
    "\\text{Log Loss} = -\\frac{1}{N} \\sum_{i=1}^{N} \\left( y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\", label_smoothing=0.1, from_logits=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf286f2",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.layers import Dropout\n",
    "\n",
    "with strategy.scope():\n",
    "\n",
    "    # 将所有输入层整合到一个字典中\n",
    "    inputs = {\n",
    "        \"token_ids\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n",
    "        \"padding_mask\": keras.layers.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n",
    "        \"features_a\": keras.layers.Input(shape=(14,), name=\"features_a\", dtype=tf.float32),\n",
    "        \"features_b\": keras.layers.Input(shape=(14,), name=\"features_b\", dtype=tf.float32),\n",
    "    }\n",
    "    \n",
    "    # Create a DebertaV3Classifier backbone\n",
    "    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n",
    "        CFG.preset,\n",
    "    )\n",
    "\n",
    "   # 修改 response_a 和 response_b 的创建方式，包含 padding_mask\n",
    "    response_a = {\n",
    "        \"token_ids\": inputs[\"token_ids\"][:, 0, :],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"][:, 0, :]\n",
    "    }\n",
    "    embed_a = backbone(response_a)\n",
    "\n",
    "    response_b = {\n",
    "        \"token_ids\": inputs[\"token_ids\"][:, 1, :],\n",
    "        \"padding_mask\": inputs[\"padding_mask\"][:, 1, :]\n",
    "    }\n",
    "    embed_b = backbone(response_b)\n",
    "    \n",
    "    # 将数值特征嵌入\n",
    "    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_a\"])\n",
    "    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs[\"features_b\"])\n",
    "    \n",
    "    # 使用 Flatten 层将数值特征嵌入展平为二维张量\n",
    "    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)\n",
    "    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)\n",
    "    \n",
    "    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)\n",
    "    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)\n",
    "    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])\n",
    "    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])\n",
    "    \n",
    "    # 合并文本嵌入和数值特征嵌入\n",
    "    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_a])\n",
    "    \n",
    "    # 添加L2正则化和Dropout到模型中\n",
    "    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2正则化\n",
    "    combined_embeds = Dropout(0.05)(combined_embeds)  # Dropout层，丢弃5%的神经元\n",
    "    \n",
    "    # 定义 temperature_scale 函数\n",
    "    def temperature_scale(logits, T=1.0):\n",
    "        return logits / T\n",
    "    \n",
    "    # 定义温度参数 T\n",
    "    T = 0.85\n",
    "    # 应用温度缩放\n",
    "    scaled_logits = temperature_scale(combined_embeds, T)\n",
    "    outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(scaled_logits)\n",
    "    \n",
    "    model = keras.Model(inputs,  outputs)\n",
    "    \n",
    "    # Compile the model with optimizer, loss, and metrics\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),\n",
    "        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),\n",
    "        metrics=[\n",
    "            log_loss,\n",
    "            keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    # 添加 AWP 回调到模型训练中\n",
    "    awp_cb = AWPCallback(epsilon=1e-4)  # 您可以根据需要调整 epsilon 的值"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6658a4fe",
   "metadata": {},
   "source": [
    "### Model Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b56cd3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "# try:\n",
    "#     history = model.fit(\n",
    "#         train_ds,\n",
    "#         epochs=CFG.epochs,\n",
    "#         validation_data=valid_ds,\n",
    "#         callbacks=[lr_cb, ckpt_cb]\n",
    "#     )\n",
    "# except tf.errors.InvalidArgumentError as e:\n",
    "#     print(f\"出现无效参数错误：{e}\")\n",
    "try:\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        epochs=CFG.epochs,\n",
    "        validation_data=valid_ds,\n",
    "        callbacks=[lr_cb, ckpt_cb, awp_cb]  # 将 AWP 回调添加到训练回调列表中\n",
    "    )\n",
    "except tf.errors.InvalidArgumentError as e:\n",
    "    print(f\"出现无效参数错误：{e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb8606",
   "metadata": {},
   "source": [
    "## Load Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/kaggle/working/best_model.weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070811c7",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 使用 FGM 扰动的数据集评估模型\n",
    "# fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,\n",
    "#                                      is_fgm=True, epsilon=1.0)\n",
    "# evaluation_results = model.evaluate(fgm_ds)\n",
    "\n",
    "# print(f\"Evaluation results on FGM perturbed dataset: {evaluation_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_features_processor = DataFrameStatsProcessor(test_df)\n",
    "test_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = test_df.options.tolist()\n",
    "test_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,\n",
    "                         batch_size=min(len(test_df), CFG.batch_size),\n",
    "                         shuffle=False)\n",
    "print(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4e9062",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test_df[[\"id\"]].copy()\n",
    "sub_df[CFG.class_names] = test_preds.tolist()\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5306138,
     "sourceId": 8820093,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5310688,
     "sourceId": 8826860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5310716,
     "sourceId": 8826894,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 4684,
     "sourceId": 6063,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
