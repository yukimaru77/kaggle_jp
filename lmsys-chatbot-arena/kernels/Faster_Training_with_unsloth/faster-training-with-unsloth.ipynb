{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8892139,"sourceType":"datasetVersion","datasetId":5347743},{"sourceId":8892571,"sourceType":"datasetVersion","datasetId":5347939},{"sourceId":8912614,"sourceType":"datasetVersion","datasetId":5348748},{"sourceId":8914163,"sourceType":"datasetVersion","datasetId":5360500},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":172868266,"sourceType":"kernelVersion"}],"dockerImageVersionId":30733,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/accelerate-0.29.3-py3-none-any.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/fsspec-2024.3.1-py3-none-any.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/unsloth-2024.4-py3-none-any.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/peft-0.10.0-py3-none-any.whl'\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:25:59.08682Z","iopub.execute_input":"2024-07-10T10:25:59.087504Z","iopub.status.idle":"2024-07-10T10:29:55.319775Z","shell.execute_reply.started":"2024-07-10T10:25:59.087468Z","shell.execute_reply":"2024-07-10T10:29:55.318764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl'\n##!pip install '/kaggle/input/llm-detect-pip/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/llm-detect-pip/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'\n!pip install '/kaggle/input/nvidia-nccl-2-19-3/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl'\n#!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:29:55.322203Z","iopub.execute_input":"2024-07-10T10:29:55.323037Z","iopub.status.idle":"2024-07-10T10:37:14.893909Z","shell.execute_reply.started":"2024-07-10T10:29:55.322997Z","shell.execute_reply":"2024-07-10T10:37:14.892806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/shtab-1.7.1-py3-none-any.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/tyro-0.8.3-py3-none-any.whl'\n!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/trl-0.8.5-py3-none-any.whl'","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:37:14.895257Z","iopub.execute_input":"2024-07-10T10:37:14.895594Z","iopub.status.idle":"2024-07-10T10:38:53.247798Z","shell.execute_reply.started":"2024-07-10T10:37:14.895564Z","shell.execute_reply":"2024-07-10T10:38:53.246889Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:38:53.250772Z","iopub.execute_input":"2024-07-10T10:38:53.251145Z","iopub.status.idle":"2024-07-10T10:39:30.246471Z","shell.execute_reply.started":"2024-07-10T10:38:53.251106Z","shell.execute_reply":"2024-07-10T10:39:30.245554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:39:30.247776Z","iopub.execute_input":"2024-07-10T10:39:30.248056Z","iopub.status.idle":"2024-07-10T10:40:03.138693Z","shell.execute_reply.started":"2024-07-10T10:39:30.248028Z","shell.execute_reply":"2024-07-10T10:40:03.137614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl'","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:40:03.140186Z","iopub.execute_input":"2024-07-10T10:40:03.14058Z","iopub.status.idle":"2024-07-10T10:41:12.858359Z","shell.execute_reply.started":"2024-07-10T10:40:03.140542Z","shell.execute_reply":"2024-07-10T10:41:12.857284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl'","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:12.859899Z","iopub.execute_input":"2024-07-10T10:41:12.860576Z","iopub.status.idle":"2024-07-10T10:41:50.850618Z","shell.execute_reply.started":"2024-07-10T10:41:12.860537Z","shell.execute_reply":"2024-07-10T10:41:50.849532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nprint(f\"pytorch version {torch.__version__}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:50.852066Z","iopub.execute_input":"2024-07-10T10:41:50.852366Z","iopub.status.idle":"2024-07-10T10:41:52.738932Z","shell.execute_reply.started":"2024-07-10T10:41:50.852331Z","shell.execute_reply":"2024-07-10T10:41:52.738033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from unsloth import FastLanguageModel","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:52.739965Z","iopub.execute_input":"2024-07-10T10:41:52.74036Z","iopub.status.idle":"2024-07-10T10:41:56.530285Z","shell.execute_reply.started":"2024-07-10T10:41:52.740306Z","shell.execute_reply":"2024-07-10T10:41:56.529348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:56.53473Z","iopub.execute_input":"2024-07-10T10:41:56.53512Z","iopub.status.idle":"2024-07-10T10:41:56.539576Z","shell.execute_reply.started":"2024-07-10T10:41:56.535094Z","shell.execute_reply":"2024-07-10T10:41:56.538575Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:56.540819Z","iopub.execute_input":"2024-07-10T10:41:56.541155Z","iopub.status.idle":"2024-07-10T10:41:56.856077Z","shell.execute_reply.started":"2024-07-10T10:41:56.541121Z","shell.execute_reply":"2024-07-10T10:41:56.855101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom peft import PeftModel\nfrom trl import SFTTrainer\nfrom trl import setup_chat_format\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:41:56.857161Z","iopub.execute_input":"2024-07-10T10:41:56.857585Z","iopub.status.idle":"2024-07-10T10:42:08.764902Z","shell.execute_reply.started":"2024-07-10T10:41:56.85756Z","shell.execute_reply":"2024-07-10T10:42:08.764014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:08.766101Z","iopub.execute_input":"2024-07-10T10:42:08.766822Z","iopub.status.idle":"2024-07-10T10:42:08.782489Z","shell.execute_reply.started":"2024-07-10T10:42:08.766786Z","shell.execute_reply":"2024-07-10T10:42:08.781038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_test_prompt(data_point):\n    return f\"\"\"Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred \nresponse based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.\n\nPrompt: {data_point[\"prompt\"]}\nModel A Response: {data_point[\"response_a\"]}\nModel B Response: {data_point[\"response_b\"]}\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:08.783797Z","iopub.execute_input":"2024-07-10T10:42:08.784749Z","iopub.status.idle":"2024-07-10T10:42:08.793324Z","shell.execute_reply.started":"2024-07-10T10:42:08.784703Z","shell.execute_reply":"2024-07-10T10:42:08.792421Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nMODEL_PATH = \"/kaggle/input/llama-3/transformers/8b-hf/1\"\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=True,\n)\n\nmodel1 = AutoModelForCausalLM.from_pretrained(\n    MODEL_PATH,\n    device_map = \"auto\",\n    trust_remote_code = True,\n    quantization_config=quantization_config,\n)\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=torch.float16)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:08.794471Z","iopub.execute_input":"2024-07-10T10:42:08.794786Z","iopub.status.idle":"2024-07-10T10:42:08.806665Z","shell.execute_reply.started":"2024-07-10T10:42:08.794758Z","shell.execute_reply":"2024-07-10T10:42:08.805714Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\ndtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\nload_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n\n# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\nfourbit_models = [\n    \"unsloth/mistral-7b-bnb-4bit\",\n\n    \"unsloth/llama-2-7b-bnb-4bit\",\n    \"unsloth/llama-2-13b-bnb-4bit\",\n    \"unsloth/codellama-34b-bnb-4bit\",\n    \"unsloth/tinyllama-bnb-4bit\",\n    \"unsloth/llama-3-8b-bnb-4bit\"\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"/kaggle/input/unslothmistral-7b-instruct-v0-2-bnb-4bit/unsloth-mistral-7b-instruct-v0.2-bnb-4bit\", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B\n    max_seq_length = max_seq_length,\n    dtype = dtype,\n    load_in_4bit = load_in_4bit,\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:08.80798Z","iopub.execute_input":"2024-07-10T10:42:08.808893Z","iopub.status.idle":"2024-07-10T10:42:55.88766Z","shell.execute_reply.started":"2024-07-10T10:42:08.80884Z","shell.execute_reply":"2024-07-10T10:42:55.886851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 16,\n    lora_dropout = 0, # Supports any, but = 0 is optimized\n    bias = \"none\",    # Supports any, but = \"none\" is optimized\n    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n    random_state = 3407,\n    use_rslora = False,  # We support rank stabilized LoRA\n    loftq_config = None, # And LoftQ\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:55.888875Z","iopub.execute_input":"2024-07-10T10:42:55.889224Z","iopub.status.idle":"2024-07-10T10:42:56.74953Z","shell.execute_reply.started":"2024-07-10T10:42:55.889194Z","shell.execute_reply":"2024-07-10T10:42:56.748684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'))\ntest = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))\ntrain = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:56.750832Z","iopub.execute_input":"2024-07-10T10:42:56.75117Z","iopub.status.idle":"2024-07-10T10:42:59.675322Z","shell.execute_reply.started":"2024-07-10T10:42:56.751144Z","shell.execute_reply":"2024-07-10T10:42:59.674548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result = []\nfor index, row in train.iterrows():\n    if(row['winner_model_a']==1): result.append('winner_model_a')\n    elif(row['winner_model_b']==1): result.append('winner_model_b')\n    elif(row['winner_tie']==1): result.append('winner_tie')\ntrain.insert(6, \"Preference\", result, True)\ntrain","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:42:59.676385Z","iopub.execute_input":"2024-07-10T10:42:59.676648Z","iopub.status.idle":"2024-07-10T10:43:02.931949Z","shell.execute_reply.started":"2024-07-10T10:42:59.676627Z","shell.execute_reply":"2024-07-10T10:43:02.930952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[:1000]\ntrain.shape","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:02.933113Z","iopub.execute_input":"2024-07-10T10:43:02.933408Z","iopub.status.idle":"2024-07-10T10:43:02.941456Z","shell.execute_reply.started":"2024-07-10T10:43:02.933384Z","shell.execute_reply":"2024-07-10T10:43:02.940541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(train.shape[0]*0.6)\ntest_val_size = train.shape[0]-train_size\ntrain_, test_val  = train_test_split(train, train_size=train_size,\n                                    test_size=test_val_size,\n                                    random_state=42)\n\ntest_, val_ = train_test_split(test_val, train_size=int(test_val_size/2),\n                                    test_size=int(test_val_size/2),\n                                    random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:02.942749Z","iopub.execute_input":"2024-07-10T10:43:02.943066Z","iopub.status.idle":"2024-07-10T10:43:02.953419Z","shell.execute_reply.started":"2024-07-10T10:43:02.943034Z","shell.execute_reply":"2024-07-10T10:43:02.952579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def past_generate_prompt(data_point):\n    return f\"\"\"\n            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).\n            Then predict the human preference of those responses- if it is \"winner_model_a\", \"winner_model_b\" or\n            \"winner_tie\". Return the answer as the correspoding preference label \"winner_model_a\", \"winner_model_b\" or\n            \"winner_tie\" and the logits for each label in the order of preference.\n            ----------------------------------------------------------------------------------------------------------\n            prompt: {data_point[\"prompt\"]}\n            ----------------------------------------------------------------------------------------------------------\n            model_a: {data_point[\"model_a\"]}\n            response_a: {data_point[\"response_a\"]}\n            ----------------------------------------------------------------------------------------------------------\n            model_b: {data_point[\"model_b\"]}\n            response_b: {data_point[\"response_b\"]}\n            ----------------------------------------------------------------------------------------------------------\n            Preference= {data_point[\"Preference\"]} \"\"\".strip()\n\ndef generate_prompt(data_point):\n    return f\"\"\"Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred\nresponse based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.\n\nPrompt: {data_point[\"prompt\"]}\nModel A Response: {data_point[\"response_a\"]}\nModel B Response: {data_point[\"response_b\"]}\nHuman Preference Label: {data_point[\"Preference\"]}\n\"\"\"\n\ndef past_generate_test_prompt(data_point):\n    return f\"\"\"\n            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).\n            Then predict the human preference of those responses- if it is \"winner_model_a\", \"winner_model_b\" or\n            \"winner_tie\". Return the answer as the correspoding preference label \"winner_model_a\", \"winner_model_b\" or\n            \"winner_tie\" and the logits for each label in the order of preference.\n            ----------------------------------------------------------------------------------------------------------\n            prompt: {data_point[\"prompt\"]}\n            ----------------------------------------------------------------------------------------------------------\n\n            response_a: {data_point[\"response_a\"]}\n            ----------------------------------------------------------------------------------------------------------\n\n            response_b: {data_point[\"response_b\"]}\n            ----------------------------------------------------------------------------------------------------------\n            Preference: \"\"\".strip()\n\ndef generate_test_prompt(data_point):\n    return f\"\"\"Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred\nresponse based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.\n\nPrompt: {data_point[\"prompt\"]}\nModel A Response: {data_point[\"response_a\"]}\nModel B Response: {data_point[\"response_b\"]}\n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:02.954696Z","iopub.execute_input":"2024-07-10T10:43:02.954965Z","iopub.status.idle":"2024-07-10T10:43:02.963729Z","shell.execute_reply.started":"2024-07-10T10:43:02.954942Z","shell.execute_reply":"2024-07-10T10:43:02.962866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = pd.DataFrame(train_.apply(generate_prompt, axis=1),\n                       columns=[\"text\"])\nX_eval = pd.DataFrame(val_.apply(generate_prompt, axis=1),\n                      columns=[\"text\"])\nX_test = pd.DataFrame(test_.apply(generate_test_prompt, axis=1), columns=[\"text\"])\n\ny_true = test_.Preference\n\ntrain_data = Dataset.from_pandas(X_train)\neval_data = Dataset.from_pandas(X_eval)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:02.964663Z","iopub.execute_input":"2024-07-10T10:43:02.964926Z","iopub.status.idle":"2024-07-10T10:43:03.039265Z","shell.execute_reply.started":"2024-07-10T10:43:02.964904Z","shell.execute_reply":"2024-07-10T10:43:03.03853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate(y_true, y_pred):\n    labels = ['winner_model_a', 'winner_model_b', 'winner_tie']\n    mapping = {'none':0, 'winner_model_a': 1, 'winner_model_b': 2, 'winner_tie': 3}\n    def map_func(x):\n        return mapping.get(x, 1)\n\n    y_true = np.vectorize(map_func)(y_true)\n    y_pred = np.vectorize(map_func)(y_pred)\n\n    # Calculate accuracy\n    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)\n    print(f'Accuracy: {accuracy:.3f}')\n\n    # Generate accuracy report\n    unique_labels = set(y_true)  # Get unique labels\n\n    for label in unique_labels:\n        label_indices = [i for i in range(len(y_true))\n                         if y_true[i] == label]\n        label_y_true = [y_true[i] for i in label_indices]\n        label_y_pred = [y_pred[i] for i in label_indices]\n        accuracy = accuracy_score(label_y_true, label_y_pred)\n        print(f'Accuracy for label {label}: {accuracy:.3f}')\n\n    # Generate classification report\n    class_report = classification_report(y_true=y_true, y_pred=y_pred)\n    print('\\nClassification Report:')\n    print(class_report)\n\n    # Generate confusion matrix\n    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])\n    print('\\nConfusion Matrix:')\n    print(conf_matrix)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:03.040292Z","iopub.execute_input":"2024-07-10T10:43:03.040602Z","iopub.status.idle":"2024-07-10T10:43:03.049908Z","shell.execute_reply.started":"2024-07-10T10:43:03.040577Z","shell.execute_reply":"2024-07-10T10:43:03.049098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from trl import SFTTrainer\nfrom transformers import TrainingArguments\n\n\ntrainer = SFTTrainer(\n    model = model,\n    tokenizer = tokenizer,\n    train_dataset = train_data,\n    eval_dataset = eval_data,\n    dataset_text_field = \"text\",\n    max_seq_length = max_seq_length,\n    dataset_num_proc = 2,\n    packing = False, # Can make training 5x faster for short sequences.\n    args = TrainingArguments(\n        per_device_train_batch_size = 1,\n        gradient_accumulation_steps = 2,\n        warmup_steps = 5,\n        #evaluation_strategy=\"steps\",         # evaluate at the end of each epoch\n        #save_strategy=\"steps\",\n        #save_steps=10,                      # save checkpoint every 500 steps\n        #eval_steps=10,\n        max_steps = 100,\n        learning_rate = 2e-4,\n        fp16 = True,\n        bf16 = False,\n        logging_steps = 1,\n        optim = \"adamw_8bit\",\n        weight_decay = 0.01,\n        #load_best_model_at_end=True,\n        lr_scheduler_type = \"linear\",\n        seed = 3407,\n        output_dir = \"outputs\",\n    ),\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:03.051007Z","iopub.execute_input":"2024-07-10T10:43:03.051368Z","iopub.status.idle":"2024-07-10T10:43:05.993287Z","shell.execute_reply.started":"2024-07-10T10:43:03.051334Z","shell.execute_reply":"2024-07-10T10:43:05.992363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import wandb\nwandb.init(mode='disabled')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:05.994695Z","iopub.execute_input":"2024-07-10T10:43:05.994987Z","iopub.status.idle":"2024-07-10T10:43:26.523295Z","shell.execute_reply.started":"2024-07-10T10:43:05.994959Z","shell.execute_reply":"2024-07-10T10:43:26.522386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:26.524734Z","iopub.execute_input":"2024-07-10T10:43:26.525786Z","iopub.status.idle":"2024-07-10T10:43:26.90078Z","shell.execute_reply.started":"2024-07-10T10:43:26.525749Z","shell.execute_reply":"2024-07-10T10:43:26.899796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:43:26.906186Z","iopub.execute_input":"2024-07-10T10:43:26.906483Z","iopub.status.idle":"2024-07-10T10:52:42.252855Z","shell.execute_reply.started":"2024-07-10T10:43:26.906459Z","shell.execute_reply":"2024-07-10T10:52:42.251909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text'] = test.apply(lambda x: generate_test_prompt(x), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:52:54.417352Z","iopub.execute_input":"2024-07-10T10:52:54.417721Z","iopub.status.idle":"2024-07-10T10:52:54.423373Z","shell.execute_reply.started":"2024-07-10T10:52:54.417692Z","shell.execute_reply":"2024-07-10T10:52:54.422416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\nprompt = test.loc[2][\"text\"]\n\n#print(prompt)\n\n#FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ninputs = tokenizer(prompt, return_tensors='pt').to(device)\n\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nout1 = tokenizer.batch_decode(outputs)\nprint(\"*************\")\nprint(out1)\n'''","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:52:42.263006Z","iopub.execute_input":"2024-07-10T10:52:42.263272Z","iopub.status.idle":"2024-07-10T10:52:42.278052Z","shell.execute_reply.started":"2024-07-10T10:52:42.263249Z","shell.execute_reply":"2024-07-10T10:52:42.277046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def extract_logits(text):\n    try:\n        d = {'winner_a': [], 'winner_b': [], 'winner_tie': []}\n        logits = re.findall(r\"Logits: \\[(.*?)\\]\", text)[0]\n        logits_dict = {k.strip(): float(v) for k, v in [item.split(\": \") for item in logits[1:-1].split(\", \")]}\n\n        for k, v in logits_dict.items():\n            if '_a' in k:\n                d['winner_a'].append(v)\n            elif '_b' in k:\n                d['winner_b'].append(v)\n            elif '_tie' in k:\n                d['winner_tie'].append(v)\n            \n        df = pd.DataFrame.from_dict(d)\n    \n    except:\n        d = {'winner_a': [0.33], 'winner_b': [0.33], 'winner_tie': [0.33]}\n        df = pd.DataFrame.from_dict(d)\n\n\n    \n    \n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:50:46.890628Z","iopub.execute_input":"2024-07-10T11:50:46.891361Z","iopub.status.idle":"2024-07-10T11:50:46.899251Z","shell.execute_reply.started":"2024-07-10T11:50:46.89133Z","shell.execute_reply":"2024-07-10T11:50:46.898245Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prompt = X_test.iloc[2][\"text\"]\n\n#print(prompt)\n\n\n\nFastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\ninputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n\n#print(inputs)\n\nprint(\"*************\")\noutputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\nout1 = tokenizer.batch_decode(outputs)\n\nprint(out1)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:15:11.308208Z","iopub.execute_input":"2024-07-10T11:15:11.308893Z","iopub.status.idle":"2024-07-10T11:15:17.689235Z","shell.execute_reply.started":"2024-07-10T11:15:11.308862Z","shell.execute_reply":"2024-07-10T11:15:17.688298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import re\n# label_pattern = re.compile(r'Human Preference Label:\\s*(\\w+)')","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:54:28.332006Z","iopub.execute_input":"2024-07-10T10:54:28.332695Z","iopub.status.idle":"2024-07-10T10:54:28.337437Z","shell.execute_reply.started":"2024-07-10T10:54:28.33266Z","shell.execute_reply":"2024-07-10T10:54:28.336371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_logits(text):\n#     logits_pattern = re.compile(r'Logits:\\s*\\[(.*?)\\]')\n#     logits_match = logits_pattern.search(text[0])\n\n#     if logits_match:\n#         logits_str = logits_match.group(1)\n#         logits_list = [(x.strip()) for x in logits_str.split(',')]\n\n#         # Output results\n#         #print(\"Logits:\", logits_list)\n#     else:\n#         # if it is not able to find string, give deafult values\n#         logits_list = [\"'winner_model_a': 0.33\", \"'winner_model_b': 0.33\", \"'winner_tie': 0.33\"]\n    \n    \n    \n#     return logits_list","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:59:58.566929Z","iopub.execute_input":"2024-07-10T10:59:58.567276Z","iopub.status.idle":"2024-07-10T10:59:58.57341Z","shell.execute_reply.started":"2024-07-10T10:59:58.567246Z","shell.execute_reply":"2024-07-10T10:59:58.572391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred = []\n# logits = pd.DataFrame()\n# for i in tqdm(range(len(X_test))):\n#     prompt = X_test.iloc[i][\"text\"]\n#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n#     inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n\n#     outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n#     out1 = tokenizer.batch_decode(outputs)\n#     try:\n#         preferences = label_pattern.search(out1[0]).group(1)\n#         lgts = extract_logits(out1[0])\n#     except Exception as e:\n#         print(out1)\n#         preferences = 'winner_tie'\n#     print(preferences)\n#     print(lgts)\n#     y_pred.append(preferences)\n    \n#     logits = pd.concat([logits, lgts],axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:51:52.784158Z","iopub.execute_input":"2024-07-10T11:51:52.784893Z","iopub.status.idle":"2024-07-10T11:53:29.467747Z","shell.execute_reply.started":"2024-07-10T11:51:52.784859Z","shell.execute_reply":"2024-07-10T11:53:29.466108Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##evaluate(y_true, y_pred)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T07:13:41.509169Z","iopub.execute_input":"2024-07-10T07:13:41.510049Z","iopub.status.idle":"2024-07-10T07:13:41.535584Z","shell.execute_reply.started":"2024-07-10T07:13:41.510014Z","shell.execute_reply":"2024-07-10T07:13:41.534514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_logits(text):\n#     logits_pattern = re.compile(r'Logits:\\s*\\[(.*?)\\]')\n#     logits_match = logits_pattern.search(text[0])\n\n#     if logits_match:\n#         logits_str = logits_match.group(1)\n#         logits_list = [(x.strip()) for x in logits_str.split(',')]\n\n#         # Output results\n#         #print(\"Logits:\", logits_list)\n#     else:\n#         # if it is not able to find string, give deafult values\n#         logits_list = [\"'winner_model_a': 0.33\", \"'winner_model_b': 0.33\", \"'winner_tie': 0.33\"]\n    \n    \n    \n#     return logits_list","metadata":{"execution":{"iopub.status.busy":"2024-07-10T10:57:15.870466Z","iopub.execute_input":"2024-07-10T10:57:15.871253Z","iopub.status.idle":"2024-07-10T10:57:15.877448Z","shell.execute_reply.started":"2024-07-10T10:57:15.871214Z","shell.execute_reply":"2024-07-10T10:57:15.876541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logits_list = pd.DataFrame()\nfor i in tqdm(range(len(test))):\n    prompt = test.iloc[i][\"text\"]\n    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n\n    inputs = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n\n    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n    out1 = tokenizer.batch_decode(outputs)\n    preferences = extract_logits(out1[0])\n    #print(preferences)\n    logits_list = pd.concat([logits_list, preferences],axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:55:16.122934Z","iopub.execute_input":"2024-07-10T11:55:16.123826Z","iopub.status.idle":"2024-07-10T11:55:30.183725Z","shell.execute_reply.started":"2024-07-10T11:55:16.123791Z","shell.execute_reply":"2024-07-10T11:55:30.182815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n\n\n\n# # Function to convert string list to dictionary\n# def logits_to_dict(logits):\n#     return {item.split(':')[0].strip(\"' \"): float(item.split(':')[1].strip()) for item in logits}\n\n# # Convert each list to a dictionary\n# logits_dicts = [logits_to_dict(logits) for logits in logits_list]\n\n# # Create DataFrame\n# df = pd.DataFrame(logits_dicts)\n\n# df\n","metadata":{"execution":{"iopub.status.busy":"2024-07-10T07:20:15.411334Z","iopub.execute_input":"2024-07-10T07:20:15.41206Z","iopub.status.idle":"2024-07-10T07:20:15.427803Z","shell.execute_reply.started":"2024-07-10T07:20:15.412027Z","shell.execute_reply":"2024-07-10T07:20:15.426739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1 = logits_list.reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:57:16.97443Z","iopub.execute_input":"2024-07-10T11:57:16.974817Z","iopub.status.idle":"2024-07-10T11:57:16.979436Z","shell.execute_reply.started":"2024-07-10T11:57:16.974789Z","shell.execute_reply":"2024-07-10T11:57:16.978434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.concat([test['id'], df1],axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:57:34.688941Z","iopub.execute_input":"2024-07-10T11:57:34.68929Z","iopub.status.idle":"2024-07-10T11:57:34.695568Z","shell.execute_reply.started":"2024-07-10T11:57:34.689262Z","shell.execute_reply":"2024-07-10T11:57:34.694298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:57:36.383548Z","iopub.execute_input":"2024-07-10T11:57:36.383901Z","iopub.status.idle":"2024-07-10T11:57:36.395696Z","shell.execute_reply.started":"2024-07-10T11:57:36.383872Z","shell.execute_reply":"2024-07-10T11:57:36.394766Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-10T11:57:39.69845Z","iopub.execute_input":"2024-07-10T11:57:39.698822Z","iopub.status.idle":"2024-07-10T11:57:39.707217Z","shell.execute_reply.started":"2024-07-10T11:57:39.698794Z","shell.execute_reply":"2024-07-10T11:57:39.706152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}