{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":34046,"sourceType":"modelInstanceVersion","modelInstanceId":28500,"modelId":39106},{"sourceId":81968,"sourceType":"modelInstanceVersion","modelInstanceId":68878,"modelId":91102},{"sourceId":82943,"sourceType":"modelInstanceVersion","modelInstanceId":69677,"modelId":94809}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Import libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel\nfrom transformers import LlamaForCausalLM, LlamaTokenizer\nimport torch.nn.functional as F\nnp.random.seed(1337)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-27T09:07:36.874432Z","iopub.execute_input":"2024-07-27T09:07:36.874819Z","iopub.status.idle":"2024-07-27T09:07:36.883946Z","shell.execute_reply.started":"2024-07-27T09:07:36.874773Z","shell.execute_reply":"2024-07-27T09:07:36.88256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenizer","metadata":{}},{"cell_type":"code","source":"model = \"/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf\"\n\ntokenizer = AutoTokenizer.from_pretrained(model)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-07-27T09:07:36.886008Z","iopub.execute_input":"2024-07-27T09:07:36.886467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare train set","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntrain.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def put_text(train):\n    train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A:\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\n    return train\n\ntrain = put_text(train)\nprint(train['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n\ntrain = put_text(train)\nprint(train['text'][0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of tokens which covers 90% of data, we'll still take 1024 length!\nnp.percentile(train['token_count'], 90)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"# Tokenize Data\ntokens = tokenizer(\n    train['text'].tolist(), \n    max_length=1024, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_IDS = tokens['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS = tokens['attention_mask']\n# Label of Texts\nLABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_features = 14300\nmax_len = 1024\nmaxlen = max_len\nbatch_size = 16\nembedding_dims = 100\nnb_filter = 150\nfilter_length = 3\nhidden_dims = 100\nnb_epoch = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function\nimport numpy as np\n\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Lambda\nfrom keras.layers import Embedding\nfrom keras.layers import Convolution1D, LSTM\nfrom keras.datasets import imdb\nfrom keras import backend as K\nfrom keras.optimizers import Adadelta,Adamax\nfrom keras.preprocessing import sequence as sq\n\nfrom keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\n\nfrom tensorflow.python.keras.backend import set_session as K\nnum_samples = INPUT_IDS.shape[0]\n\n# Số lượng mẫu cho X_valid (20% của X_train)\nnum_valid_samples = int(num_samples * 0.2)\n\n# Xáo trộn các chỉ số của X_train\nindices = np.random.permutation(num_samples)\n\n# Chọn 20% chỉ số đầu tiên làm chỉ số cho X_valid\nvalid_indices = indices[:num_valid_samples]\n\n# Các chỉ số còn lại làm chỉ số cho X_train\ntrain_indices = indices[num_valid_samples:]\n\n# Tạo X_valid và X_train mới từ các chỉ số đã chọn\nX_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=max_len)\nX_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=max_len)\ny_train = LABELS[train_indices]\n\nX_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=max_len)\nX_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=max_len)\ny_valid = LABELS[valid_indices]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = np.array(X_train)\ny_train = np.array(y_train)\nX_valid = np.array(X_valid)\ny_valid = np.array(y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define Model","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Layer\nfrom keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D\nfrom keras.layers import  GlobalMaxPooling1D\nimport tensorflow as tf\n\nclass ApplyAttentionMask(Layer):\n    def call(self, inputs):\n        embeddings, attention_mask = inputs\n        return embeddings * tf.expand_dims(attention_mask, -1)\n\ninput_layer = Input(shape=(max_len,),dtype='int32', name='main_input')\nattention_masks = Input(shape=(max_len,), dtype='float32', name=\"attention_masks\")\n\nemb_layer = Embedding(max_features,\n                      embedding_dims,\n                      input_length=max_len\n                      )(input_layer)\n\nmasked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])\n\n# LSTM branch (with Batch Normalization and Dropout)\nlstm_out = LSTM(128, return_sequences=True)(masked_embeddings)\nlstm_out = BatchNormalization()(lstm_out) # Batch Normalization helps to normalize activations and speed up convergence\nlstm_out = Dropout(0.5)(lstm_out) # Dropout = 0.5 helps to prevent overfitting\nlstm_out = LSTM(64, return_sequences=True)(lstm_out)\nlstm_out = BatchNormalization()(lstm_out)\nlstm_out = Dropout(0.5)(lstm_out)\nlstm_out = LSTM(32)(lstm_out)\nlstm_out = BatchNormalization()(lstm_out)\nlstm_out = Dropout(0.5)(lstm_out)\n\n# CNN layer branch (with Batch Normalization and Dropout)\ncnn_out = Conv1D(64, 5, activation='relu')(masked_embeddings)\ncnn_out = BatchNormalization()(cnn_out)\ncnn_out = Dropout(0.5)(cnn_out)\ncnn_out = Conv1D(32, 5, activation='relu')(cnn_out)\ncnn_out = BatchNormalization()(cnn_out)\ncnn_out = Dropout(0.5)(cnn_out)\ncnn_out = GlobalMaxPooling1D()(cnn_out)\n\n\n# Concatenate LSTM and CNN outputs\nmerged = concatenate([lstm_out, cnn_out])\nmerged = Dense(32, activation='sigmoid')(merged)\nmerged = BatchNormalization()(merged)\nmerged = Dropout(0.5)(merged)\npred = Dense(3, activation='softmax')(merged)\n\n\n# Build model\nmodel = Model(inputs=[input_layer, attention_masks], outputs=[pred])\nadadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)\nadamax = Adamax(learning_rate=0.001)\nmodel.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"def clip_indices(data, max_index):\n    return np.where(data >= max_index, max_index - 1, data)\nX_train = clip_indices(X_train, 14300)\nX_train_attention = clip_indices(X_train_attention, 14300)\nX_valid = clip_indices(X_valid, 14300)\nX_valid_attention = clip_indices(X_valid_attention, 14300)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.callbacks import EarlyStopping\ncheckpoint = ModelCheckpoint('/kaggle/working/model.keras',\n                                 monitor='val_acc', verbose=0, save_best_only=True,\n                                 mode='max')\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)\n\nmodel.fit([X_train,X_train_attention], y_train,\n          batch_size=16,\n          epochs=nb_epoch,\n#           callbacks=[checkpoint, early_stopping],\n          callbacks=[early_stopping],\n          validation_data=([X_valid,X_valid_attention], y_valid))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(loss='categorical_crossentropy',\n              optimizer='adadelta',\n              metrics=['accuracy'])\n\nmodel.save('model.keras', overwrite=True)\nmodel.save_weights(\"model.weights.h5\", overwrite=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Test Model","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index\ntest.drop(indexes, inplace=True)\ntest.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(test))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A:\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(test['text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize Data\ntokens_test = tokenizer(\n    test['text'].tolist(), \n    max_length=1024, \n    truncation=True, \n    return_tensors='np')\n\n# Input IDs are the token IDs\nINPUT_test = tokens_test['input_ids']\n# Attention Masks to Ignore Padding Tokens\nATTENTION_MASKS2 = tokens_test['attention_mask']\n\n\nprint(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = sq.pad_sequences(INPUT_test, maxlen=max_len)\nX_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=max_len)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_predict = model.predict([X_test,X_test_attention])\ny_predict","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\nresult_df = pd.concat([test['id'], winner_df], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result_df.to_csv('submission.csv', index=False)\nresult_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"____________________","metadata":{}},{"cell_type":"code","source":"# # Import necessary libraries\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.feature_extraction.text import TfidfVectorizer\n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.metrics import log_loss\n# from sklearn.preprocessing import LabelEncoder\n\n# # Load the data\n# train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n# test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n\n# # Inspect the data\n# print(train.head())\n# print(test.head())\n\n# # Data Preprocessing\n# # Combine responses into one text feature\n# train['response_combined'] = train['response_a'] + ' ' + train['response_b']\n# test['response_combined'] = test['response_a'] + ' ' + test['response_b']\n\n# # Encode the target labels\n# label_encoder = LabelEncoder()\n# train['winner'] = label_encoder.fit_transform(train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1))\n\n# # Feature Engineering\n# # Vectorize the combined responses using TF-IDF\n# tfidf = TfidfVectorizer(max_features=1000)\n# X_train_tfidf = tfidf.fit_transform(train['response_combined'])\n# X_test_tfidf = tfidf.transform(test['response_combined'])\n\n# # Prepare the data for model training\n# X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, train['winner'], test_size=0.2, random_state=42)\n\n# # Train the model\n# model = RandomForestClassifier(n_estimators=100, random_state=42)\n# model.fit(X_train, y_train)\n\n# # Validate the model\n# y_val_pred_proba = model.predict_proba(X_val)\n# val_log_loss = log_loss(y_val, y_val_pred_proba)\n# print(f'Validation Log Loss: {val_log_loss}')\n\n# # Predict on the test set\n# test_pred_proba = model.predict_proba(X_test_tfidf)\n\n# # Prepare the submission file\n# submission = pd.DataFrame(test['id'], columns=['id'])\n# submission['winner_model_a'] = test_pred_proba[:, label_encoder.transform(['winner_model_a'])]\n# submission['winner_model_b'] = test_pred_proba[:, label_encoder.transform(['winner_model_b'])]\n# submission['winner_tie'] = test_pred_proba[:, label_encoder.transform(['winner_tie'])]\n# submission.to_csv('submission.csv', index=False)\n\n# # Inspect the submission file\n# print(submission.head())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]}]}