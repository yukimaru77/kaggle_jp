{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa265e46",
   "metadata": {},
   "source": [
    "# Create SMOTE for imbalanced data in train csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:31.438732Z",
     "iopub.status.busy": "2024-07-23T12:09:31.438382Z",
     "iopub.status.idle": "2024-07-23T12:09:37.112563Z",
     "shell.execute_reply": "2024-07-23T12:09:37.111523Z",
     "shell.execute_reply.started": "2024-07-23T12:09:31.438703Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ddca8",
   "metadata": {},
   "source": [
    "I copy methods from https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning?scriptVersionId=187770530 but i changed it using my own balanced data with gemma model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.115204Z",
     "iopub.status.busy": "2024-07-23T12:09:37.114783Z",
     "iopub.status.idle": "2024-07-23T12:09:37.157922Z",
     "shell.execute_reply": "2024-07-23T12:09:37.156859Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.115156Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train.head(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.159315Z",
     "iopub.status.busy": "2024-07-23T12:09:37.159036Z",
     "iopub.status.idle": "2024-07-23T12:09:37.171021Z",
     "shell.execute_reply": "2024-07-23T12:09:37.170067Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.159291Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.172583Z",
     "iopub.status.busy": "2024-07-23T12:09:37.172204Z",
     "iopub.status.idle": "2024-07-23T12:09:37.19172Z",
     "shell.execute_reply": "2024-07-23T12:09:37.190775Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.172558Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Combine the datasets\n",
    "combined_df = pd.concat([train, test], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.194727Z",
     "iopub.status.busy": "2024-07-23T12:09:37.19445Z",
     "iopub.status.idle": "2024-07-23T12:09:39.575687Z",
     "shell.execute_reply": "2024-07-23T12:09:39.574742Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.194703Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Fill missing values in target columns with 0 (assuming the missing values should be treated as no win)\n",
    "combined_df['winner_model_a'] = combined_df['winner_model_a'].fillna(0)\n",
    "combined_df['winner_model_b'] = combined_df['winner_model_b'].fillna(0)\n",
    "combined_df['winner_tie'] = combined_df['winner_tie'].fillna(0)\n",
    "\n",
    "# Combine the target columns into a single column for SMOTE\n",
    "combined_df['winner'] = combined_df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\n",
    "\n",
    "# Map the winner column to numerical values\n",
    "winner_mapping = {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}\n",
    "combined_df['winner'] = combined_df['winner'].map(winner_mapping)\n",
    "\n",
    "# Apply SMOTE to the combined dataset\n",
    "smote = SMOTE()\n",
    "X = combined_df.drop(columns=['winner', 'winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "y = combined_df['winner']\n",
    "\n",
    "# For simplicity, we'll encode the textual data using simple numerical encoding\n",
    "X_encoded = X.apply(lambda col: col.astype('category').cat.codes if col.dtype == 'object' else col)\n",
    "\n",
    "# Apply SMOTE\n",
    "X_resampled, y_resampled = smote.fit_resample(X_encoded, y)\n",
    "\n",
    "# Create a DataFrame from the resampled data\n",
    "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "resampled_df['winner'] = y_resampled\n",
    "\n",
    "# Decode the winner column back to its original form\n",
    "inverse_winner_mapping = {v: k for k, v in winner_mapping.items()}\n",
    "resampled_df['winner'] = resampled_df['winner'].map(inverse_winner_mapping)\n",
    "\n",
    "# Split the winner column back into the original three columns\n",
    "resampled_df['winner_model_a'] = (resampled_df['winner'] == 'winner_model_a').astype(int)\n",
    "resampled_df['winner_model_b'] = (resampled_df['winner'] == 'winner_model_b').astype(int)\n",
    "resampled_df['winner_tie'] = (resampled_df['winner'] == 'winner_tie').astype(int)\n",
    "\n",
    "# Drop the combined winner column\n",
    "resampled_df = resampled_df.drop(columns=['winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.577426Z",
     "iopub.status.busy": "2024-07-23T12:09:39.576973Z",
     "iopub.status.idle": "2024-07-23T12:09:39.591628Z",
     "shell.execute_reply": "2024-07-23T12:09:39.590631Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.577398Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "resampled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.593395Z",
     "iopub.status.busy": "2024-07-23T12:09:39.593071Z",
     "iopub.status.idle": "2024-07-23T12:09:39.683427Z",
     "shell.execute_reply": "2024-07-23T12:09:39.682529Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.593368Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Fill missing values with the mode\n",
    "combined_df['model_a'].fillna(combined_df['model_a'].mode()[0], inplace=True)\n",
    "combined_df['model_b'].fillna(combined_df['model_b'].mode()[0], inplace=True)\n",
    "combined_df['winner_model_a'].fillna(combined_df['winner_model_a'].mode()[0], inplace=True)\n",
    "combined_df['winner_model_b'].fillna(combined_df['winner_model_b'].mode()[0], inplace=True)\n",
    "combined_df['winner_tie'].fillna(combined_df['winner_tie'].mode()[0], inplace=True)\n",
    "\n",
    "# Check for missing values in the combined DataFrame\n",
    "missing_values = combined_df.isnull().sum()\n",
    "\n",
    "# Display the missing values\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.684964Z",
     "iopub.status.busy": "2024-07-23T12:09:39.684687Z",
     "iopub.status.idle": "2024-07-23T12:09:47.407961Z",
     "shell.execute_reply": "2024-07-23T12:09:47.407106Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.684941Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save the combined DataFrame to a new CSV file\n",
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeaac1",
   "metadata": {},
   "source": [
    "# Why i created imbalanced (smote) methods ?\n",
    "\n",
    "The dataset needs to address class imbalance because an imbalanced dataset can lead to biased model performance. When a machine learning model is trained on an imbalanced dataset, it tends to become biased towards the majority class, resulting in poor performance on the minority classes. This can significantly impact the model's ability to generalize well to new, unseen data.\n",
    "\n",
    "In this specific dataset, the columns related to the winners (winner_model_a, winner_model_b, winner_tie) are imbalanced. Here's a breakdown of the issue:\n",
    "\n",
    "winner_model_a: This column indicates whether model A won. If the count of 1s (indicating a win for model A) is much lower or higher compared to the other winner columns, it creates an imbalance.\n",
    "\n",
    "winner_model_b: This column indicates whether model B won. Similarly, if the count of 1s in this column is disproportionate to the others, it contributes to imbalance.\n",
    "\n",
    "winner_tie: This column indicates whether the result was a tie. A significantly lower count of 1s here compared to the others further highlights the imbalance.\n",
    "\n",
    "From the initial analysis, we observed that the winner_tie column had fewer instances compared to the other two columns, suggesting an imbalance in how often ties occur relative to wins by either model A or model B. This imbalance can skew the model's learning process, making it less effective at predicting ties. Addressing this imbalance using techniques like SMOTE (Synthetic Minority Over-sampling Technique) helps to ensure that the model has a more balanced view of all possible outcomes, leading to better overall performance.\n",
    "\n",
    "# Use Gemma Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:47.40936Z",
     "iopub.status.busy": "2024-07-23T12:09:47.409067Z",
     "iopub.status.idle": "2024-07-23T12:10:23.562038Z",
     "shell.execute_reply": "2024-07-23T12:10:23.558604Z",
     "shell.execute_reply.started": "2024-07-23T12:09:47.409336Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# gemma-2 is available from transformers>=4.42.3\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:23.564219Z",
     "iopub.status.busy": "2024-07-23T12:10:23.563809Z",
     "iopub.status.idle": "2024-07-23T12:10:52.13093Z",
     "shell.execute_reply": "2024-07-23T12:10:52.129568Z",
     "shell.execute_reply.started": "2024-07-23T12:10:23.564178Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f5d21",
   "metadata": {},
   "source": [
    "# Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.13298Z",
     "iopub.status.busy": "2024-07-23T12:10:52.132314Z",
     "iopub.status.idle": "2024-07-23T12:10:52.142939Z",
     "shell.execute_reply": "2024-07-23T12:10:52.141938Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.132954Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"\n",
    "    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 1024\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 2  # global batch size is 8 \n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: float = lora_r * 2\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7474725",
   "metadata": {},
   "source": [
    "# Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.144638Z",
     "iopub.status.busy": "2024-07-23T12:10:52.144254Z",
     "iopub.status.idle": "2024-07-23T12:10:52.220815Z",
     "shell.execute_reply": "2024-07-23T12:10:52.219891Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.144607Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbf0ed",
   "metadata": {},
   "source": [
    "# LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.222318Z",
     "iopub.status.busy": "2024-07-23T12:10:52.222023Z",
     "iopub.status.idle": "2024-07-23T12:10:52.228669Z",
     "shell.execute_reply": "2024-07-23T12:10:52.227538Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.222294Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5435e88",
   "metadata": {},
   "source": [
    "# Instantiate the tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.233974Z",
     "iopub.status.busy": "2024-07-23T12:10:52.233669Z",
     "iopub.status.idle": "2024-07-23T12:10:54.035041Z",
     "shell.execute_reply": "2024-07-23T12:10:54.033927Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.233949Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:54.036892Z",
     "iopub.status.busy": "2024-07-23T12:10:54.036481Z",
     "iopub.status.idle": "2024-07-23T12:11:35.437885Z",
     "shell.execute_reply": "2024-07-23T12:11:35.436859Z",
     "shell.execute_reply.started": "2024-07-23T12:10:54.036854Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:35.439329Z",
     "iopub.status.busy": "2024-07-23T12:11:35.439006Z",
     "iopub.status.idle": "2024-07-23T12:11:35.451467Z",
     "shell.execute_reply": "2024-07-23T12:11:35.450523Z",
     "shell.execute_reply.started": "2024-07-23T12:11:35.439301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38d6a8",
   "metadata": {},
   "source": [
    "# Load Train File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:35.452941Z",
     "iopub.status.busy": "2024-07-23T12:11:35.452621Z",
     "iopub.status.idle": "2024-07-23T12:11:39.93426Z",
     "shell.execute_reply": "2024-07-23T12:11:39.933292Z",
     "shell.execute_reply.started": "2024-07-23T12:11:35.452911Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = Dataset.from_csv(\"/kaggle/working/combined_df.csv\")\n",
    "ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:39.935749Z",
     "iopub.status.busy": "2024-07-23T12:11:39.935455Z",
     "iopub.status.idle": "2024-07-23T12:11:39.945941Z",
     "shell.execute_reply": "2024-07-23T12:11:39.944964Z",
     "shell.execute_reply.started": "2024-07-23T12:11:39.935724Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels=[]\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "            if a_win:\n",
    "                label = 0\n",
    "            elif b_win:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "        return {**tokenized, \"labels\": labels}\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:39.947511Z",
     "iopub.status.busy": "2024-07-23T12:11:39.94713Z",
     "iopub.status.idle": "2024-07-23T12:11:40.964043Z",
     "shell.execute_reply": "2024-07-23T12:11:40.96298Z",
     "shell.execute_reply.started": "2024-07-23T12:11:39.94748Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5aef9",
   "metadata": {},
   "source": [
    "# Compute metric logloss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.965804Z",
     "iopub.status.busy": "2024-07-23T12:11:40.965506Z",
     "iopub.status.idle": "2024-07-23T12:11:40.975126Z",
     "shell.execute_reply": "2024-07-23T12:11:40.974369Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.965779Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.976878Z",
     "iopub.status.busy": "2024-07-23T12:11:40.976233Z",
     "iopub.status.idle": "2024-07-23T12:11:40.993606Z",
     "shell.execute_reply": "2024-07-23T12:11:40.992576Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.976846Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.995705Z",
     "iopub.status.busy": "2024-07-23T12:11:40.994867Z",
     "iopub.status.idle": "2024-07-23T12:16:58.97206Z",
     "shell.execute_reply": "2024-07-23T12:16:58.971067Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.995672Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68216a",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.429109Z",
     "iopub.status.busy": "2024-07-23T12:17:24.428206Z",
     "iopub.status.idle": "2024-07-23T12:17:24.43499Z",
     "shell.execute_reply": "2024-07-23T12:17:24.434121Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.429068Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.437302Z",
     "iopub.status.busy": "2024-07-23T12:17:24.436577Z",
     "iopub.status.idle": "2024-07-23T12:17:24.448863Z",
     "shell.execute_reply": "2024-07-23T12:17:24.448008Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.437268Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir = '/kaggle/working/output/checkpoint-20'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.450467Z",
     "iopub.status.busy": "2024-07-23T12:17:24.45018Z",
     "iopub.status.idle": "2024-07-23T12:17:24.465361Z",
     "shell.execute_reply": "2024-07-23T12:17:24.464324Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.450444Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.466785Z",
     "iopub.status.busy": "2024-07-23T12:17:24.466509Z",
     "iopub.status.idle": "2024-07-23T12:17:24.48208Z",
     "shell.execute_reply": "2024-07-23T12:17:24.481009Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.466751Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.484224Z",
     "iopub.status.busy": "2024-07-23T12:17:24.483928Z",
     "iopub.status.idle": "2024-07-23T12:17:24.500736Z",
     "shell.execute_reply": "2024-07-23T12:17:24.499687Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.484189Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.50217Z",
     "iopub.status.busy": "2024-07-23T12:17:24.501816Z",
     "iopub.status.idle": "2024-07-23T12:17:25.538457Z",
     "shell.execute_reply": "2024-07-23T12:17:25.537483Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.502138Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.540235Z",
     "iopub.status.busy": "2024-07-23T12:17:25.539846Z",
     "iopub.status.idle": "2024-07-23T12:17:25.54738Z",
     "shell.execute_reply": "2024-07-23T12:17:25.546393Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.540201Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.548742Z",
     "iopub.status.busy": "2024-07-23T12:17:25.548466Z",
     "iopub.status.idle": "2024-07-23T12:17:25.562109Z",
     "shell.execute_reply": "2024-07-23T12:17:25.561243Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.548714Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.563662Z",
     "iopub.status.busy": "2024-07-23T12:17:25.563361Z",
     "iopub.status.idle": "2024-07-23T12:18:58.380996Z",
     "shell.execute_reply": "2024-07-23T12:18:58.380081Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.563637Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:58.382991Z",
     "iopub.status.busy": "2024-07-23T12:18:58.382306Z",
     "iopub.status.idle": "2024-07-23T12:18:59.003837Z",
     "shell.execute_reply": "2024-07-23T12:18:59.002849Z",
     "shell.execute_reply.started": "2024-07-23T12:18:58.382954Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:59.005429Z",
     "iopub.status.busy": "2024-07-23T12:18:59.005087Z",
     "iopub.status.idle": "2024-07-23T12:18:59.015504Z",
     "shell.execute_reply": "2024-07-23T12:18:59.014256Z",
     "shell.execute_reply.started": "2024-07-23T12:18:59.005401Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:59.017085Z",
     "iopub.status.busy": "2024-07-23T12:18:59.016807Z",
     "iopub.status.idle": "2024-07-23T12:19:03.335739Z",
     "shell.execute_reply": "2024-07-23T12:19:03.334689Z",
     "shell.execute_reply.started": "2024-07-23T12:18:59.017062Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:19:03.337429Z",
     "iopub.status.busy": "2024-07-23T12:19:03.337028Z",
     "iopub.status.idle": "2024-07-23T12:19:03.346082Z",
     "shell.execute_reply": "2024-07-23T12:19:03.345023Z",
     "shell.execute_reply.started": "2024-07-23T12:19:03.337395Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:32:09.646006Z",
     "iopub.status.busy": "2024-07-23T12:32:09.64565Z",
     "iopub.status.idle": "2024-07-23T12:32:09.656235Z",
     "shell.execute_reply": "2024-07-23T12:32:09.655442Z",
     "shell.execute_reply.started": "2024-07-23T12:32:09.64598Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Align the probabilities with the test_data order based on 'id'\n",
    "aligned_proba = proba[[result_df.index[result_df['id'] == id][0] for id in test['id']]]\n",
    "\n",
    "# Extract the predictions\n",
    "test_pred_a = aligned_proba[:, 0]\n",
    "test_pred_b = aligned_proba[:, 1]\n",
    "test_pred_tie = aligned_proba[:, 2]\n",
    "\n",
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'winner_model_a': test_pred_a,\n",
    "    'winner_model_b': test_pred_b,\n",
    "    'winner_tie': test_pred_tie\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_path = '/kaggle/working/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:32:12.743462Z",
     "iopub.status.busy": "2024-07-23T12:32:12.743066Z",
     "iopub.status.idle": "2024-07-23T12:32:12.755173Z",
     "shell.execute_reply": "2024-07-23T12:32:12.754301Z",
     "shell.execute_reply.started": "2024-07-23T12:32:12.74343Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee1d20",
   "metadata": {},
   "source": [
    "The less number of logloss, the better result of performances ."
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
