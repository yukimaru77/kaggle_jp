{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8330401,"sourceType":"datasetVersion","datasetId":4946449},{"sourceId":8449074,"sourceType":"datasetVersion","datasetId":5034873},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083},{"sourceId":62188,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51944},{"sourceId":62308,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":52038}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:08:19.41131Z","iopub.execute_input":"2024-07-04T02:08:19.41193Z","iopub.status.idle":"2024-07-04T02:09:08.666863Z","shell.execute_reply.started":"2024-07-04T02:08:19.411898Z","shell.execute_reply":"2024-07-04T02:09:08.665859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The work in this notebook is inspired by these notebooks:\n* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b","metadata":{}},{"cell_type":"markdown","source":"## Importing Libraries","metadata":{}},{"cell_type":"code","source":"import torch\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport time\n\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification,Qwen2ForSequenceClassification, BitsAndBytesConfig\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\nimport gc\nimport os\nimport io\nimport time\nimport json\nimport random\nimport pickle\nimport zipfile\nimport datetime\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nfrom collections import Counter\nfrom collections import defaultdict\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nimport pytorch_lightning as pl\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import log_loss\nimport tokenizers\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:08.669749Z","iopub.execute_input":"2024-07-04T02:09:08.670156Z","iopub.status.idle":"2024-07-04T02:09:08.679613Z","shell.execute_reply.started":"2024-07-04T02:09:08.670116Z","shell.execute_reply":"2024-07-04T02:09:08.678788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n\nMODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'\nWEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\nMAX_LENGTH = 1284\nBATCH_SIZE = 8\nDEVICE = torch.device(\"cuda\")    ","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:08.680953Z","iopub.execute_input":"2024-07-04T02:09:08.681276Z","iopub.status.idle":"2024-07-04T02:09:08.690924Z","shell.execute_reply.started":"2024-07-04T02:09:08.681252Z","shell.execute_reply":"2024-07-04T02:09:08.690021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Data ","metadata":{}},{"cell_type":"code","source":"test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\ntrain = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))\nsample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:08.69213Z","iopub.execute_input":"2024-07-04T02:09:08.692402Z","iopub.status.idle":"2024-07-04T02:09:10.572558Z","shell.execute_reply.started":"2024-07-04T02:09:08.692379Z","shell.execute_reply":"2024-07-04T02:09:10.571466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# concatenate strings in list\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntest.loc[:, 'prompt'] = test['prompt'].apply(process)\ntest.loc[:, 'response_a'] = test['response_a'].apply(process)\ntest.loc[:, 'response_b'] = test['response_b'].apply(process)\n\ndisplay(sample_sub)\ndisplay(test.head(5))\n\n# Prepare text for model\ntest['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\nprint(test['text'][0])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:10.575733Z","iopub.execute_input":"2024-07-04T02:09:10.576123Z","iopub.status.idle":"2024-07-04T02:09:10.599148Z","shell.execute_reply.started":"2024-07-04T02:09:10.576089Z","shell.execute_reply":"2024-07-04T02:09:10.598191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tokenize","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n\ntokens = tokenizer(test['text'].tolist(), padding='max_length',\n                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\nATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n\n# Move tensors to CPU and convert them to lists\ninput_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\nattention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = input_ids_cpu\ndata['ATTENTION_MASKS'] = attention_masks_cpu\ndata[:2]","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:10.600186Z","iopub.execute_input":"2024-07-04T02:09:10.60044Z","iopub.status.idle":"2024-07-04T02:09:11.096387Z","shell.execute_reply.started":"2024-07-04T02:09:10.600417Z","shell.execute_reply":"2024-07-04T02:09:11.095463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load model \n> We load 1 model on each gpu.  ","metadata":{}},{"cell_type":"code","source":"# BitsAndBytes configuration\n# bnb_config =  BitsAndBytesConfig(\n#     load_in_8bit=True,\n#     bnb_8bit_compute_dtype=torch.float16,\n#     bnb_8bit_use_double_quant=False)\n\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_compute_dtype=torch.bfloat16,\n)\n# Load base model on GPU 0\ndevice0 = torch.device('cuda:0')\n\nbase_model_0 = Qwen2ForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:0')\n\nbase_model_0.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:09:11.097574Z","iopub.execute_input":"2024-07-04T02:09:11.0979Z","iopub.status.idle":"2024-07-04T02:10:42.858209Z","shell.execute_reply.started":"2024-07-04T02:09:11.097873Z","shell.execute_reply":"2024-07-04T02:10:42.857325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load base model on GPU 1\ndevice1 = torch.device('cuda:1')\nbase_model_1 = Qwen2ForSequenceClassification.from_pretrained(\n    MODEL_NAME,\n    num_labels=3,\n    torch_dtype=torch.float16,\n    quantization_config=bnb_config,\n    device_map='cuda:1')\nbase_model_1.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:10:42.859332Z","iopub.execute_input":"2024-07-04T02:10:42.859611Z","iopub.status.idle":"2024-07-04T02:12:16.527811Z","shell.execute_reply.started":"2024-07-04T02:10:42.859586Z","shell.execute_reply":"2024-07-04T02:12:16.527017Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Load weights \n","metadata":{}},{"cell_type":"code","source":"# LoRa configuration\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.10,\n    bias='none',\n    inference_mode=True,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=['o_proj', 'v_proj'])","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:16.529128Z","iopub.execute_input":"2024-07-04T02:12:16.529485Z","iopub.status.idle":"2024-07-04T02:12:16.534776Z","shell.execute_reply.started":"2024-07-04T02:12:16.529453Z","shell.execute_reply":"2024-07-04T02:12:16.533756Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Get peft\nmodel_0 = get_peft_model(base_model_0, peft_config).to(device0) \n#Load weights\n# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_0.eval()\n\nmodel_1 = get_peft_model(base_model_1, peft_config).to(device1)\n# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\nmodel_1.eval()\n\n#Trainable Parameters\nmodel_0.print_trainable_parameters(), model_1.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:16.536064Z","iopub.execute_input":"2024-07-04T02:12:16.53639Z","iopub.status.idle":"2024-07-04T02:12:24.092142Z","shell.execute_reply.started":"2024-07-04T02:12:16.536352Z","shell.execute_reply":"2024-07-04T02:12:24.091243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:24.093493Z","iopub.execute_input":"2024-07-04T02:12:24.093939Z","iopub.status.idle":"2024-07-04T02:12:24.402154Z","shell.execute_reply.started":"2024-07-04T02:12:24.093906Z","shell.execute_reply":"2024-07-04T02:12:24.401243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"def inference(df, model, device, batch_size=BATCH_SIZE):\n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n    \n    generated_class_a = []\n    generated_class_b = []\n    generated_class_c = []\n\n    model.eval()\n    \n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        \n        with torch.no_grad():\n            with autocast():\n                outputs = model(\n                    input_ids=batch_input_ids,\n                    attention_mask=batch_attention_mask\n                )\n        \n        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n        \n        generated_class_a.extend(probabilities[:, 0])\n        generated_class_b.extend(probabilities[:, 1])\n        generated_class_c.extend(probabilities[:, 2])\n    \n    df['winner_model_a'] = generated_class_a\n    df['winner_model_b'] = generated_class_b\n    df['winner_tie'] = generated_class_c\n\n    torch.cuda.empty_cache()  \n\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:24.403499Z","iopub.execute_input":"2024-07-04T02:12:24.404761Z","iopub.status.idle":"2024-07-04T02:12:24.41517Z","shell.execute_reply.started":"2024-07-04T02:12:24.404724Z","shell.execute_reply":"2024-07-04T02:12:24.414251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\n\n# Split the data into two subsets\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\n# Function to run inference in a thread\ndef run_inference(df, model, device, results, index):\n    results[index] = inference(df, model, device)\n\n# Dictionary to store results from threads\nresults = {}","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:24.416431Z","iopub.execute_input":"2024-07-04T02:12:24.416736Z","iopub.status.idle":"2024-07-04T02:12:24.431371Z","shell.execute_reply.started":"2024-07-04T02:12:24.416711Z","shell.execute_reply":"2024-07-04T02:12:24.430582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# start threads\nt0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\n# Wait for all threads to finish\nt0.join()\nt1.join()\n\n# Combine results back into the original DataFrame\ndata = pd.concat([results[0], results[1]], axis=0)\n\nprint(f\"Processing complete. Total time: {time.time() - st}\")\n\nTARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n\nsample_sub[TARGETS] = data[TARGETS]","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:24.435211Z","iopub.execute_input":"2024-07-04T02:12:24.435578Z","iopub.status.idle":"2024-07-04T02:12:27.427665Z","shell.execute_reply.started":"2024-07-04T02:12:24.435554Z","shell.execute_reply":"2024-07-04T02:12:27.426669Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"llama_preds = data[TARGETS].values","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:27.432162Z","iopub.execute_input":"2024-07-04T02:12:27.432543Z","iopub.status.idle":"2024-07-04T02:12:27.438717Z","shell.execute_reply.started":"2024-07-04T02:12:27.432504Z","shell.execute_reply":"2024-07-04T02:12:27.437781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nout = pd.DataFrame(llama_preds, \n                index = test.id, \n                    columns = train.columns[-3:])\ndisplay(out.head())\n","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:27.43983Z","iopub.execute_input":"2024-07-04T02:12:27.440117Z","iopub.status.idle":"2024-07-04T02:12:27.453037Z","shell.execute_reply.started":"2024-07-04T02:12:27.440094Z","shell.execute_reply":"2024-07-04T02:12:27.452123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"out.to_csv('submission.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-04T02:12:27.454149Z","iopub.execute_input":"2024-07-04T02:12:27.454408Z","iopub.status.idle":"2024-07-04T02:12:27.463274Z","shell.execute_reply.started":"2024-07-04T02:12:27.454386Z","shell.execute_reply":"2024-07-04T02:12:27.462504Z"},"trusted":true},"execution_count":null,"outputs":[]}]}