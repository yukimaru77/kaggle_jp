{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a882136",
   "metadata": {},
   "source": [
    "## llama3-8b\n",
    "\n",
    "shout out to:\n",
    "https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:22:58.329854Z",
     "iopub.status.busy": "2024-07-05T03:22:58.329426Z",
     "iopub.status.idle": "2024-07-05T03:23:53.200284Z",
     "shell.execute_reply": "2024-07-05T03:23:53.198969Z",
     "shell.execute_reply.started": "2024-07-05T03:22:58.329828Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:23:53.202855Z",
     "iopub.status.busy": "2024-07-05T03:23:53.202545Z",
     "iopub.status.idle": "2024-07-05T03:26:11.922417Z",
     "shell.execute_reply": "2024-07-05T03:26:11.921452Z",
     "shell.execute_reply.started": "2024-07-05T03:23:53.202827Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\")    \n",
    "\n",
    "# # Prepare Data \n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])\n",
    "\n",
    "# # Tokenize\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]\n",
    "\n",
    "# # Load model \n",
    "# We load 1 model on each gpu.  \n",
    "\n",
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Now, we have sucessfully loaded one model on each GPU!\n",
    "\n",
    "# # Load weights \n",
    "\n",
    "# LoRa configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj'])\n",
    "\n",
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "# Load weights\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()\n",
    "\n",
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "\n",
    "# # Inference\n",
    "# \n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}\n",
    "\n",
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")\n",
    "\n",
    "# Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) \n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:11.928099Z",
     "iopub.status.busy": "2024-07-05T03:26:11.927672Z",
     "iopub.status.idle": "2024-07-05T03:26:11.933799Z",
     "shell.execute_reply": "2024-07-05T03:26:11.932872Z",
     "shell.execute_reply.started": "2024-07-05T03:26:11.928064Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8d88a6",
   "metadata": {},
   "source": [
    "## LGBM + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:11.935398Z",
     "iopub.status.busy": "2024-07-05T03:26:11.935092Z",
     "iopub.status.idle": "2024-07-05T03:26:12.001814Z",
     "shell.execute_reply": "2024-07-05T03:26:12.000853Z",
     "shell.execute_reply.started": "2024-07-05T03:26:11.935373Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'\n",
    "\n",
    "import os\n",
    "RUNPOD = os.path.exists('/workspace/')\n",
    "KAGGLE = not RUNPOD\n",
    "if KAGGLE: print('kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.003301Z",
     "iopub.status.busy": "2024-07-05T03:26:12.003018Z",
     "iopub.status.idle": "2024-07-05T03:26:12.013545Z",
     "shell.execute_reply": "2024-07-05T03:26:12.012779Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.003277Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.014825Z",
     "iopub.status.busy": "2024-07-05T03:26:12.014548Z",
     "iopub.status.idle": "2024-07-05T03:26:12.029324Z",
     "shell.execute_reply": "2024-07-05T03:26:12.028419Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.014803Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "import os\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/ \n",
    "        \n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.030826Z",
     "iopub.status.busy": "2024-07-05T03:26:12.030511Z",
     "iopub.status.idle": "2024-07-05T03:26:12.040806Z",
     "shell.execute_reply": "2024-07-05T03:26:12.039827Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.030795Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = '/kaggle/input/'  \n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n",
    "# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\n",
    "print(MODEL_PATH)\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.042334Z",
     "iopub.status.busy": "2024-07-05T03:26:12.041989Z",
     "iopub.status.idle": "2024-07-05T03:26:14.692364Z",
     "shell.execute_reply": "2024-07-05T03:26:14.691542Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.042304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:14.696304Z",
     "iopub.status.busy": "2024-07-05T03:26:14.695993Z",
     "iopub.status.idle": "2024-07-05T03:26:17.778317Z",
     "shell.execute_reply": "2024-07-05T03:26:17.777217Z",
     "shell.execute_reply.started": "2024-07-05T03:26:14.696278Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.779974Z",
     "iopub.status.busy": "2024-07-05T03:26:17.779658Z",
     "iopub.status.idle": "2024-07-05T03:26:17.785073Z",
     "shell.execute_reply": "2024-07-05T03:26:17.784149Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.779949Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "if False:#len(test) < 10: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    # params['subsample'] = 2\n",
    "    params['fold'] = -1\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1\n",
    "params['n_lgb'] = 1\n",
    "params['model'] = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.786808Z",
     "iopub.status.busy": "2024-07-05T03:26:17.786464Z",
     "iopub.status.idle": "2024-07-05T03:26:17.798101Z",
     "shell.execute_reply": "2024-07-05T03:26:17.79707Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.786777Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0\n",
    "N_FOLDS = int(params.get('n_folds', 3)); \n",
    "FOLD = int(params.get('fold', 0))\n",
    "SEED = int(params.get('seed', 3))\n",
    "SS = int(params.get('subsample', 1))\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.799617Z",
     "iopub.status.busy": "2024-07-05T03:26:17.799332Z",
     "iopub.status.idle": "2024-07-05T03:26:17.824537Z",
     "shell.execute_reply": "2024-07-05T03:26:17.823635Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.799593Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n",
    "if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.825901Z",
     "iopub.status.busy": "2024-07-05T03:26:17.825663Z",
     "iopub.status.idle": "2024-07-05T03:26:17.830321Z",
     "shell.execute_reply": "2024-07-05T03:26:17.829485Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.825881Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_strings(x, ):\n",
    "    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.831869Z",
     "iopub.status.busy": "2024-07-05T03:26:17.831631Z",
     "iopub.status.idle": "2024-07-05T03:26:17.839093Z",
     "shell.execute_reply": "2024-07-05T03:26:17.838221Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.831849Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings(x, ):\n",
    "    return len(join_strings(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.840214Z",
     "iopub.status.busy": "2024-07-05T03:26:17.839938Z",
     "iopub.status.idle": "2024-07-05T03:26:17.849138Z",
     "shell.execute_reply": "2024-07-05T03:26:17.848218Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.840184Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings_j(x):\n",
    "    x = json.loads(x)\n",
    "    return len_join_strings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.850794Z",
     "iopub.status.busy": "2024-07-05T03:26:17.850445Z",
     "iopub.status.idle": "2024-07-05T03:26:17.859198Z",
     "shell.execute_reply": "2024-07-05T03:26:17.858439Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.850764Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.860595Z",
     "iopub.status.busy": "2024-07-05T03:26:17.860334Z",
     "iopub.status.idle": "2024-07-05T03:26:17.873495Z",
     "shell.execute_reply": "2024-07-05T03:26:17.872508Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.860573Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN = True and not KAGGLE\n",
    "TRAIN = False\n",
    "INFER = True # or KAGGLE \n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.875032Z",
     "iopub.status.busy": "2024-07-05T03:26:17.874756Z",
     "iopub.status.idle": "2024-07-05T03:26:20.474174Z",
     "shell.execute_reply": "2024-07-05T03:26:20.473348Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.875011Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:20.475474Z",
     "iopub.status.busy": "2024-07-05T03:26:20.47522Z",
     "iopub.status.idle": "2024-07-05T03:26:20.480687Z",
     "shell.execute_reply": "2024-07-05T03:26:20.479612Z",
     "shell.execute_reply.started": "2024-07-05T03:26:20.47545Z"
    }
   },
   "outputs": [],
   "source": [
    "LGB = True\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n",
    "INFER_LGB = not TRAIN and LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:20.48293Z",
     "iopub.status.busy": "2024-07-05T03:26:20.48218Z",
     "iopub.status.idle": "2024-07-05T03:26:29.993702Z",
     "shell.execute_reply": "2024-07-05T03:26:29.992737Z",
     "shell.execute_reply.started": "2024-07-05T03:26:20.482897Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:29.99511Z",
     "iopub.status.busy": "2024-07-05T03:26:29.994843Z",
     "iopub.status.idle": "2024-07-05T03:26:31.218575Z",
     "shell.execute_reply": "2024-07-05T03:26:31.217496Z",
     "shell.execute_reply.started": "2024-07-05T03:26:29.995087Z"
    }
   },
   "outputs": [],
   "source": [
    "def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())\n",
    "    x = symlog(x)\n",
    "    return x\n",
    "\n",
    "def get_features(df):\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    v = np.hstack([\n",
    "    # pfeat, \n",
    "          afeat - bfeat, np.abs(afeat - bfeat), \n",
    "    # afeat + bfeat\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n",
    "    except: pass\n",
    "\n",
    "    extras = []\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)\n",
    "            \n",
    "    extras.append(df[c].str.len())\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n",
    "    return np.hstack([v, extras])\n",
    "    # return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.220049Z",
     "iopub.status.busy": "2024-07-05T03:26:31.219725Z",
     "iopub.status.idle": "2024-07-05T03:26:31.348239Z",
     "shell.execute_reply": "2024-07-05T03:26:31.347217Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.220019Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.349678Z",
     "iopub.status.busy": "2024-07-05T03:26:31.349385Z",
     "iopub.status.idle": "2024-07-05T03:26:31.398862Z",
     "shell.execute_reply": "2024-07-05T03:26:31.397897Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.349653Z"
    }
   },
   "outputs": [],
   "source": [
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test\n",
    "    yps = []; b = 1000\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])\n",
    "        ypms = []\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))\n",
    "        yps.append(np.stack(ypms).mean(0))\n",
    "        # break;\n",
    "        print('.', end = '')\n",
    "        \n",
    "        if len(yps) % 2 == 0:\n",
    "            gc.collect()\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.400185Z",
     "iopub.status.busy": "2024-07-05T03:26:31.3999Z",
     "iopub.status.idle": "2024-07-05T03:26:31.404215Z",
     "shell.execute_reply": "2024-07-05T03:26:31.403205Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.400143Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_preds = yp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ca95de",
   "metadata": {},
   "source": [
    "## Blend predictions\n",
    "\n",
    "$\\operatorname{preds} = 0.2 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.406004Z",
     "iopub.status.busy": "2024-07-05T03:26:31.405649Z",
     "iopub.status.idle": "2024-07-05T03:26:31.414649Z",
     "shell.execute_reply": "2024-07-05T03:26:31.413643Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.405973Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_wt = 0.2 \n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.415752Z",
     "iopub.status.busy": "2024-07-05T03:26:31.41551Z",
     "iopub.status.idle": "2024-07-05T03:26:31.430607Z",
     "shell.execute_reply": "2024-07-05T03:26:31.42974Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.415727Z"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(preds, \n",
    "                index = df.id, \n",
    "                    columns = train.columns[-3:])\n",
    "display(out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.435887Z",
     "iopub.status.busy": "2024-07-05T03:26:31.435492Z",
     "iopub.status.idle": "2024-07-05T03:26:31.444213Z",
     "shell.execute_reply": "2024-07-05T03:26:31.443233Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.435861Z"
    }
   },
   "outputs": [],
   "source": [
    "out.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
