{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e1a63b",
   "metadata": {},
   "source": [
    "# LMSYS | XGB Baseline\n",
    "\n",
    "(original notebook: https://www.kaggle.com/code/sercanyesiloz/lmsys-xgb-baseline)\n",
    "\n",
    "# 1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-14T20:14:37.923824Z",
     "iopub.status.busy": "2024-07-14T20:14:37.922961Z",
     "iopub.status.idle": "2024-07-14T20:14:37.929988Z",
     "shell.execute_reply": "2024-07-14T20:14:37.928836Z",
     "shell.execute_reply.started": "2024-07-14T20:14:37.923789Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d26eaf",
   "metadata": {},
   "source": [
    "# 2. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T20:14:37.932291Z",
     "iopub.status.busy": "2024-07-14T20:14:37.93187Z",
     "iopub.status.idle": "2024-07-14T20:14:37.939317Z",
     "shell.execute_reply": "2024-07-14T20:14:37.938498Z",
     "shell.execute_reply.started": "2024-07-14T20:14:37.932255Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e34b45",
   "metadata": {},
   "source": [
    "# 3. Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T20:14:37.9702Z",
     "iopub.status.busy": "2024-07-14T20:14:37.969439Z",
     "iopub.status.idle": "2024-07-14T20:14:39.831322Z",
     "shell.execute_reply": "2024-07-14T20:14:39.830418Z",
     "shell.execute_reply.started": "2024-07-14T20:14:37.970176Z"
    }
   },
   "outputs": [],
   "source": [
    "# Read the training, test, and sample submission datasets from the specified paths\n",
    "train = pd.read_csv(config.train_path)\n",
    "test = pd.read_csv(config.test_path)\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "# If the test dataset has fewer than 10 rows, limit the training dataset to the first 10,000 rows\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "\n",
    "# Define a function to process strings by removing brackets and splitting sentences\n",
    "# NOTE: Another way would be to convert to JSON and then join, but this is probably most efficient in Python\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # Remove leading and trailing square brackets\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # Split by \",\" and remove surrounding quotes\n",
    "    return ' '.join(sentences)  # Join the sentences with a space\n",
    "\n",
    "# Apply the `process` function to the prompt and response columns in the train dataset\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "# Apply the `process` function to the prompt and response columns in the test dataset\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "# Print the shapes of the train and test datasets\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Print the total number of missing values in the train and test datasets\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "# Display the first few rows of the train dataset\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09727dd1",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:05:22.146047Z",
     "iopub.status.busy": "2024-07-14T21:05:22.145693Z",
     "iopub.status.idle": "2024-07-14T21:05:22.181055Z",
     "shell.execute_reply": "2024-07-14T21:05:22.180186Z",
     "shell.execute_reply.started": "2024-07-14T21:05:22.146017Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    # Calculate cosine similarity between two texts\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Create a TF-IDF vectorizer (word-importance) with n-grams from 1 to 3\n",
    "            vectorizer.fit([text1, text2])  # Fit the vectorizer on both texts\n",
    "            output = vectorizer.transform([text1, text2]).toarray()  # Transform texts to TF-IDF vectors\n",
    "            cos_sim = cosine_similarity(output)  # Calculate cosine similarity between vectors\n",
    "            return cos_sim[0][1]  # Return the similarity score between text1 and text2\n",
    "        except:\n",
    "            print(f\"cosine_sim exception with '{text1}' and '{text2}'\")\n",
    "            return np.nan  # Return NaN in case of an exception\n",
    "\n",
    "    # Calculate Jaccard similarity between two texts\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # Split text1 into set of words\n",
    "        set2 = set(text2.split())  # Split text2 into set of words\n",
    "        intersection = set1.intersection(set2)  # Find intersection of both sets\n",
    "        union = set1.union(set2)  # Find union of both sets\n",
    "        return len(intersection) / len(union)  # Return Jaccard similarity score\n",
    "    \n",
    "    # Count the number of quoted segments in a text\n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # Pattern for single quotes\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # Pattern for double quotes\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # Find all single-quoted segments\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # Find all double-quoted segments\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # Sum the counts of both types of quotes\n",
    "        return total_quotes  # Return the total count of quoted segments\n",
    "    \n",
    "    # Count the number of new-lines in a text\n",
    "    def count_new_lines(self, text: str) -> int:\n",
    "        return text.count('\\\\n')  # Return the count of newline characters in the text\n",
    "    \n",
    "    # Count the number of bulleted lists in the text\n",
    "    def count_bulleted_lists(self, text: str) -> int:\n",
    "        bullet_pattern = r'(\\\\n|^)[\\*\\-\\+]\\s'  # Pattern for bulleted list items\n",
    "        return len(re.findall(bullet_pattern, text))  # Return the count of bulleted list items\n",
    "    \n",
    "    # Tokenize text into lowercase words\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    # Generate n-grams from the tokenized text\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # Tokenize the text\n",
    "        return list(ngrams(tokens, n))  # Generate n-grams from tokens\n",
    "\n",
    "    # Count overlapping n-grams between two texts\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # Generate n-grams for text1\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # Generate n-grams for text2\n",
    "            counter1 = Counter(ngrams1)  # Count n-grams in text1\n",
    "            counter2 = Counter(ngrams2)  # Count n-grams in text2\n",
    "            overlap = counter1 & counter2  # Find the overlap between the two counters\n",
    "            overlap_count = sum(overlap.values())  # Sum the counts of overlapping n-grams\n",
    "            return overlap_count  # Return the overlap count\n",
    "        except:\n",
    "            return 0  # Return 0 in case of an exception\n",
    "        \n",
    "    # Run preprocessing on the data\n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # Calculate unigram, bigram, and trigram overlaps between response_a and response_b\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        # Calculate unigram, bigram, and trigram overlaps between response_a and prompt\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        # Calculate unigram, bigram, and trigram overlaps between response_b and prompt\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        # Calculate the length of tokenized texts\n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        \n",
    "        # Calculate length ratios between response_a, response_b, and prompt\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]\n",
    "        \n",
    "        # Calculate length differences between response_a, response_b, and prompt\n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]\n",
    "        \n",
    "        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_a and prompt\n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_b and prompt\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        # Count the number of quotes in response_a, response_b, and prompt\n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))\n",
    "\n",
    "        # Count the number of new-lines in response_a, response_b, and prompt\n",
    "        data[\"respa_new_lines\"] = data[\"response_a\"].apply(lambda x: self.count_new_lines(x))\n",
    "        data[\"respb_new_lines\"] = data[\"response_b\"].apply(lambda x: self.count_new_lines(x))\n",
    "        data[\"prompt_new_lines\"] = data[\"prompt\"].apply(lambda x: self.count_new_lines(x))\n",
    "\n",
    "        # Count the number of bulleted lists in response_a, response_b, and prompt\n",
    "        data[\"respa_bullets\"] = data[\"response_a\"].apply(lambda x: self.count_bulleted_lists(x))\n",
    "        data[\"respb_bullets\"] = data[\"response_b\"].apply(lambda x: self.count_bulleted_lists(x))\n",
    "        data[\"prompt_bullets\"] = data[\"prompt\"].apply(lambda x: self.count_bulleted_lists(x))\n",
    "        \n",
    "        # Calculate cosine and Jaccard similarities between response_a and response_b\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        # Calculate cosine and Jaccard similarities between response_a and prompt\n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        # Calculate cosine and Jaccard similarities between response_b and prompt\n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data  # Return the processed dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:05:41.99047Z",
     "iopub.status.busy": "2024-07-14T21:05:41.98966Z",
     "iopub.status.idle": "2024-07-14T21:16:22.019971Z",
     "shell.execute_reply": "2024-07-14T21:16:22.019142Z",
     "shell.execute_reply.started": "2024-07-14T21:05:41.99042Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "train = preprocessor.run(train)\n",
    "test = preprocessor.run(test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:16:53.578669Z",
     "iopub.status.busy": "2024-07-14T21:16:53.57794Z",
     "iopub.status.idle": "2024-07-14T21:16:53.609194Z",
     "shell.execute_reply": "2024-07-14T21:16:53.608166Z",
     "shell.execute_reply.started": "2024-07-14T21:16:53.578632Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of columns to drop from the dataset\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]\n",
    "\n",
    "# List of target columns indicating the winner\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# Name of the final target column\n",
    "target = \"target\"\n",
    "\n",
    "# Initialize the target column with NaN values\n",
    "train[target] = np.nan\n",
    "\n",
    "# Iterate over the target columns and set the corresponding index in the target column\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx  # Set target column to the index where target column value is 1\n",
    "\n",
    "# Convert the target column to integer type\n",
    "train[target] = train[target].astype(\"int32\")\n",
    "\n",
    "# Display the first few rows of the updated dataframe\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180ad568",
   "metadata": {},
   "source": [
    "# 5. Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:17:33.095261Z",
     "iopub.status.busy": "2024-07-14T21:17:33.094902Z",
     "iopub.status.idle": "2024-07-14T21:17:33.117348Z",
     "shell.execute_reply": "2024-07-14T21:17:33.116641Z",
     "shell.execute_reply.started": "2024-07-14T21:17:33.095232Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop specified columns from the training dataset and assign the result to X\n",
    "X = train.drop(columns=target_cols + drop_cols + [target] + [\"model_a\", \"model_b\"], axis=1)\n",
    "\n",
    "# Assign the target column to y\n",
    "y = train[target]\n",
    "\n",
    "# Drop specified columns from the test dataset and assign the result to X_test\n",
    "X_test = test.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "# Replace infinite values (-inf and inf) with NaN in the training feature set\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "\n",
    "# Replace infinite values (-inf and inf) with NaN in the test feature set\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:18:48.99339Z",
     "iopub.status.busy": "2024-07-14T21:18:48.993015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up stratified cross-validation with the specified number of splits, shuffle, and seed\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# Initialize an array to store the average predictions for the test set\n",
    "test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))\n",
    "\n",
    "# Initialize a list to store cross-validation scores (log loss) for each fold\n",
    "cv_scores = list()\n",
    "\n",
    "# Get the list of feature names\n",
    "features = X.columns.tolist()\n",
    "\n",
    "# Prepare a DataFrame to store feature importances for each fold\n",
    "feat_imp_df = pd.DataFrame({\"feature\": features})\n",
    "\n",
    "# Loop over each fold in the cross-validation\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"| Fold {idx+1} |\".center(90, \"=\"))  # Print the fold number\n",
    "\n",
    "    # Split the data into training and validation sets for the current fold\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_val, y_val = X.loc[val_idx], y.loc[val_idx]\n",
    "\n",
    "    # Print the shapes of the training and validation sets\n",
    "    print(f'train: {X_train.shape}')\n",
    "    print(f'val: {X_val.shape}')\n",
    "    \n",
    "    # Initialize the XGBoost classifier with specified hyperparameters\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=3,\n",
    "        eval_metric='mlogloss',\n",
    "        subsample=0.8,\n",
    "        n_estimators=650,\n",
    "        learning_rate=0.045,\n",
    "        max_depth=5,\n",
    "        random_state=config.seed,\n",
    "        device=\"gpu\"\n",
    "    )\n",
    "    \n",
    "    # Train the model with early stopping on the validation set\n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        early_stopping_rounds=75,\n",
    "        verbose=75\n",
    "    )\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    val_preds = model.predict_proba(X_val)\n",
    "\n",
    "    # Calculate the log loss for the validation set\n",
    "    val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")\n",
    "    print(f\"val log loss: {val_log_loss:.5f}\")\n",
    "\n",
    "    # Append the log loss to the list of cross-validation scores\n",
    "    cv_scores.append(val_log_loss)\n",
    "    \n",
    "    # Update test predictions with the current fold's predictions, averaged over all folds\n",
    "    test_preds += model.predict_proba(X_test) / cv.get_n_splits()\n",
    "    \n",
    "    # Merge the current fold's feature importances into the DataFrame\n",
    "    feat_imp_df = feat_imp_df.merge(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": features,\n",
    "                f\"fold_{idx+1}_feat_imp\": model.feature_importances_,\n",
    "            }\n",
    "        ),\n",
    "        on=[\"feature\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "# Print a separator line and the average cross-validated log loss\n",
    "print(\"=\"*90)\n",
    "print(f\"CV: {np.mean(cv_scores):.5f}\")\n",
    "\n",
    "# Calculate the average feature importance across all folds\n",
    "feat_imp_df[\"avg_importance\"] = feat_imp_df.iloc[:, 1:].mean(axis=1)\n",
    "\n",
    "# Plot the top 50 features by average importance\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(\n",
    "    data=feat_imp_df.sort_values(by=\"avg_importance\", ascending=False).iloc[\n",
    "        :50\n",
    "    ],\n",
    "    x=\"avg_importance\",\n",
    "    y=\"feature\",\n",
    "    color=\"royalblue\",\n",
    "    width=0.75,\n",
    ")\n",
    "plt.title(\"Average Feature Importances for All Folds\", size=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df744b05",
   "metadata": {},
   "source": [
    "# 6. Saving Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:19:29.815185Z",
     "iopub.status.busy": "2024-07-14T21:19:29.814539Z",
     "iopub.status.idle": "2024-07-14T21:19:29.825732Z",
     "shell.execute_reply": "2024-07-14T21:19:29.824857Z",
     "shell.execute_reply.started": "2024-07-14T21:19:29.815156Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-14T21:19:35.805085Z",
     "iopub.status.busy": "2024-07-14T21:19:35.804732Z",
     "iopub.status.idle": "2024-07-14T21:19:35.811097Z",
     "shell.execute_reply": "2024-07-14T21:19:35.810075Z",
     "shell.execute_reply.started": "2024-07-14T21:19:35.805058Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
