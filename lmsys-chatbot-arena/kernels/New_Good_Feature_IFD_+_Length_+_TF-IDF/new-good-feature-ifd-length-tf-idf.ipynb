{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72963460",
   "metadata": {},
   "source": [
    "## Refer: [IFD](https://github.com/tianyi-lab/Superfiltering)\n",
    "$$\\mathrm{IFD}_\\theta(Q,A)=\\frac{\\mathrm{PPL}_\\theta(A|Q)}{\\mathrm{PPL}_\\theta(A)}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.328936Z",
     "iopub.status.busy": "2024-07-02T11:20:07.328566Z",
     "iopub.status.idle": "2024-07-02T11:20:07.337874Z",
     "shell.execute_reply": "2024-07-02T11:20:07.33688Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.328911Z"
    },
    "papermill": {
     "duration": 4.432239,
     "end_time": "2024-05-07T00:30:21.264459",
     "exception": false,
     "start_time": "2024-05-07T00:30:16.83222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import itertools\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import joblib\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.340504Z",
     "iopub.status.busy": "2024-07-02T11:20:07.340064Z",
     "iopub.status.idle": "2024-07-02T11:20:07.352823Z",
     "shell.execute_reply": "2024-07-02T11:20:07.351793Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.340472Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024\n",
    "deberta_path = \"/kaggle/input/debertav3base\"\n",
    "gpt_path = \"/kaggle/input/qwen2-1-5b-instruct\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.354644Z",
     "iopub.status.busy": "2024-07-02T11:20:07.354153Z",
     "iopub.status.idle": "2024-07-02T11:20:09.08421Z",
     "shell.execute_reply": "2024-07-02T11:20:09.083316Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.354612Z"
    },
    "papermill": {
     "duration": 4.40411,
     "end_time": "2024-05-07T00:30:25.698148",
     "exception": false,
     "start_time": "2024-05-07T00:30:21.294038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "vectorize_on_train_and_test = True\n",
    "#quick_test for training on small part of train data (and not using bunch of GPU on submit)\n",
    "#(if this is on - saved models won't be fully trained)\n",
    "quick_test = True\n",
    "quick_test_items = 1000\n",
    "#automatically disable quick_test if we detect actual test data... (assures full training when scoring)\n",
    "if (len(test)) > 3:quick_test = False\n",
    "if quick_test: train = train.head(quick_test_items)\n",
    "\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")\n",
    "print('Total train samples: ', len(train))\n",
    "\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n",
    "train.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7db5f6",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:09.085923Z",
     "iopub.status.busy": "2024-07-02T11:20:09.085638Z",
     "iopub.status.idle": "2024-07-02T11:20:09.105478Z",
     "shell.execute_reply": "2024-07-02T11:20:09.104656Z",
     "shell.execute_reply.started": "2024-07-02T11:20:09.085899Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "if vectorize_on_train_and_test:\n",
    "    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)\n",
    "else:\n",
    "    vector_fit_text = train_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:09.107995Z",
     "iopub.status.busy": "2024-07-02T11:20:09.10771Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'[^\\W]+', text)\n",
    "\n",
    "#word-level vectorizer\n",
    "tfidf_word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),\n",
    "    tokenizer=custom_tokenizer,\n",
    "    token_pattern=None,\n",
    "    strip_accents='unicode',\n",
    "    min_df=4,\n",
    "    max_features=300\n",
    ")\n",
    "\n",
    "#char-level vectorizer\n",
    "tfidf_char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5), \n",
    "    max_features=1000, \n",
    "    min_df=4\n",
    ")\n",
    "\n",
    "def batch_process(texts, batch_size):\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        yield texts[i:i + batch_size]\n",
    "\n",
    "#doing in batches so we can see progress\n",
    "batch_size = 1000\n",
    "for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):\n",
    "    if len(batch) >= tfidf_word_vectorizer.min_df:\n",
    "        tfidf_word_vectorizer.fit(batch)\n",
    "    if len(batch) >= tfidf_char_vectorizer.min_df:\n",
    "        tfidf_char_vectorizer.fit(batch)\n",
    "        \n",
    "def get_tfidf_vectors(df):\n",
    "    vectorized_columns = []\n",
    "    for column in columns_to_vectorize:\n",
    "        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))\n",
    "        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))\n",
    "    return hstack(vectorized_columns)\n",
    "\n",
    "tfidf_train_vectors = get_tfidf_vectors(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e8a5ac",
   "metadata": {},
   "source": [
    "### LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_none(vals) -> int:\n",
    "    # some responses contains null and probably they are useful for prediction\n",
    "    return int(any(val is None for val in vals))\n",
    "\n",
    "def str_length(vals) -> int:\n",
    "    length = 0\n",
    "    for val in vals:\n",
    "        if isinstance(val, str):\n",
    "            length += len(val)\n",
    "    return length\n",
    "\n",
    "def get_length_features(data: pd.DataFrame):\n",
    "    length_feature_array = []\n",
    "    length_feature_array.append(data[\"response_a\"].apply(str_length))\n",
    "    length_feature_array.append(data[\"response_b\"].apply(str_length))\n",
    "    length_feature_array.append(length_feature_array[0] - length_feature_array[1])\n",
    "    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)\n",
    "    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none))\n",
    "    length_feature_array.append(data[\"response_b\"].apply(has_none))\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none) - data[\"response_b\"].apply(has_none))\n",
    "    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)\n",
    "    length_feature_array = np.transpose(length_feature_array, (1, 0))\n",
    "    return length_feature_array\n",
    "train_length_features = get_length_features(train)\n",
    "print(train_length_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfd059b",
   "metadata": {},
   "source": [
    "### IFD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ifd_features(data: pd.DataFrame):\n",
    "    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gpt_path)\n",
    "    model.eval()\n",
    "    def get_ppl_features(output, instruct=''):\n",
    "        try:\n",
    "            answer_start_index = 0\n",
    "            if instruct != '':\n",
    "                answer_start_index = tokenizer.encode(instruct, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).shape[1]\n",
    "            input_ids = tokenizer.encode(instruct + output, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "            labels = input_ids.clone()\n",
    "            labels[:, :answer_start_index] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=labels)\n",
    "            perplexity = torch.exp(outputs.loss)\n",
    "            return perplexity.to('cpu').item()\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    ppl_ifd_feature_array = []\n",
    "    for i in tqdm(range(len(data)), desc=\"Scoring ifd\"):\n",
    "        instruct = data['prompt'][i]\n",
    "        output_a = data['response_a'][i]\n",
    "        output_b = data['response_b'][i]\n",
    "        \n",
    "        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)\n",
    "        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)\n",
    "        try:\n",
    "            ifd_a = ppl_ca_a / ppl_da_a\n",
    "        except ZeroDivisionError:\n",
    "            ifd_a = 0\n",
    "        try:\n",
    "            ifd_b = ppl_ca_b / ppl_da_b\n",
    "        except ZeroDivisionError:\n",
    "            ifd_b = 0\n",
    "        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])\n",
    "    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)\n",
    "    return ppl_ifd_feature_array\n",
    "ppl_ifd_features = get_ifd_features(train)\n",
    "print(ppl_ifd_features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903f48b",
   "metadata": {},
   "source": [
    "### Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2\n",
    "ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7\n",
    "train_length_features_csr = csr_matrix(train_length_features)*0.1\n",
    "combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])\n",
    "print(combined_train_vectors.shape)\n",
    "print(\"Vectorizing test text...\")\n",
    "tfidf_test_vectors = get_tfidf_vectors(test)\n",
    "tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2\n",
    "test_ppl_ifd_features = get_ifd_features(test)\n",
    "test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7\n",
    "test_length_features = get_length_features(test)\n",
    "test_length_features_csr = csr_matrix(test_length_features)*0.1\n",
    "combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr]) \n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1122.903557,
     "end_time": "2024-05-07T01:06:29.367194",
     "exception": false,
     "start_time": "2024-05-07T00:47:46.463637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "max_estimators = 1000\n",
    "early_stopping_limit = 100\n",
    "\n",
    "# Data preparation\n",
    "X = combined_train_vectors\n",
    "y_encoded = train['label'].values\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'n_estimators': max_estimators,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'random_state': 42,\n",
    "    'learning_rate': 0.03,\n",
    "    'verbose': -1  # keep logs quiet\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "logloss_scores = []\n",
    "accuracy_scores = []\n",
    "test_pred_list = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "    X_train_fold, X_val_fold = X[train_indices], X[val_indices]\n",
    "    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]\n",
    "\n",
    "    def callback(env):\n",
    "        if env.iteration % 10 == 0: print (\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]\n",
    "    )\n",
    "\n",
    "    y_pred_proba_fold = model.predict_proba(X_val_fold)\n",
    "    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)\n",
    "    logloss_scores.append(logloss_fold)\n",
    "    print(f\"Log Loss: {logloss_fold}\")\n",
    "    \n",
    "    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)\n",
    "    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)\n",
    "    accuracy_scores.append(accuracy_fold)\n",
    "    print(f\"Accuracy: {accuracy_fold}\")\n",
    "\n",
    "    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))\n",
    "\n",
    "# Calculate and print average scores\n",
    "average_logloss = np.mean(logloss_scores)\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "print(f\"\\nAverage Log Loss: {average_logloss}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.106303,
     "end_time": "2024-05-07T01:06:34.373427",
     "exception": false,
     "start_time": "2024-05-07T01:06:34.267124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_test = np.mean(test_pred_list, axis=0)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test[\"id\"],\n",
    "    'winner_model_a': preds_test[:, 0],\n",
    "    'winner_model_b': preds_test[:, 1], \n",
    "    'winner_tie': preds_test[:, 2]\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 2210196,
     "sourceId": 3693646,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5306724,
     "sourceId": 8821100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5306732,
     "sourceId": 8821109,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5313994,
     "sourceId": 8831552,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5314871,
     "sourceId": 8832702,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2185.862577,
   "end_time": "2024-05-07T01:06:39.269064",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-07T00:30:13.406487",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
