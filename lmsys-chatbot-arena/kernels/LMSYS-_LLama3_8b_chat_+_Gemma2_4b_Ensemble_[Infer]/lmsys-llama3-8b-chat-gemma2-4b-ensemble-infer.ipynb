{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-26T08:18:49.996957Z",
     "iopub.status.busy": "2024-07-26T08:18:49.996574Z",
     "iopub.status.idle": "2024-07-26T08:19:06.27664Z",
     "shell.execute_reply": "2024-07-26T08:19:06.275735Z",
     "shell.execute_reply.started": "2024-07-26T08:18:49.996928Z"
    },
    "papermill": {
     "duration": 31.479497,
     "end_time": "2024-07-10T01:13:41.690971",
     "exception": false,
     "start_time": "2024-07-10T01:13:10.211474",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\-U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T08:19:10.515174Z",
     "iopub.status.busy": "2024-07-26T08:19:10.51481Z",
     "iopub.status.idle": "2024-07-26T08:19:59.863019Z",
     "shell.execute_reply": "2024-07-26T08:19:59.861931Z",
     "shell.execute_reply.started": "2024-07-26T08:19:10.515142Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bc6088",
   "metadata": {},
   "source": [
    "# PART 1 (LLama 3 8b Chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:08:53.562705Z",
     "iopub.status.busy": "2024-07-24T12:08:53.562325Z",
     "iopub.status.idle": "2024-07-24T12:09:03.686073Z",
     "shell.execute_reply": "2024-07-24T12:09:03.685309Z",
     "shell.execute_reply.started": "2024-07-24T12:08:53.562667Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:03.688537Z",
     "iopub.status.busy": "2024-07-24T12:09:03.688058Z",
     "iopub.status.idle": "2024-07-24T12:09:15.236577Z",
     "shell.execute_reply": "2024-07-24T12:09:15.235721Z",
     "shell.execute_reply.started": "2024-07-24T12:09:03.688508Z"
    },
    "papermill": {
     "duration": 19.200405,
     "end_time": "2024-07-10T01:14:00.90474",
     "exception": false,
     "start_time": "2024-07-10T01:13:41.704335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:15.238201Z",
     "iopub.status.busy": "2024-07-24T12:09:15.237656Z",
     "iopub.status.idle": "2024-07-24T12:09:15.273581Z",
     "shell.execute_reply": "2024-07-24T12:09:15.272685Z",
     "shell.execute_reply.started": "2024-07-24T12:09:15.238174Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "MAX_LENGTH = 1284\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:15.274901Z",
     "iopub.status.busy": "2024-07-24T12:09:15.274624Z",
     "iopub.status.idle": "2024-07-24T12:09:15.294576Z",
     "shell.execute_reply": "2024-07-24T12:09:15.293899Z",
     "shell.execute_reply.started": "2024-07-24T12:09:15.274875Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:15.295891Z",
     "iopub.status.busy": "2024-07-24T12:09:15.295592Z",
     "iopub.status.idle": "2024-07-24T12:09:15.327817Z",
     "shell.execute_reply": "2024-07-24T12:09:15.326985Z",
     "shell.execute_reply.started": "2024-07-24T12:09:15.295861Z"
    }
   },
   "outputs": [],
   "source": [
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81716603",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:15.329386Z",
     "iopub.status.busy": "2024-07-24T12:09:15.329081Z",
     "iopub.status.idle": "2024-07-24T12:09:16.0095Z",
     "shell.execute_reply": "2024-07-24T12:09:16.008542Z",
     "shell.execute_reply.started": "2024-07-24T12:09:15.329361Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5690c62",
   "metadata": {},
   "source": [
    "## Load model \n",
    "> We load 1 model on each gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:09:16.01138Z",
     "iopub.status.busy": "2024-07-24T12:09:16.010948Z",
     "iopub.status.idle": "2024-07-24T12:10:53.114572Z",
     "shell.execute_reply": "2024-07-24T12:10:53.113618Z",
     "shell.execute_reply.started": "2024-07-24T12:09:16.011331Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:31.153653Z",
     "iopub.status.busy": "2024-07-24T12:11:31.153247Z",
     "iopub.status.idle": "2024-07-24T12:11:45.396074Z",
     "shell.execute_reply": "2024-07-24T12:11:45.395106Z",
     "shell.execute_reply.started": "2024-07-24T12:11:31.153622Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:10:53.118235Z",
     "iopub.status.busy": "2024-07-24T12:10:53.117943Z",
     "iopub.status.idle": "2024-07-24T12:10:53.123104Z",
     "shell.execute_reply": "2024-07-24T12:10:53.122302Z",
     "shell.execute_reply.started": "2024-07-24T12:10:53.118209Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRa configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:45.397943Z",
     "iopub.status.busy": "2024-07-24T12:11:45.397643Z",
     "iopub.status.idle": "2024-07-24T12:11:45.913842Z",
     "shell.execute_reply": "2024-07-24T12:11:45.912826Z",
     "shell.execute_reply.started": "2024-07-24T12:11:45.397917Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "#Load weights\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()\n",
    "\n",
    "#Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:45.915588Z",
     "iopub.status.busy": "2024-07-24T12:11:45.915194Z",
     "iopub.status.idle": "2024-07-24T12:11:46.223001Z",
     "shell.execute_reply": "2024-07-24T12:11:46.222145Z",
     "shell.execute_reply.started": "2024-07-24T12:11:45.915555Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246ecee7",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:46.225558Z",
     "iopub.status.busy": "2024-07-24T12:11:46.225199Z",
     "iopub.status.idle": "2024-07-24T12:11:46.235248Z",
     "shell.execute_reply": "2024-07-24T12:11:46.234307Z",
     "shell.execute_reply.started": "2024-07-24T12:11:46.225521Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:46.236656Z",
     "iopub.status.busy": "2024-07-24T12:11:46.236384Z",
     "iopub.status.idle": "2024-07-24T12:11:46.253432Z",
     "shell.execute_reply": "2024-07-24T12:11:46.252574Z",
     "shell.execute_reply.started": "2024-07-24T12:11:46.236634Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:46.254967Z",
     "iopub.status.busy": "2024-07-24T12:11:46.254637Z",
     "iopub.status.idle": "2024-07-24T12:11:49.096809Z",
     "shell.execute_reply": "2024-07-24T12:11:49.095876Z",
     "shell.execute_reply.started": "2024-07-24T12:11:46.254942Z"
    }
   },
   "outputs": [],
   "source": [
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:49.098847Z",
     "iopub.status.busy": "2024-07-24T12:11:49.098545Z",
     "iopub.status.idle": "2024-07-24T12:11:49.103965Z",
     "shell.execute_reply": "2024-07-24T12:11:49.103005Z",
     "shell.execute_reply.started": "2024-07-24T12:11:49.098822Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467f8a19",
   "metadata": {},
   "source": [
    "## LGBM + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:51.044986Z",
     "iopub.status.busy": "2024-07-24T12:11:51.044554Z",
     "iopub.status.idle": "2024-07-24T12:11:51.050748Z",
     "shell.execute_reply": "2024-07-24T12:11:51.04996Z",
     "shell.execute_reply.started": "2024-07-24T12:11:51.044954Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'\n",
    "\n",
    "import os\n",
    "RUNPOD = os.path.exists('/workspace/')\n",
    "KAGGLE = not RUNPOD\n",
    "if KAGGLE: print('kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:52.15405Z",
     "iopub.status.busy": "2024-07-24T12:11:52.153451Z",
     "iopub.status.idle": "2024-07-24T12:11:52.161817Z",
     "shell.execute_reply": "2024-07-24T12:11:52.160899Z",
     "shell.execute_reply.started": "2024-07-24T12:11:52.154018Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:53.202169Z",
     "iopub.status.busy": "2024-07-24T12:11:53.200992Z",
     "iopub.status.idle": "2024-07-24T12:11:53.216958Z",
     "shell.execute_reply": "2024-07-24T12:11:53.216098Z",
     "shell.execute_reply.started": "2024-07-24T12:11:53.202129Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "import os\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/ \n",
    "        \n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:54.208891Z",
     "iopub.status.busy": "2024-07-24T12:11:54.208239Z",
     "iopub.status.idle": "2024-07-24T12:11:54.215323Z",
     "shell.execute_reply": "2024-07-24T12:11:54.214327Z",
     "shell.execute_reply.started": "2024-07-24T12:11:54.20886Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = '/kaggle/input/'  \n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n",
    "# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\n",
    "print(MODEL_PATH)\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:55.364878Z",
     "iopub.status.busy": "2024-07-24T12:11:55.364195Z",
     "iopub.status.idle": "2024-07-24T12:11:55.369156Z",
     "shell.execute_reply": "2024-07-24T12:11:55.368236Z",
     "shell.execute_reply.started": "2024-07-24T12:11:55.364845Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:56.363914Z",
     "iopub.status.busy": "2024-07-24T12:11:56.363142Z",
     "iopub.status.idle": "2024-07-24T12:11:59.61088Z",
     "shell.execute_reply": "2024-07-24T12:11:59.609557Z",
     "shell.execute_reply.started": "2024-07-24T12:11:56.36388Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:59.613705Z",
     "iopub.status.busy": "2024-07-24T12:11:59.612829Z",
     "iopub.status.idle": "2024-07-24T12:11:59.618275Z",
     "shell.execute_reply": "2024-07-24T12:11:59.617456Z",
     "shell.execute_reply.started": "2024-07-24T12:11:59.613677Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "if False:#len(test) < 10: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    # params['subsample'] = 2\n",
    "    params['fold'] = -1\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1\n",
    "params['n_lgb'] = 1\n",
    "params['model'] = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:59.620886Z",
     "iopub.status.busy": "2024-07-24T12:11:59.620596Z",
     "iopub.status.idle": "2024-07-24T12:11:59.632249Z",
     "shell.execute_reply": "2024-07-24T12:11:59.631398Z",
     "shell.execute_reply.started": "2024-07-24T12:11:59.620852Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0\n",
    "N_FOLDS = int(params.get('n_folds', 3)); \n",
    "FOLD = int(params.get('fold', 0))\n",
    "SEED = int(params.get('seed', 3))\n",
    "SS = int(params.get('subsample', 1))\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:59.634137Z",
     "iopub.status.busy": "2024-07-24T12:11:59.633871Z",
     "iopub.status.idle": "2024-07-24T12:11:59.66221Z",
     "shell.execute_reply": "2024-07-24T12:11:59.661402Z",
     "shell.execute_reply.started": "2024-07-24T12:11:59.634113Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n",
    "if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:11:59.976431Z",
     "iopub.status.busy": "2024-07-24T12:11:59.976092Z",
     "iopub.status.idle": "2024-07-24T12:11:59.981269Z",
     "shell.execute_reply": "2024-07-24T12:11:59.980159Z",
     "shell.execute_reply.started": "2024-07-24T12:11:59.976405Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_strings(x, ):\n",
    "    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:01.098757Z",
     "iopub.status.busy": "2024-07-24T12:12:01.09814Z",
     "iopub.status.idle": "2024-07-24T12:12:01.105381Z",
     "shell.execute_reply": "2024-07-24T12:12:01.104535Z",
     "shell.execute_reply.started": "2024-07-24T12:12:01.098727Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings(x, ):\n",
    "    return len(join_strings(x).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:02.038287Z",
     "iopub.status.busy": "2024-07-24T12:12:02.037941Z",
     "iopub.status.idle": "2024-07-24T12:12:02.042755Z",
     "shell.execute_reply": "2024-07-24T12:12:02.041845Z",
     "shell.execute_reply.started": "2024-07-24T12:12:02.038241Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings_j(x):\n",
    "    x = json.loads(x)\n",
    "    return len_join_strings(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:02.903973Z",
     "iopub.status.busy": "2024-07-24T12:12:02.903114Z",
     "iopub.status.idle": "2024-07-24T12:12:02.909451Z",
     "shell.execute_reply": "2024-07-24T12:12:02.908312Z",
     "shell.execute_reply.started": "2024-07-24T12:12:02.903938Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:03.970416Z",
     "iopub.status.busy": "2024-07-24T12:12:03.96976Z",
     "iopub.status.idle": "2024-07-24T12:12:03.975137Z",
     "shell.execute_reply": "2024-07-24T12:12:03.974274Z",
     "shell.execute_reply.started": "2024-07-24T12:12:03.970382Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN = True and not KAGGLE\n",
    "TRAIN = False\n",
    "INFER = True # or KAGGLE \n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:04.865127Z",
     "iopub.status.busy": "2024-07-24T12:12:04.864772Z",
     "iopub.status.idle": "2024-07-24T12:12:07.509471Z",
     "shell.execute_reply": "2024-07-24T12:12:07.508461Z",
     "shell.execute_reply.started": "2024-07-24T12:12:04.865099Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:07.512598Z",
     "iopub.status.busy": "2024-07-24T12:12:07.511426Z",
     "iopub.status.idle": "2024-07-24T12:12:17.245555Z",
     "shell.execute_reply": "2024-07-24T12:12:17.244657Z",
     "shell.execute_reply.started": "2024-07-24T12:12:07.512563Z"
    }
   },
   "outputs": [],
   "source": [
    "LGB = True\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n",
    "INFER_LGB = not TRAIN and LGB\n",
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:17.247059Z",
     "iopub.status.busy": "2024-07-24T12:12:17.246735Z",
     "iopub.status.idle": "2024-07-24T12:12:18.471764Z",
     "shell.execute_reply": "2024-07-24T12:12:18.470786Z",
     "shell.execute_reply.started": "2024-07-24T12:12:17.247034Z"
    }
   },
   "outputs": [],
   "source": [
    "def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())\n",
    "    x = symlog(x)\n",
    "    return x\n",
    "\n",
    "def get_features(df):\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    v = np.hstack([\n",
    "    # pfeat, \n",
    "          afeat - bfeat, np.abs(afeat - bfeat), \n",
    "    # afeat + bfeat\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n",
    "    except: pass\n",
    "\n",
    "    extras = []\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)\n",
    "            \n",
    "    extras.append(df[c].str.len())\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n",
    "    return np.hstack([v, extras])\n",
    "    # return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:18.474527Z",
     "iopub.status.busy": "2024-07-24T12:12:18.474066Z",
     "iopub.status.idle": "2024-07-24T12:12:18.598137Z",
     "shell.execute_reply": "2024-07-24T12:12:18.597093Z",
     "shell.execute_reply.started": "2024-07-24T12:12:18.474494Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:18.599906Z",
     "iopub.status.busy": "2024-07-24T12:12:18.599368Z",
     "iopub.status.idle": "2024-07-24T12:12:19.862494Z",
     "shell.execute_reply": "2024-07-24T12:12:19.861488Z",
     "shell.execute_reply.started": "2024-07-24T12:12:18.599877Z"
    }
   },
   "outputs": [],
   "source": [
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test\n",
    "    yps = []; b = 1000\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])\n",
    "        ypms = []\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))\n",
    "        yps.append(np.stack(ypms).mean(0))\n",
    "        # break;\n",
    "        print('.', end = '')\n",
    "        \n",
    "        if len(yps) % 2 == 0:\n",
    "            gc.collect()\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:19.863785Z",
     "iopub.status.busy": "2024-07-24T12:12:19.863509Z",
     "iopub.status.idle": "2024-07-24T12:12:19.868043Z",
     "shell.execute_reply": "2024-07-24T12:12:19.866995Z",
     "shell.execute_reply.started": "2024-07-24T12:12:19.86376Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_preds = yp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:19.869933Z",
     "iopub.status.busy": "2024-07-24T12:12:19.869318Z",
     "iopub.status.idle": "2024-07-24T12:12:19.878749Z",
     "shell.execute_reply": "2024-07-24T12:12:19.877971Z",
     "shell.execute_reply.started": "2024-07-24T12:12:19.869899Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_wt = 0.2 \n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:19.880127Z",
     "iopub.status.busy": "2024-07-24T12:12:19.879866Z",
     "iopub.status.idle": "2024-07-24T12:12:19.897318Z",
     "shell.execute_reply": "2024-07-24T12:12:19.89637Z",
     "shell.execute_reply.started": "2024-07-24T12:12:19.880105Z"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(preds, \n",
    "                index = df.id, \n",
    "                    columns = train.columns[-3:])\n",
    "display(out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599401ce",
   "metadata": {},
   "source": [
    "# Part 2 (Gemma 2 4b QLora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:22.985184Z",
     "iopub.status.busy": "2024-07-24T12:12:22.98444Z",
     "iopub.status.idle": "2024-07-24T12:12:22.989426Z",
     "shell.execute_reply": "2024-07-24T12:12:22.988307Z",
     "shell.execute_reply.started": "2024-07-24T12:12:22.985155Z"
    },
    "papermill": {
     "duration": 0.047799,
     "end_time": "2024-07-10T01:14:00.965921",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.918122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dfbd53",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:25.513642Z",
     "iopub.status.busy": "2024-07-24T12:12:25.513303Z",
     "iopub.status.idle": "2024-07-24T12:12:25.51911Z",
     "shell.execute_reply": "2024-07-24T12:12:25.518107Z",
     "shell.execute_reply.started": "2024-07-24T12:12:25.513611Z"
    },
    "papermill": {
     "duration": 0.021338,
     "end_time": "2024-07-10T01:14:01.000606",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.979268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False \n",
    "    spread_max_length = False \n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84a8b60",
   "metadata": {},
   "source": [
    "## Load & pre-process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:27.335823Z",
     "iopub.status.busy": "2024-07-24T12:12:27.335459Z",
     "iopub.status.idle": "2024-07-24T12:12:27.344047Z",
     "shell.execute_reply": "2024-07-24T12:12:27.343061Z",
     "shell.execute_reply.started": "2024-07-24T12:12:27.335794Z"
    },
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-07-10T01:14:01.06946",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.03979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:28.12208Z",
     "iopub.status.busy": "2024-07-24T12:12:28.121735Z",
     "iopub.status.idle": "2024-07-24T12:12:28.136224Z",
     "shell.execute_reply": "2024-07-24T12:12:28.135461Z",
     "shell.execute_reply.started": "2024-07-24T12:12:28.122052Z"
    },
    "papermill": {
     "duration": 0.040127,
     "end_time": "2024-07-10T01:14:01.12241",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.082283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfbf28ff",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:29.725952Z",
     "iopub.status.busy": "2024-07-24T12:12:29.72561Z",
     "iopub.status.idle": "2024-07-24T12:12:29.734604Z",
     "shell.execute_reply": "2024-07-24T12:12:29.733731Z",
     "shell.execute_reply.started": "2024-07-24T12:12:29.725928Z"
    },
    "papermill": {
     "duration": 0.030237,
     "end_time": "2024-07-10T01:14:01.194318",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.164081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    # TODO: change prompt\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:30.52816Z",
     "iopub.status.busy": "2024-07-24T12:12:30.527078Z",
     "iopub.status.idle": "2024-07-24T12:12:31.43597Z",
     "shell.execute_reply": "2024-07-24T12:12:31.435031Z",
     "shell.execute_reply.started": "2024-07-24T12:12:30.528126Z"
    },
    "papermill": {
     "duration": 1.169844,
     "end_time": "2024-07-10T01:14:02.377579",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.207735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:32.184013Z",
     "iopub.status.busy": "2024-07-24T12:12:32.183193Z",
     "iopub.status.idle": "2024-07-24T12:12:32.190035Z",
     "shell.execute_reply": "2024-07-24T12:12:32.189059Z",
     "shell.execute_reply.started": "2024-07-24T12:12:32.183978Z"
    },
    "papermill": {
     "duration": 0.024759,
     "end_time": "2024-07-10T01:14:02.419091",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.394332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:32.955751Z",
     "iopub.status.busy": "2024-07-24T12:12:32.955045Z",
     "iopub.status.idle": "2024-07-24T12:12:32.961638Z",
     "shell.execute_reply": "2024-07-24T12:12:32.960556Z",
     "shell.execute_reply.started": "2024-07-24T12:12:32.95572Z"
    },
    "papermill": {
     "duration": 0.021982,
     "end_time": "2024-07-10T01:14:02.454045",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.432063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0af9b6",
   "metadata": {},
   "source": [
    "## Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:12:34.573593Z",
     "iopub.status.busy": "2024-07-24T12:12:34.572774Z",
     "iopub.status.idle": "2024-07-24T12:13:59.726653Z",
     "shell.execute_reply": "2024-07-24T12:13:59.725835Z",
     "shell.execute_reply.started": "2024-07-24T12:12:34.573558Z"
    },
    "papermill": {
     "duration": 83.919146,
     "end_time": "2024-07-10T01:15:26.412583",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.493437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f68b3",
   "metadata": {},
   "source": [
    "#### Load LoRA adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:13:59.728541Z",
     "iopub.status.busy": "2024-07-24T12:13:59.728223Z",
     "iopub.status.idle": "2024-07-24T12:14:00.740309Z",
     "shell.execute_reply": "2024-07-24T12:14:00.739503Z",
     "shell.execute_reply.started": "2024-07-24T12:13:59.728515Z"
    },
    "papermill": {
     "duration": 1.265087,
     "end_time": "2024-07-10T01:15:27.719297",
     "exception": false,
     "start_time": "2024-07-10T01:15:26.45421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9230167",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:00.741702Z",
     "iopub.status.busy": "2024-07-24T12:14:00.741429Z",
     "iopub.status.idle": "2024-07-24T12:14:00.750854Z",
     "shell.execute_reply": "2024-07-24T12:14:00.74998Z",
     "shell.execute_reply.started": "2024-07-24T12:14:00.741679Z"
    },
    "papermill": {
     "duration": 0.026726,
     "end_time": "2024-07-10T01:15:27.838497",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.811771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:00.753594Z",
     "iopub.status.busy": "2024-07-24T12:14:00.753162Z",
     "iopub.status.idle": "2024-07-24T12:14:05.056001Z",
     "shell.execute_reply": "2024-07-24T12:14:05.055038Z",
     "shell.execute_reply.started": "2024-07-24T12:14:00.753563Z"
    },
    "papermill": {
     "duration": 4.598663,
     "end_time": "2024-07-10T01:15:32.45234",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.853677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:05.058092Z",
     "iopub.status.busy": "2024-07-24T12:14:05.057356Z",
     "iopub.status.idle": "2024-07-24T12:14:05.065824Z",
     "shell.execute_reply": "2024-07-24T12:14:05.064951Z",
     "shell.execute_reply.started": "2024-07-24T12:14:05.058053Z"
    },
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-07-10T01:15:32.491283",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.466724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:09.437377Z",
     "iopub.status.busy": "2024-07-24T12:14:09.436537Z",
     "iopub.status.idle": "2024-07-24T12:14:09.442595Z",
     "shell.execute_reply": "2024-07-24T12:14:09.441623Z",
     "shell.execute_reply.started": "2024-07-24T12:14:09.437343Z"
    },
    "papermill": {
     "duration": 0.034664,
     "end_time": "2024-07-10T01:15:32.539974",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.50531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:13.111682Z",
     "iopub.status.busy": "2024-07-24T12:14:13.110893Z",
     "iopub.status.idle": "2024-07-24T12:14:13.12977Z",
     "shell.execute_reply": "2024-07-24T12:14:13.128762Z",
     "shell.execute_reply.started": "2024-07-24T12:14:13.111637Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:14:44.823637Z",
     "iopub.status.busy": "2024-07-24T12:14:44.823197Z",
     "iopub.status.idle": "2024-07-24T12:14:44.834115Z",
     "shell.execute_reply": "2024-07-24T12:14:44.833171Z",
     "shell.execute_reply.started": "2024-07-24T12:14:44.82361Z"
    }
   },
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:18:34.915568Z",
     "iopub.status.busy": "2024-07-24T12:18:34.911504Z",
     "iopub.status.idle": "2024-07-24T12:18:34.937622Z",
     "shell.execute_reply": "2024-07-24T12:18:34.936588Z",
     "shell.execute_reply.started": "2024-07-24T12:18:34.915526Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(out, result_df, on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:21:10.389271Z",
     "iopub.status.busy": "2024-07-24T12:21:10.388546Z",
     "iopub.status.idle": "2024-07-24T12:21:10.412403Z",
     "shell.execute_reply": "2024-07-24T12:21:10.411424Z",
     "shell.execute_reply.started": "2024-07-24T12:21:10.389223Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:21:07.31789Z",
     "iopub.status.busy": "2024-07-24T12:21:07.317231Z",
     "iopub.status.idle": "2024-07-24T12:21:07.326201Z",
     "shell.execute_reply": "2024-07-24T12:21:07.32519Z",
     "shell.execute_reply.started": "2024-07-24T12:21:07.317858Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_df['winner_model_a']= (0.7*merged_df['winner_model_a_y']) + (0.3*merged_df['winner_model_a_x'])\n",
    "merged_df['winner_model_b']= (0.7*merged_df['winner_model_b_y']) + (0.3*merged_df['winner_model_b_x'])\n",
    "merged_df['winner_tie']= (0.7*merged_df['winner_tie_y']) + (0.3*merged_df['winner_tie_x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-24T12:21:19.307437Z",
     "iopub.status.busy": "2024-07-24T12:21:19.306809Z",
     "iopub.status.idle": "2024-07-24T12:21:19.322251Z",
     "shell.execute_reply": "2024-07-24T12:21:19.321347Z",
     "shell.execute_reply.started": "2024-07-24T12:21:19.307407Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = merged_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb0d028",
   "metadata": {},
   "source": [
    "credits: https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.347272,
   "end_time": "2024-07-10T01:15:35.655682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T01:13:07.30841",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f59addf0d2f40309e025976c382cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_268e3946106b4e41849bf11c5a375dac",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb130c471af4927a6644f932ae47523",
       "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
      }
     },
     "19ef2d43bafa44a8b20dc5230aea5ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca042ffa14e4dbebdc66435f7b1f07f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d03719518b8423099b8b68a92e449d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64ef70cfa9c04764868fa52963323322",
       "placeholder": "​",
       "style": "IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d89705c74d34016bbc1e0601ead825c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d409334237014614bf9ae742597d98ea",
       "placeholder": "​",
       "style": "IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6",
       "value": " 2/2 [01:18&lt;00:00, 34.68s/it]"
      }
     },
     "1ef6d64d40d8461d9e6adddd513089b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268e3946106b4e41849bf11c5a375dac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "324c2396f44f45c89d9ec264007ef9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5bb130c471af4927a6644f932ae47523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d243712a1a545e99fa858b0cf19831d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e81b324ca1b46a2a96d02bb2acadc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64ef70cfa9c04764868fa52963323322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ae4b02a13f4570ad729c437ebd28ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f",
        "IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef",
        "IPY_MODEL_0f59addf0d2f40309e025976c382cad8"
       ],
       "layout": "IPY_MODEL_9422a87b93ab4473913e601da3a18689"
      }
     },
     "81b4a9a7cde64b17856b89dbd238c0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff",
       "value": 2
      }
     },
     "85d217d6b45847869cea506db59e8b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d243712a1a545e99fa858b0cf19831d",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96385fd98f304649ab5c4ae81333fb63",
       "value": 2
      }
     },
     "875758123e4f41f0b5c3fa0cd4fb47c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9422a87b93ab4473913e601da3a18689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96385fd98f304649ab5c4ae81333fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d409334237014614bf9ae742597d98ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d576a283e6424206ab4c25d809241c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8a7714cd80d479e859b6ae31ebce7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d98918fae8174629b4819a1114f21202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d03719518b8423099b8b68a92e449d7",
        "IPY_MODEL_85d217d6b45847869cea506db59e8b42",
        "IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"
       ],
       "layout": "IPY_MODEL_d576a283e6424206ab4c25d809241c21"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
