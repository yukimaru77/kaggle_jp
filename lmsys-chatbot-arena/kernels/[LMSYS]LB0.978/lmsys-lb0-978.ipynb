{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:31.881515Z",
     "iopub.status.busy": "2024-06-13T15:44:31.881147Z",
     "iopub.status.idle": "2024-06-13T15:45:25.464117Z",
     "shell.execute_reply": "2024-06-13T15:45:25.462947Z",
     "shell.execute_reply.started": "2024-06-13T15:44:31.881486Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bad97",
   "metadata": {},
   "source": [
    "The work in this notebook is inspired by these notebooks:\n",
    "* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n",
    "* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "## If it helps you, I hope you can give me a thumbs up\n",
    "\n",
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.805645Z",
     "iopub.status.busy": "2024-06-13T15:45:31.805368Z",
     "iopub.status.idle": "2024-06-13T15:45:31.816648Z",
     "shell.execute_reply": "2024-06-13T15:45:31.815718Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.805621Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from IPython.display import display\n",
    "import torch.nn.functional as F\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.818268Z",
     "iopub.status.busy": "2024-06-13T15:45:31.817891Z",
     "iopub.status.idle": "2024-06-13T15:45:31.825778Z",
     "shell.execute_reply": "2024-06-13T15:45:31.824978Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.818231Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030d5a2",
   "metadata": {},
   "source": [
    "## Prepare Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.827124Z",
     "iopub.status.busy": "2024-06-13T15:45:31.826869Z",
     "iopub.status.idle": "2024-06-13T15:45:31.847758Z",
     "shell.execute_reply": "2024-06-13T15:45:31.846742Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.8271Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.849516Z",
     "iopub.status.busy": "2024-06-13T15:45:31.849026Z",
     "iopub.status.idle": "2024-06-13T15:45:31.883908Z",
     "shell.execute_reply": "2024-06-13T15:45:31.882997Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.849481Z"
    }
   },
   "outputs": [],
   "source": [
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd5338",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.40071Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40106Z",
     "shell.execute_reply": "2024-06-13T15:44:22.400895Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.400882Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa8092",
   "metadata": {},
   "source": [
    "## Load model \n",
    "> We load 1 model on each gpu.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.403623Z",
     "iopub.status.idle": "2024-06-13T15:44:22.404009Z",
     "shell.execute_reply": "2024-06-13T15:44:22.403846Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.403831Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.405382Z",
     "iopub.status.idle": "2024-06-13T15:44:22.405698Z",
     "shell.execute_reply": "2024-06-13T15:44:22.405553Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.405539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b851b53",
   "metadata": {},
   "source": [
    "## Load weights \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.407084Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40743Z",
     "shell.execute_reply": "2024-06-13T15:44:22.407278Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.407264Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRa configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.408351Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4087Z",
     "shell.execute_reply": "2024-06-13T15:44:22.408536Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.408521Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "#Load weights\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()\n",
    "\n",
    "#Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.410059Z",
     "iopub.status.idle": "2024-06-13T15:44:22.410418Z",
     "shell.execute_reply": "2024-06-13T15:44:22.410257Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.410242Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454507d",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.411343Z",
     "iopub.status.idle": "2024-06-13T15:44:22.411663Z",
     "shell.execute_reply": "2024-06-13T15:44:22.411513Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.4115Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.412909Z",
     "iopub.status.idle": "2024-06-13T15:44:22.413243Z",
     "shell.execute_reply": "2024-06-13T15:44:22.413073Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41306Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.414669Z",
     "iopub.status.idle": "2024-06-13T15:44:22.414992Z",
     "shell.execute_reply": "2024-06-13T15:44:22.414841Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.414828Z"
    }
   },
   "outputs": [],
   "source": [
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.416588Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41692Z",
     "shell.execute_reply": "2024-06-13T15:44:22.416764Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41675Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9523a5",
   "metadata": {},
   "source": [
    "## LGBM + tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.417972Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41834Z",
     "shell.execute_reply": "2024-06-13T15:44:22.418143Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.418129Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'\n",
    "RUNPOD = os.path.exists('/workspace/')\n",
    "KAGGLE = not RUNPOD\n",
    "if KAGGLE: \n",
    "    print('kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.419857Z",
     "iopub.status.idle": "2024-06-13T15:44:22.420268Z",
     "shell.execute_reply": "2024-06-13T15:44:22.420061Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.420047Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.421762Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4221Z",
     "shell.execute_reply": "2024-06-13T15:44:22.421948Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.421934Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/ \n",
    "        \n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.42329Z",
     "iopub.status.idle": "2024-06-13T15:44:22.423599Z",
     "shell.execute_reply": "2024-06-13T15:44:22.423456Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.423443Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = '/kaggle/input/'  \n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n",
    "print(MODEL_PATH)\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.425238Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42556Z",
     "shell.execute_reply": "2024-06-13T15:44:22.425407Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.425394Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.426467Z",
     "iopub.status.idle": "2024-06-13T15:44:22.426784Z",
     "shell.execute_reply": "2024-06-13T15:44:22.426631Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.426618Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.427766Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42813Z",
     "shell.execute_reply": "2024-06-13T15:44:22.427955Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.42793Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "if False: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    params['fold'] = -1\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1\n",
    "params['n_lgb'] = 1\n",
    "params['model'] = 'microsoft/deberta-v3-small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.429052Z",
     "iopub.status.idle": "2024-06-13T15:44:22.429388Z",
     "shell.execute_reply": "2024-06-13T15:44:22.429238Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.429225Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0\n",
    "N_FOLDS = int(params.get('n_folds', 3)); \n",
    "FOLD = int(params.get('fold', 0))\n",
    "SEED = int(params.get('seed', 3))\n",
    "SS = int(params.get('subsample', 1))\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:22.459744Z",
     "iopub.status.busy": "2024-06-13T15:44:22.459023Z",
     "iopub.status.idle": "2024-06-13T15:44:23.504374Z",
     "shell.execute_reply": "2024-06-13T15:44:23.502893Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.45971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n",
    "if SS > 1:\n",
    "    train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.505134Z",
     "iopub.status.idle": "2024-06-13T15:44:23.505514Z",
     "shell.execute_reply": "2024-06-13T15:44:23.505348Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.505333Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.506746Z",
     "iopub.status.idle": "2024-06-13T15:44:23.507061Z",
     "shell.execute_reply": "2024-06-13T15:44:23.50692Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.506907Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN = False\n",
    "INFER = True \n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:23.666596Z",
     "iopub.status.busy": "2024-06-13T15:44:23.666257Z",
     "iopub.status.idle": "2024-06-13T15:44:27.845064Z",
     "shell.execute_reply": "2024-06-13T15:44:27.844121Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.66657Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:27.847295Z",
     "iopub.status.busy": "2024-06-13T15:44:27.846672Z",
     "iopub.status.idle": "2024-06-13T15:44:27.985709Z",
     "shell.execute_reply": "2024-06-13T15:44:27.984569Z",
     "shell.execute_reply.started": "2024-06-13T15:44:27.847259Z"
    }
   },
   "outputs": [],
   "source": [
    "LGB = True\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n",
    "INFER_LGB = not TRAIN and LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def symlog(x):\n",
    "    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())\n",
    "    x = symlog(x)\n",
    "    return x\n",
    "\n",
    "def get_features(df):\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    v = np.hstack([\n",
    "          afeat - bfeat, np.abs(afeat - bfeat), \n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    extras = []\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)\n",
    "            \n",
    "    extras.append(df[c].str.len())\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n",
    "    return np.hstack([v, extras])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test\n",
    "    yps = []; b = 1000\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])\n",
    "        ypms = []\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))\n",
    "        yps.append(np.stack(ypms).mean(0))\n",
    "        print('.', end = '')\n",
    "        \n",
    "        if len(yps) % 2 == 0:\n",
    "            gc.collect()\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds = yp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b7cd3",
   "metadata": {},
   "source": [
    "## Blend predictions\n",
    "\n",
    "$\\operatorname{preds} = 0.05 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_wt = 0.05\n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])\n",
    "display(out.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
