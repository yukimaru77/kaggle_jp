{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30748,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#  Llama-3 8b [TPU Train]\n\nLearning to train llms on tpu, Hope this will help you too!\n\nNotebook inspired from:\n\n* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)\n* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)\n* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)\n\n\nPrerequisite: Access to using llama-3\n\nNote: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)\n\nPlease upvote if you learn or find this helpful!","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"# Install libs\n!pip install -qq peft==0.6.0\n!pip install -qq bitsandbytes==0.41.1\n!pip install -qq accelerate==0.24.1\n!pip install -qq transformers==4.35.0\n!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:36.902248Z","iopub.execute_input":"2024-07-18T06:45:36.902952Z","iopub.status.idle":"2024-07-18T06:45:58.797746Z","shell.execute_reply.started":"2024-07-18T06:45:36.902917Z","shell.execute_reply":"2024-07-18T06:45:58.796513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\nimport random\nimport warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\n\nimport torch\nimport transformers\nfrom sklearn.metrics import accuracy_score\nfrom transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\nfrom peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\nimport torch.nn.functional as F\n\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\nfrom spmd_util import partition_module\n\ntqdm.pandas()\n\nprint(f'Torch Version: {torch.__version__}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:06.395209Z","iopub.execute_input":"2024-07-18T06:18:06.395601Z","iopub.status.idle":"2024-07-18T06:18:06.563828Z","shell.execute_reply.started":"2024-07-18T06:18:06.395564Z","shell.execute_reply":"2024-07-18T06:18:06.563166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# import gc\n# import re\n# from time import time\n# import random\n# import warnings\n# import numpy as np\n# import pandas as pd\n# import matplotlib.pyplot as plt\n# from tqdm.auto import tqdm\n\n# import torch\n# import transformers\n# from sklearn.metrics import accuracy_score\n# from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n# from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n# import torch.nn.functional as F\n\n# import torch_xla.debug.profiler as xp\n# import torch_xla.core.xla_model as xm\n# import torch_xla.experimental.xla_sharding as xs\n# import torch_xla.runtime as xr\n\n# xr.use_spmd()\n\n# from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n# from torch_xla.experimental.xla_sharding import Mesh\n# from spmd_util import partition_module\n\n# tqdm.pandas()\n\n# print(f'Torch Version: {torch.__version__}')\n# /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n#   from .autonotebook import tqdm as notebook_tqdm\n# /usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n#   warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n# /usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32\n# Torch Version: 2.1.2+cpu\n\n# # Configs\n# class CFG:\n#     NUM_EPOCHS = 1\n#     BATCH_SIZE = 1  # It was 16 before\n#     DROPOUT = 0.05 \n#     MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n#     SEED = 2024 \n#     MAX_LENGTH = 1024 \n#     NUM_WARMUP_STEPS = 128\n#     LR_MAX = 5e-5 \n#     NUM_LABELS = 3 \n#     LORA_RANK = 4\n#     LORA_ALPHA = 8\n#     LORA_MODULES = ['o_proj', 'v_proj']\n    \n# DEVICE = xm.xla_device()  # Initialize TPU Device","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:39:35.94052Z","iopub.execute_input":"2024-07-18T06:39:35.941175Z","iopub.status.idle":"2024-07-18T06:39:35.957638Z","shell.execute_reply.started":"2024-07-18T06:39:35.941134Z","shell.execute_reply":"2024-07-18T06:39:35.95664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs","metadata":{}},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 1 # It was 16 before\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']\n    \nDEVICE = xm.xla_device() # Initialize TPU Device","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:29.086452Z","iopub.execute_input":"2024-07-18T06:45:29.087051Z","iopub.status.idle":"2024-07-18T06:45:29.133175Z","shell.execute_reply.started":"2024-07-18T06:45:29.087017Z","shell.execute_reply":"2024-07-18T06:45:29.132226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def set_seeds(seed):\n    \"\"\"Set seeds for reproducibility \"\"\"\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed(seed)\n        torch.cuda.manual_seed_all(seed)\n        \n    # Set seed for all TPU cores\n    xm.set_rng_state(seed, device=xm.xla_device())  \n\nset_seeds(seed=CFG.SEED)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:24.36047Z","iopub.execute_input":"2024-07-18T06:18:24.361144Z","iopub.status.idle":"2024-07-18T06:18:24.36895Z","shell.execute_reply.started":"2024-07-18T06:18:24.361105Z","shell.execute_reply":"2024-07-18T06:18:24.368251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = 'right'\ntokenizer.add_eos_token = True\n\n# save tokenizer to load offline during inference\ntokenizer.save_pretrained('tokenizer')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:27.768498Z","iopub.execute_input":"2024-07-18T06:18:27.768896Z","iopub.status.idle":"2024-07-18T06:18:28.392621Z","shell.execute_reply.started":"2024-07-18T06:18:27.76886Z","shell.execute_reply":"2024-07-18T06:18:28.391957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Utility function giving token length\ndef get_token_lengths(texts):\n    # tokenize and receive input_ids for reach text\n    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n    # return length of inputs_ids for each text\n    return [len(t) for t in input_ids]","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:32.690884Z","iopub.execute_input":"2024-07-18T06:18:32.691262Z","iopub.status.idle":"2024-07-18T06:18:32.696118Z","shell.execute_reply.started":"2024-07-18T06:18:32.69123Z","shell.execute_reply":"2024-07-18T06:18:32.695375Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train\n","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:36.451226Z","iopub.execute_input":"2024-07-18T06:18:36.452066Z","iopub.status.idle":"2024-07-18T06:18:40.062409Z","shell.execute_reply.started":"2024-07-18T06:18:36.452026Z","shell.execute_reply":"2024-07-18T06:18:40.061689Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(5)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:40.063715Z","iopub.execute_input":"2024-07-18T06:18:40.06401Z","iopub.status.idle":"2024-07-18T06:18:40.079067Z","shell.execute_reply.started":"2024-07-18T06:18:40.06398Z","shell.execute_reply":"2024-07-18T06:18:40.078222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:18:43.496034Z","iopub.execute_input":"2024-07-18T06:18:43.496382Z","iopub.status.idle":"2024-07-18T06:18:43.808097Z","shell.execute_reply.started":"2024-07-18T06:18:43.496349Z","shell.execute_reply":"2024-07-18T06:18:43.807411Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train with only take 50% train dataset\n# train = train[:int(len(train) * 0.5)]\ntrain = train[int(len(train) * 0.5):]\n\n# Tsplit to two halves\nfirst_half = train[:int(len(train) * 0.5)]\n\nfirst_half.loc[:, 'token_count'] = get_token_lengths(first_half['text'])\nfirst_half.loc[:, 'label'] = np.argmax(first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)\n\n\ntrain.loc[:, 'token_count'] = get_token_lengths(train['text'])\n\n# prepare label for model and get the max index\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\n# Display data\ndisplay(train.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:20.628221Z","iopub.execute_input":"2024-07-18T06:45:20.62856Z","iopub.status.idle":"2024-07-18T06:45:20.650268Z","shell.execute_reply.started":"2024-07-18T06:45:20.62853Z","shell.execute_reply":"2024-07-18T06:45:20.649282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.label.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:19:43.073305Z","iopub.execute_input":"2024-07-18T06:19:43.073681Z","iopub.status.idle":"2024-07-18T06:19:43.080943Z","shell.execute_reply.started":"2024-07-18T06:19:43.073646Z","shell.execute_reply":"2024-07-18T06:19:43.080199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# token Count\ndisplay(train['token_count'].describe().to_frame().astype(int))","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:19:43.919256Z","iopub.execute_input":"2024-07-18T06:19:43.919899Z","iopub.status.idle":"2024-07-18T06:19:43.930622Z","shell.execute_reply.started":"2024-07-18T06:19:43.919861Z","shell.execute_reply":"2024-07-18T06:19:43.929879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get length of tokens which covers 90% of data, we'll still take 1024 length!\nnp.percentile(train['token_count'], 90)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:19:47.811444Z","iopub.execute_input":"2024-07-18T06:19:47.811834Z","iopub.status.idle":"2024-07-18T06:19:47.818289Z","shell.execute_reply.started":"2024-07-18T06:19:47.811798Z","shell.execute_reply":"2024-07-18T06:19:47.817447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize","metadata":{}},{"cell_type":"code","source":"# # Tokenize Data\n# tokens = tokenizer(\n#     train['text'].tolist(), \n#     padding='max_length', \n#     max_length=CFG.MAX_LENGTH, \n#     truncation=True, \n#     return_tensors='np')\n\n# # Input IDs are the token IDs\n# INPUT_IDS = tokens['input_ids']\n# # Attention Masks to Ignore Padding Tokens\n# ATTENTION_MASKS = tokens['attention_mask']\n# # Label of Texts\n# LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n\n# print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n# print(f'LABELS shape: {LABELS.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:53:02.164336Z","iopub.execute_input":"2024-07-18T00:53:02.164609Z","iopub.status.idle":"2024-07-18T00:53:18.442152Z","shell.execute_reply.started":"2024-07-18T00:53:02.164583Z","shell.execute_reply":"2024-07-18T00:53:18.440966Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokens = tokenizer(\n    first_half['text'].tolist(), \n    padding='max_length', \n    max_length=CFG.MAX_LENGTH, \n    truncation=True, \n    return_tensors='np'\n)\n\nINPUT_IDS = tokens['input_ids']\nATTENTION_MASKS = tokens['attention_mask']\nLABELS = first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values\nprint(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\nprint(f'LABELS shape: {LABELS.shape}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:20:44.782805Z","iopub.execute_input":"2024-07-18T06:20:44.78341Z","iopub.status.idle":"2024-07-18T06:20:48.556603Z","shell.execute_reply.started":"2024-07-18T06:20:44.783355Z","shell.execute_reply":"2024-07-18T06:20:48.555869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_dataset(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        # Shuffle Indices\n        np.random.shuffle(IDXS)\n        # Iterate Over All Indices Once\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n            \n            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            \n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:13.638158Z","iopub.execute_input":"2024-07-18T06:45:13.638946Z","iopub.status.idle":"2024-07-18T06:45:13.662002Z","shell.execute_reply.started":"2024-07-18T06:45:13.638914Z","shell.execute_reply":"2024-07-18T06:45:13.661088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define dataset generator for the first half\ndef train_dataset_first_half(batch_size):\n    N_SAMPLES = LABELS.shape[0]\n    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n    while True:\n        np.random.shuffle(IDXS)\n        for idxs in IDXS.reshape(-1, batch_size):\n            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n            labels = torch.tensor(LABELS[idxs]).to(DEVICE)\n            xs.mark_sharding(input_ids, mesh, (0, 1))\n            xs.mark_sharding(attention_mask, mesh, (0, 1))\n            xs.mark_sharding(labels, mesh, (0, 1))\n            yield input_ids, attention_mask, labels\n\nTRAIN_DATASET = train_dataset_first_half(CFG.BATCH_SIZE)\n# Calculate STEPS_PER_EPOCH for the first half\nSTEPS_PER_EPOCH = len(first_half) // CFG.BATCH_SIZE\n\n","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:42:27.643351Z","iopub.execute_input":"2024-07-18T06:42:27.644213Z","iopub.status.idle":"2024-07-18T06:42:27.650446Z","shell.execute_reply.started":"2024-07-18T06:42:27.644165Z","shell.execute_reply":"2024-07-18T06:42:27.649653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Model","metadata":{}},{"cell_type":"code","source":"# Load model for classification with 3 target label\nbase_model = LlamaForSequenceClassification.from_pretrained(\n    CFG.MODEL_NAME,\n    num_labels=CFG.NUM_LABELS,\n    torch_dtype=torch.bfloat16)\n\nbase_model.config.pretraining_tp = 1 \n\n# Assign Padding TOKEN\nbase_model.config.pad_token_id = tokenizer.pad_token_id","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:07.266175Z","iopub.execute_input":"2024-07-18T06:45:07.266502Z","iopub.status.idle":"2024-07-18T06:45:07.284182Z","shell.execute_reply.started":"2024-07-18T06:45:07.266473Z","shell.execute_reply":"2024-07-18T06:45:07.283207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Low-Rank Adaptation [LORA]","metadata":{}},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n    lora_dropout= CFG.DROPOUT, \n    bias='none',\n    inference_mode=False,\n    task_type=TaskType.SEQ_CLS,\n    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:45:03.421575Z","iopub.execute_input":"2024-07-18T06:45:03.421955Z","iopub.status.idle":"2024-07-18T06:45:03.44123Z","shell.execute_reply.started":"2024-07-18T06:45:03.421921Z","shell.execute_reply":"2024-07-18T06:45:03.440229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create LoRa Model\nmodel = get_peft_model(base_model, lora_config)\n# Trainable Parameters\nmodel.print_trainable_parameters()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:44:56.91778Z","iopub.execute_input":"2024-07-18T06:44:56.918569Z","iopub.status.idle":"2024-07-18T06:44:56.934615Z","shell.execute_reply.started":"2024-07-18T06:44:56.918535Z","shell.execute_reply":"2024-07-18T06:44:56.93381Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Number of TPU Nodes\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n# distribute model\npartition_module(model, mesh)\n\nprint(f'num_devices: {num_devices}')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Verfy The Trainable Layers\nMODEL_LAYERS_ROWS = []\nTRAINABLE_PARAMS = []\nN_TRAINABLE_PARAMS = 0\n\nfor name, param in model.named_parameters():\n    # Layer Parameter Count\n    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n    # Only Trainable Layers\n    if param.requires_grad:\n        # Add Layer Information\n        MODEL_LAYERS_ROWS.append({\n            'param': n_parameters,\n            'name': name,\n            'dtype': param.data.dtype,\n        })\n        # Append Trainable Parameter\n        TRAINABLE_PARAMS.append({ 'params': param })\n        # Add Number Of Trainable Parameters\"\n        N_TRAINABLE_PARAMS += n_parameters\n        \ndisplay(pd.DataFrame(MODEL_LAYERS_ROWS))\n\nprint(f\"\"\"\n===============================\nN_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\nN_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n===============================\n\"\"\")","metadata":{"execution":{"iopub.status.busy":"2024-07-18T06:44:31.2459Z","iopub.execute_input":"2024-07-18T06:44:31.246215Z","iopub.status.idle":"2024-07-18T06:44:31.476841Z","shell.execute_reply.started":"2024-07-18T06:44:31.246187Z","shell.execute_reply":"2024-07-18T06:44:31.476084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training","metadata":{}},{"cell_type":"code","source":"# LR & Optimizer\nN_SAMPLES = len(train)\nSTEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n\nOPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n\n# Cosine Learning Rate With Warmup\nlr_scheduler = transformers.get_cosine_schedule_with_warmup(\n    optimizer=OPTIMIZER,\n    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n\nprint(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T01:00:10.070122Z","iopub.execute_input":"2024-07-18T01:00:10.070423Z","iopub.status.idle":"2024-07-18T01:00:12.011654Z","shell.execute_reply.started":"2024-07-18T01:00:10.070395Z","shell.execute_reply":"2024-07-18T01:00:12.010562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set the data type for the optimizer's state (e.g., momentum buffers)\nfor state in OPTIMIZER.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n            state[v] = v.to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T01:00:12.012839Z","iopub.execute_input":"2024-07-18T01:00:12.013144Z","iopub.status.idle":"2024-07-18T01:00:15.084005Z","shell.execute_reply.started":"2024-07-18T01:00:12.013118Z","shell.execute_reply":"2024-07-18T01:00:15.082811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_ids, attention_mask, labels = next(TRAIN_DATASET)\n\nprint(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\nprint(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\nprint(f'labels shape: {labels.shape}, dtype: {labels.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:33:36.095004Z","iopub.execute_input":"2024-07-18T00:33:36.095228Z","iopub.status.idle":"2024-07-18T00:33:36.108934Z","shell.execute_reply.started":"2024-07-18T00:33:36.095206Z","shell.execute_reply":"2024-07-18T00:33:36.108232Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Dummy Prediction\nwith torch.no_grad():\n    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n    \nprint(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:33:36.109801Z","iopub.execute_input":"2024-07-18T00:33:36.110039Z","iopub.status.idle":"2024-07-18T00:33:58.48704Z","shell.execute_reply.started":"2024-07-18T00:33:36.110017Z","shell.execute_reply":"2024-07-18T00:33:58.486299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Put Model In Train Mode\nmodel.train()\n\n# Loss Function, Cross Entropy\nLOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:33:58.488884Z","iopub.execute_input":"2024-07-18T00:33:58.489137Z","iopub.status.idle":"2024-07-18T00:33:58.496974Z","shell.execute_reply.started":"2024-07-18T00:33:58.489112Z","shell.execute_reply":"2024-07-18T00:33:58.496317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time()\nwarnings.filterwarnings(\"error\")\nMETRICS = {\n    'loss': [],\n    'accuracy': {'y_true': [], 'y_pred': [] }}\n\nfor epoch in tqdm(range(CFG.NUM_EPOCHS)):\n    ste = time()\n    for step in range(STEPS_PER_EPOCH):\n        # Zero Out Gradients\n        OPTIMIZER.zero_grad()\n        \n        # Get Batch\n        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n        \n        # Forward Pass\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n       \n        # Logits Float32\n        logits = outputs.logits.to(dtype=torch.float32)\n        \n        # Backward Pass\n        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n        loss.backward()\n        \n        # optimizer step\n        OPTIMIZER.step()\n        xm.mark_step()\n        \n        # Update Learning Rate Scheduler\n        lr_scheduler.step()\n        \n        # Update Metrics And Progress Bar\n        METRICS['loss'].append(float(loss))\n        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n        \n        if (step + 1) % 200 == 0:  \n            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n                                                               METRICS['accuracy']['y_pred']))\n            lr = OPTIMIZER.param_groups[0]['lr']\n            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n    \n    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n\n    # If stopped, and to continue training in future on tpu we save model and optimizer\n    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n    \n    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:33:58.497867Z","iopub.execute_input":"2024-07-18T00:33:58.4981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 6))\nplt.plot(METRICS['loss'])    \nplt.xlabel('Step per epoch')\nplt.ylabel('Loss')\nplt.title('Loss Plot step per epoch')    \nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-18T00:39:32.034322Z","iopub.execute_input":"2024-07-18T00:39:32.034628Z","iopub.status.idle":"2024-07-18T00:39:32.2572Z","shell.execute_reply.started":"2024-07-18T00:39:32.034602Z","shell.execute_reply":"2024-07-18T00:39:32.25633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save Model\n","metadata":{}},{"cell_type":"code","source":"model = model.cpu()\ntorch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-06-14T14:52:40.497608Z","iopub.execute_input":"2024-06-14T14:52:40.497887Z","iopub.status.idle":"2024-06-14T14:54:33.373114Z","shell.execute_reply.started":"2024-06-14T14:52:40.49786Z","shell.execute_reply":"2024-06-14T14:54:33.371833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Conclusion \n\nThere is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!","metadata":{}}]}