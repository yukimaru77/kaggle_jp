{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":7715453,"sourceType":"datasetVersion","datasetId":4505960},{"sourceId":7715470,"sourceType":"datasetVersion","datasetId":4505971},{"sourceId":8756307,"sourceType":"datasetVersion","datasetId":5260293},{"sourceId":8811121,"sourceType":"datasetVersion","datasetId":5299906},{"sourceId":148861315,"sourceType":"kernelVersion"},{"sourceId":69765,"sourceType":"modelInstanceVersion","modelInstanceId":58215}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Gemma 2 - 9b \n\nWe use Gemma 2 9b model to get embeddings and train a classifier on it. This is first part and in this we only compute embed. You can also use other models. Let's get started!\n\nUpvote if you found this helpful!","metadata":{}},{"cell_type":"markdown","source":"# Import libs ","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes \n!pip install -q git+https://github.com/huggingface/transformers\n!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2024-07-09T17:56:31.035487Z","iopub.execute_input":"2024-07-09T17:56:31.035848Z","iopub.status.idle":"2024-07-09T17:56:57.430566Z","shell.execute_reply.started":"2024-07-09T17:56:31.035816Z","shell.execute_reply":"2024-07-09T17:56:57.429382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport gc\nimport re\nfrom time import time\n\nimport torch\nimport transformers\nimport sklearn\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig\n\nimport time\nfrom torch.cuda.amp import autocast\nfrom threading import Thread\n\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\n# if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")","metadata":{"execution":{"iopub.status.busy":"2024-07-09T17:57:07.709215Z","iopub.execute_input":"2024-07-09T17:57:07.709581Z","iopub.status.idle":"2024-07-09T17:57:07.715439Z","shell.execute_reply.started":"2024-07-09T17:57:07.709548Z","shell.execute_reply":"2024-07-09T17:57:07.714762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Configs ","metadata":{}},{"cell_type":"code","source":"class CFG:\n    MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'\n    MAX_LENGTH = 1024\n    BATCH_SIZE = 2\n    \ndevice0 = torch.device('cuda:0')\ndevice1 = torch.device('cuda:1')","metadata":{"execution":{"iopub.status.busy":"2024-07-09T17:57:12.482639Z","iopub.execute_input":"2024-07-09T17:57:12.483406Z","iopub.status.idle":"2024-07-09T17:57:12.487232Z","shell.execute_reply.started":"2024-07-09T17:57:12.483371Z","shell.execute_reply":"2024-07-09T17:57:12.486597Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load model","metadata":{}},{"cell_type":"code","source":"tokenizer = GemmaTokenizer.from_pretrained(CFG.MODEL_PATH)\n\nbnb_config_4bit = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False)\n\nmodel_0 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,\n                                        revision=\"float16\",\n                                        device_map='cuda:0',\n                                        quantization_config=bnb_config_4bit)        \n\nmodel_1 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,\n                                        revision=\"float16\",\n                                        device_map='cuda:1',\n                                        quantization_config=bnb_config_4bit)     ","metadata":{"execution":{"iopub.status.busy":"2024-07-09T17:57:14.120318Z","iopub.execute_input":"2024-07-09T17:57:14.120673Z","iopub.status.idle":"2024-07-09T17:57:14.174055Z","shell.execute_reply.started":"2024-07-09T17:57:14.120646Z","shell.execute_reply":"2024-07-09T17:57:14.173221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare train ","metadata":{"execution":{"iopub.status.busy":"2024-06-28T13:35:51.616741Z","iopub.execute_input":"2024-06-28T13:35:51.617798Z","iopub.status.idle":"2024-06-28T13:36:00.407012Z","shell.execute_reply.started":"2024-06-28T13:35:51.61776Z","shell.execute_reply":"2024-06-28T13:36:00.405918Z"}}},{"cell_type":"code","source":"def process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return sentences[-1] if sentences else ''\n  \ntrain = pd.read_csv('/kaggle/input/lmsys-chatbot-arena-additional-data-90k-columns/Merged_data.csv')\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n\ntrain['text'] = '<start_of_turn>User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n----\\n\\nModel B:\\n'  + train['response_b'] + '<end_of_turn><eos>'","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:58:10.186737Z","iopub.execute_input":"2024-06-29T12:58:10.187015Z","iopub.status.idle":"2024-06-29T12:58:12.63297Z","shell.execute_reply.started":"2024-06-29T12:58:10.18699Z","shell.execute_reply":"2024-06-29T12:58:12.631908Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# taking only 40k sample\ntrain = train[:40000]\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:58:12.634177Z","iopub.execute_input":"2024-06-29T12:58:12.634501Z","iopub.status.idle":"2024-06-29T12:58:12.650767Z","shell.execute_reply.started":"2024-06-29T12:58:12.634468Z","shell.execute_reply":"2024-06-29T12:58:12.649794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train['text'][10])","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:58:12.651973Z","iopub.execute_input":"2024-06-29T12:58:12.653707Z","iopub.status.idle":"2024-06-29T12:58:12.658726Z","shell.execute_reply.started":"2024-06-29T12:58:12.653681Z","shell.execute_reply":"2024-06-29T12:58:12.657852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Tokenize ","metadata":{}},{"cell_type":"code","source":"tokens = tokenizer(train['text'].tolist(),\n                   padding='max_length',\n                   max_length=CFG.MAX_LENGTH,\n                   truncation=True,\n                   return_tensors='pt')\n\nINPUT_IDS = tokens['input_ids']\nATTENTION_MASKS = tokens['attention_mask']\n\ndata = pd.DataFrame()\ndata['INPUT_IDS'] = [tensor.tolist() for tensor in INPUT_IDS]\ndata['ATTENTION_MASKS'] = [tensor.tolist() for tensor in ATTENTION_MASKS]\ndata[:2]","metadata":{"execution":{"iopub.status.busy":"2024-06-29T12:59:23.82832Z","iopub.execute_input":"2024-06-29T12:59:23.829068Z","iopub.status.idle":"2024-06-29T12:59:24.451438Z","shell.execute_reply.started":"2024-06-29T12:59:23.829032Z","shell.execute_reply":"2024-06-29T12:59:24.45049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute embedding","metadata":{}},{"cell_type":"code","source":"def get_embeddings(df, model, device, batch_size=CFG.BATCH_SIZE):  \n    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n\n    embed_list = []\n\n    for start_idx in range(0, len(df), batch_size):\n        end_idx = min(start_idx + batch_size, len(df))\n        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n        gc.collect()\n        torch.cuda.empty_cache()\n        with torch.no_grad():\n            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)\n            embed = outputs.hidden_states[-1]\n            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool\n            embed_list.append(embed_mean) \n            \n            torch.cuda.empty_cache()\n        \n    embeddings = torch.cat(embed_list, dim=0)\n    return embeddings\n\ndef compute_embed(df, model, device, results, index):\n    results[index] = get_embeddings(df, model, device)","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:00:18.723585Z","iopub.execute_input":"2024-06-29T13:00:18.724238Z","iopub.status.idle":"2024-06-29T13:00:18.734299Z","shell.execute_reply.started":"2024-06-29T13:00:18.724204Z","shell.execute_reply":"2024-06-29T13:00:18.733293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st = time.time()\n\nN_SAMPLES = len(data)\nhalf = round(N_SAMPLES / 2)\nsub1 = data.iloc[0:half].copy()\nsub2 = data.iloc[half:N_SAMPLES].copy()\n\nresults = {}\n\nt0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))\nt1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))\n\nt0.start()\nt1.start()\n\nt0.join()\nt1.join()\n\nprint(f\"Processing complete. Total time: {time.time() - st:.2f} seconds\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:00:19.180946Z","iopub.execute_input":"2024-06-29T13:00:19.18125Z","iopub.status.idle":"2024-06-29T13:01:33.893412Z","shell.execute_reply.started":"2024-06-29T13:00:19.181227Z","shell.execute_reply":"2024-06-29T13:01:33.892379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embeddings = torch.cat([results[0], results[1]], dim=0)\nembeddings.shape","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:02:04.394965Z","iopub.execute_input":"2024-06-29T13:02:04.396297Z","iopub.status.idle":"2024-06-29T13:02:04.403043Z","shell.execute_reply.started":"2024-06-29T13:02:04.396259Z","shell.execute_reply":"2024-06-29T13:02:04.401994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"gc.collect()\ndel model_1\ndel  model_0\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:02:05.603536Z","iopub.execute_input":"2024-06-29T13:02:05.604684Z","iopub.status.idle":"2024-06-29T13:02:05.935315Z","shell.execute_reply.started":"2024-06-29T13:02:05.604649Z","shell.execute_reply":"2024-06-29T13:02:05.934344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save embed","metadata":{}},{"cell_type":"code","source":"save_path = 'gemma2_train_embed.npy'\n\n# Save the embeddings as .npy file\nnp.save(save_path, embeddings.numpy())\n# we also save train just for completeness\ntrain.to_csv('train_embed.csv', index=False)\n\nprint(f\"Concatenated embeddings saved to {save_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-29T13:03:26.133583Z","iopub.execute_input":"2024-06-29T13:03:26.133939Z","iopub.status.idle":"2024-06-29T13:03:26.164319Z","shell.execute_reply.started":"2024-06-29T13:03:26.133915Z","shell.execute_reply":"2024-06-29T13:03:26.163407Z"},"trusted":true},"execution_count":null,"outputs":[]}]}