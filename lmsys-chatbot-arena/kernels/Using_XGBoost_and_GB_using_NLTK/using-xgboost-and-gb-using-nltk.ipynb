{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T03:45:33.930479Z",
     "iopub.status.busy": "2024-07-08T03:45:33.92994Z",
     "iopub.status.idle": "2024-07-08T03:46:48.731426Z",
     "shell.execute_reply": "2024-07-08T03:46:48.72923Z",
     "shell.execute_reply.started": "2024-07-08T03:45:33.930436Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl\n",
    "!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl\n",
    "import textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:57:54.415241Z",
     "iopub.status.busy": "2024-07-08T02:57:54.414809Z",
     "iopub.status.idle": "2024-07-08T02:57:54.426416Z",
     "shell.execute_reply": "2024-07-08T02:57:54.424526Z",
     "shell.execute_reply.started": "2024-07-08T02:57:54.41521Z"
    }
   },
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "import nltk\n",
    "import textstat\n",
    "from textblob import TextBlob\n",
    "from collections import Counter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_rows', None)  # Show all rows\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:32:35.517063Z",
     "iopub.status.busy": "2024-07-07T05:32:35.516642Z",
     "iopub.status.idle": "2024-07-07T05:32:39.187137Z",
     "shell.execute_reply": "2024-07-07T05:32:39.185863Z",
     "shell.execute_reply.started": "2024-07-07T05:32:35.517031Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:32:42.051025Z",
     "iopub.status.busy": "2024-07-07T05:32:42.050567Z",
     "iopub.status.idle": "2024-07-07T05:32:42.082817Z",
     "shell.execute_reply": "2024-07-07T05:32:42.08115Z",
     "shell.execute_reply.started": "2024-07-07T05:32:42.050989Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:32:47.403048Z",
     "iopub.status.busy": "2024-07-07T05:32:47.402627Z",
     "iopub.status.idle": "2024-07-07T05:32:47.416005Z",
     "shell.execute_reply": "2024-07-07T05:32:47.414552Z",
     "shell.execute_reply.started": "2024-07-07T05:32:47.403014Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:32:52.892621Z",
     "iopub.status.busy": "2024-07-07T05:32:52.892097Z",
     "iopub.status.idle": "2024-07-07T05:32:52.898579Z",
     "shell.execute_reply": "2024-07-07T05:32:52.897322Z",
     "shell.execute_reply.started": "2024-07-07T05:32:52.892578Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"The size of the train data: {train.shape} is and the test data is: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:34:12.864865Z",
     "iopub.status.busy": "2024-07-07T05:34:12.864408Z",
     "iopub.status.idle": "2024-07-07T05:34:12.886202Z",
     "shell.execute_reply": "2024-07-07T05:34:12.884996Z",
     "shell.execute_reply.started": "2024-07-07T05:34:12.864831Z"
    }
   },
   "outputs": [],
   "source": [
    "print(train['winner_model_a'].value_counts())\n",
    "print(train['winner_model_b'].value_counts())\n",
    "print(train['winner_tie'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:34:19.213053Z",
     "iopub.status.busy": "2024-07-07T05:34:19.21252Z",
     "iopub.status.idle": "2024-07-07T05:34:20.118606Z",
     "shell.execute_reply": "2024-07-07T05:34:20.117368Z",
     "shell.execute_reply.started": "2024-07-07T05:34:19.21301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a figure and axes\n",
    "fig, axes = plt.subplots(3, 1, figsize=(7, 6))\n",
    "\n",
    "# Columns to plot\n",
    "columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "# Define colors for 0 and 1\n",
    "colors = {0: 'steelblue', 1: 'salmon'}\n",
    "\n",
    "# Plot each column in its respective subplot\n",
    "for i, column in enumerate(columns):\n",
    "    ax = axes[i]\n",
    "    value_counts = train[column].value_counts().sort_index()\n",
    "    \n",
    "    # Plot bars with specified colors and labels for legend\n",
    "    bars = ax.bar(value_counts.index.astype(str), value_counts, color=[colors[idx] for idx in value_counts.index],\n",
    "                  label=value_counts.index.map({0: 'Lose (0)', 1: 'Win (1)'}))\n",
    "    \n",
    "    # Annotate counts on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.annotate(f'{height}',\n",
    "                    xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "    \n",
    "    ax.set_xlabel('Winner')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Model {column.split(\"_\")[-1].capitalize()} Counts')\n",
    "    ax.legend(title='Outcome', loc='upper right')\n",
    "\n",
    "# Add overall title and adjust layout\n",
    "fig.suptitle('Distribution of Winners Across Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:34:55.667447Z",
     "iopub.status.busy": "2024-07-07T05:34:55.666991Z",
     "iopub.status.idle": "2024-07-07T05:34:56.066463Z",
     "shell.execute_reply": "2024-07-07T05:34:56.06515Z",
     "shell.execute_reply.started": "2024-07-07T05:34:55.667412Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:34:57.853126Z",
     "iopub.status.busy": "2024-07-07T05:34:57.852697Z",
     "iopub.status.idle": "2024-07-07T05:34:57.868674Z",
     "shell.execute_reply": "2024-07-07T05:34:57.867465Z",
     "shell.execute_reply.started": "2024-07-07T05:34:57.853093Z"
    }
   },
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:34:59.482959Z",
     "iopub.status.busy": "2024-07-07T05:34:59.482496Z",
     "iopub.status.idle": "2024-07-07T05:34:59.497273Z",
     "shell.execute_reply": "2024-07-07T05:34:59.496004Z",
     "shell.execute_reply.started": "2024-07-07T05:34:59.482899Z"
    }
   },
   "outputs": [],
   "source": [
    "test.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T05:35:40.647479Z",
     "iopub.status.busy": "2024-07-07T05:35:40.647056Z",
     "iopub.status.idle": "2024-07-07T06:04:05.299653Z",
     "shell.execute_reply": "2024-07-07T06:04:05.298472Z",
     "shell.execute_reply.started": "2024-07-07T05:35:40.647444Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Function to compute word count\n",
    "def word_count(text):\n",
    "    return len(nltk.word_tokenize(text))\n",
    "\n",
    "# Function to compute character count\n",
    "def char_count(text):\n",
    "    return len(text)\n",
    "\n",
    "# Function to compute sentence count\n",
    "def sentence_count(text):\n",
    "    return len(nltk.sent_tokenize(text))\n",
    "\n",
    "# Function to compute average word length\n",
    "def avg_word_length(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    return sum(len(word) for word in words) / len(words)\n",
    "\n",
    "# Function to compute average sentence length\n",
    "def avg_sentence_length(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    if len(sentences) == 0:\n",
    "        return 0\n",
    "    return len(words) / len(sentences)\n",
    "\n",
    "# Function to compute type-token ratio\n",
    "def ttr(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    if len(words) == 0:\n",
    "        return 0\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words) / len(words)\n",
    "\n",
    "# Function to compute word frequency\n",
    "def word_freq(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return Counter(words)\n",
    "\n",
    "# Function to compute bigram frequency\n",
    "def bigram_freq(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    bigrams = list(nltk.bigrams(words))\n",
    "    return Counter(bigrams)\n",
    "\n",
    "# Function to compute readability scores\n",
    "def readability_scores(text):\n",
    "    scores = {\n",
    "        \"flesch_kincaid_score\": textstat.flesch_kincaid_grade(text),\n",
    "        \"gunning_fog_index\": textstat.gunning_fog(text),\n",
    "        \"smog_index\": textstat.smog_index(text),\n",
    "        \"ari\": textstat.automated_readability_index(text)\n",
    "    }\n",
    "    return scores\n",
    "\n",
    "# Compute additional metrics and add to DataFrame\n",
    "for column in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    train[f\"{column}_word_count\"] = train[column].apply(word_count)\n",
    "    train[f\"{column}_char_count\"] = train[column].apply(char_count)\n",
    "    train[f\"{column}_sentence_count\"] = train[column].apply(sentence_count)\n",
    "    train[f\"{column}_avg_word_length\"] = train[column].apply(avg_word_length)\n",
    "    train[f\"{column}_avg_sentence_length\"] = train[column].apply(avg_sentence_length)\n",
    "#     train[f\"{column}_ttr\"] = train[column].apply(ttr)\n",
    "#     readability = train[column].apply(readability_scores)\n",
    "#     train[f\"{column}_flesch_kincaid_score\"] = readability.apply(lambda x: x[\"flesch_kincaid_score\"])\n",
    "#     train[f\"{column}_gunning_fog_index\"] = readability.apply(lambda x: x[\"gunning_fog_index\"])\n",
    "#     train[f\"{column}_smog_index\"] = readability.apply(lambda x: x[\"smog_index\"])\n",
    "#     train[f\"{column}_ari\"] = readability.apply(lambda x: x[\"ari\"])\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T09:46:55.217811Z",
     "iopub.status.busy": "2024-07-07T09:46:55.217348Z",
     "iopub.status.idle": "2024-07-07T10:38:17.625572Z",
     "shell.execute_reply": "2024-07-07T10:38:17.623941Z",
     "shell.execute_reply.started": "2024-07-07T09:46:55.217775Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "# Convert the target into a single column with categorical labels\n",
    "train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)\n",
    "\n",
    "# Define features and target\n",
    "columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', \n",
    "                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}\n",
    "\n",
    "features = [col for col in train.columns if col not in columns_to_remove]\n",
    "\n",
    "X = train[features]\n",
    "y = train['winner'] - 1\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'GradientBoostingClassifier': GradientBoostingClassifier(),\n",
    "    'XGBClassifier': XGBClassifier()\n",
    "}\n",
    "\n",
    "# Define the parameter distributions for random search\n",
    "param_distributions = {\n",
    "    'GradientBoostingClassifier': {\n",
    "        'n_estimators': [100,200,350,300],\n",
    "        'max_depth': [2,3,4,5,7,9]\n",
    "    },\n",
    "    'XGBClassifier': {\n",
    "        'n_estimators': [100,200,350,300],\n",
    "        'max_depth': [2,3,4,5,7,9]\n",
    "    }\n",
    "}\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)\n",
    "\n",
    "best_models = {}  # Dictionary to store the best models\n",
    "\n",
    "# Iterate over each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Model training for {model_name}\")\n",
    "    \n",
    "    # Perform RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=10, scoring='neg_log_loss', \n",
    "                                       n_jobs=-1, cv=skf, random_state=42)\n",
    "    random_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = random_search.best_estimator_\n",
    "    best_models[model_name] = best_model  # Store the best model for the current type\n",
    "    \n",
    "    logloss_scores = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    count = 0\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        best_model.fit(X_train_fold, y_train_fold)\n",
    "        y_test_pred_proba = best_model.predict_proba(X_test_fold)\n",
    "\n",
    "        logloss = log_loss(y_test_fold, y_test_pred_proba)\n",
    "        logloss_scores.append(logloss)\n",
    "        print(f\"The log loss score for fold {count}: {logloss}\")\n",
    "        count += 1\n",
    "\n",
    "    average_logloss = sum(logloss_scores) / len(logloss_scores)\n",
    "    print(f\"The average log loss score for {model_name} across all folds: {average_logloss}\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Time taken for {model_name}: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict probabilities on the validation set\n",
    "    y_val_prob = best_model.predict_proba(X_val)\n",
    "    # Calculate log loss on the validation set\n",
    "    val_loss = log_loss(y_val, y_val_prob)\n",
    "    print(f'Log Loss using {model_name} on validation set: {val_loss}')\n",
    "\n",
    "# Identify the best model based on validation set performance\n",
    "best_model_name = min(best_models, key=lambda k: log_loss(y_val, best_models[k].predict_proba(X_val)))\n",
    "best_average_logloss = log_loss(y_val, best_models[best_model_name].predict_proba(X_val))\n",
    "\n",
    "print(f\"The best model is {best_model_name} with an average log loss score of {best_average_logloss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:43.661176Z",
     "iopub.status.busy": "2024-07-07T10:42:43.660688Z",
     "iopub.status.idle": "2024-07-07T10:42:43.671235Z",
     "shell.execute_reply": "2024-07-07T10:42:43.669902Z",
     "shell.execute_reply.started": "2024-07-07T10:42:43.66114Z"
    }
   },
   "outputs": [],
   "source": [
    "model_to_use = best_models[best_model_name]\n",
    "model_to_use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:47.716873Z",
     "iopub.status.busy": "2024-07-07T10:42:47.716419Z",
     "iopub.status.idle": "2024-07-07T10:42:47.838442Z",
     "shell.execute_reply": "2024-07-07T10:42:47.837278Z",
     "shell.execute_reply.started": "2024-07-07T10:42:47.716839Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute additional metrics and add to DataFrame\n",
    "for column in [\"prompt\", \"response_a\", \"response_b\"]:\n",
    "    test[f\"{column}_word_count\"] = test[column].apply(word_count)\n",
    "    test[f\"{column}_char_count\"] = test[column].apply(char_count)\n",
    "    test[f\"{column}_sentence_count\"] = test[column].apply(sentence_count)\n",
    "    test[f\"{column}_avg_word_length\"] = test[column].apply(avg_word_length)\n",
    "    test[f\"{column}_avg_sentence_length\"] = test[column].apply(avg_sentence_length)\n",
    "    \n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:50.207123Z",
     "iopub.status.busy": "2024-07-07T10:42:50.20663Z",
     "iopub.status.idle": "2024-07-07T10:42:50.220057Z",
     "shell.execute_reply": "2024-07-07T10:42:50.218288Z",
     "shell.execute_reply.started": "2024-07-07T10:42:50.207087Z"
    }
   },
   "outputs": [],
   "source": [
    "test_features = test[features]\n",
    "test_predictions = model_to_use.predict_proba(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:52.48017Z",
     "iopub.status.busy": "2024-07-07T10:42:52.479148Z",
     "iopub.status.idle": "2024-07-07T10:42:52.488907Z",
     "shell.execute_reply": "2024-07-07T10:42:52.487241Z",
     "shell.execute_reply.started": "2024-07-07T10:42:52.480126Z"
    }
   },
   "outputs": [],
   "source": [
    "test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:53.791742Z",
     "iopub.status.busy": "2024-07-07T10:42:53.791246Z",
     "iopub.status.idle": "2024-07-07T10:42:53.800145Z",
     "shell.execute_reply": "2024-07-07T10:42:53.798649Z",
     "shell.execute_reply.started": "2024-07-07T10:42:53.791699Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'winner_model_a': test_predictions[:, 0],\n",
    "    'winner_model_b': test_predictions[:, 1],\n",
    "    'winner_tie': test_predictions[:, 2]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:55.43495Z",
     "iopub.status.busy": "2024-07-07T10:42:55.434499Z",
     "iopub.status.idle": "2024-07-07T10:42:55.44877Z",
     "shell.execute_reply": "2024-07-07T10:42:55.447018Z",
     "shell.execute_reply.started": "2024-07-07T10:42:55.434893Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-07T10:42:57.389724Z",
     "iopub.status.busy": "2024-07-07T10:42:57.389244Z",
     "iopub.status.idle": "2024-07-07T10:42:57.397559Z",
     "shell.execute_reply": "2024-07-07T10:42:57.396252Z",
     "shell.execute_reply.started": "2024-07-07T10:42:57.389692Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.to_csv('/kaggle/working/submission.csv', index= False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 1325242,
     "sourceId": 2206666,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
