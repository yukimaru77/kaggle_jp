{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# LMSYS - Chatbot Arena Human Preference Predictions\n\n","metadata":{}},{"cell_type":"markdown","source":"**Due to the size of the train data, and I only using 0.5% of the train data!**\n\nWIP: Compute embeddings using TPU in a differente notebook to use the full train data and then load the embeddings here!","metadata":{}},{"cell_type":"markdown","source":"## Install and load libraries","metadata":{}},{"cell_type":"code","source":"!pip install textstat SweetViz","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:33:41.415887Z","iopub.execute_input":"2024-07-15T10:33:41.416352Z","iopub.status.idle":"2024-07-15T10:33:59.382928Z","shell.execute_reply.started":"2024-07-15T10:33:41.416315Z","shell.execute_reply":"2024-07-15T10:33:59.381338Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport sweetviz as sv\n\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport textstat\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom transformers import BertTokenizer, TFBertModel\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\n\nfrom sklearn.metrics import accuracy_score, \\\n                            log_loss, \\\n                            confusion_matrix, \\\n                            classification_report","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-15T10:33:59.385457Z","iopub.execute_input":"2024-07-15T10:33:59.385882Z","iopub.status.idle":"2024-07-15T10:34:26.626598Z","shell.execute_reply.started":"2024-07-15T10:33:59.385844Z","shell.execute_reply":"2024-07-15T10:34:26.625291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english'))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:26.628304Z","iopub.execute_input":"2024-07-15T10:34:26.629331Z","iopub.status.idle":"2024-07-15T10:34:26.705902Z","shell.execute_reply.started":"2024-07-15T10:34:26.629283Z","shell.execute_reply":"2024-07-15T10:34:26.704543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loading data","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ntest_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:26.708734Z","iopub.execute_input":"2024-07-15T10:34:26.709185Z","iopub.status.idle":"2024-07-15T10:34:30.485346Z","shell.execute_reply.started":"2024-07-15T10:34:26.709137Z","shell.execute_reply":"2024-07-15T10:34:30.483633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)","metadata":{}},{"cell_type":"code","source":"train_analysis = sv.analyze(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:30.486668Z","iopub.execute_input":"2024-07-15T10:34:30.487034Z","iopub.status.idle":"2024-07-15T10:34:39.325863Z","shell.execute_reply.started":"2024-07-15T10:34:30.487001Z","shell.execute_reply":"2024-07-15T10:34:39.32424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_analysis.show_html('train_analysis.html')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:39.327819Z","iopub.execute_input":"2024-07-15T10:34:39.328624Z","iopub.status.idle":"2024-07-15T10:34:39.67726Z","shell.execute_reply.started":"2024-07-15T10:34:39.328578Z","shell.execute_reply":"2024-07-15T10:34:39.675712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:39.678987Z","iopub.execute_input":"2024-07-15T10:34:39.679353Z","iopub.status.idle":"2024-07-15T10:34:39.700291Z","shell.execute_reply.started":"2024-07-15T10:34:39.679322Z","shell.execute_reply":"2024-07-15T10:34:39.699087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Training Data -\", train_data.shape)\nprint(\"Test Data -\", test_data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:39.701764Z","iopub.execute_input":"2024-07-15T10:34:39.702126Z","iopub.status.idle":"2024-07-15T10:34:39.708363Z","shell.execute_reply.started":"2024-07-15T10:34:39.702092Z","shell.execute_reply":"2024-07-15T10:34:39.707161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.describe(include=['O'])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:39.709973Z","iopub.execute_input":"2024-07-15T10:34:39.710466Z","iopub.status.idle":"2024-07-15T10:34:39.979519Z","shell.execute_reply.started":"2024-07-15T10:34:39.710412Z","shell.execute_reply":"2024-07-15T10:34:39.977988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_data.info())","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:39.985109Z","iopub.execute_input":"2024-07-15T10:34:39.985526Z","iopub.status.idle":"2024-07-15T10:34:40.039905Z","shell.execute_reply.started":"2024-07-15T10:34:39.985495Z","shell.execute_reply":"2024-07-15T10:34:40.038678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.drop(\"id\", axis=1).duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:40.041135Z","iopub.execute_input":"2024-07-15T10:34:40.041517Z","iopub.status.idle":"2024-07-15T10:34:40.551835Z","shell.execute_reply.started":"2024-07-15T10:34:40.041484Z","shell.execute_reply":"2024-07-15T10:34:40.550554Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There exist 14 duplicated rows forming 7 groups, I will just keep one row per group.","metadata":{}},{"cell_type":"code","source":"train_data = train_data.drop_duplicates(keep=\"first\", ignore_index=True)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:40.553437Z","iopub.execute_input":"2024-07-15T10:34:40.55393Z","iopub.status.idle":"2024-07-15T10:34:41.059542Z","shell.execute_reply.started":"2024-07-15T10:34:40.553889Z","shell.execute_reply":"2024-07-15T10:34:41.058435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Checking the quality of the train data with respect the `id` column.","metadata":{}},{"cell_type":"code","source":"train_data.nunique()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:41.060808Z","iopub.execute_input":"2024-07-15T10:34:41.061181Z","iopub.status.idle":"2024-07-15T10:34:41.60334Z","shell.execute_reply.started":"2024-07-15T10:34:41.061135Z","shell.execute_reply":"2024-07-15T10:34:41.602224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert train_data[\"id\"].nunique() == len(train_data)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:41.605228Z","iopub.execute_input":"2024-07-15T10:34:41.605597Z","iopub.status.idle":"2024-07-15T10:34:41.613706Z","shell.execute_reply.started":"2024-07-15T10:34:41.605566Z","shell.execute_reply":"2024-07-15T10:34:41.61272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:41.615071Z","iopub.execute_input":"2024-07-15T10:34:41.615433Z","iopub.status.idle":"2024-07-15T10:34:41.666185Z","shell.execute_reply.started":"2024-07-15T10:34:41.615402Z","shell.execute_reply":"2024-07-15T10:34:41.664758Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Distribution","metadata":{}},{"cell_type":"code","source":"model_df = pd.concat([train_data.model_a, train_data.model_b])\ncounts = model_df.value_counts().reset_index()\ncounts.columns = ['LLM', 'Count']\n\n# Create a bar plot with custom styling using Plotly\nfig = px.bar(counts, x='LLM', y='Count',\n             title='Distribution of LLMs',\n             color='Count')\n\nfig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:41.667864Z","iopub.execute_input":"2024-07-15T10:34:41.668356Z","iopub.status.idle":"2024-07-15T10:34:43.90637Z","shell.execute_reply.started":"2024-07-15T10:34:41.668315Z","shell.execute_reply":"2024-07-15T10:34:43.905165Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"counts_a = train_data['winner_model_a'].value_counts().reset_index()\ncounts_b = train_data['winner_model_b'].value_counts().reset_index()\ncounts_tie = train_data['winner_tie'].value_counts().reset_index()\n\n# Renaming columns for convinience\ncounts_a.columns = ['Winner', 'Count']\ncounts_b.columns = ['Winner', 'Count']\ncounts_tie.columns = ['Winner', 'Count']\n\n# Adding column to identify the model\ncounts_a['Model'] = 'Model A'\ncounts_b['Model'] = 'Model B'\ncounts_tie['Model'] = 'Tie'\n\ncounts = pd.concat([counts_a, counts_b, counts_tie])\n\nfig = px.bar(counts, x='Model', y='Count', \n             color='Model',\n             title='Winner Distribution for Train Data',\n             labels={'Model': 'Model', 'Count': 'Win Count', 'Winner': 'Winner'})\n\nfig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Win Count\")\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:43.907864Z","iopub.execute_input":"2024-07-15T10:34:43.908277Z","iopub.status.idle":"2024-07-15T10:34:44.035778Z","shell.execute_reply.started":"2024-07-15T10:34:43.908231Z","shell.execute_reply":"2024-07-15T10:34:44.034645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Conclusions:\n\n* There are 57477 training rows and 3 test rows.\n    * Note: Test data will be replaced with the full test set (~25k rows, 70% for private LB) during scoring phase.\n* The column `id` has no duplicated values.\n* Model identities aren't revealed in the test set.\n* Strings in columns prompt, `response_a`, and `response_a` are wrapped in a list. \n    * The reason is that each chat can contains more than one prompt/response pairs.\n* After dropping `id` column, there exist 14 duplicated rows forming 7 groups, we just keep one row per group and shape of the training DataFrame becomes (57470, 8).","metadata":{}},{"cell_type":"markdown","source":"## Data preparation and Feature Engineering\n\n* Cleaning data: clean text, such as removing special characters, normalizing to lowercase, removing stopwords and tokenizing.\n* Tokenize Inputs: using the TensorFlow/Kerar tokenizer by training on training data and fitting on both training and test data.\n* Padding sequences to `max_len`.\n* Create BERT embeddings.\n* Compute Similarity Features using BERT between the prompt and responses for each model. \n* Compute word count, character count, and lexical diversity for each response.\n* Tokenize the text inputs for the BERT model.","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv').sample(frac=0.001)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:44.037333Z","iopub.execute_input":"2024-07-15T10:34:44.037769Z","iopub.status.idle":"2024-07-15T10:34:46.277777Z","shell.execute_reply.started":"2024-07-15T10:34:44.037729Z","shell.execute_reply":"2024-07-15T10:34:46.276634Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cleaning data\n\nClean text, such as removing special characters, normalizing to lowercase and removing stopwords.","metadata":{}},{"cell_type":"code","source":"def clean_text(text):\n    text = text.lower()\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'<.*?>+', '', text)\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub(r'\\n', '', text)\n    text = re.sub(r'\\w*\\d\\w*', '', text)\n    text = ' '.join(word for word in text.split() if word not in stop_words)\n    return text","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.280195Z","iopub.execute_input":"2024-07-15T10:34:46.280599Z","iopub.status.idle":"2024-07-15T10:34:46.28861Z","shell.execute_reply.started":"2024-07-15T10:34:46.280566Z","shell.execute_reply":"2024-07-15T10:34:46.287229Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cleaning texts\ntrain_data['prompt_clean'] = train_data['prompt'].apply(clean_text)\ntrain_data['response_a_clean'] = train_data['response_a'].apply(clean_text)\ntrain_data['response_b_clean'] = train_data['response_b'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.290311Z","iopub.execute_input":"2024-07-15T10:34:46.290769Z","iopub.status.idle":"2024-07-15T10:34:46.316756Z","shell.execute_reply.started":"2024-07-15T10:34:46.290725Z","shell.execute_reply":"2024-07-15T10:34:46.315617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tokenize Inputs\n\nUsing the TensorFlow/Kerar tokenizer on both training and test data. Padding sequences to `max_len`.","metadata":{}},{"cell_type":"code","source":"max_len = 512","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.318825Z","iopub.execute_input":"2024-07-15T10:34:46.319341Z","iopub.status.idle":"2024-07-15T10:34:46.326669Z","shell.execute_reply.started":"2024-07-15T10:34:46.319296Z","shell.execute_reply":"2024-07-15T10:34:46.325432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = Tokenizer(num_words=20000)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.328108Z","iopub.execute_input":"2024-07-15T10:34:46.328517Z","iopub.status.idle":"2024-07-15T10:34:46.336728Z","shell.execute_reply.started":"2024-07-15T10:34:46.328475Z","shell.execute_reply":"2024-07-15T10:34:46.335637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer.fit_on_texts(pd.concat([train_data['prompt_clean'], train_data['response_a_clean'], train_data['response_b_clean']]))","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.338196Z","iopub.execute_input":"2024-07-15T10:34:46.338587Z","iopub.status.idle":"2024-07-15T10:34:46.355879Z","shell.execute_reply.started":"2024-07-15T10:34:46.338554Z","shell.execute_reply":"2024-07-15T10:34:46.354609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_sequences = tokenizer.texts_to_sequences(train_data['prompt_clean'])\nresponse_a_sequences = tokenizer.texts_to_sequences(train_data['response_a_clean'])\nresponse_b_sequences = tokenizer.texts_to_sequences(train_data['response_b_clean'])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.357412Z","iopub.execute_input":"2024-07-15T10:34:46.357828Z","iopub.status.idle":"2024-07-15T10:34:46.36959Z","shell.execute_reply.started":"2024-07-15T10:34:46.357793Z","shell.execute_reply":"2024-07-15T10:34:46.36843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padding\ntrain_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')\nresponse_a_sequences = pad_sequences(response_a_sequences, maxlen=max_len, padding='post')\nresponse_b_sequences = pad_sequences(response_b_sequences, maxlen=max_len, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.37117Z","iopub.execute_input":"2024-07-15T10:34:46.371665Z","iopub.status.idle":"2024-07-15T10:34:46.382058Z","shell.execute_reply.started":"2024-07-15T10:34:46.371623Z","shell.execute_reply":"2024-07-15T10:34:46.380991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sentiment Analysis\n\nSentiment analysis using `vaderSentiment`. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media","metadata":{}},{"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.383538Z","iopub.execute_input":"2024-07-15T10:34:46.38392Z","iopub.status.idle":"2024-07-15T10:34:46.410086Z","shell.execute_reply.started":"2024-07-15T10:34:46.383888Z","shell.execute_reply":"2024-07-15T10:34:46.40907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sentiment_analysis(text):\n    return analyzer.polarity_scores(text)['compound']","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.411397Z","iopub.execute_input":"2024-07-15T10:34:46.411742Z","iopub.status.idle":"2024-07-15T10:34:46.41693Z","shell.execute_reply.started":"2024-07-15T10:34:46.411701Z","shell.execute_reply":"2024-07-15T10:34:46.41562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['sentiment_prompt'] = train_data['prompt_clean'].apply(sentiment_analysis)\ntrain_data['sentiment_response_a'] = train_data['response_a_clean'].apply(sentiment_analysis)\ntrain_data['sentiment_response_b'] = train_data['response_b_clean'].apply(sentiment_analysis)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.427898Z","iopub.execute_input":"2024-07-15T10:34:46.428431Z","iopub.status.idle":"2024-07-15T10:34:46.46698Z","shell.execute_reply.started":"2024-07-15T10:34:46.428396Z","shell.execute_reply":"2024-07-15T10:34:46.46566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Features\n\nCalculate text features such as word count, character count, \nlexical diversity, syllable count, sentence count and \ncalculating the Flesch Reading Ease score (quantitative \nmeasurement of how readable a piece of text is) for each response. \n\nI am using `textstat` library that analyze text statistics.","metadata":{}},{"cell_type":"code","source":"def word_count(text):\n    return len(text.split())\n\ndef char_count(text):\n    return len(text)\n\ndef lexical_diversity(text):\n    words = text.split()\n    return len(set(words)) / len(words) if words else 0\n\ndef syllable_count(text):\n    return textstat.syllable_count(text)\n\ndef sentence_count(text):\n    return textstat.sentence_count(text)\n\ndef flesch_reading_ease(text):\n    return textstat.flesch_reading_ease(text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.468702Z","iopub.execute_input":"2024-07-15T10:34:46.469188Z","iopub.status.idle":"2024-07-15T10:34:46.479754Z","shell.execute_reply.started":"2024-07-15T10:34:46.46913Z","shell.execute_reply":"2024-07-15T10:34:46.478069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['word_count_prompt'] = train_data['prompt_clean'].apply(word_count)\ntrain_data['word_count_response_a'] = train_data['response_a_clean'].apply(word_count)\ntrain_data['word_count_response_b'] = train_data['response_b_clean'].apply(word_count)\ntrain_data['char_count_prompt'] = train_data['prompt_clean'].apply(char_count)\ntrain_data['char_count_response_a'] = train_data['response_a_clean'].apply(char_count)\ntrain_data['char_count_response_b'] = train_data['response_b_clean'].apply(char_count)\ntrain_data['lexical_diversity_prompt'] = train_data['prompt_clean'].apply(lexical_diversity)\ntrain_data['lexical_diversity_response_a'] = train_data['response_a_clean'].apply(lexical_diversity)\ntrain_data['lexical_diversity_response_b'] = train_data['response_b_clean'].apply(lexical_diversity)\ntrain_data['syllable_count_prompt'] = train_data['prompt_clean'].apply(syllable_count)\ntrain_data['syllable_count_response_a'] = train_data['response_a_clean'].apply(syllable_count)\ntrain_data['syllable_count_response_b'] = train_data['response_b_clean'].apply(syllable_count)\ntrain_data['sentence_count_prompt'] = train_data['prompt_clean'].apply(sentence_count)\ntrain_data['sentence_count_response_a'] = train_data['response_a_clean'].apply(sentence_count)\ntrain_data['sentence_count_response_b'] = train_data['response_b_clean'].apply(sentence_count)\ntrain_data['flesch_reading_ease_prompt'] = train_data['prompt_clean'].apply(flesch_reading_ease)\ntrain_data['flesch_reading_ease_response_a'] = train_data['response_a_clean'].apply(flesch_reading_ease)\ntrain_data['flesch_reading_ease_response_b'] = train_data['response_b_clean'].apply(flesch_reading_ease)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.481399Z","iopub.execute_input":"2024-07-15T10:34:46.481803Z","iopub.status.idle":"2024-07-15T10:34:46.555079Z","shell.execute_reply.started":"2024-07-15T10:34:46.481769Z","shell.execute_reply":"2024-07-15T10:34:46.553915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Create BERT embeddings\n\nCompute BERT embeddings for prompt and response for both train and test data.\nAlso compute Cosine Similarity features using BERT between the prompt and responses for each model. \n\nI will use `tf.data.Dataset` to create an efficient pipeline, and process the features in batches using GPU. Also I am going to save the intermediate embeddings using `joblib` library","metadata":{}},{"cell_type":"code","source":"# Load BERT\nbert_model_name = 'bert-base-uncased'\nbert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)\nbert_model = TFBertModel.from_pretrained(bert_model_name)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:46.556727Z","iopub.execute_input":"2024-07-15T10:34:46.557088Z","iopub.status.idle":"2024-07-15T10:34:52.940246Z","shell.execute_reply.started":"2024-07-15T10:34:46.557055Z","shell.execute_reply":"2024-07-15T10:34:52.939065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef get_bert_embeddings(texts):\n    inputs = bert_tokenizer(texts, \n                       return_tensors='tf', \n                       padding=True, \n                       truncation=True, \n                       max_length=512)\n    outputs = bert_model(inputs)\n    return outputs.last_hidden_state[:, 0, :]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:52.941823Z","iopub.execute_input":"2024-07-15T10:34:52.942183Z","iopub.status.idle":"2024-07-15T10:34:52.949586Z","shell.execute_reply.started":"2024-07-15T10:34:52.942133Z","shell.execute_reply":"2024-07-15T10:34:52.948307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_column(column_data):\n    column_data = column_data.dropna().tolist()\n    column_data = [str(text) for text in column_data]  \n    dataset = tf.data.Dataset.from_tensor_slices(column_data)\n    dataset = dataset.batch(8)  \n    \n    embeddings = []\n    for batch in dataset:\n        batch_list = [str(text) for text in batch.numpy().tolist()]  \n        batch_embeddings = get_bert_embeddings(batch_list)\n        embeddings.append(batch_embeddings)\n    \n    return np.concatenate(embeddings, axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:52.950899Z","iopub.execute_input":"2024-07-15T10:34:52.951293Z","iopub.status.idle":"2024-07-15T10:34:52.972941Z","shell.execute_reply.started":"2024-07-15T10:34:52.95126Z","shell.execute_reply":"2024-07-15T10:34:52.971525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def add_embeddings_to_dataframe(df, column_names):\n    for column in column_names:\n        print(f\"Processing column: {column}\")\n        embeddings = process_column(df[column])\n        df[f'{column}_embedding'] = list(embeddings)\n    return df","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:52.974743Z","iopub.execute_input":"2024-07-15T10:34:52.975178Z","iopub.status.idle":"2024-07-15T10:34:52.983785Z","shell.execute_reply.started":"2024-07-15T10:34:52.975124Z","shell.execute_reply":"2024-07-15T10:34:52.982354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"columns_to_embed = ['prompt_clean', 'response_a_clean', 'response_b_clean']","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:52.985495Z","iopub.execute_input":"2024-07-15T10:34:52.985878Z","iopub.status.idle":"2024-07-15T10:34:52.99625Z","shell.execute_reply.started":"2024-07-15T10:34:52.985846Z","shell.execute_reply":"2024-07-15T10:34:52.994963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = add_embeddings_to_dataframe(train_data, columns_to_embed)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:34:52.997849Z","iopub.execute_input":"2024-07-15T10:34:52.998794Z","iopub.status.idle":"2024-07-15T10:39:58.701392Z","shell.execute_reply.started":"2024-07-15T10:34:52.998754Z","shell.execute_reply":"2024-07-15T10:39:58.700299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data['similarity_prompt_response_a'] = train_data.apply(\n    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),\n                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)\n\ntrain_data['similarity_prompt_response_b'] = train_data.apply(\n    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),\n                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.702927Z","iopub.execute_input":"2024-07-15T10:39:58.7034Z","iopub.status.idle":"2024-07-15T10:39:58.780638Z","shell.execute_reply.started":"2024-07-15T10:39:58.703357Z","shell.execute_reply":"2024-07-15T10:39:58.779496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Prepare Data","metadata":{}},{"cell_type":"code","source":"X = train_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',\n                'char_count_prompt', 'char_count_response_a', 'char_count_response_b',\n                'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',\n                'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',\n                'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',\n                'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',\n                'similarity_prompt_response_a', 'similarity_prompt_response_b', \n                'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]\n","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.78195Z","iopub.execute_input":"2024-07-15T10:39:58.782321Z","iopub.status.idle":"2024-07-15T10:39:58.791644Z","shell.execute_reply.started":"2024-07-15T10:39:58.782289Z","shell.execute_reply":"2024-07-15T10:39:58.790528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definindo a coluna alvo\ntrain_data['winner'] = train_data.apply(lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2), axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.793015Z","iopub.execute_input":"2024-07-15T10:39:58.793424Z","iopub.status.idle":"2024-07-15T10:39:58.809536Z","shell.execute_reply.started":"2024-07-15T10:39:58.793392Z","shell.execute_reply":"2024-07-15T10:39:58.807975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_data['winner']","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.811071Z","iopub.execute_input":"2024-07-15T10:39:58.811502Z","iopub.status.idle":"2024-07-15T10:39:58.820646Z","shell.execute_reply.started":"2024-07-15T10:39:58.811468Z","shell.execute_reply":"2024-07-15T10:39:58.819425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting training and validation data","metadata":{}},{"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, \n                                                  random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.822473Z","iopub.execute_input":"2024-07-15T10:39:58.82304Z","iopub.status.idle":"2024-07-15T10:39:58.835465Z","shell.execute_reply.started":"2024-07-15T10:39:58.822995Z","shell.execute_reply":"2024-07-15T10:39:58.834233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Defining Models\n\n* Random Forest\n* Logistic Regression\n* Support Vector Machin\n* Gradient Boosting\n* Neural Network","metadata":{}},{"cell_type":"code","source":"models = {\n    'Random Forest': RandomForestClassifier(),\n    'SVM': SVC(probability=True),\n    'Gradient Boosting': GradientBoostingClassifier()\n}","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.837088Z","iopub.execute_input":"2024-07-15T10:39:58.837525Z","iopub.status.idle":"2024-07-15T10:39:58.8461Z","shell.execute_reply.started":"2024-07-15T10:39:58.83749Z","shell.execute_reply":"2024-07-15T10:39:58.844944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Neural Network\ndef create_nn_model(input_shape):\n    model = Sequential()\n    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(64, activation='relu'))\n    model.add(Dropout(0.2))\n    model.add(Dense(3, activation='softmax'))\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.847872Z","iopub.execute_input":"2024-07-15T10:39:58.848381Z","iopub.status.idle":"2024-07-15T10:39:58.857714Z","shell.execute_reply.started":"2024-07-15T10:39:58.848339Z","shell.execute_reply":"2024-07-15T10:39:58.856626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nn_model = create_nn_model(X_train.shape[1])\nmodels['Neural Network'] = nn_model","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.859381Z","iopub.execute_input":"2024-07-15T10:39:58.860205Z","iopub.status.idle":"2024-07-15T10:39:58.949196Z","shell.execute_reply.started":"2024-07-15T10:39:58.860164Z","shell.execute_reply":"2024-07-15T10:39:58.947845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Training models and evaluating:","metadata":{}},{"cell_type":"code","source":"results = {}\n\nfor name, model in models.items():\n    print(f\"Treinando e avaliando {name}...\")\n    \n    if name == 'Neural Network':\n        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=2)\n        y_pred = np.argmax(model.predict(X_val), axis=1)\n        y_pred_proba = model.predict(X_val)\n    else:\n        model.fit(X_train, y_train)\n        y_pred = model.predict(X_val)\n        y_pred_proba = model.predict_proba(X_val)\n\n    accuracy = accuracy_score(y_val, y_pred)\n    logloss = log_loss(y_val, y_pred_proba)\n\n    results[name] = {\n        'Acurácia': accuracy,\n        'Log Loss': logloss,\n        'Relatório de Classificação': classification_report(y_val, y_pred),\n        'Matriz de Confusão': confusion_matrix(y_val, y_pred)\n    }\n\n    print(f\"Acurácia de {name}: {accuracy}\")\n    print(f\"Log Loss de {name}: {logloss}\")\n    print(f\"Relatório de Classificação de {name}:\\n{classification_report(y_val, y_pred)}\")\n    print(f\"Matriz de Confusão de {name}:\\n{confusion_matrix(y_val, y_pred)}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:39:58.951172Z","iopub.execute_input":"2024-07-15T10:39:58.951637Z","iopub.status.idle":"2024-07-15T10:40:02.02086Z","shell.execute_reply.started":"2024-07-15T10:39:58.951595Z","shell.execute_reply":"2024-07-15T10:40:02.019202Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Selecting the best model with respect the Log Loss","metadata":{}},{"cell_type":"code","source":"best_model_name = max(results, key=lambda name: results[name]['Log Loss'])\nbest_model = models[best_model_name]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.022929Z","iopub.execute_input":"2024-07-15T10:40:02.023456Z","iopub.status.idle":"2024-07-15T10:40:02.030486Z","shell.execute_reply.started":"2024-07-15T10:40:02.023412Z","shell.execute_reply":"2024-07-15T10:40:02.029099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prediction and Submission\n\nPreparing test data, predicting on test data and creating submission data.","metadata":{}},{"cell_type":"code","source":"# Cleaning text from test data\ntest_data['prompt_clean'] = test_data['prompt'].apply(clean_text)\ntest_data['response_a_clean'] = test_data['response_a'].apply(clean_text)\ntest_data['response_b_clean'] = test_data['response_b'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.032348Z","iopub.execute_input":"2024-07-15T10:40:02.032815Z","iopub.status.idle":"2024-07-15T10:40:02.046864Z","shell.execute_reply.started":"2024-07-15T10:40:02.032778Z","shell.execute_reply":"2024-07-15T10:40:02.045704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Tokenize text from test data\ntest_sequences = tokenizer.texts_to_sequences(test_data['prompt_clean'])\nresponse_a_test_sequences = tokenizer.texts_to_sequences(test_data['response_a_clean'])\nresponse_b_test_sequences = tokenizer.texts_to_sequences(test_data['response_b_clean'])","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.048561Z","iopub.execute_input":"2024-07-15T10:40:02.04956Z","iopub.status.idle":"2024-07-15T10:40:02.060016Z","shell.execute_reply.started":"2024-07-15T10:40:02.049512Z","shell.execute_reply":"2024-07-15T10:40:02.058788Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Padding sequences from test data\ntest_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')\nresponse_a_test_sequences = pad_sequences(response_a_test_sequences, maxlen=max_len, padding='post')\nresponse_b_test_sequences = pad_sequences(response_b_test_sequences, maxlen=max_len, padding='post')","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.061838Z","iopub.execute_input":"2024-07-15T10:40:02.062371Z","iopub.status.idle":"2024-07-15T10:40:02.071771Z","shell.execute_reply.started":"2024-07-15T10:40:02.06233Z","shell.execute_reply":"2024-07-15T10:40:02.070235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Sentiment analysis for test data\ntest_data['sentiment_prompt'] = test_data['prompt_clean'].apply(sentiment_analysis)\ntest_data['sentiment_response_a'] = test_data['response_a_clean'].apply(sentiment_analysis)\ntest_data['sentiment_response_b'] = test_data['response_b_clean'].apply(sentiment_analysis)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.073274Z","iopub.execute_input":"2024-07-15T10:40:02.073651Z","iopub.status.idle":"2024-07-15T10:40:02.087868Z","shell.execute_reply.started":"2024-07-15T10:40:02.073619Z","shell.execute_reply":"2024-07-15T10:40:02.086574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating features of text structure from test data\ntest_data['word_count_prompt'] = test_data['prompt_clean'].apply(word_count)\ntest_data['word_count_response_a'] = test_data['response_a_clean'].apply(word_count)\ntest_data['word_count_response_b'] = test_data['response_b_clean'].apply(word_count)\ntest_data['char_count_prompt'] = test_data['prompt_clean'].apply(char_count)\ntest_data['char_count_response_a'] = test_data['response_a_clean'].apply(char_count)\ntest_data['char_count_response_b'] = test_data['response_b_clean'].apply(char_count)\ntest_data['lexical_diversity_prompt'] = test_data['prompt_clean'].apply(lexical_diversity)\ntest_data['lexical_diversity_response_a'] = test_data['response_a_clean'].apply(lexical_diversity)\ntest_data['lexical_diversity_response_b'] = test_data['response_b_clean'].apply(lexical_diversity)\ntest_data['syllable_count_prompt'] = test_data['prompt_clean'].apply(syllable_count)\ntest_data['syllable_count_response_a'] = test_data['response_a_clean'].apply(syllable_count)\ntest_data['syllable_count_response_b'] = test_data['response_b_clean'].apply(syllable_count)\ntest_data['sentence_count_prompt'] = test_data['prompt_clean'].apply(sentence_count)\ntest_data['sentence_count_response_a'] = test_data['response_a_clean'].apply(sentence_count)\ntest_data['sentence_count_response_b'] = test_data['response_b_clean'].apply(sentence_count)\ntest_data['flesch_reading_ease_prompt'] = test_data['prompt_clean'].apply(flesch_reading_ease)\ntest_data['flesch_reading_ease_response_a'] = test_data['response_a_clean'].apply(flesch_reading_ease)\ntest_data['flesch_reading_ease_response_b'] = test_data['response_b_clean'].apply(flesch_reading_ease)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.089469Z","iopub.execute_input":"2024-07-15T10:40:02.089988Z","iopub.status.idle":"2024-07-15T10:40:02.117195Z","shell.execute_reply.started":"2024-07-15T10:40:02.089945Z","shell.execute_reply":"2024-07-15T10:40:02.115839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Embedding from test data\ntest_data = add_embeddings_to_dataframe(test_data, columns_to_embed)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:02.118583Z","iopub.execute_input":"2024-07-15T10:40:02.119023Z","iopub.status.idle":"2024-07-15T10:40:12.682698Z","shell.execute_reply.started":"2024-07-15T10:40:02.11899Z","shell.execute_reply":"2024-07-15T10:40:12.681274Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cosine similarity from test data\ntest_data['similarity_prompt_response_a'] = test_data.apply(\n    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),\n                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)\n\ntest_data['similarity_prompt_response_b'] = test_data.apply(\n    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),\n                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:12.684291Z","iopub.execute_input":"2024-07-15T10:40:12.684691Z","iopub.status.idle":"2024-07-15T10:40:12.702179Z","shell.execute_reply.started":"2024-07-15T10:40:12.684656Z","shell.execute_reply":"2024-07-15T10:40:12.700706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',\n                    'char_count_prompt', 'char_count_response_a', 'char_count_response_b',\n                    'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',\n                    'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',\n                    'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',\n                    'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',\n                    'similarity_prompt_response_a', 'similarity_prompt_response_b', \n                    'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:12.703587Z","iopub.execute_input":"2024-07-15T10:40:12.703966Z","iopub.status.idle":"2024-07-15T10:40:12.72027Z","shell.execute_reply.started":"2024-07-15T10:40:12.703935Z","shell.execute_reply":"2024-07-15T10:40:12.718688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_pred_proba = best_model.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:12.722101Z","iopub.execute_input":"2024-07-15T10:40:12.722607Z","iopub.status.idle":"2024-07-15T10:40:12.855745Z","shell.execute_reply.started":"2024-07-15T10:40:12.722567Z","shell.execute_reply":"2024-07-15T10:40:12.854613Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame(test_data['id'])\nsubmission['winner_model_a'] = test_pred_proba[:, 0]\nsubmission['winner_model_b'] = test_pred_proba[:, 1]\nsubmission['winner_tie'] = test_pred_proba[:, 2]\n\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:12.85759Z","iopub.execute_input":"2024-07-15T10:40:12.857957Z","iopub.status.idle":"2024-07-15T10:40:12.870832Z","shell.execute_reply.started":"2024-07-15T10:40:12.857926Z","shell.execute_reply":"2024-07-15T10:40:12.8695Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2024-07-15T10:40:12.872415Z","iopub.execute_input":"2024-07-15T10:40:12.872888Z","iopub.status.idle":"2024-07-15T10:40:12.887076Z","shell.execute_reply.started":"2024-07-15T10:40:12.87285Z","shell.execute_reply":"2024-07-15T10:40:12.88578Z"},"trusted":true},"execution_count":null,"outputs":[]}]}