{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dcae374",
   "metadata": {},
   "source": [
    "## What this notebook is\n",
    "This notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).\n",
    "I used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.\n",
    "\n",
    "## Result\n",
    "\n",
    "I used `id % 5 == 0` as an evaluation set and used all the rest for training.\n",
    "\n",
    "| subset | log loss |\n",
    "| - | - |\n",
    "| eval | 0.9371|\n",
    "| LB | 0.941 |\n",
    "\n",
    "## What is QLoRA fine-tuning?\n",
    "\n",
    "In the conventional fine-tuning, weight ($\\mathbf{W}$) is updated as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{{\\partial L}}{{\\partial \\mathbf{W}}} = \\mathbf{W} + \\Delta \\mathbf{W}\n",
    "$$\n",
    "\n",
    "where $L$ is a loss at this step and $\\eta$ is a learning rate.\n",
    "\n",
    "[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\\Delta \\mathbf{W} \\in \\mathbb{R}^{\\text{d} \\times \\text{k}}$ by factorizing $\\Delta \\mathbf{W}$ into two (much) smaller matrices, $\\mathbf{B} \\in \\mathbb{R}^{\\text{d} \\times \\text{r}}$ and $\\mathbf{A} \\in \\mathbb{R}^{\\text{r} \\times \\text{k}}$ with $r \\ll \\text{min}(\\text{d}, \\text{k})$.\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W}_{s} \\approx \\mathbf{B} \\mathbf{A}\n",
    "$$\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/pii_data_detection/lora_diagram.png\">\n",
    "\n",
    "During training, only $\\mathbf{A}$ and $\\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.\n",
    "\n",
    "[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. \n",
    "Note that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.\n",
    "\n",
    "1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.\n",
    "\n",
    "## Note\n",
    "It takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.\n",
    "This notebook uses only 100 samples for demo purpose, but everything else is same as my setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T07:13:50.404672Z",
     "iopub.status.busy": "2024-07-11T07:13:50.404354Z",
     "iopub.status.idle": "2024-07-11T07:14:20.94426Z",
     "shell.execute_reply": "2024-07-11T07:14:20.943289Z",
     "shell.execute_reply.started": "2024-07-11T07:13:50.404645Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gemma-2 is available from transformers>=4.42.3\n",
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:20.946359Z",
     "iopub.status.busy": "2024-07-11T07:14:20.946064Z",
     "iopub.status.idle": "2024-07-11T07:14:38.94904Z",
     "shell.execute_reply": "2024-07-11T07:14:38.948106Z",
     "shell.execute_reply.started": "2024-07-11T07:14:20.946332Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61db61d",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:38.951055Z",
     "iopub.status.busy": "2024-07-11T07:14:38.950324Z",
     "iopub.status.idle": "2024-07-11T07:14:38.959597Z",
     "shell.execute_reply": "2024-07-11T07:14:38.958706Z",
     "shell.execute_reply.started": "2024-07-11T07:14:38.951021Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"\n",
    "    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4-bit quantized gemma-2-9b-instruct\n",
    "    max_length: int = 1024\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 2\n",
    "    gradient_accumulation_steps: int = 2  # global batch size is 8 \n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: float = lora_r * 2\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "    \n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5344ccb",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:38.962833Z",
     "iopub.status.busy": "2024-07-11T07:14:38.962415Z",
     "iopub.status.idle": "2024-07-11T07:14:39.037571Z",
     "shell.execute_reply": "2024-07-11T07:14:39.036838Z",
     "shell.execute_reply.started": "2024-07-11T07:14:38.962803Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4efc3a",
   "metadata": {},
   "source": [
    "#### LoRA config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:39.038969Z",
     "iopub.status.busy": "2024-07-11T07:14:39.038641Z",
     "iopub.status.idle": "2024-07-11T07:14:39.044206Z",
     "shell.execute_reply": "2024-07-11T07:14:39.043317Z",
     "shell.execute_reply.started": "2024-07-11T07:14:39.038945Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    # only target self-attention\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35517a",
   "metadata": {},
   "source": [
    "### Instantiate the tokenizer & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:39.045553Z",
     "iopub.status.busy": "2024-07-11T07:14:39.0453Z",
     "iopub.status.idle": "2024-07-11T07:14:41.322578Z",
     "shell.execute_reply": "2024-07-11T07:14:41.321566Z",
     "shell.execute_reply.started": "2024-07-11T07:14:39.04553Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:41.324066Z",
     "iopub.status.busy": "2024-07-11T07:14:41.323778Z",
     "iopub.status.idle": "2024-07-11T07:15:52.220632Z",
     "shell.execute_reply": "2024-07-11T07:15:52.219728Z",
     "shell.execute_reply.started": "2024-07-11T07:14:41.324043Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:52.222528Z",
     "iopub.status.busy": "2024-07-11T07:15:52.222259Z",
     "iopub.status.idle": "2024-07-11T07:15:52.233448Z",
     "shell.execute_reply": "2024-07-11T07:15:52.232506Z",
     "shell.execute_reply.started": "2024-07-11T07:15:52.222506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c435889",
   "metadata": {},
   "source": [
    "### Instantiate the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:52.234875Z",
     "iopub.status.busy": "2024-07-11T07:15:52.234511Z",
     "iopub.status.idle": "2024-07-11T07:15:55.421506Z",
     "shell.execute_reply": "2024-07-11T07:15:55.420793Z",
     "shell.execute_reply.started": "2024-07-11T07:15:52.234845Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:55.422997Z",
     "iopub.status.busy": "2024-07-11T07:15:55.422634Z",
     "iopub.status.idle": "2024-07-11T07:15:55.43295Z",
     "shell.execute_reply": "2024-07-11T07:15:55.432028Z",
     "shell.execute_reply.started": "2024-07-11T07:15:55.422961Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels=[]\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "            if a_win:\n",
    "                label = 0\n",
    "            elif b_win:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "        return {**tokenized, \"labels\": labels}\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:55.43584Z",
     "iopub.status.busy": "2024-07-11T07:15:55.435517Z",
     "iopub.status.idle": "2024-07-11T07:15:56.036485Z",
     "shell.execute_reply": "2024-07-11T07:15:56.035519Z",
     "shell.execute_reply.started": "2024-07-11T07:15:55.435805Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645436e",
   "metadata": {},
   "source": [
    "### Compute metrics\n",
    "\n",
    "We'll compute the log-loss used in LB and accuracy as a auxiliary metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.037921Z",
     "iopub.status.busy": "2024-07-11T07:15:56.037608Z",
     "iopub.status.idle": "2024-07-11T07:15:56.043676Z",
     "shell.execute_reply": "2024-07-11T07:15:56.042738Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.037896Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf35b02",
   "metadata": {},
   "source": [
    "### Split\n",
    "\n",
    "Here, train and eval is splitted according to their `id % 5`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.045232Z",
     "iopub.status.busy": "2024-07-11T07:15:56.044967Z",
     "iopub.status.idle": "2024-07-11T07:15:56.057744Z",
     "shell.execute_reply": "2024-07-11T07:15:56.056724Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.04521Z"
    }
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.059267Z",
     "iopub.status.busy": "2024-07-11T07:15:56.058902Z",
     "iopub.status.idle": "2024-07-11T07:21:12.617061Z",
     "shell.execute_reply": "2024-07-11T07:21:12.616188Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.059235Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
