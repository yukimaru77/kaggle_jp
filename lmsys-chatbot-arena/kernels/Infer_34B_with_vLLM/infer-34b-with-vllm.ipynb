{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8218776,"sourceType":"datasetVersion","datasetId":4871830},{"sourceId":8300737,"sourceType":"datasetVersion","datasetId":4746046},{"sourceId":8982890,"sourceType":"datasetVersion","datasetId":5409557}],"dockerImageVersionId":30747,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Submit LLM 34B Model in 5 hours!\nThis notebook demonstrates how to submit a LLM 34B model in only 5 hours! Amazing! The key tricks are:\n* use vLLM (for speed)\n* use AWQ 4bit quantization (to avoid GPU VRAM OOM)\n* limit input size to 1024 tokens (for speed)\n* limit output size to 1 token (for speed)","metadata":{}},{"cell_type":"markdown","source":"# Pip Install vLLM\nThe package vLLM is an incredibly fast LLM inference library! The vLLM that is installed in Kaggle notebooks will produce errors, therefore we need to reinstall vLLM. The code below was taken from notebook [here][1]\n\n[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution","metadata":{}},{"cell_type":"code","source":"import os, math, numpy as np\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"","metadata":{"execution":{"iopub.status.busy":"2024-07-17T05:49:10.666899Z","iopub.execute_input":"2024-07-17T05:49:10.667256Z","iopub.status.idle":"2024-07-17T05:49:10.681111Z","shell.execute_reply.started":"2024-07-17T05:49:10.667207Z","shell.execute_reply":"2024-07-17T05:49:10.68007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n!pip uninstall -y torch\n!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-17T05:49:10.682943Z","iopub.execute_input":"2024-07-17T05:49:10.683275Z","iopub.status.idle":"2024-07-17T05:52:22.306408Z","shell.execute_reply.started":"2024-07-17T05:49:10.683249Z","shell.execute_reply":"2024-07-17T05:52:22.305139Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load 34B Quantized Model with vLLM!\nWe will load and use LLM 34B Bagel [here][1]. This is a strong model.\n\n[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2","metadata":{}},{"cell_type":"code","source":"import vllm\n\nllm = vllm.LLM(\n    \"/kaggle/input/bagel-v3-343\",\n    quantization=\"awq\",\n    tensor_parallel_size=2, \n    gpu_memory_utilization=0.95, \n    trust_remote_code=True,\n    dtype=\"half\", \n    enforce_eager=True,\n    max_model_len=1024,\n    #distributed_executor_backend=\"ray\",\n)\ntokenizer = llm.get_tokenizer()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T05:52:25.217197Z","iopub.execute_input":"2024-07-17T05:52:25.217523Z","iopub.status.idle":"2024-07-17T05:54:46.759248Z","shell.execute_reply.started":"2024-07-17T05:52:25.217498Z","shell.execute_reply":"2024-07-17T05:54:46.756148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load Test Data\nDuring **commit** we load 128 rows of train to compute CV score. During **submit**, we load the test data.","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nVALIDATE = 128\n\ntest = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\") \nif len(test)==3:\n    test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n    test = test.iloc[:VALIDATE]\nprint( test.shape )\ntest.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T05:54:46.82197Z","iopub.execute_input":"2024-07-17T05:54:46.822513Z","iopub.status.idle":"2024-07-17T05:54:53.847478Z","shell.execute_reply.started":"2024-07-17T05:54:46.82246Z","shell.execute_reply":"2024-07-17T05:54:53.8454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Engineer Prompt\nIf we want to submit zero shot LLM, we need to experiment with different system prompts to improve CV score. If we finetune the model, then system is not as important because the model will learn from the targets what to do regardless of which system prompt we use.\n\nWe use a logits processor to force the model to output the 3 tokens we are interested in.","metadata":{}},{"cell_type":"code","source":"from typing import Any, Dict, List\nfrom transformers import LogitsProcessor\nimport torch\n\nchoices = [\"A\",\"B\",\"tie\"]\n\nKEEP = []\nfor x in choices:\n    c = tokenizer.encode(x,add_special_tokens=False)[0]\n    KEEP.append(c)\nprint(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n\nclass DigitLogitsProcessor(LogitsProcessor):\n    def __init__(self, tokenizer):\n        self.allowed_ids = KEEP\n        \n    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n        scores[self.allowed_ids] += 100\n        return scores","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:06:47.562396Z","iopub.execute_input":"2024-07-17T06:06:47.562754Z","iopub.status.idle":"2024-07-17T06:06:47.571032Z","shell.execute_reply.started":"2024-07-17T06:06:47.562718Z","shell.execute_reply":"2024-07-17T06:06:47.570126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sys_prompt = \"\"\"Please read the following prompt and two responses. Determine which response is better.\nIf the responses are relatively the same, respond with 'tie'. Otherwise respond with 'A' or 'B' to indicate which is better.\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-07-17T05:54:53.882323Z","iopub.execute_input":"2024-07-17T05:54:53.883237Z","iopub.status.idle":"2024-07-17T05:54:53.890156Z","shell.execute_reply.started":"2024-07-17T05:54:53.883211Z","shell.execute_reply":"2024-07-17T05:54:53.887959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SS = \"#\"*25 + \"\\n\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_prompts = []\nfor index,row in test.iterrows():\n    \n    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))\n    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))\n    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))\n    \n    prompt = f\"{SS}PROMPT: \"+a+f\"\\n\\n{SS}RESPONSE A: \"+b+f\"\\n\\n{SS}RESPONSE B: \"+c+\"\\n\\n\"\n    \n    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n    \n    all_prompts.append( formatted_sample )","metadata":{"execution":{"iopub.status.busy":"2024-07-17T05:54:53.917724Z","iopub.execute_input":"2024-07-17T05:54:53.918708Z","iopub.status.idle":"2024-07-17T05:54:54.168859Z","shell.execute_reply.started":"2024-07-17T05:54:53.918626Z","shell.execute_reply":"2024-07-17T05:54:54.166722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Infer Test\nWe infer test using fast vLLM. We ask vLLM to output probabilties of the top 5 tokens considered to be predicted in the first token. We also limit prediction to 1 token to increase inference speed.\n\nBased on the speed it takes to infer 128 train samples, we can deduce how long inferring 25,000 test samples will take.","metadata":{}},{"cell_type":"code","source":"%%time\n\nfrom time import time\nstart = time()\n\nlogits_processors = [DigitLogitsProcessor(tokenizer)]\nresponses = llm.generate(\n    all_prompts,\n    vllm.SamplingParams(\n        n=1,  # Number of output sequences to return for each prompt.\n        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n        temperature=0,  # randomness of the sampling\n        seed=777, # Seed for reprodicibility\n        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n        logits_processors=logits_processors,\n        logprobs = 5\n    ),\n    use_tqdm = True\n)\n\nend = time()\nelapsed = (end-start)/60. #minutes\nprint(f\"Inference of {VALIDATE} samples took {elapsed} minutes!\")","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-17T06:00:22.240185Z","iopub.execute_input":"2024-07-17T06:00:22.241023Z","iopub.status.idle":"2024-07-17T06:02:04.057293Z","shell.execute_reply.started":"2024-07-17T06:00:22.240983Z","shell.execute_reply":"2024-07-17T06:02:04.056389Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submit = 25_000 / 128 * elapsed / 60\nprint(f\"Submit will take {submit} hours\")","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:02:13.417593Z","iopub.execute_input":"2024-07-17T06:02:13.41828Z","iopub.status.idle":"2024-07-17T06:02:13.423213Z","shell.execute_reply.started":"2024-07-17T06:02:13.418247Z","shell.execute_reply":"2024-07-17T06:02:13.422183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Extract Inference Probabilites\nWe now extract the probabilties of \"A\", \"B\", \"tie\" from the vLLM predictions.","metadata":{}},{"cell_type":"code","source":"results = []\nerrors = 0\n\nfor i,response in enumerate(responses):\n    try:\n        x = response.outputs[0].logprobs[0]\n        logprobs = []\n        for k in KEEP:\n            if k in x:\n                logprobs.append( math.exp(x[k].logprob) )\n            else:\n                logprobs.append( 0 )\n                print(f\"bad logits {i}\")\n        logprobs = np.array( logprobs )\n        logprobs /= logprobs.sum()\n        results.append( logprobs )\n    except:\n        #print(f\"error {i}\")\n        results.append( np.array([1/3., 1/3., 1/3.]) )\n        errors += 1\n        \nprint(f\"There were {errors} inference errors out of {i+1} inferences\")\nresults = np.vstack(results)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:02:33.681318Z","iopub.execute_input":"2024-07-17T06:02:33.681684Z","iopub.status.idle":"2024-07-17T06:02:33.691347Z","shell.execute_reply.started":"2024-07-17T06:02:33.681654Z","shell.execute_reply":"2024-07-17T06:02:33.690341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Create Submission CSV","metadata":{}},{"cell_type":"code","source":"sub = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")\n\nif len(test)!=VALIDATE:\n    sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = results\n    \nsub.to_csv(\"submission.csv\",index=False)\nsub.head()","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:02:54.392381Z","iopub.execute_input":"2024-07-17T06:02:54.392772Z","iopub.status.idle":"2024-07-17T06:02:54.410711Z","shell.execute_reply.started":"2024-07-17T06:02:54.392737Z","shell.execute_reply":"2024-07-17T06:02:54.409747Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Compute CV Score","metadata":{}},{"cell_type":"code","source":"if len(test)==VALIDATE:\n    true = test[['winner_model_a','winner_model_b','winner_tie']].values\n    print(true.shape)","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:03:21.15151Z","iopub.execute_input":"2024-07-17T06:03:21.152367Z","iopub.status.idle":"2024-07-17T06:03:21.158548Z","shell.execute_reply.started":"2024-07-17T06:03:21.152333Z","shell.execute_reply":"2024-07-17T06:03:21.157417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if len(test)==VALIDATE:\n    from sklearn.metrics import log_loss\n    print(f\"CV loglosss is {log_loss(true,results)}\" )","metadata":{"execution":{"iopub.status.busy":"2024-07-17T06:03:23.264976Z","iopub.execute_input":"2024-07-17T06:03:23.265602Z","iopub.status.idle":"2024-07-17T06:03:23.273439Z","shell.execute_reply.started":"2024-07-17T06:03:23.265556Z","shell.execute_reply":"2024-07-17T06:03:23.272475Z"},"trusted":true},"execution_count":null,"outputs":[]}]}