{
    "comments": [
        {
            "author": "Cody_Null",
            "content": "Great work Chris, I was just curious if you knew anything about performance loss with VLLM? I know nothing about it and was just trying to get an idea of what I should expect if I use this in other notebooks\n\n",
            "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "I don't think there is any performance loss because of vLLM. The package vLLM just runs our models. It is other choices like vllm.SamplingParams, quantize or not, what type of quantization, max_model_length, tokenizer truncation that affects performance.\n\n",
                    "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "Cody_Null",
                            "content": "Great! I didn‚Äôt know if it was similar to ONNX or exactly what I needed haha\n\n",
                            "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "Chris DeotteTopic Author",
                            "content": "We don't need to do any conversion of our models (like ONNX). We can take both non-quantized and quantized models directly from Hugging Face and run inference on them. (In other words, it doesn't change our models at all).\n\n",
                            "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "SeshuRaju üßò‚Äç‚ôÇÔ∏è",
            "content": "[@cdeotte](https://www.kaggle.com/cdeotte) Thanks for sharing, \n\nis this model fine tuned with sft or dpo? \n\nhow much gpu needed to fine tune? \n\n",
            "date": "Posted 9 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "This model was finetuned with SFT. Depending on chosen LoRA rank parameter r, max_model_len, batch_size, and the amount of train data, we can finetune on any amount of GPUs.\n\nDuring finetuning, we quantize 34B into 4bit reducing it's size to 20GB. So the only requirement is that the combined VRAM of your GPUs exceeds 20GB by enough to train with. We can probably finetune on Kaggle's 2xT4 with total VRAM 32GB if we reduce the above mentioned parameters.\n\n",
                    "date": "Posted 9 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "SeshuRaju üßò‚Äç‚ôÇÔ∏è",
                            "content": "\nThanks [@cdeotte](https://www.kaggle.com/cdeotte), due to limited Kaggle gpu hours, i am trying to train on 16GB VRAM. I'm exploring is any other methods to train with batch=1, r=3 and max_model_len=1024\n\nchosen LoRA rank parameter r, max_model_len, batch_size, and the amount of train data, we can finetune on any amount of GPUs. - how we can fit 34B -> 4bit compressed -> 20GB ( further can be reduced in below 16GB ? ) [@cdeotte](https://www.kaggle.com/cdeotte) \n\n",
                            "date": "Posted 9 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        },
                        {
                            "author": "Chris DeotteTopic Author",
                            "content": "I'm not sure how to finetune 34B LLM on 16GB VRAM. I think the minimum is closer to 32GB VRAM. There is a nice youtube video [here](https://www.youtube.com/watch?v=XpoKB3usmKc) explaining some details about efficient finetuning. During finetuning, we need memory for\n\n- model weights\n\n- gradients\n\n- optimizer\n\nTo reduce memory the most, we load model in 4bit quantized (i.e. QLoRA to reduce model weights memory usage), we can use llm_int8_enable_fp32_cpu_offload to allow swapping model weights between GPU and CPU memory, we finetune with PEFT with small r parameter (i.e. QLoRA), we use gradient checkpointing (to reduce gradient memory usage), and we use paged_adam_8bit optimizer (with small batches and small max token length). This optimizer swaps saved variables from GPU to CPU as needed and uses 8bit to reduce the size of optimizer variables. \n\n",
                            "date": "Posted 9 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        },
                        {
                            "author": "SeshuRaju üßò‚Äç‚ôÇÔ∏è",
                            "content": "Thanks [@cdeotte](https://www.kaggle.com/cdeotte) - i use page_ada_8bit, QLoRA, PEFT. will try gradient checkpointing. I'm trying \n\n```\ndevice_map = {\n    \"encoder\": \"cuda:0\", \n    \"decoder\": \"cpu\",    \n    \"lm_head\": \"cpu\",    \n}\n load_in_8bit_fp32_cpu_offload=True, \n\n```\n\n",
                            "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "ano",
            "content": "Thanks for sharing this great model! If you can share, could you tell me what dataset did you use for training? Did you use the whole training dataset or part of it? Or any external dataset?\n\n",
            "date": "Posted 12 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "I finetuned this model with 80% of competition data. I used all data excluding every 5th row. So the correct validation would be to use the following data pd.read_csv(\"train.csv\").iloc[0::5]. Currently this notebook uses pd.read_csv(\"train.csv\").iloc[:128] but a more accurate quick validation would be \n\n```\nVALIDATE = 128\ntest = test.iloc[0:VALIDATE*5:5]\n\n```\n\nThen we use 128 samples from the first index%5==0.\n\n",
                    "date": "Posted 12 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "2",
                    "reply": []
                }
            ]
        },
        {
            "author": "JM",
            "content": "Are you actually seeing 5 hours on submit stage? Mine is going way over\n\n",
            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "4",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "No, I am not seeing 5 hours. Mine is taking between 8 and 9 hours. If I release another version of this notebook, I will fix the time estimate code in code cell #10 (which involves removing \"inference errors\"). And I will update the introduction paragraph to say \"under 9 hours\" and not \"in 5 hours\".\n\n",
                    "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "ano",
                            "content": "Any idea why the estimated inference time (5 hours) is so different from the actual one (8-9 hours)? I also experimented to infer about 1000 samples and got the similar estimated inference time for 25000 samples (5-6 hours). \n\n",
                            "date": "Posted 12 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Luan Ngo Dinh",
            "content": "Hello, I would like to ask how the results of quantizing GPTQ compare to AWQ, and whether it is feasible on Kaggle compared to AWQ?\n\n",
            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "To use GPTQ we set quantization=\"gptq\" in the vLLM parameters (and save our model as GPTQ beforehand). I have not tried it on Kaggle but I have used both AWQ and GPTQ successfully in the past on T4 GPUs. The accuracy of each was basically similar. And the inference times were similar too.\n\n",
                    "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "2",
                    "reply": [
                        {
                            "author": "Akhila datta dola",
                            "content": "Thank you for sharing !\n\nJust wanted to ask does vLLM support on the fly 4-bit quantization like in bitsandbytes. How do they compare in general with gptq and awq?\n\n",
                            "date": "Posted 8 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Luan Ngo Dinh",
            "content": "Thank you for your wonderful sharing!!! \n\nMay I ask why there were 'There were 33 inference errors out of 128 inferences' in infenrence process ? \n\n",
            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "For 33 out of 128 input text, the model did not predict any generation text. Therefore we have no tokens to extract probabilities from for those 33. Our try/except code block catches this and predicts [1/3, 1/3, 1/3] in these cases.\n\n",
                    "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "Luan Ngo Dinh",
                            "content": "Do you mean the model did not generate the tokens 'a', 'b', 'tie' even though it was fine-tuned?\n\n",
                            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        },
                        {
                            "author": "Chris DeotteTopic Author",
                            "content": "Yes. The problem is because we have truncated our model to max_model_len=1024 (for speed). Therefore when input text is larger than 1024 then vLLM truncate to 1024 and there is no more room to output any generated text. With smarter prompt engineering and/or truncation strategy we can avoid these inference errors.\n\n",
                            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        },
                        {
                            "author": "floriandev",
                            "content": "just working on the \"prompt engineering and/or truncation strategy\", good to know I might be on the right track ;-)\n\n",
                            "date": "Posted 13 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Xinian Guo",
            "content": "Hi, Is the model finetuning by competitins data ?\n\n",
            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "Yes. This model (notebook version 8) is finetuned with comp data. Notebook version 6 [here](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm?scriptVersionId=188642633) is zero shot without finetuning.\n\n",
                    "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "yechenzhi1",
                            "content": "Hi, may I ask what the LB score is for the fine-tuned version?\n\n",
                            "date": "Posted 14 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        },
                        {
                            "author": "floriandev",
                            "content": "I got 0.972 with that notebook. Good luck ;-)\n\n",
                            "date": "Posted 13 days ago  ¬∑  Posted on Version 8 of \n        8",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Qihang Wang",
            "content": "Hello  Chris, .  I would like to confirm your process: \n\nCMIIW\n\nqlora fine-tuning 4bit model ? \nmerge qlora to the model ?\nconvert 4bit to ?\nAWQ quantize\nUsing vLLM for inference\n\nI'm not very familiar with the first three steps and I'm not sure if this is how you're doing it.\n\nCould you explain them in a bit more detail?\n\n",
            "date": "Posted 4 days ago  ¬∑  Posted on Version 8 of \n        8",
            "votes": "0",
            "reply": []
        }
    ]
}