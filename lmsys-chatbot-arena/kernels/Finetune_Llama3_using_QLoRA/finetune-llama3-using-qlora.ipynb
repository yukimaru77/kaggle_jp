{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":7711309,"sourceType":"datasetVersion","datasetId":4484051},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\n\n\nIn this notebook we demostrate how to finetune **Llama3** (8B) model from Meta using **QLoRA** and **SFTTrainer** from **tlr**.\n\n## What is Llama3?\n\nLlama3 is the latest release of open-source LLMs from Meta, with features pretrained and instruction-fine-tuned language models with 8B and 70B parameters.\n\n## What is LoRA?\n\nLoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times.\n\n\n## What is QLoRA?\n\nQLoRA builds on LoRA by incorporating quantization techniques to further reduce memory usage while maintaining, or even enhancing, model performance. With QLoRA it is possible to finetune a 70B parameter model that requires 36 GPUs with only 2!\n\n\n## What is PEFT?\n\nParameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained model’s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task.\n\n## What is SFTTrainer?\n\nSFT in SFTTrainer stands for supervised fine-tuning. The **trl** (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.\n\n## What is UltraChat200k?  \n\nUltraChat-200k is an invaluable resource for natural language understanding, generation and dialog system research. With 1.4 million dialogues spanning a variety of topics, this parquet-formatted dataset offers researchers four distinct formats to aid in their studies: test_sft, train_sft, train_gen and test_gen. More details [here](https://www.kaggle.com/datasets/thedevastator/ultrachat-200k-nlp-dataset).\n\n## Inspiration\n\nFor this notebook, I took inspiration from several sources:\n* [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3)  \n* [Fine-tuning LLMs using LoRA](https://medium.com/@rajatsharma_33357/fine-tuning-llama-using-lora-fb3f48a557d5)  \n* [Fine-tuning Llama-3–8B-Instruct QLORA using low cost resources](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)  \n* [Llama2 Fine-Tuning with Low-Rank Adaptations (LoRA) on Gaudi 2 Processors](https://eduand-alvarez.medium.com/llama2-fine-tuning-with-low-rank-adaptations-lora-on-gaudi-2-processors-52cf1ee6ce11)  ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Install and import libraries","metadata":{}},{"cell_type":"code","source":"!pip install -q -U bitsandbytes\n!pip install -q -U transformers\n!pip install -q -U peft\n!pip install -q -U accelerate\n!pip install -q -U datasets\n!pip install -q -U trl","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-06T02:35:28.440086Z","iopub.execute_input":"2024-07-06T02:35:28.440439Z","iopub.status.idle":"2024-07-06T02:37:02.76497Z","shell.execute_reply.started":"2024-07-06T02:35:28.440406Z","shell.execute_reply":"2024-07-06T02:37:02.763847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:37:02.767069Z","iopub.execute_input":"2024-07-06T02:37:02.767377Z","iopub.status.idle":"2024-07-06T02:37:06.467674Z","shell.execute_reply.started":"2024-07-06T02:37:02.767349Z","shell.execute_reply":"2024-07-06T02:37:06.466776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# wandb_key = user_secrets.get_secret(\"wandb_api\")\n# import wandb\n# ! wandb login $wandb_key","metadata":{"execution":{"iopub.status.busy":"2024-07-06T01:48:13.893506Z","iopub.execute_input":"2024-07-06T01:48:13.893775Z","iopub.status.idle":"2024-07-06T01:48:13.897497Z","shell.execute_reply.started":"2024-07-06T01:48:13.893754Z","shell.execute_reply":"2024-07-06T01:48:13.896594Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport torch\nfrom time import time\nfrom datasets import load_dataset\nfrom peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\nfrom transformers import (\n    AutoConfig,\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    AutoTokenizer,\n    TrainingArguments,\n    AutoModelForSequenceClassification,\n    Trainer\n)\nfrom trl import SFTTrainer,setup_chat_format\nimport numpy as np","metadata":{"_kg_hide-output":true,"execution":{"iopub.status.busy":"2024-07-06T02:45:53.927153Z","iopub.execute_input":"2024-07-06T02:45:53.928208Z","iopub.status.idle":"2024-07-06T02:45:53.933768Z","shell.execute_reply.started":"2024-07-06T02:45:53.928173Z","shell.execute_reply":"2024-07-06T02:45:53.932674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CFG:\n    NUM_EPOCHS = 1\n    BATCH_SIZE = 16\n    DROPOUT = 0.05 \n    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n    \n    SEED = 2024 \n    MAX_LENGTH = 1024 \n    NUM_WARMUP_STEPS = 128\n    LR_MAX = 5e-5 \n    NUM_LABELS = 3 \n    LORA_RANK = 4\n    LORA_ALPHA = 8\n    LORA_MODULES = ['o_proj', 'v_proj']","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:37:25.013904Z","iopub.execute_input":"2024-07-06T02:37:25.014169Z","iopub.status.idle":"2024-07-06T02:37:25.019686Z","shell.execute_reply.started":"2024-07-06T02:37:25.014147Z","shell.execute_reply":"2024-07-06T02:37:25.018662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\ndef process(input_str):\n    stripped_str = input_str.strip('[]')\n    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n    return  ' '.join(sentences)\n\ntrain.loc[:, 'prompt'] = train['prompt'].apply(process)\ntrain.loc[:, 'response_a'] = train['response_a'].apply(process)\ntrain.loc[:, 'response_b'] = train['response_b'].apply(process)\n\n# Drop 'Null' for training\nindexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\ntrain.drop(indexes, inplace=True)\ntrain.reset_index(inplace=True, drop=True)\ntrain.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n\nprint(f\"Total {len(indexes)} Null response rows dropped\")\nprint('Total train samples: ', len(train))\n\ntrain['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\nprint(train['text'][4])","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:48:17.517134Z","iopub.execute_input":"2024-07-06T02:48:17.517798Z","iopub.status.idle":"2024-07-06T02:48:19.811371Z","shell.execute_reply.started":"2024-07-06T02:48:17.517764Z","shell.execute_reply":"2024-07-06T02:48:19.810436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialize model\n\n\nThe model used is:\n\n* **Model**: Llama3  \n* **Framework**: Transformers   \n* **Size**: 8B   \n* **Type**: 8b-chat-hf (hf stands for HuggingFace). \n* **Version**: V1  ","metadata":{}},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification\n\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit = True, \n    bnb_4bit_quant_type = 'nf4',\n    bnb_4bit_use_double_quant = True, \n    bnb_4bit_compute_dtype = torch.bfloat16 \n)\n\nmodel_name = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    quantization_config=quantization_config,\n    num_labels=4,\n    device_map='auto'\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:39:50.484999Z","iopub.execute_input":"2024-07-06T02:39:50.485368Z","iopub.status.idle":"2024-07-06T02:41:28.758504Z","shell.execute_reply.started":"2024-07-06T02:39:50.48534Z","shell.execute_reply":"2024-07-06T02:41:28.757702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n\nlora_config = LoraConfig(\n    r = 16, \n    lora_alpha = 8,\n    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n    lora_dropout = 0.05, \n    bias = 'none',\n    task_type = 'SEQ_CLS'\n)\n\nmodel = prepare_model_for_kbit_training(model)\nmodel = get_peft_model(model, lora_config)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:41:28.76007Z","iopub.execute_input":"2024-07-06T02:41:28.760355Z","iopub.status.idle":"2024-07-06T02:41:29.156629Z","shell.execute_reply.started":"2024-07-06T02:41:28.760331Z","shell.execute_reply":"2024-07-06T02:41:29.155721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n\ntokenizer.pad_token_id = tokenizer.eos_token_id\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:41:29.157753Z","iopub.execute_input":"2024-07-06T02:41:29.158084Z","iopub.status.idle":"2024-07-06T02:41:29.602672Z","shell.execute_reply.started":"2024-07-06T02:41:29.158057Z","shell.execute_reply":"2024-07-06T02:41:29.601879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.config.pad_token_id = tokenizer.pad_token_id\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:41:29.604292Z","iopub.execute_input":"2024-07-06T02:41:29.604578Z","iopub.status.idle":"2024-07-06T02:41:29.608971Z","shell.execute_reply.started":"2024-07-06T02:41:29.604553Z","shell.execute_reply":"2024-07-06T02:41:29.608015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from datasets import DatasetDict, Dataset\n\ndef data_preprocesing(row):\n    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')\n\ndataset = Dataset.from_pandas(train)\ntokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])\ntokenized_data.set_format(\"torch\")","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:48:50.283453Z","iopub.execute_input":"2024-07-06T02:48:50.284166Z","iopub.status.idle":"2024-07-06T02:49:58.185042Z","shell.execute_reply.started":"2024-07-06T02:48:50.284134Z","shell.execute_reply":"2024-07-06T02:49:58.184137Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\n\ndel train\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:49:58.186425Z","iopub.execute_input":"2024-07-06T02:49:58.186681Z","iopub.status.idle":"2024-07-06T02:49:58.589252Z","shell.execute_reply.started":"2024-07-06T02:49:58.186659Z","shell.execute_reply":"2024-07-06T02:49:58.58825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\ncollate_fn = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:43:04.460125Z","iopub.execute_input":"2024-07-06T02:43:04.460816Z","iopub.status.idle":"2024-07-06T02:43:04.466942Z","shell.execute_reply.started":"2024-07-06T02:43:04.460787Z","shell.execute_reply":"2024-07-06T02:43:04.46592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(evaluations):\n    predictions, labels = evaluations\n    predictions = np.argmax(predictions, axis=1)\n    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),\n    'accuracy':accuracy_score(predictions,labels)}","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:43:25.020424Z","iopub.execute_input":"2024-07-06T02:43:25.021317Z","iopub.status.idle":"2024-07-06T02:43:25.026141Z","shell.execute_reply.started":"2024-07-06T02:43:25.021287Z","shell.execute_reply":"2024-07-06T02:43:25.025203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass CustomTrainer(Trainer):\n    def __init__(self, *args, class_weights=None, **kwargs):\n        super().__init__(*args, **kwargs)\n        if class_weights is not None:\n            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n        else:\n            self.class_weights = None\n\n    def compute_loss(self, model, inputs, return_outputs=False):\n#         print(inputs)\n        labels = inputs.pop(\"labels\").long()\n\n        outputs = model(**inputs)\n\n        logits = outputs.get('logits')\n\n        if self.class_weights is not None:\n            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n        else:\n            loss = F.cross_entropy(logits, labels)\n\n        return (loss, outputs) if return_outputs else loss","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:47.346937Z","iopub.execute_input":"2024-07-06T02:56:47.34782Z","iopub.status.idle":"2024-07-06T02:56:47.356087Z","shell.execute_reply.started":"2024-07-06T02:56:47.347789Z","shell.execute_reply":"2024-07-06T02:56:47.354928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir = 'sentiment_classification',\n    learning_rate = 1e-4,\n    per_device_train_batch_size = 8,\n    per_device_eval_batch_size = 8,\n    num_train_epochs = 1,\n    logging_steps=1,\n    weight_decay = 0.01,\n    evaluation_strategy = 'epoch',\n    save_strategy = 'epoch',\n    load_best_model_at_end = True,\n    report_to=\"none\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:48.085674Z","iopub.execute_input":"2024-07-06T02:56:48.086406Z","iopub.status.idle":"2024-07-06T02:56:48.121806Z","shell.execute_reply.started":"2024-07-06T02:56:48.086375Z","shell.execute_reply":"2024-07-06T02:56:48.120877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = CustomTrainer(\n    model = model,\n    args = training_args,\n    train_dataset = tokenized_data,\n    tokenizer = tokenizer,\n    data_collator = collate_fn,\n    compute_metrics = compute_metrics,\n#     class_weights=class_weights,\n)\n\ntrain_result = trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-07-06T02:56:49.480104Z","iopub.execute_input":"2024-07-06T02:56:49.480493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}