{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:54:32.838395Z",
     "iopub.status.busy": "2024-07-01T05:54:32.837591Z",
     "iopub.status.idle": "2024-07-01T05:54:41.973058Z",
     "shell.execute_reply": "2024-07-01T05:54:41.972212Z",
     "shell.execute_reply.started": "2024-07-01T05:54:32.83836Z"
    }
   },
   "outputs": [],
   "source": [
    "# reduce 50% data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load the train.csv file\n",
    "train_data_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # Update with the correct path\n",
    "train_data = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'\n",
    "test_data = pd.read_csv(test_data_path)\n",
    "\n",
    "# Randomly sample 10% of the data\n",
    "sampled_train_data = train_data.sample(frac=0.5, random_state=42)\n",
    "\n",
    "# Save the sampled data if needed\n",
    "sampled_train_data_path = '/kaggle/working/sample_train.csv'\n",
    "sampled_train_data.to_csv(sampled_train_data_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:54:41.975072Z",
     "iopub.status.busy": "2024-07-01T05:54:41.974779Z",
     "iopub.status.idle": "2024-07-01T05:55:29.759404Z",
     "shell.execute_reply": "2024-07-01T05:55:29.758543Z",
     "shell.execute_reply.started": "2024-07-01T05:54:41.975047Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Combine prompts and responses for feature extraction\n",
    "sampled_train_data['text_a'] = sampled_train_data['prompt'] + \" \" + sampled_train_data['response_a']\n",
    "sampled_train_data['text_b'] = sampled_train_data['prompt'] + \" \" + sampled_train_data['response_b']\n",
    "test_data['text_a'] = test_data['prompt'] + \" \" + test_data['response_a']\n",
    "test_data['text_b'] = test_data['prompt'] + \" \" + test_data['response_b']\n",
    "\n",
    "# Initialize the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(pd.concat([sampled_train_data['text_a'], sampled_train_data['text_b'], test_data['text_a'], test_data['text_b']]))\n",
    "\n",
    "# Convert texts to sequences\n",
    "X_train_a = tokenizer.texts_to_sequences(sampled_train_data['text_a'])\n",
    "X_train_b = tokenizer.texts_to_sequences(sampled_train_data['text_b'])\n",
    "X_test_a = tokenizer.texts_to_sequences(test_data['text_a'])\n",
    "X_test_b = tokenizer.texts_to_sequences(test_data['text_b'])\n",
    "\n",
    "# Pad sequences to ensure equal length\n",
    "max_length = max(max(len(seq) for seq in X_train_a), max(len(seq) for seq in X_train_b))\n",
    "X_train_a = pad_sequences(X_train_a, maxlen=max_length, padding='post')\n",
    "X_train_b = pad_sequences(X_train_b, maxlen=max_length, padding='post')\n",
    "X_test_a = pad_sequences(X_test_a, maxlen=max_length, padding='post')\n",
    "X_test_b = pad_sequences(X_test_b, maxlen=max_length, padding='post')\n",
    "\n",
    "# Extract targets\n",
    "y_train_a = sampled_train_data['winner_model_a']\n",
    "y_train_b = sampled_train_data['winner_model_b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T05:55:29.761158Z",
     "iopub.status.busy": "2024-07-01T05:55:29.760611Z",
     "iopub.status.idle": "2024-07-01T06:11:55.56115Z",
     "shell.execute_reply": "2024-07-01T06:11:55.560129Z",
     "shell.execute_reply.started": "2024-07-01T05:55:29.761128Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "def create_lstm_model(input_length):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "input_length = X_train_a.shape[1]\n",
    "\n",
    "# Train model for text_a\n",
    "model_a = create_lstm_model(input_length)\n",
    "model_a.fit(X_train_a, y_train_a, epochs=5, batch_size=64, validation_split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:11:55.563635Z",
     "iopub.status.busy": "2024-07-01T06:11:55.563305Z",
     "iopub.status.idle": "2024-07-01T06:28:17.045495Z",
     "shell.execute_reply": "2024-07-01T06:28:17.04446Z",
     "shell.execute_reply.started": "2024-07-01T06:11:55.563607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model for text_b\n",
    "model_b = create_lstm_model(input_length)\n",
    "model_b.fit(X_train_b, y_train_b, epochs=5, batch_size=64, validation_split=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:28:17.047207Z",
     "iopub.status.busy": "2024-07-01T06:28:17.046895Z",
     "iopub.status.idle": "2024-07-01T06:28:18.045769Z",
     "shell.execute_reply": "2024-07-01T06:28:18.04487Z",
     "shell.execute_reply.started": "2024-07-01T06:28:17.047179Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_pred_a = model_a.predict(X_test_a).flatten()\n",
    "test_pred_b = model_b.predict(X_test_b).flatten()\n",
    "\n",
    "# Calculate probabilities for tie (assuming uniform distribution for simplicity)\n",
    "test_pred_tie = np.full(test_pred_a.shape, 1/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T06:28:18.048115Z",
     "iopub.status.busy": "2024-07-01T06:28:18.047055Z",
     "iopub.status.idle": "2024-07-01T06:28:18.056549Z",
     "shell.execute_reply": "2024-07-01T06:28:18.055693Z",
     "shell.execute_reply.started": "2024-07-01T06:28:18.048074Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the submission file\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_data['id'],\n",
    "    'winner_model_a': test_pred_a,\n",
    "    'winner_model_b': test_pred_b,\n",
    "    'winner_tie': test_pred_tie\n",
    "})\n",
    "\n",
    "# Save the submission file\n",
    "submission_path = '/kaggle/working/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"Submission file saved to {submission_path}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
