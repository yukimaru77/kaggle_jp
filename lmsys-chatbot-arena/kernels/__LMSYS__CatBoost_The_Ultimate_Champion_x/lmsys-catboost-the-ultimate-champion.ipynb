{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52a7e6af",
   "metadata": {},
   "source": [
    "# üìö Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:33.526535Z",
     "iopub.status.busy": "2024-07-11T07:20:33.526078Z",
     "iopub.status.idle": "2024-07-11T07:20:38.692753Z",
     "shell.execute_reply": "2024-07-11T07:20:38.691507Z",
     "shell.execute_reply.started": "2024-07-11T07:20:33.526503Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ea7665",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è Configuration Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.695283Z",
     "iopub.status.busy": "2024-07-11T07:20:38.694735Z",
     "iopub.status.idle": "2024-07-11T07:20:38.701258Z",
     "shell.execute_reply": "2024-07-11T07:20:38.699693Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.695252Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c83b0fd",
   "metadata": {},
   "source": [
    "\n",
    "# üìä Loading and Processing Data\n",
    "\n",
    "we will load our training and test datasets, and apply some preprocessing. This includes:\n",
    "\n",
    "\n",
    "1. **Loading Data**: Read the CSV files into pandas DataFrames.\n",
    "2. **Subsampling**: If the test dataset has less than 10 rows, subsample the training dataset to 10,000 rows for quicker processing.\n",
    "3. **Processing Strings**: Clean and process the string columns (`prompt`, `response_a`, `response_b`) by removing unwanted characters.\n",
    "4. **Shape and Missing Values**: Print the shape of the datasets and count missing values to understand the data structure and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.703195Z",
     "iopub.status.busy": "2024-07-11T07:20:38.702888Z",
     "iopub.status.idle": "2024-07-11T07:20:42.781785Z",
     "shell.execute_reply": "2024-07-11T07:20:42.780465Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.703171Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.train_path)\n",
    "test = pd.read_csv(config.test_path)\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc82da87",
   "metadata": {},
   "source": [
    "# üõ†Ô∏è Preprocessing Class Definition\n",
    "\n",
    "This class, `Preprocessor`, contains several methods to process and feature-engineer the text data. Here‚Äôs a breakdown of its functionalities:\n",
    "\n",
    "#### Cosine Similarity\n",
    "- **Formula**: \n",
    "  $$\\text{cosine_similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **Description**: Cosine similarity measures the cosine of the angle between two vectors, providing a metric of how similar the texts are.\n",
    "\n",
    "#### Jaccard Similarity\n",
    "- **Formula**:\n",
    "\n",
    "  $$\\text{Jaccard_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- **Description**: Jaccard similarity measures the similarity between two sets by comparing their intersection and union.\n",
    "\n",
    "#### Count Quotes\n",
    "- **Description**: This method identifies and counts both single and double quoted texts in a string, giving an idea of how many quotations are present.\n",
    "\n",
    "#### Tokenize\n",
    "- **Description**: This method splits the text into individual words (tokens), which can be used for further analysis like generating n-grams or calculating overlaps.\n",
    "\n",
    "#### Generate N-grams\n",
    "- **Description**: N-grams are contiguous sequences of 'n' items from a given text. This method helps in analyzing the text at different levels of granularity (unigrams, bigrams, trigrams, etc.).\n",
    "\n",
    "#### Count N-gram Overlaps\n",
    "- **Description**: This method calculates how many n-grams are common between two texts, helping to measure their similarity.\n",
    "\n",
    "#### Run\n",
    "- **Description**: This method processes the entire dataset, generating new features based on the above calculations, which can be used for training machine learning models.\n",
    "\n",
    "\n",
    "$\\frac{n!}{k!(n-k)!} = \\binom{n}{k}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.785579Z",
     "iopub.status.busy": "2024-07-11T07:20:42.784505Z",
     "iopub.status.idle": "2024-07-11T07:20:42.810829Z",
     "shell.execute_reply": "2024-07-11T07:20:42.809399Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.785544Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "            vectors = vectorizer.toarray()\n",
    "            cos_sim = cosine_similarity(vectors)\n",
    "            return cos_sim[0][1]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())\n",
    "        set2 = set(text2.split())\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"\n",
    "        double_quote_pattern = r'\"(.*?)\"'\n",
    "        single_quotes = re.findall(single_quote_pattern, text)\n",
    "        double_quotes = re.findall(double_quote_pattern, text)\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)\n",
    "        return len(single_quotes) + len(double_quotes)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)\n",
    "            ngrams2 = self.generate_ngrams(text2, n)\n",
    "            counter1 = Counter(ngrams1)\n",
    "            counter2 = Counter(ngrams2)\n",
    "            overlap = counter1 & counter2\n",
    "            overlap_count = sum(overlap.values())\n",
    "            return overlap_count\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.812893Z",
     "iopub.status.busy": "2024-07-11T07:20:42.812433Z",
     "iopub.status.idle": "2024-07-11T07:27:19.005204Z",
     "shell.execute_reply": "2024-07-11T07:27:19.003828Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.812855Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "train = preprocessor.run(train)\n",
    "test = preprocessor.run(test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa08ac3",
   "metadata": {},
   "source": [
    "### Data Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.007225Z",
     "iopub.status.busy": "2024-07-11T07:27:19.006777Z",
     "iopub.status.idle": "2024-07-11T07:27:19.041533Z",
     "shell.execute_reply": "2024-07-11T07:27:19.040457Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.007186Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = np.nan\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx\n",
    "train[target] = train[target].astype(\"int32\")\n",
    "    \n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.04354Z",
     "iopub.status.busy": "2024-07-11T07:27:19.043159Z",
     "iopub.status.idle": "2024-07-11T07:27:19.071594Z",
     "shell.execute_reply": "2024-07-11T07:27:19.070564Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.043511Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)\n",
    "y = train[target]\n",
    "X_test = test.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.073423Z",
     "iopub.status.busy": "2024-07-11T07:27:19.072999Z",
     "iopub.status.idle": "2024-07-11T07:27:19.095853Z",
     "shell.execute_reply": "2024-07-11T07:27:19.094538Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.073387Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.097367Z",
     "iopub.status.busy": "2024-07-11T07:27:19.096993Z",
     "iopub.status.idle": "2024-07-11T07:27:19.122964Z",
     "shell.execute_reply": "2024-07-11T07:27:19.121688Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.09734Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature Selection\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b34717e",
   "metadata": {},
   "source": [
    "# üß© Model Training and Evaluation\n",
    "\n",
    "### Model Definitions\n",
    "\n",
    "Defines several machine learning models including Random Forest, Gradient Boosting, SVM, XGBoost, CatBoost, and a Voting Classifier.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Uses SelectKBest to select 25 best features based on ANOVA F-value between feature and target.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Uses Stratified K-Fold cross-validation for model evaluation.\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "Iterates through each model, trains it using the training data, evaluates using cross-validation, and calculates the mean CV Log Loss.\n",
    "\n",
    "### Feature Importances\n",
    "\n",
    "Calculates and stores feature importances for applicable models (Random Forest, Gradient Boosting, XGBoost, CatBoost).\n",
    "\n",
    "### Best Model Identification\n",
    "\n",
    "Identifies the best performing model based on the lowest CV Log Loss.\n",
    "\n",
    "### Results Display\n",
    "\n",
    "Displays the results in a DataFrame showing the CV Log Loss for each model and, if applicable, feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.154308Z",
     "iopub.status.busy": "2024-07-11T07:27:19.153925Z",
     "iopub.status.idle": "2024-07-11T07:53:41.875009Z",
     "shell.execute_reply": "2024-07-11T07:53:41.873898Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.154277Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Define models and their configurations\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # GPU acceleration if available\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # Use GPU if available\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select features using SelectKBest\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# Dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"Training model: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))\n",
    "    cv_scores = []\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train\n",
    "            )\n",
    "\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")\n",
    "            cv_scores.append(val_log_loss)\n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})\n",
    "        print(f\"Mean CV Log Loss: {mean_cv_log_loss:.5f}\")\n",
    "\n",
    "# Store feature importances if applicable\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best model\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]\n",
    "print(f\"\\nBest Model:\\n{best_model}\")\n",
    "\n",
    "# Display results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:53:41.877465Z",
     "iopub.status.busy": "2024-07-11T07:53:41.876688Z",
     "iopub.status.idle": "2024-07-11T07:53:41.890065Z",
     "shell.execute_reply": "2024-07-11T07:53:41.888965Z",
     "shell.execute_reply.started": "2024-07-11T07:53:41.877426Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:36:46.168878Z",
     "iopub.status.busy": "2024-07-11T08:36:46.168386Z",
     "iopub.status.idle": "2024-07-11T08:36:46.17975Z",
     "shell.execute_reply": "2024-07-11T08:36:46.178638Z",
     "shell.execute_reply.started": "2024-07-11T08:36:46.168847Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
