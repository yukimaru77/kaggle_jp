{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:51:04.175257Z",
     "iopub.status.busy": "2024-07-29T13:51:04.174328Z",
     "iopub.status.idle": "2024-07-29T13:51:54.577962Z",
     "shell.execute_reply": "2024-07-29T13:51:54.57686Z",
     "shell.execute_reply.started": "2024-07-29T13:51:04.175208Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llama3-1-dependencies/dependencies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:51:54.580225Z",
     "iopub.status.busy": "2024-07-29T13:51:54.579915Z",
     "iopub.status.idle": "2024-07-29T13:52:07.53373Z",
     "shell.execute_reply": "2024-07-29T13:52:07.532613Z",
     "shell.execute_reply.started": "2024-07-29T13:51:54.580196Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:07.535399Z",
     "iopub.status.busy": "2024-07-29T13:52:07.535087Z",
     "iopub.status.idle": "2024-07-29T13:52:07.924142Z",
     "shell.execute_reply": "2024-07-29T13:52:07.923271Z",
     "shell.execute_reply.started": "2024-07-29T13:52:07.53537Z"
    }
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:07.927368Z",
     "iopub.status.busy": "2024-07-29T13:52:07.926794Z",
     "iopub.status.idle": "2024-07-29T13:52:14.42691Z",
     "shell.execute_reply": "2024-07-29T13:52:14.425946Z",
     "shell.execute_reply.started": "2024-07-29T13:52:07.927334Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizerFast,\n",
    "    PreTrainedTokenizerBase,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.429323Z",
     "iopub.status.busy": "2024-07-29T13:52:14.428309Z",
     "iopub.status.idle": "2024-07-29T13:52:14.438543Z",
     "shell.execute_reply": "2024-07-29T13:52:14.437188Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.429286Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"\n",
    "    checkpoint: str = \"/kaggle/input/unsloth-meta-llama-3.1-8b-bnb-4bit/transformers/default/1/Meta-Llama-3.1-8B-bnb-4bit\"\n",
    "    max_length: int = 2048\n",
    "    n_splits: int = 5\n",
    "    fold_idx: int = 0\n",
    "    optim_type: str = \"adamw_8bit\"\n",
    "    per_device_train_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    per_device_eval_batch_size: int = 8\n",
    "    n_epochs: int = 1\n",
    "    freeze_layers: int = 16  # there're 32 layers in total, we don't add adapters to the first 16 layers\n",
    "    lr: float = 2e-4\n",
    "    warmup_steps: int = 20\n",
    "    lora_r: int = 4\n",
    "    lora_alpha: float = lora_r * 2\n",
    "    lora_dropout: float = 0.05\n",
    "    lora_bias: str = \"none\"\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.439956Z",
     "iopub.status.busy": "2024-07-29T13:52:14.439666Z",
     "iopub.status.idle": "2024-07-29T13:52:14.454669Z",
     "shell.execute_reply": "2024-07-29T13:52:14.453826Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.439931Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,\n",
    "    lora_alpha=config.lora_alpha,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],\n",
    "    lora_dropout=config.lora_dropout,\n",
    "    bias=config.lora_bias,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.456298Z",
     "iopub.status.busy": "2024-07-29T13:52:14.45587Z",
     "iopub.status.idle": "2024-07-29T13:52:14.947589Z",
     "shell.execute_reply": "2024-07-29T13:52:14.946815Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.456266Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(config.checkpoint)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.948901Z",
     "iopub.status.busy": "2024-07-29T13:52:14.948607Z",
     "iopub.status.idle": "2024-07-29T13:53:01.613151Z",
     "shell.execute_reply": "2024-07-29T13:53:01.612176Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.948865Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.614976Z",
     "iopub.status.busy": "2024-07-29T13:53:01.614625Z",
     "iopub.status.idle": "2024-07-29T13:53:01.741845Z",
     "shell.execute_reply": "2024-07-29T13:53:01.740868Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.614948Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.746928Z",
     "iopub.status.busy": "2024-07-29T13:53:01.746237Z",
     "iopub.status.idle": "2024-07-29T13:53:01.755231Z",
     "shell.execute_reply": "2024-07-29T13:53:01.754267Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.746869Z"
    }
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.757223Z",
     "iopub.status.busy": "2024-07-29T13:53:01.75664Z",
     "iopub.status.idle": "2024-07-29T13:53:01.766005Z",
     "shell.execute_reply": "2024-07-29T13:53:01.764991Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.757197Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.768169Z",
     "iopub.status.busy": "2024-07-29T13:53:01.767361Z",
     "iopub.status.idle": "2024-07-29T13:53:04.250674Z",
     "shell.execute_reply": "2024-07-29T13:53:04.249787Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.768136Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates(subset=['response_a', 'response_b'], keep=False)\n",
    "df[\"len\"] = df[\"prompt\"].apply(len) + df[\"response_a\"].apply(len) + df[\"response_b\"].apply(len)\n",
    "df = df.sort_values(by=['len'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:04.252112Z",
     "iopub.status.busy": "2024-07-29T13:53:04.251818Z",
     "iopub.status.idle": "2024-07-29T13:53:05.960179Z",
     "shell.execute_reply": "2024-07-29T13:53:05.959221Z",
     "shell.execute_reply.started": "2024-07-29T13:53:04.252087Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)\n",
    "ds = ds.select(torch.arange(1000)) #for demo purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:05.961664Z",
     "iopub.status.busy": "2024-07-29T13:53:05.961368Z",
     "iopub.status.idle": "2024-07-29T13:53:05.970848Z",
     "shell.execute_reply": "2024-07-29T13:53:05.969929Z",
     "shell.execute_reply.started": "2024-07-29T13:53:05.961637Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"Which is the better response for the prompt? response_a or response_b or tie? \\n'n give score for each lable \\n\\n <prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels=[]\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "            if a_win:\n",
    "                label = 0\n",
    "            elif b_win:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "        return {**tokenized, \"labels\": labels}\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:05.972086Z",
     "iopub.status.busy": "2024-07-29T13:53:05.971815Z",
     "iopub.status.idle": "2024-07-29T13:53:06.33407Z",
     "shell.execute_reply": "2024-07-29T13:53:06.333205Z",
     "shell.execute_reply.started": "2024-07-29T13:53:05.972064Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)\n",
    "ds = ds.map(encode, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.335337Z",
     "iopub.status.busy": "2024-07-29T13:53:06.335075Z",
     "iopub.status.idle": "2024-07-29T13:53:06.34184Z",
     "shell.execute_reply": "2024-07-29T13:53:06.340816Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.335315Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions\n",
    "    labels = eval_preds.label_ids\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "    # Check for NaNs in predictions and labels\n",
    "    if np.isnan(probs).any() or np.isnan(labels).any():\n",
    "        raise ValueError(\"NaN values found in predictions or labels\")\n",
    "\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "    return {\"acc\": acc, \"log_loss\": loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.343396Z",
     "iopub.status.busy": "2024-07-29T13:53:06.343073Z",
     "iopub.status.idle": "2024-07-29T13:53:06.356154Z",
     "shell.execute_reply": "2024-07-29T13:53:06.355302Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.34336Z"
    }
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "        (\n",
    "            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "            [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "        ) \n",
    "        for fold_idx in range(config.n_splits)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.357518Z",
     "iopub.status.busy": "2024-07-29T13:53:06.357275Z",
     "iopub.status.idle": "2024-07-29T13:53:06.646525Z",
     "shell.execute_reply": "2024-07-29T13:53:06.64568Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.357497Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,\n",
    "    report_to=\"none\",\n",
    "    num_train_epochs=config.n_epochs,\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    "    logging_steps=1000,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_steps=100,\n",
    "    optim=config.optim_type,\n",
    "    fp16=True,\n",
    "    learning_rate=config.lr,\n",
    "    warmup_steps=config.warmup_steps,\n",
    "    packing=True, \n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=config.max_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.647907Z",
     "iopub.status.busy": "2024-07-29T13:53:06.647613Z",
     "iopub.status.idle": "2024-07-29T13:53:06.984498Z",
     "shell.execute_reply": "2024-07-29T13:53:06.983565Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.64787Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=ds,\n",
    "        args=sft_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.986055Z",
     "iopub.status.busy": "2024-07-29T13:53:06.985713Z",
     "iopub.status.idle": "2024-07-29T14:33:18.62325Z",
     "shell.execute_reply": "2024-07-29T14:33:18.622123Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.986026Z"
    }
   },
   "outputs": [],
   "source": [
    "for fold_idx in range(config.n_splits):\n",
    "    \n",
    "    train_idx, eval_idx = folds[fold_idx]\n",
    "\n",
    "    train_data = ds.select(train_idx).sort(\"len\")\n",
    "    val_data = ds.select(eval_idx).sort(\"len\")\n",
    "    \n",
    "    #split training data into batches with the same range of length\n",
    "    batch_size = 200\n",
    "    num_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)\n",
    "    \n",
    "    for batch_idx in range(num_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len(train_data))\n",
    "        ds_temp = train_data.select(range(start_idx, end_idx))\n",
    "        \n",
    "        trainer.train_dataset = ds_temp\n",
    "        \n",
    "        print(f\"Training batch {batch_idx + 1}/{num_batches} on fold {fold_idx + 1}/{config.n_splits}...\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        trainer.save_model(f\"model_fold_{fold_idx}_batch{batch_idx}\")\n",
    "\n",
    "    \n",
    "    # Validate after training on all batches\n",
    "    trainer.eval_dataset = val_data\n",
    "    \n",
    "    print(f\"Validating on fold {fold_idx + 1}/{config.n_splits}...\")\n",
    "    eval_results = trainer.evaluate()\n",
    "\n",
    "    # Save metrics if needed\n",
    "    print(f\"Evaluation results for fold {fold_idx + 1}: {eval_results}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5457551,
     "sourceId": 9051512,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 95591,
     "modelInstanceId": 70540,
     "sourceId": 83988,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
