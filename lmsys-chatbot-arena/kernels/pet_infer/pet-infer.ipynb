{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:31:27.934536Z",
     "iopub.status.busy": "2024-07-17T06:31:27.934207Z",
     "iopub.status.idle": "2024-07-17T06:31:31.786168Z",
     "shell.execute_reply": "2024-07-17T06:31:31.785184Z",
     "shell.execute_reply.started": "2024-07-17T06:31:27.934512Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from  tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import  pandas as pd\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "\n",
    "def process(example):\n",
    "    sentences = [s.strip('\"').replace('[MASK]','') for s in example['prompt'].strip('[]').split('\",\"')]\n",
    "    sentences_a = [s.strip('\"').replace('[MASK]','') for s in example['response_a'].strip('[]').split('\",\"')]\n",
    "    sentences_b = [s.strip('\"').replace('[MASK]','') for s in example['response_b'].strip('[]').split('\",\"')]\n",
    "    texts_a = [p for pair in zip_longest(sentences, sentences_a, fillvalue='') for p in pair if p]\n",
    "    texts_b = [p for pair in zip_longest(sentences, sentences_b, fillvalue='') for p in pair if p]\n",
    "    return pd.Series([' '.join(sentences), ' '.join(sentences_a), ' '.join(sentences_b), '\\n'.join(texts_a), '\\n'.join(texts_b)], index=['prompt', 'response_a', 'response_b', 'text_a','text_b'])\n",
    "\n",
    "test[['prompt', 'response_a', 'response_b', 'text_a','text_b']] = test.apply(process, axis=1)\n",
    "test = test.dropna()\n",
    "\n",
    "\n",
    "# pet\n",
    "class Senmamtic_news(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "\n",
    "        prompt_new = []\n",
    "        tk0 = tqdm(data['prompt'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            prompt_new.append(text)\n",
    "        # response_a\n",
    "        print(f'== response_a ==')\n",
    "        response_a_new = []\n",
    "        tk0 = tqdm(data['response_a'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            response_a_new.append(text)\n",
    "\n",
    "        # response_b\n",
    "        print(f'== response_b ==')\n",
    "        response_b_new = []\n",
    "        tk0 = tqdm(data['response_b'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            response_b_new.append(text)\n",
    "        self.data['prompt'] = prompt_new\n",
    "        self.data['response_a'] = response_a_new\n",
    "        self.data['response_b'] = response_b_new\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # text, label = self.data[idx]\n",
    "#         semantic_label = self.data['semantic_labels'][idx]\n",
    "        prompt = self.data['prompt'][idx]\n",
    "        response_a = self.data['response_a'][idx]\n",
    "        response_b = self.data['response_b'][idx]\n",
    "        system_prompt = f\"\"\"{prompt}:\n",
    "                        Response A: {response_a}\n",
    "                        Response B: {response_b}\n",
    "                        Which is better? Choose 'A', 'B', or 'both'.\n",
    "\"\"\" +self.mask_token\n",
    "\n",
    "\n",
    "        inputs = self.tokenizer(system_prompt, truncation=True, max_length=1600)\n",
    "#         semantic_label = self.tokenizer(semantic_label, truncation=True, max_length=2,add_special_tokens=False)['input_ids']\n",
    "        input_ids  = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        mask_index = torch.where(input_ids == self.tokenizer.mask_token_id)[0]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "#             'semantic_label': torch.tensor(semantic_label, dtype=torch.long),\n",
    "            # 'semantic_label': torch.tensor([-100]*(len(inputs['input_ids'])-3)+semantic_label+[-100], dtype=torch.long),\n",
    "            'mask_index': mask_index\n",
    "        }\n",
    "def collate_fn_semantic(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "#     semantic_label = [item['semantic_label'] for item in batch]\n",
    "    mask_index = [item['mask_index'] for item in batch]\n",
    "\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True,)\n",
    "#     semantic_label = pad_sequence(semantic_label, batch_first=True,padding_value=-100)\n",
    "    mask_index = torch.stack(mask_index)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "#         'semantic_label': semantic_label,\n",
    "        'mask_index': mask_index\n",
    "    }\n",
    "def get_semantic_data_loader(batch_size=32, mode='train',shuffle=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/deberta-small/deberta')\n",
    "    if mode == 'train':\n",
    "        dataset = Senmamtic_news(train, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,collate_fn=collate_fn_semantic)\n",
    "        return dataloader\n",
    "    else:\n",
    "        dataset = Senmamtic_news(test, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,collate_fn=collate_fn_semantic,drop_last = False)\n",
    "        return dataloader\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForMaskedLM, AutoConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 定义数据加载器函数和索引字典\n",
    "# from get_dataloader import get_semantic_data_loader\n",
    "\n",
    "\n",
    "# 定义模型类\n",
    "class BertPET(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "    # {\"336\": 0,\n",
    "    #             \"736\": 1,\n",
    "    #             \"462\": 2 }\n",
    "    def forward(self, input_ids, attention_mask, mask_index, labels=None, return_logits=False):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        mask_index = mask_index.unsqueeze(-1).expand(-1, -1, output.size(-1))\n",
    "        output = torch.gather(output, 1, mask_index).squeeze(1)\n",
    "        if return_logits:\n",
    "            # 返回特定索引的 logits\n",
    "            logits = output[:,[336, 732, 462]]\n",
    "            return logits\n",
    "\n",
    "# 推理函数\n",
    "def inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            mask_index = batch['mask_index'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, mask_index=mask_index, return_logits=True)\n",
    "#             logits = logits.squeeze(1)\n",
    "\n",
    "            probs = softmax(logits)\n",
    "            probs_list = probs.cpu().numpy().tolist()\n",
    "            for i in probs_list:\n",
    "                all_probs.append(i)\n",
    "        \n",
    "\n",
    "    return all_probs\n",
    "\n",
    "# 设置设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载模型和权重\n",
    "model_path = '/kaggle/input/deberta-small/deberta'  # 预训练模型路径\n",
    "checkpoint_path = '/kaggle/input/deberta-v3/model_checkpoint_epoch_2.pt'  # 最佳权重文件路径\n",
    "model = BertPET(model_path)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# 获取测试数据\n",
    "test_loader = get_semantic_data_loader(2, mode='test')\n",
    "\n",
    "# 进行推理\n",
    "test_probs = inference(model, test_loader, device)\n",
    "\n",
    "\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "test[['winner_model_a', 'winner_model_b', 'winner_tie']] = test_probs\n",
    "sample_sub = sample_sub[['id']].merge(test[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']], on='id', how='left')\n",
    "sample_sub.to_csv('submission.csv', index=False)\n",
    "display(sample_sub.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5401422,
     "sourceId": 8971973,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5401268,
     "sourceId": 8971994,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
