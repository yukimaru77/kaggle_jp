# Comments 

> ## ano
> 
> I submitted version 18 of this notebook twice. Both resulted in error submission due to timeout. We need to improve the code to speed up. Anyways, great work to start with!
> 
> 
> 
> > ## Pranshu BahadurTopic Author
> > 
> > Yeah…I'm hoping version 19 doesn't time out. Do you have any suggestions on how we could speed it up?
> > 
> > 
> > 
> > > ## ano
> > > 
> > > The first thing that came to my mind is parallel computing like this [this notebook](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora?scriptVersionId=187740026). You use two models, the one on cuda 0 and the other on cuda 1, and infer at the same time using threadpool. But I'm not 100% for sure. I can try to code, but honestly, I'm new to LLM, keras_nlp, etc…
> > > 
> > > 
> > > 
> > > ## Pranshu BahadurTopic Author
> > > 
> > > I'll look into the threadpool stuff might work somehow. Tbh I'm also very new to LLMs but trying to learn through implementation!
> > > 
> > > 
> > > 


---

> ## carvingfate
> 
> Nice Work! Gemma-2 needs a recent version of the transformer, which has caused me a lot of problems while training it on my older cloud GPU platforms. Thank you for sharing your code.
> 
> 
> 


---

> ## Lorry Zou
> 
> It’s Saturday now. Any luck with submitting?
> 
> 
> 


---

