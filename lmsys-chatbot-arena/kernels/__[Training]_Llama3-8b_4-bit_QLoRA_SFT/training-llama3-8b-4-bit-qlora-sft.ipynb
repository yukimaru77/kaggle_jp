{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5689abaf",
   "metadata": {},
   "source": [
    "## Result\n",
    "- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    \n",
    "\n",
    "- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)\n",
    "\n",
    "| subset | log loss |\n",
    "| - | - |\n",
    "| Eval | 0.9231|\n",
    "| LB | 0.936 |\n",
    "\n",
    "## Note\n",
    "If you want to reproduce the code, please note the following:\n",
    "- use all data\n",
    "- set per_device_train_batch_size=4\n",
    "- 1 epoch using A10 took ~15h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:00.436001Z",
     "iopub.status.busy": "2024-07-23T02:48:00.435641Z",
     "iopub.status.idle": "2024-07-23T02:48:30.272547Z",
     "shell.execute_reply": "2024-07-23T02:48:30.271445Z",
     "shell.execute_reply.started": "2024-07-23T02:48:00.435972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:30.274819Z",
     "iopub.status.busy": "2024-07-23T02:48:30.274517Z",
     "iopub.status.idle": "2024-07-23T02:48:48.854217Z",
     "shell.execute_reply": "2024-07-23T02:48:48.853427Z",
     "shell.execute_reply.started": "2024-07-23T02:48:30.274791Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb69c5",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:48.855763Z",
     "iopub.status.busy": "2024-07-23T02:48:48.855193Z",
     "iopub.status.idle": "2024-07-23T02:48:52.431712Z",
     "shell.execute_reply": "2024-07-23T02:48:52.430895Z",
     "shell.execute_reply.started": "2024-07-23T02:48:48.855736Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(100)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b83e6",
   "metadata": {},
   "source": [
    "### Tokenizer and prepare dataset, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:52.435094Z",
     "iopub.status.busy": "2024-07-23T02:48:52.434437Z",
     "iopub.status.idle": "2024-07-23T02:48:56.08418Z",
     "shell.execute_reply": "2024-07-23T02:48:56.083219Z",
     "shell.execute_reply.started": "2024-07-23T02:48:52.435058Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.086412Z",
     "iopub.status.busy": "2024-07-23T02:48:56.086086Z",
     "iopub.status.idle": "2024-07-23T02:48:56.724024Z",
     "shell.execute_reply": "2024-07-23T02:48:56.723106Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.086368Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}\n",
    "\n",
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc5b11",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.725702Z",
     "iopub.status.busy": "2024-07-23T02:48:56.725215Z",
     "iopub.status.idle": "2024-07-23T02:48:56.740638Z",
     "shell.execute_reply": "2024-07-23T02:48:56.739744Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.725661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.742612Z",
     "iopub.status.busy": "2024-07-23T02:48:56.741994Z",
     "iopub.status.idle": "2024-07-23T02:48:56.752799Z",
     "shell.execute_reply": "2024-07-23T02:48:56.752051Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.742586Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.754274Z",
     "iopub.status.busy": "2024-07-23T02:48:56.75395Z",
     "iopub.status.idle": "2024-07-23T02:49:25.370393Z",
     "shell.execute_reply": "2024-07-23T02:49:25.369414Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.754242Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/kaggle/working/model\"\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38267f7",
   "metadata": {},
   "source": [
    "#### Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:49:25.372766Z",
     "iopub.status.busy": "2024-07-23T02:49:25.372167Z",
     "iopub.status.idle": "2024-07-23T02:49:25.405271Z",
     "shell.execute_reply": "2024-07-23T02:49:25.404419Z",
     "shell.execute_reply.started": "2024-07-23T02:49:25.37273Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=\"none\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bd691",
   "metadata": {},
   "source": [
    "### Training !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:49:25.408265Z",
     "iopub.status.busy": "2024-07-23T02:49:25.407977Z",
     "iopub.status.idle": "2024-07-23T02:56:58.882175Z",
     "shell.execute_reply": "2024-07-23T02:56:58.88118Z",
     "shell.execute_reply.started": "2024-07-23T02:49:25.40824Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:58:21.537341Z",
     "iopub.status.busy": "2024-07-23T02:58:21.536598Z",
     "iopub.status.idle": "2024-07-23T02:58:22.106251Z",
     "shell.execute_reply": "2024-07-23T02:58:22.105397Z",
     "shell.execute_reply.started": "2024-07-23T02:58:21.537313Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('pretrained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:56:58.885003Z",
     "iopub.status.busy": "2024-07-23T02:56:58.883844Z",
     "iopub.status.idle": "2024-07-23T02:56:58.891142Z",
     "shell.execute_reply": "2024-07-23T02:56:58.890124Z",
     "shell.execute_reply.started": "2024-07-23T02:56:58.884966Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# # Đường dẫn tới mô hình đã được huấn luyện trước và file Lora adapter\n",
    "# model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "# lora_adapter_path = \"/kaggle/input/model-1\"\n",
    "\n",
    "# # Tải mô hình gốc\n",
    "# model_1 = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# # Tải tokenizer tương ứng\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# # Cấu hình Lora\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,            # Rank của Lora\n",
    "#     lora_alpha=16,  # Hệ số khuếch đại của Lora\n",
    "#     task_type=TaskType.CAUSAL_LM  # Loại nhiệm vụ của mô hình\n",
    "# )\n",
    "\n",
    "# # Chuẩn bị mô hình cho k-bit training nếu cần\n",
    "# model_1 = prepare_model_for_kbit_training(model_1)\n",
    "\n",
    "# # Áp dụng Lora Adapter vào mô hình\n",
    "# model_1 = get_peft_model(model_1, lora_config)\n",
    "\n",
    "# # Tải các tham số của Lora Adapter đã lưu trước đó\n",
    "# model_1.load_adapter(lora_adapter_path, adapter_name=\"test\")\n",
    "\n",
    "# # Mô hình hoàn chỉnh đã sẵn sàng sử dụng\n",
    "# model_1.eval()  # Đặt mô hình vào chế độ đánh giá nếu cần\n",
    "\n",
    "# # Tokenize một câu ví dụ và sử dụng mô hình để tạo văn bản\n",
    "# sentence = \"Hello, how are you?\"\n",
    "# inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "# outputs = model_1.generate(**inputs)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T01:36:23.566344Z",
     "iopub.status.busy": "2024-07-23T01:36:23.565754Z",
     "iopub.status.idle": "2024-07-23T01:36:23.570455Z",
     "shell.execute_reply": "2024-07-23T01:36:23.569455Z",
     "shell.execute_reply.started": "2024-07-23T01:36:23.566313Z"
    }
   },
   "outputs": [],
   "source": [
    "# !zip -r model_2.zip /kaggle/working/saved_model_2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5430972,
     "sourceId": 9013695,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
