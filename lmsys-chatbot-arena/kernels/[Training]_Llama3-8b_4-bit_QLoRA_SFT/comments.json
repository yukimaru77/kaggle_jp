{
    "comments": [
        {
            "author": "KeShuang Liu",
            "content": "Hello, I would like to know why max_length is set to 1024 for training and 2400 for inference. Have you tried using max_length=2400 for training?\n\n",
            "date": "Posted 7 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "2",
            "reply": []
        },
        {
            "author": "OHIRA",
            "content": "Thanks for great work!!!\n\nI have a question about the code.\n\nIf I set load_best_model_at_end = True  such as\n\nargs = TrainingArguments(\n    output_dir='/kaggle/output',\n    overwrite_output_dir = True,\n    evaluation_strategy = \"steps\",\n    save_strategy = \"steps\",\n    save_steps=20,\n    save_total_limit=5,\n    logging_strategy=\"steps\",\n    logging_steps=20,\n    warmup_steps=20,\n    optim=\"adamw_8bit\",\n    learning_rate=2e-4,\n    per_device_train_batch_size=2,\n    per_device_eval_batch_size=4,\n    gradient_accumulation_steps=2,\n    num_train_epochs=1,\n    fp16=True,\n    metric_for_best_model=\"log_loss\",\n    greater_is_better = False,\n    report_to=\"none\",\n    load_best_model_at_end = True\n)\n\nI can get five best model parameters in eval set?\n\nor I can get five last model parameters?\n\n",
            "date": "Posted a day ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": []
        },
        {
            "author": "daichisaito-cs",
            "content": "Thank you for sharing your outstanding work!\n\nI have one question:\n\nHow many epochs are needed to reproduce the scores of 0.9231 for Eval and 0.936 for LB?\n\nThe default number of training epochs is set to 1. Is this the same value used to achieve those scores?\n\nThanks\n\n",
            "date": "Posted 3 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": [
                {
                    "author": "ShelterWTopic Author",
                    "content": "yep.       \n\n",
                    "date": "Posted 3 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "Lorry Zou",
            "content": "Have you try to train Gemma2 9B using this method(next-word-prediction) as well? For Llama3, this method seems to have much better performance than directly using LlamaForSequenceClassification.\n\n",
            "date": "Posted 4 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": []
        },
        {
            "author": "Stringersolo",
            "content": "Hey [@shelterw](https://www.kaggle.com/shelterw) , thank you for sharing, I have a problems with reproducing the same result. More specifically, during training the metrics are basically the same, but when scoring on LB it's about 1.2, which is very strange and close to average/random prediction\n\nI've also tried to read the weights directly, but it didnt help:\n\nmodel_0.load_state_dict(torch.load(RAW_WEIGHTS), strict=False)\n\n",
            "date": "Posted 12 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": [
                {
                    "author": "ShelterWTopic Author",
                    "content": "This may be due to lm head weight reloading randomly while loading the model.\n\nUpdate the transformers and peft version, or choose to inherit the LlamaCausalModel class instead of LlamaPretrainedModel.\n\nThis also happened to me when I used Gemma2, which is weird.\n\nRefer to [here](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518408#2912471).\n\n",
                    "date": "Posted 11 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "Stringersolo",
                            "content": "thank you [@shelterw](https://www.kaggle.com/shelterw) , I am gonna try it, at this link they suggested to save a score layer:\n\ntorch.save(classifier.score.state_dict(), f'{output_directory_path}/score_state_dict.pth')\n\nin your case it would be:\n\ntorch.save(trainer.model.lm_head.state_dict(), f'output/lm_head_dict.pth')\n\nright?\n\n",
                            "date": "Posted 10 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Yi-Fu Chen",
            "content": "May I ask why you need to implement Llama3ForSFT instead of using AutoModelForCausalLM directly? Is there any special reason?\n\n",
            "date": "Posted 13 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": [
                {
                    "author": "ShelterWTopic Author",
                    "content": "For  label_token logits  and loss.\n\n",
                    "date": "Posted 13 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "0",
                    "reply": []
                }
            ]
        },
        {
            "author": "Eido Mike",
            "content": "outstanding works! thanks for sharing it\n\n",
            "date": "Posted 14 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": []
        },
        {
            "author": "AbaoJiang",
            "content": "Hi [@shelterw](https://www.kaggle.com/shelterw),\n\nThanks for sharing. I noticed that you used CAUSAL_LM task for training. Have you compared the performance with the one trained with the SEQ_CLS task based on LlamaForSequenceClassification?\n\n",
            "date": "Posted 14 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": [
                {
                    "author": "ShelterWTopic Author",
                    "content": "I did not compare llama3-8b with SEQ_CLS, my previous experiments based on llama3-8b were worse, but it was better than gemma2-9b with SEQ_CLS.\n\n",
                    "date": "Posted 14 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "2",
                    "reply": []
                }
            ]
        },
        {
            "author": "__ChrisQ__",
            "content": "Hi, thanks for the notebook.\n\nOne question: if you use all data, how do you calculate the eval score?\n\n",
            "date": "Posted 14 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "0",
            "reply": [
                {
                    "author": "ShelterWTopic Author",
                    "content": "The 'compute_metrics' function in the script will be calculated automatically after 1epoch\n\n",
                    "date": "Posted 14 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "1",
                    "reply": [
                        {
                            "author": "raconion",
                            "content": "The 'compute_metrics' functions uses 20% of the training data for cross validation. Did you use the eval data to train the model further after evaluation? Sorry, I am not understanding what you refer to by \"use all data\". Does it mean that you use all data for 5 fold CV or you train the model with all data?\n\n",
                            "date": "Posted 13 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "ShelterWTopic Author",
                            "content": "Just use 80% of the data for training and 20% for validation.\n\n",
                            "date": "Posted 12 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "0",
                            "reply": []
                        },
                        {
                            "author": "raconion",
                            "content": "Thank you for your clarification :)\n\n",
                            "date": "Posted 12 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}