{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6202593",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:04:41.143497Z",
     "iopub.status.busy": "2024-07-23T12:04:41.143135Z",
     "iopub.status.idle": "2024-07-23T12:04:41.157259Z",
     "shell.execute_reply": "2024-07-23T12:04:41.156264Z",
     "shell.execute_reply.started": "2024-07-23T12:04:41.143463Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    OFFLINE = True#False #False # requirement for submision competition, during development recommend online model support LLM Evaluation Tools\n",
    "    USE_LLAMA3 = False # for GPU version\n",
    "    USE_GEMMA2 = False # for GPU version only \n",
    "    TASK_GEN = False # for generative Text output task (seem not suitble for this competition)\n",
    "    TASK_CLASSIFICATION = True  # for text classiction (suitable for this competition)\n",
    "#     model1 = \"/kaggle/input/llama-3/transformers/8b-hf/1\"  # llama3 8B |\n",
    "    model2 = \"/kaggle/input/gemma/transformers/2b-it/3\" #  gemma 2B\n",
    "    model3 = \"/kaggle/input/gemma/transformers/7b-it/3\" # gemma 7B\n",
    "#     model4 = \"/kaggle/input/gemma-2/pytorch/gemma-2-9b-it/1\" # gemma 2 9B\n",
    "    trainFile = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "    testFile = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n",
    "    submitSample = \"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\"\n",
    "    FEW_SHOT_TEST= False#True\n",
    "    USE_RAG = False#False#False #True#True , in this project, prefer use fine tuning for p\n",
    "    USE_WANDB = False#True # for  LLM evalution and debug , track fine tuning performance\n",
    "    USE_TRULENS = False # for LLM evalution For RAG prefer \n",
    "    USE_DEEPEVAL = False # for LLM evalution   (require openAI API key)\n",
    "    USE_TRAIN =  True #True #False#True Much be use GPU for Training\n",
    "    USE_INFER =  False # for submision prediction only , no test model\n",
    "    loggingSteps= 10#100 #100, #20, #5,#10,\n",
    "    maxTrainData = 1500#3500#5000 #10000#5000 #10000\n",
    "    maxEvalData = 20#100 # 20 \n",
    "    maxToken=  650 #512#768#512#768 # 512 for test only\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b577535e",
   "metadata": {},
   "source": [
    "# Install Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:04:42.692642Z",
     "iopub.status.busy": "2024-07-23T12:04:42.691979Z",
     "iopub.status.idle": "2024-07-23T12:05:31.1364Z",
     "shell.execute_reply": "2024-07-23T12:05:31.135106Z",
     "shell.execute_reply.started": "2024-07-23T12:04:42.692607Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# installDir = #\"/kaggle/input/ai-math-llm-install-package/Universal-LLM-install-page/Universal-LLM-install-page\"\n",
    "installDir = \"/kaggle/input/universal-llm-install-package2/Universal-LLM-install-page\"\n",
    "# install Libary for offline\n",
    "if CFG.OFFLINE:\n",
    "    !pip install transformers --no-index --no-deps --find-links=file://{installDir}/tranforemers\n",
    "    !pip install -U datasets     --no-index --no-deps --find-links=file://{installDir}/datasets\n",
    "    !pip install -U accelerate   --no-index --no-deps --find-links=file://{installDir}/accelerate\n",
    "    !pip install build        --no-index  --no-deps --find-links=file://{installDir}/build-1.2.1-py3-none-any.whl\n",
    "    !pip install -U bitsandbytes --no-index --no-deps --find-links=file://{installDir}/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl\n",
    "    !pip install langchain --no-index --no-deps --find-links=file://{installDir}/langchain-0.2.6-py3-none-any.whl\n",
    "    !pip install langchain-core  --no-index --no-deps --find-links=file://{installDir}/langchain_core-0.2.10-py3-none-any.whl\n",
    "    !pip install langsmith   --no-index --no-deps --find-links=file://{installDir}/langsmith-0.1.82-py3-none-any.whl\n",
    "    !pip install langchain-community --no-index --no-deps --find-links=file://{installDir}/langchain_community-0.2.5-py3-none-any.whl\n",
    "    !pip install sentence-transformers --no-index --no-deps --find-links=file://{installDir}/sentence_transformers-3.0.1-py3-none-any.whl\n",
    "    !pip install chromadb --no-index --no-deps --find-links=file://{installDir}/chromadb-0.5.3-py3-none-any.whl\n",
    "    !pip install faiss-cpu --no-index --no-deps --find-links=file://{installDir}/faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "    !pip install -U huggingface_hub --no-index --no-deps --find-links=file://{installDir}/huggingface_hub\n",
    "    !pip install -qU langchain-text-splitters --no-index --no-deps --find-links=file://{installDir}/langchain_text_splitters-0.2.2-py3-none-any.whl\n",
    "    !pip install -U peft  --no-index --no-deps --find-links=file://{installDir}/peft-0.11.1-py3-none-any.whl\n",
    "    !pip install -U trl  --no-index --no-deps --find-links=file://{installDir}/trl-0.9.4-py3-none-any.whl \n",
    "    !pip install umap-learn  --no-index --no-deps --find-links=file://{installDir}/umap_learn\n",
    "    !pip install evaluate  --no-index --no-deps --find-links=file://{installDir}/evaluate-0.4.2-py3-none-any.whl \n",
    "    !pip install deepeval  --no-index --no-deps --find-links=file://{installDir}/deepeval-0.21.62-py3-none-any.whl\n",
    "    !pip install weave  --no-index --no-deps --find-links=file://{installDir}/weave-0.50.2-py3-none-any.whl\n",
    "    !pip install openai --no-index --no-deps --find-links=file://{installDir}/openai-1.35.7-py3-none-any.whl\n",
    "    !pip install langchain_openai --no-index --no-deps --find-links=file://{installDir}/langchain_openai-0.1.13-py3-none-any.whl\n",
    "    !pip install trulens --no-index --no-deps --find-links=file://{installDir}/trulens-0.13.4-py3-none-any.whl\n",
    "    !pip install trulens-eval --no-index --no-deps --find-links=file://{installDir}/trulens_eval-0.32.0-py3-none-any.whl\n",
    "    \n",
    "else: # install libary for online\n",
    "    !pip install git+https://github.com/huggingface/transformers.git  # install transformer from source\n",
    "    # !pip install --upgrade torch datasets accelerate peft bitsandbytes trl\n",
    "    # !pip install --upgrade accelerate peft bitsandbytes trl\n",
    "    !pip install --upgrade datasets accelerate bitsandbytes  # add datasets, accelerate , bitsndbytes\n",
    "    !pip install langchain  langchain-community sentence-transformers chromadb  faiss-cpu #pypdf\n",
    "    !pip install --upgrade huggingface_hub\n",
    "    !pip install -qU langchain-text-splitters\n",
    "    # for loRA fine tuning \n",
    "    !pip install --upgrade peft trl\n",
    "    # for advance RAG & LLM evalution \n",
    "    !pip install portalocker openai langchain_openai # for deepeval dependance library (require openai api key)\n",
    "    !pip install --upgrade umap-learn evaluate deepeval weave trulens trulens-eval\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e57cbd4",
   "metadata": {},
   "source": [
    "# Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:31.138782Z",
     "iopub.status.busy": "2024-07-23T12:05:31.138455Z",
     "iopub.status.idle": "2024-07-23T12:05:39.014641Z",
     "shell.execute_reply": "2024-07-23T12:05:39.013797Z",
     "shell.execute_reply.started": "2024-07-23T12:05:31.138754Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json, time\n",
    "import gc\n",
    "from IPython.display import display, Markdown\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments\n",
    "from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding\n",
    "from langchain_community.document_loaders import TextLoader # new version\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "from langchain_core.runnables import ConfigurableField\n",
    "from langchain_community.vectorstores import FAISS, Chroma\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Text chunk spliter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter , SentenceTransformersTokenTextSplitter# Text Splitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# for evaluate  LLM \n",
    "import evaluate\n",
    "import pytest\n",
    "import trulens\n",
    "from sklearn.metrics import (classification_report, ConfusionMatrixDisplay  , \n",
    "                             f1_score , accuracy_score, precision_score, recall_score)\n",
    "\n",
    "import warnings\n",
    "# warnings.filterwarnings(\"error\") # for handle warning as error\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:39.016541Z",
     "iopub.status.busy": "2024-07-23T12:05:39.015825Z",
     "iopub.status.idle": "2024-07-23T12:05:39.082072Z",
     "shell.execute_reply": "2024-07-23T12:05:39.081103Z",
     "shell.execute_reply.started": "2024-07-23T12:05:39.016501Z"
    }
   },
   "outputs": [],
   "source": [
    "device =  torch.device('cuda'  if  torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:39.085031Z",
     "iopub.status.busy": "2024-07-23T12:05:39.084401Z",
     "iopub.status.idle": "2024-07-23T12:05:39.091933Z",
     "shell.execute_reply": "2024-07-23T12:05:39.091189Z",
     "shell.execute_reply.started": "2024-07-23T12:05:39.085005Z"
    }
   },
   "outputs": [],
   "source": [
    "def clearMemory():\n",
    "    for _ in range(5):\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        time.sleep(0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:39.093359Z",
     "iopub.status.busy": "2024-07-23T12:05:39.092912Z",
     "iopub.status.idle": "2024-07-23T12:05:41.943121Z",
     "shell.execute_reply": "2024-07-23T12:05:41.942162Z",
     "shell.execute_reply.started": "2024-07-23T12:05:39.09333Z"
    }
   },
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:41.945139Z",
     "iopub.status.busy": "2024-07-23T12:05:41.944472Z",
     "iopub.status.idle": "2024-07-23T12:05:41.950374Z",
     "shell.execute_reply": "2024-07-23T12:05:41.949528Z",
     "shell.execute_reply.started": "2024-07-23T12:05:41.9451Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "if False:\n",
    "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1898f5c6",
   "metadata": {},
   "source": [
    "# W&B inital For Online Only (LLM evalution Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:41.951714Z",
     "iopub.status.busy": "2024-07-23T12:05:41.951448Z",
     "iopub.status.idle": "2024-07-23T12:05:41.966725Z",
     "shell.execute_reply": "2024-07-23T12:05:41.96577Z",
     "shell.execute_reply.started": "2024-07-23T12:05:41.951691Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_WANDB:\n",
    "    import wandb\n",
    "    from kaggle_secrets import UserSecretsClient\n",
    "    user_secrets = UserSecretsClient()\n",
    "    my_secret = user_secrets.get_secret(\"wandb_api_key\")\n",
    "    wandb.login(key=my_secret) # login\n",
    "    reportTo= \"wandb\" # use for fine tuning training logging \n",
    "else:\n",
    "    reportTo = \"none\"# None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbff9c8",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:41.968645Z",
     "iopub.status.busy": "2024-07-23T12:05:41.968287Z",
     "iopub.status.idle": "2024-07-23T12:05:43.793704Z",
     "shell.execute_reply": "2024-07-23T12:05:43.792603Z",
     "shell.execute_reply.started": "2024-07-23T12:05:41.968614Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF =  pd.read_csv(CFG.trainFile)\n",
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.795216Z",
     "iopub.status.busy": "2024-07-23T12:05:43.79492Z",
     "iopub.status.idle": "2024-07-23T12:05:43.813467Z",
     "shell.execute_reply": "2024-07-23T12:05:43.812637Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.79519Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF[trainDF[\"winner_tie\"] ==1] # "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.817096Z",
     "iopub.status.busy": "2024-07-23T12:05:43.816839Z",
     "iopub.status.idle": "2024-07-23T12:05:43.831905Z",
     "shell.execute_reply": "2024-07-23T12:05:43.83102Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.817073Z"
    }
   },
   "outputs": [],
   "source": [
    "nullTranDF=trainDF[trainDF.response_a == 'null']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.834035Z",
     "iopub.status.busy": "2024-07-23T12:05:43.833058Z",
     "iopub.status.idle": "2024-07-23T12:05:43.841519Z",
     "shell.execute_reply": "2024-07-23T12:05:43.84071Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.834008Z"
    }
   },
   "outputs": [],
   "source": [
    "len(nullTranDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.842804Z",
     "iopub.status.busy": "2024-07-23T12:05:43.842539Z",
     "iopub.status.idle": "2024-07-23T12:05:43.883675Z",
     "shell.execute_reply": "2024-07-23T12:05:43.882793Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.842781Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.885099Z",
     "iopub.status.busy": "2024-07-23T12:05:43.88477Z",
     "iopub.status.idle": "2024-07-23T12:05:43.896942Z",
     "shell.execute_reply": "2024-07-23T12:05:43.89608Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.885068Z"
    }
   },
   "outputs": [],
   "source": [
    "testDF =  pd.read_csv(CFG.testFile)\n",
    "testDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.899483Z",
     "iopub.status.busy": "2024-07-23T12:05:43.899157Z",
     "iopub.status.idle": "2024-07-23T12:05:43.913693Z",
     "shell.execute_reply": "2024-07-23T12:05:43.91268Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.899459Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDF = pd.read_csv(CFG.submitSample)\n",
    "submitDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1def75ce",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.915117Z",
     "iopub.status.busy": "2024-07-23T12:05:43.91486Z",
     "iopub.status.idle": "2024-07-23T12:05:43.920873Z",
     "shell.execute_reply": "2024-07-23T12:05:43.919917Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.915095Z"
    }
   },
   "outputs": [],
   "source": [
    "def printUniqueValue(df, showAll= True):\n",
    "    for col in df.columns:\n",
    "        if showAll ==True:\n",
    "            print(f\"\"\"{col} :  {df[col].unique()}\"\"\")\n",
    "        else:\n",
    "            if df[col].dtype == \"object\": # only show object type columns unique values\n",
    "                print(f\"{col} : {df[col].unique()}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbebca0f",
   "metadata": {},
   "source": [
    "## print  unqiue Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:43.923096Z",
     "iopub.status.busy": "2024-07-23T12:05:43.922163Z",
     "iopub.status.idle": "2024-07-23T12:05:44.348265Z",
     "shell.execute_reply": "2024-07-23T12:05:44.347327Z",
     "shell.execute_reply.started": "2024-07-23T12:05:43.923063Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "printUniqueValue(trainDF, showAll=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1f8e58",
   "metadata": {},
   "source": [
    "## Model distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:44.350092Z",
     "iopub.status.busy": "2024-07-23T12:05:44.349695Z",
     "iopub.status.idle": "2024-07-23T12:05:44.3569Z",
     "shell.execute_reply": "2024-07-23T12:05:44.355891Z",
     "shell.execute_reply.started": "2024-07-23T12:05:44.350058Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f30658",
   "metadata": {},
   "source": [
    "# Load LLM Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:44.358363Z",
     "iopub.status.busy": "2024-07-23T12:05:44.358077Z",
     "iopub.status.idle": "2024-07-23T12:05:45.15409Z",
     "shell.execute_reply": "2024-07-23T12:05:45.153279Z",
     "shell.execute_reply.started": "2024-07-23T12:05:44.358339Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN == True:\n",
    "    # for LoRA fine tuning\n",
    "    from trl import SFTTrainer\n",
    "    from peft import LoraConfig, PeftModel, get_peft_model , prepare_model_for_kbit_training #prepare_model_for_int8_training deprecated \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:45.155411Z",
     "iopub.status.busy": "2024-07-23T12:05:45.155144Z",
     "iopub.status.idle": "2024-07-23T12:05:45.161561Z",
     "shell.execute_reply": "2024-07-23T12:05:45.160697Z",
     "shell.execute_reply.started": "2024-07-23T12:05:45.155387Z"
    }
   },
   "outputs": [],
   "source": [
    "do_sample= True \n",
    "top_p=0.95 \n",
    "top_k= 2\n",
    "temperature=0.2#0.7 \n",
    "num_beams = 3\n",
    "max_length= 512\n",
    "\n",
    "# Quantized Config for GPU support only\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit = True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype= torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9ff8e7",
   "metadata": {},
   "source": [
    "# define Label for Text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:45.163227Z",
     "iopub.status.busy": "2024-07-23T12:05:45.162892Z",
     "iopub.status.idle": "2024-07-23T12:05:45.17222Z",
     "shell.execute_reply": "2024-07-23T12:05:45.171274Z",
     "shell.execute_reply.started": "2024-07-23T12:05:45.163196Z"
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"winner_model_a\", 1: \"winner_model_b\", 2: \"winner_tie\"}\n",
    "label2id = {\"winner_model_a\": 0, \"winner_model_b\": 1, \"winner_tie\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:05:45.174424Z",
     "iopub.status.busy": "2024-07-23T12:05:45.174132Z",
     "iopub.status.idle": "2024-07-23T12:06:01.369744Z",
     "shell.execute_reply": "2024-07-23T12:06:01.368898Z",
     "shell.execute_reply.started": "2024-07-23T12:05:45.1744Z"
    }
   },
   "outputs": [],
   "source": [
    "if device.type == \"cuda\": # use 7b/8b/9b model gain performance\n",
    "    if CFG.USE_LLAMA3:\n",
    "        modelSel = CFG.model1\n",
    "        llmModel = \"llama3_8b\"\n",
    "        \n",
    "    elif CFG.USE_GEMMA2:\n",
    "        modelSel = CFG.model4\n",
    "        llmModel = \"gemma2_9b\"\n",
    "    \n",
    "    else:\n",
    "        modelSel = CFG.model3\n",
    "        llmModel = \"gemma_7b\"\n",
    "    if CFG.TASK_GEN:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config)\n",
    "#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\")\n",
    "    elif CFG.TASK_CLASSIFICATION:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map=\"auto\", \n",
    "                                                                   num_labels =3 ,\n",
    "                                                                   id2label=id2label, \n",
    "                                                                   label2id=label2id,\n",
    "#                                                                    problem_type= \"multi_label_classification\", #problem unmatch issues\n",
    "                                                 quantization_config= bnb_config) #remove quantization\n",
    "#          model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  num_labels =3)\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\",  \n",
    "                                                 quantization_config= bnb_config)\n",
    "#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer\n",
    "    tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "\n",
    "else: # for cpu select smaller model\n",
    "    modelSel = CFG.model2\n",
    "    llmModel = 'gemma_2b'\n",
    "    if CFG.TASK_GEN:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\")\n",
    "        \n",
    "    elif CFG.TASK_CLASSIFICATION:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map=\"auto\", \n",
    "                                                                   num_labels =3,\n",
    "                                                                   id2label=id2label, \n",
    "                                                                   label2id=label2id,\n",
    "#                                                                    problem_type= \"multi_label_classification\",\n",
    "                                                                  )\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map=\"auto\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer\n",
    "    tokenizer.add_eos_token = True  # We'll add <eos> at the end\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.371313Z",
     "iopub.status.busy": "2024-07-23T12:06:01.370988Z",
     "iopub.status.idle": "2024-07-23T12:06:01.380275Z",
     "shell.execute_reply": "2024-07-23T12:06:01.379349Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.371273Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.381724Z",
     "iopub.status.busy": "2024-07-23T12:06:01.381442Z",
     "iopub.status.idle": "2024-07-23T12:06:01.396634Z",
     "shell.execute_reply": "2024-07-23T12:06:01.395742Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.381694Z"
    }
   },
   "outputs": [],
   "source": [
    "llmModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc4975",
   "metadata": {},
   "source": [
    "# Prompt Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.398507Z",
     "iopub.status.busy": "2024-07-23T12:06:01.39788Z",
     "iopub.status.idle": "2024-07-23T12:06:01.407767Z",
     "shell.execute_reply": "2024-07-23T12:06:01.406891Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.398466Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    templatePrompt1 = \"\"\"Question: {question}.\\nOnly require given final result in JSON format with key 'answer'\n",
    "            \"\"\"\n",
    "    templatePrompt2 = \"Answer the user Question.\\n###\\n{format_instructions}\\n###\\nQuestion: {query}\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4889db46",
   "metadata": {},
   "source": [
    "### Generate Response from LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.409558Z",
     "iopub.status.busy": "2024-07-23T12:06:01.408962Z",
     "iopub.status.idle": "2024-07-23T12:06:01.420142Z",
     "shell.execute_reply": "2024-07-23T12:06:01.419437Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.409524Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    def generateResponse(query, maxNewToken =256):\n",
    "        \"\"\"\n",
    "        Direct send message to LLM model, get resposne\n",
    "        \"\"\"\n",
    "    \n",
    "        inputIds = tokenizer(query, return_tensors=\"pt\").to(device)\n",
    "        response = model.generate(**inputIds,\n",
    "                              do_sample= True,\n",
    "                              top_p =0.95,\n",
    "                              top_k= 3,\n",
    "                              temperature= 0.5,\n",
    "#                               max_lenght = 1024, \n",
    "                              max_new_tokens= maxNewToken,\n",
    "                             )\n",
    "    \n",
    "        return tokenizer.decode(response[0][len(inputIds[\"input_ids\"]):], skip_special_tokens = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df00d06c",
   "metadata": {},
   "source": [
    "# Simple parser to extract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.421458Z",
     "iopub.status.busy": "2024-07-23T12:06:01.421185Z",
     "iopub.status.idle": "2024-07-23T12:06:01.440522Z",
     "shell.execute_reply": "2024-07-23T12:06:01.439659Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.421434Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from  json.decoder import JSONDecodeError\n",
    "if CFG.TASK_GEN:\n",
    "\n",
    "    def isInteger(text):\n",
    "        try:\n",
    "            if int(text) >= 0:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except ValueError:\n",
    "            return False\n",
    "\n",
    "    def llmJSONparser(txt, key=\"answer:\", integerOut= False):\n",
    "        \"\"\"\n",
    "        try to get answer from LLM response , expect in JSON format, \n",
    "        \"\"\"\n",
    "        try:\n",
    "            subText = txt.split(\"{\") # split several {} in list \n",
    "            for txtSeg in subText: # loop in list to find answer\n",
    "                end = txtSeg.find(\"}\") # find end position in text segment\n",
    "                sub = txtSeg[:end] #subsring with {} context\n",
    "                temp = sub.replace(\"*\", \"\") # remove * symbol\n",
    "                temp = temp.replace(\"\\\"\", \"\") # reomve \\\" symbol\n",
    "                temp = temp.lower() # convert to lower case\n",
    "                answerloc = temp.find(key) # find key word \"answer\" position\n",
    "                if answerloc != -1:\n",
    "                    print(f\"find answer location : {answerloc}\")\n",
    "                    newTxt = temp[answerloc:] # substring start answer\n",
    "#                   print(\"Temp: \", temp)\n",
    "                    subTxt = newTxt.split(\"\\n\")\n",
    "                    #       print(subTxt)\n",
    "                    rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "                    rel= rel.replace(',', '') # remove , symbol\n",
    "                    print(rel)\n",
    "                    if integerOut: # expect integer output \n",
    "                        if isInteger(rel):\n",
    "                            return rel\n",
    "                        else:\n",
    "                            continue # not find the value\n",
    "                    else:\n",
    "                        return rel\n",
    "                \n",
    "            return None # can't find answer\n",
    "        except :\n",
    "            print(f\"\"\"Error LLM JSON parser input txt {txt}\"\"\" )\n",
    "            return None\n",
    "        return None\n",
    "\n",
    "\n",
    "    def getLLMAnswerParser(txt, key=\"answer:\"):\n",
    "        \"\"\"\n",
    "        when json parser failure, seem answer not JSON format, \n",
    "        use \"answer\" for key word search final answer \n",
    "        \"\"\"\n",
    "         # find answer  \n",
    "        temp = txt.replace(\"*\", \"\") # remove * symbol\n",
    "        temp = temp.replace(\"\\\"\", \"\") # reomve \"\" symbol\n",
    "        temp = temp.lower() # convert to lower case\n",
    "        # find answer key word\n",
    "        start = temp.find(key)\n",
    "        print(f\"Start loc: {start}\")\n",
    "        subStr = temp[start:]\n",
    "        if start != -1:\n",
    "            subTxt = subStr.split(\"\\n\")\n",
    "           #print(subTxt)\n",
    "            rel =subTxt[0][len(key):].strip() # get answer value with remove space\n",
    "            rel= rel.replace(',', '') # remove , symbol\n",
    "            print(rel)\n",
    "            return rel\n",
    "    \n",
    "        print(subStr)\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c7068c9",
   "metadata": {},
   "source": [
    "# Add parser to control extract data from LLM Structure Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.441755Z",
     "iopub.status.busy": "2024-07-23T12:06:01.441514Z",
     "iopub.status.idle": "2024-07-23T12:06:01.454511Z",
     "shell.execute_reply": "2024-07-23T12:06:01.45371Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.441729Z"
    }
   },
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import (StrOutputParser, \n",
    "                                           JsonOutputParser,\n",
    "                                           PydanticOutputParser,\n",
    "                                          )\n",
    "# for LLM structure output\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field, validator\n",
    "# from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.464185Z",
     "iopub.status.busy": "2024-07-23T12:06:01.463625Z",
     "iopub.status.idle": "2024-07-23T12:06:01.468725Z",
     "shell.execute_reply": "2024-07-23T12:06:01.467776Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.464159Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "# define data strauctrue for LLM output structure \n",
    "    class Answer(BaseModel):\n",
    "        answer: str = Field(description=\"the answer of question response from LLM\")\n",
    "        explanation: str = Field(description=\"explain how answer come from and reasoning\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.470107Z",
     "iopub.status.busy": "2024-07-23T12:06:01.46985Z",
     "iopub.status.idle": "2024-07-23T12:06:01.482331Z",
     "shell.execute_reply": "2024-07-23T12:06:01.481204Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.470085Z"
    }
   },
   "outputs": [],
   "source": [
    "# test Parser\n",
    "if CFG.TASK_GEN:\n",
    "    jsonParser = JsonOutputParser(pydantic_object=Answer)  # json parser\n",
    "    pydanticParser =  PydanticOutputParser(pydantic_object=Answer) # pydantic base parse\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.483653Z",
     "iopub.status.busy": "2024-07-23T12:06:01.483381Z",
     "iopub.status.idle": "2024-07-23T12:06:01.494051Z",
     "shell.execute_reply": "2024-07-23T12:06:01.493058Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.48363Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    print(jsonParser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.49524Z",
     "iopub.status.busy": "2024-07-23T12:06:01.494991Z",
     "iopub.status.idle": "2024-07-23T12:06:01.504958Z",
     "shell.execute_reply": "2024-07-23T12:06:01.504122Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.495218Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    print(pydanticParser.get_format_instructions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.506129Z",
     "iopub.status.busy": "2024-07-23T12:06:01.505901Z",
     "iopub.status.idle": "2024-07-23T12:06:01.516157Z",
     "shell.execute_reply": "2024-07-23T12:06:01.515287Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.506108Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if CFG.TASK_GEN:\n",
    "    ret = generateResponse(\"What is Machine Learning?\", maxNewToken=256) # test Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.517569Z",
     "iopub.status.busy": "2024-07-23T12:06:01.517208Z",
     "iopub.status.idle": "2024-07-23T12:06:01.527204Z",
     "shell.execute_reply": "2024-07-23T12:06:01.526223Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.517545Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    print(ret) # seem LLM default output in Markdown format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.528739Z",
     "iopub.status.busy": "2024-07-23T12:06:01.52844Z",
     "iopub.status.idle": "2024-07-23T12:06:01.538943Z",
     "shell.execute_reply": "2024-07-23T12:06:01.538099Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.528713Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    display(Markdown(ret))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:01.540293Z",
     "iopub.status.busy": "2024-07-23T12:06:01.540001Z",
     "iopub.status.idle": "2024-07-23T12:06:04.491534Z",
     "shell.execute_reply": "2024-07-23T12:06:04.490263Z",
     "shell.execute_reply.started": "2024-07-23T12:06:01.540269Z"
    }
   },
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:04.493196Z",
     "iopub.status.busy": "2024-07-23T12:06:04.492883Z",
     "iopub.status.idle": "2024-07-23T12:06:04.499912Z",
     "shell.execute_reply": "2024-07-23T12:06:04.499029Z",
     "shell.execute_reply.started": "2024-07-23T12:06:04.493167Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "if CFG.TASK_GEN:\n",
    "    # test prompt template with structure format and test parser\n",
    "    query = \"What is Machine Learning?\"\n",
    "    newPrompt =PromptTemplate(input_variables=[\"question\"], template=templatePrompt1)\n",
    "    finalPrompt = newPrompt.format(\n",
    "        question= query\n",
    "    )\n",
    "    rel = generateResponse(finalPrompt,  maxNewToken=1024)\n",
    "    jsonTxt = llmJSONparser(rel, key=\"answer:\", integerOut= False)\n",
    "    print(f\"Question : {query}\\nResponse Answer: {jsonTxt}\") # convert output structure format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:04.501381Z",
     "iopub.status.busy": "2024-07-23T12:06:04.501071Z",
     "iopub.status.idle": "2024-07-23T12:06:04.51054Z",
     "shell.execute_reply": "2024-07-23T12:06:04.509622Z",
     "shell.execute_reply.started": "2024-07-23T12:06:04.501348Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:04.512781Z",
     "iopub.status.busy": "2024-07-23T12:06:04.511887Z",
     "iopub.status.idle": "2024-07-23T12:06:04.521223Z",
     "shell.execute_reply": "2024-07-23T12:06:04.520219Z",
     "shell.execute_reply.started": "2024-07-23T12:06:04.512744Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.TASK_GEN:\n",
    "    templatePrompt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:04.522791Z",
     "iopub.status.busy": "2024-07-23T12:06:04.52245Z",
     "iopub.status.idle": "2024-07-23T12:06:04.533556Z",
     "shell.execute_reply": "2024-07-23T12:06:04.532605Z",
     "shell.execute_reply.started": "2024-07-23T12:06:04.522766Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "if CFG.TASK_GEN:\n",
    "    # test Structure output control by PydanticOutputParser\n",
    "    query = \"What is Machine Learning?\"\n",
    "    newPrompt = PromptTemplate(template=templatePrompt2,\n",
    "                          input_variables=[\"query\"],\n",
    "                          partial_variables={\"format_instructions\": pydanticParser.get_format_instructions()},  \n",
    "                          )\n",
    "    finalPrompt = newPrompt.format(\n",
    "            query=query,\n",
    "        )\n",
    "    print(f\"Final Prompt: {finalPrompt}\")\n",
    "    print(\"Response:\\n\")\n",
    "    rel = generateResponse(finalPrompt,  maxNewToken=2048)\n",
    "    print(rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:04.535026Z",
     "iopub.status.busy": "2024-07-23T12:06:04.534712Z",
     "iopub.status.idle": "2024-07-23T12:06:07.47383Z",
     "shell.execute_reply": "2024-07-23T12:06:07.47279Z",
     "shell.execute_reply.started": "2024-07-23T12:06:04.535001Z"
    }
   },
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.476087Z",
     "iopub.status.busy": "2024-07-23T12:06:07.475234Z",
     "iopub.status.idle": "2024-07-23T12:06:07.48007Z",
     "shell.execute_reply": "2024-07-23T12:06:07.479051Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.476051Z"
    }
   },
   "outputs": [],
   "source": [
    "# pydanticParser.parse(rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb595e7",
   "metadata": {},
   "source": [
    "# Prepare Dataset for Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.481582Z",
     "iopub.status.busy": "2024-07-23T12:06:07.48128Z",
     "iopub.status.idle": "2024-07-23T12:06:07.491848Z",
     "shell.execute_reply": "2024-07-23T12:06:07.49098Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.481558Z"
    }
   },
   "outputs": [],
   "source": [
    "tempTrainData= (trainDF[\"prompt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.495067Z",
     "iopub.status.busy": "2024-07-23T12:06:07.494804Z",
     "iopub.status.idle": "2024-07-23T12:06:07.505194Z",
     "shell.execute_reply": "2024-07-23T12:06:07.50427Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.495044Z"
    }
   },
   "outputs": [],
   "source": [
    "tempTrainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.506656Z",
     "iopub.status.busy": "2024-07-23T12:06:07.506362Z",
     "iopub.status.idle": "2024-07-23T12:06:07.515487Z",
     "shell.execute_reply": "2024-07-23T12:06:07.514652Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.506632Z"
    }
   },
   "outputs": [],
   "source": [
    "def dataPreprocess(inputStr):\n",
    "    # concatenate strings in list\n",
    "    stripStr = inputStr.strip(\"[]\") # remove list symbol\n",
    "#     print(stripStr)\n",
    "    sentence = [s.strip(\"\\\"\") for s in stripStr.split('\",\"')] # list spliter and remove start \n",
    "#     print(sentence)\n",
    "    finalStr = \" \".join(sentence) #concatenate snetence into single string \n",
    "    return finalStr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cd323",
   "metadata": {},
   "source": [
    "# Preprocessing Training / Testing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.516895Z",
     "iopub.status.busy": "2024-07-23T12:06:07.516568Z",
     "iopub.status.idle": "2024-07-23T12:06:07.536628Z",
     "shell.execute_reply": "2024-07-23T12:06:07.535723Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.51687Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.537922Z",
     "iopub.status.busy": "2024-07-23T12:06:07.537662Z",
     "iopub.status.idle": "2024-07-23T12:06:07.548212Z",
     "shell.execute_reply": "2024-07-23T12:06:07.547217Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.537899Z"
    }
   },
   "outputs": [],
   "source": [
    "tempTrainData[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.549579Z",
     "iopub.status.busy": "2024-07-23T12:06:07.549279Z",
     "iopub.status.idle": "2024-07-23T12:06:07.559063Z",
     "shell.execute_reply": "2024-07-23T12:06:07.55823Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.549555Z"
    }
   },
   "outputs": [],
   "source": [
    "dataPreprocess(tempTrainData[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.560667Z",
     "iopub.status.busy": "2024-07-23T12:06:07.560388Z",
     "iopub.status.idle": "2024-07-23T12:06:07.580666Z",
     "shell.execute_reply": "2024-07-23T12:06:07.57968Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.560643Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.582082Z",
     "iopub.status.busy": "2024-07-23T12:06:07.581722Z",
     "iopub.status.idle": "2024-07-23T12:06:07.596623Z",
     "shell.execute_reply": "2024-07-23T12:06:07.595749Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.582057Z"
    }
   },
   "outputs": [],
   "source": [
    "testDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:07.59799Z",
     "iopub.status.busy": "2024-07-23T12:06:07.597724Z",
     "iopub.status.idle": "2024-07-23T12:06:08.06293Z",
     "shell.execute_reply": "2024-07-23T12:06:08.062099Z",
     "shell.execute_reply.started": "2024-07-23T12:06:07.597967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning Training Dataset\n",
    "trainDF.loc[:, \"prompt\"] = trainDF[\"prompt\"].apply(dataPreprocess) # convert list of string into single sentence string\n",
    "trainDF.loc[:, \"response_a\"] = trainDF[\"response_a\"].apply(dataPreprocess) # convert list of string into single sentence string\n",
    "trainDF.loc[:, \"response_b\"] = trainDF[\"response_b\"].apply(dataPreprocess) # convert list of string into single sentence string \n",
    "\n",
    "# Clearning Testing Dataset \n",
    "testDF.loc[:, \"prompt\"] = testDF[\"prompt\"].apply(dataPreprocess) # convert list of string into single sentence string\n",
    "testDF.loc[:, \"response_a\"] =  testDF[\"response_a\"].apply(dataPreprocess) # convert list of string into single sentence string\n",
    "testDF.loc[:, \"response_b\"] = testDF[\"response_b\"].apply(dataPreprocess) # convert list of string into single sentence string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.064644Z",
     "iopub.status.busy": "2024-07-23T12:06:08.064238Z",
     "iopub.status.idle": "2024-07-23T12:06:08.080048Z",
     "shell.execute_reply": "2024-07-23T12:06:08.078873Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.064608Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.08169Z",
     "iopub.status.busy": "2024-07-23T12:06:08.081399Z",
     "iopub.status.idle": "2024-07-23T12:06:08.094817Z",
     "shell.execute_reply": "2024-07-23T12:06:08.093831Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.081665Z"
    }
   },
   "outputs": [],
   "source": [
    "testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eb641a",
   "metadata": {},
   "source": [
    "## Create Training datase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.09611Z",
     "iopub.status.busy": "2024-07-23T12:06:08.095879Z",
     "iopub.status.idle": "2024-07-23T12:06:08.105633Z",
     "shell.execute_reply": "2024-07-23T12:06:08.10471Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.09609Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    def getTokenLength(texts):\n",
    "        ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "        # return length of inputs_ids for each text\n",
    "        return [len(t) for t in ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.107181Z",
     "iopub.status.busy": "2024-07-23T12:06:08.106879Z",
     "iopub.status.idle": "2024-07-23T12:06:08.123734Z",
     "shell.execute_reply": "2024-07-23T12:06:08.122843Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.107155Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert targat from one hot encoding into caterogy (numberic)\n",
    "targetCol = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "trainDF[\"label\"] = np.argmax(trainDF[targetCol].values, axis =1 )  # 0: for winner model a, 1 for winner model b , 2 for winner tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.125494Z",
     "iopub.status.busy": "2024-07-23T12:06:08.125156Z",
     "iopub.status.idle": "2024-07-23T12:06:08.136006Z",
     "shell.execute_reply": "2024-07-23T12:06:08.135174Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.125451Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.137385Z",
     "iopub.status.busy": "2024-07-23T12:06:08.137087Z",
     "iopub.status.idle": "2024-07-23T12:06:08.149487Z",
     "shell.execute_reply": "2024-07-23T12:06:08.148726Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.13736Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.15082Z",
     "iopub.status.busy": "2024-07-23T12:06:08.150531Z",
     "iopub.status.idle": "2024-07-23T12:06:08.39896Z",
     "shell.execute_reply": "2024-07-23T12:06:08.398034Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.150784Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF[\"label\"].value_counts().plot(kind=\"bar\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.400363Z",
     "iopub.status.busy": "2024-07-23T12:06:08.400065Z",
     "iopub.status.idle": "2024-07-23T12:06:08.404788Z",
     "shell.execute_reply": "2024-07-23T12:06:08.403544Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.40033Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainDF[\"labelStr\"]= trainDF[\"label\"].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.405993Z",
     "iopub.status.busy": "2024-07-23T12:06:08.405724Z",
     "iopub.status.idle": "2024-07-23T12:06:08.415326Z",
     "shell.execute_reply": "2024-07-23T12:06:08.414362Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.40597Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainDF[\"labelStr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.416781Z",
     "iopub.status.busy": "2024-07-23T12:06:08.416455Z",
     "iopub.status.idle": "2024-07-23T12:06:08.427392Z",
     "shell.execute_reply": "2024-07-23T12:06:08.426554Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.416745Z"
    }
   },
   "outputs": [],
   "source": [
    "# create training data set for training feature\n",
    "# trainDF[\"text\"]=  (\"User Prompt: \" + trainDF[\"prompt\"] + \n",
    "#                      \"\\n\\n---\\n\\nModel A: \" + trainDF[\"response_a\"] +\n",
    "#                      \"\\n\\n---\\n\\nModel B: \" + trainDF[\"response_b\"] \n",
    "#                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.428981Z",
     "iopub.status.busy": "2024-07-23T12:06:08.428545Z",
     "iopub.status.idle": "2024-07-23T12:06:08.633149Z",
     "shell.execute_reply": "2024-07-23T12:06:08.632176Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.428948Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF[\"text\"]=  (\"<prompt>: \" + trainDF[\"prompt\"] + \n",
    "                     \"\\n\\n<response_a>: \" + trainDF[\"response_a\"] +\n",
    "                     \"\\n\\n<response_b>: \" + trainDF[\"response_b\"] \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.634511Z",
     "iopub.status.busy": "2024-07-23T12:06:08.634219Z",
     "iopub.status.idle": "2024-07-23T12:06:08.645167Z",
     "shell.execute_reply": "2024-07-23T12:06:08.64409Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.634487Z"
    }
   },
   "outputs": [],
   "source": [
    "testDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28570c37",
   "metadata": {},
   "source": [
    "## Prepare Test Data set for submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.646669Z",
     "iopub.status.busy": "2024-07-23T12:06:08.646365Z",
     "iopub.status.idle": "2024-07-23T12:06:08.65448Z",
     "shell.execute_reply": "2024-07-23T12:06:08.653355Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.646643Z"
    }
   },
   "outputs": [],
   "source": [
    "# testDF[\"text\"] = (\"User Prompt: \" + testDF[\"prompt\"] + \n",
    "#                      \"\\n\\n---\\n\\nModel A: \" + testDF[\"response_a\"] +\n",
    "#                      \"\\n\\n---\\n\\nModel B: \" + testDF[\"response_b\"]\n",
    "#                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.656202Z",
     "iopub.status.busy": "2024-07-23T12:06:08.655905Z",
     "iopub.status.idle": "2024-07-23T12:06:08.666245Z",
     "shell.execute_reply": "2024-07-23T12:06:08.665461Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.656178Z"
    }
   },
   "outputs": [],
   "source": [
    "testDF[\"text\"]=  (\"<prompt>: \" + testDF[\"prompt\"] + \n",
    "                     \"\\n\\n<response_a>: \" + testDF[\"response_a\"] +\n",
    "                     \"\\n\\n<response_b>: \" + testDF[\"response_b\"] \n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.667733Z",
     "iopub.status.busy": "2024-07-23T12:06:08.667474Z",
     "iopub.status.idle": "2024-07-23T12:06:08.678635Z",
     "shell.execute_reply": "2024-07-23T12:06:08.67777Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.66771Z"
    }
   },
   "outputs": [],
   "source": [
    "print(trainDF[\"text\"][1]) #print the train data sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.680511Z",
     "iopub.status.busy": "2024-07-23T12:06:08.679858Z",
     "iopub.status.idle": "2024-07-23T12:06:08.691008Z",
     "shell.execute_reply": "2024-07-23T12:06:08.690122Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.680478Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:  # only for statistic count the length of token\n",
    "    trainDF.loc[:, 'token_count'] = getTokenLength(trainDF['text']) #calucate each "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9779f1",
   "metadata": {},
   "source": [
    "## Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.692384Z",
     "iopub.status.busy": "2024-07-23T12:06:08.692097Z",
     "iopub.status.idle": "2024-07-23T12:06:08.712589Z",
     "shell.execute_reply": "2024-07-23T12:06:08.711539Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.692361Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.714684Z",
     "iopub.status.busy": "2024-07-23T12:06:08.714296Z",
     "iopub.status.idle": "2024-07-23T12:06:08.722604Z",
     "shell.execute_reply": "2024-07-23T12:06:08.721788Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.714651Z"
    }
   },
   "outputs": [],
   "source": [
    "if False: # only for statistic count the length of token\n",
    "    print(trainDF['token_count'].describe().to_frame().astype(int)) # check token length in statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.724154Z",
     "iopub.status.busy": "2024-07-23T12:06:08.723902Z",
     "iopub.status.idle": "2024-07-23T12:06:08.734355Z",
     "shell.execute_reply": "2024-07-23T12:06:08.733518Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.724133Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:  # only for statistic count the length of token\n",
    "    # get length of tokens which covers 80% of data, we'll still take 1024 length!\n",
    "    print(np.percentile(trainDF[\"token_count\"],  q=65))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.736065Z",
     "iopub.status.busy": "2024-07-23T12:06:08.735605Z",
     "iopub.status.idle": "2024-07-23T12:06:08.748356Z",
     "shell.execute_reply": "2024-07-23T12:06:08.747549Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.736034Z"
    }
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    trainDF.drop(\"token_count\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.749956Z",
     "iopub.status.busy": "2024-07-23T12:06:08.749375Z",
     "iopub.status.idle": "2024-07-23T12:06:08.772515Z",
     "shell.execute_reply": "2024-07-23T12:06:08.771641Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.749932Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.773698Z",
     "iopub.status.busy": "2024-07-23T12:06:08.773455Z",
     "iopub.status.idle": "2024-07-23T12:06:08.785262Z",
     "shell.execute_reply": "2024-07-23T12:06:08.784222Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.773676Z"
    }
   },
   "outputs": [],
   "source": [
    "print(testDF[\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.788931Z",
     "iopub.status.busy": "2024-07-23T12:06:08.788069Z",
     "iopub.status.idle": "2024-07-23T12:06:08.797778Z",
     "shell.execute_reply": "2024-07-23T12:06:08.796854Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.788904Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_WANDB and CFG.USE_TRAIN:\n",
    "     # Start a new wandb run\n",
    "    wandbFineTuningProject = \"lmsys-chatbot-araena-fine-tuning\"\n",
    "    runTask1 = wandb.init(project=wandbFineTuningProject, job_type=\"generation\", anonymous=\"allow\")\n",
    "    # define W&B Table\n",
    "    wandbCol1 =  [\"model\", \"user query\", \"modela_ans\", \"modelb_ans\", \"label\"] #define column for record\n",
    "    wandbFineTuneTable =wandb.Table(columns=wandbCol1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.798873Z",
     "iopub.status.busy": "2024-07-23T12:06:08.798614Z",
     "iopub.status.idle": "2024-07-23T12:06:08.808273Z",
     "shell.execute_reply": "2024-07-23T12:06:08.807353Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.79885Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN == True: #requred GPU support\n",
    "    # for LoRA fine tuning\n",
    "    from trl import SFTTrainer\n",
    "    from peft import LoraConfig, PeftModel, get_peft_model #, prepare_model_for_kbit_training #prepare_model_for_int8_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9122ba",
   "metadata": {},
   "source": [
    "# Setup LoRA For Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.809686Z",
     "iopub.status.busy": "2024-07-23T12:06:08.809336Z",
     "iopub.status.idle": "2024-07-23T12:06:08.820966Z",
     "shell.execute_reply": "2024-07-23T12:06:08.820119Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.809652Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    #LoRA config \n",
    "    lora_config = LoraConfig(\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        task_type=\"SEQ_CLS\",  # for Sequence Classification \n",
    "        target_modules = [\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "        lora_dropout= 0.05,\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.822227Z",
     "iopub.status.busy": "2024-07-23T12:06:08.821982Z",
     "iopub.status.idle": "2024-07-23T12:06:08.831939Z",
     "shell.execute_reply": "2024-07-23T12:06:08.830961Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.822206Z"
    }
   },
   "outputs": [],
   "source": [
    "# if False:#CFG.USE_TRAIN:\n",
    "#     model = prepare_model_for_kbit_training(model) # convert float32 to int8 # take care will out of memory for T4 GPU\n",
    "#     model= get_peft_model(model, lora_config) #get Model With LORD  # take care int8 train will out of memory for T4 GPU\n",
    "#     model.print_trainable_parameters()\n",
    "    \n",
    "if False:#True:\n",
    "    model.config.use_cache = False\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    model= get_peft_model(model, lora_config)\n",
    "    model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.833182Z",
     "iopub.status.busy": "2024-07-23T12:06:08.832927Z",
     "iopub.status.idle": "2024-07-23T12:06:08.846054Z",
     "shell.execute_reply": "2024-07-23T12:06:08.84509Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.83316Z"
    }
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.847468Z",
     "iopub.status.busy": "2024-07-23T12:06:08.847157Z",
     "iopub.status.idle": "2024-07-23T12:06:08.854747Z",
     "shell.execute_reply": "2024-07-23T12:06:08.853846Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.847434Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    # Create a preprocessing function to tokenize train data and truncate sequences \n",
    "    def tokenizeProcess(sample):\n",
    "        return tokenizer(sample[\"text\"],  max_length=CFG.maxToken, padding=True, truncation=True)\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb71cf1",
   "metadata": {},
   "source": [
    "# Create Training/Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.855952Z",
     "iopub.status.busy": "2024-07-23T12:06:08.855657Z",
     "iopub.status.idle": "2024-07-23T12:06:08.891642Z",
     "shell.execute_reply": "2024-07-23T12:06:08.890773Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.855929Z"
    }
   },
   "outputs": [],
   "source": [
    "trainDF.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.892938Z",
     "iopub.status.busy": "2024-07-23T12:06:08.892689Z",
     "iopub.status.idle": "2024-07-23T12:06:08.969012Z",
     "shell.execute_reply": "2024-07-23T12:06:08.968029Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.892916Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "#     maxTrainData = 300#5000#10000#5000 #10000\n",
    "#     maxEvalData = 20#100 \n",
    "    tempTrainDF = trainDF[:CFG.maxTrainData]\n",
    "    tempEvalDF =  trainDF[CFG.maxTrainData: CFG.maxTrainData+ CFG.maxEvalData]\n",
    "    # convert HuggingFace dataset\n",
    "    trainDataset = Dataset.from_pandas(tempTrainDF, split=\"train\")\n",
    "    evalDataset = Dataset.from_pandas(tempEvalDF, split=\"test\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.97073Z",
     "iopub.status.busy": "2024-07-23T12:06:08.970348Z",
     "iopub.status.idle": "2024-07-23T12:06:08.978542Z",
     "shell.execute_reply": "2024-07-23T12:06:08.97724Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.97069Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    print(tempTrainDF[\"label\"].value_counts())\n",
    "    print(tempEvalDF[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f9ebd",
   "metadata": {},
   "source": [
    "### Create Submit Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:08.991075Z",
     "iopub.status.busy": "2024-07-23T12:06:08.990776Z",
     "iopub.status.idle": "2024-07-23T12:06:09.005251Z",
     "shell.execute_reply": "2024-07-23T12:06:09.004475Z",
     "shell.execute_reply.started": "2024-07-23T12:06:08.991049Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataset = Dataset.from_pandas(testDF, split=\"test\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.006904Z",
     "iopub.status.busy": "2024-07-23T12:06:09.006516Z",
     "iopub.status.idle": "2024-07-23T12:06:09.013857Z",
     "shell.execute_reply": "2024-07-23T12:06:09.012997Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.006872Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.015492Z",
     "iopub.status.busy": "2024-07-23T12:06:09.015092Z",
     "iopub.status.idle": "2024-07-23T12:06:09.02377Z",
     "shell.execute_reply": "2024-07-23T12:06:09.022878Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.01545Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    del tempTrainDF\n",
    "    del tempEvalDF\n",
    "    print(len(trainDataset), len(evalDataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.025157Z",
     "iopub.status.busy": "2024-07-23T12:06:09.024862Z",
     "iopub.status.idle": "2024-07-23T12:06:09.034261Z",
     "shell.execute_reply": "2024-07-23T12:06:09.03333Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.025133Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    # convert dataset to  Datasetdict \n",
    "    datasetDict= DatasetDict({\n",
    "        \"train\": trainDataset,\n",
    "        'test': evalDataset\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.036044Z",
     "iopub.status.busy": "2024-07-23T12:06:09.035488Z",
     "iopub.status.idle": "2024-07-23T12:06:09.046398Z",
     "shell.execute_reply": "2024-07-23T12:06:09.045497Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.036012Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataDict = DatasetDict({\n",
    "    \"test\": submitDataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.047573Z",
     "iopub.status.busy": "2024-07-23T12:06:09.047284Z",
     "iopub.status.idle": "2024-07-23T12:06:09.059297Z",
     "shell.execute_reply": "2024-07-23T12:06:09.058364Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.047551Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.061095Z",
     "iopub.status.busy": "2024-07-23T12:06:09.060431Z",
     "iopub.status.idle": "2024-07-23T12:06:09.070986Z",
     "shell.execute_reply": "2024-07-23T12:06:09.069992Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.06107Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    print(datasetDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.072279Z",
     "iopub.status.busy": "2024-07-23T12:06:09.071983Z",
     "iopub.status.idle": "2024-07-23T12:06:09.085299Z",
     "shell.execute_reply": "2024-07-23T12:06:09.084364Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.072256Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    # drop unused column \n",
    "#     datasetDict =datasetDict.remove_columns(['id', 'model_a', 'model_b', 'response_a', 'response_b'])\n",
    "    datasetDict = datasetDict.remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',\n",
    "                                 'winner_model_a', 'winner_model_b', 'winner_tie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:09.086913Z",
     "iopub.status.busy": "2024-07-23T12:06:09.086559Z",
     "iopub.status.idle": "2024-07-23T12:06:11.812357Z",
     "shell.execute_reply": "2024-07-23T12:06:11.811378Z",
     "shell.execute_reply.started": "2024-07-23T12:06:09.086878Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert Train dataset into tokenize \n",
    "if CFG.USE_TRAIN:\n",
    "    datasetDict = datasetDict.map(tokenizeProcess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:11.814528Z",
     "iopub.status.busy": "2024-07-23T12:06:11.813857Z",
     "iopub.status.idle": "2024-07-23T12:06:11.818744Z",
     "shell.execute_reply": "2024-07-23T12:06:11.817814Z",
     "shell.execute_reply.started": "2024-07-23T12:06:11.814489Z"
    }
   },
   "outputs": [],
   "source": [
    "# datasetDict[\"train\"].remove_columns(['id', 'model_a', 'model_b','winner_model_a', 'winner_model_b', 'winner_tie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:11.820063Z",
     "iopub.status.busy": "2024-07-23T12:06:11.819797Z",
     "iopub.status.idle": "2024-07-23T12:06:11.834484Z",
     "shell.execute_reply": "2024-07-23T12:06:11.833651Z",
     "shell.execute_reply.started": "2024-07-23T12:06:11.82004Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb129d72",
   "metadata": {},
   "source": [
    "## Convert Test submisioin datasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:11.835811Z",
     "iopub.status.busy": "2024-07-23T12:06:11.835556Z",
     "iopub.status.idle": "2024-07-23T12:06:12.339209Z",
     "shell.execute_reply": "2024-07-23T12:06:12.33836Z",
     "shell.execute_reply.started": "2024-07-23T12:06:11.835789Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataDict.map(tokenizeProcess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.340743Z",
     "iopub.status.busy": "2024-07-23T12:06:12.340462Z",
     "iopub.status.idle": "2024-07-23T12:06:12.346798Z",
     "shell.execute_reply": "2024-07-23T12:06:12.345925Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.340712Z"
    }
   },
   "outputs": [],
   "source": [
    "submitDataDict[\"test\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.348097Z",
     "iopub.status.busy": "2024-07-23T12:06:12.34784Z",
     "iopub.status.idle": "2024-07-23T12:06:12.381986Z",
     "shell.execute_reply": "2024-07-23T12:06:12.380995Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.348074Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datasetDict[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.383232Z",
     "iopub.status.busy": "2024-07-23T12:06:12.382975Z",
     "iopub.status.idle": "2024-07-23T12:06:12.390398Z",
     "shell.execute_reply": "2024-07-23T12:06:12.389575Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.383209Z"
    }
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    datasetDict = datasetDict.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.391779Z",
     "iopub.status.busy": "2024-07-23T12:06:12.391499Z",
     "iopub.status.idle": "2024-07-23T12:06:12.406657Z",
     "shell.execute_reply": "2024-07-23T12:06:12.405763Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.391756Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.40795Z",
     "iopub.status.busy": "2024-07-23T12:06:12.407692Z",
     "iopub.status.idle": "2024-07-23T12:06:12.418407Z",
     "shell.execute_reply": "2024-07-23T12:06:12.417562Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.407927Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    accList = []\n",
    "    f1List = []\n",
    "    recallList = []\n",
    "    preciseList = []\n",
    "    def compute_metrics1(pred):\n",
    "        logits, labels= pred\n",
    "        predictions = np.argmax(logits, axis=1) # \n",
    "        return {\"accuracy\": (predictions == labels).mean()}\n",
    "    \n",
    "    def compute_metrics2(pred):\n",
    "        logits, labels= pred\n",
    "        predictions = np.argmax(logits, axis=-1) # \n",
    "        accuracy = (predictions == labels).mean()\n",
    "        f1score = f1_score(labels, predictions, average='weighted')\n",
    "        recallScore = recall_score(labels, predictions,  average='weighted')\n",
    "        precision = precision_score(labels, predictions,  average='weighted')\n",
    "        accList.append(accuracy)\n",
    "        f1List.append(f1score)\n",
    "        recallList.append(recallScore)\n",
    "        preciseList.append(precision)\n",
    "        return {\"accuracy\": accuracy , \"recall\":  recallScore,  \"precision\":precision, 'f1-score': f1score }\n",
    "    \n",
    "    def compute_metrics3(eval_preds) -> dict:\n",
    "        preds = eval_preds.predictions\n",
    "        labels = eval_preds.label_ids\n",
    "        probs = torch.from_numpy(preds).float().softmax(-1).numpy()\n",
    "        loss = log_loss(y_true=labels, y_pred=probs)\n",
    "        acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))\n",
    "        return {\"acc\": acc, \"log_loss\": loss}\n",
    "    \n",
    "    def formatFuc(sample):\n",
    "        text = f\"{sample['text']}\"\n",
    "        retrun [text]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:12.420214Z",
     "iopub.status.busy": "2024-07-23T12:06:12.419531Z",
     "iopub.status.idle": "2024-07-23T12:06:12.430786Z",
     "shell.execute_reply": "2024-07-23T12:06:12.429934Z",
     "shell.execute_reply.started": "2024-07-23T12:06:12.420189Z"
    }
   },
   "outputs": [],
   "source": [
    "datasetDict[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:21.577395Z",
     "iopub.status.busy": "2024-07-23T12:06:21.576391Z",
     "iopub.status.idle": "2024-07-23T12:06:23.416071Z",
     "shell.execute_reply": "2024-07-23T12:06:23.415013Z",
     "shell.execute_reply.started": "2024-07-23T12:06:21.577353Z"
    }
   },
   "outputs": [],
   "source": [
    "if CFG.USE_TRAIN:\n",
    "    from transformers import DataCollatorWithPadding\n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    trainArg = TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        per_device_eval_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        eval_accumulation_steps=1,\n",
    "        warmup_steps=2,\n",
    "#         max_steps=300, # , the total number of training steps to perform, overide num_train epochs\n",
    "        num_train_epochs=1, # set train epochs\n",
    "        learning_rate=2e-5,\n",
    "#         evaluation_strategy= 'steps', # older version \n",
    "        eval_strategy=\"steps\",\n",
    "        save_strategy=\"steps\",\n",
    "        fp16=True,\n",
    "        logging_steps=CFG.loggingSteps, # 10,#100, #20, #5,#10,\n",
    "#         save_steps= 100,\n",
    "        output_dir= '/kaggle/working/lora_model',#\"./results\",#\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        load_best_model_at_end=True,\n",
    "#         overwrite_output_dir=True,\n",
    "#         label_names=[\"label\"],\n",
    "        report_to= reportTo # avoid wnb access token request during training\n",
    "    )\n",
    "    \n",
    "    trainer = SFTTrainer(\n",
    "        model = model,\n",
    "        train_dataset=datasetDict[\"train\"],\n",
    "        eval_dataset= datasetDict[\"test\"],\n",
    "        args = trainArg,\n",
    "        max_seq_length=2048,\n",
    "        peft_config = lora_config,\n",
    "#         formatting_func = formatFuc, #formatFuc1, #dataset aleader input_ids  \n",
    "        data_collator=data_collator,\n",
    "        compute_metrics = compute_metrics2 #compute_metrics1        \n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:24.375567Z",
     "iopub.status.busy": "2024-07-23T12:06:24.37519Z",
     "iopub.status.idle": "2024-07-23T12:06:27.364618Z",
     "shell.execute_reply": "2024-07-23T12:06:27.363711Z",
     "shell.execute_reply.started": "2024-07-23T12:06:24.375537Z"
    }
   },
   "outputs": [],
   "source": [
    "clearMemory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:47.142571Z",
     "iopub.status.busy": "2024-07-23T12:06:47.141875Z",
     "iopub.status.idle": "2024-07-23T12:06:47.14858Z",
     "shell.execute_reply": "2024-07-23T12:06:47.14771Z",
     "shell.execute_reply.started": "2024-07-23T12:06:47.142535Z"
    }
   },
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:06:47.521006Z",
     "iopub.status.busy": "2024-07-23T12:06:47.52056Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if CFG.USE_TRAIN:\n",
    "    print(\"Training the Model\")\n",
    "    trainer.train()\n",
    "#     print(\"Saving the model!\")\n",
    "    # only saves the incremental 🤗 PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\n",
    "#     trainer.model.save_pretrained('fine-tuned-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # only saves the incremental 🤗 PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.\n",
    "if CFG.USE_TRAIN:\n",
    "    print(\"Saving the model!\")\n",
    "    trainer.model.save_pretrained('lora_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearMemory()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c77864",
   "metadata": {},
   "source": [
    "# Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getClassifierOutput(text):\n",
    "        \"\"\"\n",
    "        Direct sendtext LLM model, get classification output ,\n",
    "        \n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            inputIds = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "            logits = model(**inputIds).logits\n",
    "            probabilities = nn.functional.softmax(logits, dim=-1)\n",
    "            classID = logits.argmax().item()\n",
    "            return probabilities , classID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submitDataDict[\"test\"][\"text\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accList\n",
    "# f1List\n",
    "# recallList\n",
    "# preciseList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(1, len(accList)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, len(accList)+1)), accList)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Accuary\")\n",
    "plt.title(\"Accuracy\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, len(f1List)+1)), f1List)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"F1-Score\")\n",
    "plt.title(\"F1 Score\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, len(recallList)+1)), recallList)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Recall\")\n",
    "plt.title(\"Recall\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1, len(preciseList)+1)), preciseList)\n",
    "plt.xlabel(\"Step\")\n",
    "plt.ylabel(\"Precision\")\n",
    "plt.title(\"Precision\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submitDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF = submitDF.copy()\n",
    "finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wina , winb , tie =[], [], [] \n",
    "for i, text in enumerate(submitDataDict[\"test\"][\"text\"]):\n",
    "    probs , classID = getClassifierOutput(text)\n",
    "#     print(\"{}: \", probs.tolist()[0])\n",
    "    rounded_probs = [round(val, 6) for val in probs.tolist()[0]]\n",
    "    print(rounded_probs)\n",
    "    print(\"Classified ID : \", classID)\n",
    "    wina.append(rounded_probs[0])\n",
    "    winb.append(rounded_probs[1])\n",
    "    tie.append(rounded_probs[2])\n",
    "    #calculate probability\n",
    "#     print(\"Probabilities: \", [round(val, 6) for val in probabilities.tolist()[0]])\n",
    "    #write to submit DF\n",
    "#     finalDF.iloc[i, 1:] = [round(val, 6) for val in probabilities.tolist()[0]] #logits.tolist()[0] #update column \n",
    "finalDF[\"winner_model_a\"] = wina\n",
    "finalDF[\"winner_model_b\"] = winb\n",
    "finalDF[\"winner_tie\"] = tie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalDF.to_csv('submission.csv' , index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clearMemory()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5404413,
     "sourceId": 8975866,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8318,
     "sourceId": 28785,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 3301,
     "modelInstanceId": 8332,
     "sourceId": 28808,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
