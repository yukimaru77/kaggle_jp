{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":8984100,"sourceType":"datasetVersion","datasetId":5410430},{"sourceId":8985626,"sourceType":"datasetVersion","datasetId":5411525}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install transformers datasets","metadata":{"execution":{"iopub.status.busy":"2024-07-18T13:40:44.463973Z","iopub.execute_input":"2024-07-18T13:40:44.464908Z","iopub.status.idle":"2024-07-18T13:41:20.741215Z","shell.execute_reply.started":"2024-07-18T13:40:44.464841Z","shell.execute_reply":"2024-07-18T13:41:20.739757Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer\nimport torch.nn as nn\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss\n\n# Define the SiameseLSTM class\nclass SiameseLSTM(nn.Module):\n    def __init__(self, input_size, hidden_size, num_layers):\n        super(SiameseLSTM, self).__init__()\n        self.embedding = nn.Embedding(input_size, hidden_size)\n        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, 3)  # Output size 3 for 3 classes: model A wins, model B wins, tie\n\n    def forward_one(self, x):\n        x = self.embedding(x)\n        _, (h, _) = self.lstm(x)\n        return h[-1]\n\n    def forward(self, x1, x2):\n        h1 = self.forward_one(x1)\n        h2 = self.forward_one(x2)\n        return self.fc(torch.abs(h1 - h2))\n\n# Step 1: Load your training dataset\ndf_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')\n\n# Filter out invalid cases and prepare data\ndata = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values\n\ndef determine_label(row):\n    if row[2] == 1:\n        return 0  # model A wins\n    elif row[3] == 1:\n        return 1  # model B wins\n    elif row[4] == 1:\n        return 2  # tie\n    else:\n        return -1  # Invalid or unclear case\n\nlabels = [determine_label(row) for row in data if determine_label(row) != -1]\ndata = [row[:2] for row in data if determine_label(row) != -1]\n\n# Step 2: Define a custom Dataset class\nclass SiameseDataset(Dataset):\n    def __init__(self, data, labels, tokenizer, max_length):\n        self.data = data\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        pair = self.data[idx]\n        response_a = pair[0]\n        response_b = pair[1]\n        label = self.labels[idx]\n\n        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n\n        return {\n            'input_ids_a': torch.tensor(tokens_a['input_ids']),\n            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),\n            'input_ids_b': torch.tensor(tokens_b['input_ids']),\n            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n# Step 3: Initialize BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')\nmax_length = 128  # Adjust according to your dataset\n\n# Step 4: Create instances of Dataset and DataLoader for training\ntrain_dataset = SiameseDataset(data, labels, tokenizer, max_length)\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n\n# Step 5: Define the Siamese network model\ninput_size = len(tokenizer)\nhidden_size = 300\nnum_layers = 1\nmodel = SiameseLSTM(input_size, hidden_size, num_layers)\n\n# Step 6: Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n\n# Step 7: Training loop\nnum_epochs = 5  # Adjust as needed\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\n\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0\n    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n        input_ids_a = batch['input_ids_a'].to(device)\n        attention_mask_a = batch['attention_mask_a'].to(device)\n        input_ids_b = batch['input_ids_b'].to(device)\n        attention_mask_b = batch['attention_mask_b'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids_a, input_ids_b)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')\n\n# Load your test dataset\ndf_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')\n\n# Define a custom Dataset class for testing\nclass SiameseTestDataset(Dataset):\n    def __init__(self, data, tokenizer, max_length):\n        self.data = data\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        pair = self.data[idx]\n        response_a = pair[0]\n        response_b = pair[1]\n\n        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n\n        return {\n            'input_ids_a': torch.tensor(tokens_a['input_ids']),\n            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),\n            'input_ids_b': torch.tensor(tokens_b['input_ids']),\n            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),\n        }\n\n# Prepare test data\ntest_data = df_test[['response_a', 'response_b']].values.tolist()\n\n# Create instance of SiameseTestDataset\ntest_dataset = SiameseTestDataset(test_data, tokenizer, max_length)\n\n# Create DataLoader for the test dataset\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# Load the saved model\nmodel.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))\nmodel.eval()  # Set model to evaluation mode\n\n# Perform inference on the test data and generate predictions\nall_preds = []\nwith torch.no_grad():\n    for batch in tqdm(test_loader, desc='Testing'):\n        input_ids_a = batch['input_ids_a'].to(device)\n        attention_mask_a = batch['attention_mask_a'].to(device)\n        input_ids_b = batch['input_ids_b'].to(device)\n        attention_mask_b = batch['attention_mask_b'].to(device)\n\n        outputs = model(input_ids_a, input_ids_b)\n        probabilities = nn.Softmax(dim=1)(outputs)\n        all_preds.extend(probabilities.cpu().numpy().tolist())\n\n# Create a DataFrame for predictions\npred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\npred_df['id'] = df_test['id']\n\n# Reorder columns to match the required format\npred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n\n# Save predictions to CSV for submission\npred_df.to_csv('submission.csv', index=False)\nprint(pred_df.head())\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-18T17:01:34.312463Z","iopub.execute_input":"2024-07-18T17:01:34.312889Z","iopub.status.idle":"2024-07-18T20:21:54.723113Z","shell.execute_reply.started":"2024-07-18T17:01:34.312857Z","shell.execute_reply":"2024-07-18T20:21:54.721672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2024-07-19T02:24:36.878642Z","iopub.execute_input":"2024-07-19T02:24:36.879044Z","iopub.status.idle":"2024-07-19T02:24:36.922822Z","shell.execute_reply.started":"2024-07-19T02:24:36.879013Z","shell.execute_reply":"2024-07-19T02:24:36.920948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}