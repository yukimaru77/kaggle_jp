{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-26T07:03:59.61667Z",
     "iopub.status.busy": "2024-07-26T07:03:59.616067Z",
     "iopub.status.idle": "2024-07-26T07:03:59.630139Z",
     "shell.execute_reply": "2024-07-26T07:03:59.629298Z",
     "shell.execute_reply.started": "2024-07-26T07:03:59.616636Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Credits:\n",
    "\n",
    "https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference\n",
    "https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora\n",
    "\n",
    "LB: 0.945\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:03:59.647261Z",
     "iopub.status.busy": "2024-07-26T07:03:59.646755Z",
     "iopub.status.idle": "2024-07-26T07:04:18.020077Z",
     "shell.execute_reply": "2024-07-26T07:04:18.018911Z",
     "shell.execute_reply.started": "2024-07-26T07:03:59.647236Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:04:18.023082Z",
     "iopub.status.busy": "2024-07-26T07:04:18.022391Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import (\n",
    "    Gemma2ForSequenceClassification, GemmaTokenizerFast, \n",
    "    AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    ")\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    gemma_lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    llama_model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    llama_weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    tta = False\n",
    "    spread_max_length = False\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):\n",
    "    if isinstance(tokenizer, GemmaTokenizerFast):\n",
    "        prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    else:\n",
    "        prompt = [\"User prompt: \" + p for p in prompt]\n",
    "        response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "        response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    \n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-26T07:04:36.823168Z",
     "shell.execute_reply": "2024-07-26T07:04:36.822425Z",
     "shell.execute_reply.started": "2024-07-26T07:04:35.178994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gemma Tokenizer\n",
    "gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "gemma_tokenizer.add_eos_token = True\n",
    "gemma_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Llama Tokenizer\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# Prepare data for both models\n",
    "gemma_data = pd.DataFrame()\n",
    "gemma_data[\"id\"] = test[\"id\"]\n",
    "gemma_data[\"input_ids\"], gemma_data[\"attention_mask\"] = tokenize(gemma_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "gemma_data[\"length\"] = gemma_data[\"input_ids\"].apply(len)\n",
    "\n",
    "llama_data = pd.DataFrame()\n",
    "llama_data[\"id\"] = test[\"id\"]\n",
    "llama_data[\"input_ids\"], llama_data[\"attention_mask\"] = tokenize(llama_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "llama_data[\"length\"] = llama_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:04:36.824439Z",
     "iopub.status.busy": "2024-07-26T07:04:36.82418Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Gemma model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "gemma_model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False\n",
    ")\n",
    "gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-26T07:08:14.149927Z",
     "shell.execute_reply": "2024-07-26T07:08:14.147703Z",
     "shell.execute_reply.started": "2024-07-26T07:06:08.525096Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load Llama model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False\n",
    ")\n",
    "llama_base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.llama_model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj']\n",
    ")\n",
    "llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)\n",
    "llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)\n",
    "llama_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.150889Z",
     "iopub.status.idle": "2024-07-26T07:08:14.151222Z",
     "shell.execute_reply": "2024-07-26T07:08:14.15108Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.151066Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.152742Z",
     "iopub.status.idle": "2024-07-26T07:08:14.153108Z",
     "shell.execute_reply": "2024-07-26T07:08:14.152929Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.152915Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "# Sort data by input length\n",
    "gemma_data = gemma_data.sort_values(\"length\", ascending=False)\n",
    "llama_data = llama_data.sort_values(\"length\", ascending=False)\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, \n",
    "                           (gemma_data, llama_data), \n",
    "                           (gemma_model, llama_model),\n",
    "                           (gemma_tokenizer, llama_tokenizer),\n",
    "                           (device_0, device_1))\n",
    "\n",
    "gemma_result_df, llama_result_df = list(results)\n",
    "\n",
    "# Combine results (simple average)\n",
    "combined_result_df = gemma_result_df.copy()\n",
    "combined_result_df[\"winner_model_a\"] = (gemma_result_df[\"winner_model_a\"] + llama_result_df[\"winner_model_a\"]) / 2\n",
    "combined_result_df[\"winner_model_b\"] = (gemma_result_df[\"winner_model_b\"] + llama_result_df[\"winner_model_b\"]) / 2\n",
    "combined_result_df[\"winner_tie\"] = (gemma_result_df[\"winner_tie\"] + llama_result_df[\"winner_tie\"]) / 2\n",
    "\n",
    "print(f\"Inference time: {time.time() - st:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.155209Z",
     "iopub.status.idle": "2024-07-26T07:08:14.155526Z",
     "shell.execute_reply": "2024-07-26T07:08:14.155387Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.155374Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = combined_result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
