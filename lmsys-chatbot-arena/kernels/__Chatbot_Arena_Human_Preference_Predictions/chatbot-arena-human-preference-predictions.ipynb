{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:05.995908Z",
     "iopub.status.busy": "2024-07-18T13:18:05.995544Z",
     "iopub.status.idle": "2024-07-18T13:18:07.552234Z",
     "shell.execute_reply": "2024-07-18T13:18:07.551436Z",
     "shell.execute_reply.started": "2024-07-18T13:18:05.995877Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly import express as px\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:07.554145Z",
     "iopub.status.busy": "2024-07-18T13:18:07.553697Z",
     "iopub.status.idle": "2024-07-18T13:18:07.778611Z",
     "shell.execute_reply": "2024-07-18T13:18:07.777686Z",
     "shell.execute_reply.started": "2024-07-18T13:18:07.554119Z"
    }
   },
   "outputs": [],
   "source": [
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# set max column width to 500\n",
    "pd.set_option(\"display.max_colwidth\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:07.780031Z",
     "iopub.status.busy": "2024-07-18T13:18:07.779733Z",
     "iopub.status.idle": "2024-07-18T13:18:07.784883Z",
     "shell.execute_reply": "2024-07-18T13:18:07.783956Z",
     "shell.execute_reply.started": "2024-07-18T13:18:07.779986Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA_PATH = \"/kaggle/input/lmsys-chatbot-arena/train.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad85804",
   "metadata": {},
   "source": [
    "# Data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:07.787554Z",
     "iopub.status.busy": "2024-07-18T13:18:07.787237Z",
     "iopub.status.idle": "2024-07-18T13:18:11.428448Z",
     "shell.execute_reply": "2024-07-18T13:18:11.42755Z",
     "shell.execute_reply.started": "2024-07-18T13:18:07.787528Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:11.429951Z",
     "iopub.status.busy": "2024-07-18T13:18:11.42967Z",
     "iopub.status.idle": "2024-07-18T13:18:11.477802Z",
     "shell.execute_reply": "2024-07-18T13:18:11.476954Z",
     "shell.execute_reply.started": "2024-07-18T13:18:11.429926Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeec98a",
   "metadata": {},
   "source": [
    "# Models performance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:11.479153Z",
     "iopub.status.busy": "2024-07-18T13:18:11.478877Z",
     "iopub.status.idle": "2024-07-18T13:18:11.503085Z",
     "shell.execute_reply": "2024-07-18T13:18:11.502308Z",
     "shell.execute_reply.started": "2024-07-18T13:18:11.479129Z"
    }
   },
   "outputs": [],
   "source": [
    "model_usage = pd.concat([df[\"model_a\"], df[\"model_b\"]]).value_counts()\n",
    "\n",
    "# How many unique models are there?\n",
    "len(model_usage.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:11.504359Z",
     "iopub.status.busy": "2024-07-18T13:18:11.504095Z",
     "iopub.status.idle": "2024-07-18T13:18:12.802732Z",
     "shell.execute_reply": "2024-07-18T13:18:12.801577Z",
     "shell.execute_reply.started": "2024-07-18T13:18:11.504336Z"
    }
   },
   "outputs": [],
   "source": [
    "# plot the model usage\n",
    "fig = px.bar(\n",
    "    x=model_usage.index,\n",
    "    y=model_usage.values,\n",
    "    labels={\"x\": \"Model\", \"y\": \"Usage times\"},\n",
    "    title=\"Model Usage Distribution\",\n",
    ")\n",
    "\n",
    "fig.update_layout(width=1200, height=700)\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:12.804625Z",
     "iopub.status.busy": "2024-07-18T13:18:12.804229Z",
     "iopub.status.idle": "2024-07-18T13:18:12.846275Z",
     "shell.execute_reply": "2024-07-18T13:18:12.845471Z",
     "shell.execute_reply.started": "2024-07-18T13:18:12.804591Z"
    }
   },
   "outputs": [],
   "source": [
    "# count the number of times each pair of models are compared, regardless of the order\n",
    "compared_models_count = (\n",
    "    pd.DataFrame(\n",
    "        np.sort(df[[\"model_a\", \"model_b\"]].values, axis=1),\n",
    "        columns=[\"model_a\", \"model_b\"],\n",
    "    )\n",
    "    .value_counts()\n",
    "    .reset_index(name=\"counts\")\n",
    ")\n",
    "\n",
    "len(compared_models_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:12.847708Z",
     "iopub.status.busy": "2024-07-18T13:18:12.847402Z",
     "iopub.status.idle": "2024-07-18T13:18:12.932164Z",
     "shell.execute_reply": "2024-07-18T13:18:12.931256Z",
     "shell.execute_reply.started": "2024-07-18T13:18:12.847681Z"
    }
   },
   "outputs": [],
   "source": [
    "top_compared_models = compared_models_count.head(20)\n",
    "\n",
    "fig = px.bar(\n",
    "    x=top_compared_models[\"model_a\"] + \" vs \" + top_compared_models[\"model_b\"],\n",
    "    y=top_compared_models[\"counts\"],\n",
    "    labels={\"x\": \"Model Comparison\", \"y\": \"Comparison times\"},\n",
    "    title=\"Top 20 Model Comparison Distribution\",\n",
    ")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:12.937165Z",
     "iopub.status.busy": "2024-07-18T13:18:12.936838Z",
     "iopub.status.idle": "2024-07-18T13:18:12.946389Z",
     "shell.execute_reply": "2024-07-18T13:18:12.945351Z",
     "shell.execute_reply.started": "2024-07-18T13:18:12.937139Z"
    }
   },
   "outputs": [],
   "source": [
    "# there are 1275 combinations of compared models. How often given numbers of comparisons occur?\n",
    "\n",
    "compared_models_count[\"counts\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:12.947783Z",
     "iopub.status.busy": "2024-07-18T13:18:12.947529Z",
     "iopub.status.idle": "2024-07-18T13:18:12.972359Z",
     "shell.execute_reply": "2024-07-18T13:18:12.971485Z",
     "shell.execute_reply.started": "2024-07-18T13:18:12.94776Z"
    }
   },
   "outputs": [],
   "source": [
    "# bin the comparison count distribution to 10 bins\n",
    "comparision_count_distribution_bins = pd.cut(\n",
    "    compared_models_count[\"counts\"],\n",
    "    bins=[\n",
    "        0,\n",
    "        5,\n",
    "        10,\n",
    "        20,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "        80,\n",
    "        90,\n",
    "        100,\n",
    "        compared_models_count[\"counts\"].max(),\n",
    "    ],\n",
    "    precision=0,\n",
    "    retbins=False,\n",
    ")\n",
    "\n",
    "comparision_count_distribution_bins.value_counts(normalize=True).sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6c05a1",
   "metadata": {},
   "source": [
    "**Which models performs the best and the worst?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:12.973691Z",
     "iopub.status.busy": "2024-07-18T13:18:12.973426Z",
     "iopub.status.idle": "2024-07-18T13:18:13.009194Z",
     "shell.execute_reply": "2024-07-18T13:18:13.008181Z",
     "shell.execute_reply.started": "2024-07-18T13:18:12.973668Z"
    }
   },
   "outputs": [],
   "source": [
    "model_scores_part1 = df.groupby(\"model_a\")[\"winner_model_a\"].sum()\n",
    "model_scores_part2 = df.groupby(\"model_b\")[\"winner_model_b\"].sum()\n",
    "\n",
    "model_scores = model_scores_part1.add(model_scores_part2, fill_value=0)\n",
    "model_scores = model_scores / (\n",
    "    df[\"model_a\"].value_counts() + df[\"model_b\"].value_counts()\n",
    ")\n",
    "model_scores = model_scores.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.010637Z",
     "iopub.status.busy": "2024-07-18T13:18:13.010385Z",
     "iopub.status.idle": "2024-07-18T13:18:13.095275Z",
     "shell.execute_reply": "2024-07-18T13:18:13.094376Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.010615Z"
    }
   },
   "outputs": [],
   "source": [
    "# top 10 best models\n",
    "fig = px.bar(\n",
    "    x=model_scores.head(10).index,\n",
    "    y=model_scores.head(10).values,\n",
    "    labels={\"x\": \"Model\", \"y\": \"Score\"},\n",
    "    title=\"Top 10 best-rated models\",\n",
    ")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.097256Z",
     "iopub.status.busy": "2024-07-18T13:18:13.096513Z",
     "iopub.status.idle": "2024-07-18T13:18:13.178939Z",
     "shell.execute_reply": "2024-07-18T13:18:13.178047Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.09722Z"
    }
   },
   "outputs": [],
   "source": [
    "# top 10 worst models\n",
    "fig = px.bar(\n",
    "    x=model_scores.tail(10).index,\n",
    "    y=model_scores.tail(10).values,\n",
    "    labels={\"x\": \"Model\", \"y\": \"Score\"},\n",
    "    title=\"Top 10 worst-rated models\",\n",
    ")\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.180335Z",
     "iopub.status.busy": "2024-07-18T13:18:13.18006Z",
     "iopub.status.idle": "2024-07-18T13:18:13.196651Z",
     "shell.execute_reply": "2024-07-18T13:18:13.195879Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.180311Z"
    }
   },
   "outputs": [],
   "source": [
    "tie_df = df.query(\"winner_tie == 1\")\n",
    "(tie_df[\"winner_model_a\"] + tie_df[\"winner_model_b\"]).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b712cdf3",
   "metadata": {},
   "source": [
    "**How frequent is tie?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.197958Z",
     "iopub.status.busy": "2024-07-18T13:18:13.197678Z",
     "iopub.status.idle": "2024-07-18T13:18:13.204018Z",
     "shell.execute_reply": "2024-07-18T13:18:13.203071Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.197933Z"
    }
   },
   "outputs": [],
   "source": [
    "f\"{(tie_df.shape[0] / df.shape[0]) * 100:.1f}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab41fee",
   "metadata": {},
   "source": [
    "# Positive and negative responses analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.205436Z",
     "iopub.status.busy": "2024-07-18T13:18:13.205165Z",
     "iopub.status.idle": "2024-07-18T13:18:13.809205Z",
     "shell.execute_reply": "2024-07-18T13:18:13.808273Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.205413Z"
    }
   },
   "outputs": [],
   "source": [
    "# exclude tied examples\n",
    "df_no_ties = df.query(\"winner_tie == 0\")\n",
    "\n",
    "loosing_responses = df_no_ties.apply(\n",
    "    lambda x: (x[\"response_a\"] if x[\"winner_model_a\"] == 0 else x[\"response_b\"]),\n",
    "    axis=1,\n",
    ")\n",
    "\n",
    "loosing_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:13.811314Z",
     "iopub.status.busy": "2024-07-18T13:18:13.81093Z",
     "iopub.status.idle": "2024-07-18T13:18:14.400014Z",
     "shell.execute_reply": "2024-07-18T13:18:14.399214Z",
     "shell.execute_reply.started": "2024-07-18T13:18:13.811281Z"
    }
   },
   "outputs": [],
   "source": [
    "winning_responses = df_no_ties.apply(\n",
    "    lambda x: (x[\"response_a\"] if x[\"winner_model_a\"] == 1 else x[\"response_b\"]),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:14.401842Z",
     "iopub.status.busy": "2024-07-18T13:18:14.401226Z",
     "iopub.status.idle": "2024-07-18T13:18:14.649426Z",
     "shell.execute_reply": "2024-07-18T13:18:14.648617Z",
     "shell.execute_reply.started": "2024-07-18T13:18:14.401807Z"
    }
   },
   "outputs": [],
   "source": [
    "# loosing responses must be cleaned\n",
    "loosing_responses = loosing_responses.str.strip(\"[]\")\n",
    "loosing_responses = loosing_responses.str.strip('\"')\n",
    "\n",
    "winning_responses = winning_responses.str.strip(\"[]\")\n",
    "winning_responses = winning_responses.str.strip('\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:14.651348Z",
     "iopub.status.busy": "2024-07-18T13:18:14.650648Z",
     "iopub.status.idle": "2024-07-18T13:18:14.659286Z",
     "shell.execute_reply": "2024-07-18T13:18:14.658396Z",
     "shell.execute_reply.started": "2024-07-18T13:18:14.651313Z"
    }
   },
   "outputs": [],
   "source": [
    "df_no_ties[\"winning_response\"] = winning_responses\n",
    "df_no_ties[\"loosing_response\"] = loosing_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:14.660982Z",
     "iopub.status.busy": "2024-07-18T13:18:14.66066Z",
     "iopub.status.idle": "2024-07-18T13:18:17.009972Z",
     "shell.execute_reply": "2024-07-18T13:18:17.00926Z",
     "shell.execute_reply.started": "2024-07-18T13:18:14.660946Z"
    }
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I do not feel comfortable\",\n",
    "    \"I'm sorry, but\",\n",
    "    \"I am sorry, but\",\n",
    "    \"I apologize, but\",\n",
    "    \"Unfortunately I\",\n",
    "    \"I do not have enough\",\n",
    "    \"I'm just an AI\",\n",
    "    \"I'm an AI and\",\n",
    "    \"I'm afraid\",\n",
    "    \"I can't\",\n",
    "]\n",
    "\n",
    "unprecise_responses = []\n",
    "for _, row in df_no_ties.iterrows():\n",
    "    if row[\"loosing_response\"].startswith(tuple(texts)):\n",
    "        if row[\"winning_response\"].startswith(tuple(texts)):\n",
    "            unprecise_responses.append(\"both\")\n",
    "        else:\n",
    "            unprecise_responses.append(\"loosing\")\n",
    "    elif row[\"winning_response\"].startswith(tuple(texts)):\n",
    "        unprecise_responses.append(\"winning\")\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "pd.Series(unprecise_responses).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:18:17.011267Z",
     "iopub.status.busy": "2024-07-18T13:18:17.010992Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"jax\"  # or \"tensorflow\" or \"torch\"\n",
    "\n",
    "import keras_nlp\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    seed = 42  # Random seed\n",
    "    preset = \"deberta_v3_extra_small_en\" # Name of pretrained models\n",
    "    sequence_length = 512  # Input sequence length\n",
    "    epochs = 3 # Training epochs\n",
    "    batch_size = 16  # Batch size\n",
    "    scheduler = 'cosine'  # Learning rate scheduler\n",
    "    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}\n",
    "    name2label = {v:k for k, v in label2name.items()}\n",
    "    class_labels = list(label2name.keys())\n",
    "    class_names = list(label2name.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.set_random_seed(CFG.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Data\n",
    "df = pd.read_csv(f'{BASE_PATH}/train.csv') \n",
    "\n",
    "# Sample data\n",
    "# df = df.sample(frac=0.10)\n",
    "\n",
    "# Take the first prompt and its associated response\n",
    "df[\"prompt\"] = df.prompt.map(lambda x: eval(x)[0])\n",
    "df[\"response_a\"] = df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\n",
    "df[\"response_b\"] = df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n",
    "\n",
    "# Label conversion\n",
    "df[\"class_name\"] = df[[\"winner_model_a\", \"winner_model_b\" , \"winner_tie\"]].idxmax(axis=1)\n",
    "df[\"class_label\"] = df.class_name.map(CFG.name2label)\n",
    "\n",
    "# Show Sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Test Data\n",
    "test_df = pd.read_csv(f'{BASE_PATH}/test.csv')\n",
    "\n",
    "# Take the first prompt and response\n",
    "test_df[\"prompt\"] = test_df.prompt.map(lambda x: eval(x)[0])\n",
    "test_df[\"response_a\"] = test_df.response_a.map(lambda x: eval(x.replace(\"null\",\"''\"))[0])\n",
    "test_df[\"response_b\"] = test_df.response_b.map(lambda x: eval(x.replace(\"null\", \"''\"))[0])\n",
    "\n",
    "# Show Sample\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create options based on the prompt and choices\n",
    "def make_pairs(row):\n",
    "    row[\"encode_fail\"] = False\n",
    "    try:\n",
    "        prompt = row.prompt.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        prompt = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "\n",
    "    try:\n",
    "        response_a = row.response_a.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        response_a = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "\n",
    "    try:\n",
    "        response_b = row.response_b.encode(\"utf-8\").decode(\"utf-8\")\n",
    "    except:\n",
    "        response_b = \"\"\n",
    "        row[\"encode_fail\"] = True\n",
    "        \n",
    "    row['options'] = [f\"Prompt: {prompt}\\n\\nResponse: {response_a}\",  # Response from Model A\n",
    "                      f\"Prompt: {prompt}\\n\\nResponse: {response_b}\"  # Response from Model B\n",
    "                     ]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\n",
    "display(df.head(2))  # Display the first 2 rows of df\n",
    "\n",
    "test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df\n",
    "display(test_df.head(2))  # Display the first 2 rows of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.encode_fail.value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.concat([df.model_a, df.model_b])\n",
    "counts = model_df.value_counts().reset_index()\n",
    "counts.columns = ['LLM', 'Count']\n",
    "\n",
    "# Create a bar plot with custom styling using Plotly\n",
    "fig = px.bar(counts, x='LLM', y='Count',\n",
    "             title='Distribution of LLMs',\n",
    "             color='Count', color_continuous_scale='viridis')\n",
    "\n",
    "fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = df['class_name'].value_counts().reset_index()\n",
    "counts.columns = ['Winner', 'Win Count']\n",
    "\n",
    "fig = px.bar(counts, x='Winner', y='Win Count',\n",
    "             title='Winner distribution for Train Data',\n",
    "             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},\n",
    "             color='Winner', color_continuous_scale='viridis')\n",
    "\n",
    "fig.update_layout(xaxis_title=\"Winner\", yaxis_title=\"Win Count\")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  # Import package\n",
    "\n",
    "train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df[\"class_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(\n",
    "    preset=CFG.preset, # Name of the model\n",
    "    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = preprocessor(df.options.iloc[0])  # Process options for the first row\n",
    "\n",
    "# Display the shape of each processed output\n",
    "for k, v in outs.items():\n",
    "    print(k, \":\", v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_fn(text, label=None):\n",
    "    text = preprocessor(text)  # Preprocess text\n",
    "    return (text, label) if label is not None else text  # Return processed text and label if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(texts, labels=None, batch_size=32,\n",
    "                  cache=True, shuffle=1024):\n",
    "    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option\n",
    "    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices\n",
    "    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices\n",
    "    ds = ds.cache() if cache else ds  # Cache dataset if enabled\n",
    "    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function\n",
    "    opt = tf.data.Options()  # Create dataset options\n",
    "    if shuffle: \n",
    "        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled\n",
    "        opt.experimental_deterministic = False\n",
    "    ds = ds.with_options(opt)  # Set dataset options\n",
    "    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset\n",
    "    ds = ds.prefetch(AUTO)  # Prefetch next batch\n",
    "    return ds  # Return the built dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_texts = train_df.options.tolist()  # Extract training texts\n",
    "train_labels = train_df.class_label.tolist()  # Extract training labels\n",
    "train_ds = build_dataset(train_texts, train_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=True)\n",
    "\n",
    "# Valid\n",
    "valid_texts = valid_df.options.tolist()  # Extract validation texts\n",
    "valid_labels = valid_df.class_label.tolist()  # Extract validation labels\n",
    "valid_ds = build_dataset(valid_texts, valid_labels,\n",
    "                         batch_size=CFG.batch_size,\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):\n",
    "    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6\n",
    "    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8\n",
    "\n",
    "    def lrfn(epoch):  # Learning rate update function\n",
    "        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start\n",
    "        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max\n",
    "        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min\n",
    "        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)\n",
    "        elif mode == 'cos':\n",
    "            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep\n",
    "            phase = math.pi * decay_epoch_index / decay_total_epochs\n",
    "            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min\n",
    "        return lr\n",
    "\n",
    "    if plot:  # Plot lr curve if plot is True\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')\n",
    "        plt.xlabel('epoch'); plt.ylabel('lr')\n",
    "        plt.title('LR Scheduler')\n",
    "        plt.show()\n",
    "\n",
    "    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cb = get_lr_callback(CFG.batch_size, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',\n",
    "                                          monitor='val_log_loss',\n",
    "                                          save_best_only=True,\n",
    "                                          save_weights_only=True,\n",
    "                                          mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_loss = keras.metrics.CategoricalCrossentropy(name=\"log_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input layers\n",
    "inputs = {\n",
    "    \"token_ids\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"token_ids\"),\n",
    "    \"padding_mask\": keras.Input(shape=(2, None), dtype=tf.int32, name=\"padding_mask\"),\n",
    "}\n",
    "# Create a DebertaV3Classifier backbone\n",
    "backbone = keras_nlp.models.DebertaV3Backbone.from_preset(\n",
    "    CFG.preset,\n",
    ")\n",
    "\n",
    "# Compute embeddings for first response: (P + R_A) using backbone\n",
    "response_a = {k: v[:, 0, :] for k, v in inputs.items()}\n",
    "embed_a = backbone(response_a)\n",
    "\n",
    "# Compute embeddings for second response: (P + R_B), using the same backbone\n",
    "response_b = {k: v[:, 1, :] for k, v in inputs.items()}\n",
    "embed_b = backbone(response_b)\n",
    "\n",
    "# Compute final output\n",
    "embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])\n",
    "embeds = keras.layers.GlobalAveragePooling1D()(embeds)\n",
    "outputs = keras.layers.Dense(3, activation=\"softmax\", name=\"classifier\")(embeds)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model with optimizer, loss, and metrics\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(5e-6),\n",
    "    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),\n",
    "    metrics=[\n",
    "        log_loss,\n",
    "        keras.metrics.CategoricalAccuracy(name=\"accuracy\"),\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training the model\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    epochs=CFG.epochs,\n",
    "    validation_data=valid_ds,\n",
    "    callbacks=[lr_cb, ckpt_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('/kaggle/working/best_model.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build test dataset\n",
    "test_texts = test_df.options.tolist()\n",
    "test_ds = build_dataset(test_texts,\n",
    "                         batch_size=min(len(test_df), CFG.batch_size),\n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = model.predict(test_ds, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df = test_df[[\"id\"]].copy()\n",
    "sub_df[CFG.class_names] = test_preds.tolist()\n",
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "sub_df.head()"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 4684,
     "sourceId": 6063,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
