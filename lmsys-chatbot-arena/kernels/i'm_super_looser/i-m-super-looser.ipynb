{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"sourceType":"competition"},{"sourceId":3570,"sourceType":"datasetVersion","datasetId":2120},{"sourceId":9054357,"sourceType":"datasetVersion","datasetId":934701}],"dockerImageVersionId":30734,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nfrom datasets import load_dataset, DatasetDict\nfrom transformers import BertTokenizer, BertTokenizerFast, TFBertModel, DataCollatorWithPadding, TFAutoModel\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nimport shutil\nimport pandas as pd\nfrom tqdm.keras import TqdmCallback\nimport re\nimport math\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nimport numpy as np\nfrom tensorflow import keras\nfrom tensorflow.keras.optimizers import Adam","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:27:35.818503Z","iopub.execute_input":"2024-08-04T04:27:35.818778Z","iopub.status.idle":"2024-08-04T04:27:55.999849Z","shell.execute_reply.started":"2024-08-04T04:27:35.818753Z","shell.execute_reply":"2024-08-04T04:27:55.998858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check for GPU availability\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n    except RuntimeError as e:\n        print(e)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:27:56.001615Z","iopub.execute_input":"2024-08-04T04:27:56.002662Z","iopub.status.idle":"2024-08-04T04:27:56.354389Z","shell.execute_reply.started":"2024-08-04T04:27:56.002624Z","shell.execute_reply":"2024-08-04T04:27:56.353351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect hardware, return appropriate distribution strategy\ntry:\n    # TPU detection. No parameters necessary if TPU_NAME environment variable is\n    # set: this is always the case on Kaggle.\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n    strategy = tf.distribute.MirroredStrategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:27:56.355745Z","iopub.execute_input":"2024-08-04T04:27:56.356362Z","iopub.status.idle":"2024-08-04T04:27:56.395689Z","shell.execute_reply.started":"2024-08-04T04:27:56.356335Z","shell.execute_reply":"2024-08-04T04:27:56.394684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# path for sets\ntrain_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'\ntest_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'\n\n# loading datasets\ntrain_dataset = load_dataset('csv', data_files={'train': train_path})['train']\ntest_dataset = load_dataset('csv', data_files={'test': test_path})['test']\n\n# saving ID\ntest_ids = test_dataset['id']","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:27:56.398335Z","iopub.execute_input":"2024-08-04T04:27:56.398926Z","iopub.status.idle":"2024-08-04T04:28:41.150646Z","shell.execute_reply.started":"2024-08-04T04:27:56.398897Z","shell.execute_reply":"2024-08-04T04:28:41.14973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# adding missing columns in the test set\nfor col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:\n    if col not in test_dataset.column_names:\n        test_dataset = test_dataset.add_column(col, [\"\"] * len(test_dataset))\n\n# transformation to int64\nfor col in ['winner_model_a', 'winner_model_b', 'winner_tie']:\n    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})\n    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != \"\" else 0})","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:28:41.15193Z","iopub.execute_input":"2024-08-04T04:28:41.152281Z","iopub.status.idle":"2024-08-04T04:29:07.461536Z","shell.execute_reply.started":"2024-08-04T04:28:41.152257Z","shell.execute_reply":"2024-08-04T04:29:07.46063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using bert-base-cased's files locally\nsource_dir = '/kaggle/input/huggingface-bert/bert-base-cased'\n\nmodel_dir = '/kaggle/working/bert-base-cased'\nos.makedirs(model_dir, exist_ok=True)\n\nshutil.copy(os.path.join(source_dir, 'config.json'), model_dir)\nshutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)\nshutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)\nshutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)\nshutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)\nshutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:07.462893Z","iopub.execute_input":"2024-08-04T04:29:07.463595Z","iopub.status.idle":"2024-08-04T04:29:19.760404Z","shell.execute_reply.started":"2024-08-04T04:29:07.463557Z","shell.execute_reply":"2024-08-04T04:29:19.759476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stopwords_path = '/kaggle/input/stopwords/stopwords/english'\n\n# Функция для загрузки стоп-слов из файла\ndef load_stopwords(stopwords_path):\n    with open(stopwords_path, 'r') as file:\n        stopwords = file.read().splitlines()\n    return set(stopwords)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:19.761565Z","iopub.execute_input":"2024-08-04T04:29:19.76185Z","iopub.status.idle":"2024-08-04T04:29:19.766684Z","shell.execute_reply.started":"2024-08-04T04:29:19.761826Z","shell.execute_reply":"2024-08-04T04:29:19.76577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize the tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(model_dir)\n# download stopwords\nstopwords = load_stopwords(stopwords_path)\n# Function for text cleaning\ndef clean_text(text):\n    text = text.lower()\n    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n    text = re.sub(r'\\@\\w+|\\#','', text)\n    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)  # Удалить пунктуацию\n    text = ' '.join([word for word in text.split() if word not in stopwords])  # Удалить стоп-слова\n    return text\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:19.76774Z","iopub.execute_input":"2024-08-04T04:29:19.768007Z","iopub.status.idle":"2024-08-04T04:29:19.83253Z","shell.execute_reply.started":"2024-08-04T04:29:19.767985Z","shell.execute_reply":"2024-08-04T04:29:19.831765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_function(examples):\n    # Clean each text field\n    cleaned_prompts = [clean_text(text) for text in examples['prompt']]\n    cleaned_responses_a = [clean_text(text) for text in examples['response_a']]\n    cleaned_responses_b = [clean_text(text) for text in examples['response_b']]\n    \n    # Tokenize the cleaned texts\n    return tokenizer(cleaned_prompts,\n                     cleaned_responses_a,\n                     cleaned_responses_b,\n                     padding=\"max_length\", \n                     truncation=True, \n                     max_length=512)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:19.833429Z","iopub.execute_input":"2024-08-04T04:29:19.833715Z","iopub.status.idle":"2024-08-04T04:29:19.83965Z","shell.execute_reply.started":"2024-08-04T04:29:19.83369Z","shell.execute_reply":"2024-08-04T04:29:19.838752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Пример использования функции\nexamples = {\n    'prompt': [\"This is a sample prompt.\"],\n    'response_a': [\"This is a sample response A.\"],\n    'response_b': [\"This is a sample response B.\"]\n}\n\ntokenized_output = tokenize_function(examples)\nprint(tokenized_output)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:19.842517Z","iopub.execute_input":"2024-08-04T04:29:19.843076Z","iopub.status.idle":"2024-08-04T04:29:19.853633Z","shell.execute_reply.started":"2024-08-04T04:29:19.843043Z","shell.execute_reply":"2024-08-04T04:29:19.852719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# apply the tokenization and cleaning function with multiprocessing num_proc=num_proc\nnum_proc = multiprocessing.cpu_count()\n\n# add try-except block for better error handling\ntry:\n    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n    test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)\nexcept Exception as e:\n    print(f\"Error during tokenization: {e}\")\n    \ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:29:19.854925Z","iopub.execute_input":"2024-08-04T04:29:19.855306Z","iopub.status.idle":"2024-08-04T04:30:55.623425Z","shell.execute_reply.started":"2024-08-04T04:29:19.855274Z","shell.execute_reply":"2024-08-04T04:30:55.622493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add debug prints after tokenization\nprint(\"Sample tokenized train dataset entry:\")\nprint(tokenized_datasets[0])\nif len(tokenized_datasets) == 0:\n    raise ValueError(\"The tokenized training dataset is empty.\")\nif len(test_tokenized_datasets) == 0:\n    raise ValueError(\"The tokenized test dataset is empty.\")\n# Print column names for debugging\nprint(f\"Tokenized training dataset columns: {tokenized_datasets.column_names}\")\nprint(f\"Tokenized test dataset columns: {test_tokenized_datasets.column_names}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:30:55.624821Z","iopub.execute_input":"2024-08-04T04:30:55.625297Z","iopub.status.idle":"2024-08-04T04:30:55.635319Z","shell.execute_reply.started":"2024-08-04T04:30:55.625262Z","shell.execute_reply":"2024-08-04T04:30:55.63426Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert to tf.data.Dataset with the correct shape\ndef convert_to_tf_dataset(dataset, label_col=None, for_inference=False):\n    input_columns = tokenizer.model_input_names\n    \n    if label_col and not for_inference:\n        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in input_columns])\n    else:\n        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in input_columns])\n    \n    # ensure labels are not sequences\n    if label_col:\n        dataset = dataset.map(lambda x: {label_col: int(x[label_col])})\n    \n    shuffle = not for_inference\n    batch_size = 16 if for_inference else 450\n\n    tf_dataset = dataset.to_tf_dataset(\n        columns=input_columns,\n        label_cols=[label_col] if label_col and not for_inference else None,\n        shuffle=shuffle,\n        batch_size=batch_size,\n        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)\n    )\n\n    return tf_dataset","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:30:55.636644Z","iopub.execute_input":"2024-08-04T04:30:55.636992Z","iopub.status.idle":"2024-08-04T04:30:55.75732Z","shell.execute_reply.started":"2024-08-04T04:30:55.636961Z","shell.execute_reply":"2024-08-04T04:30:55.756491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# run the conversion\ntry:\n    train_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')\n    test_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')\nexcept Exception as e:\n    print(f\"Error during dataset conversion: {e}\")","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:30:55.758474Z","iopub.execute_input":"2024-08-04T04:30:55.758748Z","iopub.status.idle":"2024-08-04T04:31:01.799613Z","shell.execute_reply.started":"2024-08-04T04:30:55.758723Z","shell.execute_reply":"2024-08-04T04:31:01.798622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add debug prints after dataset conversion\nprint(\"Sample from converted train tf.data.Dataset:\")\nfor batch in train_tf_dataset.take(1):\n    inputs, labels = batch\n    print(f'Input IDs shape: {inputs[\"input_ids\"].shape}')\n    print(f'Attention mask shape: {inputs[\"attention_mask\"].shape}')\n    print(f'Labels shape: {labels.shape}')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:31:01.800784Z","iopub.execute_input":"2024-08-04T04:31:01.801088Z","iopub.status.idle":"2024-08-04T04:31:03.111396Z","shell.execute_reply.started":"2024-08-04T04:31:01.801062Z","shell.execute_reply":"2024-08-04T04:31:03.110403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# building a custom model\nclass BertLayer(Layer):\n    def __init__(self, **kwargs):\n        super(BertLayer, self).__init__(**kwargs)\n        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)\n    \n    def call(self, inputs):\n        input_ids, attention_mask = inputs\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        return outputs.last_hidden_state\n\ndef create_keras_model():\n    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')\n    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n\n    bert_output = BertLayer()([input_ids, attention_mask])\n    pooled_output = GlobalAveragePooling1D()(bert_output)\n    output = Dense(3, activation='softmax')(pooled_output)\n\n    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n    return model","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:31:03.112778Z","iopub.execute_input":"2024-08-04T04:31:03.113201Z","iopub.status.idle":"2024-08-04T04:31:03.121011Z","shell.execute_reply.started":"2024-08-04T04:31:03.113168Z","shell.execute_reply":"2024-08-04T04:31:03.120034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = create_keras_model()\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    model.fit(train_tf_dataset, epochs=3, callbacks=[TqdmCallback(verbose=1)])","metadata":{"execution":{"iopub.status.busy":"2024-08-04T04:31:03.122224Z","iopub.execute_input":"2024-08-04T04:31:03.122591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# getting prediction\npredictions = model.predict(test_tf_dataset)\n\n# check lengths\nprint(f\"Length of test_ids: {len(test_ids)}\")\nprint(f\"Shape of predictions: {predictions.shape}\")\nif len(test_ids) != predictions.shape[0]:\n    predictions = predictions[:len(test_ids)]\n\n# creating DataFrame\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'winner_model_a': predictions[:, 0],\n    'winner_model_b': predictions[:, 1],\n    'winner_model_tie': predictions[:, 2]\n})\n\n# saving DataFrame\nsubmission.to_csv('submission.csv', index=False)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}