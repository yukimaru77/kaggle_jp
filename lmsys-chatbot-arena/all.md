# All Discussions

* --- discussion numver 0, the number of votes :151 ---

# Starter Notebook 34B LLM wow! - Nvidia Wins KDD Cup 2024 wow!

**Chris Deotte** *Sat Jul 20 2024 10:19:17 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (151 votes)

Hi everyone. I would like to share a starter notebook showing how to infer a 34B LLM to infer 25k test samples under 9 hours [here](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm). Since there are only 2.5 weeks left, I won't share too much, but I'll share a few ideas.

# Use Quantization 4bit AWQ

In this competition we must infer on 2xT4 GPU 16GB VRAM. Therefore we have a total of 32GB VRAM. A 34B LLM in fp16 is 70GB size however when using 4bit it becomes 20GB size (i.e. X billion parameter models become 0.6X GB size in 4bit). Reducing a 34B LLM to 20GB will fit in 32GB VRAM Hooray!

# Infer Using vLLM

The test data is 25k samples therefore we must infer fast. We need to infer each sample on average 1.3 seconds! The library vLLM [here](https://docs.vllm.ai/en/latest/) is faster than Hugging Face and pure PyTorch code! It will easily infer 1 second per sample or less!

# Starter Notebook

I publish a starter notebook [here](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm) demonstrating how to infer 34B LLM in Kaggle's LMSYS competition to complete 25k test predictions in under 9 hours! My starter notebook illustrates a few tricks:

- How to install vLLM in Kaggle notebooks

- How to use logits processor to force our model to only output "A", "B", "tie"

- How to extract probabilities from these 3 tokens

- How to optimize input token length and output token length

- How to format LLM prompts

# KDD Cup 2024

This year I had the opportunity to team up with 5 NVIDIAN coworkers in KDD Cup 2024 [@aerdem4](https://www.kaggle.com/aerdem4) [@titericz](https://www.kaggle.com/titericz) [@sorokin](https://www.kaggle.com/sorokin) [@simjeg](https://www.kaggle.com/simjeg) [@benediktschifferer](https://www.kaggle.com/benediktschifferer) . The challenge was to build an LLM to answer 11k ecommerce questions under 2 hours using 4xT4 for inference. Competition page [here](https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms). I learned so much from my fellow NVIDIA Kaggle Grandmasters (KGMON [here](https://www.nvidia.com/en-us/ai-data-science/kaggle-grandmasters/)), Thank you! Our collective ideas earned us 1st place on all 5 tracks of KDD Cup 2024 wow! (LB [here](https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms/leaderboards))

# Solution

Our solution involved these 3 key components:

- Generate lots of train data

- Finetune largest model possible with QLoRA

- Infer as fast as possible on limited hardward within time constraint

At the end of August we will present our KDD Cup solution live in Barcelona, Spain at KDD Cup 2024 conference, and we will publish a paper. Stay tuned to hear all the details!



---

 # Comments from other users

> ## Anish Vijay
> 
> excellent!
> 
> 
> 


---

> ## hwz13
> 
> å¾ˆæ¸…æ™°ï¼very clear
> 
> 
> 


---

> ## kaggk
> 
> Brilliantï¼
> 
> 
> 


---

> ## Timmy Juicehouse
> 
> Glad to see you here Chris, and congratulation to your Nvidia team. We team also won a good rank but you are much stronger than us. I need to prepare for my wedding next month and cannot leave for Barcelona. I'm looking forward to your solution in Kdd2024. 
> 
> 
> 


---

> ## Metin Meki Abullrahman
> 
> N!ce Work ðŸ˜
> 
> 
> 


---

> ## Cindy Y
> 
> Very good! 
> 
> 
> 


---

> ## Kid Liu
> 
> Nice work!
> 
> 
> 


---

> ## Sanket Pramod Bhure
> 
> wow Great work!
> 
> 
> 


---

> ## Liuyanfen166
> 
> Nice work!
> 
> 
> 


---

> ## Rise_Hand
> 
> Great job always! I have a small question that according to my view, different models have different tricks which based on their structure and hyper-parameters. So what should we do to make less cost from past experience, namely how should we trade off between the cost and best results from new different models based on our past experience?
> 
> 
> 


---

> ## YingxiZhang
> 
> wow~~~nice work
> 
> 
> 


---

> ## Cody_Null
> 
> I see you talk about generating data. Is that done with something like this/ do you have like a go to template? 
> 
> ```
> %pip install -q -U ipywidgets
> %pip install -q -U transformers
> %pip install -q -U tokenizers
> %pip install -q -U bitsandbytes
> %pip install -q -U torch
> 
> import pandas as pd
> from tqdm import tqdm
> import torch
> from huggingface_hub import login
> from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
> import random
> 
> # Define base prompts and parameters
> base_prompts = [
>     "Generate a single response that would likely merit a reply (Score: 8 to 10) and provide a score from 1 to 10 indicating how probable it is that the response would merit a reply. Examples: 'How can I help you today?' - 10, 'What's your name?' - 10, 'Tell me more about that.' - 9. Format: 'response' - score.",
>     "Generate a single response that might merit a reply (Score: 6 to 7) and provide a score from 1 to 10 indicating how probable it is that the response would merit a reply. Examples: 'Hello there!' - 6, 'Good morning.' - 7. Format: 'response' - score.",
>     "Generate a single response that would likely not merit a reply (Score: 4 to 5) and provide a score from 1 to 10 indicating how probable it is that the response would merit a reply. Examples: 'I don't know.' - 5, 'Maybe you are right, I can look into that.' - 4. Format: 'response' - score.",
>     "Generate a single response with a low probability of meriting a reply (Score: 1 to 3) and provide a score from 1 to 10 indicating how probable it is that the response would merit a reply. Examples: 'Yes.' - 3, 'No.' - 2, 'Umm' - 1. Format: 'response' - score."
> ]
> num_convos = 10000
> batch_size = 128  # Adjust based on your GPU memory
> max_length = 512  # Adjust based on desired response length
> 
> # Model and tokenizer configurations
> model_name = "google/gemma-2-9b"   #"mistralai/Mistral-7B-Instruct-v0.3"  #"meta-llama/Meta-Llama-3-8B-Instruct"
> bnb_config = BitsAndBytesConfig(
>     load_in_4bit=True,
>     bnb_4bit_compute_dtype=torch.float16
> )
> 
> # Log in to Hugging Face
> login(token="hf_JB")
> 
> # Load model and tokenizer
> model = AutoModelForCausalLM.from_pretrained(
>     model_name,
>     quantization_config=bnb_config,
>     device_map="auto",
>     token="hf_JB"
> )
> tokenizer = AutoTokenizer.from_pretrained(model_name, token="hf_JB")
> 
> # Set the pad_token to eos_token
> tokenizer.pad_token = tokenizer.eos_token
> 
> # Set up the device and check GPU availability
> torch.backends.cuda.enable_mem_efficient_sdp(False)
> torch.backends.cuda.enable_flash_sdp(False)
> if not torch.cuda.is_available():
>     raise EnvironmentError("Sorry - GPU required!")
> DEVICE = torch.device("cuda")
> 
> # Specify data extraction
> def extract_data(outputs, base_prompt):
>     convos = []
>     labels = []
>     for output in outputs:
>         response = tokenizer.decode(output, skip_special_tokens=True).strip()
> 
>         # Remove the base prompt from the response
>         if response.startswith(base_prompt):
>             response = response[len(base_prompt):].strip()
> 
>         # Extract response and score
>         if " - " not in response:
>             print('no - in output')
>             continue
> 
>         try:
>             response_text = response.split(' - ')[0].strip().strip('"')
>             score = int(response.split(' - ')[1][:2].strip())
> 
>             # Ensure score is within the expected range
>             if score < 1 or score > 10:
>                 print('invalid score')
>                 continue
> 
>         except (IndexError, ValueError):
>             print('broken output')
>             continue
> 
>         convos.append(response_text)
>         labels.append(score)
> 
>     return convos, labels
> 
> # Generate conversation responses in batches
> convos_df_lst = []
> labels_df_lst = []
> 
> for i in tqdm(range(0, num_convos, batch_size)):
>     # Randomly select a base prompt to encourage diverse responses
>     base_prompt = random.choice(base_prompts)
>     batch_prompts = [base_prompt] * batch_size
>     encoding = tokenizer(batch_prompts, return_tensors="pt", padding=True, truncation=True, max_length=max_length)
>     input_ids = encoding["input_ids"].to(DEVICE)
>     attention_mask = encoding["attention_mask"].to(DEVICE)
> 
>     outputs = model.generate(input_ids, attention_mask=attention_mask, max_length=max_length, pad_token_id=tokenizer.eos_token_id, do_sample=True, temperature=0.7)
>     convos_batch, labels_batch = extract_data(outputs, base_prompt)
> 
>     convos_df_lst.extend(convos_batch)
>     labels_df_lst.extend(labels_batch)
> 
> # Filter out duplicate responses
> unique_responses = list(set(zip(convos_df_lst, labels_df_lst)))
> convos_df_lst, labels_df_lst = zip(*unique_responses) if unique_responses else ([], [])
> 
> # Save to CSV
> df = pd.DataFrame({'convo': convos_df_lst, 'label': labels_df_lst})
> df['label'] = df['label'] >= 7
> display(df)
> output_path = "/mnt/batch/tasks/shared/LS_root/mounts/clusters/cn1/code/Users/CN/Output/dataset.csv"
> df.to_csv(output_path, index=False)
> 
> print(f"Dataset generated and saved to {output_path}")
> 
> ```
> 
> 
> 


---

> ## Waqar Ali
> 
> Congratulations on your upcoming presentation in Barcelona! I'm also thrilled to share that we won all the tracks in the OAG challenge. Despite your few submissions, your top-ranking performance in this competition is truly impressive. I have great admiration for your achievements. ðŸŽ
> 
> 
> 


---

> ## WoNiu666
> 
> wow~~~great work
> 
> 
> 


---

> ## Yixiao Yuan
> 
> Thank you for sharing. I have a question about quantization in training and inference. If we use int4 for inference, is it better to use QLoRA instead of LoRA during training? This might align the training and inference conditions better, but I'm concerned QLoRA may not perform as well as LoRA. What are the trade-offs here?
> 
> 
> 


---

> ## sayoulala
> 
> Congratulations! Looking forward to your presentation in Barcelona. We also won all the tracks in another competition.ï¼ˆ OAG-challengeï¼‰.
> 
> Seeing that you submitted very few entries in this competition but still ranked among the top, I admire you greatly.
> 
> 
> 
> > ## Chris DeotteTopic Author
> > 
> > Thanks [@sayoulala](https://www.kaggle.com/sayoulala) Congratulations on your great performance here!
> > 
> > I joined this competition 1 week ago and I'm trying out insights that I learned from KDD Cup 2024. So far the ideas transfer well and are helping me do well here too!
> > 
> > 
> > 
> > > ## sayoulala
> > > 
> > > Thank you. I would like to ask if using vllm will result in a loss of accuracy?
> > > 
> > > 
> > > 
> > > ## sayoulala
> > > 
> > > By the way, my friend [@chizhu2018](https://www.kaggle.com/chizhu2018) is a huge fan of yours. Could I get your autograph for him if you're going to the KDD in Barcelona?
> > > 
> > > 
> > > 
> > ## yechenzhi1
> > 
> > may I ask if you are using models larger than 9B in the lmsys competition?
> > 
> > 
> > 
> > > ## sayoulala
> > > 
> > > Sorry, I won't disclose it before the competition ends.
> > > 
> > > 
> > > 
> > > ## yechenzhi1
> > > 
> > > That's totally okay! I'll try larger models by myselfðŸ˜ 
> > > 
> > > 
> > > 
> > > ## Ilia Zaitsev
> > > 
> > > So far, it looks like the bigger the model, the lower the loss isâ€¦
> > > 
> > > 
> > > 


---

> ## HinePo
> 
> Good to see you here, [@cdeotte](https://www.kaggle.com/cdeotte), I'm expecting to learn a lot from you again once the competition ends.
> 
> Do you know if vLLM can be used for Sequence Classification rather than Generation?
> 
> I've searched a bit but couldn't find anything.
> 
> 
> 
> > ## Chris DeotteTopic Author
> > 
> > I do not think we can use vLLM for Sequence Classification. So we need to use vLLM with my notebook or with the Llama3-8B starter notebook [here](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference) which uses text generation.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> Why does this feel like a winning solution post to this challenge, just 2 weeks to early? Congratz on your KDD Cup performance, looking forward to see another of these being posted from you, tailored to this challenge ðŸ˜‰
> 
> EDIT: Also - now we have to train 34B models, I guess?
> 
> 
> 
> > ## yechenzhi1
> > 
> > 
> > now we have to train 34B models, I guess?
> > 
> > I think soðŸ˜‚ just not sure if the leading solutions are currently utilizing larger models.
> > 
> > 
> > 
> > > ## hn
> > > 
> > > I guess some areâ€¦my current LB score is actually based on small LMs so far.
> > > 
> > > 
> > > 


---

> ## Rishan Hasan Tenis
> 
> Congratulation, Thank You for sharing! [@cdeotte](https://www.kaggle.com/cdeotte) 
> 
> 
> 


---

> ## S J Moudry
> 
> Can AWQ be used with qlora?  From what I see SequenceClassification model types are not supported by AutoAWQ/PEFT yet.  Would you be performing full fine tuning and then converting to AWQ?
> 
> 
> 
> > ## Chris DeotteTopic Author
> > 
> > We QLoRA finetune with any type of 4bit quantization. After finetuning, we merge the LoRA adapter, and quantize with AWQ (to prepare for inference).
> > 
> > 
> > 


---

> ## dexterxin
> 
> Congratulation on your KDD Cup! 
> 
> It looks like that it's possible to finetune and infer 9B+ pretrained model on limited hardward within time constraint. It's a good news for participants without sufficient computing resources.
> 
> Thanks so much and looking forward to more of your ideas!
> 
> 
> 


---

> ## Ched Martin
> 
> Congratulations!
> 
> 
> 


---

> ## superferg
> 
> May I ask if you have tried gemma2 27B?
> 
> 
> 
> > ## Yichuan Gao
> > 
> > Sadly AutoAWQ does not support Gemma-2 yet, I also wanted to try gemma-2 27b :(
> > 
> > 
> > 


---

> ## yechenzhi1
> 
> Impressive! I thought  9b is the largest model we can use in this competition. 
> 
> 
> 
> > ## Chris DeotteTopic Author
> > 
> > No. It is good to know that we can at least infer 34B quickly on Kaggle's 2xT4 GPU. Therefore we can use 34B in all Kaggle competitions. It has also been shown [here](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/440620), that we can use 70B on Kaggle but the customization makes it slower than using vLLM and 34B. (And that notebook is too slow as is for Kaggle's LMSYS comp).
> > 
> > 
> > 


---

> ## Harshit Sharma
> 
> Congratulations [@cdeotte](https://www.kaggle.com/cdeotte) on the impressive win and for sharing these invaluable insights! Your innovative approach and efficient use of resources are truly inspiring. Can't wait to see your presentation on KDD Cup solution in Barcelona! ðŸŒŸðŸš€
> 
> 
> 


---



* --- discussion numver 1, the number of votes :96 ---

# External data - additional 157k human preference ratings ðŸ”¥ðŸ”¥ðŸ”¥

**Darek KÅ‚eczek** *Fri May 03 2024 07:09:30 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (96 votes)

I'm super excited to see if smaller models (Kaggle GPU compatible) can effectively rate responses from much larger LLMs. To help you improve your models, I published a dataset with external data:

[https://www.kaggle.com/datasets/thedrcat/llm-human-preference-data-ultrafeedback/data](https://www.kaggle.com/datasets/thedrcat/llm-human-preference-data-ultrafeedback/data)

This is based on Ultrafeedback dataset published on HF by Argilla. I additionally converted it into the competition train data format. 

EDIT: Note that Ultrafeedback uses GPT4 as a judge as a proxy for human raters. I also added ties between models in version 2 that were previously filtered out. See original dataset paper [here](https://arxiv.org/pdf/2310.01377). Thanks [@nbroad](https://www.kaggle.com/nbroad) for catching this. 

Enjoy â¤ï¸ðŸ™ðŸ‘



---

 # Comments from other users

> ## Dlond Mike
> 
> no useâ€¦but just for my notebook
> 
> 
> 


---

> ## Rich Olson
> 
> I added 50k of the items from this to my "Deberta + TF-IDF + Word2Vec + Length" notebook (it's public - I'd post a link - but Kaggle thinks I'm spamming).
> 
> got an identical 1.011 on the LB.  Had same experience with [this dataset](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500973).
> 
> I take this as indication the data is probably good (or at least not bad) - it's just my notebook isn't able to benefit from the extra data.
> 
> 
> 


---

> ## Nicholas Broad
> 
> How do you think the tie should be handled? If the scores are equal, it should be a tie? (Your dataset only has "model a" winning)
> 
> Update:
> 
> I read more into how they processed the dataset and I noticed a few things:
> 
> I don't think this is human preferences. I think this is using [GPT-4 to rate the responses](https://github.com/OpenBMB/UltraFeedback/tree/main?tab=readme-ov-file#introduction)
> [Ultrafeedback intentionally filters out ties](https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences/blob/main/README.md#dataset-processing), whereas the LMSYS dataset has roughly even split between model a winning, b winning, and ties.  
> 
> 
> > ## Darek KÅ‚eczekTopic Author
> > 
> > Great catch, thanks! I'll update the thread. I think this can be still useful for pretraining or pseudolabeling. Also found the paper here: [https://arxiv.org/pdf/2310.01377](https://arxiv.org/pdf/2310.01377)
> > 
> > 
> > 
> > > ## Darek KÅ‚eczekTopic Author
> > > 
> > > I'll see if I can reproduce the binarization while keeping ties too. 
> > > 
> > > 
> > > 
> > > ## Darek KÅ‚eczekTopic Author
> > > 
> > > Version 2 of the [dataset](https://www.kaggle.com/datasets/thedrcat/llm-human-preference-data-ultrafeedback) has ties between models added now. 
> > > 
> > > 
> > > 
> > > ## Turbo
> > > 
> > > Intersting dataset.
> > > 
> > > Did you use this dataset and boost your score?
> > > 
> > > 
> > > 


---

> ## eli plutchok
> 
> Wouldn't you expect gpt-4 rankings to be very different than human rankings? 
> 
> 
> 
> > ## Darek KÅ‚eczekTopic Author
> > 
> > There's research pointing that GPT-4 correlates well with human ratings, for example [here](https://arxiv.org/pdf/2306.05685):
> > 
> > The agreement [â€¦] between GPT-4 and humans reaches 85%, which is even higher than the agreement among humans (81%). This means GPT-4â€™s judgments closely align with the majority of humans.
> > 
> > 
> > 
> > > ## eli plutchok
> > > 
> > > Wow. Have you tested GPT4 on the training examples to see how well it scores?
> > > 
> > > 
> > > 


---

> ## justin1357
> 
> Is any data from kaggle covered in this dataset?
> 
> 
> 


---



* --- discussion numver 2, the number of votes :75 ---

# Additional 21k Labelled Conversations ðŸš€

**Abdullah Meda** *Wed May 08 2024 01:17:13 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (75 votes)

This dataset was from the authors themselves at [https://huggingface.co/datasets/lmsys/chatbot_arena_conversations](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations)

The format was quite different to the one being used here for the competition. I have processed it to be in a similar format. Here they are:

- Dataset: [kaggle.com/datasets/abdullahmeda/lmsys-additional-33k-labelled-conversations](https://www.kaggle.com/datasets/abdullahmeda/lmsys-additional-33k-labelled-conversations)

- Processing Script: [kaggle.com/code/abdullahmeda/33k-lmsys-chatbot-arena-conversations/](https://www.kaggle.com/code/abdullahmeda/33k-lmsys-chatbot-arena-conversations/)

An upvote on the [dataset](https://www.kaggle.com/datasets/abdullahmeda/lmsys-additional-33k-labelled-conversations) would be greatly appreciated. Thank you! ðŸ™

I'll check if there are any duplicates between the datasets if I do get the time tomorrow. Happy coding!

UPDATE: Using the prompts column as deduplication criteria brings the sample count to around 21k. The dataset and script have been updated.



---

 # Comments from other users

> ## eli plutchok
> 
> Hi [@abdullahmeda](https://www.kaggle.com/abdullahmeda) , I tested adding in this training data, and for some reason  it makes the submission score a lot worse.
> 
> 
> 


---

> ## eli plutchok
> 
> Hey, I just realized that a lot of the lines are already on the main dataset. Maybe you can make a new cleaned version of this removing all the duplicates. I'm not sure about the percentage.
> 
> 
> 
> > ## eli plutchok
> > 
> > I think about a third are duplicates from the main training set, but there are also many duplicates within the data set, and I think there are additional ones that are very similar but not exact duplicates.
> > 
> > 
> > 
> > > ## Abdullah MedaTopic Author
> > > 
> > > [@eliplutchok](https://www.kaggle.com/eliplutchok) You may be right when you say that. I have dropped all rows that have similar prompts for now. More columns can be used as a subset when dropping rows, but I have noticed that lesser rows were dropped when using multiple columns. Using just the prompts as deduplication criteria brings the number down to just 21k new samples. I have updated the script as well as the dataset to reflect this. I'll update the post in a bit
> > > 
> > > ```
> > > superset = pd.concat([external_data, train]).reset_index(drop=True)
> > > external_data_deduplicated = superset.drop_duplicates(subset=['prompt'], keep='last')
> > > external_data_deduplicated = external_data_deduplicated[external_data_deduplicated.index.isin(external_data.index)]
> > > 
> > > len(external_data_deduplicated)
> > > >>> 21187
> > > 
> > > ```
> > > 
> > > 
> > > 
> > > ## eli plutchok
> > > 
> > > Btw, I realized another thing. It seems that the lines that had "tie (both bad)" as the winner, you just left blank, but these should all be counted as ties, or else you are left with only 10% ties u unlike the main dataset which has 30% ties.
> > > 
> > > 
> > > 
> > > ## Abdullah MedaTopic Author
> > > 
> > > [@eliplutchok](https://www.kaggle.com/eliplutchok) Thank you for pointing this out. I have made the respective changes!
> > > 
> > > ```
> > > >>> external_data[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1).all()
> > > True
> > > 
> > > ```
> > > 
> > > 
> > > 


---

> ## Rich Olson
> 
> I just submitted a version of my 1.011 LB notebook which adds the de-duped version to train:
> 
> [https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec-length](https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec-length)
> 
> I'll post what I find out.
> 
> 
> 
> > ## Rich Olson
> > 
> > I got an identical 1.011 on the LB (see version 6 of above notebook).
> > 
> > Same results with using 50k items from the "[ultrafeedback](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756)" dataset.
> > 
> > I take this as indication the data is probably good (or at least not bad) - it's just my notebook isn't able to benefit from the extra data.
> > 
> > 
> > 
> > ## Ivan Vybornov
> > 
> > A bulk of data comes from the models that are not present in the actual train set, I doubt this will complement tf-idf approach.
> > 
> > 
> > 


---

> ## xiaotingting
> 
> After adding this dataset, the results are significantly better. The use of additional datasets is indeed useful.
> 
> 
> 
> > ## Erik
> > 
> > Hi, both cv and lb at the same time improved?
> > 
> > 
> > 
> > > ## KeShuang Liu
> > > 
> > > Why did I perform better on cv but worse on lb after using the dataset?
> > > 
> > > 
> > > 


---

> ## eli plutchok
> 
> Have you tried it yet with a submission? I'm scared that taking any external data may unintentionally make my model's predictions on the test data worse.
> 
> 
> 
> > ## Sparsh Tewatia
> > 
> > Bro if you try please update the results here too
> > 
> > 
> > 
> > > ## eli plutchok
> > > 
> > > k, will let you know, I hope to try this tomorrow (for me - I'm in NY).
> > > 
> > > 
> > > 
> > > ## go
> > > 
> > > before add data cv is 1.01
> > > 
> > > after add data cv is 1.03â€¦
> > > 
> > > but I haven't submit this version
> > > 
> > > 
> > > 


---



* --- discussion numver 3, the number of votes :45 ---

# The magic behind Gemma2

**Yixiao Yuan** *Thu Jul 25 2024 05:32:05 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (45 votes)

According to other benchmarks, llama3.1 8B should be better than Gemma2. But from our experiments and other discussions, in this competiton, gemma2 is better. We found a possible reason in Gemma2's tech report. Gemma2 is pretrained on LMSYS. ðŸ¤£



---

 # Comments from other users

> ## Cody_Null
> 
> LMSYS-Chat-1M?? Does that mean there is a dataset of 1M for LMSYS?
> 
> 
> 
> > ## Kishan Vavdara
> > 
> > Yes, but it only contains conversation. [Here](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499800)  the earlier discussion. 
> > 
> > 
> > 
> > ## Robson
> > 
> > I found this paper: [https://arxiv.org/pdf/2309.11998](https://arxiv.org/pdf/2309.11998)
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> They trained on it - so you don't have to. 
> 
> It is interesting that they only use prompts, not responses, so the use case is very different from ours. I do not see a large benefit from it, but maybe somebody else can explain to me why this could help?
> 
> At the end of the day, many prompts in the lmsys dataset are VERY bad.
> 
> 
> 
> > ## Sparsh Tewatia
> > 
> > It is good with diffrerentiating between a good and bad answers for these type of prompts but don't know when it should be a tie.
> > 
> > 
> > 


---

> ## yechenzhi1
> 
> Hi, can you share the link to Gemma2's tech report?
> 
> 
> 
> > ## Yixiao YuanTopic Author
> > 
> > [https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf](https://storage.googleapis.com/deepmind-media/gemma/gemma-2-report.pdf)
> > 
> > 
> > 


---

> ## yuanzhe zhou
> 
> it seems that there are many similar open source dataset?
> 
> 
> 


---



* --- discussion numver 4, the number of votes :44 ---

# Suspicious Surge of Novice Accounts on the Leaderboard

**Sercan YeÅŸilÃ¶z** *Thu Aug 01 2024 07:46:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (44 votes)

I was at 45-50th place 2-3 days ago, and today I see that there are too many novice accounts on the top of the leaderboard. They merged up as a team and are getting similar scores on LB. When I check some of these accounts, I see that most were just signed up a few days ago. I don't want to blame anyone without concrete evidence but these movements in the LB seem suspicious given my previous experiences in the competitions.



---

 # Comments from other users

> ## Fangzhou Liu
> 
> But it doesn't seem to affect the top 100 positions in the charts.
> 
> 
> 
> > ## Victor Lu
> > 
> > Why are some people replying with the same comment? I saw this comment three times in this discussion. One has been deleted. ðŸ˜‚ 
> > 
> > 
> > 
> > > ## Cody_Null
> > > 
> > > Some new accounts with strange behavior it seems 
> > > 
> > > 
> > > 


---

> ## RUOFEI_Maggie
> 
> That's great.
> 
> 
> 


---

> ## Huang Jing Stark
> 
> I feel you, there was one day I wake up and found my place dropped 100+ðŸ˜…. I was thinking that did any expert share another great solution, but instead I found a lot of accounts which seems to registerd just recently and got same scores suddently came before me. Now I realize that some dirty things must happen. And I am still struggling to fight for the leaderboard. I will not surrender to those boosters. ðŸ˜¡
> 
> 
> 
> > ## Rise_Hand
> > 
> > Huwawei ! Beat the boosters! Beat the cheaters !!!
> > 
> > It looks like I am a joker compared to the team who easily get a silver medal by sharing privatelyðŸ¤¡
> > 
> > 
> > 


---

> ## CPMP
> 
> Finetuning decoder only models is a relatively new thing on Kaggle. It is not surprising to see newcomers doing well IMHO.
> 
> Each time there has been a paradigm shift in Kaggle competition, a new breed of top performers emerged. It happened with XGBoost, then LightGBM, then with CNNs, then with transformers, then with pretrained LLMs, and now with Lora and variants.
> 
> That being said I have no clue about the specifics of the people you target. If you have evidence of cheating then you should send a mail to compliance@kaggle.conm.
> 
> 
> 


---

> ## Victor Lu
> 
> Because some people are selling solutions to this competition on Xiaohongshu, there are cases where a mentor leads a team of a few people to participate in the competition and charges a fee. Especially in the days leading up to the competition team registration deadline, these types of posts appear more frequently. Each post has about a dozen or twenty people inquiring about the price. This issue has been discussed in previous posts: [https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366313](https://www.kaggle.com/competitions/open-problems-multimodal/discussion/366313)
> 
> 
> 
> > ## Valentin Werner
> > 
> > That's insane. How was this adressed in previous competitions? 
> > 
> > I am a bit torn on this:
> > 
> > - If a person is selling their solution by simply adding 4 team members and carrying them to gold, I think that is fine for me?
> > 
> > - If a person is selling their solution away to other teams, that would obviously be against the rules.
> > 
> > I am wondering if the the second case is happening too. When looking at the top 100, several teams with novice or contributor as highest rank have a similar submission pattern and very similar scores.
> > 
> > 
> > 
> > > ## Enter your display name
> > > 
> > > In fact, some people selling solutions come from the same educational institution or know each other. However, to earn more money, they form multiple teams. Therefore, it is almost impossible for them not to communicate with each other.
> > > 
> > > 
> > > 


---

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> In my opinion, it's not good to make claims about new accounts without proof (not by LB positions). At the end of every competition, the leaderboard often changes, indicating that Kaggle staff have identified and removed multi-accounts or internal sharing teams.
> 
> 
> 


---

> ## Nicholas Broad
> 
> if you have concrete evidence, report it, otherwise, just wait for kaggle to deal with it. After the comp ends, kaggle will remove cheaters
> 
> 
> 
> > ## Greenland chipper shark
> > 
> > [@nbroad](https://www.kaggle.com/nbroad) It's hard to prove without proof, but it's easy for those active and newly created numbers to suspect ðŸ¤¨, which is the sad story
> > 
> > 
> > 
> > ## Greenland chipper shark
> > 
> > [@nbroad](https://www.kaggle.com/nbroad) I'm checking it out now, nothing against anyone, just finding a flood of contributors and newbies pouring out like a river
> > 
> > 
> > 
> > ## Cody_Null
> > 
> > I have new found faith in Kaggles ability to handle these cheaters. In IMC 2024 they managed to clear 20 cheaters from the top 50. And many more from the rest of the leaderboard. I am confident the Kaggle staff will do everything they can to get it right!
> > 
> > 
> > 


---

> ## zsyyyy_y
> 
> But it doesnâ€™t seem to affect the top 100 positions in the charts
> 
> 
> 
> > ## Cody_Null
> > 
> > lol is that right?
> > 
> > 
> > 


---

> ## Vincy
> 
> It look like a â€¦.
> 
> 
> 


---

> ## Pmantony
> 
> Might be a sign of increased interest in machine learning 
> 
> 
> 


---



* --- discussion numver 5, the number of votes :42 ---

# ChatBot Arena Prompts Can Distinguish Models. The Bradley-Terry model. Elo Ratings on Kaggle.

**MarÃ­lia Prata** *Fri May 03 2024 14:27:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (42 votes)

# ChatBot Arena

Super cool Notebook Chatbot Arena MLE Elo Rating

[Chatbot Arena: MLE Elo Rating (Bradley-Terry model) Calculation (Apr 22, 2024)](#https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO)

"Maximum Likelihood Estimation for Elo Ratings (aka Bradley-Terry model). In the context of LLM evaluation, models can be assumed to be static. In this case, the authors could directly fit the ratings by maximum likelihood estimation method (aka Bradley-Terry model), which produce significantly stable ratings. Here they provided an implementation with logistic regression."

[https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=PbTdhkLQp113](https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=PbTdhkLQp113)

# Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference

Authors: Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios N. Angelopoulos, Tianle Li, Dacheng Li, Banghua Zhu, HaoZhang, Michael I. Jordan, Joseph E. Gonzalez, Ion Stoica.

"To assess the performance of LLMs, the research community has introduced a variety of benchmarks. These benchmarks can be categorized based on two factors: the source of questions (either static or live) and the evaluation metric (either ground truth or human preference). According to these fac tors, benchmarks can be classified into four categories. While a range of benchmarks is beneficial, the most prevalent current method for evaluating LLMs remains a static, ground-truth-based evaluation, partly because such evaluations are inexpensive and reproducible."

"However, these static, ground-truth-based benchmarks exhibit several limitations. Firstly, the questions within these benchmarks are not open-ended, hindering the ability to capture the flexible and interactive use found in real-world settings. Secondly, the test sets in these benchmarks are static, meaning they can become contaminated over time, which undermines the reliability of the evaluation results. Furthermore,for many complex tasks, establishing a definitive ground truth is not only challenging but sometimes unattainable."

" Consequently, current benchmarks fail to adequately address the needs of state-of-the-art LLMs, particularly in evaluating user preferences. Thus, there is an urgent necessity for an open, live evaluation platform based on human preference that can more accurately mirror real-world usage."

"Creating such a benchmark platform entails significant challenges. It requires the collection of live, fresh, and diverse user questions to accurately represent real-world scenarios."

CONTRIBUTIONS MADE by the AUTHORS:

"They built the first large-scale crowd-sourced live LLM evaluation platform with over 1M users visit."

"They conducted an in-depth analysis of the collected data, including prompt diversity, quality, vote quality, and insights on human feedback."

"They will publicly release a human preference dataset with over 100K pairwise votes collected from Chatbot Arena."

"They designed an efficient sampling algorithm that actively chooses which model pairs to show, such that our sample efficiency improves, sometimes to a large degree."

# Risks of Static Benchmarks.

"Static benchmarks have certain issues, including contamination, saturation, overfitting, and a lack of human alignment. DynaBench identifies these challenges and recommends the use of a live benchmark that incorporates a human-in-the-loop approach for classical NLP benchmarks. Their system adopts a similar spirit."

DATA STATISTICS 

"The authors began collecting data in April 2023. As of Jan 2024, they have received around 240K votes from over 90K users. Their data involves more than 50 models, including both proprietary models like GPT-4, Claude, and Gemini, as well as open models such as LLaMA and Mistral. These conversations cover more than 100 languages, with 77% being in English, 5% in Chinese, and the remaining languages, such as Russian, German, Spanish, French, and Japanese, each representing less than 2% of the total. Each data point includes multi-turn conversations between the user and two LLMs, and a vote to indicate which model the user prefers."

# Can Arena Prompts Distinguish Models?

"The authors studied how effective are these topic clusters in distinguishing models strengths. Their result shows models may exhibit varying strengths in different areas, but also highlights some of the topic clusters in Chatbot Arena are effective in differentiate models."

[https://arxiv.org/pdf/2403.04132](https://arxiv.org/pdf/2403.04132)

# Elo Ratings

Remember that it is "Elo", not "ELO" since it was named after Arpad Elo. The USCF implemented Arpad Elo's suggestions in 1960, and the system quickly gained recognition as being both fairer and more accurate than the Harkness rating system.

[Elo rating system](https://en.wikipedia.org/wiki/Elo_rating_system)

"The Elo rating system is a method for calculating the relative skill levels of players, which has been widely adopted in chess and other competitive games. The difference in the ratings between two players serves as a predictor of the outcome of a match. The Elo rating system works well for our case because we have multiple models and we run pairwise battles between them. In this section, we present different methods for calculating Elo ratings."

Compute Ratings

"The authors first use the online linear update algorithm to compute Elo ratings. They chose a small K-factor of 4 to make the Elo ratings more stable and less biased towards recent games."

[https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO](https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=B_PYA7oVyaHO)

# Elo Ratings on Kaggle

KAGGLE TOPICS:

[Intro to Rating Systems](https://www.kaggle.com/competitions/chess/discussion/92) By Jeff Sonas - 14 y ago

[Elo Ratings Shiny App for Euro Soccer Teams](https://www.kaggle.com/datasets/hugomathien/soccer/discussion/31154) By Kevin Pan - 8 y ago

[Elo RATING ALGORITHM](https://www.kaggle.com/discussions/getting-started/216048) By Kaushik Deb

KAGGLE NOTEBOOKS:

[Elo Ratings in Python](https://www.kaggle.com/code/kplauritzen/elo-ratings-in-python) By Kasper P. Lauritzen

[Simple Simulation & Elo Rating Approach](https://www.kaggle.com/code/kenjee/simple-simulation-elo-rating-approach) By Ken Jee

[Custom Football Elo Rating](https://www.kaggle.com/code/thomasstokes/custom-football-elo-rating) By Thomas Stokes

There are many more on Kaggle. I've just picked a few to exemplify. Therefore, search by Code or Discussion if you intend to read more material.





* --- discussion numver 6, the number of votes :40 ---

# LMSYS Dataset Explorer â€“ Cluster, segment, inspect

**Emmanuel Turlay** *Thu May 30 2024 02:56:10 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (40 votes)

Hey folks â€“ Today, weâ€™re releasing a free dataset explorer for the LMSYS challenge.

With our tool, you can browse through individual rows and inspect their content in great detail. We generated semantic clusters, and you can filter conversations by cluster, model pairs, winner model, and so on.

The tool is free and no login is required. Be sure to let the community know what you discover.

Click this link to start exploring: [https://app.airtrain.ai/dataset/bb589c26-8f45-4f42-8ba3-07ef51b73a03/null/1/0](https://app.airtrain.ai/dataset/bb589c26-8f45-4f42-8ba3-07ef51b73a03/null/1/0)



---

 # Comments from other users

> ## Lisa DunlapCompetition Host
> 
> OMG THIS IS SO COOL!!!
> 
> 
> 


---

> ## Cviko Dukanovic
> 
> Amazing product. The meta clusters and clusters are greatly produced!
> 
> 
> 


---

> ## Josh Bauer
> 
> One fun cluster I've found: plenty of variations on the "how much wood could a woodchuck chuck if a woodchuck could chuck wood" tongue twister. Ex: "If, IF, a woodchuck could chuck wood, how much wood could a woodchuck chuck before it lost its job as a project manager?" ðŸ˜†
> 
> Under "Riddles and Reasoning" > "Woodchuck Tongue Twister", row 4308
> 
> 
> 
> > ## Idriss Chebak
> > 
> > one cool prompt i found was "Construct a rap battle, in the style of Epic Rap Battles of History, with rhyme and meter and personal attacks relevant to each character, between Pikachu and Zeus. Each participant gets 2 verses. There is no announcer and Zeus goes first." ðŸ˜„
> > 
> > 
> > 


---

> ## Felipe Maia Polo
> 
> Thanks for the amazing work! Is there a way to download the cluster assignments? Thanks!
> 
> 
> 
> > ## Josh Bauer
> > 
> > Glad you like it! You can't export the clusters right now, but it's on our TODO list.
> > 
> > 
> > 
> > ## Emmanuel TurlayTopic Author
> > 
> > We just shipped an update that includes clusters and token counts in the exported file. Have fun!
> > 
> > 
> > 
> > > ## Felipe Maia Polo
> > > 
> > > thank you!
> > > 
> > > 
> > > 


---

> ## Valentin Werner
> 
> The tool looks amazing!
> 
> As you are the the CEO of the company behind the tool, did you check the licenses? I am not sure if you are allowed to publish data from the competition on your platform, given the cc-by-nc license. I know that there was the recommendation not to do such in previous conpetitions.
> 
> Maybe you can instead show us how to use your tool to create such a dashboard instead?
> 
> All the best for your company and product!
> 
> (Also quite the PR stunt to have your employees comment on your thread (Josh & Idriss) ðŸ˜‰)
> 
> 
> 
> > ## Emmanuel TurlayTopic Author
> > 
> > Hi Valentin, thanks for the compliment!
> > 
> > We did check the [license](https://creativecommons.org/licenses/by-nc/4.0/) and it says we are allowed to share, copy, and redistribute the material in any medium or format; adapt, remix, transform, and build upon the material; as long as we attribute credit and offer it under non-commercial terms.
> > 
> > The Dataset Explorer is free and requires no sign-up. The Kaggle page is linked to from the description for attribution.
> > 
> > We certainly want to remain compliant and are offering this work to support the community. Thanks for making us double-check the license ðŸ˜‰
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Great stuff!
> > > 
> > > 
> > > 


---

> ## YingxiZhang
> 
> The tool is amazing!ðŸ‘
> 
> 
> 


---



* --- discussion numver 7, the number of votes :39 ---

# [Insights] on 165k Dataset and Model Performance

**justin1357** *Thu Aug 01 2024 20:21:38 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (39 votes)

In the past two days, I attempted to integrate the 165k dataset proposed in the discussion area three months ago into the training pipeline. However, this attempt failed on the leaderboard. Therefore, I carefully reviewed the information of this dataset.

Deduplication: I performed deduplication on the 165k dataset and the 55k Kaggle dataset at the prompt and response level. There was no significant duplication found, indicating that the performance decline was not due to overfitting on duplicate data.

Data Source Analysis: After ruling out technical issues, I examined the source of the data. Many people know that this dataset comes from UltraFeedback. However, what most people do not know is that the prompts are not human-generated but sourced from various evaluation datasets such as UltraChat, ShareGPT, Evol-Instruct, TruthfulQA, FalseQA, and FLAN. I believe this is the main reason for the performance dropâ€”the distribution of human prompts from Chatbot Arena differs significantly from the prompts in professional evaluation datasets, which misleads the model.

Model Performance Explanation: This also explains why LLama-3.1, despite performing exceptionally well on various evaluation datasets, did not perform well in this competition. LLama-3.1 uses DPO (Direct Preference Optimization), which essentially "overfits" the professional evaluation prompts. However, these differ from the prompt distribution collected by ChatBot Arena, leading to different standards for evaluating responses.

Successful Models Analysis: This also explains why models like Gemma-2 and LLama-3 performed well in this competition. They share a common feature in the post-training phase: using RLHF (Reinforcement Learning from Human Feedback). Although this method is more costly compared to automatic learning methods like DPO, it aligns more closely with the real human prompt distribution, allowing the model to better understand real human prompts.

Testing Results: I also tested several Gemma-2 models fine-tuned with DPO or SimPO on the UltraFeedback dataset, and their performance was unsatisfactory. This indirectly reflects the issues with DPO-like methods.

I hope these insights are helpful to you. If you found this response useful, I would greatly appreciate your vote.



---

 # Comments from other users

> ## xiaotingting
> 
> Yes, I feel that the processing of data in this competition may be an important point for the increase in the later stage.
> 
> 
> 


---



* --- discussion numver 8, the number of votes :34 ---

# Ranking of models estimated from train data.

**takaito** *Fri May 03 2024 16:08:51 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (34 votes)

I overlooked the fact that there is no model name in the test data.

I have estimated which model to choose from train data.

Since I have estimated for all model pair combinations, I'll share it. [notebook](https://www.kaggle.com/code/takaito/lmsys-model-name-catboostclassifier/notebook)

gpt4 tends to be selected more often.

If the model can be predicted 100% from the output, it is possible to score around 1.04 based on the name of the model alone. (This is an expectation based on the CV score at the time of the training.)

['gpt-4-1106-preview', 'gpt-4-0314', 'gpt-4-0125-preview', 'claude-1', 'gpt-4-0613', 'gpt-3.5-turbo-0314', 'claude-instant-1', 'gpt-3.5-turbo-0613', 'claude-2.0', 'claude-2.1', 'mistral-medium', 'vicuna-33b', 'llama-2-70b-chat', 'mixtral-8x7b-instruct-v0.1', 'wizardlm-70b', 'vicuna-13b', 'yi-34b-chat', 'qwen1.5-72b-chat', 'wizardlm-13b', 'starling-lm-7b-alpha', 'guanaco-33b', 'mpt-30b-chat', 'llama-2-13b-chat', 'gemini-pro-dev-api', 'koala-13b', 'gpt-3.5-turbo-1106', 'gemini-pro', 'zephyr-7b-beta', 'tulu-2-dpo-70b', 'gpt-3.5-turbo-0125', 'palm-2', 'pplx-70b-online', 'vicuna-7b', 'openchat-3.5', 'llama-2-7b-chat', 'openhermes-2.5-mistral-7b', 'nous-hermes-2-mixtral-8x7b-dpo', 'solar-10.7b-instruct-v1.0', 'zephyr-7b-alpha', 'codellama-34b-instruct', 'llama2-70b-steerlm-chat', 'dolphin-2.2.1-mistral-7b', 'openchat-3.5-0106', 'falcon-180b-chat', 'mistral-7b-instruct-v0.2', 'qwen1.5-7b-chat', 'deepseek-llm-67b-chat', 'gpt4all-13b-snoozy', 'pplx-7b-online', 'stripedhyena-nous-7b', 'mpt-7b-chat', 'mistral-7b-instruct', 'qwen-14b-chat', 'alpaca-13b', 'RWKV-4-Raven-14B', 'qwen1.5-4b-chat', 'oasst-pythia-12b', 'chatglm-6b', 'fastchat-t5-3b', 'stablelm-tuned-alpha-7b', 'chatglm3-6b', 'llama-13b', 'dolly-v2-12b', 'chatglm2-6b']

I hope this will be of some help!!



---

 # Comments from other users

> ## Fritz Cremer
> 
> This is interesting, since some models like Gemini, annoyingly often starts answers with "Absolutely! â€¦" or "You're right! â€¦". This should make predicting the model quite easy.
> 
> 
> 
> > ## takaitoTopic Author
> > 
> > Thanks for your comment.
> > 
> > As you commented, each model has its own output characteristics.
> > 
> > I think it is possible to predict rough models.
> > 
> > The predicted model name could be used for the features.
> > 
> > 
> > 
> > > ## JunHua Liao
> > > 
> > > Good idea. But I don't know if there is a significant difference between the distribution of model types in the training set and the test set, as there is no model type given for the test dataset.
> > > 
> > > 
> > > 


---

> ## Kishan Vavdara
> 
> 
> 
> Yes! gpt4 wins more often! 
> 
> 
> 


---

> ## Heroseo
> 
> oh, it is really instresting. 
> 
> It's a different story, but it would be more fun if there was llama-3 as well.
> 
> 
> 
> > ## Lisa DunlapCompetition Host
> > 
> > Agreed! The task of just predicting the model itself if quite interesting , perhaps there could be another competition about this in the futureâ€¦
> > 
> > 
> > 
> > > ## Lisa DunlapCompetition Host
> > > 
> > > Especially if you can describe in language what are the defining characteristics of the model
> > > 
> > > 
> > > 


---

> ## tanaka
> 
> Yeah, this kind of analytics is already done in lmsys.org itself and calculated to elo ranking. You can see current lmsys's ranking in lmsys leaderboard.
> 
> old elo ranking (2023-12-7?)
> 
> current elo ranking (2024-5-27)
> 
> Refs:
> 
> - [https://lmsys.org/blog/2023-12-07-leaderboard/](https://lmsys.org/blog/2023-12-07-leaderboard/)
> 
> 
> 


---

> ## Easter Bunny
> 
> Worth delving into this
> 
> 
> 


---



* --- discussion numver 9, the number of votes :33 ---

# Prediction Using Generation Header

**Takamichi Toda** *Tue Jul 16 2024 16:07:36 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (33 votes)

This is still a tried and tested idea and has not yet been successful in my current environment, but I would like to share it.

Currently, from what I can see in public code, the mainstream approach seems to be to base it on Llama or other LLMs and use LoRA to train a classification head. However, LLMs are originally trained to predict the next token, so I think this method is inefficient as it diverges from the original training of the LLM.

My idea is to use the same generative header (CausalLM) as the original LLM.

I adjust the prompts and use the generation probabilities of the tokens A, B and tie.ã€€The predictions are post-processed using softmax so that they sum to one.

Below is a simple sample code:

```
text = """
### Instruction
Which model's answer is appropriate for the prompt?ã€€If both are appropriate, answer `tie`.

### Prompt
{prompt text}

### A
{response A}

### B
{response B}

### Answer
"""

inputs = tokenizer(text)
out = model(inputs)
pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
pred = out.logits[0, -1, pred_token_id].softmax(0)

```

Here is the code that evaluates this method using Llama3 8B.

[https://www.kaggle.com/code/takamichitoda/lmsys-zeroshot-prediction](https://www.kaggle.com/code/takamichitoda/lmsys-zeroshot-prediction)

Llama3 has not undergone any special fine-tuning and is used as loaded from the Kaggle model. The evaluation data uses 1/5 of the competition data, which is equivalent to my current validation strategy (correlates well with the public leaderboard).

As a result, we obtained a score of 1.234. It was surprising to me that I could achieve such a result with ZeroShot.

Currently, I am performing SFT on the competition data and adjusting the prompts. However, models that learn classification headers still score better.

Are there others working on a similar approach?



---

 # Comments from other users

> ## James Day
> 
> I tried a similar experiment, primarily because I was hoping to take advantage of training and inference libraries for causal language models that are faster and more memory efficient than the HuggingFace transformers library, namely unsloth and vLLM. However, I actually finetuned the LLMs, as opposed to just doing zero shot inference like [@takamichitoda](https://www.kaggle.com/takamichitoda)'s initial experiment.
> 
> I got 0.902 (CV) with Llama 3 8B Instruct doing next token prediction, which is almost as good as I've been getting with "normal" Llama 3 based classification models. However, the same approach did not work well with Gemma 2 9B (0.990 CV ðŸ¤®), possibly due to Gemma's tied embeddings. My CV scores are pretty consistently ~0.03 lower than the corresponding LB scores, so those results translate to ~0.93 & 1.02 LB, which isn't good enough for me to bother submitting.
> 
> 
> 
> > ## Takamichi TodaTopic Author
> > 
> > Thank you for sharing
> > 
> > 0.9 is a great score for me ;)
> > 
> > By the way, I would like to know if possible, what kind of prompt did you use to perform finetuning?
> > 
> > I am using trl's SFTTrainer and learning only output with DataCollatorForCompletionOnlyLM.
> > 
> > 
> > 
> > > ## James Day
> > > 
> > > I used prompts like:
> > > 
> > > ```
> > > Which one of the chatbots below did a better job responding to the user request? Or were they tied?
> > > 
> > > ~~~~~~~~~~ CONVERSATION WITH BOT A ~~~~~~~~~~
> > > 
> > > ### User: "{initial prompt}"
> > > 
> > > ### Bot A Response: "{initial response}"
> > > 
> > > ### User: "{maybe a follow up prompt if available - I included as many conversation turns as will fit in a 3k token context window, discarding the first part of each conversation if necessary}"
> > > 
> > > ### Bot A Response: "{follow up response}"
> > > 
> > > ~~~~~~~~~~ CONVERSATION WITH BOT B ~~~~~~~~~~
> > > 
> > > ### User: "{...}"
> > > 
> > > ### Bot B Response: "{...}"
> > > 
> > > ### User: "{...}"
> > > 
> > > ### Bot B Response: "{...}"
> > > 
> > > ### BEST RESPONSE:
> > > 
> > > ```
> > > 
> > > It was then trained to output " A", " B", or " Tie". The spaces were part of the response tokens.
> > > 
> > > 
> > > 
> > ## Valentin Werner
> > 
> > Do you have the CV discrepancy only for this experiment or for all your models? We did several experiments where it is way below 0.01 in difference, and some where its similar to yours
> > 
> > 
> > 
> > > ## James Day
> > > 
> > > All experiments.
> > > 
> > > Also, I was oversimplifying for the sake of being able to do the math in my head. A more precise estimate of what 0.902 CV translates to on the leaderboard would be 0.902*0.890 + 0.125 = 0.928. The CV-LB correlation data that is based on is included below.
> > > 
> > > 
> > > 
> > > ## ShelterW
> > > 
> > > I think it is your extra prompts that cause the relatively big difference between CV and LB.
> > > 
> > > By the way, do you use qlora or lora to finetune llmï¼Ÿ
> > > 
> > > 
> > > 
> > > ## James Day
> > > 
> > > I use qlora.
> > > 
> > > As for the difference between my CV & LB scores, I doubt it has anything to do with the use of external training datasets not provided by the competition organizers (which I assume is what you meant by "extra prompts"), primarily because I did not observe any significant deviations from the preexisting trendline when adding extra data. I think a more likely explanation is that the data provided by the competition organizers isn't perfectly representative of their test data. For example, they may have partitioned their data based on the date on which each conversation occurred, thereby causing the test data to contain responses from new models that aren't present in the training data (or my cross validation data for that matter).
> > > 
> > > Also, one consequence of the trendline having a slope < 1 is that the discrepancies tend to get bigger as CV & LB scores improve. Extrapolating to a ridiculous extent, a model with perfect accuracy in cross validation (CV 0) would likely score ~0.125 on the leaderboard, which is a huge score discrepancy.
> > > 
> > > 
> > > 


---

> ## AbaoJiang
> 
> Hi [@takamichitoda](https://www.kaggle.com/takamichitoda),
> 
> As you mentioned, the performance of Zeroshot prediction is 1.234, which doesn't surpass the score of 1.098 by predicting the global mean.
> 
> However, it's still an interesting idea to try. Thanks for your sharing!
> 
> 
> 
> > ## Valentin Werner
> > 
> > I think the issue is in the softmax assumption without training the models first. The model has basically no intent to predict "A", "B" or "tie" whatsoever, if it not finetuned to realize it should do so. Therefore, the logits are also pretty much nonesense. 
> > 
> > This simple baseline experiment they did does not speak at all for whether their actual experiment will work or how well it will perform.
> > 
> > To me, at first glance I do not see any benefit over the seq class approach directly, as you still probably need to disable the autoregressive generation etc. But it definetly is an interesting idea
> > 
> > 
> > 
> > ## Takamichi TodaTopic Author
> > 
> > I was able to improve the score to 1.037 with SFT and Prompt Tuning. Although the classification header is still better, I intend to continue verification.
> > 
> > 
> > 
> > > ## ShelterW
> > > 
> > > I used SFT to finetune the llama3-8b and improved the LB score to 0.935 [here](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft), welcome to refer to and make suggestions.
> > > 
> > > 
> > > 


---



* --- discussion numver 10, the number of votes :32 ---

# Danube3 (0.5 B & 4B) just dropped!

**Valentin Werner** *Mon Jul 15 2024 15:57:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (32 votes)

I have used Danube2 for several experiments and even with QLoRA on Kaggle GPU it seems to be a way better alternative to DeBERTa Large. 

Danube3 just came out as 0.5B and 4B and also comes with a Chat model, which might be an upside for this competition. The 4B model outperforms its 1.5B predecessor on all benchmarks by a lot (also a bit of a size difference), while the 0.5B outperforms Qwen2 0.5B on most benchmarks. However, to me it will be particularly interesting, how the 4B model compares to Phi3-Mini, as this is the only other model I know in its weight class. Maybe this is team Danube's secret? ðŸ˜‰

From my experience smaller models, like 0.5B will still not fit on Kaggle GPUs (it should work on a 4090), so I will focus on the 4B model.

I also want to applaud the H2O Team, which is quite active on Kaggle, on this new release! It is always amazing, when talented researchers and Data Scientists contribute towards the Open LLM efforts (also the sheer speed of new releases). Looking forward to see how good this model is!

Links: 

Model card: [https://huggingface.co/h2oai/h2o-danube3-4b-chat](https://huggingface.co/h2oai/h2o-danube3-4b-chat)

Technical Report: [https://arxiv.org/abs/2407.09276](https://arxiv.org/abs/2407.09276)

Benchmarks:

Some benchmarks I aggrated from the [old open LLM leaderboard](https://huggingface.co/spaces/open-llm-leaderboard-old/open_llm_leaderboard). Danube3 is not included in the leaderboard yet, but reports these values on their model card. I think it is very interesting to see how close Danube3 comes to Gemma-7B and Mistral-7B.

| Category | Benchmark | Danube3-4b-Chat | Danube2-1.8B-Chat | Phi-3-Mini-4K-Ins | Gemma-7B | Mistral-7B Ins 0.2 |
| --- | --- | --- | --- | --- | --- | --- |
| Popular aggregated | MMLU  (5-shot) | 54.74 | 37.77 | 69.08 | 64.56 | 60.78 |
| Language Understanding | HellaSwag (5-shot) | 80.36 | 73.54 | 80.60 | 82.20 | 84.88 |
| Reasoning | ARC Challenge (5-shot) | 58.96 | 43.43 | 62.97 | 61.09 | 63.14 |
|  | TruthfulQA (0-shot) | 47.79 | 39.96 | 59.88 | 44.79 | 68.26 |
|  | WinoGrande (5-shot) | 76.48 | 69.77 | 71.6 | 79.01 | 77.19 |
| Math | GSM8K CoT   (5-shot) | 50.18 | 26.16 | 85.7 | 50.87 | 40.03 |
| Average |  | 61.42 | 48.44 | 69.91 | 63.75 | 63.14 |

Models were chosen based on the models microsoft phi3-mini is reporting against on their model card.



---

 # Comments from other users

> ## chaneyMA
> 
> nice work!!!!
> 
> 
> 


---

> ## madarshbb
> 
> Just for curiosity,
> 
> From my experience smaller models, like 0.5B will still not fit on Kaggle GPUs (it should work on a 4090), so I will focus on the 4B model.
> 
> What do you mean by this? Shouldn't 0.5B model be easier to fit than 4B?
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > I mean 0.5 is just big enough so you can't train it on Kaggle without quantization. This is basically similar size as DeBERTa Large
> > 
> > 
> > 


---

> ## Abhay Ayare
> 
> Fantastic guide! Thank you for sharing these valuable resources and insights on becoming a data scientist. Your passion for data science is inspiring. Looking forward to exploring your book "Kaggle for Beginners."
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > There are plenty of kaggle books, but I certainly have not written one of them ðŸ˜‰
> > 
> > 
> > 


---

> ## sayoulala
> 
> Thanks for you share, May I ask that the scores of this competition by the model ?
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > I have no trained it yet. Some experiments (did not try super hard) got danube2-1.8B to .98x for me
> > 
> > 
> > 


---

> ## The-Hai Nguyen
> 
> You are always shedding light on my learning progress all the way back from the PII-detection competition. Really appreciate and thanks for your sharing, it helps me and the others learn a lot throughout the journey ðŸ™.
> 
> 
> 


---



* --- discussion numver 11, the number of votes :32 ---

# Data Annotation Intuition - why the labels are noisy

**Valentin Werner** *Mon May 13 2024 23:10:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (32 votes)

I see that there are several ongoing discussions regarding label quality. As someone, who has spent a significant amount of time annotating data (and asking other people to annotate data for me), I want to share an opinion and intution of mine too.

In Data Annotation, you generally want professionals to annotate the data. They are supposed to (but sometimes do not do so) read the data carefully, select the labels carefully etc.; the annotated data is considered the GROUND TRUTH as these experts should be able to objectively decide the correct label (given same understand of the problem and annotation task). 

Then you generally compute an Inter-Annotator Agreement (are n people giving the same label on the same text), which was often seen as a ceiling for performance. Although this is not always the case in reality, this makes sense, because that means that your model is able to learn the intersection of knowledge from multiple annotators. 

Why is this important? The data we are training on is annotated by random people who wanted to try LLMs. While LMSYS is a great tool that I often use and recommend, it is for our problem mostly an annotation tool where the annotator can decide what question they want to annotate for and the data to annotate is generated in real time. 

However, there are several issues with this for our challenge:

- Users are not experts in using or understanding LLMs

- Users are often not experts in the topic they are asking about (and are often not fact-checking the responses)

- Unless users specify the same prompt and receive the same response, there is no way to evaluate Inter-Annotator Agreement

- LMSYS does not allow to undo or redo annotations (e.g., misclicked the wrong side)

- And most importantly: users have different preferences. This annotation task is not objective at all but PURELY subjective

This means we have NOISY labels and should employ techniques to deal with this; there are techniques such as active learning, ensembling, changing loss etc. which might work to address this issue - all of this needs to be tested (although ensembling is something we will do anyways ðŸ˜‰).



---

 # Comments from other users

> ## aotiandragon
> 
> Thanks a lot, It helped me to know the datas
> 
> 
> 


---

> ## Pranav Belhekar
> 
> Thanks for sharing your point. It helped me to analyze the competition.
> 
> 
> 


---

> ## Fae Gaze
> 
> Excellent insights on label noise! You might also explore robust loss functions like focal loss to mitigate noise impact, and consider frameworks like Snorkel to efficiently manage training data through programmable labeling functions
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Have not heard of Snorkel yet - can you recommend some literature? 
> > 
> > 
> > 


---

> ## Takamichi Toda
> 
> Thank you for sharing. And I was thinking the same thing just now.
> 
> There are some samples in the training data consisting only of very short prompts (one word). A typical example is when the prompt is just "hey". The responses of LLMs to this can generally be divided into two patterns:
> 
> Simply respond with "Hello!".
> After saying "Hello", provide a cue to continue the conversation, such as "How can I assist you today?".
> 
> I think 2 seems to be better, but the training data shows that there were a reasonable number of tie and cases where 1 was winning.
> 
> |  | n_sample | id |
> | --- | --- | --- |
> | hello_lose | 5 | 189242591, 211357242, 326037335, 458677274, 3947327386 |
> | tie | 4 | 1329170872, 3422926530, 4197301939, 4265282380 |
> | hello_win | 2 | 1655058446, 2171261721 |
> 
> The "hay" pattern trend seems to be more to my liking (2 mostly), but there are many other patterns like this that need to be treated as a NOISY label, as you say.
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > And I think is one on the more obvious side, where people just voted a side eventhough both models give the same answer. These people were obviously not thinking of poor ML Developers that need to explain why they did it ðŸ˜‰
> > 
> > I think evaluating how truthful the responses are (if there is a good way to do it) could also be a good feature for training our models
> > 
> > 
> > 


---

> ## Lisa DunlapCompetition Host
> 
> I think this is an amazing point: one of the big challenges with this challenge (no pun intended) - the data is crowdsourced with very minimal filtering so learning how to deal with label noise is incredibly important!
> 
> 
> 


---

> ## JunHua Liao
> 
> I have also discovered the issue of labels noise, mainly due to the same prompt and reponses, where there is a winner, which should be winner_tie. The two solutions currently in mind are: (1). Change the label to winner_tie; (2) Delete noise data
> 
> 
> 
> > ## Lisa DunlapCompetition Host
> > 
> > It may also be beneficial to look in to prompt deduplication or down weighting overrepresented prompts
> > 
> > 
> > 


---

> ## xiaotingting
> 
> At present, cleaning data and selecting models have the greatest impact on the results. I feel that no matter what field you are in, even if you use a large model, the quality of the data is very important.
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Looking forward to see how you cleaned data, we tried it a bit but were not able to get it to a point where it actually helped
> > 
> > 
> > 
> > > ## Fae Gaze
> > > 
> > > Hi, that is right. Too much cleaning will affect on the score adversely
> > > 
> > > 
> > > 


---

> ## AbChk
> 
> Thanks for sharing your point. It seems like this issue makes us wonder if the test data also has noisy labels?
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > very likely so. I (maybe not so) boldly assume that they did not manually check 25k samples for quality. It is like chosen based on label distribution and models.
> > 
> > 
> > 
> > ## Fae Gaze
> > 
> > the test is also noisy. But, we are not able to clean the noise. Even cleaning the training will affect the score
> > 
> > 
> > 


---



* --- discussion numver 12, the number of votes :31 ---

# Solution file to be updated the week of May 28th

**Sohier Dane** *Tue May 21 2024 01:30:43 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (31 votes)

A Kaggle community member informed us that the hidden test set for this competition contains values that have already been published. We were aware of some existing public data, but not that there are actually different versions of the public data using one name. We will update the solution file to ignore the affected rows next week and then rescore all existing submissions. You are welcome to continue to make submissions in the meantime.

Thank you very much to the Kaggler that disclosed this issue.

To everyone else, apologies for the disruption and thank you in advance for your patience. 



---

 # Comments from other users

> ## Psi
> 
> Given the top score, high probability there is another leakâ€¦
> 
> 
> 
> > ## Chris Deotte
> > 
> > If it is a leak, this is frustrating. Many teams spent a lot of money and time on compute. 
> > 
> > If it is a leak in the form of released test data, doesn't the host know where they published their test data? Couldn't this have been discovered day one?
> > 
> > 
> > 
> > > ## Psi
> > > 
> > > as you can see in this thread, it already happened onceâ€¦
> > > 
> > > I hope it is not an explicit data leak, or no leak at all (which seems unlikely though), but rather some clever data exploitation
> > > 
> > > 
> > > 
> > > ## CPMP
> > > 
> > > I miss the days where people had to disclose every external data they used in competition forums.
> > > 
> > > 
> > > 
> > > ## Fae Gaze
> > > 
> > > Hi, I also miss that part. I would appreciate any help on that
> > > 
> > > 
> > > 


---

> ## Paul Mooney
> 
> Following up to note that we updated the solution file and rescored the submissions. The leaderboard page now shows the updated scores. Thanks again to the community members that disclosed this issue!
> 
> 
> 


---

> ## heartkilla
> 
> A link to the data would be appreciated 
> 
> 
> 
> > ## heartkilla
> > 
> > I will clarify. If this data is public and someone has access to it, they can still use it for analysis, even if itâ€™s removed from the test set. 
> > 
> > 
> > 
> > ## dott
> > 
> > It is the 33k LMSYS dataset published on HF [https://huggingface.co/datasets/lmsys/chatbot_arena_conversations](https://huggingface.co/datasets/lmsys/chatbot_arena_conversations) and preprocessed by one of the Kagglers [https://www.kaggle.com/code/abdullahmeda/33k-lmsys-chatbot-arena-conversations](https://www.kaggle.com/code/abdullahmeda/33k-lmsys-chatbot-arena-conversations) into the competition format. We used the latter to detect the leak.
> > 
> > 
> > 
> > > ## heartkilla
> > > 
> > > That's the spirit. Thanks.
> > > 
> > > 
> > > 


---

> ## Fae Gaze
> 
> Thank you for the update and for handling this issue. Looking forward to the revised scores next week!
> 
> 
> 


---

> ## Wasiu Olaitan Garuba 
> 
> That's a lovely idea 
> 
> 
> 


---



* --- discussion numver 13, the number of votes :29 ---

# Unstable Deberta Training Results

**Valentin Werner** *Sat Jun 15 2024 18:19:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (29 votes)

I spent a lot of time trying the boosting approach without any tf-idf or transformer embeddings and am now moving back to training transformers. Now, early in the competition I trained a deberta-3-large model, which did not break any records, but at least learned something (like 1.039). However, all my current attempts are failing to learn yet again - even with the same parameters as the last time I trained.

Have you experienced similar results where doing rather small changes (e.g., the structure of the input string) results in the model suddenly being unable to learn at all? What are the "best practices" you learned for training deberta / llama & co during this competition (if you dont mind sharing).

Cheers!



---

 # Comments from other users

> ## James Day
> 
> I got 0.997 with deberta-v3-large by having it produce an embedding for each side of the conversation separately, then passing those embeddings to a small 2 layer fully connected classifier. That was my first baseline approach in this competition. It certainly isn't the most accurate, but worked better than what you're describing.
> 
> I haven't really had any stability problems in this competition, but most stability problems where a model fails to converge to anything better than random guessing that I've encountered in the past have stemmed from a misconfigured learning rate schedule, so you might want to try tinkering with that if you haven't already.
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Welcome back to the competition, James - I remember you die some impressive training in DAIGT too. Looking forward to See you on top of the lb again!
> > 
> > Do I understand correctly that you are only using the embeddings or did you combine two deberta Models and add layers on top of it?
> > 
> > 
> > 
> > > ## James Day
> > > 
> > > My 0.997 baseline used the same deberta backbone to process each "side" of the conversation (where each side is essentially a concatenation of the initial prompt, model X's first response, the follow up prompt (if available), model X's second responseâ€¦ up to a 768 token max context length). The embeddings from the CLS token on each side (A & B) were then concatenated and fed to a small classification head. In other words, there was a single debeta model with a couple extra layers stacked on top. The whole thing was trainable - I did not use a frozen pretrained backbone to compute the embeddings.
> > > 
> > > The approach described above is easily beaten by scaling up to using Llama 3 8B as the foundation model.
> > > 
> > > 
> > > 


---

> ## Takamichi Toda
> 
> I am sharing what has been effective in my experiments. 
> 
> Now difficulty in securing computational resources, I am conducting experiments with DeBERTa xsmall. Please note that you may not achieve the same results due to environmental differences.
> 
> ### Label Smoothing
> 
> I am using CrossEntropyLoss and setting the label_smoothing parameter to 0.2. The reason is that competition data can be labelled differently for the same data, and I thought it could be said to be a kind of noisy data.
> 
> ### Within-task Pre-training
> 
> I train the Masked Language Model using the competition data and use these weights for fine-tuning.
> 
> ### Dropout Off
> 
> I adjusted the Dropout Ratio, but 0 was the most effective. 
> 
> Although I have heard that Dropout should be off for regression problems, this is not. I do not understand why the absence of Dropout yielded better accuracy.ðŸ§
> 
> ### Adversarial Training
> 
> I tried AWP, and it was effective. I also plan to test other methods such as FGM.
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > have you had stable results between XSmall and Large? for me, the smaller models are not converging, so I only trained Large. This obviously has terrible Iteration Speed for the experiments.
> > 
> > Thanks for sharing!
> > 
> > 
> > 
> > ## Valentin WernerTopic Author
> > 
> > Once I tried training with AWP the model instantly learned nothing again - its quite interesting
> > 
> > 
> > 
> > > ## Takamichi Toda
> > > 
> > > Hmm, I wonder why.
> > > 
> > > Which model are you using? I am still using DeBERTa xsmall, so it might be due to the difference in model size.
> > > 
> > > How about applying a small value to the AWP Learning Rate?
> > > 
> > > In my case, it's 1e-4. By the way, the overall learning rate is 2e-5.
> > > 
> > > 
> > > 
> > > ## Valentin WernerTopic Author
> > > 
> > > I will have to look further into AWP, I guess. I have not used it before and took an existing kaggle notebook as basis. 
> > > 
> > > I had no success with any small model and only ever got close to 1.00 with deberta-3-large. I am also using effective batch size of 8 (2 x 4) and a lr of about 8e-6 - so that is muuuuch lower than yoursâ€¦ Time to do some more experiments :)
> > > 
> > > 
> > > 


---

> ## Valentin WernerTopic Author
> 
> I trained a deberta-3-large model yesterday and achieved 1.005 - same training params today get me about 1.07. It seems very unreliable to me - I have yet to schiebe good scores with lora
> 
> 
> 
> > ## James Day
> > 
> > Weird. For me the random variation from run to run is < 0.01. CV & LB are very well correlated too (pearson r = 0.97).
> > 
> > It sounds to me like something is broken or misconfigured in your training pipeline. It isn't a problem inherent to the data itself.
> > 
> > 
> > 
> > > ## yechenzhi1
> > > 
> > > Hi, may I ask how do you get your CV split? I randomly split 10% from the training dataset, and the score from CV and LB are very different, my CV score is 0.889, and LB is 0.922. 
> > > 
> > > 
> > > 


---



* --- discussion numver 14, the number of votes :27 ---

# Load 7b Gemma Keras without any memory issue and FAST.

**MarÃ­lia Prata** *Sun May 12 2024 07:28:29 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (27 votes)

# A tip  to avoid memory issues while running your 1.1 -7b_instruct_en Gemma/Keras Model:

[Gemma 1.1 7B Int8 Load](https://www.kaggle.com/code/awsaf49/gemma-1-1-7b-int8-load) By Awsaf.

On the last topic (2 days ago ), I asked "How to work with Gemma Keras 1.1- 7b instruct-en WITHOUT your Kaggle Notebook being restarted cause you've allocated more memory than is avaiable. Then we should opt to Google Cloud or dismiss our work.

Some answers that I got to that previous topic:  I read/learned  that reducing batches and max_length could help me to load the model and face the memory issue.  Not always, it's a 7b (7 billion parameters model).

But, what if we don't have max_lenght and batches written on our Kaggle Notebook script? Sometimes it happens. Therefore, it's great to have a Plan B:

Fortunately, I found Awsaf's code and published my 1st Gemma 1.1-7b-instruct-en.  

So, take a look and check Awsaf's amazing, cristal clear code:

[Gemma 1.1 7B Int8 Load](https://www.kaggle.com/code/awsaf49/gemma-1-1-7b-int8-load) By Awsaf.

For the record, there aren't many 7b Gemma Keras  Kaggle Notebooks. Though we can find plenty of 2b Models.



---

 # Comments from other users

> ## Adnan Alaref
> 
> Good news for find solution, thanks for sharing  [@mpwolke](https://www.kaggle.com/mpwolke) 
> 
> 
> 
> > ## MarÃ­lia PrataTopic Author
> > 
> > Indeed Alaref,
> > 
> > I was so happy that I was able to work with Gemma/Keras 1.1-7b_instruct-en without any memory issue that I felt that I should share this topic because very few showed appreciation to Awsaf's code (till yesterday he had only 6 votes for such a remarkable and useful code and  his 2 datasets.  
> > 
> > Maybe, kagglers didn't realize the importance of that code.
> > 
> > For the record, the Notebook ran in only GPU 15 minutes!  Isn't that great?
> > 
> > Besides, I was able to deliver the last Model that the hosts had pinned on this competition.
> > 
> > Not many users are working with 1.1_7b_instruct. In fact, I didn't read any other, except Awsaf's code.
> > 
> > It was almost my "Moby Dick" of models.
> > 
> > Thank you Alaref.
> > 
> > 
> > 


---



* --- discussion numver 15, the number of votes :26 ---

# LLM Models that can be used

**superferg** *Sat Jul 06 2024 21:28:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (26 votes)

May I ask which models everyone has tried? I tried the following modelï¼ŒRandomly select 20% of the samples as the validation set.ï¼š

| Model | Local Validation | Public Leaderboard |
| --- | --- | --- |
| Llama3-8B-instruct | 0.9419 | 0.954 |
| Llama3-8B | 0.9818 | 0.987 |
| Gemma2-9B-instruct | 0.9262 | 1.206 |
| Gemma2-9B | 0.9499 | 1.299 |

Gemma2-9B has obtained abnormal results, I guess it might be a problem with the inference. Does anyone have similar problems?

UPDATE:

With the [new public notebook](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora), the correct results were obtained.

| Model | Local Validation | Public Leaderboard |
| --- | --- | --- |
| Llama3-8B-instruct | 0.9419 | 0.954 |
| Llama3-8B | 0.9818 | 0.987 |
| Gemma2-9B-instruct | 0.9262 | 0.930 |
| Gemma2-9B | 0.9499 | TODOâ€¦ |


---

 # Comments from other users

> ## Valentin Werner
> 
> gonna leave this one here ðŸ˜‰
> 
> 
> 
> > ## superfergTopic Author
> > 
> > The current local validation set is 0.91X, I still can't migrate to LB. LoL
> > 
> > 
> > 
> > ## SAY WHAT
> > 
> > so funnyï¼ï¼ï¼
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> Gemma2-9B came out recently. The 9B makes it even harder to train, but it tops the performance benchmarks among these models
> 
> 
> 
> > ## Cody_Null
> > 
> > Were you able to pull the gemma2-9B into kaggle from huggingface or are you using the Gemma 2 Â· gemma-2-9b-pt Â· V1 on kaggle models? 
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > We pulled gemma2-9b from huggingface into kaggle.
> > > 
> > > 
> > > 
> > ## s111mple
> > 
> > Finetuned model donnot get fine results~ Have you tried it?
> > 
> > 
> > 


---

> ## xiaotingting
> 
> It seems that the validation set index is positively correlated with the public score, and there is still room for further improvement of the index.
> 
> 
> 


---

> ## Xiot1206
> 
> thanks for providing these key information
> 
> 
> 


---

> ## lllleeeo
> 
> As an nlp newbie, I'd like to ask a possibly stupid question, how did you determine how many parameters you needed to use to participate in the fine-tuning, did you try them one by one? How much is generally best based on experience, is it different for different models, I observed that the public laptop fine-tuning in liama 8b only used 0.02% of the parameters is this too little?
> 
> 
> 
> > ## superfergTopic Author
> > 
> > If there is not enough computing power, using the Lora fine-tuning method may be the only choice.
> > 
> > 
> > 
> > > ## lllleeeo
> > > 
> > > Thanks for your reply! I've rented an A100 and a 4090 and want to do some experiments in parallel, I'm wondering if I can try more parameters based on that computing power, but I'm not sure how much I should start trying.
> > > 
> > > 
> > > 
> > > ## superfergTopic Author
> > > 
> > > The first step can try the top-level public notebook.
> > > 
> > > 
> > > 
> > > ## lllleeeo
> > > 
> > > Thank you it worksï¼
> > > 
> > > 
> > > 


---

> ## Mr.T
> 
> How do you load gemma 2-9b during inference?
> 
> 
> 
> > ## superfergTopic Author
> > 
> > Please refer to the notebook belowï¼š
> > 
> > [https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)
> > 
> > 
> > 


---

> ## EISLab_hwlee
> 
> Can the Gemma2-27B-instruct model perform better?
> 
> 
> 
> > ## EISLab_hwlee
> > 
> > As a result of the experiment, it was observed that the performance was poor.
> > 
> > 
> > 
> > > ## superfergTopic Author
> > > 
> > > I still can't complete the reasoning of 27B within 9 hours, theoretically, 27B should achieve better results.
> > > 
> > > 
> > > 
> > > ## EISLab_hwlee
> > > 
> > > I also failed to submit it.
> > > 
> > > However, in training, the loss did not fall below 1.0, and the evaluation loss did not fall below 1.0.
> > > 
> > > 
> > > 


---

> ## hn
> 
> Just curious, what was the missing piece that lead to your poor inference results from Gemma2? I see that you mentioned itâ€™s fixed with the public notebook 
> 
> 
> 
> > ## superfergTopic Author
> > 
> > I don't have enough time to figure out the reason, but you can analyze the reason by comparing the following two notebooks.
> > 
> > [https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference](https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference)
> > 
> > [https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)
> > 
> > 
> > 


---

> ## Mukatai
> 
> In a recent public notebook, a score of 0.941 was recorded with fine-tuning of Gemma2, but this table shows a score of 0.930 with Gemma2-9B-instruct. Is there any difference?
> 
> 
> 
> > ## superfergTopic Author
> > 
> > I am using my own training script, so there should be some differences, I can make it public after the competition ends.
> > 
> > 
> > 
> > > ## Mukatai
> > > 
> > > Thank you. Is Gemma's training conducted on Kaggle? With a public notebook, training on a single dataset exceeds the 30-hour weekly limit
> > > 
> > > 
> > > 


---

> ## Femca7
> 
> May I ask the results you get is from pre-trained or finetuned model ?
> 
> 
> 
> > ## superfergTopic Author
> > 
> > You can see the details in the table I provided, those with an 'instruct' suffix are fine-tuned models.
> > 
> > 
> > 


---

> ## yechenzhi1
> 
> May I ask if Instruct model is better than the base model? I have only tried Instruct model.
> 
> 
> 
> > ## superfergTopic Author
> > 
> > According to my local testing, Llama3-8B instruct is better than Llama3-8B. But perhaps the appropriate hyperparameters for  Llama3-8B have not been found.
> > 
> > 
> > 
> > ## ducnh279
> > 
> > I also had a similar question in the early days when I started with fine-tuning decoder-only models for text classification! 
> > 
> > I asked [@rasbtn](https://www.kaggle.com/rasbtn) (a prominent researcher/educator) on Twitter! He replied:
> > 
> > I also conducted some experiments, and the results indicate that using instruction-tuned versions often gives better performance and faster convergence compared to the base model.
> > 
> > 
> > 
> > > ## yechenzhi1
> > > 
> > > Thanks! That's really helpful!
> > > 
> > > 
> > > 


---



* --- discussion numver 16, the number of votes :26 ---

# Gemma 2 has been released

**Anil Ozturk** *Fri Jun 28 2024 00:49:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (26 votes)

Google has released the v2 for Gemma. It is available in two versions: 9B and 27B. You might want to try the 9B one.

HuggingFace: [https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315)



---

 # Comments from other users

> ## Valentin Werner
> 
> If they keep making the small models bigger, kaggle should keep making GPUs bigger ðŸ˜‰
> 
> 
> 
> > ## Enter your display name
> > 
> > Agree, also many packages now no longer support installation on older GPUs like the T4.
> > 
> > 
> > 
> > ## Yashchavn
> > 
> > true, lets see what happens
> > 
> > 
> > 
> > ## SunshineMoment
> > 
> > Agree! we need more powerful gpu
> > 
> > 
> > 


---

> ## Cody_Null
> 
> Update: I have found the reason. The top here causes an OOM error while the bottom works fine. 
> 
> `
> 
> # BitsAndBytes configuration
> 
> ```
> bnb_config =  BitsAndBytesConfig(
>     load_in_8bit=True,
>     bnb_8bit_compute_dtype=torch.float16,
>     bnb_8bit_use_double_quant=False)
> 
> bnb_config = BitsAndBytesConfig(
>     load_in_8bit=True,
>     bnb_8bit_quant_type="nf8",
>     bnb_8bit_use_double_quant=True,
>     bnb_8bit_compute_dtype=torch.bfloat16)
> 
> ```
> 
> `
> 
> 
> 
> > ## Lucifer_is_back_
> > 
> > thanks for that!
> > 
> > 
> > 
> > > ## Matous Famera
> > > 
> > > [@luciferisback](https://www.kaggle.com/luciferisback) I have read Gemma 2 outperforms Llama 3 8b in several benchmarks. I don't know if Gamma 2 can be implemented in this competition though.
> > > 
> > > 
> > > 
> > ## mbyc_xkyz_2023
> > 
> > but , after i strat my code, Unused kwargs: ['bnb_8bit_quant_type', 'bnb_8bit_use_double_quant', 'bnb_8bit_compute_dtype']. These kwargs are not used in , how to understand?
> > 
> > 
> > 


---

> ## xiaotingting
> 
> Gemma v2 is indeed the most useful one I have tried in this competition.
> 
> 
> 


---

> ## Nikhil Tumbde
> 
> Added the 9b base model on kaggle, [here](https://www.kaggle.com/models/nikhiltumbde/gemma-2-9b-hf)
> 
> 
> 


---

> ## Rishit Jakharia
> 
> ### Regarding the GGUF files
> 
> - Did anyone manage to use the Gemma 2 GGUF files on Kaggle
> 
> I am unable to do so myself, as I'm using llama cpp  and the latest version of llamaCPP seems to not be compatible with Kaggle
> 
> 
> 


---

> ## Guocheng Song
> 
> wowï¼Œ that's amazing
> 
> 
> 


---



* --- discussion numver 17, the number of votes :25 ---

# Converted Ultrafeedback data (External Data)

**Takamichi Toda** *Fri Jun 07 2024 19:14:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (25 votes)

I have converted Ultrafeedback data shared by [@thedrcat](https://www.kaggle.com/thedrcat) into competition data.

Unlike competition data, Ultrafeedback data has a rating. This means that the model with the higher rating is the winner and the tie is the "tie".

Please see that:

Convert Code: [https://www.kaggle.com/code/takamichitoda/lmsys-convert-ultrafeedback-to-competition](https://www.kaggle.com/code/takamichitoda/lmsys-convert-ultrafeedback-to-competition)

Dataset: [https://www.kaggle.com/datasets/takamichitoda/lmsys-ultrafeedback](https://www.kaggle.com/datasets/takamichitoda/lmsys-ultrafeedback)

I would like to thank [@thedrcat](https://www.kaggle.com/thedrcat) for initially sharing this data.

Please make sure to upvote the [original discussion](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756).





* --- discussion numver 18, the number of votes :24 ---

# lmsys chat 1m (is Allowed? - dataset consists of user interactions from the ChatBot Arena) [Solved - Allowed]

**SeshuRaju ðŸ§˜â€â™‚ï¸** *Fri May 03 2024 14:00:35 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (24 votes)

## [IMSYS Chat 1M](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)

## KeyValue

| Metric | Value |
| --- | --- |
| Conversations | 1,000,000 |
| Models | 25 |
| Users | 210,479 |
| Languages | 154 |
| Avg. # Turns per Sample | 2.0 |
| Avg. # Tokens per Prompt | 69.5 |
| Avg. # Tokens per Response | 214.5 |

## [Paper - LMSYS-CHAT-1M: A LARGE-SCALE REAL-WORLD LLM CONVERSATION DATASET](https://arxiv.org/pdf/2309.11998)

- LMSYS-Chat-1M is collected from April to August 2023 - on website [https://chat.lmsys.org/](https://chat.lmsys.org/)

- The dataset contains raw conversation text without any processing. To ensure the safe release of

  data, we have made our best efforts to remove conversations that contain personally identifiable

  information (PII).

- The dataset includes one million conversations from 25 state-of-the-art LLMs with 210K users

  across more than 150 languages.

- We remove prompts that are either too short (fewer than 32 characters) or too long (more than 1536 characters).

- Biased user distribution : The majority of users of our website are LLM hobbyists and researchers who are interested in trying and testing the latest LLMs. This suggests that the data

  might not fully represent the broader population. For instance, everyday users or individuals

  from different professions might interact with the LLMs in varied ways. Consequently, results

  derived from this dataset might not generalize across all user groups.

- Containing repeated and low-quality data : The lack of user registration and data filtering can

  result in a significant amount of low-quality and duplicate data. However, we choose to not

  apply any filtering on purpose to reflect the real-world distribution.

- No human preference annotations. This dataset contains raw conversations without any human

  preference annotations. While our website does collect some user votes, we plan to examine

  the quality further before releasing them. We encourage the community to check the human

  preference data released in (Zheng et al., 2023).

# We can compare the Kaggle dataset with 1m dataset

- is PII added and removed more similar prompts or questions as suggested by paper ?

- Generate targets for the filtered dataset using GPT-4

- We can probe LB to check is this data topics exists in private LB ( as 20 clusters  for random 100k as per paper )



---

 # Comments from other users

> ## Lisa DunlapCompetition Host
> 
> Hello! Organizer here: yes it is allowed :)
> 
> 
> 


---

> ## Gaurav Rawat
> 
> Had exactly the same question about some of the lmsys datasets on hugging face ideally most are open I am guessing should be fine
> 
> 
> 
> > ## SeshuRaju ðŸ§˜â€â™‚ï¸Topic Author
> > 
> > I expected the same, till now organiser not conformed. maybe we can consider it as Yes
> > 
> > 
> > 


---



* --- discussion numver 19, the number of votes :22 ---

# Mistral-NeMo release

**Ashwani** *Fri Jul 19 2024 01:09:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (22 votes)

Mistral-NeMo 12B released 

- Outperforms Gemma2 9B and Llama3 8B

- 128K context window

- Multilingual in 100+ languages: excels in European, Asian & Indian languages

- Quantization-Aware Training at FP8

- Apache 2.0 license

Blog: [https://mistral.ai/news/mistral-nemo/](https://mistral.ai/news/mistral-nemo/)

HF Weights (Base): [https://huggingface.co/mistralai/Mistral-Nemo-Base-2407](https://huggingface.co/mistralai/Mistral-Nemo-Base-2407)

HF Weights (Instruct): [https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)



---

 # Comments from other users

> ## James Day
> 
> FYI, it appears finetuning for Mistral-NeMo is currently broken in the transformers library (see [https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/6](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407/discussions/6)). A fix should be released soon ([https://github.com/huggingface/transformers/pull/32065](https://github.com/huggingface/transformers/pull/32065)).
> 
> As usual, I'm inclined to wait at least a couple days for bugs to be discovered and fixed before attempting to use any new model ðŸ˜‰.
> 
> 
> 


---

> ## Lorry Zou
> 
> I just fune-tuned the instruct model yesterday, seems like it's not even on par with Gemma2 9bâ€¦Weird
> 
> 
> 
> > ## Valentin Werner
> > 
> > Might be the bugs James mentioned. These bugs are not always not always black and white, as in they raise Exceptions. Could also be that a different attention mechanism is used, which the model was not trained on or such (not sure if that is actually a thing and if it would cause an Exception, but you probably get the gist)
> > 
> > 
> > 
> > ## Eisuke Mizutani
> > 
> > I installed the latest transformers from source and could run training without error.
> > 
> > But as Lorry Zou mentioned, the result was not so good (even worse than llama3 in my case).
> > 
> > 
> > 


---

> ## EISLab_hwlee
> 
> It's very difficult to implement the codeâ€¦
> 
> 
> 


---

> ## Valentin Werner
> 
> Release Season going hard in the last 5 weeks of the competition ðŸš€
> 
> 
> 
> > ## Psi
> > 
> > Thankfully, only three weeks left :)
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > It's quite funny with the GenAI Hype. You may have a breakthrough in the NLP competitions not by modelling techniques, but by sheer coincidence, having companies like H2O, Google or Mistral (& NVIDIA) release some high quality models. Not so long ago, we used to train Mistral-7B for peak performance - now it seems like a 3rd choice model.
> > > 
> > > 
> > > 


---

> ## gentle bird
> 
> new model. who is trying this?
> 
> 
> 


---



* --- discussion numver 20, the number of votes :22 ---

# Share my initial experiment

**suguuuuu** *Thu Jul 04 2024 01:24:22 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (22 votes)

Since it seems I won't be able to continue this competition, I am sharing a concept that I explored a month ago. It's possible that similar content has already been shared in other discussions.

It might be completely meaningless information.

## update (2024/07/06)

Upload results. [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/516806#2905904](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/516806#2905904)

## Initial Approach

Compare texts across various criteria  based on a weighted average score according to categories.

### Evaluation Criteria:

```
1. **Clarity**: AI can provide clear answers to specific questions. However, when questions are ambiguous or open to multiple interpretations, AI may not always deliver the optimal response.

2. **Accuracy of Information**: AI answers are based on the data and information provided. Thus, if the source is accurate, the AI's response is accurate. However, AI lacks the capability to discern incorrect information from sources.

3. **Completeness**: AI can provide comprehensive answers to questions, but only within the scope of what it has understood and learned. Unlike humans, AI does not possess the ability to supplement information based on intuition or experience.

4. **Conciseness**: AI is capable of delivering concise responses. However, what qualifies as "concise" can largely depend on context and human subjectivity, making it challenging for AI to always meet human expectations for brevity.

```

### Categories:

- Informational Queries:

Accuracy of Information: Most crucial. If the information sought by the user is inaccurate, the value of the response significantly decreases.
Completeness: Next in importance. Providing all necessary information comprehensively is expected to fully resolve the user's queries.
Clarity: It is also important that accurate information is presented in an understandable manner.
Conciseness: Important, but often secondary to accuracy and completeness. However, irrelevant information should be avoided.

- Dialogue and Engagement Queries:

Clarity: For engaging effectively, it is crucial that responses are easy to understand.
Conciseness: Favored for maintaining fluid dialogue; responses should be brief and clear.
Accuracy of Information: Accuracy remains important in dialogic contexts, but because entertainment elements may be involved, strict accuracy isn't always necessary.
Completeness: Important, but not always necessary to cover every aspect to advance the dialogue.

- Emotional Support and Consultation Queries:

Clarity: Responses that resonate with users' emotions need to be particularly clear.
Accuracy of Information: Reliable information or advice is necessary when providing emotional support.
Completeness: Providing a comprehensive view on the issue helps users feel reassured.
Conciseness: While important, it is somewhat secondary compared to other factors.

- Troubleshooting Queries:

Accuracy of Information: Accurate troubleshooting steps and information are extremely crucial.
Clarity: Solutions must be clear for users to resolve issues.
Completeness: Covering all necessary steps required for problem resolution is demanded.
Conciseness: Useful but considered least crucial among the factors, though irrelevant information should be omitted.

## Experiments :

Utilized ChatGPT-4o for a trial with 500 data points.

- normal_prediction
Accuracy : 0.492

- predict with criteria
Accuracy : 0.514

Example result:

- ground truth and input data

```
  winner_model_a    0
  winner_model_b    1
  winner_tie        0
  ["What is the scientific probability of earth being the only habitable planet in the universe?","I don't think it really matters what some scientists \"think\" because it's very possible it could all be a simulation. And there's actual evidence to back that up.","But what makes that a \"radical\" possibility, when it's the possibility with actual evidence?"]

```

- Predict with ChatGPT-4o

winner model B

```
  â˜…res_a:
  Clarity: 4
  Accuracy of Information: 4
  Completeness: 5
  Conciseness: 3

  Reasoning:
  - Clarity: Responses are generally clear and appropriate for the questions, but occasionally complex words or concepts are used, requiring specialized knowledge for full understanding.
  - Accuracy of Information: The provided information is accurate and based on the latest scientific findings, although some claims about the simulation hypothesis are philosophical and lack scientific evidence.
  - Completeness: Responses are very comprehensive, considering the issue from multiple perspectives.
  - Conciseness: While detailed, the responses are sometimes lengthy and can feel redundant, so a more concise presentation could improve them.

  â˜…res_b:
  Clarity: 5
  Accuracy of Information: 5
  Completeness: 5
  Conciseness: 4

  This response is clear and accurate, providing relevant information comprehensively. However, some parts are slightly verbose, thus a conciseness score of 4.

```

- prompt/code

```
import requests
import pandas as pd
from time import sleep
from tqdm import tqdm

def generate_response_for_LMSYS(api_key, prompt):
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json",
    }
    data = {
        "model": "gpt-4o",
        "messages": [
            {"role": "system",                  
                "content": "Please evaluate each response on a scale of up to 5 points. Format it as 'Clarity:x, Accuracy of Information:x, Completeness:x, Conciseness:x'. "},         
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }

    response = requests.post(url, json=data, headers=headers)
    if response.status_code == 200:
        return response.json()['choices'][0]['message']['content']
    else:
        return "Error: " + response.text    

idx = 150
print(train[["winner_model_a","winner_model_b","winner_tie"]].iloc[idx])

print(train["prompt"].iloc[idx])

print()
print("======================")
print( train["response_a"].iloc[idx])

print()
print("======================")
print( train["response_b"].iloc[idx])

all_prompt = f"""
        Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
        Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
        "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
        "winner_tie".
        ----------------------------------------------------------------------------------------------------------
        prompt: {train["prompt"].iloc[idx]}
        ----------------------------------------------------------------------------------------------------------
        response_a: {train["response_a"].iloc[idx]}
        ----------------------------------------------------------------------------------------------------------
        response_b: {train["response_b"].iloc[idx]}
        ----------------------------------------------------------------------------------------------------------
        Preference=  """.strip()

res = generate_response_for_LMSYS(api_key, all_prompt)
print(res)

```



---

 # Comments from other users

> ## Valentin Werner
> 
> What was your idea behind this approach?
> 
> At first it seems logical to try this with text generation, apart from the downfall of generation speed, which is way slower than sequence classification. However, you dont want to classify a winner, but probabilities - so every misclassification hurts so much more. 
> 
> I think its pretty interesting to see that GPT-4o with Reasoning does not necessarily outperform / is on-par with a properly finetuned deberta3base 
> 
> 
> 
> > ## suguuuuuTopic Author
> > 
> > Thanks for your comments!
> > 
> > I am going to implement DeBERTa or LLaMA3 as an auxiliary loss. 
> > 
> > The reason is that I hypothesized this will enable the model itself to score based on the Criteria and Categories of the text, ultimately having a positive impact on performance.
> > 
> > I came up with this idea from this discussion. I thought that performance might improve by making this more detailed.
> > 
> > [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499756)
> > 
> > I'll skip the explanation of the effects of AuxLoss, as it's used in other Kaggle competitions as well.
> > 
> > e.g. [https://www.kaggle.com/competitions/rsna-breast-cancer-detection/leaderboard](https://www.kaggle.com/competitions/rsna-breast-cancer-detection/leaderboard)
> > 
> > I was preparing to do labeling using ChatGPT.
> > 
> > 
> > 
> > > ## nahyat
> > > 
> > > I've learned a lot from this. Thank you.
> > > 
> > > You said that you use Llama3 or Deberta as auxiliary loss, but does auxiliary loss mean that you obtain the output of the main task and the output of the subtask from a single model by branching and use them for the loss?
> > > 
> > > I would appreciate it if you could let me know if you could share your thought and ideas.
> > > 
> > > 
> > > 
> > > ## suguuuuuTopic Author
> > > 
> > > Yes, at first, I was going to use subtask as you said.
> > > 
> > > My just idea, It would be interesting to use the subtask prediction results and lightgbm to predict the final results.
> > > 
> > > 
> > > 


---

> ## suguuuuuTopic Author
> 
> I uploaded results of my experiment. It is written in Japanease. 
> 
> [https://www.kaggle.com/datasets/sugupoko/chatbotarena-output-by-gpt4o](https://www.kaggle.com/datasets/sugupoko/chatbotarena-output-by-gpt4o)
> 
> 
> 
> > ## Shota Yamasaki
> > 
> > æœ‰ç›Šãªæƒ…å ±ã‚’ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼
> > 
> > ã‚«ãƒ†ã‚´ãƒªã«å¿œã˜ãŸåŠ é‡å¹³å‡ã‚¹ã‚³ã‚¢ã«åŸºã¥ã„ã¦ã€ã•ã¾ã–ã¾ãªåŸºæº–ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã¯é‡è¦ã ã¨ç†è§£ã§ãã¾ã—ãŸã€‚
> > 
> > ã“ã®å®Ÿé¨“ã§å¾—ãŸçµæžœã‚’ã“ã®å¾Œã©ã†æ´»ã‹ã™ã¤ã‚‚ã‚Šã ã£ãŸã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ
> > 
> > éžå¸¸ã«èˆˆå‘³æ·±ã„ã§ã™ã€‚
> > 
> > 
> > 
> > > ## suguuuuuTopic Author
> > > 
> > > I wrote it above!
> > > 
> > > 
> > > 


---



* --- discussion numver 21, the number of votes :22 ---

# Both the A and B responses are [null]

**Takamichi Toda** *Mon May 13 2024 09:43:54 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (22 votes)

During the data analysis, I found samples where the responses for both A and B were [null]. 

Most of these cases arewinner_tie, so it would be best to handle them with rules rather than using ML model.

```
import pandas as pd
train_df = pd.read_csv(f"/kaggle/input/lmsys-chatbot-arena/train.csv")

row = train_df[train_df["id"] == 57180984].iloc[0]
res = row[["response_a", "response_b", "winner_model_a", "winner_model_b", "winner_tie"]].to_dict()
print(res)
# {'response_a': '[null]', 'response_b': '[null]', 'winner_model_a': 0, 'winner_model_b': 0, 'winner_tie': 1}

```

On the other hand, there are some cases where both are [null] yet a winner is determined. 

```
row = train_df[train_df["id"] == 867270727].iloc[0]
res = row[["response_a", "response_b", "winner_model_a", "winner_model_b", "winner_tie"]].to_dict()
print(res)
# {'response_a': '[null]', 'response_b': '[null]', 'winner_model_a': 1, 'winner_model_b': 0, 'winner_tie': 0}

```

How should this be interpreted? 

|  | n_sample | id |
| --- | --- | --- |
| winner_tie | 12 | 57180984, 249576331, 563620901, 939431975, 1224714333, 1433968841, 1833691834, 2624561104, 3013893052, 3697544388, 3731007975, 3870030183 |
| winner_model_b | 4 | 2369712796, 2542474454, 3044249115, 3174500072 |
| winner_model_a | 3 | 867270727, 2941706797, 3235570281 |

For now, it seems better to exclude both [null] data from the training data.



---

 # Comments from other users

> ## Lisa DunlapCompetition Host
> 
> While we removed any single turn conversations with null values with both responses, we chose to not filter these out in multi turn conversations.
> 
> Two things to take into consideration when interpreting the data are: (1) nothing prevents users on Chatbot Arena from voting erratically; and (2) users on Chatbot Arena vote one time per conversation (even for multi-turn conversations).
> 
> For example, if someone submits multiple prompts in rapid back-to-back succession, or if there is some sort of platform error, this can introduce null responses into a conversation that also has valid responses, and then it is up to the individual user how they want to rank the full conversations.
> 
> 
> 
> > ## Lisa DunlapCompetition Host
> > 
> > Correction: it looks like there are a few instances of single turn conversations with "[null]" values in the dataset. Since this appears very rarely, we will not be making any changes to the training dataset
> > 
> > 
> > 
> > > ## Takamichi TodaTopic Author
> > > 
> > > [This thread](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502449) has also been discussed in this thread, and it seems like it will make for an interesting problem setting as a noisy problem.
> > > 
> > > Thank you for your reply.
> > > 
> > > 
> > > 
> > ## Kaizhao Liang
> > 
> > is it safe to assume this edge case will also be in the test set? mostly coming from how we should parse the inputs.
> > 
> > 
> > 
> > > ## Lisa DunlapCompetition Host
> > > 
> > > yes this could appear as a (very rare) edge case in the test set
> > > 
> > > 
> > > 


---

> ## Valentin Werner
> 
> You either ignore these cases completely and add these rules to your prediction post-processing OR you fix the labels to tie and train on that, hoping that your model learns it. 
> 
> 
> 
> > ## Takamichi TodaTopic Author
> > 
> > Thanks for the reply.
> > 
> > I am going to try to rule it out for now.
> > 
> > 
> > 


---



* --- discussion numver 22, the number of votes :21 ---

# Llama 3.1 has just been released

**lightsource<3** *Wed Jul 24 2024 00:08:21 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (21 votes)

There is 8b version weights: [https://llama.meta.com](https://llama.meta.com)

HF: [https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)

Link to a technical article describing the development process: 

[https://scontent.fdxb2-1.fna.fbcdn.net/v/t39.2365-6/452256780_3788187148167392_9020150332553839453_n.pdf?_nc_cat=103&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=XG3_BvYG0wwQ7kNvgEI9-4V&_nc_ht=scontent.fdxb2-1.fna&oh=00_AYAmG3EQLSTDlGlgdUqlvT6Z0uNBXoQcm_bCMhlFzDJ96A&oe=66A5A0DC](url)



---

 # Comments from other users

> ## Nicholas Broad
> 
> If anyone wants to use it in kaggle offline, it requires a newer transformers, and it is already a part of my [offline dataset for hugging face libraries](https://www.kaggle.com/datasets/nbroad/hf-libraries)
> 
> ```
> !pip install --no-deps --no-index /kaggle/input/hf-libraries/transformers/transformers-4.43.1-py3-none-any.whl
> 
> ```
> 
> 
> 
> > ## SAY WHAT
> > 
> > Thank you.
> > 
> > what is the version of torch?
> > 
> > 
> > 
> > > ## Nicholas Broad
> > > 
> > > you can use default in notebook
> > > 
> > > 
> > > 
> > ## YingxiZhang
> > 
> > Thanks for the reminder.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> 
> 
> Can't stop, won't stop.
> 
> 
> 
> > ## Valentin Werner
> > 
> > Thank god Mistral-Large 2 is closed source
> > 
> > 
> > 


---

> ## aadiAR
> 
> Thankyou for informing !
> 
> 
> 


---

> ## Kishan Vavdara
> 
> 
> 
> 
> 


---

> ## Taimo
> 
> 4bit 8B model has been uploaded by unsloth!
> 
> Base model:
> 
> [https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-bnb-4bit)
> 
> Instruct model:
> 
> [https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct](https://huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct)
> 
> 
> 
> > ## Rishit Jakharia
> > 
> > Thanks ! For the information 
> > 
> > 
> > 


---

> ## Weiren
> 
> Currently training. Looking at the loss plot for a few steps, doesn't seem to outperform Gemma-2â€¦. Maybe just my hyper params isn't good enough.ðŸ¤¡
> 
> 
> 
> > ## Rishit Jakharia
> > 
> > Please keep us updated if you plan to tune llama 3.1, also what quantization and config are you using, if I may know
> > 
> > 
> > 
> > ## Ivan Vybornov
> > 
> > I've attempted to train it 3 times by now and it does not even seem to outperform llama3 or at least it is not better by a margin.
> > 
> > 
> > 
> > > ## justin1357
> > > 
> > > same, worse than llama3
> > > 
> > > 
> > > 


---

> ## Robert0921
> 
> training, and testing
> 
> 
> 


---

> ## Muhammad Anas
> 
> Sounds great
> 
> 
> 


---

> ## SAY WHAT
> 
> There seems to be some problems with loading.
> 
> Anyway, let the bullets fly.
> 
> 
> 


---



* --- discussion numver 23, the number of votes :19 ---

# Data Understanding: Why prompt is list of strings?

**Siddhantoon** *Mon May 06 2024 20:36:28 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (19 votes)

| prompt examples |
| --- |
| ["Is it morally right to try to have a certain percentage of females on managerial positions?","OK, does pineapple belong on a pizza? Relax and give me fun answer."] |
| ["hey","write \"lollipop\" reversed"] |
| ["What's the difference between a sine and a line?","can you explain it even simpilier to me using examples?","how does a sine keep going or whats an analogy using sine the expresses a continuation?","What if AI becomes better than humans at everything. Maybe come up with an analogy involving geometry, thanks"] |

For some the output of model a and b is also list of 2 strings for some it is single string.



---

 # Comments from other users

> ## steubk
> 
> 87% of train samples are chats with single prompt, while others have more prompts and responses
> 
> 
> 


---

> ## namtran
> 
> Thank you for your finding. I will try to extract individual conversations and see if it improves the model.
> 
> 
> 


---

> ## Valentin Werner
> 
> I recommend playing around with the tool in general. This might also gives you a better feeling for the data and competition in general!
> 
> The answer is pretty simple: You are not evaluating individual prompts, but full chats.
> 
> While this opens a new question of "What happens if you evaluate a chat after every prompt" (which is possible) - I don't think it matters for the competition and assume that the data provided is always until the first evaluation.
> 
> 
> 
> > ## SiddhantoonTopic Author
> > 
> > So actually we aren't evaluating a "prompt and response", technically we are evaluating a "chat". This increases the complexity on how long the chat is in the data
> > 
> > 
> > 


---

> ## Rich Olson
> 
> great find.  looking through the data - it seems like this is very common.  
> 
> often the prompts seem disconnected from each-other - but sometimes they are clearly a continuing conversation.
> 
> 
> 


---

> ## Sparsh Tewatia
> 
> Even data is corrupt for around 200 rows, some null values , syntax errors. Will have to check for it in the test data
> 
> 
> 


---



* --- discussion numver 24, the number of votes :18 ---

# This Competition has an Official Discord Channel

**Myles O'Neill** *Fri May 03 2024 04:35:29 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (18 votes)

In addition to this competition forum, you can continue the discussion in our official Kaggle Discord Server here: 

# [discord.gg/kaggle](http://discord.gg/kaggle)

The Discord is a great place to ask getting started questions, chat about the nuances of this competition, and connect with potential team mates. Learn more about Discord at our [announcement here](https://www.kaggle.com/discussions/general/429933). Here are a few things to keep in mind though:

1. Discord Competition Channels are 'Public'

Discord channels for specific competitions are considered 'public' spaces where you are allowed to talk about competition details (it will not count as private sharing).

2. Discord Competition Channels are Not Monitored by Staff

Kaggle Staff and Hosts running competitions will not monitor Discord or be available to answer questions in Discord. Always post important questions in the forums.

3. Keep the Good Stuff on the Forums

Please keep important questions, insights, writeups, and other valuable conversation on the Kaggle forums. Discord is intended to be a more casual space to discuss competitions and help each other, we want to keep all the best information on the forums.

4. Remember to never privately share competition code or data

Please remember that private sharing of competition code or data is, as always, not permitted. Code sharing must always be done publicly through the Kaggle forums/notebooks.

I hope youâ€™ll join us to chat on Discord soon!



---

 # Comments from other users

> ## Abel Punnoose
> 
> The best place to learn RLHF
> 
> 
> 


---

> ## Othmane Manessouri
> 
> Even though i'm still on my learning process i'll try to get better at this
> 
> 
> 


---

> ## kartikey bartwal
> 
> Thank You. I absolutely love Kaggle's collaboration mindset ðŸ™Œ
> 
> 
> 
> > ## Yuwen He
> > 
> > I agree, I can learn a lot here
> > 
> > 
> > 


---



* --- discussion numver 25, the number of votes :17 ---

# More Interesting Observations to Share

**AbaoJiang** *Thu May 30 2024 01:06:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (17 votes)

Hi everyone,

Continuing [the previous discussion](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/508200), we want to share some more interesting observations.

### Empty Prompts

We all know that there exist missing responses, either None or empty (now  detected with regex ^\s*$). Today, we observe there are 5 samples with at least one empty prompt present during the conversation,

[](https://postimg.cc/q6hgRT8P)

Most of the time, models can continue to respond normally even if an empty prompt is sent by users. Another finding is that some models will throw an error message if an empty prompt is sent,

### Winner is the Model with an Missing Response

For a single-turn conversation, we expect the winner to be the one with a non-missing response. However, there's an interesting sample in which the winner is the model with an empty response, "" . Looking into the prompt, we realize what's going on! The prompt says Please reply with â€œ<|endoftext|>â€.

[](https://postimg.cc/GB9kYHnN)

That's all, happy kaggling!



---

 # Comments from other users

> ## Hafiz Nouman
> 
> Thanks for sharing this valuable information with references.
> 
> 
> 


---



* --- discussion numver 26, the number of votes :17 ---

# Less than 1.3 seconds per inference?

**Rishiraj Acharya** *Fri May 03 2024 14:21:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (17 votes)

There are approximately 25000 rows in test data and 9 hours runtime translates to less than 1.3 seconds per inference. Does this make usage of Large Language Models obsolete for this competition? I might not know of any LLM that runs this fast but I'm open to learning.



---

 # Comments from other users

> ## Raja Biswas
> 
> For my subs, the inference runtimes were as below (T4 x2):
> 
> - deberta-v3-large (~1.5hrs)
> 
> - mistral 7b (~4hr)
> 
> - llama3 8b (~4hr)
> 
> Max sequence length used: 1.8k
> 
> 
> 


---

> ## Siddhantoon
> 
> You can even batch process the data, why run every row sequentially.
> 
> 
> 


---

> ## Fritz Cremer
> 
> I published a deberta-v3-base notebook which predicts in under an hour. I think even deberta-v3-large should be no problem:
> 
> [https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline](https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline)
> 
> 
> 


---

> ## Angela
> 
> You are right. It seems that it is unable to utilize prompt engineering for LLM in this competition. 
> 
> 
> 


---



* --- discussion numver 27, the number of votes :17 ---

# How to work with Gemma Keras 1.1_7b instruct _en WITHOUT Google Cloud? On the 1.1_2b_instruct_en No Memory issue.

**MarÃ­lia Prata** *Fri May 10 2024 10:43:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (17 votes)

I'm facing some memory issue with Gemma Keras 1.1 -7b- instruct-en.  It appeared that message "Your notebook tried to allocate more memory than is available. It has restarted".   Go to Google Cloud or dismiss it.

I even ran:

os.environ["XLA_PYTHON_CLIENT_PREALLOCATE"]="false"

  os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"]=".XX"

  os.environ["XLA_PYTHON_CLIENT_ALLOCATOR"]="platform"

Besides, I reduced the number of rows.

When I ran GemmaCausalLM the message that "The notebook tried to allocate more memory than is available" popped-up.

# Is there a way to work with Gemma Keras 1.1- 7b instruct -en WITHOUT  Google Cloud?

For the record, that doesn't occur on the other 7b models (7 billion parameters) that were pinned on this LMSYS competition.

Fortunately, I found Awsaf's code and published my 1st (Gemma 1.1-7b-instruct-en just 34 min. ago on May 10, 2024)

[Gemma 1.1 7B Int8 Load](https://www.kaggle.com/code/awsaf49/gemma-1-1-7b-int8-load) By Awsaf.

Thanks in advance,

MarÃ­lia. 



---

 # Comments from other users

> ## Adnan Alaref
> 
> Hi [@mpwolke](https://www.kaggle.com/mpwolke) ,try to reduce batch size,restart the kernel
> 
> 
> 
> > ## MarÃ­lia PrataTopic Author
> > 
> > My  batch_size = 1    Could it be lower? Zero or negative ðŸ˜†
> > 
> > 
> > 


---

> ## Kaizhao Liang
> 
> I don't think we could load any pretrained model bigger than 1B, since the RAM runs out.
> 
> 
> 
> > ## MarÃ­lia PrataTopic Author
> > 
> > I don't know how the model works on a submission due to its memory. However, I was facing issues even without submitting. Just at the beginning of the code.
> > 
> > Fortunately, I found Awsaf's code: [Gemma 1.1 7B Int8 Load](https://www.kaggle.com/code/awsaf49/gemma-1-1-7b-int8-load) and published my 1st (Gemma 1.1-7b-instruct-en just 34 min. ago)
> > 
> > Thank you Kaizhao.
> > 
> > 
> > 


---

> ## Matin Mahmoudi âœ¨
> 
> Try reducing the batch size, using mixed precision (float16), or lowering the memory fraction to handle Gemma Keras 1.1-7b. If that doesn't work, maybe go for a smaller model or use gradient accumulation [@mpwolke](https://www.kaggle.com/mpwolke).
> 
> 
> 
> > ## MarÃ­lia PrataTopic Author
> > 
> > Hi Matin,
> > 
> > The batch size is only 1.
> > 
> > I changed to Gemma Keras 1.1_2b_instruct_en to reach at the end of the code (instead of the 7b).
> > 
> > Though the hosts pinned the 7b.
> > 
> > Thank you.
> > 
> > 
> > 


---



* --- discussion numver 28, the number of votes :16 ---

# Interpreting the metric & why current baselines are basically guessing

**Valentin Werner** *Tue May 07 2024 23:54:57 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (16 votes)

As all currently available baselines have a score > 1.0, I wanted to explore how to interprete this.

You can find my exploration in this notebook: [https://www.kaggle.com/code/valentinwerner/log-loss-what-are-good-scores/notebook](https://www.kaggle.com/code/valentinwerner/log-loss-what-are-good-scores/notebook)

Note that I am making some assumptions for simplicity, such as assuming that bad predictions are evenly distributed (e.g., instead of prediction [1,0,0] you are prediting [0.3, 0.35, 0.35] or [0.2, 0.4, 0.4]).

What I noticed it that:

- accuracy and loss are strongly misaligned and I am making the assumption that well calibrated models will get us a long way until we are able to achieve very good accuracy (which I assume is hard to do for this problem).

- current solutions are all basically guessing, as you can see in the graph below



---

 # Comments from other users

> ## bogoconic1
> 
> I feel that one contributing factor is
> 
> - The user doesnâ€™t understand the responses from the LLM. It can arise from lack of domain knowledge of the topic he/she is trying to ask.
> 
> How does the user know if the answer is good in this case, or which one is better ? From personal experience, I have asked and seen LLM responses like this and I donâ€™t know how to rate them
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Agreed, I showed the tool many of my colleagues and there is a 50/50 chance that we disagree on what the better answer is. This makes it quite interesting. However, if you look at it in terms of individual model winrates, it must be possible to get scores well above the current levels
> > 
> > 
> > 
> > ## Dr. Gopalashivabalasubramanium Chandrashekaran
> > 
> > Similar to what Valentin said. The win rate will speak for itself for some models. Your thought on the subjective understanding of the user prompting is completely valid. It is an unknown. But maybe a weight can be applied to user prompt data based on grammar/length.
> > 
> > 
> > 


---



* --- discussion numver 29, the number of votes :15 ---

# Get started here!

**Addison Howard** *Tue Apr 09 2024 05:28:18 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (15 votes)

New to machine learning and data science? No question is too basic or too simple. Feel free to start your own thread, or use this thread as a place to post any first-timer clarifying questions for the Kaggle community to help you with!

New to Kaggle? Take a look at a few videos to learn a bit more about [site etiquette](https://www.youtube.com/watch?v=aIus8si_Et0), [Kaggle lingo](https://www.youtube.com/watch?v=sEJHyuWKd-s), and [how to enter a competition using Kaggle Notebooks](https://www.youtube.com/watch?&v=GJBOMWpLpTQ). Publish and share your [models on Kaggle Models](https://www.kaggle.com/docs/models#publishing-a-model)!

Looking for a team? Express your interest in joining a team through our [Team Up](https://www.kaggle.com/discussions/product-feedback/341195) feature .

Remember: Kaggle is for everyone. Whether you're teaming up or sharing tips in the competition forum, we expect everyone to follow our Kaggle community guidelines.



---

 # Comments from other users

> ## Roschild.Rui
> 
> Hi i am at a loss -> Why was it possible to submit almost identical inference weights and inference logic without any issues a few days ago, but recently there have been constant submission errors? It seems that the imbalance in Kaggle's computational resource load significantly affects the competition submissions. I would greatly appreciate it if you could answer my question.@Addison Howard
> 
> 
> 
> > ## Cody_Null
> > 
> > Yes, we have much better submissions we arenâ€™t able to get working that were working only a few days ago. An idea of what is going on with the back end of things would be great!
> > 
> > 
> > 


---

> ## Yuliana Setianingrum
> 
> HAY EVERYONE !! Let's Enjoy Our journey in the exciting world of machine learning and data science!
> 
> 
> 


---

> ## Riad
> 
> hi all ðŸ˜„
> 
> is there anyone want to make a group with me we can help each other to make better models
> 
> my linkdin :- [https://www.linkedin.com/in/riad-almadani-a735a01bb/](https://www.linkedin.com/in/riad-almadani-a735a01bb/)
> 
> 
> 


---

> ## ascender1729.eth
> 
> Excited for my first competition, any beginner tips?
> 
> 
> 


---

> ## Eetu Rantala
> 
> Hi everyone! I'm enjoying Fast.ai practical course which led me to try out kaggle competitions. I hope I can get ok result on this competition with my skills! :) 
> 
> 
> 


---

> ## Kumar Deblin
> 
> Hi Every one!
> 
> 
> 


---

> ## Vishal Maurya
> 
> Kaggle provides CPU and GPU while submission and scoring of notebooks. Just want to know, can we use TPUs? 
> 
> 
> 


---



* --- discussion numver 30, the number of votes :13 ---

# Feel overwhelmed with this competition  

**ducnh279** *Thu Jul 04 2024 04:37:33 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (13 votes)

Hi everyone,

Unlike other NLP competitions I've participated in, I believe that decoder-only models might outperform DeBERTa in this one. Running experiments with LLMs is very computationally and financially expensive for me, especially in this competition.

- For DeBERTa (large), I can manage to get LB: 0.993 (tuned) with 6-hour training using 2 x T4 on Kaggle

- For LLMs,  I've just run only one experiment with Mistral 7B (4-bit quantized + LoRA) and got LB: 0.991. 

For fine-tuning LLMs, one experiment is very slow and expensive to finish one fold in 15 hours with 1 A10G on Lightning Studios. If there is no special magic to get 0.9 to 0.95. I believe that by tuning (batch size, learning rate, warm-up steps, prompts), trying training tricks to stablize the training and avoid early performance saturation, or simply being able to run more than 1 epoch, I think I could get closer to LB: <= 0.95.

As a student, I find Kaggle competitions increasingly challenging and computationally expensive, particularly due to the limited access to free hardware. Relying on free GPUs from Kaggle and Colab, I often feel constrained and overwhelmed when competing.

Do you think I need to invest big bucks for this competition to pay off? As a broke student, I might have to hit up the Bank of Mom and Dad for a 'strategic investment'  hahaðŸ˜‚



---

 # Comments from other users

> ## kagglethebest
> 
> Same feeling. ðŸ˜‚ I am trying to find a way to get nice score by using Deberta Base on Kaggle GPUs. If my trials are not worked, I will give up this competition.
> 
> 
> 
> > ## ducnh279Topic Author
> > 
> > Don't give up [@judith007](https://www.kaggle.com/judith007)! Let's fight until the last minute!
> > 
> > By the way, I recommend trying DeBERTa large and learning how to utilize two GPUs. I achieved a 0.988 score with DeBERTa large, which is the same score as the first public notebook using the 8-bit quantized LLaMA 8B.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> 
> 
> 
> 
> > ## Valentin Werner
> > 
> > Jokes aside, always prefer training models on hosted rental services rather than buying an expensive GPU. You can first validate on the slow kaggle GPUs / TPU or Google Collab etc. before going to the rental. The math for buying a 3090TI / 4080 / 4090 for Data Science it is not really mathing. I have a 4090 which is great for experiments, but I still cannot scale to the same experiments as the Kaggle TPU on it. 
> > 
> > It feels really bad being gates by compute resources. Stuff you can try out if renting is not an option: Some cloud providers provide research compute for limited time; you can ask your university / professors if they have compute you can do (maybe try to sell it as extra curicular, present your results in the end for some bonus points; my university had a 4x V100 setup with 128GB total that was mostly idling and my professor almost begged me to train some stuff on there so its used when nobody does research); 
> > 
> > 
> > 
> > > ## ducnh279Topic Author
> > > 
> > > Hahaha your meme tells my story! 
> > > 
> > > After this competition, I would learn about TPU training! Thanks SO much for sharing your experience!
> > > 
> > > 
> > > 


---

> ## Cody_Null
> 
> Glad someone else was able to get 7b models like mistral working in 4bit. Mine had a bug but didnâ€™t seem like it was going to beat the llama models anyway :/ I understand your position though. It does feel that way sometime 
> 
> 
> 
> > ## ducnh279Topic Author
> > 
> > Thanks for your understanding! For sure, 7b models quantized in 4-bit will be definitely degraded in performance. You can use scaling law to set up the hyparams and try training techniques before running on the whole training set. Sorry I can't talk more about this before the competition ends.
> > 
> > 
> > 


---

> ## Taimo
> 
> Kaggle is a good starting point for students.
> 
> For educational purposes, Kaggle should remain such a place even though the size of models continues to be big. 
> 
> Google (Alphabet) should invest in more high-spec hardware for Kaggle.
> 
> 
> 
> > ## ducnh279Topic Author
> > 
> > For educational purposes, Kaggle should remain such a place even though the size of models continues to be big.
> > 
> > Agreed! I learned a lot through Kaggle competitions and the sharings from Kagglers! The "large" in models and datasets are not a too big problem with me! I will definitely continue learning and competing.
> > 
> > Google (Alphabet) should invest in more high-spec hardware for Kaggle.
> > 
> > We all hope so! hahaha
> > 
> > 
> > 


---

> ## xiaotingting
> 
> After I became a graduate student, I was fine and could use the server in the lab. But because I had to submit a paper recently and needed to do additional experiments, and there were other people in the lab using the server, I could only use it when they were not using it. If I want to fine-tune a large model, I really need a card. I currently rent two A100 cards to prepare for the experiments here, and each training takes at least two days. It is more cost-effective to rent it for the whole day, about 200 yuan for two cards a day, and it costs more than a thousand yuan to rent it for a week.
> 
> 
> 
> > ## KeShuang Liu
> > 
> > I was interning at the company and they provided me with two A800s, but due to my technical issues, I was unable to achieve good results.
> > 
> > 
> > 


---



* --- discussion numver 31, the number of votes :13 ---

# CV vs LB thread

**Stochoshi G** *Sat May 04 2024 12:56:33 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (13 votes)

Deberta-v3-xsmall: ~1.02 cv / 1.03 lb

Tfidf: ~1.02 cv / ~1.03 lb

Combined: ~1.00 cv / tbd

Siamese Deberta: [https://www.kaggle.com/code/stochoshi/deberta-starter-lmsys](https://www.kaggle.com/code/stochoshi/deberta-starter-lmsys)



---

 # Comments from other users

> ## Kishan Vavdara
> 
> My CV :
> 
>  1.02 Lb -> LB - 0.996
> 
>  0.98 Lb -> LB - 0.971
> 
>  0.96 Lb -> LB - 0.955
> 
>  0.95 Lb -> LB - 0.959
> 
> 
> 


---

> ## Takamichi Toda
> 
> My CV strategy is one hold-out, with a validation rate of 20%.
> 
> At present, it correlates well with the Public LB.
> 
> Best: Local=1.034074/Public LB1.036
> 
> 
> 
> > ## heartkilla
> > 
> > hi, you use random or stratified split?
> > 
> > 
> > 
> > > ## Takamichi Toda
> > > 
> > > random split
> > > 
> > > 
> > > 


---

> ## heartkilla
> 
> Any updates on this?
> 
> 
> 


---



* --- discussion numver 32, the number of votes :11 ---

# Expectation for Kaggle Resources?

**Cody_Null** *Thu Aug 01 2024 08:16:38 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (11 votes)

Hi all, my team as well as what seems like others seem to be struggling with submission times. Even submissions that we ran in 6 hours before are now timing out? We have significantly raised our CV in the last few days but have been entirely unable to get results for it due to this compute issue. I was hoping some of the Kaggle staff could help speak to what is going on? I am familiar that close to the end of competitions in the last 2 or 3 days things run a bit slower but we are experiencing 2 times longer inference time than 4 days ago and have been ever since 7 days to go! It would be a shame to put in all this work over the last several months and not get to benefit from putting it all together. 



---

 # Comments from other users

> ## Valentin Werner
> 
> Are you 100% sure you are running exactly the same script? We have been struggling with Resources too, but not along the lines of 50%. Eventhough this wastes a submission, if you have time for it, try the old notebook and version again.
> 
> To me it does not seem reasonable that participation rates raise GPU Processing times. It is not like we are all on the same two T4s. Also all the environments are containers, so they should be clean on every run.
> 
> 
> 
> > ## Cody_NullTopic Author
> > 
> > I totally agree, it is not completely identical but it is a small change and identical in simulated run times. Itâ€™s not quite a 50% slowdown but itâ€™s fairly close! What once took 5.5 is now 9+
> > 
> > 
> > 
> > ## Rise_Hand
> > 
> > It's not same as container actually. Some sub obviously need more computer source can finish running faster than the one which need less. So it's just a luck game to be allocated to a better machine tbh
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Agreed, over the lifecycle of every chip performance may vary. This also goes for room temperature at servers etc. 
> > > 
> > > The point, I am making about containerization is that when you submission is over you have a clean GPU, its not like you will share compute etc. - However, you can and will get worse chips. Its impossible for the kaggle team to have all GPUs available perform exactly the same. 
> > > 
> > > 
> > > 


---

> ## yechenzhi1
> 
> Try to reduce your max-length? I've encountered a similar issue, and I suspect that the test data may be longer than what we used for training.
> 
> 
> 


---

> ## Attacker
> 
> People's participation rate rises before the competition closes, so the server becomes unstable.
> 
> 
> 
> > ## Cody_NullTopic Author
> > 
> > That is true but I donâ€™t notice anything different from this comp than others but I have never seen a 50% slowdown of submissions before :/ Of course we canâ€™t expect Kaggle to have it be seamless especially when they are providing these GPUs for free but I would like a little more clarity on what is going on and if we should expect it to change before the deadline.
> > 
> > 
> > 


---

> ## Korey Ma
> 
> I am afraid that my new submissions will time outðŸ« 
> 
> 
> 
> > ## Valentin Werner
> > 
> > 
> > 
> > 
> > 


---



* --- discussion numver 33, the number of votes :10 ---

# Has anyone tried lora variants like lora+, rslora, dora, and the latest lora-ga, lora-pro, and do they work better ?

**ShelterW** *Tue Jul 30 2024 04:27:50 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (10 votes)

I hear they're closer to full fine-tuning, so has anyone tried it in this competition? I tried dora and it worked just as well.

### Refer:

[1] LoRA: Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., â€¦ & Chen, W. (2021). Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.

[2] LoRA+: Hayou, S., Ghosh, N., & Yu, B. (2024). LoRA+: Efficient Low Rank Adaptation of Large Models. arXiv preprint arXiv:2402.12354.

[3] VeRA: Kopiczko, D. J., Blankevoort, T., & Asano, Y. M. (2023). Vera: Vector-based random matrix adaptation. arXiv preprint arXiv:2310.11454.

[4] LoRA-FA: Zhang, L., Zhang, L., Shi, S., Chu, X., & Li, B. (2023). Lora-fa: Memory-efficient low-rank adaptation for large language models fine-tuning. arXiv preprint arXiv:2308.03303.

[5] LoRA-drop: Zhou, H., Lu, X., Xu, W., Zhu, C., & Zhao, T. (2024). LoRA-drop: Efficient LoRA Parameter Pruning based on Output Evaluation. arXiv preprint arXiv:2402.07721.

[6] AdaLoRA: Zhang, Q., Chen, M., Bukharin, A., He, P., Cheng, Y., Chen, W., & Zhao, T. (2023). Adaptive budget allocation for parameter-efficient fine-tuning. arXiv preprint arXiv:2303.10512.

[7] DoRA: Liu, S. Y., Wang, C. Y., Yin, H., Molchanov, P., Wang, Y. C. F., Cheng, K. T., & Chen, M. H. (2024). DoRA: Weight-Decomposed Low-Rank Adaptation. arXiv preprint arXiv:2402.09353.

[8] Delta-LoRA: Zi, B., Qi, X., Wang, L., Wang, J., Wong, K. F., & Zhang, L. (2023). Delta-lora: Fine-tuning high-rank parameters with the delta of low-rank matrices. arXiv preprint arXiv:2309.02411.

[9] LoRA-GA: Wang, S., Yu, L., & Li, J. (2024). LoRA-GA: Low-Rank Adaptation with Gradient Approximation. arXiv preprint arXiv:2407.05000.

[10] LoRA-Pro: Wang, Z., & Liang, J. (2024). LoRA-Pro: Are Low-Rank Adapters Properly Optimized?. arXiv preprint arXiv:2407.18242.



---

 # Comments from other users

> ## James Day
> 
> I tried several of them with marginal success:
> 
> - LoRA+ - Seemed to converge faster earlier in training, but didn't make any difference after processing tens of thousands of examples (converged  to equally good model). Might be helpful for small datasets, but doesn't seem too useful for this competition.
> 
> - DoRA - Saw slight accuracy improvement vs. LoRA in early experiment back before I was tuning all linear layers, albeit at the expense of a 2x increase in training runtime. After various other changes (tuning all linear layers + some other things that might have a performance impact), the slowdown from using DoRA is even bigger, like 20x. Had to abandon this because my hardware isn't powerful enough to properly train even a single model with DoRA before the end of the competition and the gain would probably be small.
> 
> - AdaLoRA - Seemed to converge slower than normal LoRA (worse sample efficiency). Final model was worse than traditional LoRA baseline. Might be possible to get it working well with additional hyperparameter tuning, but continuing to experiment with this didn't seem like a good use of GPU time.
> 
> 
> 
> > ## CPMP
> > 
> > I tried DORA and have similar experience: 2x slowdown for no clear gain.
> > 
> > 
> > 


---

> ## Ashwani
> 
> I tried using RSLORA, DORA. 
> 
> RSLORA gave me performance improvements. 
> 
> DORA was taking way too much time (8-9x) hence didn't use it. 
> 
> 
> 
> > ## justin1357
> > 
> > In my exp, rs-lora did not help
> > 
> > 
> > 


---

> ## justin1357
> 
> LoRA-GA is the best in theory, it simulates the grad of finetune. But in the case of large dataset, all the methods are almost the same.
> 
> 
> 


---



* --- discussion numver 34, the number of votes :10 ---

# Llama3.1 works not as good as expect

**justin1357** *Wed Jul 24 2024 13:44:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (10 votes)

After first 1000 steps, i found its performence is worse than gemma-2 slightly.



---

 # Comments from other users

> ## Rise_Hand
> 
> I got a very bad result according to llama3.1 while using the ft method of QLoRA+SFT. CV = 0.914
> 
> 
> 


---

> ## Nicholas Broad
> 
> Make sure your tokenizer uses the [correct bos token](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct/discussions/29)
> 
> 
> 
> > ## justin1357Topic Author
> > 
> > still bad performance
> > 
> > 
> > 


---

> ## Xinyuan Qiao
> 
> I tested it in [@emiz6413](https://www.kaggle.com/emiz6413) notebook with exact same parameter, the evaluation log loss is 0.958, the gemma2 version was 0.927.
> 
> 
> 


---

> ## sayoulala
> 
> Thanks for share it. That's great, it looks like I'll be able to save a lot on my electricity bill, hahaha!
> 
> 
> 
> > ## william.wu
> > 
> > You're safe for the 1st place. It's tough to make improvementsðŸ˜­
> > 
> > 
> > 


---

> ## justin1357Topic Author
> 
> After 4000 steps, its significantly worse than gemma-2 haha
> 
> 
> 


---

> ## Lorry Zou
> 
> Thank you for saving our time and TPU quotaðŸ˜
> 
> 
> 


---

> ## Yixiao Yuan
> 
> Same here.
> 
> 
> 


---



* --- discussion numver 35, the number of votes :10 ---

# How long does it tke to be granted access to Llama3 on Kaggle? [Solved: 24 hours]

**CPMP** *Thu Jul 11 2024 19:49:54 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (10 votes)

I got access immediately yesterday on Meta's site, but my request is still pending here.

Edit: I got access after 24 hours, which is reasonable. It is just that I got access on Meta immediately which sets optimistic expectations.



---

 # Comments from other users

> ## samson
> 
> Six days left for me. Access is not granted still. Thats why I got weights on HF
> 
> 
> 
> > ## CPMPTopic Author
> > 
> > Did you get access from Meta's site before asking for access here?
> > 
> > 
> > 
> > > ## samson
> > > 
> > > Yes, I did. BTW, I finally go it. It took them 7 days to give me an access
> > > 
> > > 
> > > 
> > > ## william.wu
> > > 
> > > OMG, I just requested the access yesterdayâ€¦ Is there any public llama3.1 models on Kaggle?
> > > 
> > > 
> > > 
> > > ## william.wu
> > > 
> > > Okay, I found one [https://www.kaggle.com/datasets/gmhost/llama31instruct](https://www.kaggle.com/datasets/gmhost/llama31instruct)
> > > 
> > > 
> > > 


---

> ## Allie K.
> 
> Good question. 
> 
> I have been waiting since last Friday morning, constantly updating the number of days of waiting (now it's 6) and repeating my questions to Kaggle team.
> 
> Perhaps you, with your authority, will be able to push things forward.ðŸ™‚ 
> 
> 
> 
> > ## CPMPTopic Author
> > 
> > I don't think I have more weight than anyone here.
> > 
> > 
> > 
> > > ## Allie K.
> > > 
> > > Apparently you do have!ðŸ˜€
> > > 
> > > As a magic, the access is here, after "only" 6 days.
> > > 
> > > 
> > > 
> > > ## CPMPTopic Author
> > > 
> > > Did you post? If yes then I will agree with you.
> > > 
> > > 
> > > 


---

> ## Psi
> 
> you can get it on HF, no need to apply here on kaggle
> 
> 
> 
> > ## CPMPTopic Author
> > 
> > I have access on HF and on Meta. I am asking why it is long here.
> > 
> > 
> > 


---

> ## RB
> 
> Hello , similar post here - [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518813](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518813) 
> 
> and a workaround  [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518813#2913166](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518813#2913166)
> 
> 
> 
> > ## CPMPTopic Author
> > 
> > I downloaded the model, no worries. I am asking a specific question.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> Welcome to the competition!
> 
> I already posted this before, but took 10 minutes for me a weekend.. 
> 
> 
> 


---

> ## Shelton
> 
> Nice work!
> 
> 
> 


---

> ## Nguyá»…n Anh TÃº
> 
> Why i always get the error "Incorrect path_or_model_id: '/kaggle/input/llama-3/transformers/8b-hf/1'. Please provide either the path to a local folder or the repo_id of a model on the Hub" when using "tokenizer = AutoTokenizer.from_pretrained("/kaggle/input/llama-3/transformers/8b-hf/1")" ? Please helps me. Thanks.
> 
> 
> 
> > ## Valentin Werner
> > 
> > You have not been granted access for llama3 on Kaggle yet. Make sure to apply for access on meta and kaggle.
> > 
> > 
> > 
> > > ## Nguyá»…n Anh TÃº
> > > 
> > > To solve that problem, I use another pretrain model from my another notebook. But I got the error "Submission Scoring Error" when I submitted my notebook, I thought that I set the wrong format for my submission.csv. Then, I read the sample_submission.csv and change the the value of ['winner_model_a', 'winner_model_b', 'winner_tie'] columns with my y_predict. The worst thing is my notebook ran successful but when I submitted again I got the error "Notebook Threw Exception", please help me!
> > > 
> > > 
> > > 
> > > ## XXX
> > > 
> > > Submission Scoring Error: Your notebook generated a submission file with incorrect format. Some examples causing this are: wrong number of rows or columns, empty values, an incorrect data type for a value, or invalid submission values from what is expected.
> > > 
> > > above is from kaggle debugging tips.
> > > 
> > > I think may be you can check the value of your submissionðŸ¤”
> > > 
> > > 
> > > 


---

> ## Feisx Song
> 
> helpful tips!
> 
> 
> 


---



* --- discussion numver 36, the number of votes :10 ---

# LB Experiment: Modify Prediction Temperature

**Rich Olson** *Wed May 08 2024 10:31:30 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (10 votes)

I just put together a new notebook to see if adjusting the confidence of my predictions can improve LB performance:

[https://www.kaggle.com/code/richolson/lb-experiment-modify-prediction-temperature](https://www.kaggle.com/code/richolson/lb-experiment-modify-prediction-temperature)

The answer seems to be yes (a little).

The model for this notebook is identical to TF-IDF approach I used here:

[https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees](https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees) (LB 1.038)

The notebook works by adjusting the "temperature" of predictions.  Raw scores are divided by the temperature factor before being converted to probabilities.

In this case - increasing the temperature moves predictions closer to .33 (decreasing confidence).

Decreasing the temperature moves scores out towards 0 or 1 (increasing confidence).

I did a bunch of submissions.  Here are the resulting LB scores:

| Temp. Adjustment | LB |
| --- | --- |
| 1.3 | 1.044 |
| 1.0 | 1.038 (unchanged - as expected) |
| 0.85 | 1.036 (improved!) |
| 0.7 | 1.036 (improved!) |
| 0.5 | 1.052 |

So - it seems like the existing confidence of my model was close-to-optimal - but not quite.  Based on the clustering of scores - I doubt there is a lot more improvement to be made.

Adjusting the temperature of your predictions is quite easy:

```
#1. get raw logits
y_pred_raw = model.predict(combined_test_tfidf[-test.shape[0]:], raw_score = True)

#2. adjust temperature
adjusted_logits = y_pred_raw / temperature_factor

#3. convert to probs using softmax (from scipy.special)
preds_test = softmax(adjusted_logits, 1)

```

If this is interesting - you should also check out [@valentinwerner](https://www.kaggle.com/valentinwerner)'s notebook on this topic:

[https://www.kaggle.com/code/valentinwerner/log-loss-what-are-good-scores](https://www.kaggle.com/code/valentinwerner/log-loss-what-are-good-scores)

-Rich



---

 # Comments from other users

> ## Takamichi Toda
> 
> Thank you for suggesting this useful post-processing.
> 
> I also tried this post-processing, and the results were very good!!
> 
> When I looked at the relationship between temperature and score in the validation data, I found that it matched well with the LB results.
> 
> | Temp. Adjustment | LB |
> | --- | --- |
> | 0.8 | 1.036 |
> | 0.9 | 1.028 |
> | 1.0 | 1.025 |
> | 1.2 | 1.022 |
> | 1.4 | 1.024 |
> 
> (The vertical axis is logloss)
> 
> The temperature of 1.2, which had the highest score on the LB, was also close to the best in validation.
> 
> 
> 


---



* --- discussion numver 37, the number of votes :9 ---

# Why are there winners for the same prompt and response?

**JunHua Liao** *Mon May 13 2024 22:47:54 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (9 votes)

Why is prompt, response_a, and response_b the same, and there is a situation where model_a wins or model_b wins? Shouldn't it be winner_tie?



---

 # Comments from other users

> ## Valentin Werner
> 
> Does it make sense? No. Did the user click it? Yes.
> 
> 
> 


---

> ## Sergey Saharovskiy
> 
> [@feattar](https://www.kaggle.com/feattar) thanks for posting your findings, I will leave it here:
> 
> 
> 
> > ## Valentin Werner
> > 
> > 
> > 
> > 
> > 


---

> ## Asher B.
> 
> According to the blog [https://huyenchip.com/2024/02/28/predictive-human-preference.html](https://huyenchip.com/2024/02/28/predictive-human-preference.html)
> 
> shared in this discussion: [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499847](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/499847)
> 
> These are the noises and we may improve our model by droping these instances. Thanks for sharing!
> 
> 
> 
> > ## JunHua LiaoTopic Author
> > 
> > Thanks for sharing!
> > 
> > 
> > 
> > ## Kishan Vavdara
> > 
> > I think dropping them won't help much, test data may contain similar instances. If the model predicts tie for such instances with high prob, then such instances will be penalized more increasing log loss. Solution would be ensembles :)  
> > 
> > 
> > 
> > > ## Asher B.
> > > 
> > > Thanks for correction. I think dropping should be a good idea in production, but in this competition, that's ture! 
> > > 
> > > 
> > > 
> > > ## Valentin Werner
> > > 
> > > I am not sure if I agree - if we are unsure about the test data (much like we would be in producton), shoud we not strive to create a model that is robust, in the sense of predicting the objective truth?
> > > 
> > > It might be worth testing if we should provide more balanced predictions on these labels, like [0.3, 0.2, 0.5] - as first model might be preferred due to position bias - while tie is the objective truth on these labels.
> > > 
> > > 
> > > 


---



* --- discussion numver 38, the number of votes :9 ---

# Prometheus 2 for Evaluating Language Models

**Groz** *Tue May 07 2024 04:01:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (9 votes)

Today, I stumbled upon a GitHub repository that seems tailor-made for this competition. Prometheus 2, available in two sizes, 7B and 8x7B, claims to achieve state-of-the-art results in language model evaluation, Whats really intriguing is its claim that the 7B model outperforms Lama-2 70B and is on par with Mixtral 8x7B.

Unfortunately, I don't have the time to personally verify these claims or participate in the competition. 

Hopfully this comes in handy. Good luck everyone!

Paper: [https://github.com/prometheus-eval/prometheus-eval](https://github.com/prometheus-eval/prometheus-eval)

Offical Implementation [https://github.com/prometheus-eval/prometheus-eval/](https://prometheus-eval.github.io/prometheus-eval/). 



---

 # Comments from other users

> ## Valentin Werner
> 
> Both of the urls you have linked lead to the competition page instead - looking forward to checking out the model!
> 
> 
> 
> > ## GrozTopic Author
> > 
> > Thanks, i adjusted the link accordingly
> > 
> > 
> > 


---

> ## Nicholas Broad
> 
> [https://huggingface.co/prometheus-eval](https://huggingface.co/prometheus-eval)
> 
> 
> 


---



* --- discussion numver 39, the number of votes :8 ---

# [???] Crazy lb 0.707 run in just 1h

**justin1357** *Sun Aug 04 2024 04:34:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (8 votes)

It's impossible to get this score by modeling, is it leak?



---

 # Comments from other users

> ## heartkilla
> 
> It is either a leak or a huge breakthrough in preference modeling and the guy is one step away from creating GPT-6
> 
> 
> 


---

> ## sayoulala
> 
> I believe it's very likely. My training set loss couldn't even reach that score. As of now, his score has risen to 0.6 againâ€¦ I hope Kaggle officials can handle this fairly.
> 
> 
> 


---

> ## sayoulala
> 
> This is incredibly hard to accept.I think there might have been a data leak. Kaggle officials should address this issue; otherwise, it wouldn't be fair.   [@sohier](https://www.kaggle.com/sohier) [@addisonhoward](https://www.kaggle.com/addisonhoward)
> 
> 
> 


---

> ## Psi
> 
> very likely a leak given the score, he had short subs for weeks, now revealing the full leak apparently
> 
> 
> 
> > ## NguyenThanhNhan
> > 
> > i felt pretty demotivated now, after burning so much computes for this comp
> > 
> > 
> > 
> > > ## Psi
> > > 
> > > I feel particularly bad because we reported a leak early in this competition which is why the scoring changed.
> > > 
> > > And now the top scores were all very close approaching similar limits, so an exciting finish was ahead of us.
> > > 
> > > And now thisâ€¦ hope Kaggle can find some way to salvage this somehow. [@sohier](https://www.kaggle.com/sohier) [@addisonhoward](https://www.kaggle.com/addisonhoward) 
> > > 
> > > 
> > > 


---

> ## Ashwani
> 
> I honestly hope its not a leak but some brilliant out of the mind solution. Fingers crossed 
> 
> 
> 


---

> ## Anh Pham
> 
> hilarious :D 
> 
> 
> 


---



* --- discussion numver 40, the number of votes :8 ---

# Data Analysis with Chatbot Arena-like Chat Renderer

**AbaoJiang** *Mon May 27 2024 17:00:34 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (8 votes)

Hi everyone,

This is the first time I join an NLP competition. I'm so excited because I need to learn everything from scratch! The very first step is to analyze the data. To facilitate model comparison side by side, instead of scrolling up and down to analyze responses from two models, I write a simple static chat renderer with Chatbot Arena-like UI (co-author by ChatGPT). Following is a screenshot of one chat,

[](https://postimg.cc/Tyyhq5RC)

This renderer supports,

Pair comparison between responses from two models.
Markdown rendering powered by [<md-block>](https://md-block.verou.me/).
- e.g., strong and italic fonts, unordered and ordered lists, etc.

Unicode rendering.
- Characters like emojis can be shown.

[](https://postimg.cc/VdffWZ1K)

Also, winner is displayed at the bottom! I hope this can make raw text analysis more handy.

In addition, I also implement the win rate and battle count heatmaps in [the official paper](https://arxiv.org/pdf/2403.04132). We can use this to find frequent model pairs (i.e., battle counts) and which model has the higher win rate (e.g., gpt-4-1106-preview has only 17.42% lose rate).

[](https://postimg.cc/ThswTMDB)

For detailed implementation, please refer to [LMSYS - Detailed EDA](https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda/notebook).

I'll share more analysis and insights during this interesting learning journey. Hope you like it!



---

 # Comments from other users

> ## Hafiz Nouman
> 
> Amazing Improvement keep it up 
> 
> Review my dataset and give some suggestions on it how I can improve my work
> 
> 
> 


---



* --- discussion numver 41, the number of votes :8 ---

# Chatbot Arena's Rules

**steubk** *Thu May 09 2024 14:01:52 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (8 votes)

It has been noticed that the train set consists of conversations rather than single prompts and responses. ([https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500633)](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500633)).

These are the rules of the chatbot arena ( [https://chat.lmsys.org/](https://chat.lmsys.org/) ) :

- Ask any question to two anonymous models (e.g., ChatGPT, Claude, Llama) and vote for the better one!

- You can chat for multiple turns until you identify a winner.

- Votes won't be counted if model identities are revealed during the conversation.

As a rough approximation, you might think that the last question has a winner while all the previous ones are ties.

Hope this helps!



---

 # Comments from other users

> ## Valentin Werner
> 
> This is highly important because this means that the last prompt and responses are more valuable than the first ones.
> 
> If you are truncating these information, you should truncate the start, not the end!
> 
> 
> 
> > ## Shreshth Sharma
> > 
> > Ideally, the model difference is generated by the prompt asked first and the response generated at the end. But don't you think that the responses generated in between also preserve important information since if a human makes the final selection, he/she will also consider the thinking process of LLM? Since reaching the right answer with unexplainable thinking itself will result in less credibility of the model.
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Models might be able to learn which prompt and response combination was the one that actually mattered to the user. I can only speak for myself, that I always voted once I was "that response is better", kind of disregarding everything else beforehand
> > > 
> > > 
> > > 


---



* --- discussion numver 42, the number of votes :7 ---

# Time out always

**Rise_Hand** *Wed Jul 31 2024 18:18:40 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (7 votes)

why I always met timeout these day with same infer logic



---

 # Comments from other users

> ## Attacker
> 
> People's participation rate rises before the competition closes, and then the server becomes unstable.
> 
> 
> 


---

> ## Cody_Null
> 
> Yep same thing with our solutions
> 
> 
> 


---

> ## JM
> 
> GPUs running hot and slowing down in last few days probably ðŸ˜…
> 
> 
> 


---

> ## Krupal Patel
> 
> i also facing same problem with LLMs notebooks.
> 
> 
> 


---

> ## justin1357
> 
> Same, a waste of submission times
> 
> 
> 
> > ## justin1357
> > 
> > Each GPU's performance is different, which cause this prob
> > 
> > 
> > 


---

> ## Roschild.Rui
> 
> Hi HAN, our team is also at a loss -> Why was it possible to submit almost identical inference weights and inference logic without any issues a few days ago, but recently there have been constant submission errors? It seems that the imbalance in Kaggle's computational resource load significantly affects the competition submissions.
> 
> 
> 


---

> ## JamshaidSohail
> 
> Guys. My 15 submissions wasted. Are you able to figure it out ? How to get out of it ?
> 
> 
> 


---

> ## hwz13
> 
> æ˜¯çš„ï¼Œgpuè¿è¡Œæ—¶é—´å¤ªé•¿äº†
> 
> 
> 


---



* --- discussion numver 43, the number of votes :7 ---

# Combine Gemma-2 9b & Llama-3 8b

**G John Rao** *Fri Jul 26 2024 17:50:20 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (7 votes)

Hi everyone, 

I have the two highest-scoring LB notebooks together in each device of T4 GPU.

LB: 0.945

[notebook](https://www.kaggle.com/code/jaejohn/lmsys-combine-gemma-2-9b-llama-3-8b)



---

 # Comments from other users

> ## xiaotingting
> 
> The effect is better after integrating the two models, but is there any way to fuse the two models other than directly adding them together?
> 
> 
> 
> > ## G John RaoTopic Author
> > 
> > You can surely explore weighted averaging, stacking, bagging, etc. I am sure it'll improve the predictions. 
> > 
> > 
> > 


---

> ## Akeryu Ryuu
> 
> I tried this and the results weren't that good, lb of 1.15. And with each submission taking about 8~9 hours each, tuning the submission weights to lb is hard so hopefully you have better luck than me.
> 
> 
> 
> > ## Valentin Werner
> > 
> > Results this bad might indicate that you did not align indices properly. You may want to sort by index before ensembling
> > 
> > 
> > 
> > > ## Akeryu Ryuu
> > > 
> > > Thank you for the advice but I don't really believe that's the problem because I was joining the submissions by id before ensembling. 
> > > 
> > > This is the code I used
> > > 
> > > ```
> > > gemma_sub = pd.read_csv("gemma_submission.csv")
> > > llama_sub = pd.read_csv("llama_submission.csv")
> > > 
> > > merged_submission = pd.merge(gemma_sub, llama_sub, on='id', suffixes=("_1", "_2"))
> > > 
> > > merged_submission["winner_model_a"] = (merged_submission["winner_model_a_1"] + merged_submission["winner_model_a_2"])/2
> > > merged_submission["winner_model_b"] = (merged_submission["winner_model_b_1"] + merged_submission["winner_model_b_2"])/2
> > > merged_submission["winner_tie"] = (merged_submission["winner_tie_1"] + merged_submission["winner_tie_2"])/2
> > > 
> > > final_submission = merged_submission[["id", "winner_model_a", "winner_model_b", "winner_tie"]]
> > > 
> > > ```
> > > 
> > > 
> > > 
> > > ## Valentin Werner
> > > 
> > > Did you validate individual performance of these models? Have you maybe mixed up the input format you used for training with another input format? Or maybe you have mixed up the IDs during inference?
> > > 
> > > Mathematically it is EXTREMELY unlike that two models that perform below / at .950 together go to 1.15. In general, to achieve 1.1x your models need to be overconfident in wrong labels.
> > > 
> > > 
> > > 
> > > ## Akeryu Ryuu
> > > 
> > > Thanks to your comment, I decided to double-check my setup. After about half an hour of searching, I discovered that I hadn't loaded the fine-tuned LoRA weights for the Gemma model. It turns out I missed those two lines while copying the code. So, a big thank you for pointing this out.
> > > 
> > > 
> > > 


---

> ## Ravshan Kutkovin
> 
> Can you explain more about Combine Gemma-2 9b & Llama-3 8b?
> 
> 
> 
> > ## G John RaoTopic Author
> > 
> > Another user did a good job explaining everything, here: [https://www.kaggle.com/code/nabojyotipandey/dual-model-inference-gemma2-9b-llama3-8b](https://www.kaggle.com/code/nabojyotipandey/dual-model-inference-gemma2-9b-llama3-8b)
> > 
> > 
> > 


---



* --- discussion numver 44, the number of votes :7 ---

# 7b OOM while 8b works fine, is this strange?

**Cody_Null** *Wed Jun 26 2024 05:48:42 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (7 votes)

I am trying to compare the performance of different base models, for example we can compare base mistral 7B model quantized to 8bit and compare this to the llama 3 8B model also quantized to 8bit. I am noticing I get OOM errors for the 7B model (and others) but not the llama3 8b? I understand they can have different architectures with different memory requirements and that their size is not fully dependent on the number of parameters but just to be sure does anyone else find this strange? 



---

 # Comments from other users

> ## Valentin Werner
> 
> It cannot be due to size - Mistral 7b 8 bit takes 6.87 GB,  Llama 3 8B 8 bit takes 7.05 GB (see: [https://huggingface.co/spaces/hf-accelerate/model-memory-usage)](https://huggingface.co/spaces/hf-accelerate/model-memory-usage)). From what I can see they also have the same hidden sizes and dimensions, so embeddings for Mistral should not take more RAM than for Llama
> 
> Are you getting the error while loading? This might be due to kaggle infrastructure. For fair comparisons you should always load from a freshly restarted environment (as torch.cuda.empty_cache has not the same effect from my experience)
> 
> 
> 
> > ## Cody_NullTopic Author
> > 
> > Glad I am not crazy, I will circle back and try it again today just to double check I have not made some silly mistake. I will update this if I find anything.
> > 
> > 
> > 


---

> ## Cody_NullTopic Author
> 
> Just now realized I totally put this in the wrong thread: 
> 
> Update: I have found the reason. The top here causes an OOM error while the bottom works fine.
> 
> `
> 
> BitsAndBytes configuration
> 
> bnb_config =  BitsAndBytesConfig(
> 
>     load_in_8bit=True,
> 
>     bnb_8bit_compute_dtype=torch.float16,
> 
>     bnb_8bit_use_double_quant=False)
> 
> bnb_config = BitsAndBytesConfig(
> 
>     load_in_8bit=True,
> 
>     bnb_8bit_quant_type="nf8",
> 
>     bnb_8bit_use_double_quant=True,
> 
>     bnb_8bit_compute_dtype=torch.bfloat16)
> 
> `
> 
> 
> 
> > ## Valentin Werner
> > 
> > I was wondering lol 
> > 
> > still got 4 upvotes on the other one ðŸ˜‰
> > 
> > 
> > 
> > > ## Cody_NullTopic Author
> > > 
> > > lol as long as it is useful I guess haha figured I might as well let this side be complete. 
> > > 
> > > 
> > > 


---



* --- discussion numver 45, the number of votes :7 ---

# Additional Data Issues to Note

**AbaoJiang** *Wed May 29 2024 01:05:17 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (7 votes)

Hi everyone,

After more EDA in my [LMSYS - Detailed EDA](https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda?scriptVersionId=180273328), I find additional data issues to report and discuss with you.

### Not Only null But Also Empty Responses

As pointed out in [this forum](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303), we know that there exists null response issue. Here, I want to share another similar issue that responses from models are empty strings. I currently check if there's at least one model response equal to "" during one conversation, and the answer is yes. 

[](https://postimg.cc/75R2GD0h)

As illustrated above, we can observe that,

There exists no samples with None/empty prompts.
There exist 120+ rows with None responses for both A and B.
There exist 30+ rows with empty responses for both A and B.

You can also test strings like space only (e.g., " ", "    "), new line only (e.g., \n), etc.

### Unintentional Mixed Prompts and Responses?

[](https://postimg.cc/F1xDBP2p)

When exploring missing values, we find another interesting chat example shown above. As can be seen, model A gpt-4-0613 responds an empty string, but model B responds normally. Also, the ending of the prompt is the same as the response.

Out of curiosity, we feed the same prompt to Chatbot Arena in two forms,

#### a. Feed this Prompt As Is

[](https://postimg.cc/7fzm9sPK)

#### b. Feed this Prompt without QAs at the End of Prompt

[](https://postimg.cc/qg9pnynr)

As can be seen, model A in the first case might take QA information at the end of the prompt as a response. In the second case, model B provides a similar answer without QA information in the prompt.

So, our questions are

What's the difference between null responses and empty responses?
Is it possible that there exist samples which unintentionally mixes responses into prompts?

Please feel free to share your thoughts on these topics, thanks!



---

 # Comments from other users

> ## dragon zhang
> 
> thanks for sharing.  The score is changing. Is metric changed or test data?
> 
> 
> 
> > ## AbaoJiangTopic Author
> > 
> > Hi, 
> > 
> > There exists data leakage issue, so the solution file is modified and notebooks are re-scored. You can refer to [this forum](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/506137). Hope this helps.
> > 
> > 
> > 


---

> ## Dr. Gopalashivabalasubramanium Chandrashekaran
> 
> Interesting! I was avoiding checking the text data columns because of this. 
> 
> Qualitative response analysis will be tricky. What if the user asked a question the model could not answer such as outside of the regulations that the model is put under? 
> 
> However, since there are 50k+ rows, I would think dropping these null values wouldn't affect training a model too intensely.
> 
> It gives me idea that some type of ensembled model where user prompt is filtered for potential to generate a null response.
> 
> 
> 


---



* --- discussion numver 46, the number of votes :6 ---

# Holy...did I just see a 0.707 on lb?

**Ya Xu** *Sun Aug 04 2024 04:31:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

And that's his only submission today, which means it's most likely took him less than 2 hours to do the inference. Seems someone just completely broke this competition and did it alone.

Good news: just need 48 hours to see the solution. I can't help my curiosity.



---

 # Comments from other users

> ## Cody_Null
> 
> Yeah super strange, must have found the dataset used in public LB by accident? 
> 
> 
> 
> > ## SeshuRaju ðŸ§˜â€â™‚ï¸
> > 
> > 
> > Similar to [PetFinder Competition](https://www.kaggle.com/c/petfinder-adoption-prediction/discussion/125436) [@cody11null](https://www.kaggle.com/cody11null) - i feel this 1st solution will be treated similar way or discard his solution by Kaggle 
> > 
> > I believe, we only have 50 hours left. Instead of panicking, I'll trust that Kaggle will handle it. It's better to focus on making wise choices and selecting the best two submissions.
> > 
> > 
> > 
> > ## Kamau
> > 
> > 
> > must have found the dataset used in public LB by accident?
> > 
> > My thoughts too. I hope Kaggle addresses this asap
> > 
> > 
> > 


---

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> is it happen in any other competitions, its my first time seeing the last min leak at end of competition.
> 
> 
> 
> > ## flg
> > 
> > It is last minute if you hide it. Make some early submissions with only part of the leak used to hide your score, then submit full leak shortly before the end. We probably won't know though until Kaggle checks it ..
> > 
> > 
> > 
> > > ## SeshuRaju ðŸ§˜â€â™‚ï¸
> > > 
> > > Make sense Make some early submissions with only part of the leak used to hide your score [@ferdinandlimburg](https://www.kaggle.com/ferdinandlimburg) 
> > > 
> > > 
> > > 


---



* --- discussion numver 47, the number of votes :6 ---

# Notebook timeout problem 

**JamshaidSohail** *Thu Aug 01 2024 21:47:25 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Hi. The notebook which was working extremely fine in the past few days. Now is timing out on submission. Please, it would be nice to fix this. [@sohier](https://www.kaggle.com/sohier) [@paultimothymooney](https://www.kaggle.com/paultimothymooney) 



---

 # Comments from other users

> ## sayoulala
> 
> I have the same problemâ€¦
> 
> 
> 
> > ## JamshaidSohailTopic Author
> > 
> > Glad to see the response from no.1 in this competition. The submissions are getting wasted and no new ideas are being able to be tested out :/
> > 
> > 
> > 
> > > ## sayoulala
> > > 
> > > Submit a few more times and it should run successfully. It might be that the Kaggle system is not very stable.
> > > 
> > > 
> > > 
> > > ## Roschild.Rui
> > > 
> > > Tried around 10 times, finally success 1 crazy!
> > > 
> > > 
> > > 
> > > ## JamshaidSohailTopic Author
> > > 
> > > Guys. I am still getting timeouts. Have wasted 15 submissions.
> > > 
> > > 
> > > 


---

> ## Attacker
> 
> These days, kaggle seems to be effectively closed a week before the competition endsâ€¦
> 
> 
> 


---



* --- discussion numver 48, the number of votes :6 ---

# How to tokenize prompts and responses efficiently

**irishu** *Sun Jul 28 2024 13:56:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

# Experiment

I have tried the following three methods so far, and the first has performed the best in LB.

### Methods

tokenize up to the max_tokens by joining strings like prompt + responseA + responseB
allocate one-third of the max_tokens to each sentence and tokenize up to the limit
allocate the number of tokens in the appropriate ratio(ex;1:2:2)

### Conditions

- using Gemma-2 9b 4-bit QLoRA

- max_tokens = 1024

- using only the last prompt and responses 

- 1 epoch using all train data

- referring to the excellent work [[Training] Gemma-2 9b 4-bit QLoRA fine-tuning](https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning)

# Question

### Don't you think there is a more efficient way than the simple method1?

Looking at the distribution of the number of tokens in the prompt and response (only the last one), it appears that around 10% of them contain more than 1024 tokens in total. That is, in some cases, response B may not contain enough information.

### How much would a larger max_tokens improve the score?

I have not been able to test this yet due to computational resources.



---

 # Comments from other users

> ## irishuTopic Author
> 
> I changed max_tokens to 2048 in learning and inference and the score improved.
> 
> Now I am wondering if I should adjust the token length with padding.
> 
> 
> 


---



* --- discussion numver 49, the number of votes :6 ---

# Damn Ties! 

**cm391** *Thu Jul 25 2024 05:02:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

The labels used in this competition are assigned by a diverse group of annotators and by its very name preference based. We can see from the following two examples the difficulty with correctly predicting a tie can be tricky.

perhaps Morphalumpaliciousness simply isn't long enough??

two sentences too many??

how you guys dealing with this sort of noise? I cannot seem to get better than random for predicting the drawsâ€¦



---

 # Comments from other users

> ## Hadi Ai
> 
> I wonder if we could tell something about the annotators from their prompts -- and use that in a smaller model just to predict the tie class; then ensemble that with whatever one is doingâ€¦ Anyway not much time left for exploration in this competition :-)
> 
> 
> 


---

> ## justin1357
> 
> Tie is really hard to predictâ€¦ As a human, actually I can't tell when it will be tie as well. Everyone's standard is different and this standard will even change as time went by. I prefer to trust modelâ€¦
> 
> 
> 


---



* --- discussion numver 50, the number of votes :6 ---

# [question]9h time limit 

**Roschild.Rui** *Thu Jul 25 2024 17:26:58 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Will the organizers rerun our submission on the test set, or will we receive a PV score as long as we successfully submit and have an LB score? 

I saw some situations that when the running time was out of 9 hours the notebook was still running and got a lb score



---

 # Comments from other users

> ## Valentin Werner
> 
> Private scores are already calculated during your submission. So if you got a LB score, your submission is fine.
> 
> 
> 


---

> ## Rise_Hand
> 
> Yes, thus we should be very careful to ensemble our models or make any post processing
> 
> 
> 


---



* --- discussion numver 51, the number of votes :6 ---

# CUDA OOM when ensemble Gemma2 and Llama3

**Lorry Zou** *Tue Jul 16 2024 00:39:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Hi everyone, I'm trying to ensemble gemma2 and llama3. My strategy is load data -> load gemma2 model -> gemma2 inference -> load llama3 model -> llama3 inference -> ensemble. I use T4*2 and my code is mainly based on [@kishanvavdara](https://www.kaggle.com/kishanvavdara) 's inference notebook.

My issue is: When I try to load llama3 model after gemma2 inference, I encounter CUDA OOM. I try to clear memory by removing gemmas from the two GPUs (I load one gemma model on each GPU) using gemma_model.cpu(); del gemma_model; torch.cuda.empty_cache(), but it doesn't help. Only GPU 0 is freed and GPU 1 is still using 8.9GB memory. 

Is there any way to release all the memory from both GPUs? Or perhaps reduce of size of the models?



---

 # Comments from other users

> ## no fit just luck
> 
> I would like to share a simple method. You can use '%%writefile' to create a '.py' file and then run this file by "!python file_name.py" to generate your submission. Specifically, you can create two py files for gemma and llama. In each of the file, you can save the model output as a csv file. At last, you can load them and do your ensemble. 
> 
> The key point is that by using  "!python file_name.py", the memory will be clean. Hope this can solve your problem.
> 
> 
> 
> > ## Lorry ZouTopic Author
> > 
> > Yeah I just converted the whole notebook to python script and it works well with releasing memory. I didn't know we can even directly submit a python script LOL.
> > 
> > 
> > 


---

> ## Priyanshu Joshi
> 
> Make sure you are correctly clearing all references to the model and intermediate tensors.
> 
> ```
> import gc
> 
> gemma_model.cpu()
> del gemma_model
> torch.cuda.empty_cache()
> gc.collect()
> 
> ```
> 
> Ensure your inference environment has no other processes using the GPUs. Sometimes background processes can consume significant memory. Use gradient checkpointing to trade computational cost for memory usage. This saves memory by recomputing some parts of the model during the backward pass. Experiment with batch size and max_length as Veletin mentioned in his comment. You can try [model parallelism](https://huggingface.co/docs/transformers/v4.15.0/parallelism).
> 
> 
> 


---

> ## Lorry ZouTopic Author
> 
> I'm wondering why only GPU 0's memory can be released after inference. Maybe only one of the model is actually used during inference? The code:
> 
> `@torch.no_grad()
> 
> [@torch.cuda.amp.autocast](https://www.kaggle.com/torch.cuda.amp.autocast)()
> 
> def gemma_inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
> 
>     a_win, b_win, tie = [], [], []
> 
> ```
> for start_idx in range(0, len(df), batch_size):
>     end_idx = min(start_idx + batch_size, len(df))
>     tmp = df.iloc[start_idx:end_idx]
>     input_ids = tmp["input_ids"].to_list()
>     attention_mask = tmp["attention_mask"].to_list()
>     inputs = pad_without_fast_tokenizer_warning(
>         gemma_tokenizer,
>         {"input_ids": input_ids, "attention_mask": attention_mask},
>         padding=True,
>         max_length=max_length,
>         pad_to_multiple_of=None,
>         return_tensors="pt",
>     )
>     outputs = model(**inputs.to(device))
>     proba = outputs.logits.softmax(-1).cpu()
> 
>     a_win.extend(proba[:, 0].tolist())
>     b_win.extend(proba[:, 1].tolist())
>     tie.extend(proba[:, 2].tolist())
> 
>     df["winner_model_a"] = a_win
>     df["winner_model_b"] = b_win
>     df["winner_tie"] = tie
>     return df` and
> 
> ```
> 
> with ThreadPoolExecutor(max_workers=2) as executor:
>     gemma_results = executor.map(gemma_inference, (gemma_sub_1, gemma_sub_2), (gemma_model_0, gemma_model_1), (device_0, device_1))
> 
> I also tried batch_size=4 and 2, there's no difference.
> 
> 
> 
> > ## Valentin Werner
> > 
> > are you actually using gc.collect() - i had it before where it wouldnt be released until gc.collect() was done. exatly like ShelterW described in their comment.
> > 
> > 
> > 
> > > ## Lorry ZouTopic Author
> > > 
> > > Yes I'm suing gc.collect(), but it doesn't work: 
> > > 
> > > gemma_model_0.to('cpu')
> > > del gemma_model_0
> > > gc.collect()
> > > gemma_model_1.to('cpu')
> > > del gemma_model_1
> > > gc.collect()
> > > with torch.no_grad():
> > >     torch.cuda.set_device('cuda:0')
> > >     torch.cuda.empty_cache()
> > >     torch.cuda.set_device('cuda:1')
> > >     torch.cuda.empty_cache()
> > > 
> > > 
> > > 


---

> ## ShelterW
> 
> When I used the Gemma2 and Llama3 ensemble, it was even worse.
> 
> ```
> import torch
> import gc
> del proba, model_0, model_1, test, data, aug_data
> gc.collect()
> torch.cuda.empty_cache()
> 
> ```
> 
> 
> 
> > ## Lorry ZouTopic Author
> > 
> > I believe there's something remaining in the memory and we forgot to delete itâ€¦ðŸ˜†
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > This gets both GPUs down to below 300 MB. Else turn down max_length and / or batch size
> > > 
> > > 
> > > 
> > ## Allen Wang
> > 
> > Yes, I have the same problem as you. Is there any way to solve it
> > 
> > 
> > 


---



* --- discussion numver 52, the number of votes :6 ---

# Why LLAMA3 dominates the leaderboards, not deberta.

**kagglethebest** *Fri Jul 05 2024 22:41:58 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

When I looked at the public notebook, I was surprised to find that LLAMA3 had the highest score, not Deberta. I have the impression that there are competitions about text classification tasks (let's say this competition is also text classification tasks), and basically Deberta is the optimal solution, at least not by a large margin.

I think there could be two reasons for this:

We haven't found a more suitable categorical loss function for deberta.
Decoder Only models such as LLAMA are more sensitive to the text output by LLMs.

ps: Please let me know if anyone uses Deberta to exceed the score of the best LLAMA notebook.



---

 # Comments from other users

> ## Valentin Werner
> 
> I think your second reason definetly applies. But you should also acknowledge that Llama3-8B has 20x amount of parameters compared to DeBERTa and was pre-trained accordingly. It will be able to represent language much better. Simply adding an classification head will make up the difference between encoding and decoding.
> 
> If I am not mistaken, the architectural differences between encoder-only (DeBERTa) and decoder-only (LLama) for seq classification are marginal, as the decoder are no longer in need to generate the next tokens auto-regressively and instead will generate the classification, just like encoders do.
> 
> Often, the amount of parameters only makes a small difference towards a better score, however, as this problem his very nuanced (even a human could not predict the dataset very well), the sheer amount of parameters helps learning these nuances. This problem is simply too complex for DeBERTa, in my opinion.
> 
> 
> 
> > ## CristÃ³bal Mackenzie
> > 
> > This makes sense, since I gave a couple of shots using TinyLlama and absolutely failed. Amount of parameters seems to be key for learning anything at all in this problem.
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > I heard some people had some success with Deberta XS regarding "anything at all". But my best DeBERTa (Large) got barely below 1.0, which already included some secret sauce
> > > 
> > > 
> > > 
> > > ## justin1357
> > > 
> > > Could llama be much better?
> > > 
> > > 
> > > 


---



* --- discussion numver 53, the number of votes :6 ---

# submissions get timeout?

**yechenzhi1** *Sun May 19 2024 11:28:12 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Hi, I'm new to kaggle. I've submitted several times, all all my submissions failed due to timeout, but when I run it in my local Kaggle environment with T4*2 the inference time is as follows:

And I got a warning like this:

2024-05-19 01:36:52.192095: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered

  2024-05-19 01:36:52.192192: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered

  2024-05-19 01:36:52.309490: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered

But I'm sure GPU is used during the inference.

Any help would be appreciated. 



---

 # Comments from other users

> ## yechenzhi1Topic Author
> 
> thanks everyone helped! Setting batch size=1 solved my problemðŸ˜ƒ
> 
> 
> 


---

> ## yechenzhi1Topic Author
> 
> Another question is that, when we score in the public leaderboard, is the test dataset about 25000 * 0.3 rows? And when tested in the private leaderboard, it's about 25000 * 0.7 rows?
> 
> 
> 
> > ## Kishan Vavdara
> > 
> > Yes, that's correct! 
> > 
> > 
> > 
> > ## Rich Olson
> > 
> > I'll add that assuming this is like most contests - you're notebook is always run for the entire private / public test set.  It's just the scores for the private data portion of the scores is revealed at the end of the contest.
> > 
> > 
> > 


---

> ## lijiang3859
> 
> Hey, [@yechenzhi1](https://www.kaggle.com/yechenzhi1). Thanks for your sharing! I also got this warning:
> 
> ```
>   warnings.warn(
> 2024-07-06 05:05:32.818151: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
> 2024-07-06 05:05:32.818272: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
> 2024-07-06 05:05:32.956771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
> 
> ```
> 
> However, my program does not raise bugs. Is there any influence on it?  By setting batch_size=1, the warning is gone?
> 
> 
> 
> > ## yechenzhi1Topic Author
> > 
> > We can ignore this warning.
> > 
> > 
> > 


---

> ## lijiang3859
> 
> I think I also have the same issue with model=llama3-8B. Here is my script:
> 
> ```
> results = []
> df = pd.read_csv(args.test_file, dtype={'prompt': str, "response_a":str, "response_b":str})
> df.fillna("''", inplace=True)
> df.replace('null', "'null'", inplace=True)
> 
> eval_dataset = Dataset.from_pandas(df)
> length =  len(eval_dataset)
> for i in tqdm(range(length)): # batch_size = 1
>     data = eval_dataset[i]
>     idx = data["id"]
>     resp_a = template.format(data['prompt'], data['response_a'])
>     resp_b = template.format(data['prompt'], data['response_b'])
>     resp_tokens = tokenizer(
>         [resp_a, resp_b],
>         max_length=args.max_total_length,
>         padding=True,
>         truncation=True,
>         return_tensors="pt",
>     )
>     # concated responses to save inference time -> batch_size =2
>     output = model(resp_tokens)
> 
> ```
> 
> Here is some other settings to speed up the inference process:
> 
> use bf116=True for model initialization.
> use autocast() and
> 
> Is there any other process to speed up the inference? I have tested it with 25000 samples, it is very risky to excel the total training budget with 9hrs.
> 
> 
> 
> > ## yechenzhi1Topic Author
> > 
> > [https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference](https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference)  you can check this notebook to see if it can help.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> One Remark, the test data has 25000 samples, so this will 10x your runtime. Technically that is still less than 540 minutes, but it is a lot slower
> 
> 
> 
> > ## yechenzhi1Topic Author
> > 
> > yes, so the prediction time should be 40x10 minutes, that's about 7 hours,  so it shouldn't be timeout.
> > 
> > 
> > 


---

> ## Rich Olson
> 
> how many rows are you testing prediction with?
> 
> (when you score - it scores against 25,000)
> 
> 
> 
> > ## yechenzhi1Topic Author
> > 
> > I tested 2500 rows, it was about 40 minutes.
> > 
> > 
> > 
> > > ## Rich Olson
> > > 
> > > well - I can't think of anything obvious.  just assuming you aren't doing anything that takes a bunch of time before inference? (training / pre-processing / generating embeddings)?
> > > 
> > > if you run out of ideas - I would try to test the workflow as close as possible to submission.
> > > 
> > > I would load 25k rows from "train" into your "test" dataframe (and drop columns / etc to make it look like test).
> > > 
> > > Then - I would save a version of your notebook.  That will run it like it was getting submitted.
> > > 
> > > You should then be able to look at the logs (even if it times out before finishing).
> > > 
> > > Might want to try adding some logging / debug statements before doing.
> > > 
> > > 
> > > 


---



* --- discussion numver 54, the number of votes :6 ---

# is 'Both are bad' = 'Tie' ? 

**waechter** *Fri May 17 2024 02:38:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Hello, 

In [https://chat.lmsys.org/](https://chat.lmsys.org/) (data source) there is 4options :

ðŸ‘ˆ A is better
ðŸ‘‰ B is better
ðŸ¤ Tie
ðŸ‘Ž Both are bad

Since we only have 3options, I'm wondering if 'Both are bad' is a Tie,  or if these are filtered out

Thanks !



---

 # Comments from other users

> ## Addison Howard
> 
> "Both are Bad" and "Tie" have been merged in this competition.
> 
> 
> 


---



* --- discussion numver 55, the number of votes :6 ---

# DeBERTa is not learning patterns?

**Valentin Werner** *Mon May 06 2024 18:08:22 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

Hello everybody - I am currently facing the issue that my starter notebook always predicts label 0 (which is most prevalent in the subset of the dataset that I am using).

I did not have this experience in the past, where even though labels are balanced, the model is not learning.

Did you experience the same and were you able to solve it?



---

 # Comments from other users

> ## Rich Olson
> 
> I had the same experience with deberta:
> 
> [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/501848](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/501848)
> 
> Short version: Things didn't converge for me until I started training with more data.  
> 
> Definitely had that "huh - this isn't work" feeling training on a small subset.  Got LB 1.030 after training on all the data.
> 
> Notebook here:
> 
> [https://www.kaggle.com/code/richolson/deberta](https://www.kaggle.com/code/richolson/deberta) (copy and paste as you please)
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Interesting! 1.030 is definetly an improvement, did you also evaluate accuracy? wondering whether it is still in the 30s/40s..
> > 
> > 
> > 
> > > ## Rich Olson
> > > 
> > > Well - my validation on 20% of train is:
> > > 
> > > Log Loss: 1.0217662463425792
> > > 
> > > Accuracy: 0.48329853862212946
> > > 
> > > Considering my LB score is a little lower - I'd guess mid 40s at best.
> > > 
> > > Since the amount of train data seems to be a factor - wondering if tossing a bunch more train at it might help. (there are some datasetsâ€¦)
> > > 
> > > Considering run-time is about 3 hours - could maybe double the train data.  I haven't really looked at if I can do anything to speed things up yet though.
> > > 
> > > 
> > > 
> > > ## Gaurav Rawat
> > > 
> > > Nice how do the loss curves look they seems to be like fluctuating with no end . Also the accuracy you got at what step earlier or later epochs . As I see that also fluctuates 
> > > 
> > > 
> > > 
> > > ## Rich Olson
> > > 
> > > so - in another notebook that uses deberta - I've gone up to 1000 LGBM iterators - and it still seems like loss is slowly fallingâ€¦ (and LB score improvingâ€¦)
> > > 
> > > [https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec-length](https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec-length) (LB 1.011 on last run)
> > > 
> > > I've added tf-idf, word2vec and length features in that one - so hard to say what's going onâ€¦  taking it as a suggestion I may need to use something more than LGBM to fully use the deberta embeddingsâ€¦
> > > 
> > > 
> > > 


---

> ## Huang Jing Stark
> 
> Facing same issue here, my eval_loss is not decreasing 
> 
> 
> 


---

> ## Valentin WernerTopic Author
> 
> Code in case you care
> 
> Config:
> 
> ```
> class CFG:
>     model = "microsoft/deberta-v3-small"
>     add_tokens = ["<[PROMPT]>","<[RESP_A]>","<[RESP_B]>","<[...]>","\n"]
>     output_dir="."
>     learning_rate=2e-5
>     per_device_train_batch_size=2
>     per_device_eval_batch_size=2
>     num_train_epochs=2
>     weight_decay=0.01
>     evaluation_strategy="epoch"
>     save_strategy="epoch"
>     max_length=2048
>     warmup_ratio=0.1
>     fp16=True
> 
> ```
> 
> Tokenizer (note that I also tried without new tokens and got same result)
> 
> ```
> # Prepare Tokenizer
> tokenizer = AutoTokenizer.from_pretrained(CFG.model)
> 
> new_tokens = set(CFG.add_tokens) - set(tokenizer.vocab.keys())
> tokenizer.add_tokens(list(new_tokens))
> 
> def tokenize(examples):
>     """use with huggingface datasets"""
>     return tokenizer(
>         examples["train_input"], 
>         truncation=True,
>         max_length=CFG.max_length
>     )
> 
> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
> 
> [... dataset preparation ...]
> 
> ```
> 
> Model loading (note that I also tried without num_labels and got same result):
> 
> ```
> # Initialize model
> model = AutoModelForSequenceClassification.from_pretrained(
>     CFG.model,
>     num_labels=3
> )
> model.resize_token_embeddings(len(tokenizer))
> 
> ```
> 
> Metric used:
> 
> ```
> accuracy = evaluate.load("accuracy")
> 
> def compute_metrics(eval_pred):
>     predictions, labels = eval_pred
>     predictions = np.argmax(predictions, axis=1)
>     return accuracy.compute(predictions=predictions, references=labels)
> 
> ```
> 
> Training:
> 
> ```
> training_args = TrainingArguments(
>     output_dir=CFG.output_dir,
>     learning_rate=CFG.learning_rate,
>     per_device_train_batch_size=CFG.per_device_train_batch_size,
>     per_device_eval_batch_size=CFG.per_device_eval_batch_size,
>     num_train_epochs=CFG.num_train_epochs,
>     weight_decay=CFG.weight_decay,
>     evaluation_strategy=CFG.evaluation_strategy,
>     save_strategy=CFG.save_strategy,
>     fp16=CFG.fp16
> )
> 
> trainer = Trainer(
>     model=model,
>     args=training_args,
>     train_dataset=ds["train"],
>     eval_dataset=ds["test"],
>     tokenizer=tokenizer,
>     data_collator=data_collator,
>     compute_metrics=compute_metrics,
> )
> 
> trainer.train()
> 
> ```
> 
> 
> 
> > ## Ho Dinh Trieu
> > 
> > hi [@valentinwerner](https://www.kaggle.com/valentinwerner),does the train takes long? 
> > 
> > 
> > 
> > > ## Valentin WernerTopic Author
> > > 
> > > No, I sample 10% of the training data and only train 2 epochs. Takes about 35 min on kaggle GPU.
> > > 
> > > I also noticed that other notebooks have the same issue.
> > > 
> > > 
> > > 
> > > ## Gaurav Rawat
> > > 
> > > What was the best loss you got from the baseline not getting it past 1 right now and seems not converging at this moment for me . ðŸ˜€ 
> > > 
> > > 
> > > 
> > > ## Valentin WernerTopic Author
> > > 
> > > Same for me, I also tried rephrasing the task but cannot make it lear at all.
> > > 
> > > Loss is stuck at 1.07 or so; which is what you get when you just predict the distribution
> > > 
> > > 
> > > 


---



* --- discussion numver 56, the number of votes :6 ---

# Predictive Human Preference: From Model Ranking to Model Routing (Idea for build a baseline)

**KhanhVD** *Fri May 03 2024 17:28:13 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6 votes)

This is [cool blog](https://huyenchip.com/2024/02/28/predictive-human-preference.html) from Chip Huyen about Predictive Human Preference I think it can help for this competition and give some idea to build baseline model

# Bradley-Terry algorithm

Given a history of match outcomes, the Bradley-Terry algorithm finds the model scores that maximize the likelihood of these match outcomes, turning model scoring into a maximum likelihood estimation problem. The input, for each training example, is the models that participate in the match. The output is the outcome of the match. Assuming thereâ€™s no draw, the outcome of a match is either 0 (a wins) or 1 (b wins).

[https://huyenchip.com/assets/pics/predictive-preference/3-bradley-terry.png](https://huyenchip.com/assets/pics/predictive-preference/3-bradley-terry.png)

# Predicting Human Preference For Each Prompt

If a ranking algorithm is about figuring out which model is better overall, predictive human preference is about figuring out which model is better for each prompt. If we know in advance that for a particular prompt, GPT-3.5 works just as well as GPT-4, and GPT-3.5 is cheaper, we can route that prompt to GPT-3.5 instead. Or if we know that Mistral-7B works just as well as GPT-4 and Mistral-7B is faster, we can route our query to Mistral-7B instead.

## Experiment setup

We can treat predictive human preference as a binary classification task. Given a match between 2 models, predict which one wins. If the probability of model_a winning is around 0.5, it can be considered a tie. If a Bradley-Terry model takes only (model_a, model_b) as the input, a preference predictor takes (prompt, model_a, model_b) as the input.

[https://huyenchip.com/assets/pics/predictive-preference/4-preference-predictor.png](https://huyenchip.com/assets/pics/predictive-preference/4-preference-predictor.png)

The architecture of my preference predictor looks like this. The model encoder and preference predictor are neural networks that can be trained independently or together. We can use BERT, Roberta, Deberta,.. or other encoder model as my prompt encoder.

[imagehttps://huyenchip.com/assets/pics/predictive-preference/5-predictive-preference-architecture.png](https://huyenchip.com/assets/pics/predictive-preference/5-predictive-preference-architecture.png)





* --- discussion numver 57, the number of votes :5 ---

# Llama 3.1 7b vs Gemma 9b (sft)?

**SeshuRaju ðŸ§˜â€â™‚ï¸** *Sun Jul 28 2024 02:44:32 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

Local cv for Gemma is better than Llama 3.1 -> is it same for you too?

- same settings as sft, qlora, 4bit, same batch size.

Gemma 9b:  

  Step 10: loss = 2.3923

  Step 20: loss = 2.0361

  Step 30: loss = 1.4534

  Step 40: loss = 1.6852

  Step 50: loss = 1.3092

LLama 3.1 7b:

  Step 10: loss = 2.6542

  Step 20: loss = 3.2993

  Step 30: loss = 2.4278

  Step 40: loss = 2.0152

  Step 50: loss = 2.3515



---

 # Comments from other users

> ## Helmut12
> 
> By looking through the Code page, I think Gemma should be better for this competition.
> 
> 
> 


---

> ## sayoulala
> 
> The training loss alone is not enough to determine which is not performing well.
> 
> 
> 


---

> ## Ashwani
> 
> In my limited experiments, gemma9b is performing better than llama3.1 and llama3. 
> 
> Both llama3.1 & llama3 are giving similar performance with llama3.1 marginally better. 
> 
> 
> 
> > ## Merlyn Wang
> > 
> > Same here.
> > 
> > 
> > 


---

> ## CPMP
> 
> This is train loss or validation loss?
> 
> 
> 
> > ## SeshuRaju ðŸ§˜â€â™‚ï¸Topic Author
> > 
> > it's training loss in the post [@cpmpml](https://www.kaggle.com/cpmpml) 
> > 
> > validation loss per epoch wise.
> > 
> >   for local cv - Llama 3.1 - 1.097 and Gemma - 0.981
> > 
> > 
> > 
> > > ## CPMP
> > > 
> > > 1.09 is a model that did not learn. Something is wrong here IMHO.
> > > 
> > > 
> > > 


---



* --- discussion numver 58, the number of votes :5 ---

# CV vs LBï¼ŒWill there be significant fluctuationsï¼Ÿ

**sayoulala** *Sun Jul 14 2024 12:02:54 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

Hello everyone, what do you think about the trends of CV (Cross-Validation) and LB (Leaderboard) in this competition's challenge? Will there be significant fluctuations, similar to recent competitions?



---

 # Comments from other users

> ## Valentin Werner
> 
> The obligatory post towards the end of each competition. 
> 
> I cannot really answer your question, but I can say that we definetly managed to overfit models on CV before. However, in a lot of cases we have pretty good LB and CV correlation. This is something I often did not have in competitions with large shakeup.
> 
> 
> 
> > ## sayoulalaTopic Author
> > 
> > Then you probably haven't participated in the competitions hosted by THE LEARNING AGENCY LAB on Kaggle, haha."
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > DAIGT was wild, but I do not recall people feeling good about their CV - LB correlation in it. 
> > > 
> > > After a quick search, I found this, which agrees with what I remember [https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/458477](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/458477)
> > > 
> > > But I also remember this gem which aged like fine milk: [https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/462235](https://www.kaggle.com/competitions/llm-detect-ai-generated-text/discussion/462235)
> > > 
> > > 
> > > 


---

> ## xiaotingting
> 
> It depends on the difference in the distribution of the training set and the test set. I can only say that there may be a big difference between the leaderboard score and the cross-validation score.
> 
> 
> 


---

> ## Dlond Mike
> 
> don't worry my friend,u are the first in LB:)
> 
> 
> 


---



* --- discussion numver 59, the number of votes :5 ---

# Request for Kaggle Llama 3 access under review for over 6 days [Solved]

**Allie K.** *Mon Jul 08 2024 20:18:16 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

On Friday early morning MDT I submitted request for Llama 3 and Llama 2 access first via Meta website (of course with the same email address as I have on Kaggle) and I was granted the access in a minute.

Immediately I successfully submitted request to access Llama 3 model via Kaggle. 

Now, after more than 6 days, the request is still "pending a review from the authors".

As it can be seen from the discussion under the model, I am not alone in this desperate situation.

[@addisonhoward](https://www.kaggle.com/addisonhoward) is the access to the model on Kaggle somehow restricted? 

In this case all the competition wouldn't be fair at all. It isn't fair even now, because I couldn't make submission with Llama 3 for 3 days due to problems on Kaggle side.  

Edited:

And suddenly, after "only" 6 days a magic happened and the access is granted.

The magic seems to be triggered by another discussion thread.



---

 # Comments from other users

> ## CPMP
> 
> Reading this only now. It is wrong that your post did not have effect until mine. 
> 
> 
> 


---

> ## RB
> 
> I downloaded Transformer weights for Gemma (since they are not [yet available on Kaggle](https://www.kaggle.com/models/google/gemma-2/discussion/516164)) You can do the same for Llama as well 
> 
> Following code will save weights in /kaggle/working directory of your kernel. You do need read access token from Huggingface and your request must be approved there.
> 
> Typically I found process is much faster when the models are released, so apply even if you are not planning to use it. 
> 
> ```
> import os
> os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"
> 
> from kaggle_secrets import UserSecretsClient
> user_secrets = UserSecretsClient()
> secret_value_0 = user_secrets.get_secret("HF_TOKEN")
> 
> from huggingface_hub import  snapshot_download, login
> login(token=secret_value_0, add_to_git_credential=False)
> 
> ## Download model from HuggingfaceHub
> ## https://huggingface.co/google/gemma-2-9b-it/tree/main
> 
> snapshot_download(repo_id="google/gemma-2-9b-it", 
>                   revision="main", 
>                   repo_type="model",
>                   allow_patterns="*",
>                   local_dir = "/kaggle/working/", 
>                   ignore_patterns="consolidated.safetensors")
> 
> ```
> 
> 
> 
> > ## BladeRunner
> > 
> > This approach seems to only support models with weight files under 20GB, because of the capacity cap of /kaggle/working/, I wonder how it should be handled for models 13b and above?ðŸ˜€
> > 
> > 
> > 
> > > ## RB
> > > 
> > > You can download in /tmp directory - I think there's 50+ GB space available there. 
> > > 
> > > From /tmp you can upload to a Kaggle Dataset with [Kaggle API  ](https://github.com/Kaggle/kaggle-api/blob/main/docs/README.md#datasets)
> > > 
> > > 
> > > 


---

> ## sayoulala
> 
> [https://www.kaggle.com/datasets/junglebeastds/llama3instruct](https://www.kaggle.com/datasets/junglebeastds/llama3instruct) .Someone upload the model here
> 
> 
> 


---

> ## Allie K.Topic Author
> 
> Big thanks to everybody who suggested me (and hopefully not only to me) a solution how to solve the unpleasant situation. I could start submitting.
> 
> Anyway I hope that Kaggle team will restore the broken Llama 3 access pipeline in a reasonable time, not only after the competition ends. 
> 
> 
> 


---

> ## Pamin
> 
> Same, 3 days ago.
> 
> 
> 


---

> ## hn
> 
> Same here actually. 
> 
> 
> 


---

> ## Valentin Werner
> 
> This is wild, it has been approved for me within 10 minutes on a weekend
> 
> 
> 


---

> ## Xinyuan Qiao
> 
> Just do it again, I got same situation before.
> 
> 
> 


---

> ## Arindam Roy
> 
> Same here 
> 
> 
> 


---

> ## samson
> 
> You can get an access via [meta's webpage](https://llama.meta.com/) or directly on [huggingface](https://huggingface.co/meta-llama/Meta-Llama-3-8B), then download the weights and upload all the stuff as a private dataset on Kaggle. Its much faster! Basically minutes (I have submitted a request for model access via Kaggle 4 days ago and still waiting)
> 
> 
> 


---



* --- discussion numver 60, the number of votes :5 ---

# Facing "CUDA out of memory" error during fine-tuning Llama3 model

**Tabassum_Nova** *Fri May 31 2024 18:06:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

I tried to fine-tune Llama3 model inspired by [fine-tune-llama-3-for-sentiment-analysis](https://www.kaggle.com/code/lucamassaron/fine-tune-llama-3-for-sentiment-analysis) notebook. But I was facing the following error:

torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 224.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 11.06 MiB is free. Process 3258 has 14.73 GiB memory in use. Of the allocated memory 14.04 GiB is allocated by PyTorch, and 509.85 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF

I have already followed the solution suggested in [this discussion](https://www.kaggle.com/discussions/getting-started/140636). But these did not help. This is the link of [my notebook](https://www.kaggle.com/code/tabassumnova/lmsys-fine-tuning-llama3-8b/notebook)

Can anyone please suggest what I should do to avoid this error?



---

 # Comments from other users

> ## Ivan Vybornov
> 
> Enable gradient_checkpointing and use paged_adamw_8bit instead of a 32bit version. If does not work, try applying lora to less target_modules, for instance finetuning just ["q_proj", "k_proj", "v_proj", "o_proj"] ain't bad.
> 
> 
> 
> > ## Tabassum_NovaTopic Author
> > 
> > Thank you. Enabling gradient _checkpointing works. Training has started ðŸ˜
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> If you are not using it already, use batch size 1. Maybe use T4 x2 
> 
> in general, kaggle GPU might be too slow for the amount and length of training data
> 
> 
> 
> > ## Tabassum_NovaTopic Author
> > 
> > I solved the issue. But itâ€™s taking a long time to train. I am using Kaggle GPU T4x2. Could you please suggest any other option to train the model other than kaggle notebook? I donâ€™t have any personal GPU
> > 
> > 
> > 
> > > ## Kishan Vavdara
> > > 
> > > There are many options , you can rent A100, Rtx4090 or any other GPU instances at [Vastai](https://vast.ai/),  [Runpod](https://www.runpod.io/), or other cloud host platforms,  train your model and then delete the instance. You can also start google cloud free trial, it will give you 300$ credits for 3 months. I think colab pro also gives access to A100 and V100 Gpu's. Personally, I found vastai to be more convenient and cheap. 
> > > 
> > > 
> > > 
> > > ## Tabassum_NovaTopic Author
> > > 
> > > Thank you for your suggestions
> > > 
> > > 
> > > 
> > > ## lijiang3859
> > > 
> > > I trained offline in my server, but it still requires memory. How can I solve it? 
> > > 
> > > If I submit this to notenotebook in the system, will the code still run on the same device I am using for inference? (so sad)
> > > 
> > > 
> > > 


---

> ## Kishan Vavdara
> 
> Try reducing LoRA config 'rank', it will reduce trainable params, in your notebook i see you're using 64 rank, try 4, 8, or 16.  And you can also try reducing max_length. 
> 
> 
> 
> > ## Tabassum_NovaTopic Author
> > 
> > I tried with rank 4, max_seq_length = 512; Still getting the same error
> > 
> > 
> > 


---

> ## kartikey bartwal
> 
> Are you doing your work on some other platform other thank kaggle notebooks or google colab ? I don't think such problem should've arrived with their TPU's
> 
> 
> 
> > ## Tabassum_NovaTopic Author
> > 
> > The training issue is solved. But itâ€™s training too slowly. I have not tried with TPU. Could you please suggest any solution too solve this training speed?
> > 
> > 
> > 
> > > ## Tabassum_NovaTopic Author
> > > 
> > > Yeah I understand 
> > > 
> > > 
> > > 


---



* --- discussion numver 61, the number of votes :5 ---

# Can I win only using Kaggle resources?

**Areej Malkawi** *Mon Jun 10 2024 03:35:55 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

The data of this competition are huge, I saw many public notebooks that used some pre-trained models for inference and I tried to use a pre-trained model to train the given competition data but I'm facing memory\score trade-off, am I supposed to use something more than Kaggle GPU\ TPU quota to overcome memory issues? as well as getting high LB score?

can I win or be at the top of LB without using external resources?



---

 # Comments from other users

> ## Ebi
> 
> 
> can I win or be at the top of LB without using external resources?
> 
> I think it's almost impossible. And this applies to most Kaggle competitions, because in most cases, you need to do a lot of experiment cycles.  
> 
> In fact, I've been participating for about three years using a CPU-only laptop and a cheap cloud service like Colab, but I've only managed to get a silver medal. I bought an RTX 4090 a few months ago and was able to get a gold medal right away, which gives me an advantage in this competition too.  
> 
> 
> 
> > ## Hassan Abedi
> > 
> > Yeah, for most NLP competitions, having a good GPU with lots of VRAM makes a big difference. 
> > 
> > 
> > 
> > ## Valentin Werner
> > 
> > Note that renting a GPU starts (in Germany) around 50 cent / hour - let's say a training takes 8 hours, thats 4â‚¬. You will do some iterations, so you will definetly spent more money, BUT a 4090 is about 1800â‚¬ in Germany - so you can do about 450 trainings (or 3600 hours of training!!) on a rented GPU. If you want to try something before doing such a heavy investment, you could start like that.
> > 
> > Further, since Kaggle provides you 30 GB of VRAM (vs. about 23 GB on a Windows PC with a 4090), you can try all the things before going into investment. The big blocker with kaggle GPUs is speed, not possibilities.
> > 
> > 
> > 


---

> ## tanaka
> 
> Hmm I understood,  it seems quite difficult to compete in these competitions without using external resources.
> 
> However, I feel that suddenly buying a GPU is a high hurdle.
> 
> it seems best to try out various options like Colab or other GPU rental servers, and then decide whether to buy or rent a GPU.
> 
> [Vast.ai](http://vast.ai/) seems to be a popular option, doesn't it ðŸ¤”?
> 
> - [https://vast.ai/](https://vast.ai/)
> 
> - [https://cloud-gpus.com/](https://cloud-gpus.com/)
> 
> - [https://gist.github.com/devinschumacher/87dd5b87234f2d0e5dba56503bfba533](https://gist.github.com/devinschumacher/87dd5b87234f2d0e5dba56503bfba533)
> 
> - [https://getdeploying.com/reference/cloud-gpu](https://getdeploying.com/reference/cloud-gpu#paperspace)
> 
> 
> 
> > ## Ebi
> > 
> > I have never used vast.ai, but I personally like  [jarvislabs.ai](https://jarvislabs.ai/). 
> > 
> > It is easy and fast to create an instance and access it via SSH. I think the price is also very cheap. I mainly used it until I switched to a home server.
> > 
> > 
> > 


---

> ## Andreas Bisi
> 
> Considering past NLP competitions, you would need external resources to finish in the gold-medal area. However, I believe with a TF-IDF solution (on kaggle hardware) it's doable to finish in the bronze-medal area with some luckâ€¦
> 
> 
> 


---



* --- discussion numver 62, the number of votes :5 ---

# Unspecified exception, only when submitted (later = Submission Scoring error)

**RickPack** *Tue May 21 2024 01:00:22 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

My [Python notebook](https://www.kaggle.com/code/rickpack/python-average-prob-per-word-no-gpu) and [R notebook](https://www.kaggle.com/code/rickpack/r-average-prob-per-word-no-gpu/) run without issue but when I submit them, I get a "Notebook Threw Exception" error that is only visible on the Submissions screen. The log shows no errors.

Does anyone have a potential solution?

R:             [https://www.kaggle.com/code/rickpack/r-average-prob-per-word-no-gpu/](https://www.kaggle.com/code/rickpack/r-average-prob-per-word-no-gpu/)

Python:   [https://www.kaggle.com/code/rickpack/python-average-prob-per-word-no-gpu](https://www.kaggle.com/code/rickpack/python-average-prob-per-word-no-gpu)

[@sohier](https://www.kaggle.com/sohier), [@addisonhoward](https://www.kaggle.com/addisonhoward) 



---

 # Comments from other users

> ## David.Ricardo.H.X
> 
> i got the same problem
> 
> 
> 


---

> ## RickPackTopic Author
> 
> Reopening this [@sohier](https://www.kaggle.com/sohier), [@addisonhoward](https://www.kaggle.com/addisonhoward) in hopes of getting thoughts, please? Both the R and Python notebooks are failing with a Submission Scoring Error after minor modifications. I see that the row sums of the probabilities are not always exactly 1 (e.g., 1.002, 0.999, 1.000). If that could be the problem, could you please see if you can comment on what might repair that problem? I have tried various kinds of rounding and standardizing as you will see in code. Thank you!
> 
> R:           [https://www.kaggle.com/code/rickpack/r-average-prob-per-word-no-gpu?scriptVersionId=179034682](https://www.kaggle.com/code/rickpack/r-average-prob-per-word-no-gpu?scriptVersionId=179034682)
> 
> Python:  [https://www.kaggle.com/code/rickpack/python-average-prob-per-word-no-gpu?scriptVersionId=179035436](https://www.kaggle.com/code/rickpack/python-average-prob-per-word-no-gpu?scriptVersionId=179035436)
> 
> 
> 
> > ## Fae Gaze
> > 
> > [@rickpack](https://www.kaggle.com/rickpack) , I think that the reason that the issue of row sum of the probabilities suggested that you may have a normalization issue. Also different scores of R and python may suggest that they handle the data differently. I mean, even if the same statistical methods or the same algorithm, their implementation in r and Python libraries can differ in terms of numerical precision or optimizations. 
> > 
> > 
> > 


---

> ## RickPackTopic Author
> 
> Fixed! I have not studied why but I appeared to not get a prediction for every record. By left joining test on my predictions and imputing predictions where missing, both notebooks produced unimpresssive scores. Interesting that little differences between the notebooks yielded different scores.
> 
> 
> 
> > ## Fae Gaze
> > 
> > [@rickpack](https://www.kaggle.com/rickpack) , I suggest that the columns used to join datasets are correctly specified and contain matching data formats. After the join, I think it is better to identify any rows where predictions are missing
> > 
> > 
> > 
> > > ## RickPackTopic Author
> > > 
> > > Thank you for your reply. I did not have any NA values in the data frame because of a replacement the code included. However, this version justworked ([https://www.kaggle.com/code/rickpack/r-average-prob-per-prompt-word-no-gpu?scriptVersionId=185084007](https://www.kaggle.com/code/rickpack/r-average-prob-per-prompt-word-no-gpu?scriptVersionId=185084007)) after I included a 3rd decimal place (zeros!) in the assignment of values to the 3 target columns where NA occurs. Compare to this version that failed to generate a score ([https://www.kaggle.com/code/rickpack/r-average-prob-per-prompt-word-no-gpu?scriptVersionId=184945388](https://www.kaggle.com/code/rickpack/r-average-prob-per-prompt-word-no-gpu?scriptVersionId=184945388))
> > > 
> > > 
> > > 


---



* --- discussion numver 63, the number of votes :5 ---

# Is the Kaggle provided compute enough to join this competition?

**Andreas Bisi** *Tue May 28 2024 13:29:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

After participating in the Home Credit competition, I am looking forward to joining a new one. The objective of this new competition seems interesting. From a quick look at public notebooks, it appears that two popular models are LightGBM and Llama 3 8B. For the latter, is it possible to do any fine-tuning on Kaggle, or will I need to rent A100 instances?



---

 # Comments from other users

> ## Ivan Vybornov
> 
> I would not recommend finetuning on kaggle, from my experience finetuning llama with QLoRA on TPU is extremely painful timewise therefore I had to rent a RTX 4090, which does the job roughly for 8 hours.
> 
> 
> 
> > ## Dr. Gopalashivabalasubramanium Chandrashekaran
> > 
> > Thank you for your input. In your experience, how much more time will fine tuning on kaggle take vs your RTX 4090's 8 hours? Will it take 2-3x longer? I don't want to rent a RTX 4090 lol
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Agreed on paying for Experiments. I think 2-4x is realistic. If I remember correctly, peft llama3 for some epochs took about 20 hours on kaggle and ca 8 hours on 4090. Main reason is that you can only use 2x T4 for this, which are even slower.
> > > 
> > > 
> > > 


---

> ## Kishan Vavdara
> 
> You can use kaggle TPU's for finetuning. 
> 
> 
> 


---

> ## bogoconic1
> 
> I would not advise using Kaggle compute to fine tune, unless you donâ€™t have another choice. Quick experiments with small turnaround is beneficial in a competition and using a faster GPU like A100 helps. Also, in Kaggle, you only have 30 GPU hours + 20 TPU hours (if you know how to use it) per week
> 
> 
> 


---

> ## Valentin Werner
> 
> If you are only playing to win, then it might not work on kaggle compute. If you are here to learn, embrace the challenge and try to come up with solutions that work with the kaggle compute.
> 
> 
> 


---



* --- discussion numver 64, the number of votes :5 ---

# Is Microsoft phi3 allowed?

**Kishan Vavdara** *Thu May 30 2024 22:56:48 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

I noticed in some competitions that the host specifies certain dates after which models released are not allowed. This competition was launched on May 2nd, and phi3 was released on April 23rd under the MIT license. So, could you [@sohier](https://www.kaggle.com/sohier) [@paultimothymooney](https://www.kaggle.com/paultimothymooney) please confirm if we can use that model? 



---

 # Comments from other users

> ## Kishan VavdaraTopic Author
> 
> Hello, [@mylesoneill](https://www.kaggle.com/mylesoneill) [@addisonhoward](https://www.kaggle.com/addisonhoward) Could you please confirm? I can't find that anywhere on competition page.  
> 
> 
> 
> > ## Addison Howard
> > 
> > Hi Kishan,
> > 
> > The Kaggle team cannot make license or rule interpretations for you or our hosts. I will note, however, that the only competition with a specified date relating to model releases is the AIMO competition, and that rule does not apply to other competitions.
> > 
> > 
> > 
> > > ## Kishan VavdaraTopic Author
> > > 
> > > Thank you so much for clarifying. Indeed, I read that somewhere in the AIMO competition rules.
> > > 
> > > 
> > > 
> > > ## Valentin Werner
> > > 
> > > If you dont mind me asking, did you find phi to perform similarly well to llama3?
> > > 
> > > 
> > > 
> > > ## Kishan VavdaraTopic Author
> > > 
> > > I haven't done extensive experiments with phi3 yet, but I've found it to be much better than other models of similar size. So far, it hasn't performed on par with llama-3. The best score I've achieved with till now is 0.994 and i think can still push further.    
> > > 
> > > 
> > > 


---



* --- discussion numver 65, the number of votes :5 ---

# Deberta Baseline - LB 1.075

**Fritz Cremer** *Fri May 03 2024 21:45:56 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (5 votes)

I made a very quick deberta-v3-base baseline:

[https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline/notebook](https://www.kaggle.com/code/fritzcremer/lmsys-deberta-v3-base-baseline/notebook)

Currently, it only uses a small fraction of the train data and doesn't get a great score. But this is how the code for a deberta submission could look like.

Possible improvements:

- Utilize all data

- K Fold cross-validation

- Swap the responses for more data

- Formulate the loss differently

Especially the last one. I think it could make sense to have a two stage model. In the first stage, just predict if a response won a duel or not (without providing the other response), in the second stage, using two such predictions + hand crafted features to predict the better response. I think this looks like a very interesting competition, with not one straight forward path.

Let me know what you think!



---

 # Comments from other users

> ## Nicholas Broad
> 
> Just so you know, this is basically just random guessing.
> 
> ```
> from sklearn.metrics import log_loss
> log_loss([1], [[1/3, 1/3, 1/3]], labels=[0,1,2])
> 
> # 1.0986122886681098
> 
> ```
> 
> 
> 
> > ## Valentin Werner
> > 
> > The Notebook exactly replicates the label distribution. It seems that a basic starter with huggingface Trainer is not able to learn from the data.
> > 
> > 
> > 
> > ## Fritz CremerTopic Author
> > 
> > [@nbroad](https://www.kaggle.com/nbroad) Yes, I know. It was more like a general setup to fit a model for this task with huggingface. I found that the training was very unstable, on some runs the model learned more than just the label distribution (e.g. on LB the notebook has 1.075 with the submitted version), and on others it failed completely. But then again, it is not a well tuned approach at all, just a quick first day approach ðŸ˜„
> > 
> > 
> > 


---



* --- discussion numver 66, the number of votes :4 ---

# What a bad score!!! I am puzzled whether it is reasonable?

**Turbo** *Tue Jul 30 2024 13:28:30 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

I encountered inference score bad problem.

I used gemma-2 to classify and got local cv(20% data) = 0.9366, lb=0.968.

Also, I used llama-3 to regression and got local cv(20% data) = 0.916, lb=0.934.

What a bad result!!!.

Inspired by [@jsday96](https://www.kaggle.com/jsday96), so I tried to both inference on kaggle and local. The results are shown below. The results are train data head 10. The difference is very small. I am puzzled whether it is reasonable?



---

 # Comments from other users

> ## KeShuang Liu
> 
> After reading your discussion, I tested my model, which was very useful to me. I don't know why my local prediction is so different from the prediction on Kaggle, which leads to a big difference between cv and lb. I think this is the reason. This gave me new ideas. Thank you very much for your discussion.
> 
> 
> 
> > ## TurboTopic Author
> > 
> > Hey, the results have big difference. Maybe some bugs in the code which you need to check.
> > 
> > 
> > 


---

> ## Helmut12
> 
> I think that may be normal in kaggle competition. Is this related to overfitting of the data? Like there is a significant pattern in our test set. I heard that there is a huge discrepancy between LB and the final result in a previous competition because of overfitting.
> 
> 
> 


---

> ## justin1357
> 
> cv is lower than lb, that's normal.
> 
> 
> 
> > ## TurboTopic Author
> > 
> > low 0.02. Others said the results of cv and lb are very small.
> > 
> > 
> > 
> > > ## justin1357
> > > 
> > > In my exp, cv low 0.02 too
> > > 
> > > 
> > > 


---



* --- discussion numver 67, the number of votes :4 ---

# Llama3.1-8B May Release Soon

**Yixiao Yuan** *Tue Jul 23 2024 09:55:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

According to the latest leaks, the upcoming llama3.1 8b model is expected to outperform the current Gemma2, delivering significant improvements over previous versions llama3. The model is anticipated to be released before the competition ends. Given its naming convention, LLama3.1 should seamlessly integrate with the existing LLama3 framework.



---

 # Comments from other users

> ## Valentin Werner
> 
> 
> 
> 
> 


---

> ## sayoulala
> 
> Oh,My electricity bill is going to exceed the limit again.
> 
> 
> 


---

> ## Yixiao YuanTopic Author
> 
> Llama3.1 is released.
> 
> [https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f](https://huggingface.co/collections/meta-llama/llama-31-669fc079a0c406a149a5738f)
> 
> 
> 


---

> ## Valentin Werner
> 
> Can you please share the source alongside the screenshot? I am quite interested in the numbers I am seeing, and sources are always extremely important for leaks like this.
> 
> 
> 
> > ## Yixiao YuanTopic Author
> > 
> > [https://x.com/iScienceLuvr/status/1815519917715730702](https://x.com/iScienceLuvr/status/1815519917715730702)
> > 
> > 
> > 
> > ## naturalseeker
> > 
> > [https://www.reddit.com/r/LocalLLaMA/comments/1e9qpgt/meta_llama_31_models_available_in_hf_8b_70b_and/](https://www.reddit.com/r/LocalLLaMA/comments/1e9qpgt/meta_llama_31_models_available_in_hf_8b_70b_and/)
> > 
> > 
> > 


---

> ## Dlond Mike
> 
> â€¦.LLMs' game
> 
> 
> 


---



* --- discussion numver 68, the number of votes :4 ---

# Train LLMs efficently in multi chip environment [GPU/TPU]

**Simon Veitner** *Wed May 29 2024 16:24:01 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

Hello guys,

I shared a customizable notebook that let's you train LLMs with FSDP and potentially TP technique on multiple GPU/TPU chips.

The architecture was inspired by [first place solution in LLM science exam.](https://www.kaggle.com/competitions/kaggle-llm-science-exam/discussion/446422)

Note that the score of the notebook is rather bad but can be easily improved by adjusting preproccesing/modelling etc etc.

[See the notebook here](https://www.kaggle.com/code/simonveitner/fsdp-with-scalax)





* --- discussion numver 69, the number of votes :4 ---

# TF-IDF + Word2Vec = FTW [LB 1.026]

**Rich Olson** *Thu May 09 2024 15:54:49 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

I've posted a new notebook that shows to combine TF-IDF with Word2Vec to get an LB of 1.026:

[https://www.kaggle.com/code/richolson/tf-idf-word2vec-ftw-lb-1-026](https://www.kaggle.com/code/richolson/tf-idf-word2vec-ftw-lb-1-026)

This builds on my prior TF-IDF only notebook (LB 1.038):

[https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees](https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees)

As previously - I'm using LGBMClassifier for deciphering thingsâ€¦

In one version of the notebook - I just used Word2Vec - and it alone was good for LB 1.038 (same as TF-IDF):

[https://www.kaggle.com/code/richolson/tf-idf-word2vec-ftw?scriptVersionId=176534641](https://www.kaggle.com/code/richolson/tf-idf-word2vec-ftw?scriptVersionId=176534641)

(this might actually be the more interesting bit - as I don't think anyone has shared a notebook using Word2Vec for this competition yet)

Since both TF-IDF and Word2Vec each score 1.038 on the LB separately - but manage 1.026 together - they presumably are generating features that are a little unique.

I've seen some other notebooks combining TF-IDF with other simple features (like text length) to boost score.  It might be that tossing in Word2Vec can boost things further.





* --- discussion numver 70, the number of votes :4 ---

# Why So Little

**eli plutchok** *Wed May 08 2024 13:44:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

Hello Everyone, 

I'm new to kaggle. I'm wondering why the competition organizers gave us a data set of just ~60K items when they surely must have a lot more from the same place. Should I expect them to release more at a later date, or are they just trying to make it extra challenging for us?

(The LMSYS arena leaderboard is based on over 900k votes: [https://chat.lmsys.org/?leaderboard](https://chat.lmsys.org/?leaderboard))



---

 # Comments from other users

> ## Valentin Werner
> 
> There are many reasons why a company may not want to disclose all of their data. I think 60k samples are more than what we often see for challenges. This is already a size where you are taking a small subset to test things first.
> 
> 
> 
> > ## eli plutchokTopic Author
> > 
> > Ok, I get it. They don't want people to have their proprietary data. But still, they won't get the best pre-trained models by withholding the data. Do they care mainly about the code used to train the models? I guess, my dumb question is: what is the primary objective of, LMSYS, the company sponsoring this? 
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > They are licensed the winning solutions (see Rules #A1). These companies are often able to transfer solutions to their own data themselves. 
> > > 
> > > However, company motivation for challenges may vary: For example, they maybe want to proof a concept or make a study whether things are achievable. Opening such a challenge to kaggle is smart because doing the same thing internally will often be more expensive and less diverse in solution approaches.
> > > 
> > > 
> > > 


---



* --- discussion numver 71, the number of votes :4 ---

# Do we need the class "tie"?

**Anh Bui** *Fri May 03 2024 15:50:05 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

I have a question about why there is a class 'tie', when based on the probabilities (winner_model_a/winner_model_b) = (0.5, 0.5), it can be determined whether the two models have a 'tie'.



---

 # Comments from other users

> ## MarÃ­lia Prata
> 
> Hi Anh Bui (bibanh),
> 
> On the original dataset instead of "winner_tie"  they have "winner" column (where the Rows are: model_a, model_b, tie AND tie (bothbad)
> 
> Battles No ties
> 
> battles_no_ties = battles[~battles["winner"].str.contains("tie")]
> 
> Battles without ties
> 
> visualize_battle_count(battles_no_ties, "Battle Count for Each Combination of Models (without Ties)")
> 
> Counting ties
> 
> visualize_battle_count(battles[battles['winner'].str.contains("tie")], "Tie Count for Each Combination of Models")
> 
> [https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=PbTdhkLQp113](https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=PbTdhkLQp113)
> 
> That's what the authors wrote:
> 
> "Statistics
> 
> "The authors allowed the user to declare a tie between the pairs of models. To collect additional data, later in the tournament they also allowed the user to declare a tie in which both models were bad. There were a significant portion of tied outcomes."
> 
> I hope it could help to clarify. I didn't read anything else about the ties on that Notebook or in their paper too.
> 
> 
> 


---

> ## bogoconic1
> 
> I would think it is necessary for reason (B)
> 
> A tie could mean
> 
> (A) The user rates both responses as equally good
> 
> (B) The user rates both responses as equally bad. The 2 models can answer in different ways, of which both responses are hallucinated (by making up completely different facts etc) or not answering the question
> 
> For (B) Iâ€™m pretty sure the 2 classes will not be predicted as (0.5, 0.5) ðŸ§
> 
> 
> 


---

> ## Rich Olson
> 
> For my first submission - I tried filling in "winner_tie" with all 0's.
> 
> That got me a score of 11.73 (almost last-place).  At least - I'm assuming "winner_tie" being 0's was a factor.
> 
> For my next entry - I'm going to try to fill in winner_tie based on the confidence of A or B winning.  Hopefully this improves thingsâ€¦
> 
> In the train data - it seems ties are very common (about 1 out of 3).
> 
> 
> 


---



* --- discussion numver 72, the number of votes :4 ---

# [Starter] Deberta-v3 Train & Inference code

**Heroseo** *Sat May 04 2024 18:55:29 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (4 votes)

# Intro

Hi, I wanted to share good train and inference notebooks. :)

- Pytorch Train & Inference

- Only prompt was used.

- I recommend adjusting token_lengh for your GPU memory.

- There are many things to try.

# Notebooks - v3

- Train : [[Train] LMSYS / Deberta-v3 starter codeðŸ”¥](https://www.kaggle.com/code/piantic/train-deberta-v3-starter-code)

- Inference : [[Inference] LMSYS / Deberta-v3 starter codeðŸ”¥](https://www.kaggle.com/code/piantic/inference-deberta-v3-starter-code)

# End

There is a lot of room for improvement. Please add more cool ideas. :)

Thank you!

p.s. It feels strange to share notebooks with Kaggle after a long time.





* --- discussion numver 73, the number of votes :3 ---

# External datasets

**Dlond Mike** *Tue Jul 23 2024 13:03:53 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

I want to know whether the additional datasets make senseâ€¦anyone tried them?just  someone share in the discussion



---

 # Comments from other users

> ## justin1357
> 
> Not workâ€¦ I think it's because the source is different, so distribution is not the same
> 
> 
> 
> > ## Songling
> > 
> > [@justin1357](https://www.kaggle.com/justin1357) Yes, I have also tried external data, and it is the same situation. I am curious about what other methods are there.
> > 
> > 
> > 
> > ## Xinyuan Qiao
> > 
> > It do works, try not just mix it together
> > 
> > 
> > 
> > > ## SAY WHAT
> > > 
> > > Other methods?
> > > 
> > > 
> > > 


---



* --- discussion numver 74, the number of votes :3 ---

# shake up ?

**Songling** *Tue Jul 23 2024 11:13:27 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Friends, do you think there will be major changes in the private leaderboard of this llm game? I think it will be relatively stable.



---

 # Comments from other users

> ## sayoulala
> 
> I think so too.
> 
> 
> 


---

> ## Dlond Mike
> 
> Nopeâ€¦it seems like all of us use LLM like gemma2-9b and llama3-8b.
> 
> 
> 


---



* --- discussion numver 75, the number of votes :3 ---

# [Need Help] Running Gemma 2 9b in Keras on 2xT4s

**Pranshu Bahadur** *Mon Jul 22 2024 12:28:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Hey guys!

I made this notebook to train Gemma 2 9b on TPUs.

But the competition doesn't allow TPUs for submissionâ€¦.which is a bit awkward haha

So I'm enlisting your help to figure this out!

Would really appreciate any feedback, I am looking to learn!

Training nb (~3 hrs for 1 epoch):

[https://www.kaggle.com/code/pranshubahadur/tf-gemma-2-9b-lmsys-training-tpu](https://www.kaggle.com/code/pranshubahadur/tf-gemma-2-9b-lmsys-training-tpu)

Unsolved inference nb:

[https://www.kaggle.com/code/pranshubahadur/unsolved-inference-tf-gemma-2-9b-lmsys](https://www.kaggle.com/code/pranshubahadur/unsolved-inference-tf-gemma-2-9b-lmsys)



---

 # Comments from other users

> ## Pranshu BahadurTopic Author
> 
> Update: Inference works now, out of tpu quota will update on saturday
> 
> 
> 
> > ## Somesh88
> > 
> > what did you do to make inference run?
> > 
> > 
> > 
> > > ## Pranshu BahadurTopic Author
> > > 
> > > mainly device allocation followed by a custom prediction loop and set_floatx('float16')
> > > 
> > > no quantization was needed 
> > > 
> > > you can check out my inference nb linked above
> > > 
> > > 
> > > 


---



* --- discussion numver 76, the number of votes :3 ---

# Are external data helpful? 

**Andreas Bisi** *Sun Jun 23 2024 18:27:18 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Has anyone used the [LMSYS - Additional 33k Labelled Conversations](https://www.kaggle.com/datasets/abdullahmeda/lmsys-additional-33k-labelled-conversations) dataset? If yes, are the CV and LB results yielded better or worse? Do you have suspicions of overfitting?



---

 # Comments from other users

> ## lllleeeo
> 
> The cv is significantly lower Don't know about the lab yet Worried about overfittingâ€¦â€¦.
> 
> 
> 


---

> ## YingxiZhang
> 
> not work.ðŸ˜­
> 
> 
> 


---

> ## justin1357
> 
> In my experiment, not workâ€¦sad
> 
> 
> 


---

> ## sayoulala
> 
> May I ask where did you get this data from?
> 
> 
> 
> > ## Andreas BisiTopic Author
> > 
> > There' not mine - I found them on an early discussion
> > 
> > 
> > 
> > > ## sayoulala
> > > 
> > > Thank you for your response. I'll try adding it to the training set and see if it makes a difference.
> > > 
> > > 
> > > 
> > > ## yechenzhi1
> > > 
> > > Hi, did you test the external data? 
> > > 
> > > 
> > > 


---



* --- discussion numver 77, the number of votes :3 ---

# Clean Text

**TheStoneMX** *Sun Jul 14 2024 07:21:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Hi there all,

I have been trying different text-cleaning techniques, but they do not workâ€¦ can someone share? or is there no text cleaning on these types of corpus?

Or what other ways to increase the score besides the Ensemble models?

Thanks!

Like:

```
import pandas as pd
import re
from datasets import Dataset

def load_and_clean_data(filepath):
    # Load dataset
    df = pd.read_csv(filepath)

    # Remove duplicates
    df.drop_duplicates(inplace=True)

    # Handle missing values (replace NaN with empty string)
    df.fillna("", inplace=True)

    # Clean text function
    def clean_text(text):
        # Convert to string in case of any non-string values
        text = str(text)

        # Remove unwanted characters
        text = re.sub(r'[\[\]\'"]', '', text)  # Corrected regular expression

        # Remove punctuation and special characters except periods, commas, apostrophes, and double quotes
        text = re.sub(r'[^\w\s\.,\'\"]', '', text)       
        text = text.lower() # Convert to lowercase
        text.strip()  # strip leading/trailing spaces

        # Remove URLs and Email Addresses
        text = re.sub(r'\b(?:https?://|www\.)\S+\b', '', text)
        text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '', text)
         # Remove numbers
        text = re.sub(r'\d+', '', text)

        return text

    # Clean text columns
    df['prompt'] = df['prompt'].apply(clean_text)
    df['response_a'] = df['response_a'].apply(clean_text)
    df['response_b'] = df['response_b'].apply(clean_text)

    return df

# Load and clean the data
df_cleaned = load_and_clean_data("../input/lmsys-chatbot-arena/train.csv")

# Convert to Hugging Face Dataset
ds = Dataset.from_pandas(df_cleaned)

# Print the first row 
print(ds[:1])

```



---

 # Comments from other users

> ## Bharat Raghavan
> 
> It seems to me like your code manages to clean up the text properly, unless you want to clean it further; in that case, what text-cleaning techniques are you talking about?
> 
> As for increasing the score, depending on the approach, hyperparameter tuning can be beneficial. However, I would just recommend that you be wary of overfitting when considering your approach to hyperparameter tuning.
> 
> 
> 


---



* --- discussion numver 78, the number of votes :3 ---

# How much data to train Llama 3?

**ano** *Thu Jul 11 2024 08:00:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

How much data are you using for training Llama 3? I use half of all the given training data for training and the other half as validation data, with cv: 0.968, lb: 0.979.

I want to know about the relationship between the amount of training data and accuracy. I remember reading a discussion somewhere that said using all the data for training does not change the score, but I lost track of that discussion.



---

 # Comments from other users

> ## James Day
> 
> Hesitant to share details about my experiments until the end of the competition, but it is possible to achieve significant accuracy improvements by scaling from tens of thousands of training examples to hundreds of thousands, so I would not expect your models to be saturating at 50% of the data we received from the competition organizers. Using more than 200% is better than 80%. I never scaled down to only training on 50%.
> 
> Broadly speaking, my intuition is that adding more data is almost always beneficial (albeit with diminishing returns) so long as that data is sufficiently high quality (not too repetitive, mislabeled, or different from the test data) and your model has sufficiently high capacity to learn from that data (which shouldn't be a problem for Llama 3 8B with a decent LoRA config).
> 
> 
> 
> > ## anoTopic Author
> > 
> > Thank you for the valuable information! It seems I was mistaken in thinking that a small amount of training data would be sufficient. I'll try optimizing by adding training data (including external data) and changing the parameters.
> > 
> > 
> > 
> > ## Cody_Null
> > 
> > You have already shared a friendly amount of information so feel free to hold back, are you generating new data from the training data?
> > 
> > 
> > 
> > > ## James Day
> > > 
> > > I don't want to elaborate on where my extra data came from until the end of the competition. ðŸ¤
> > > 
> > > 
> > > 
> > > ## Sparsh Tewatia
> > > 
> > > Thats enough for the smart one to know. ðŸ˜€
> > > 
> > > 
> > > 


---



* --- discussion numver 79, the number of votes :3 ---

# Multi GPU support

**Varun Jagannath** *Mon Jul 08 2024 14:28:09 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Since we have two GPU's of T4, how to fine-tune LLM for multi GPU capability. Any suggestions or tutorials for sequence classification





* --- discussion numver 80, the number of votes :3 ---

# Why is this competition less popular than AES2?

**sule** *Fri Jul 05 2024 04:32:15 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Sorry I am new so it is not obvious to me. 

AES2 had 2700 participants, this one probably will not reach more than 2000 at the end.

Is it due to summer holidays?

Or maybe competition requiring  a stronger compute?

The topic certainly does not seem niche, maybe even less niche than AES2?

I am not aware of any organizational mistakes?

Maybe all my assumptions are wrong, I would like to hear your thoughts.



---

 # Comments from other users

> ## Enter your display name
> 
> Because this competition requires much more computational resources.ðŸ˜­
> 
> 
> 
> > ## suleTopic Author
> > 
> > thanks for reply!
> > 
> > 
> > 


---

> ## Ravi Ramakrishnan
> 
> Perhaps we would see some additional competition here as the AES challenge is now over [@zvonimirsucic](https://www.kaggle.com/zvonimirsucic) 
> 
> 
> 
> > ## suleTopic Author
> > 
> > I hope so too, I started too late for AES2
> > 
> > 
> > 


---



* --- discussion numver 81, the number of votes :3 ---

# Does calibration matter?

**yechenzhi1** *Tue Jul 02 2024 19:06:56 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Hello everyone,

As VALENTIN WERNER mentioned in a [previous discussion](https://www.kaggle.com/code/valentinwerner/log-loss-what-are-good-scores/notebook), good calibration can greatly enhance log loss scores. Despite experimenting with various calibration techniques, such as temperature adjustments and training a binary classifier for hard examples, I haven't achieved satisfactory results. I've been pondering this issue for a while. Should I perhaps shift my focus to other strategies, like ensemble methods or exploring newer models? Thanks in advance!



---

 # Comments from other users

> ## James Day
> 
> I haven't had any success trying to make my predictions better calibrated as a post-processing step either. Platt scaling, isotonic regression, and model stacking all seem to do more harm than good.
> 
> A while ago ChatGPT suggested I investigate how well calibrated my predictions are by calculating "Expected Calibration Error" as an additional cross-validation metric and generating "reliability diagrams". My code for doing that and sample results for my best ensemble (0.899 LB) are included below. It seems the confidence values are really well correlated with the probability of the top guess being correct, so there's not much that post-processing logic can do to help. Perhaps my models are slightly biased towards being a little under confident, but my best attempts at correcting for that at inference time score within 0.001 (CV) of just using the raw predictions. Perhaps if the underlying models were weaker the post-processing would be more beneficial.
> 
> ```
> def compute_ece(predictions, labels, num_bins=25):
>     bin_boundaries = np.linspace(0, 1, num_bins + 1)
>     ece = 0.0
>     total_samples = len(labels)
> 
>     confidences = []
>     accuracies = []
> 
>     for bin_lower, bin_upper in zip(bin_boundaries[:-1], bin_boundaries[1:]):
>         bin_indices = np.where((predictions >= bin_lower) & (predictions < bin_upper))[0]
>         if len(bin_indices) == 0:
>             continue
> 
>         bin_confidence = predictions[bin_indices].max(axis=1).mean()
>         bin_accuracy = (labels[bin_indices] == predictions[bin_indices].argmax(axis=1)).mean()
> 
>         bin_size = len(bin_indices)
>         ece += (bin_size / total_samples) * np.abs(bin_confidence - bin_accuracy)
> 
>         confidences.append(bin_confidence)
>         accuracies.append(bin_accuracy)
> 
>     return ece, confidences, accuracies
> 
> ece, confidences, accuracies = compute_ece(all_predictions, np.array(labels))
> print(f'Expected Calibration Error (ECE): {ece:.4f}')
> 
> from matplotlib import pyplot as plt
> 
> plt.plot([0, 1], [0, 1], linestyle='--')
> plt.scatter(confidences, accuracies, marker='o')
> plt.xlabel('Confidence')
> plt.ylabel('Accuracy')
> plt.title('Reliability Diagram')
> plt.show()
> 
> ```
> 
> 
> 
> > ## Yu Chengzhi
> > 
> > Thank you for sharing! How can I ensemble different methods? Is it just the mean of probabilities from various models?
> > 
> > 
> > 
> > > ## yechenzhi1Topic Author
> > > 
> > > You can adjust the weight of each model's probability, for example, preds = 0.8 * model_a_preds + 0.2 * model_b_preds.
> > > 
> > > 
> > > 
> > ## yechenzhi1Topic Author
> > 
> > Thanks for your reply! I guess I will focus on the training process or try some new ideas.
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> Your models are already minimizing their loss, so post processing predictions will give no good results from my experience. However, when trainings transformers, parameters such as label smoothing may help to achieve better calibration (as the model is basically asked to predict 0.9 instead of 1.0 with an alpha of 0.1 etc.) - However, in general the data is confusing that this is one of the few challenges where my models basically never predict > 0.85 because it is so hard to be that confident.
> 
> When I wrote the discussion and linked notebook, I assumed that models would heavily overfit and strongly favour some classes, which does not seem to be the case.
> 
> Calibration definetly does matter, but it should probably be something you do during the training, rather than afterwards.
> 
> 
> 


---



* --- discussion numver 82, the number of votes :3 ---

# Training outside kaggle

**Ahmad Al-Husainy** *Sun Jun 16 2024 04:30:02 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Hello, this is my first competition, and I'm curious to know if it's possible to train large pre-trained models in an external environment and then simply upload the weights for submission. 



---

 # Comments from other users

> ## Lorry Zou
> 
> You definitely can do that. It's also what I'm doing. However, since this competition requires internet-off submission, I'm sure I will run into some issuesâ€¦
> 
> 
> 
> > ## Valentin Werner
> > 
> > Just load it as a kaggle dataset or kaggle model!
> > 
> > You only have to make sure the models you are using are open source. 
> > 
> > 
> > 
> > > ## Ivel afred
> > > 
> > > Does this mean that your model needs to be public on Kaggle? Or it's okay to just make it public on Hugging Face.
> > > 
> > > 
> > > 
> > > ## Valentin Werner
> > > 
> > > It can be private on kaggle, just available for you. You dont have to make it public on huggingface either. Its just important that the model that you finetune is also available for others. (e.g., DeBERTa or Llama are open source; GPT-4 is not - if you finetune GPT-4 for the competition, that would be not fair and you would have to make your GPT-4 tuned model available for everybody in the competition instead (I think))
> > > 
> > > 
> > > 
> > > ## Ivel afred
> > > 
> > > thanks, that helps me a lot
> > > 
> > > 
> > > 
> > ## Ahmad Al-HusainyTopic Author
> > 
> > Thank you for your comment. I want to clarify my approach: I'm currently using Google Colab for model development. When I attempt to train the models on Kaggle, I encounter GPU memory issues and other problems related to the Kaggle environment it self, even though the same code runs smoothly on Colab. I'm considering training the model in Colab and then extracting the best model weights. My plan is to rebuild the model on Kaggle, load the weights, predict on the test dataset, and submit my results. Additionally, the environment on Colab is more extensive than on Kaggle, so training on Kaggle could potentially exceed the 9-hour limit.
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > That is the correct approach
> > > 
> > > 
> > > 


---

> ## MarÃ­lia Prata
> 
> I think it depends on each competition rules. Though I'm not certain. Maybe Paul Mooney, Sohier Dane or Addison Howard could answer that.
> 
> By the way, welcome to your 1st competition.
> 
> 
> 


---



* --- discussion numver 83, the number of votes :3 ---

# What is the trick...

**TheStoneMX** *Sun Jun 16 2024 01:19:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

I find almost impossible to fine-tune with the recurses that we get from Kaggle, and at home not enough GPU memory.

What are people using?

External providers? Google, Azure, My Pods, etc.

Any suggestions?

What are people doing, on these type of competitions, Text cleaning, stop words removal, Stemming and lemmatization, etc.

Thanks for any tips guys.



---

 # Comments from other users

> ## Ravi Ramakrishnan
> 
> [@oscarrangel](https://www.kaggle.com/oscarrangel) I recommend you to try out [runpod.io](https://www.runpod.io/)
> 
> They offer excellent GPUs at moderately affordable prices and across various payment plans and options
> 
> 
> 
> > ## TheStoneMXTopic Author
> > 
> > Yes, thanks. That is what I started to use, but they have secure cloud and community cloud. I am using community one. Is that correct?
> > 
> > 
> > 


---

> ## Cody_Null
> 
> I have been in this position before myself. The best option (in my opinion) would be cloud resources. It is up to you how much you are willing to spend and what experiments are worth it and I know there are a lot of platforms to choose from and price may vary based off what you need and your area. Some popular ones are runpod, vast.ai, google cloud, and then depending on your personal situation AWS or Azure may be more accessible but those first 3 are more likely and worth researching. No doubt I have missed some, but a good starting point. 
> 
> 
> 


---



* --- discussion numver 84, the number of votes :3 ---

# Criteria and Approaches

**PierreSylvain** *Sun Jun 16 2024 01:53:09 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Choosing the right answer from two options can be tricky. A study found that when people have to choose between two equally good options, they usually pick the one thatâ€™s easier to explain and defend (Slovic, P. (1975). Choice between equally valued alternatives. Journal of Experimental Psychology: Human Perception and Performance, 1(3), 280â€“287. [https://doi.org/10.1037/0096-1523.1.3.280](https://psycnet.apa.org/doi/10.1037/0096-1523.1.3.280)).

In real life, figuring out the best answer means checking off a few key boxes:

Accuracy

- Is the answer correct? This can be tough if you donâ€™t know the answer yourself.

Clarity

- Is the answer easy to understand?

Coherence

- Is the answer well-structured and makes sense?

Detail

- Is the answer thorough? Do you need a detailed answer or just a brief one?

Sources

- Does the answer have references or sources? Are they reliable and up-to-date?

Objectivity

- Is the answer unbiased and neutral?

Practicality

- Is the answer useful and easy to apply?

Of course, itâ€™s rare to find an answer that ticks all these boxes perfectly. Usually, itâ€™s a mix of these factors that help you decide. For example, one answer might be super detailed but hard to understand, while another might be clear but not very deep.

Here are some tips for evaluating answers:

- How long is the text?

- How varied is the vocabulary?

- How is the sentence structure?

- Whatâ€™s the cosine similarity score?

- Whatâ€™s the sentiment analysis?

- Use a model to check text quality.

- Use a multi-criteria model to judge the text (accuracy, clarity, relevance, etc.)

There are plenty of other ways to evaluate, but for now, I need to code what I just wrote.



---

 # Comments from other users

> ## Valentin Werner
> 
> I tried a lot (almost exactly the features you describe) what you just wrote, my XGB model came out at 1.024 for the CV and 1.032 on LB. I hope your Code will yield better results!
> 
> 
> 


---



* --- discussion numver 85, the number of votes :3 ---

# Notebook threw exception

**Kaizhao Liang** *Thu May 16 2024 05:57:49 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

locally on the sample test csv, it runs fine. But submission throws exception without any useful feedback on the error log. shouldn't have been OOM since it's running BS = 1.



---

 # Comments from other users

> ## Valentin Werner
> 
> I had a similar error - for me this was what fixed it: 
> 
> ```
>  row.prompt.replace("null", "'null'")
>  row.response_a.replace("null", "'null'")
>  row.response_b.replace("null", "'null'")
> 
> ```
> 
> 
> 
> > ## Kaizhao LiangTopic Author
> > 
> > ah that could be the edge case it was discussing the other threads, let me give it a try thanks!
> > 
> > 
> > 
> > > ## RickPack
> > > 
> > > Please let us know if that worked.
> > > 
> > > 
> > > 


---

> ## jiangli59
> 
> I also met the same problem. Any update?
> 
> 
> 
> > ## jiangli59
> > 
> > If you use Llama-8b, I think it may raise this error due to out-of-memory. Sad! So, this error could be the source of oom?
> > 
> > 
> > 


---

> ## RickPack
> 
> i experienced similar today with an R notebook. Wondering if the submission needs to be rounded to two decimal places. What a does BS mean?
> 
> 
> 
> > ## Kaizhao LiangTopic Author
> > 
> > batch size = 1
> > 
> > 
> > 
> > ## Kaizhao LiangTopic Author
> > 
> > It also runs two hours before hitting that error, so clearly some edge cases that have not been exposed. Could be something due to parsing. But the error itself is not helpful for debugging at all.
> > 
> > 
> > 
> > > ## Alex Golubev
> > > 
> > > You can try to take a sample (e.g. 10k) from train and run your script on it. Probably you have a chance to hit the same error. Btw, what is the error message?
> > > 
> > > 
> > > 


---



* --- discussion numver 86, the number of votes :3 ---

# One-Feature Decision Tree

**AbaoJiang** *Wed Jun 05 2024 00:50:58 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Hi everyone,

We've seen many showing that features based on the length difference between response A and B are useful. So, I try to run a quick experiment using DecisionTreeClassifier fed with only a single feature [here](https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda?scriptVersionId=181492294). Following illustrates the decision tree of one fold,

[](https://postimg.cc/Y4YBzCJS)

As can be observed, the model learns the relationship between the length difference feature and winners,

On the right side, the winners are model A, which have longer responses.
In the middle, ties are the majority.
On the left side, the winners are model B.

The approach yields local CV score of 1.0588 with StratifiedKFold, which can't beat our naive baseline. This just another way to explore this important relationship (related to verbosity bias). Hope you find this interesting!



---

 # Comments from other users

> ## Valentin Werner
> 
> Interesting way to show feature value.
> 
> Length is the most valuable feature I found so far, but completely ignores the quality of the answer. I created a feature, that was actually among top 4 of my features, which looks into whether a model says something along the lines of "As an AI I cannot help you with that". This type of qualitative evaluation will be what is needed beyond the structural features such as length (and sadly also the reason why we have to go back go embeddings for some parts).
> 
> 
> 
> > ## AbaoJiangTopic Author
> > 
> > Hi [@valentinwerner](https://www.kaggle.com/valentinwerner),
> > 
> > Thanks for your reply.
> > 
> > I only try structural features so far, and nothing can beat the naive baseline based on the response length difference bucket. Though verbosity bias do exist, there still have much information to be extracted in different ways (e.g., contextual embeddings). Tbh, I'm an NLP newbie, and try to share what I discover during this learning journey. Thanks for your insightful sharing!
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > What baseline are you referring to?
> > > 
> > > Thanks for sharing your insights, its always appreciated!
> > > 
> > > 
> > > 
> > > ## AbaoJiangTopic Author
> > > 
> > > Hi [@valentinwerner](https://www.kaggle.com/valentinwerner),
> > > 
> > > Sorry for the late reply. I mean the naive baseline in the section Length Difference Bucket Mean Prediction of my EDA notebook!
> > > 
> > > 
> > > 


---

> ## KTibow Personal
> 
> A decision tree seemed like an odd choice, so I tried some polynomial regressions. It basically just ends up saying "bigger responses are better".
> 
> 
> 
> > ## AbaoJiangTopic Author
> > 
> > Hi,
> > 
> > The reason why I choose DT is that I want to do comparison with the naive baseline based on the manual binning of response length difference. Because DT itself learns to bin the length difference automatically, I just share that we can observe the similar property from different angles.
> > 
> > Anyway, thanks for your sharing.
> > 
> > 
> > 
> > ## Vishal Maurya
> > 
> > Hii [@ktibow](https://www.kaggle.com/ktibow), thanks for sharing this. Could you share the R2-score of these polynomial models above, I just want to know that how strong and significant relationships are there.
> > 
> > 
> > 


---



* --- discussion numver 87, the number of votes :3 ---

# Question about rule(fine tuning the LLM)

**eigenquant** *Sat Jun 08 2024 19:37:15 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)


- Is it okay to do fine tuning the LLM with personal gpu(outside from kaggle) and use it for this competition?



---

 # Comments from other users

> ## Valentin Werner
> 
> Yes, as long as the LLM and Data you are using is Open source (e.g., MIT, Apache 2.0). You can also rent GPUs and train there, its all fair play.
> 
> 
> 
> > ## eigenquantTopic Author
> > 
> > thanks for your confirm
> > 
> > 
> > 


---

> ## yulonghui
> 
> I want to know What kind of data we can use for fine-tuning?
> 
> 
> 


---



* --- discussion numver 88, the number of votes :3 ---

# Deberta -> LGBM [1.30 LB]

**Rich Olson** *Sat May 11 2024 10:04:01 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (3 votes)

Just shared my Deberta-feature-extraction -> LGBM notebook here:

[https://www.kaggle.com/code/richolson/deberta](https://www.kaggle.com/code/richolson/deberta)

Score is 1.030 on the LB.

I train / submit on the same run.  Runtime is about 3 hours on P100.  Most of that is Deberta doing feature extraction (LGBM train / inference is comparatively fast).

Submission is probably about 90 minutes longer (since feature extraction needs to be done on test also).

I'm using deberta-base (presumably this would run faster with a smaller model).

One note - I'll typically do quick-test-runs with 1-5k prompts to see if things are generally working.  That was enough for TF-IDF to show some results.  For whatever reason - this produced zero results with Deberta.

Once I started training on 10k+ samples - then things started to converge for me.

So - if your Deberta model isn't doing anything - maybe try more dataâ€¦

I also have a related kitchen-sink notebook that's currently scoring:

[https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec](https://www.kaggle.com/code/richolson/deberta-tf-idf-word2vec)





* --- discussion numver 89, the number of votes :2 ---

# QA: About Deadline

**Robert0921** *Sat Aug 03 2024 12:03:32 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

About Deadline, Do I just need to submit before FSD (Final Submission Deadline), or need to wait for public score to come out before FSD? Because this involves a difference of 8~9 hours running timeâ€¦

Timeline

May 2, 2024 - Start Date.

July 29, 2024 - Entry Deadline. You must accept the competition rules before this date in order to compete.

July 29, 2024 - Team Merger Deadline. This is the last day participants may join or merge teams.

August 5, 2024 - Final Submission Deadline.



---

 # Comments from other users

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> 
> [@robert0921](https://www.kaggle.com/robert0921) need to wait for public score to come out before FSD, We will see private leaderboard once FSD finish.
> 
> 
> 


---



* --- discussion numver 90, the number of votes :2 ---

# Why I did not get the corresponding checkpoint when I continued training from the checkpoint and reset save_steps? My save_steps=5000 before, and now I changed it to 200, but after 200 steps, I did not get the corresponding checkpoint.

**KeShuang Liu** *Sun Jul 28 2024 21:09:52 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

training_args = TrainingArguments(

    output_dir=config.output_dir,

    overwrite_output_dir=True,

    report_to="none",

    num_train_epochs=config.n_epochs,

    per_device_train_batch_size=config.per_device_train_batch_size,

    gradient_accumulation_steps=config.gradient_accumulation_steps,

    per_device_eval_batch_size=config.per_device_eval_batch_size,

    logging_steps=10,

    eval_strategy="epoch",

    save_strategy="steps", 

    save_steps=200,        # æ¯5000æ­¥ä¿å­˜ä¸€æ¬¡

    # eval_steps=2000, 

    optim=config.optim_type,

    fp16=True,

    learning_rate=config.lr,

    warmup_steps=config.warmup_steps,

    resume_from_checkpoint="/liukeshuang/lora_model/gemma_bnb_4_g8/checkpoint-2873"

)



---

 # Comments from other users

> ## Piotr Gabrys
> 
> [EDIT] It's uncertain whether this code solves the problem.
> 
> Hi! You can do it like this:
> 
> ```
> training_args = TrainingArguments(
> output_dir=config.output_dir,
> overwrite_output_dir=True,
> report_to="none",
> num_train_epochs=config.n_epochs,
> per_device_train_batch_size=config.per_device_train_batch_size,
> gradient_accumulation_steps=config.gradient_accumulation_steps,
> per_device_eval_batch_size=config.per_device_eval_batch_size,
> logging_steps=10,
> eval_strategy="epoch",
> save_strategy="steps",
> save_steps=200, # æ¯5000æ­¥ä¿å­˜ä¸€æ¬¡
> # eval_steps=2000,
> optim=config.optim_type,
> fp16=True,
> learning_rate=config.lr,
> warmup_steps=config.warmup_steps
> 
> trainer = Trainer(
>         model=model,
>         args=training_args,
>         train_dataset=dataset['train'],
>         eval_dataset=dataset['test'],
>         tokenizer=tokenizer,
>     )
> 
> trainer.train("/liukeshuang/lora_model/gemma_bnb_4_g8/checkpoint-2873")
> 
> ```
> 
> reference: [https://github.com/huggingface/transformers/issues/7198](https://github.com/huggingface/transformers/issues/7198)
> 
> Hope that helps!
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Thanks for your reply, I will try
> > 
> > 
> > 
> > > ## Piotr Gabrys
> > > 
> > > Has it worked?
> > > 
> > > 
> > > 
> > > ## KeShuang LiuTopic Author
> > > 
> > > I used [@nbroad](https://www.kaggle.com/nbroad) method and it works fine.
> > > 
> > > 
> > > 


---

> ## Nicholas Broad
> 
> It's because you are resuming from checkpoint. It will use your old value when you resume. I think you can overwrite training_args.bin in your checkpoint to have the new values and it should work
> 
> 
> 


---

> ## Dlond Mike
> 
> same issue
> 
> 
> 


---



* --- discussion numver 91, the number of votes :2 ---

# Are We Really on the Right Track?

**Lorry Zou** *Sun Jul 21 2024 23:33:24 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

From the competition description:

"This challenge aligns with the concept of "reward models" or "preference models" in reinforcement learning from human feedback (RLHF). Previous research has identified limitations in directly prompting an existing LLM for preference predictions. These limitations often stem from biases such as favoring responses presented first (position bias), being overly verbose (verbosity bias), or exhibiting self-promotion (self-enhancement bias)."

Looks like the competition host encourage us to try reinforcement learning but everyone is still fine-tuning existing LLMs.ðŸ™‚ðŸ™ƒ



---

 # Comments from other users

> ## CPMP
> 
> RLHF is a supervised learning method.  Labels are provided by humans, and are quite similar to the labels we have in this competition.
> 
> Not sure what you suggest we do differently.
> 
> 
> 


---

> ## Dlond Mike
> 
> yepâ€¦.cause it's really perform great.it's a game for rich.(GPU :))
> 
> 
> 


---

> ## chan peter
> 
> I tried out rlhf model and use the reward score as input and build a simple classifier, it work out great, but running the rlhf model is too time comsuing and I joined comp a bit late, don't have enough time to optimize it to pass the time limit.
> 
> 
> 


---



* --- discussion numver 92, the number of votes :2 ---

# LMSYS: Research Papers Relevant for this Competition

**C R Suthikshn Kumar** *Fri Jul 26 2024 13:21:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Refer to the LMSYS - Chatbot Arena Human Preferences Predictions Competition:

[https://www.kaggle.com/competitions/lmsys-chatbot-arena/](https://www.kaggle.com/competitions/lmsys-chatbot-arena/)

Wishing all the best for participants in this competition. I am sharing useful reference papers, articles :

W Chiang et al., Chatbot Arena: An Open Platform for Evaluating LLMs by Human Preference, [https://arxiv.org/abs/2403.04132](https://arxiv.org/abs/2403.04132)
Chatbot Arena: Benchmarking LLMs in the Wild with Elo Ratings

by: Lianmin Zheng*, [https://lmsys.org/blog/2023-05-03-arena/](https://lmsys.org/blog/2023-05-03-arena/)
Introducing Hard Prompts Category in Chatbot Arena

by: Tianle Li, [https://lmsys.org/blog/2024-05-17-category-hard/](https://lmsys.org/blog/2024-05-17-category-hard/)
MSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset, [https://huggingface.co/datasets/lmsys/lmsys-chat-1m](https://huggingface.co/datasets/lmsys/lmsys-chat-1m)
A Survey on Human Preference Learning for Large Language Models

Ruili Jiang, et al, [https://arxiv.org/html/2406.11191v2](https://arxiv.org/html/2406.11191v2)
Align LLMs with Reinforcement Learning from Human Feedback: AI Learns Best with Rewards

by Maddie Lupu, [https://medium.com/@madalina.lupu.d/align-llms-with-reinforcement-learning-from-human-feedback-595d61f160d5](https://medium.com/@madalina.lupu.d/align-llms-with-reinforcement-learning-from-human-feedback-595d61f160d5)


---

 # Comments from other users

> ## Aadit Shukla
> 
> Thank you for sharing these valuable resources [@crsuthikshnkumar](https://www.kaggle.com/crsuthikshnkumar) and best of luck to all participants in the LMSYS competition!
> 
> 
> 


---

> ## Muhammad Ehsan
> 
> Great resources, [@crsuthikshnkumar](https://www.kaggle.com/crsuthikshnkumar) ! 
> 
> Your post provides valuable references for those diving into human preference predictions in chatbots. The papers and articles listed offer deep insights into evaluating and aligning LLMs with human preferences. Best of luck to everyone participating in the competition.
> 
> 
> 


---



* --- discussion numver 93, the number of votes :2 ---

# Can anyone share some trick about kaggle competitions?

**YEI0907** *Sat Jul 27 2024 01:54:10 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Hello everyone, this is my first time competing on Kaggle. Here are some of my questions, and I really hope someone can answer them for me

How to perform hyperparameter optimization? Random method or Bayesian method?

Have you adopted cross validation methods for large language models such as Llama and Gemma. Should we choose the model with the lowest loss fold for inference after cross validation, or train the model on all data?

How to effectively avoid 'CUDA OUT of Memory'?, Sometimes my training code is consistent with some public notebooks, and even the config is consistent, but there are still "CUDA OUT of Memory" issues, even if the length is kept at 1024. In addition, my GPU is A100-80g

Is QLora really more effective than FB16 fine-tuning?

How to increase inference time more effectively

I would greatly appreciate it if someone could answer my question



---

 # Comments from other users

> ## justin1357
> 
> 
> In many competiiton you can use optuna to search hyperparameters automatically, but in this one, not. My solution is to tune them by hand and check if it will work better by experiment.
> Cross Val is great but we don't have so much money and time to do a 5-fold training, in this competition, the relation between cv and   lb is stable, so you can just use like 20% of full data as your val data.
> check your code, its more likely caused by bug.
> Yes in this competition.
> You mean 'reduce infer time'? There are some way to optimize your speed like flash-attn, deepspeed, and so onâ€¦
> 
> 
> > ## YEI0907Topic Author
> > 
> > thanks! good luck to  you ,my friend
> > 
> > 
> > 


---



* --- discussion numver 94, the number of votes :2 ---

# Larger model, better result?

**Dylan Liu** *Fri Jul 26 2024 18:25:10 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

I tried to train gemma2-27b but it's valid score is not better than that of 9b. What's your experience?



---

 # Comments from other users

> ## justin1357
> 
> Better is greater.. You can compare them in poe, and then you will find they are totally different. Zero Shot.
> 
> 
> 


---

> ## ShelterW
> 
> What method do you useï¼ŸSEQ_CLS?
> 
> 
> 
> > ## Dylan LiuTopic Author
> > 
> > yes SEQ_CLS
> > 
> > 
> > 


---



* --- discussion numver 95, the number of votes :2 ---

# [Need help] How to custom Head for AutoModelForSequenceClassification with LoRA?

**Bao Loc Pham** *Sat Jul 20 2024 13:52:51 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

I've a model look like this

```
Gemma2ForSequenceClassification(
  (model): Gemma2Model(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x Gemma2DecoderLayer(
        (self_attn): Gemma2SdpaAttention(
          (q_proj): Linear4bit(in_features=3584, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)
          (v_proj): Linear4bit(in_features=3584, out_features=2048, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=3584, bias=False)
          (rotary_emb): Gemma2RotaryEmbedding()
        )
        (mlp): Gemma2MLP(
          (gate_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=3584, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=3584, bias=False)
          (act_fn): PytorchGELUTanh()
        )
        (input_layernorm): Gemma2RMSNorm()
        (post_attention_layernorm): Gemma2RMSNorm()
        (pre_feedforward_layernorm): Gemma2RMSNorm()
        (post_feedforward_layernorm): Gemma2RMSNorm()
      )
    )
    (norm): Gemma2RMSNorm()
  )
  (score): Linear(in_features=3584, out_features=3, bias=False)
)

```

After applied LoRA, the model look like this

```
PeftModelForSequenceClassification(
  (base_model): LoraModel(
    (model): Gemma2ForSequenceClassification(
      (model): Gemma2Model(
        (embed_tokens): Embedding(256000, 3584, padding_idx=0)
        (layers): ModuleList(
         ....
            )
            (mlp): Gemma2MLP(
              ....
            )
            ...
          )
        )
        (norm): Gemma2RMSNorm()
      )
      (score): ModulesToSaveWrapper(
        (original_module): Linear(in_features=3584, out_features=3, bias=False)
        (modules_to_save): ModuleDict(
          (default): Linear(in_features=3584, out_features=3, bias=False)
        )
      )
    )
  )
)

```

Because of score module is just a simple fully connected layer, I want to make it more complex,

I tried to replace the head like this

```
CustomGemmaForSequenceClassification(
  (model): GemmaModel(
    (embed_tokens): Embedding(256000, 3584, padding_idx=0)
    (layers): ModuleList(
      (0-41): 42 x GemmaDecoderLayer(
        ...
        )
        (mlp): GemmaMLP(
        ...
        )
        (input_layernorm): GemmaRMSNorm()
        (post_attention_layernorm): GemmaRMSNorm()
      )
    )
    (norm): GemmaRMSNorm()
  )
  **(score): Sequential(
    (0): Linear4bit(in_features=7168, out_features=3584, bias=True)
    (1): Linear(in_features=3584, out_features=3, bias=False)
  )**
)

```

But after applied peft and LoRA, there is an error 

```
p.requires_grad_(requires_grad)
RuntimeError: only Tensors of floating point dtype can require gradients

```

I already put lora module_to_save=["score"] like this [huggingface tutorial](https://huggingface.co/docs/peft/en/developer_guides/custom_models) but seem not working yet



---

 # Comments from other users

> ## CPMP
> 
> Why use LORA on a matrix of rank 3?
> 
> The classification head linear layer has a rank of at most 3 because its dimension is 3584x3. 
> 
> TL;DR it does not make sense to apply LORA to the classification head.
> 
> 
> 
> > ## Bao Loc PhamTopic Author
> > 
> > [@cpmpml](https://www.kaggle.com/cpmpml) 
> > 
> > thank for your comment, the rank 3 you mean is the number of class.
> > 
> > - Yes, I don't want to apply LORA to the classification head.
> > 
> > I just apply these code like the huggingface tutorial
> > 
> > ```
> > lora_config = LoraConfig(
> >     r=config.lora_r,
> >     lora_alpha=config.lora_alpha,
> >     # only target self-attention
> >     target_modules=config.target_modules,
> >     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
> >     lora_dropout=config.lora_dropout,
> >     bias=config.lora_bias,
> >     task_type=TaskType.SEQ_CLS,
> > )
> > model = AutoModelForSequenceClassification.from_pretrained(
> >     config.checkpoint,
> >     num_labels=3,
> >     torch_dtype=torch.float16,
> >     device_map="auto",
> >     quantization_config=quantization_config
> > )
> > model.config.use_cache = False
> > model = prepare_model_for_kbit_training(model)
> > model = get_peft_model(model, lora_config)
> > print(model)
> > 
> > print(model.print_trainable_parameters())
> > 
> > ```
> > 
> > the AutoModelForSequenceClassification class will create a simple neural network with 3 output.
> > 
> > but I want to replace with CustomModelForSequenceClassification with my custom head.
> > 
> > 
> > 
> > > ## Ashwani
> > > 
> > > you can specify the target_modules in which you want to apply LoRA. target_modules=["query", "key", "value"] specifies that LoRA should only be applied to the attention modules, effectively excluding the classification head.
> > > 
> > > 
> > > 
> > > ## CPMP
> > > 
> > > 
> > > the rank 3 you mean is the number of class.
> > > 
> > > The rank is  the rank of the matrix. LoRA is about approximating a high rank matrix with a low rank matrix. That's the lora_r parameter of LoRA. 
> > > 
> > > What I am saying is that applying LoRA to the classification head only makes sense with lora_r smaller than 3.
> > > 
> > > 
> > > 


---



* --- discussion numver 96, the number of votes :2 ---

# Gemma2 9b's inference time is much longer that Llama3 8b?

**Dylan Liu** *Wed Jul 17 2024 15:55:30 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

With same submission code, my Llama3 8b model takes ~4h to finish the inference, but my Gemma2 9b takes ~8h. Are you experiencing the same?



---

 # Comments from other users

> ## Ashwani
> 
> I haven't seen such difference. For me its 25% more time in gemma than lamma. 
> 
> If you want to further reduce inference time, check dynamic padding for each batch. ðŸ˜€
> 
> 
> 


---

> ## Sparsh Tewatia
> 
> 2 billion parameters more at work my friend.
> 
> 
> 
> > ## Dylan LiuTopic Author
> > 
> > 2 billion parameters? I thought it was 1b different. But even so, double inference time is still not much explainable.
> > 
> > 
> > 
> > > ## Sparsh Tewatia
> > > 
> > > gemma always claims less parameter if you count it shows 10.2 billion parameters , also LLAMA 3 uses grouped query attention , and has around 120 K tokens in tokenizer while Gemma uses self attention and has 250 K tokens in tokenizer which can explain the difference in speed.
> > > 
> > > 
> > > 


---

> ## Yichuan Gao
> 
> I would check the data type for both weights and compute_dtype. If you are using bfloat16 in compute, it will be MUCH slower since T4 does not support bfloat16, and need to emulate it by other methods. In my experience, Gemma2 9b and Mistral 7b inference time does not have much a difference (3~4h range), provides using 4bit weights and float16 dtype.
> 
> 
> 


---

> ## Valentin Werner
> 
> For me, also training time with same parameters is 50% slower than Llama3-8b which seems insane. But its all in the architecture, as Sparsh pointed out.
> 
> 
> 
> > ## Robert0921
> > 
> > For LoRa, even though Gemma2 is more accurate than Llama3, I was unable to achieve better results due to the 9-hour time limit.
> > 
> > 
> > 


---

> ## Robert0921
> 
> Not only inference, but also training takes longer, because 9b>8bï¼Ÿ
> 
> 
> 


---



* --- discussion numver 97, the number of votes :2 ---

# Is label smoothing beneficial in LLM fine-tuning?

**Yichuan Gao** *Sat Jul 13 2024 12:02:54 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

I'm using LoRA to fine-tune a Gemma2ForSequenceClassification model.

I'm wondering if add label smoothing is a good or bad thing in this process. Since if I add smoothing of 0.2 (i.e., label is [0.8, 0.1, 0.1] ), I'm getting a eval_loss higher than LB score (0.98 vs 0.96), maybe smoothing made my model less confident than it could be?

Could anyone share some experience on this topic? Would you add it, and if you do, how much is a sweet spot?



---

 # Comments from other users

> ## Valentin Werner
> 
> Normally if you are doing a task with < 60% accuracy and try to minimize loss, label smoothing should be helping, as its better to have less confident but correct classification rather than super confident wrong predictions. However, if your model is well calibrated without label smoothing, you should simply not use it. It helped a lot in my earlier experiments with DeBERTa though..
> 
> 
> 
> > ## Yichuan GaoTopic Author
> > 
> > Thanks for this information! I'll try to apply less smoothing now :)
> > 
> > 
> > 


---

> ## yechenzhi1
> 
> Same here, label smoothing is not beneficial for me.
> 
> 
> 


---



* --- discussion numver 98, the number of votes :2 ---

# Time constraint for private leaderboard [Solved]

**raconion** *Fri Jul 05 2024 11:15:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

There are roughly 25,000 rows in the test set. Among them, 26% are used for public lb while the rest 74% are used for private lb. 

Since in Overview section, the time constraint is 9 hrs, does this mean that our notebook has to finish the inference for 74%*25,000 =18,500 rows in the test set? Or this time constraint is for public lb and will be scaled according to the number of rows when it comes to private lb?

[@addisonhoward](https://www.kaggle.com/addisonhoward) [@mylesoneill](https://www.kaggle.com/mylesoneill) Would really appreciate if you can clarify this!

Update:

Our notebook will be run for all 25k rows but only 26% shown on public lb. Thanks for the comment [@lizhecheng](https://www.kaggle.com/lizhecheng) 

This comment also clarify this issue: [link](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/516995#2904512)



---

 # Comments from other users

> ## Enter your display name
> 
> As long as you can see your public score, your private score has also been calculated, but you just can't see it for now. Thus you don't need to worry about that.
> 
> 
> 
> > ## jiangli59
> > 
> > I vote for that. Is it possible to extend the time budget over 9 hrs? Or, Do we have other opinions to solve that? My code is extremely overwhelmed for the inference budget.  
> > 
> > 
> > 
> > > ## raconionTopic Author
> > > 
> > > I don't think the time constraint can be extended unless the competition host decides so. There are way arounds though such as all kinds of efficient inference techniques.
> > > 
> > > 
> > > 


---



* --- discussion numver 99, the number of votes :2 ---

# Submission Scoring Error

**RomanZubarev** *Wed Jun 12 2024 20:49:41 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Hi! I'm a newbie and it's my first time participating in a competition here. Can anyone tell me why I get "Submission Scoring Error"? Everything seems to be under the rules and expectations of the result.

[2024-06-12 14-44-23.png](https://storage.googleapis.com/kaggle-forum-message-attachments/2868474/20808/2024-06-12 14-44-23.png)

---

 # Comments from other users

> ## Ahmad Al-Husainy
> 
> Was the notebook execution successful, or did it fail? If it succeeded, the only other possible reason I can think of, aside from what other Kagglers mentioned about probabilities summing up to more than one, could be the data format in the columns . If the notebook failed, you should check the execution logs, which will show you where it failed. 
> 
> 
> 


---

> ## Valentin Werner
> 
> You can try setting index=False during saving the csv, that caused problems for me before I think.
> 
> Is it because (winner_model_a + winner_model_b + winner_tie) > 1?
> 
> I don't think this should matter, as the log loss implementation is a wrapper for sklearn, where this is not an issue from my experiments
> 
> 
> 


---

> ## Masayuki Takahashi
> 
> Is it because (winner_model_a + winner_model_b + winner_tie) > 1?
> 
> 
> 
> > ## Anya
> > 
> > Good remind! I found my submission data has this problem.
> > 
> > 
> > 


---



* --- discussion numver 100, the number of votes :2 ---

# Download Llama3 Models from Kaggle?

**Valentin Werner** *Mon Jun 17 2024 12:41:39 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Is there a way for me to download the Llama3 Models that were trained by others from Kaggle? I am talking about the model that is used in all the top scoring public notebooks. When I click on the dataset, I only see a placeholder file with about 30 MB of data.



---

 # Comments from other users

> ## Kishan Vavdara
> 
> They are lora weights, you can take llama-3 model from kaggle and add lora weights for further training/inference. 
> 
> 
> 
> > ## Valentin WernerTopic Author
> > 
> > Ah, I have never used lora weights before like this. Thank you for the clarification!
> > 
> > 
> > 


---



* --- discussion numver 101, the number of votes :2 ---

# Replacing response A and response B for Data Augmentation

**Takamichi Toda** *Wed Jun 05 2024 09:29:20 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

The current approach in the public code often creates features from responses A and B and uses these to train classifiers. I thought that a simple data augmentation could be achieved by swapping responses A and B and the winner labels.

However, it not works.

|  | Local | Public |
| --- | --- | --- |
| baseline | 0.997 | 1.012 |
| Augument by replace A/B | 1.011 | 1.025 |

My CV strategy is a simple one-holdout, and so far it correlates well with the Public LB ([reference](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/500031#2824772)).

It may be that whether the response is A or B is also an important feature. I had seen a thread discussing bias in evaluation depending on whether the response is A or B, but it seems to have disappeared (probably [here](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/507091)).



---

 # Comments from other users

> ## Lisa DunlapCompetition Host
> 
> Not sure how helpful this is, but position bias is a known phenomenon in both humans and LLMs: both tend to favor the first answer they are presented with. We look at this in the [original LLM as a judge paper ](https://arxiv.org/abs/2306.05685) if you want some concrete numbers of how prevalent this is 
> 
> 
> 


---

> ## Valentin Werner
> 
> My assumption is that by simply swapping, you are not creating new value for the model to learn. You are instead basically training those rows twice.
> 
> Questions:
> 
> 1) with what percentage of samples are you augmenting? If you only do 10-20% you are just making the model overfit / learn more about those samples. There might be an argument to do 100% of samples to make the model learn that resp A or B literally does not matter! (even though this might not reflect reality)
> 
> 2) are you also doing the swap with ties (keeping the tie label)? If not, you introduce class imbalance and ties are less likely to be predicted.
> 
> 
> 
> > ## Takamichi TodaTopic Author
> > 
> > Thank you for the comment.
> > 
> > 1)
> > 
> > It's 100%. By the way, I am using DeBERTa, and I conducted experiments to enable the model to know which sentence is A and which is B by adding special tokens, but it was not very effective (only a slight improvement).
> > 
> > 2)
> > 
> > The label for "tie" remains "tie" even after swapping.
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > So If you do 100%, you basically just train two epochs at the price of one. This will effect lr scheduling etc.
> > > 
> > > Did you also tune parameters in your experiment (e.g., warm up ratio or epochs)
> > > 
> > > 
> > > 
> > > ## Takamichi TodaTopic Author
> > > 
> > > You may be right.
> > > 
> > > We tried three different patterns for the learning rate (smaller is better).
> > > 
> > > 
> > > 


---



* --- discussion numver 102, the number of votes :2 ---

# predict the model of the response to to a classification?

**Lee** *Sun May 26 2024 18:24:03 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Hello, I'm new to Kaggle and I've got an idea: What if we shift from simply evaluating responses to a 3-class classification, to a  64-class classification? Here's the plan:

First, use our training data to train a classification model. This model will help us predict which among the 64 models a given response belongs to.

Then, during the inference phase, armed with our trained classifier, we'll categorize each response into one of the 64 model types.

With this information in hand, we can ascertain which two models are in competition. Leveraging the training dataset as our prior knowledge, we'll then proceed to predict the likely winner between these two models.

[Translated from chatGPT] Sorry for your uncomfortable reading, I am not a naive English speakerðŸ™



---

 # Comments from other users

> ## Valentin Werner
> 
> I think this can be a valuable proxy or feature for prediction. But you should keep in mind that the best model had ~65% winrate, so even if you know the model, it is difficult to predict whether it will win.
> 
> As such, I can imagine that this is one feature among text embeddings or the length feture. But predicting the model that wrote a response is similar difficult to predicting the win directly. You will have less training data per class etc.
> 
> I think a similar strategy was also proposed in the Detect AI Generated Text Competion.
> 
> 
> 
> > ## Ivan Vybornov
> > 
> > Model of a response is an immensely valueable feature. Tried adding it to the lgbm locally: a few features like length of prompt and responses alongside with model name gives a score of around 0.99 with CV. 
> > 
> > Though I am concerned that new models might appear in the private set (not sure if it is a reasonable concern).
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > I dont think it is a reasonable concern. If you are able to reliably predict the model, than a new model will likely fall into the "next best" category. It would probably reduce score compared to if you know all models but you would likely still gain. 
> > > 
> > > also from my knowledge the model distribution seems not immensely imbalanced to the point where only a few responses exist for a model. Therefore, I imagine this would not be the case
> > > 
> > > 
> > > 


---



* --- discussion numver 103, the number of votes :2 ---

# Easily Visualize and Compare Prompt, Response and Winner

**Nazim Cherpanov** *Mon May 27 2024 18:42:02 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

## Introduction:

When dealing with conversational AI datasets, it's crucial to grasp the quality of the responses produced by various models. In this conversation, I will introduce a useful function that enables you to easily visualize and compare the responses for a specific prompt, as well as the top-performing model.

## Function Overview:

The function get_info_by_id(index) takes an index as input and retrieves the corresponding prompt, responses from two models (Model A and Model B), and the winner from the Kaggle dataset. It then displays the information in an easy-to-read format.

## Code Snippet:

```
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
train = pd.read_csv(train_path, index_col='id').reset_index(drop=True)

ef get_info_by_id(index):
     if index not in list(train.index):
         display("index not in train")
     else:
        print(f"\n{'*'*10} Prompt {'*'*10}\n")
        display(train.iloc[index]['prompt'])

        print(f"\n\n{'*'*10} response A {'*'*10}\n")
        display(train.iloc[index]['response_a'])

        print(f"\n\n{'*'*10} response B {'*'*10}\n")
        display(train.iloc[index]['response_b'])

        print(f"\n\n{'*'*10} Winner {'*'*10}\n")
        if train.iloc[index]['winner_model_a'] == 1:
            display('Model A')
        elif train.iloc[index]['winner_model_b'] == 1:
            display('Model B')
        else:
            display('Tie')`

get_info_by_id(3)

```

## Usage:

To use the function, simply call get_info_by_id() with the desired index from the train dataset. For example, get_info_by_id(3) will display the prompt, responses, and the winner for the row with index 3.

## Happy Kaggling!





* --- discussion numver 104, the number of votes :2 ---

# how can I load a pretrained model with internet off

**Dirk N** *Tue May 14 2024 23:56:23 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

It seems I cannot use pip install, what is the best way to load a pretrained model with internet off?



---

 # Comments from other users

> ## RobsonDSP
> 
> I tried download a model from huggingface but until now is not working. I cloned the model to my local machine and uploaded it to my private space here on Kaggle as dataset. I uploaded all files, config.json, tf_model.h5, vocab.json and others. I tried to load them using the code bellow:
> 
> from transformers import AutoModelForSequenceClassification
> 
> from transformers import TFAutoModelForSequenceClassification
> 
> from transformers import AutoTokenizer, AutoConfig
> 
> import numpy as np
> 
> from scipy.special import softmax
> 
> MODEL = f"/kaggle/input/pretrained-model-from-huggingface/"
> 
> tokenizer = AutoTokenizer.from_pretrained(MODEL)
> 
> config = AutoConfig.from_pretrained(MODEL)
> 
> model = AutoModelForSequenceClassification.from_pretrained(MODEL)
> 
> Now I'm getting the following error message:
> 
> OSError: You seem to have cloned a repository without having git-lfs installed. Please install git-lfs and run git lfs install followed by git lfs pull in the folder you cloned.
> 
> When I run the commands in my machine it starts to download a huge file. I stopped at 1GB and the progress bar at 0%. I intended to upload this file to my account on Kaggle too but I stopped because I'm probably doing something wrong. 
> 
> I really don't know what to do now because I cannot enabled the internet access.
> 
> 
> 


---

> ## Muhammad Tariq Pervez
> 
> [@dirknbr](https://www.kaggle.com/dirknbr), Kaggle competition rules don't impose restrictions to download a model and use it. In Kaggle competitions, "disabling internet" means that the code you submit to Kaggle for scoring is executed in an environment that does not have access to the internet. Ensure your submission does not include any code that requires internet access, such as downloading data from external URLs or accessing online APIs.
> 
> 
> 


---

> ## Kishan Vavdara
> 
> Keep train and inference separate notebooks, download/load/train model in train notebook and import it in inference notebook. 
> 
> 
> 


---

> ## djchen
> 
> You can download the pertrained model on huggingface and upload it to Kaggle as a model, then you can load such pertrained model in your notebook.
> 
> 
> 


---

> ## Simon Veitner
> 
> You can clone the huggingface repository and upload it as a dataset. There are many examples how to load it from there.
> 
> Also you should check, if somebody else did it already :)
> 
> 
> 


---



* --- discussion numver 105, the number of votes :2 ---

# I have started hallucinating because of data :D

**Kuldeep Rathore** *Tue May 14 2024 19:58:09 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

Only hallucinating comments are welcome on this thread â˜ºï¸



---

 # Comments from other users

> ## G John Rao
> 
> Yeah, it's funny but this goes under the pre-processing part
> 
> 
> 


---

> ## kheder yazgi
> 
> same models?
> 
> 
> 


---



* --- discussion numver 106, the number of votes :2 ---

# [New Starter] 3 texts + Huggingface

**Heroseo** *Sun May 05 2024 05:16:42 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (2 votes)

I shared a new notebook for Huggingface users.

This notebook uses huggingface and uses all three texts in the tokenizer.

- prompt, response_a, response_b

V3 - add truncate_text - but score got a little worse

- link: [[Train] LMSYS / Deberta-v3 meets Huggingface](https://www.kaggle.com/code/piantic/train-lmsys-deberta-v3-meets-huggingface/notebook)



---

 # Comments from other users

> ## Nicholas Broad
> 
> your model only predicts label 1?
> 
> 
> 
> > ## Valentin Werner
> > 
> > I guess you can save some inference time on those predictions ðŸ˜‰
> > 
> > 
> > 
> > ## HeroseoTopic Author
> > 
> > thank you for telling me. I will check again later and update.
> > 
> > p.s. But now I've run out of Kaggle GPU time. ðŸ¥²
> > 
> > 
> > 


---



* --- discussion numver 107, the number of votes :1 ---

# How crazyï¼Top 1: LB 707 â€”â€”> 663!

**Sun And Ray** *Sun Aug 04 2024 09:55:22 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I'm not sure how this guy managed to quickly move from LB: 880 to LB: 707, and then to LB: 663. I suspect his training plan is similar to those of other teams in the gold tier, but the training data might be very different!

Additionally, in a discussion thread titled â€œSuspicious Surge of Novice Accounts on the Leaderboard,â€ I initially thought these "newcomers" wouldnâ€™t affect the Top 100, but this morning when I checked the leaderboard, I found myself around 80th place. I admit my guess was wrong.

Lastly, my teammates and I are discussing whether to make our training and inference solutions public today or tomorrow in order to counter these "potentially cheating newcomers."



---

 # Comments from other users

> ## CPMP
> 
> Don't publish your solution before competition end! 
> 
> 
> 


---

> ## heartkilla
> 
> Please do not publish your solution before the deadline. This will do nothing but create unnecessary turbulence on the leaderboard by giving advantage to those who have enough time to incorporate your ideas. This imbalance will be even more amplified giving the time zone difference across the participants.
> 
> 
> 
> > ## Sun And RayTopic Author
> > 
> > Our team has spent a lot of money renting GPU resources, but it seems that our investment has yielded unequal returns.
> > 
> > 
> > 
> > > ## XXX
> > > 
> > > sorry to heard thatðŸ¥º
> > > 
> > > 
> > > 
> > > ## heartkilla
> > > 
> > > That is unfortunate but does not justify publishing high scoring submissions 2 days before the deadline.
> > > 
> > > 
> > > 


---

> ## Kamau
> 
> So sad! This leaves me thinking of all the hours, sweat, and tears some of us have put into this. It may not look like it, but this is truly disheartening. It almost explains how I moved from the top 100 to lower ranks on the leaderboard in barely a week. The situation reminds of the quote:
> 
> "What's the point of fighting my way up the mountain when someone else is just going to come up the easy way?"
> 
> ~ from The Key of the Titans
> 
> 
> 


---

> ## Ya Xu
> 
> Have faith, I believe Kaggle will deal with cheaters just like they did recently.
> 
> 
> 
> > ## Sun And RayTopic Author
> > 
> > I hope so!
> > 
> > 
> > 


---

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> 
> Lastly, my teammates and I are discussing whether to make our training and inference solutions public today or tomorrow in order to counter these "potentially cheating newcomers."
> 
> -- As per Kaggle policy, top notebooks are not allow to release in last 7 days. I surprise, won't see that message now.
> 
> 
> 


---

> ## Victor Lu
> 
> This competition is really becoming increasingly unpredictable. Iâ€™m very curious to see what changes will occur on the leaderboard after the competition ends. 
> 
> 
> 


---

> ## Dlond Mike
> 
> sadT.T,i thought it woule be my first time i could get a medalâ€¦.
> 
> 
> 


---

> ## XXX
> 
> Hi [@sunandray](https://www.kaggle.com/sunandray), may i ask how many "newcomers" do you think in top 100?ðŸ¤”
> 
> 
> 
> > ## Sun And RayTopic Author
> > 
> > I'm not sure how many teams there are in the medal zone, as my focus has primarily been on the silver zone. In the silver zone, I feel there are at least 10 teams that might be cheating. This is because I've noticed some teams were around 938 on the LB a few days ago, and suddenly they are around 900 today.
> > 
> > 
> > 
> > > ## XXX
> > > 
> > > So, by cheating, do you mean that these teams use the same leaked data as the first one?
> > > 
> > > 
> > > 
> > > ## Enter your display name
> > > 
> > > No, I think he means that they share their solutions privately.
> > > 
> > > 
> > > 
> > > ## XXX
> > > 
> > > thank you 
> > > 
> > > 
> > > 


---



* --- discussion numver 108, the number of votes :1 ---

# 1h -> .707! Is the dataset leaked?

**Roschild.Rui** *Sun Aug 04 2024 04:36:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Unbelievable! I have great desire to know whatâ€™s really help and how could the master achieve this after the competition ending



---

 # Comments from other users

> ## sayoulala
> 
> I believe it's very likely. My training set loss couldn't even reach that score. As of now, his score has risen to 0.6 againâ€¦ I hope Kaggle officials can handle this matter fairly.
> 
> 
> 
> > ## Valentin Werner
> > 
> > Training loss doesn't even get close to it. Even ignoring all remarks on speed of submission, I dont think a model that could run inference in an hour (likely the size of deberta Base or smaller), is even  able to overfit this hard on a problem this complex. 
> > 
> > Not going to lie, I would love him to have come up with some insane solution.
> > 
> > 
> > 
> > > ## Cody_Null
> > > 
> > > I feel the same way but I would be completely shocked haha
> > > 
> > > 
> > > 
> > > ## sayoulala
> > > 
> > > If he can come up with a better solution, the whole world will thank him. But if he's exploiting some loopholes, that would be truly disappointing.
> > > 
> > > 
> > > 


---

> ## Cody_Null
> 
> I hope he doesnâ€™t share before competition end. Because if he does the comp is ruined lol 
> 
> 
> 


---



* --- discussion numver 109, the number of votes :1 ---

# postprocess data that have [null] responses

**ano** *Fri Aug 02 2024 22:59:02 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Training dataset have some data whose responses for both A and B are [null].

[https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303)

Therefore, I implemented the code to postprocess null data, assuming the same percentage of winner_model_a, winner_model_b and winner_tie for the train and test dataset. However, the score got worse a little bit. Has anyone had the same experience?



---

 # Comments from other users

> ## Valentin Werner
> 
> I think the problem we are facing is how random the results for these samples are. 
> 
> Guessing average distributions for these samples might be a good approach (if actually both are null), but I think you are focusing on a very niche problem for your post-processing. On the 57k training samples, 19 samples fit into this pattern according to the post you link. Assuming same distribution in the dataset you are doing postprocessing for 10 samples. And 2-3 of which would be in the public lb.
> 
> As the 19 samples in the training set are not a representative sample size to start with, it is quite likely that you should just "trust your model". 
> 
> 
> 
> > ## anoTopic Author
> > 
> > I appreciate your detailed explanation. It seems like the randomness of the results for these samples is indeed a significant factor. The null data in the test dataset, which you estimated to be only 2-3 affecting the public leaderboard, is just too small.
> > 
> > In this case, it might be ineffective to assume the same distribution based on these few samples.
> > 
> > 
> > 


---



* --- discussion numver 110, the number of votes :1 ---

# How many epochs do people usually choose when fine-tuning?

**KeShuang Liu** *Sat Jul 27 2024 22:40:05 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

As a beginner, I am eager for a score increase, but reality keeps hitting me. If we train for 1 epoch and 2 epochs with the same parameters and configuration respectively, will training for 2 epochs produce the same results as training for 1 epoch at the end of the first epoch, or will it be slower to become proficient?



---

 # Comments from other users

> ## Valentin Werner
> 
> There are a bunch of different questions in here. First, lets losely define what an epoch is: An epoch is a Forward and Backward Propogation through your whole Training dataset; this means, that your model saw all your training data once. Two epochs mean it saw all the training data twice.
> 
> This means, that the results of one epoch training and two epochs training will be different. However, the results of epoch 1 in a single epoch training should be around the same as the results of epoch 1 in a two epoch training (assume no learning rate scheduling).
> 
> What is a go-to number of epochs? The go-to number I see most often when finetuning pretrained models is three. However, for larger models (with peft) this value tends to be lower (for example two) and for larger datasets this value tends to be lower too. This is because the model learns more information within a single epoch.
> 
> Now, the last important note on epochs is learning rate scheduling. Often learning rates are scheduled to reduce learning rate in later epochs. Lets assume the learning rate decreased linear from start of the first epoch until the end of the third epoch. This means that the model will overfit less in the second and even less in the third epoch, while still being able to learn nuances about the training data, that can improve your score. This also means that a single epoch training with lr scheduling will have different results than a two epoch training with scheduling, as the learning rate will hit 0 much earlier in the first case. 
> 
> In general, transformer trainings are non-deterministic and you need to set a seed if you want to replicate exact results.
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Thank you for your reply.I have learned a lot from it, I will keep trying more.
> > 
> > 
> > 


---

> ## Mr.T
> 
> When I train with two epochs, I experience severe overfitting.
> 
> 
> 


---

> ## xiaotingting
> 
> The more data there is, the smaller the epochs need to be fine-tuned. Otherwise, the larger the epochs need to be fine-tuned.
> 
> 
> 


---

> ## KeShuang LiuTopic Author
> 
> Can someone help me clarify my doubts
> 
> 
> 


---



* --- discussion numver 111, the number of votes :1 ---

# question about the quota of GPU

**Dlond Mike** *Mon Jul 29 2024 20:57:34 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Does submission take the quota of GPU?if so,how much quota it will take?



---

 # Comments from other users

> ## CPMP
> 
> saving your notebook runs it again using your quota. Once this is done, submitting does not use your quota anymore.
> 
> 
> 


---

> ## Yi-Fu Chen
> 
> ```
> import os
> if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):
>     pass
> else:
>     raise SystemExit
> 
> ```
> 
> Adding the above code at the front of the submitted notebook will quickly close the trial run notebook and retain the scored run.
> 
> 
> 


---

> ## Ravi Ramakrishnan
> 
> I usually submit to code competitions in a script. I use the below to ensure I don't use up my GPU quotas during my dummy LB submission- 
> 
> ```
> import pandas as pd
> sub_fl = pd.read_csv(.......submission.csv)
> 
> if len(sub_fl) <=10:
>     print(f"Submitting the dummy file")
>     sub_fl.to_csv("submission.csv", index = None)
> 
> else:
>     ....... (your script)
> 
> ```
> 
> [@dlondmike](https://www.kaggle.com/dlondmike) best of luck!
> 
> 
> 


---

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> [@dlondmike](https://www.kaggle.com/dlondmike) for scorning won't take GPU quota, but for generating the submitted notebook version ( even after submit, you can cancel the notebook, so GPU quota can be saved )
> 
> 
> 
> > ## Valentin Werner
> > 
> > From what I know (and experienes last week), the "save" notebook (not submit) also doesnt crash if it goes above quota. I think I ended last week on 32/30 hrs by accident ðŸ˜ƒ
> > 
> > 
> > 


---

> ## bao
> 
> There are two notebooks running when submitted. The scoring notebook does not use GPU quota, while the other one does.
> 
> 
> 


---



* --- discussion numver 112, the number of votes :1 ---

# Finetuned --> Quantized vs Quantized--> Finetuned

**Varun Jagannath** *Fri Jul 26 2024 23:22:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Which approach is performing better in this competition: fine-tuning a model like Llama 3 and then quantizing it, or taking a low-bit quantized model and fine-tuning it on the dataset ?



---

 # Comments from other users

> ## Valentin Werner
> 
> I hope somebody who tested it actually answers and knows better than me. My intuition is that Finetune -> Quantize should be better, as the finetuning is more precise. Obviously there is also the argument that this precision is quantized later anyways and maybe training in a quantized way makes sure your val / lb is more consistent
> 
> 
> 
> > ## Pranshu Bahadur
> > 
> > Ok so I kind of tested this scenario a bit and I agree with your hypothesis, when I trained gemma 2 9b on bfloat16 training loss went down to 0.44 (definitely a sign of overfitting). I think quantization should be done post-training.
> > 
> > 
> > 
> > ## Maksim Metelskii
> > 
> > LoRa adapters (which are 16 or 32 bit) trained on quantized model may help to fix inaccuracy stemmed from quantization. Like they address both quantization and new specific task inaccuracy. ChatGPT says Quantized--> Finetuned may be more beneficial for accuracy. But really need to be tested though
> > 
> > 
> > 
> > ## Varun JagannathTopic Author
> > 
> > My observation is that earlier TPU train notebook which was published got nearly around 0.98 LB and the latest 0.94LB of Training and Inference with unsloth gemma 2. So really wanted to understand if the quantized followed fine tuning is performing well in this competition. 
> > 
> > 
> > 


---

> ## xiaotingting
> 
> I think fine-tuning after quantization is better because it can make up for the loss caused by quantization. Maybe fine-tuning the quantized model and quantizing after training require different learning rates.
> 
> 
> 


---



* --- discussion numver 113, the number of votes :1 ---

# Am I overfitting?

**KeShuang Liu** *Sat Jul 27 2024 12:07:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)





---

 # Comments from other users

> ## Valentin Werner
> 
> Overfitting is best analyzed in combination with validation loss. If you validation loss has a similar drop to your training loss (which I guess would win the competition at these scores), you are not overfitting. In general, if training loss goes down while validation loss is either plateauing or going back up, you are likely overfitting.
> 
> Another way of estimating overfitting is to look at what performance you expect and what you see on the train loss. We would not expect the model to go to .800 or even below it - therefore, it is likely to overfit. However, this does not mean that the model was best before this downward shift at steps 550 - you should use a validation score to evaluate how well the model predicts on unseen data
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Yes, but my validation set can only be calculated after this epoch is completed, and it takes a long time, so I am considering whether to stop it directly
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > You can set how often you want to evaluate. It does indeed take a lot of time, so I think evaluating 2-4 times is feasible.
> > > 
> > > 
> > > 
> > > ## KeShuang LiuTopic Author
> > > 
> > > Yes, my model should be overfitting. Its loss on the validation set is 0.99
> > > 
> > > 
> > > 


---

> ## xiaotingting
> 
> It has to be combined with the indicators of the validation set. If the loss on the validation set is large, but very low on the training set, it is overfitting, and you can consider adding regularization such as weight decay. If the loss on both the validation set and the training set is low, it means the model is effective.
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > The next validation metric will be calculated in a few hours, and I am considering whether to abandon this training directly
> > 
> > 
> > 


---

> ## Rise_Hand
> 
> Wow so Crazy! 600 epochs !!! Which kind of model you are using 
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > steps,haha
> > 
> > 
> > 


---

> ## AYUSH KHAIRE
> 
> [@liukeshuang](https://www.kaggle.com/liukeshuang) yes about 580 to 600 you are overfitting. 
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Then I probably don't need to continue training now
> > 
> > 
> > 


---



* --- discussion numver 114, the number of votes :1 ---

# Is it possible to use gemma-2-27B with vLLM?

**yechenzhi1** *Tue Jul 23 2024 00:34:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Inspired by [@cdeotte](https://www.kaggle.com/cdeotte)'s great [work](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/521294), I'm trying to use gemma-2-27B with vLLM. First I use GPTQmodel to quantinize it to 4-bit, then use vLLM-0.5.2 to do the infer. But we have to use flashinfer as the backend because of gemma-2's logits capping, then the problem comes, it's said flashinfer only supports GPU with compute [capability >= 8.0](https://github.com/vllm-project/vllm/issues/6173#issuecomment-2214759644), and T4 is 7.5. So is gemma-2-27b impossible for this competition?



---

 # Comments from other users

> ## Yixiao Yuan
> 
> I think we can't run gemma-2 with vLLM, but we can run it by huggingface. vLLM is much better for generation task due to PagedAttention. However, if we only generate one token or use classification head (we don't need KV cache in such cases), the performance should be similar.
> 
> 
> 
> > ## yechenzhi1Topic Author
> > 
> > bad news here, it seems only vllm/sglang can infer [quantized gemma-2-27b](https://github.com/ModelCloud/GPTQModel/issues/140#issuecomment-2242221690) correctly right now. 
> > 
> > 
> > 
> > ## yechenzhi1Topic Author
> > 
> > But thanks! I'll try it anyway!
> > 
> > 
> > 
> > > ## beanpotato
> > > 
> > > Could you share if you can run gema-2-27b with vLLM?ðŸ¥°
> > > 
> > > 
> > > 
> > > ## yechenzhi1Topic Author
> > > 
> > > No, T4 GPU doesn't support FlashInfer. I stopped trying gemma-2-27B after a few days.
> > > 
> > > 
> > > 


---

> ## ShelterW
> 
> Is it possible to use gemma-2-27B with vLLM now?
> 
> 
> 
> > ## Somesh88
> > 
> > I've been trying to use gemma 2 with vllm the weights over kaggle doesn't contain config file. If I try to load it form transformers then I have to keep internet access enabled which is not allowed in submission. have you found any workround for this? 
> > 
> > 
> > 
> > > ## Kishan Vavdara
> > > 
> > > you can create kaggle dataset of configs, packages, weights, etc. and add to your inference notebook, then you can use it without enabling internet. 
> > > 
> > > 
> > > 


---



* --- discussion numver 115, the number of votes :1 ---

# elo modelling?

**kawaii** *Fri Jul 26 2024 23:17:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

i believe we dont have info on which model reponses we are looking at during test times, did anyone try modelling a mapping between the responses and the model it came from using the training data we have. Basically guessing which model the response came from. Do you think such a model will be very noisy? 





* --- discussion numver 116, the number of votes :1 ---

# Why are cv and lb badly mismatched

**Rabbit** *Fri Jul 26 2024 13:51:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I got acv of0.927 and lb of 0.960 in local training

Training notebook:[https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

Inference notebook:[https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)





* --- discussion numver 117, the number of votes :1 ---

# Has anyone used Google cloud TPUs ?

**JamshaidSohail** *Thu Jul 18 2024 19:54:51 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi. Guys has anyone used Google Cloud TPUs ? If yes, can someone share some link for using them without any difficulty. 



---

 # Comments from other users

> ## The-Hai Nguyen
> 
> I could use the two links below to apply to my code without any serious issues:
> 
> - [https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training/](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training/)
> 
> - [https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/)
> 
> Hope it helps ðŸ¤—.
> 
> 
> 


---

> ## Sparsh Tewatia
> 
> Nothing happens without difficulty bro ðŸ˜­
> 
> 
> 


---



* --- discussion numver 118, the number of votes :1 ---

# Has anyone tried using an SVR (Support Vector Regression) to replace the original classification header?

**lllleeeo** *Sat Jul 20 2024 00:48:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Has anyone tried using an SVR (Support Vector Regression) to replace the original classification head? I've noticed that the classification heads currently used in the public notebooks are simple linear fully connected layers, or two layers of linear heads with an activation function and dropout in the middle as someone mentioned in the comments section, and I'm wondering if using an SVR to generate predictions would perform better with the amount of data in the competition. 

I'm about to make an attempt at this but still have some concerns because then the parameters of the model are trained independently of the classification header that is ultimately used, and may blend even worse, so I'm wondering if anyone has experimented with this? We can discuss this together!ðŸ˜€





* --- discussion numver 119, the number of votes :1 ---

# Allowing the notebook keep on running at the back on Vast.ai

**JamshaidSohail** *Wed Jul 17 2024 00:34:15 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi. I have successfully setup my instance on vast.ai for training Gemma 2. My notebook is running. I just want to make sure that it keeps on running even if close the browser. How would that be possible [@kishanvavdara](https://www.kaggle.com/kishanvavdara) [@valentinwerner](https://www.kaggle.com/valentinwerner).



---

 # Comments from other users

> ## Valentin Werner
> 
> How did I get to the point where I am directly tagged in open discussion questions? ðŸ˜‰
> 
> I have never used vast.ai, but what I did back when I had a university server with 4xV100 at my disposal:
> 
> - you should probably put your notebooks into a python script (train.py for example)
> 
> - you should start the scrip and move it to the background. I always used the linux command "screen" for this. Make sure to detach your screen before you close the browser and make sure the server keeps running (again, I never used vast.ai)..
> 
> ChatGPT should be able to give you more details, there are also plenty of documentations online
> 
> 
> 
> > ## JamshaidSohailTopic Author
> > 
> > Thank you so much. It works :D [@valentinwerner](https://www.kaggle.com/valentinwerner) 
> > 
> > 
> > 


---



* --- discussion numver 120, the number of votes :1 ---

# P100 and T4*2 GPUs inference results are different

**Femca7** *Tue Jul 16 2024 09:47:35 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Recently, I found that the inference results differ when using a single GPU compared to using two GPUs. Another issue is that the inference results on my local machine and on Kaggle also have slight differences.

Does anyone know the reason for this?

Using one GPU:  

id     winner_model_a  winner_model_b  winner_tie 

1233961     0.245430    0.517676    0.236894

Using two GPUs:  

id     winner_model_a  winner_model_b  winner_tie 

1233961     0.238452    0.535787    0.225761



---

 # Comments from other users

> ## Valentin Werner
> 
> I also noticed that the scores are different across transformer versions.
> 
> 
> 


---



* --- discussion numver 121, the number of votes :1 ---

# Accessing Llama 3 MODEL_NAME in inference notebook

**JamshaidSohail** *Mon Jul 15 2024 00:07:29 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi. I have successfully run the amazing [TPU Llama 3 training notebook](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train) by [@kishanvavdara](https://www.kaggle.com/kishanvavdara) and now I am trying to run the inference notebook. I have already been given access to the Llama 3 usage both on hugging face and meta official page and I have the corresponding hugging face token as well and weights file as well. When I try to run the [inference notebook](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b), i get the below error 

OSError: Incorrect path_or_model_id: '/kaggle/input/llama-3/transformers/8b-chat-hf/1'. Please provide either the path to a local folder or the repo_id of a model on the Hub.

Internet access is off in the inference notebook. So I cannot use MODEL_NAME="meta-llama/Meta-Llama-3-8B-Instruct" like I did in training notebook which downloads the model from scratch from hugging face hub. Any sort of help would be highly appreciated. [@valentinwerner](https://www.kaggle.com/valentinwerner) 



---

 # Comments from other users

> ## Valentin Werner
> 
> First, you also need to request Llama Access on Kaggle, you can do so by following the model link. Then, make sure you have the llama model added as model in the notebook, then the path will be exactly right.
> 
> 
> 
> > ## JamshaidSohailTopic Author
> > 
> > So I already got access to the Llama model via using the official Meta page and now I submitted the form via the Kaggle. I hope it gets approved quickly and I can move fast :D. Thank you so much once again.
> > 
> > 
> > 
> > > ## JamshaidSohailTopic Author
> > > 
> > > Thank you [@valentinwerner](https://www.kaggle.com/valentinwerner). I have been granted access and I am able to do the inference :D
> > > 
> > > 
> > > 
> > > ## Valentin Werner
> > > 
> > > Amazing, that you got it within the hour on a saturday! Some people reported waiting times of 24+ hours. Best of luck with your training - toi toi toi ðŸ˜‰
> > > 
> > > 
> > > 


---



* --- discussion numver 122, the number of votes :1 ---

# Does the order of the submission file matter?

**Lorry Zou** *Sat Jul 13 2024 04:05:31 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Title. Is it OK if I change the order of the original submission file?

For example, the original id's are: 01, 02, 03.

Is it OK if I submit in this order: 02|prediction, 01|prediction, 03|prediction ?



---

 # Comments from other users

> ## Valentin Werner
> 
> No it won't matter. I submitted submission.csv files with reversed IDs before.
> 
> 
> 


---



* --- discussion numver 123, the number of votes :1 ---

# There is more than 1 prompt in ~10% of the ['prompt'] rows

**Matthew Hendricks** *Fri Jul 12 2024 02:04:16 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)



```
import matplotlib.pyplot as plt
from datasets import load_dataset
import ast

# Load the dataset
dataset = load_dataset("lmsys/lmsys-arena-human-preference-55k", split='train')

# Function to safely evaluate the string as a list
def safe_eval(s):
    try:
        return ast.literal_eval(s)
    except:
        return []

# Count the number of items in each prompt
item_counts = [len(safe_eval(prompt)) for prompt in dataset['prompt']]

# Count the frequency of each number of items
count_freq = {}
for count in item_counts:
    count_freq[count] = count_freq.get(count, 0) + 1

# Prepare data for plotting
counts = list(count_freq.keys())
frequencies = list(count_freq.values())

# Create the bar plot
plt.figure(figsize=(10, 6))
plt.bar(counts, frequencies)
plt.xlabel('Number of Items in Prompt')
plt.ylabel('Frequency')
plt.title('Distribution of Number of Items in Prompts')
plt.xticks(range(min(counts), max(counts)+1))

# Add value labels on top of each bar
for i, v in enumerate(frequencies):
    plt.text(counts[i], v, str(v), ha='center', va='bottom')

plt.tight_layout()
plt.show()

# Print some statistics
total_prompts = len(item_counts)
avg_items = sum(item_counts) / total_prompts
print(f"Total number of prompts: {total_prompts}")
print(f"Average number of items per prompt: {avg_items:.2f}")
print(f"Most common number of items: {max(count_freq, key=count_freq.get)}")
print(f"Maximum number of items in a prompt: {max(counts)}")

```





* --- discussion numver 124, the number of votes :1 ---

# Explanation on the prediction that the model must perform

**GregReds** *Thu Jul 11 2024 05:15:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hello everyone!

I am a master's degree student and I am using this competition for an exam; for educational purposes, I must primarily rely on the use of word embedding. My question and my doubt is: to make predictions and to determine whether to award the win to model a or model b, should the model only check the structure of the responses, or also their correctness?

Let me give an example to be more explicit:

If the prompt were: What is the capital of France?

response_a: The capital of France is Paris.

response_b: The capital of France is Rome.

Typically, a model like word2vec, if not too well-trained, might base its evaluation on the structure rather than the correctness, focusing on the fact that the sentences are structured similarly and this predicting a tie. However, model A should win because the actual capital is indeed Paris, not Rome.

Therefore, it might be useful to utilize word embedding models like Bert that theoretically also check the correctness of the response.

I hope I have made myself clear! Thanks everyone.

[Screenshot 2024-07-10 alle 22.14.58.png](https://storage.googleapis.com/kaggle-forum-message-attachments/2916192/20917/Screenshot 2024-07-10 alle 22.14.58.png)



* --- discussion numver 125, the number of votes :1 ---

# this requirement <= 9 hours run-time is for public dataset or private ?

**Dlond Mike** *Mon Jul 08 2024 22:13:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

this requirement

<= 9 hours run-time

is for public dataset or private ?



---

 # Comments from other users

> ## Valentin Werner
> 
> On submission you are running the whole test set, private and public. Therefore, if your submission finishes in 9 hours, and you see a score on LB, you have nothin to worry about.
> 
> 
> 


---

> ## gmin5y
> 
> See this previous post for details: [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/517142](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/517142)
> 
> 
> 


---

> ## Lorry Zou
> 
> It's for the whole(private) test set.
> 
> 
> 


---



* --- discussion numver 126, the number of votes :1 ---

# Too long waiting time for TPU

**Saiyan Warrior** *Thu Jul 04 2024 19:11:11 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi Kaggler, 

Are you also facing too long waiting time/queue( sometimes more than 1 hour) for TPU usage? 
To whom should we raise this issue? 
What other options are you trying for TPU besides colab?


---

 # Comments from other users

> ## Valentin Werner
> 
> Not sure about many other options. With the rise of LLMs, these resources have been used a lot more. You can raise feature requests and complain about stuff like this in Product Feedback: [https://www.kaggle.com/discussions/product-feedback?sort=published](https://www.kaggle.com/discussions/product-feedback?sort=published)
> 
> However, Kaggle provides these resources for free so you dont have to provide them yourself and to even out the playing field for these competitions a bit. I think its important to either be grateful for what Kaggle provides (of course you can still raise it in product feedback), or adapt & overcome and find a solution that works for you. There are many alternatives for the goold old GPU - which still works well ðŸ˜‰
> 
> 
> 
> > ## Saiyan WarriorTopic Author
> > 
> > [@valentinwerner](https://www.kaggle.com/valentinwerner) 
> > 
> > I am not only grateful for computing but also for the kaggle platform itself I have learned a lot from this platform.
> > I just wanted to check whether this happening to me only or in general and what option others are using.
> > 
> > 
> > > ## Sparsh Tewatia
> > > 
> > > Try applying for TRC google program , they give I think 1 month of free access to TPUs
> > > 
> > > 
> > > 


---

> ## madarshbb
> 
> 1) Yes, it is an issue for quite some time now.
> 
> 2) You can raise this in kaggle's product feedback
> 
> 3) I tried GCS and azure sagemaker. But they have quite a cumbersome setup process. Would much rather wait for kaggle's TPU for a few hours, but less TPU waiting time would be a blessing.
> 
> 
> 
> > ## Saiyan WarriorTopic Author
> > 
> > Thanks [@madarshbb](https://www.kaggle.com/madarshbb) 
> > 
> > 
> > 


---



* --- discussion numver 127, the number of votes :1 ---

# Model licenses

**gromml** *Fri Jul 05 2024 06:27:37 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi all,

I am new to HF. I can see that different models come with different licenses: [https://huggingface.co/docs/hub/repositories-licenses](https://huggingface.co/docs/hub/repositories-licenses). How can I find out which licenses conform to the competition rules?

Thanks.



---

 # Comments from other users

> ## Valentin Werner
> 
> You need to have available commercial use.
> 
> The common licenses that work are including (but others probably too): MIT, Apache 2.0, CC-BY4.0, Llama License, Gemma License.
> 
> Always make sure that you further read the licenses if you plan to distribute information (e.g., for Llama and Gemma).
> 
> 
> 
> > ## grommlTopic Author
> > 
> > For example, if I want to use this model:
> > 
> > [https://huggingface.co/vennify/t5-base-grammar-correction](https://huggingface.co/vennify/t5-base-grammar-correction)
> > 
> > I can see that it has Creative Commons Attribution Non Commercial Share Alike 4.0. Does using this model conform to this competition?
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > This sounds weird but you can try it out locally or in kaggle. From my understanding (not a lawyer) you should not submit this to the competition (because then it becomes an attempt to win money). However, as long as you dont go for a gold medal, it probably hardly matters.
> > > 
> > > 
> > > 


---



* --- discussion numver 128, the number of votes :1 ---

# Can I use model which is not in kaggle ? 

**AlphaTT30** *Sat Jun 29 2024 08:42:50 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

In this competition, internet access is not allowed. So I can't use hugging face pre-trained transformer models like this one. The submission gets an error, I think for this one. This needs to be downloaded 

```
# Load model directly
from transformers import AutoTokenizer, AutoModelForMaskedLM

tokenizer = AutoTokenizer.from_pretrained("google-bert/bert-base-uncased")
model = AutoModelForMaskedLM.from_pretrained("google-bert/bert-base-uncased")

```

So what to do? 

Are all the hugging face models in kaggle? 

should I use a model that exists on Kaggle? 

or is there another way to use this one? 



---

 # Comments from other users

> ## tanaka
> 
> You can download these kind of Bert related things and llms before internet ristriction.
> 
> Major topic of these competition is
> 
> Training llm or nlp related model using some techniques and gpus (it is updated to you)
> 
> And then use these models as inference models without internet.
> 
> 
> 


---

> ## Yichuan Gao
> 
> If the model license permits, you can just download the model from huggingface and upload it to kaggle as a model, then add it to your notebook
> 
> 
> 
> > ## AlphaTT30Topic Author
> > 
> > Can I train a model outside kaggle and then upload here and use that model for this competition?  
> > 
> > 
> > 
> > > ## Ravi Ramakrishnan
> > > 
> > > Yes of course, this is a preferred way to handle these competitions [@alphatt30](https://www.kaggle.com/alphatt30) 
> > > 
> > > 
> > > 
> > ## Ivel afred
> > 
> > Does this mean that your model needs to be made public on Kaggle? Or is it okay to just make it public on Hugging Face.
> > 
> > 
> > 
> > > ## Yichuan Gao
> > > 
> > > You don't need to upload your model to huggingface, just upload it here on kaggle is ok. Also you can make it private (by default) and use it in your notebooks
> > > 
> > > 
> > > 
> > > ## Ivel afred
> > > 
> > > Code Requirements in LMSYS states: 'Freely&publicly available external data is allowed, including pre trained models.' Does this not require your models to be public? I'm a little confused
> > > 
> > > 
> > > 


---



* --- discussion numver 129, the number of votes :1 ---

# Traing steps cannot match the sample size!

**godmysalary** *Fri Jul 05 2024 12:49:34 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hello everyone! We were using the vscode to run the amazing fine-tune work by [https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook](url)

The only difference is that we change the model to Llama3. 

And when I run the code on Kaagle GPU, everything is fine. The sample size is 500, gradient_accumulate_steps is 16, batch_size=1, so we will undergo 31 steps, which is shown by the first picture below. 

However when I just COPY the same code to my vscode which is connected to a remote server, the total steps became 10! The sample size, gradient_accumulate_steps, batch_size stay unchanged but the total steps became 10, which is shown in the second picture and means only about 160 (16*10) samples are processed? Only one GPU is used on the vscode. 

Could anybody give me a hint about what is going on? The packages are updated. 



---

 # Comments from other users

> ## godmysalaryTopic Author
> 
> Hello everybody! We checked this probelm again and finally found the "killer"! Our remote server has 3 GPUs and so as you may suspect, the program is parallelled. The most annoying part is that when we loaded the model, we set device_map={'': 0} but somehow it still ran parallelly. So we explicitly set CUDA_VISIBLE_DEVICES = 0 at the beginning of our code and the problem faded! Hope this help you for your fine-tuning.
> 
> 
> 


---

> ## Valentin Werner
> 
> Did you check the length of your data? Did you accidentally swap train and validation dataset?
> 
> 
> 
> > ## godmysalaryTopic Author
> > 
> > yes! the length of train_dataset is 500 and the length of evaluation_dataset is 100.
> > 
> > 
> > 


---



* --- discussion numver 130, the number of votes :1 ---

# Getting error in sumbission.

**AlphaTT30** *Sun Jun 30 2024 21:56:50 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Notebook successfully rans but then getting error like this 

[no idea what's going no or why happening this ] 

What to do now ? 

You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the __call__ method is faster than using a method to encode the text followed by a call to the pad method to get a padded encoding.

89.2s    2   /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["remove_papermill_header.RemovePapermillHeader"] for containers is deprecated in traitlets 5.0. You can pass --Exporter.preprocessors item â€¦ multiple times to add items to a list.

89.2s    3     warn(

89.2s    4   [NbConvertApp] WARNING | Config option kernel_spec_manager_class not recognized by NbConvertApp.

89.3s    5   [NbConvertApp] Converting notebook notebook.ipynb to notebook

89.7s    6   [NbConvertApp] Writing 32587 bytes to notebook.ipynb

91.3s    7   /opt/conda/lib/python3.10/site-packages/traitlets/traitlets.py:2930: FutureWarning: --Exporter.preprocessors=["nbconvert.preprocessors.ExtractOutputPreprocessor"] for containers is deprecated in traitlets 5.0. You can pass --Exporter.preprocessors item â€¦ multiple times to add items to a list.

91.3s    8     warn(

91.3s    9   [NbConvertApp] WARNING | Config option kernel_spec_manager_class not recognized by NbConvertApp.

91.3s    10  [NbConvertApp] Converting notebook notebook.ipynb to html

92.2s    11  [NbConvertApp] Writing 319012 bytes to results.html



---

 # Comments from other users

> ## Anya
> 
> Same situation. Waiting for resolutionðŸ¤·â€â™‚ï¸
> 
> 
> 
> > ## Anya
> > 
> > [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/511861](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/511861)
> > 
> > There is a similar tag, you can check if it helps.
> > 
> > 
> > 
> > > ## AlphaTT30Topic Author
> > > 
> > > I solved my problem. Do you need a solution? 
> > > 
> > > 
> > > 


---



* --- discussion numver 131, the number of votes :1 ---

# Why is the scoring process so time-consuming

**David.Ricardo.H.X** *Thu May 30 2024 12:13:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)


I submit successfully the code, the scoring is still running.
The submitted notebook throws error, the scoring is still running. 

Does anybody have the same issue? 



---

 # Comments from other users

> ## Valentin Werner
> 
> Note that time difference between Submission and save come from the data difference. During saving (the success / error you mentioned) the test data only has 3 rows, during submission its 25,000 rows. A subset of these rows are used for Public Leaderboard (what we see on Leaderboard right now), while most IS used for private Leaderboard / the score we see once the competition finished, and which is used for actual evaluation in the competition placement.
> 
> So you are running A LOT more data during submission, increasing runtime for row based operations
> 
> 
> 
> > ## Nguyá»…n Anh TÃº
> > 
> > Does the data in file train.csv in submission environment different from the data in that file when we training model with our private notebook sir? 
> > 
> > 
> > 


---

> ## [Deleted User]
> 
> The scoring process can be time-consuming due to several factors:
> 
> Complexity of Notebook: Long-running computations or large datasets extend execution time.
> Resource Constraints: Limited computational resources and high submission volumes cause delays.
> Error Handling: Systems may attempt to run all cells despite errors to gather complete data.
> Automated Evaluation: Comprehensive testing and validation can take a significant amount of time.
> System Overhead: Infrastructure tasks such as container setup and data transfer add to the delay
> 
> 


---



* --- discussion numver 132, the number of votes :1 ---

# offline use of fine-tuned Llama3

**nahyat** *Thu Jun 20 2024 16:00:07 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Inspired by Kishan Vavdara's wonderful notebook, I fine-tuned the Llama3-8b model in colab. I got the basemodel from huggingface's Llama3-8b model instead of the kaggle dataset.ï¼ˆMODEL_NAME = 'meta-llama/Meta-Llama-3-8B'ï¼‰

When I tried to use the fine-tuned model in kaggle, I was faced with a situation where I could not load the weights of the model I created because huggingface could not be authenticated in the offline environment.

After loading the basemodel from the kaggle dataset, I also failed to load the weights of my model.

Is it because the models in huggingface and kaggle dataset are not compatible, which is why the loading fails?

Note: Llama3 license applications for the kaggle dataset are also allowed.

Thank you for watching



---

 # Comments from other users

> ## Kishan Vavdara
> 
> I'm glad you found my notebook helpful. Can you share the error you're facing ? and did you use LoRA for fine-tuning?
> 
> 
> 
> > ## nahyatTopic Author
> > 
> > Thank you for watching!
> > 
> > I wasn't getting any errors, but I was worried that the processing was too fast (compared to your model) when loading the weights of the fine-tuned model into the basemodel. â†“
> > 
> > model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
> > 
> > I noticed that the results of inference were surprisingly low, so I guessed that there was some kind of problem when loading the model parameters.
> > 
> > However, when I reviewed the code in a local environment, I confirmed that there was a discrepancy in the variable names in the training flow, which caused the correct labels required for backward to fail to be obtained. (I don't know why trainer.train() worked without an error.)
> > 
> > I'm sorry for taking the time to watch itâ€¦
> > 
> > 
> > 
> > > ## Kishan Vavdara
> > > 
> > > No worries, It's great to hear that you found the discrepancy and resolved the issue. Happy finetuning) 
> > > 
> > > 
> > > 


---

> ## Lorry Zou
> 
> How long did it take to fine-tune on Colab? I'm also trying to fine-tune on Colab but it'll take over 20 hours using A100, which is much longer than the max session length of Colab (12 hours). How did you make it shorter than 12 hours? Thank you so much.
> 
> 
> 


---



* --- discussion numver 133, the number of votes :1 ---

# Why do I get "Notebook Threw Exception", and with the same notebook, sometimes, I get a successful submission?

**Kilaru Vasudeva** *Sat Jun 15 2024 21:32:39 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I find it wierd. Is it any wrong code specification on myside or it is something on the kaggle backend?



---

 # Comments from other users

> ## OminousDude
> 
> Can I get some more info about this problem? It could be if you don't set a random seed something like a division by zero could occur.
> 
> 
> 


---



* --- discussion numver 134, the number of votes :1 ---

# More information on test dataset

**Matous Famera** *Thu Jun 13 2024 02:12:49 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hello, I have a few questions regarding the nature of test dataset.

I already know that 26% of the entire test dataset is used for public leaderboard and 74% is used for private leaderboard.

What is the difference between the train dataset and test dataset? Are the same LLMs used? Was the same dataset used for train dataset and test dataset?
How long is the test dataset? Or atleast how long compared to the train dataset is the test dataset?

Thanks if any of these questions can be answered



---

 # Comments from other users

> ## James Day
> 
> 
> What is the difference between the train dataset and test dataset? Are the same LLMs used? Was the same dataset used for train dataset and test dataset?
> 
> I've noticed there aren't any recently released models (e.g. Llama 3) in the training dataset, so I have a suspicion they split their data based on the date on which each comparison occurred and I would expect to receive messages from different LLMs during testing.
> 
> How long is the test dataset? Or atleast how long compared to the train dataset is the test dataset?
> 
> The data tab says "you can expect roughly 25,000 rows in the test set"
> 
> 
> 
> > ## Matous FameraTopic Author
> > 
> > 
> > There are 55K rows in the training data, and you can expect roughly 25,000 rows in the test set.
> > 
> > Does it mean that the entire test dataset has 25k rows or just the public leaderboard part?
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > The entire data has 25k rows, ~26% of them is public leaderboard (so about 6.5k rows).
> > > 
> > > As you probably guess, this makes trusting the public leaderboard score similar to trusting a single validation fold in a 4-fold cv setup. Thats why it is often recommended to build a good CV strategy and try to create a correlation between the CV score (which should be reliable) and the public LB score.
> > > 
> > > Also note, that the final score is ONLY the private LB, so the other 74% of the data. Meaning the fold, you may overfit on (the public LB) is NOT part of your winning score. This can lead to what we call "Leaderboard shakeup". These concepts apply to basically all kaggle competitions.
> > > 
> > > 
> > > 
> > > ## Matous FameraTopic Author
> > > 
> > > Thanks for clarification. I was asking that questions, because the length of the test dataset is related to variance and reliability of the score.
> > > 
> > > I'm aware of the concept of overfitting for public leaderboard.
> > > 
> > > 
> > > 


---



* --- discussion numver 135, the number of votes :1 ---

# training loss doesnt converge

**ivan_c2004** *Sat Jun 08 2024 22:15:28 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hello guys, i am using deberta xsmall with peft for training. the batch size is 8 and the learning rate is 1e-4, data for training is the range 40k to 56k , everytime i train the model the loss keeps at about 1.01, doesnt decrease after 6-7 epoches. my gpu is rtx 3060 only and training for ten or more epoches would take me more than a day and that i read from online websites lora finertuning llm should take only a few epoches so i didnt try for more epoches. Does anyone know how to solve this issue, or just try training for more epoches until the loss reaches a certain number like 0.2 0.3? thank you so much

My codes for training can be found below. Thank you 

```
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import torch
import sklearn
import numpy as np
import pandas as pd
import time
from tqdm import tqdm
from torch.nn import CrossEntropyLoss
from torch.utils.data import DataLoader, Dataset
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig,get_linear_schedule_with_warmup,AutoModelForSequenceClassification,DebertaTokenizerFast
from torch.cuda.amp import autocast

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
print(torch.__version__)
if torch.cuda.is_available(): print('gpu available')
from huggingface_hub import login
login(token="")

class CustomDataset(Dataset):
    def __init__(self, df, tokenizer):
        self.df = df
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.df)

    def __getitem__(self, idx):
        text = self.df.loc[idx, 'text']
        labels_a = self.df.loc[idx, 'winner_model_a']
        labels_b = self.df.loc[idx, 'winner_model_b']
        labels_tie = self.df.loc[idx, 'winner_tie']

        max_length = 1024

        inputs = self.tokenizer(
            text,
            max_length=max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        input_ids = inputs['input_ids'].squeeze()
        attention_mask = inputs['attention_mask'].squeeze()

        labels = torch.tensor([labels_a, labels_b, labels_tie]) 

        return {
            'input_ids': input_ids,
            'attention_mask': attention_mask,
            'labels': labels  
        }

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16
)
print(torch.cuda.get_device_name(0))

model_id = "microsoft/deberta-v3-xsmall"
tokenizer_id = "microsoft/deberta-v3-xsmall"

tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)
model = AutoModelForSequenceClassification.from_pretrained(
    model_id,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
model.config.pad_token_id = tokenizer.pad_token_id
tokenizer.pad_token = tokenizer.eos_token

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    task_type=TaskType.SEQ_CLS,
    )
device = torch.device('cuda:0')
baseline_model = get_peft_model(model, peft_config).to(device)
baseline_model.print_trainable_parameters()
baseline_model.eval()

lr = 1e-4
num_epochs = 16
batch_size = 8
kaggle = False

train_df = pd.read_csv('./lmsys-chatbot-arena/train.csv')
print('number of training data: ',len(train_df))
train_df = train_df.iloc[:]
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return ' '.join(sentences)

train_df.loc[:, 'prompt'] = train_df['prompt'].apply(process)
train_df.loc[:, 'response_a'] = train_df['response_a'].apply(process)
train_df.loc[:, 'response_b'] = train_df['response_b'].apply(process)
train_df['text'] = 'User prompt: ' + train_df['prompt'] + '\n\nModel A :\n' + train_df['response_a'] + '\n\n--------\n\nModel B:\n' + train_df['response_b']
train_df = train_df.reset_index(drop=True)

if kaggle:
    print(f'number of training data {len(train_df)}')
else:
    train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)
    train_df = train_df.reset_index(drop=True)
    val_df = val_df.reset_index(drop=True)
    print(f'number of training data after spliting: {len(train_df)} number of testing data: {len(val_df)}')
    val_dataset = CustomDataset(val_df, tokenizer)
    val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

train_dataset = CustomDataset(train_df, tokenizer)
train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)

import gc

gc.collect()

torch.cuda.empty_cache()

criterion = CrossEntropyLoss()

optimizer = torch.optim.AdamW(baseline_model.parameters(), lr=lr)

lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)

training_losses = []  
validation_losses = []  

for epoch in range(num_epochs):
    baseline_model.train()
    epoch_training_loss = 0  

    for step, batch in enumerate(tqdm(train_dataloader)):
        batch = {k: v.to(device) for k, v in batch.items()} 

        inputs = {
            'input_ids': batch['input_ids'],
            'attention_mask': batch['attention_mask'],
        }
        outputs = baseline_model(**inputs)
        labels = batch['labels'].float()
        loss = criterion(outputs.logits, labels)
        loss.backward()
        optimizer.zero_grad()

        epoch_training_loss += loss.item() 

    epoch_training_loss /= len(train_dataloader)  
    training_losses.append(epoch_training_loss) 
    baseline_model.eval()
    if not kaggle:
        total_validation_loss = 0
        total_samples = 0
        correct_predictions = 0

        with torch.no_grad():
            for step, batch in enumerate(tqdm(val_dataloader)):
                batch = {k: v.to(device) for k, v in batch.items()}  

                inputs = {
                    'input_ids': batch['input_ids'],
                    'attention_mask': batch['attention_mask'],
                }
                outputs = baseline_model(**inputs)
                predictions = outputs.logits.argmax(dim=-1)
                labels = batch['labels'].float().argmax(dim=1)

                loss = criterion(outputs.logits, labels)
                total_validation_loss += loss.item()
                total_samples += len(labels)
                correct_predictions += (predictions == labels).sum().item()

        epoch_validation_loss = total_validation_loss / len(val_dataloader)
        validation_losses.append(epoch_validation_loss)  

        accuracy = correct_predictions / total_samples
        print(f"Validation Loss: {epoch_validation_loss:.4f}, Accuracy: {accuracy:.4f}")
    print(f"Epoch {epoch+1}: Training Loss: {epoch_training_loss:.4f}")

plt.plot(training_losses, label='Training Loss')
plt.plot(validation_losses, label='Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

torch.save(baseline_model.state_dict(), 'model.pt')

```



---

 # Comments from other users

> ## yechenzhi1
> 
> larger batch size helped for me( I just increased gradient_accumulation_steps to 100)
> 
> 
> 
> > ## ivan_c2004Topic Author
> > 
> > yah gradient accumulation can help! thx. 
> > 
> > how long does the training take? what is the suggested epoch and batch size?
> > 
> > 
> > 
> > > ## yechenzhi1
> > > 
> > > Training time is up to your GPUs and model size. And I simply tried 1 epoch and 400 batch size( I didn't do much experiments, this is just my personal choice)
> > > 
> > > 
> > > 
> > > ## ivan_c2004Topic Author
> > > 
> > > I see. Thank you so much!
> > > 
> > > 
> > > 


---

> ## Valentin Werner
> 
> model might be too small. took me Base model, Higher effective batch size and lower lr with all data for some epochs to her ANY convergence
> 
> Using PEFT with XSmall model might not be preferrable either, training on kaggle GPU can easily handle full finetune of the small models.
> 
> Also you leaked your huggingface token, I recommend to remove that.
> 
> 
> 
> > ## ivan_c2004Topic Author
> > 
> > i see. 
> > 
> > thank you so much for reminding me the leak 
> > 
> > 
> > 


---



* --- discussion numver 136, the number of votes :1 ---

# How far can a boosting get you?

**Ivan Vybornov** *Sat Jun 08 2024 05:37:05 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I am curious what is a score upper bound on using solely a boosting?

The best performing public notebook that only utilizes a GBDTs so far is [https://www.kaggle.com/code/andreasbis/lmsys-chatbot-arena-tf-idf#Model-Training-%F0%9F%A7%A0](https://www.kaggle.com/code/andreasbis/lmsys-chatbot-arena-tf-idf#Model-Training-%F0%9F%A7%A0)

But the difference with llama3 inference is more than significant (1.011 vs 0.989) which makes me wonder if one should even bother trying to get below 0.98 with it.



---

 # Comments from other users

> ## Valentin Werner
> 
> I was wondering the same. Different than the notebook you mentioned above I went a pure Text characteristic based Feature approach (Length, Paragraph Count, â€¦) and got to around 1.036
> 
> This is so far away from Raja's current score that I think it is not worth to be the main model for the prediction
> 
> If you are not going for the win and instead want to learn about boosting and feature engineering, I would suggest you try to get below 1.0 yourself
> 
> 
> 


---



* --- discussion numver 137, the number of votes :1 ---

# How can i use llama3?

**zyw** *Tue May 28 2024 03:11:29 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I submitted the llama3 form without applying from the meta official website, and the model author refused me to use llama3. 

After I clicked "here" and filled out the form on the meta official website, I can now use llama2 normally, but cannot use llama3. I want to resubmit the form but I don't find the submit button. Can anyone help me solve this problem?



---

 # Comments from other users

> ## Valentin Werner
> 
> Not directly answering your problem, but giving you a workaround:
> 
> You can also go to huggingface, create an Account, apply for llama3 on the model Page, wait 12 hours, put an access token into your Notebook (dont share it!!) and then use it that way by specifying "token=â€¦" in all the from_pretrained calls (model, tokenizer).
> 
> this is rather recommended for local Training. You will not be able to turn off Internet access for that notebook.
> 
> 
> 
> > ## zywTopic Author
> > 
> > This is indeed a great idea, thank you for sharing!
> > 
> > 
> > 


---



* --- discussion numver 138, the number of votes :1 ---

# Opening move for this competition

**MountainGoriillas** *Fri May 24 2024 22:03:31 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

This competition doesn't provide a starter notebook created by the host. How did you all prepare for it?



---

 # Comments from other users

> ## G John Rao
> 
> There is one pinned notebook in the code section. Here's the link to the starter notebook [https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter](https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter)
> 
> 
> 


---



* --- discussion numver 139, the number of votes :1 ---

# Internet access condition

**Kamil Machalica** *Fri May 17 2024 21:07:02 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hi Kagglers!

If there is internet access restrictions can we even use pre-trained models to download them?

Thanks

Kamil



---

 # Comments from other users

> ## Valentin Werner
> 
> There are several things you can do, and what is often done:
> 
> 1) You can download the models, save them as kaggle dataset, load them from kaggle dataset instead (same goes for pip installs you might want to do)
> 
> 2) You can train models in one notebook with internet access, save the model checkpoint at the end and then create a separate notebook without internet access. Then you can simply add the training notebook as input for the inference notebook!
> 
> Hope this helps, welcome to kaggle and good luck!
> 
> 
> 
> > ## Kamil MachalicaTopic Author
> > 
> > Thank you, it explains a lot!
> > 
> > 
> > 


---

> ## Muhammad Tariq Pervez
> 
> [@machalx](https://www.kaggle.com/machalx), In Kaggle competitions, "disabling internet" means that the code you submit to Kaggle for scoring is executed in an environment that does not have access to the internet. Ensure your submission does not include any code that requires internet access, such as downloading data from external URLs or accessing online APIs.
> 
> Otherwise no issue. 
> 
> 
> 


---



* --- discussion numver 140, the number of votes :1 ---

# Is TPU allowed to be used in this competition 

**kheder yazgi** *Fri May 17 2024 09:27:55 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Considering the huge data; we have, is allowed to train models on TPU?



---

 # Comments from other users

> ## Muhammad Tariq Pervez
> 
> Generally yes. But please read rules carefully
> 
> 
> 


---

> ## Valentin Werner
> 
> yes, you have TPU quota available, so you could use it to train.
> 
> 
> 
> > ## kheder yazgiTopic Author
> > 
> > Thanks really 
> > 
> > 
> > 


---



* --- discussion numver 141, the number of votes :1 ---

# why are there only 3 test cases in the test.csv?

**Kaizhao Liang** *Mon May 13 2024 23:44:03 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Is this expected?



---

 # Comments from other users

> ## Ravi Ramakrishnan
> 
> This is expected- your test data in the data page is a sample to be used for code check- the actual one is hidden [@lkz919](https://www.kaggle.com/lkz919) 
> 
> 
> 
> > ## Kaizhao LiangTopic Author
> > 
> > Ah thank you, so basically when they are running the notebook they would replace it with the hidden csv file. One additional question about no internet access, does it mean we can't access the trained models we upload ourselves when they run it? A bit confused by this constraint, and where should we download the pretrained weights if we use anything from huggingface.
> > 
> > 
> > 
> > > ## Ravi Ramakrishnan
> > > 
> > > Pre trained models need to be imported into a Kaggle model/ dataset and then used in the submission kernel [@lkz919](https://www.kaggle.com/lkz919) 
> > > 
> > > You can refer my baseline work to know more 
> > > 
> > > 
> > > 


---



* --- discussion numver 142, the number of votes :1 ---

# Data Understanding: Some response is NULL

**Squirrel** *Fri May 10 2024 13:10:21 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

I find some response is Null. Seems that they are deleted for some reason.



---

 # Comments from other users

> ## Austin Paxton
> 
> Based on the following discussion, it appears that it can be caused by platform error and possibly erratic user behavior.
> 
> [https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/502303)
> 
> 
> 


---



* --- discussion numver 143, the number of votes :1 ---

# TF-IDF -> Boosted Trees [LB 1.038]

**Rich Olson** *Tue May 07 2024 10:44:19 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (1 votes)

Hey all -

Sharing my first (working) effort at this:

[https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees](https://www.kaggle.com/code/richolson/lmsys-tf-idf-boosted-trees)

Simple idea is to use TF-IDF for vectorizing the texts - then see if a gradient boosted tree framework (LightGBM) can figure it out.

The TF-IDF vectorizer is fitted on prompt, response_a and response_b.

Vectorization is done on response_a and response_b separately and then combined in an hstack - and then LightGBM is trained on the whole mess.

(Using prompt for training didn't seem to obviously improve performance).

Vectorization + training takes about 30 minutes on CPU.  I don't have a time estimate on inference - but it's fast on just CPU.

I had minimal luck when vectorizing with ngram_range=(3, 5).  Performance improved a bunch when I changed that to ngram_range=(1, 5).  This approach working may be a lot about simple word frequency.

Another version of the notebook uses XGBoost - which trains much slower (about 2.5 hours).  That one is still scoring as I type this (I suspect it will have an LB score about the same).  I tried speeding up XGBoost using GPU - but for some reason it wouldn't converge.

Since I'm able to train-on-submission - one interesting option might be to try fitting the vectorizer on the test data (and then using that to vectorize the training data)â€¦

Hope this is helpful to someone!

-Rich

Side note: I just noticed validation on LightGBM reported a log-loss score of 1.036.. - shockingly close to my LB of 1.038! I can't recall another time I've had that happenâ€¦



---

 # Comments from other users

> ## Rich OlsonTopic Author
> 
> The XGBoost version finished scoring - 1.039 on the LB.
> 
> Considering XGBoost took much, much longer to train - I'll stick with LightGBM for this notebook.
> 
> If you're curious to see the XGBoost code - just look a version 8 of this notebook.
> 
> 
> 


---



* --- discussion numver 144, the number of votes :0 ---

# 0.707 -> 0.663 in 3 hours

**justin1357** *Sun Aug 04 2024 08:04:40 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

don't want to repeat anything, do our best.



---

 # Comments from other users

> ## Enter your display name
> 
> He directly used the leaked data for a match, and then used his own model to predict the rest. It's unbelievable that something like this happened so close to the end of the competition; it almost ruined the whole competition.
> 
> 
> 
> > ## Valentin Werner
> > 
> > This is actually a reasonable explanation in terms of speed and loss. Let's see how it is being handled.
> > 
> > However, I would love it to be real - because breakthoughs do happen.
> > 
> > 
> > 


---

> ## JM
> 
> Hey [@artvolgin](https://www.kaggle.com/artvolgin) are you finding a data leak or something more clever? 
> 
> 
> 
> > ## SeshuRaju ðŸ§˜â€â™‚ï¸
> > 
> > we will know in 30 hours !! [@julianmukaj](https://www.kaggle.com/julianmukaj) 
> > 
> > 
> > 
> > > ## Cody_Null
> > > 
> > > No clever technique is creating this gap on the worlds best in less than a 5th the time.
> > > 
> > > 
> > > 


---

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> 0.663 <- 0.707 ~ its margin itself 0.044
> 
> 
> 


---



* --- discussion numver 145, the number of votes :0 ---

# About Data Augmentation

**KeShuang Liu** *Sun Aug 04 2024 03:34:53 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

The competition is coming to an end. As a novice, this is my first time playing Kaggle. I encountered many problems and learned a lot. I was constantly fine-tuning the model before, but I had never enhanced the data. Today, I have realized all my ideas about fine-tuning the model. I have started a new exploration on how to get a higher score. I have learned some knowledge about data enhancement, including back-translation method and EDA method, and I am ready to use EDA for the next attempt. At the same time, I wonder if you have other methods you are willing to share.

[https://www.cnblogs.com/IllidanStormrage/p/16347433.html](https://www.cnblogs.com/IllidanStormrage/p/16347433.html)



---

 # Comments from other users

> ## Garvin Bhati
> 
> At this point in the competition, I don't think adding any additional data is going to improve the score, training set is already enough, and training hours are quite long. Using external data didn't improve the score for anyone as far as I can remember from the discussions, but I'll be curious to see if it works. 
> 
> 
> 
> > ## Garvin Bhati
> > 
> > I was trying to find ways to sample the data properly so that we could do more trials with different parameters.
> > 
> > 
> > 
> > ## Xinyuan Qiao
> > 
> > extra data works, but not in a simple way, we will find out in 48 hours
> > 
> > 
> > 


---

> ## KeShuang LiuTopic Author
> 
> For synonym replacement, what are some good ways to find its synonyms?
> 
> 
> 


---



* --- discussion numver 146, the number of votes :0 ---

# submission scoreing error

**Xin** *Fri Aug 02 2024 04:32:13 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Has anyone meet this problem when using gemma?  I just changed the inputs.

Thanks!!!



---

 # Comments from other users

> ## Enter your display name
> 
> This could be caused by many reasons, but the most common one is likely that the number of rows in your output submission.csv file does not match test.csv after rerunning on all hidden test data.
> 
> 
> 
> > ## XinTopic Author
> > 
> > Thanks!
> > 
> > By using:
> > 
> > ```
> > !python /kaggle/usr/lib/lmsys_script/lmsys_script.py
> > import pandas as pd
> > lmsys_with_metadata = pd.read_csv("/kaggle/working/lmsys_with_metadata.csv")
> > !rm /kaggle/working/lmsys_with_metadata.csv
> > test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
> > if len(test_df) != len(lmsys_with_metadata):
> >     1/0
> > 
> > ```
> > 
> > I got Notebook Throw Exception which proves you must right.
> > 
> > Why lmsys_script.py (as a utility script) will output different length after submitting submission.csv.
> > 
> > Inside the lmsys_script.py:
> > 
> > df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
> > 
> > 
> > 
> > > ## XinTopic Author
> > > 
> > > lmsys_script.py (as a utility script) will output same length using train.csv and test.csv before submitting submission.csv.
> > > 
> > > 
> > > 
> > > ## XinTopic Author
> > > 
> > > Finally I abandoned using utility script way to release GPU then the problem solved.
> > > 
> > > 
> > > 


---



* --- discussion numver 147, the number of votes :0 ---

# Did you try to explode the conversation responses?

**Mattia Vanzetto** *Thu Aug 01 2024 04:42:15 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hello guys,

I saw that the ~86% of the training conversations is composed by just a single prompt + response, but 14% is not. I saw also that, at least in the public notebooks, the fine-tuned models usually have a maximum sequence lenght of 2000/2400 characters, and often the prompt assembled for the models are just prompt_list + response_a_list + response_b_list, which surely lead to cases where the response_b is completely truncated, or anyway to a loss of information.

Did you try to explode the responses, fine-tune a model and then aggregate the predictions on the single piece of the conversation?

The mean/median length of the single piece of conversation "prompt_i + response_a_i + response_b_i" is between 2000 and 2400 characters, which seems perfect for this expirement.

I would like to try myself, but I have no fine-tuning experience, no computing power, and no time ðŸ˜‚

For what it's worth, I tried with a simple xgboost, same features preparation, same optimization procedure, the exploding+aggregating approach got 1.03 on the leaderboard vs 1.04 of the standard approach.

Another expirement I would have liked to do is to build a binary classifier considering just prompt + response_X, with target the relative winner_model_X, basically duplicating the number of rows, without considering the "opponent's response", and then aggregate all back.

I am really looking forward to see the solutions after the competitions ends. 

Good luck for the last days of the competition ðŸ€



---

 # Comments from other users

> ## JM
> 
> I tried, it increase the inference time and did not see any improvement to public LB myself
> 
> 
> 


---

> ## Yi-Fu Chen
> 
> 
> Another expirement I would have liked to do is to build a binary classifier considering just prompt + response_X, with target the relative winner_model_X, basically duplicating the number of rows, without considering the "opponent's response", and then aggregate all back.
> 
> I have thought about a similar concept, but the intuition seems unreasonable because winning and losing are compared.
> 
> 
> 
> > ## Mattia VanzettoTopic Author
> > 
> > Do you mean loosing and tie? These two would be "compared" doing so.
> > 
> > 
> > 


---



* --- discussion numver 148, the number of votes :0 ---

# [Solved] trainer.train("checkpoint-1000") ignores the first samples?: Resuming training in Transformers library. 

**ano** *Thu Aug 01 2024 15:27:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

My question is if trainer.train("checkpoint-x") ignores the first samples?

For example, I first trained using 4000 samples as train_dataset and got checkpoint-200, 400, 600, 800 and the last checkpoint as checkpoint-1000. And then I resumed training using 4800 samples by trainer.train("checkpoint-1000") and I got only checkpoint-1000 and checkpoint-1200 (no directory corresponding to 200-800). Does that mean the first 4000 samples were skipped?

In order to resume training using new training datasets, do we need to add the first samples as "dummy"? 



---

 # Comments from other users

> ## yechenzhi1
> 
> Yes, resuming training involves bypassing the previously processed data. If you wish to modify your dataset, you can specify 'model_name' as 'checkpoint-1000' and subsequently fine-tune it using your 4800 samples.
> 
> However, if you do so, your model will see the 4000 samples twice, is this what you want?
> 
> 
> 
> > ## anoTopic Author
> > 
> > Thank you for clarifying!
> > 
> > In my case, I trained with dataset_a and retrained with dataset_b from the checkpoint, but it seemed like the first samples in dataset_b was ignored. That's why I experimented a bit and asked this question here. Your answer is very helpful. Thank you so much!
> > 
> > 
> > 


---



* --- discussion numver 149, the number of votes :0 ---

# Synthetic Closed source API

**Just A game on your lips** *Thu Aug 01 2024 09:34:24 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Can I using another API such as: Chatgpt, gemini4oâ€¦ to generate datasets for training model. 



---

 # Comments from other users

> ## Valentin Werner
> 
> That profile picture irritated me (Expected Nischay) ðŸ˜€
> 
> On your question: The rules are quite vague, as there is a section about sharing non-open source data with the whole competition. However, in past competitions there were winning solutions that used self-generated data based on closed source models where data was not shared to my knowledge. Those guys still got a very deserved first place. Therefore, I would assume it to be okay. 
> 
> However, for rule questions it is always best to ask kaggle staff directly. 
> 
> 
> 
> > ## Just A game on your lipsTopic Author
> > 
> > thanks [@valentinwerner](https://www.kaggle.com/valentinwerner).  
> > 
> > 
> > 


---



* --- discussion numver 150, the number of votes :0 ---

# Help! TPU install error(From today)

**Robert0921** *Wed Jul 31 2024 14:58:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hello! When I installed TPU by the following steps,

!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q

the following error occurred from today, What happened?

ERROR: Could not find a version that satisfies the requirement libtpu-nightly==0.1.dev20230825; extra == "tpu" (from torch-xla[tpu]) (from versions: 0.1.dev20210615+nightly, 0.1.dev20210709+nightly, 0.1.dev20210809+nightly, 0.1.dev20210916+nightly, 0.1.dev20210917+nightly, 0.1.dev20210920+nightly, 0.1.dev20210921+nightly, 0.1.dev20210922+nightly, 0.1.dev20210923+nightly, 0.1.dev20210924+nightly, 0.1.dev20210925+nightly, 0.1.dev20210926+nightly, 0.1.dev20210927+nightly, 0.1.dev20210928+nightly, 0.1.dev20210929+nightly, 0.1.dev20210930+nightly, 0.1.dev20211001+nightly, 0.1.dev20211002+nightly, 0.1.dev20211003+nightly, 0.1.dev20211004+nightly, 0.1.dev20211005+nightly, 0.1.dev20211006+nightly, 0.1.dev20211007+nightly, 0.1.dev20211008+nightly, 0.1.dev20211009+nightly, 0.1.dev20211010+nightly, 0.1.dev20211011+nightly, 0.1.dev20211012+nightly, 0.1.dev20211013+nightly, 0.1.dev20211014+nightly, 0.1.dev20211015+nightly, 0.1.dev20211016+nightly, 0.1.dev20211017+nightly, 0.1.dev20211018+nightly, 0.1.dev20211019+nightly, 0.1.dev20211020+nightly, 0.1.dev20211022+nightly, 0.1.dev20211023+nightly, 0.1.dev20211024+nightly, 0.1.dev20211025+nightly, 0.1.dev20211026+nightly, 0.1.dev20211027+nightly, 0.1.dev20211028+nightly, 0.1.dev20211029+nightly, 0.1.dev20211030+nightly, 0.1.dev20211031+nightly, 0.1.dev20211101+nightly, 0.1.dev20211102+nightly, 0.1.dev20211103+nightly, 0.1.dev20211104+nightly, 0.1.dev20211105+nightly, 0.1.dev20211106+nightly, 0.1.dev20211107+nightly, 0.1.dev20211108+nightly, 0.1.dev20211109+nightly, 0.1.dev20211110+nightly, 0.1.dev20211111+nightly, 0.1.dev20211112+nightly, 0.1.dev20211113+nightly, 0.1.dev20211114+nightly, 0.1.dev20211115+nightly, 0.1.dev20211117+nightly, 0.1.dev20211118+nightly, 0.1.dev20211119+nightly, 0.1.dev20211120+nightly, 0.1.dev20211121+nightly, 0.1.dev20211122+nightly, 0.1.dev20211123+nightly, 0.1.dev20211124+nightly, 0.1.dev20211125+nightly, 0.1.dev20211126+nightly, 0.1.dev20211127+nightly, 0.1.dev20211128+nightly, 0.1.dev20211129+nightly, 0.1.dev20211130+nightly, 0.1.dev20211201+nightly, 0.1.dev20211202+nightly, 0.1.dev20211203+nightly, 0.1.dev20211204+nightly, 0.1.dev20211205+nightly, 0.1.dev20211206+nightly, 0.1.dev20211207+nightly, 0.1.dev20211208+nightly, 0.1.dev20211209+nightly, 0.1.dev20211217+nightly, 0.1.dev20211229+nightly, 0.1.dev20211230+nightly, 0.1.dev20211231+nightly, 0.1.dev20220101+nightly, 0.1.dev20220102+nightly, 0.1.dev20220103+nightly, 0.1.dev20220104+nightly, 0.1.dev20220105+nightly, 0.1.dev20220106+nightly, 0.1.dev20220107+nightly, 0.1.dev20220108+nightly, 0.1.dev20220109+nightly, 0.1.dev20220110+nightly, 0.1.dev20220111+nightly, 0.1.dev20220114+nightly, 0.1.dev20220115+nightly, 0.1.dev20220116+nightly, 0.1.dev20220117+nightly, 0.1.dev20220118+nightly, 0.1.dev20220119+nightly, 0.1.dev20220126+nightly, 0.1.dev20220127+nightly, 0.1.dev20220128+nightly, 0.1.dev20220202+nightly, 0.1.dev20220203+nightly, 0.1.dev20220204+nightly, 0.1.dev20220205+nightly, 0.1.dev20220206+nightly, 0.1.dev20220207+nightly, 0.1.dev20220208+nightly, 0.1.dev20220209+nightly, 0.1.dev20220210+nightly, 0.1.dev20220211+nightly, 0.1.dev20220212+nightly, 0.1.dev20220213+nightly, 0.1.dev20220214+nightly, 0.1.dev20220215+nightly, 0.1.dev20220216+nightly, 0.1.dev20220217+nightly, 0.1.dev20220218+nightly, 0.1.dev20220219+nightly, 0.1.dev20220220+nightly, 0.1.dev20220221+nightly, 0.1.dev20220222+nightly, 0.1.dev20220223+nightly, 0.1.dev20220224+nightly, 0.1.dev20220225+nightly, 0.1.dev20220226+nightly, 0.1.dev20220227+nightly, 0.1.dev20220228+nightly, 0.1.dev20220301+nightly, 0.1.dev20220303+nightly, 0.1.dev20220304+nightly, 0.1.dev20220305+nightly, 0.1.dev20220306+nightly, 0.1.dev20220307+nightly, 0.1.dev20220308+nightly, 0.1.dev20220309+nightly, 0.1.dev20220310+nightly, 0.1.dev20220311+nightly, 0.1.dev20220312+nightly, 0.1.dev20220313+nightly, 0.1.dev20220314+nightly, 0.1.dev20220315+nightly, 0.1.dev20220316+nightly, 0.1.dev20220317+nightly, 0.1.dev20220318+nightly, 0.1.dev20220319+nightly, 0.1.dev20220320+nightly, 0.1.dev20220321+nightly, 0.1.dev20220322+nightly, 0.1.dev20220323+nightly, 0.1.dev20220324+nightly, 0.1.dev20220325+nightly, 0.1.dev20220326+nightly, 0.1.dev20220327+nightly, 0.1.dev20220328+nightly, 0.1.dev20220329+nightly, 0.1.dev20220407+nightly, 0.1.dev20220408+nightly, 0.1.dev20220409+nightly, 0.1.dev20220410+nightly, 0.1.dev20220411+nightly, 0.1.dev20220412+nightly, 0.1.dev20220413+nightly, 0.1.dev20220414+nightly, 0.1.dev20220415+nightly, 0.1.dev20220416+nightly, 0.1.dev20220417+nightly, 0.1.dev20220418+nightly, 0.1.dev20220419+nightly, 0.1.dev20220420+nightly, 0.1.dev20220503+nightly, 0.1.dev20220504+nightly, 0.1.dev20220505+nightly, 0.1.dev20220506+nightly, 0.1.dev20220507+nightly, 0.1.dev20220509+nightly, 0.1.dev20220510+nightly, 0.1.dev20220511+nightly, 0.1.dev20220515+nightly, 0.1.dev20220516+nightly, 0.1.dev20220517+nightly, 0.1.dev20220518+nightly, 0.1.dev20220520+nightly, 0.1.dev20220521+nightly, 0.1.dev20220522+nightly, 0.1.dev20220524+nightly, 0.1.dev20220526+nightly, 0.1.dev20220527+nightly, 0.1.dev20220528+nightly, 0.1.dev20220529+nightly, 0.1.dev20220530+nightly, 0.1.dev20220531+nightly, 0.1.dev20220601+nightly, 0.1.dev20220602+nightly, 0.1.dev20220603+nightly, 0.1.dev20220604+nightly, 0.1.dev20220605+nightly, 0.1.dev20220606+nightly, 0.1.dev20220607+nightly, 0.1.dev20220608+nightly, 0.1.dev20220609+nightly, 0.1.dev20220610+nightly, 0.1.dev20220611+nightly, 0.1.dev20220612+nightly, 0.1.dev20220613+nightly, 0.1.dev20220614+nightly, 0.1.dev20220615+nightly, 0.1.dev20220616+nightly, 0.1.dev20220617+nightly, 0.1.dev20220618+nightly, 0.1.dev20220619+nightly, 0.1.dev20220620+nightly, 0.1.dev20220621+nightly, 0.1.dev20220622+nightly, 0.1.dev20220623+nightly, 0.1.dev20220624+nightly, 0.1.dev20220625+nightly, 0.1.dev20220626+nightly, 0.1.dev20220627+nightly, 0.1.dev20220628+nightly, 0.1.dev20220629+nightly, 0.1.dev20220630+nightly, 0.1.dev20220701+nightly, 0.1.dev20220702+nightly, 0.1.dev20220703+nightly, 0.1.dev20220704+nightly, 0.1.dev20220705+nightly, 0.1.dev20220706+nightly, 0.1.dev20220707+nightly, 0.1.dev20220708+nightly, 0.1.dev20220709+nightly, 0.1.dev20220710+nightly, 0.1.dev20220711+nightly, 0.1.dev20220713+nightly, 0.1.dev20220714+nightly, 0.1.dev20220715+nightly, 0.1.dev20220716+nightly, 0.1.dev20220717+nightly, 0.1.dev20220718+nightly, 0.1.dev20220719+nightly, 0.1.dev20220720+nightly, 0.1.dev20220721+nightly, 0.1.dev20220722+nightly, 0.1.dev20220723+nightly, 0.1.dev20220724+nightly, 0.1.dev20220725+nightly, 0.1.dev20220726+nightly, 0.1.dev20220727+nightly, 0.1.dev20220728+nightly, 0.1.dev20220729+nightly, 0.1.dev20220730+nightly, 0.1.dev20220731+nightly, 0.1.dev20220801+nightly, 0.1.dev20220802+nightly, 0.1.dev20220803+nightly, 0.1.dev20220804+nightly, 0.1.dev20220806+nightly, 0.1.dev20220807+nightly, 0.1.dev20220808+nightly, 0.1.dev20220809+nightly, 0.1.dev20220810+nightly, 0.1.dev20220812+nightly, 0.1.dev20220813+nightly, 0.1.dev20220814+nightly, 0.1.dev20220815+nightly, 0.1.dev20220816+nightly, 0.1.dev20220824+nightly, 0.1.dev20220908+nightly, 0.1.dev20220909+nightly, 0.1.dev20220910+nightly, 0.1.dev20220912+nightly, 0.1.dev20220913+nightly, 0.1.dev20220914+nightly, 0.1.dev20220915+nightly, 0.1.dev20220916+nightly, 0.1.dev20220917+nightly, 0.1.dev20220918+nightly, 0.1.dev20220919+nightly, 0.1.dev20220920+nightly, 0.1.dev20220921+nightly, 0.1.dev20220922+nightly, 0.1.dev20220923+nightly, 0.1.dev20220924+nightly, 0.1.dev20220925+nightly, 0.1.dev20220926+nightly, 0.1.dev20220927+nightly, 0.1.dev20220928+nightly, 0.1.dev20220929+nightly, 0.1.dev20220930+nightly, 0.1.dev20221001+nightly, 0.1.dev20221002+nightly, 0.1.dev20221003+nightly, 0.1.dev20221004+nightly, 0.1.dev20221005+nightly, 0.1.dev20221006+nightly, 0.1.dev20221007+nightly, 0.1.dev20221008+nightly, 0.1.dev20221009+nightly, 0.1.dev20221010+nightly, 0.1.dev20221011+nightly, 0.1.dev20221012+nightly, 0.1.dev20221013+nightly, 0.1.dev20221014+nightly, 0.1.dev20221015+nightly, 0.1.dev20221016+nightly, 0.1.dev20221017+nightly, 0.1.dev20221018+nightly, 0.1.dev20221019+nightly, 0.1.dev20221020+nightly, 0.1.dev20221021+nightly, 0.1.dev20221022+nightly, 0.1.dev20221023+nightly, 0.1.dev20221024+nightly, 0.1.dev20221025+nightly, 0.1.dev20221026+nightly, 0.1.dev20221027+nightly, 0.1.dev20221028+nightly, 0.1.dev20221029+nightly, 0.1.dev20221030+nightly, 0.1.dev20221031+nightly, 0.1.dev20221101+nightly, 0.1.dev20221102+nightly, 0.1.dev20221103+nightly, 0.1.dev20221104+nightly, 0.1.dev20221105+nightly, 0.1.dev20221106+nightly, 0.1.dev20221107+nightly, 0.1.dev20221108+nightly, 0.1.dev20221109+nightly, 0.1.dev20221115+nightly, 0.1.dev20221117+nightly, 0.1.dev20221118+nightly, 0.1.dev20221119+nightly, 0.1.dev20221120+nightly, 0.1.dev20221121+nightly, 0.1.dev20221122+nightly, 0.1.dev20221123+nightly, 0.1.dev20221124+nightly, 0.1.dev20221125+nightly, 0.1.dev20221126+nightly, 0.1.dev20221127+nightly, 0.1.dev20221128+nightly, 0.1.dev20221129+nightly, 0.1.dev20221130+nightly, 0.1.dev20221201+nightly, 0.1.dev20221202+nightly, 0.1.dev20221203+nightly, 0.1.dev20221204+nightly, 0.1.dev20221205+nightly, 0.1.dev20221206+nightly, 0.1.dev20221207+nightly, 0.1.dev20221208+nightly, 0.1.dev20221209+nightly, 0.1.dev20221210+nightly, 0.1.dev20221211+nightly, 0.1.dev20221212+nightly, 0.1.dev20221213+nightly, 0.1.dev20221214+nightly, 0.1.dev20221216+nightly, 0.1.dev20221217+nightly, 0.1.dev20221218+nightly, 0.1.dev20221219+nightly, 0.1.dev20221220+nightly, 0.1.dev20221221+nightly, 0.1.dev20221222+nightly, 0.1.dev20221223+nightly, 0.1.dev20221224+nightly, 0.1.dev20221225+nightly, 0.1.dev20221226+nightly, 0.1.dev20221227+nightly, 0.1.dev20221228+nightly, 0.1.dev20221229+nightly, 0.1.dev20221230+nightly, 0.1.dev20221231+nightly, 0.1.dev20230101+nightly, 0.1.dev20230102+nightly, 0.1.dev20230103+nightly, 0.1.dev20230104+nightly, 0.1.dev20230105+nightly, 0.1.dev20230106+nightly, 0.1.dev20230107+nightly, 0.1.dev20230108+nightly, 0.1.dev20230109+nightly, 0.1.dev20230110+nightly, 0.1.dev20230111+nightly, 0.1.dev20230112+nightly, 0.1.dev20230113+nightly, 0.1.dev20230114+nightly, 0.1.dev20230115+nightly, 0.1.dev20230116+nightly, 0.1.dev20230117+nightly, 0.1.dev20230118+nightly, 0.1.dev20230119+nightly, 0.1.dev20230120+nightly, 0.1.dev20230121+nightly, 0.1.dev20230122+nightly, 0.1.dev20230123+nightly, 0.1.dev20230124+nightly, 0.1.dev20230125+nightly, 0.1.dev20230126+nightly, 0.1.dev20230127+nightly, 0.1.dev20230128+nightly, 0.1.dev20230129+nightly, 0.1.dev20230130+nightly, 0.1.dev20230131+nightly, 0.1.dev20230201+nightly, 0.1.dev20230202+nightly, 0.1.dev20230203+nightly, 0.1.dev20230204+nightly, 0.1.dev20230205+nightly, 0.1.dev20230206+nightly, 0.1.dev20230207+nightly, 0.1.dev20230208+nightly, 0.1.dev20230209+nightly, 0.1.dev20230210+nightly, 0.1.dev20230211+nightly, 0.1.dev20230212+nightly, 0.1.dev20230213+nightly, 0.1.dev20230214+nightly, 0.1.dev20230215+nightly, 0.1.dev20230216+nightly, 0.1.dev20230217+nightly, 0.1.dev20230218+nightly, 0.1.dev20230219+nightly, 0.1.dev20230220+nightly, 0.1.dev20230221+nightly, 0.1.dev20230222+nightly, 0.1.dev20230223+nightly, 0.1.dev20230224+nightly, 0.1.dev20230225+nightly, 0.1.dev20230226+nightly, 0.1.dev20230228+nightly, 0.1.dev20230301+nightly, 0.1.dev20230302+nightly, 0.1.dev20230303+nightly, 0.1.dev20230304+nightly, 0.1.dev20230305+nightly, 0.1.dev20230306+nightly, 0.1.dev20230307+nightly, 0.1.dev20230308+nightly, 0.1.dev20230309+nightly, 0.1.dev20230310+nightly, 0.1.dev20230311+nightly, 0.1.dev20230312+nightly, 0.1.dev20230313+nightly, 0.1.dev20230314+nightly, 0.1.dev20230315+nightly, 0.1.dev20230316+nightly, 0.1.dev20230317+nightly, 0.1.dev20230318+nightly, 0.1.dev20230319+nightly, 0.1.dev20230320+nightly, 0.1.dev20230321+nightly, 0.1.dev20230322+nightly, 0.1.dev20230323+nightly, 0.1.dev20230324+nightly, 0.1.dev20230325+nightly, 0.1.dev20230326+nightly, 0.1.dev20230327+nightly, 0.1.dev20230328+nightly, 0.1.dev20230329+nightly, 0.1.dev20230330+nightly, 0.1.dev20230331+nightly, 0.1.dev20230401+nightly, 0.1.dev20230402+nightly, 0.1.dev20230403+nightly, 0.1.dev20230404+nightly, 0.1.dev20230405+nightly, 0.1.dev20230406+nightly, 0.1.dev20230407+nightly, 0.1.dev20230408+nightly, 0.1.dev20230409+nightly, 0.1.dev20230410+nightly, 0.1.dev20230411+nightly, 0.1.dev20230412+nightly, 0.1.dev20230413+nightly, 0.1.dev20230414+nightly, 0.1.dev20230415+nightly, 0.1.dev20230416+nightly, 0.1.dev20230417+nightly, 0.1.dev20230418+nightly, 0.1.dev20230419+nightly, 0.1.dev20230420+nightly, 0.1.dev20230421+nightly, 0.1.dev20230422+nightly, 0.1.dev20230423+nightly, 0.1.dev20230424+nightly, 0.1.dev20230425+nightly, 0.1.dev20230426+nightly, 0.1.dev20230427+nightly, 0.1.dev20230428+nightly, 0.1.dev20230429+nightly, 0.1.dev20230430+nightly, 0.1.dev20230501+nightly, 0.1.dev20230502+nightly, 0.1.dev20230503+nightly, 0.1.dev20230504+nightly, 0.1.dev20230505+nightly, 0.1.dev20230506+nightly, 0.1.dev20230507+nightly, 0.1.dev20230508+nightly, 0.1.dev20230509+nightly, 0.1.dev20230510+nightly, 0.1.dev20230511+nightly, 0.1.dev20230512+nightly, 0.1.dev20230513+nightly, 0.1.dev20230514+nightly, 0.1.dev20230515+nightly, 0.1.dev20230516+nightly, 0.1.dev20230517+nightly, 0.1.dev20230518+nightly, 0.1.dev20230519+nightly, 0.1.dev20230520+nightly, 0.1.dev20230521+nightly, 0.1.dev20230522+nightly, 0.1.dev20230523+nightly, 0.1.dev20230524+nightly, 0.1.dev20230525+nightly, 0.1.dev20230526+nightly, 0.1.dev20230527+nightly, 0.1.dev20230528+nightly, 0.1.dev20230529+nightly, 0.1.dev20230530+nightly, 0.1.dev20230531+nightly, 0.1.dev20230601+nightly, 0.1.dev20230602+nightly, 0.1.dev20230603+nightly, 0.1.dev20230604+nightly, 0.1.dev20230605+nightly, 0.1.dev20230606+nightly, 0.1.dev20230607+nightly, 0.1.dev20230608+nightly, 0.1.dev20230609+nightly, 0.1.dev20230610+nightly, 0.1.dev20230611+nightly, 0.1.dev20230612+nightly, 0.1.dev20230613+nightly, 0.1.dev20230614+nightly, 0.1.dev20230615+nightly, 0.1.dev20230616+nightly, 0.1.dev20230617+nightly, 0.1.dev20230618+nightly, 0.1.dev20230619+nightly, 0.1.dev20230620+nightly, 0.1.dev20230621+nightly, 0.1.dev20230622+nightly, 0.1.dev20230623+nightly, 0.1.dev20230624+nightly, 0.1.dev20230625+nightly, 0.1.dev20230626+nightly, 0.1.dev20230627+nightly, 0.1.dev20230628+nightly, 0.1.dev20230629+nightly, 0.1.dev20230630+nightly, 0.1.dev20230701+nightly, 0.1.dev20230702+nightly, 0.1.dev20230703+nightly, 0.1.dev20230704+nightly, 0.1.dev20230705+nightly, 0.1.dev20230706+nightly, 0.1.dev20230707+nightly, 0.1.dev20230708+nightly, 0.1.dev20230709+nightly, 0.1.dev20230710+nightly, 0.1.dev20230711+nightly, 0.1.dev20230712+nightly, 0.1.dev20230715+nightly, 0.1.dev20230716+nightly, 0.1.dev20230717+nightly, 0.1.dev20230718+nightly, 0.1.dev20230719+nightly, 0.1.dev20230720+nightly, 0.1.dev20230721+nightly, 0.1.dev20230722+nightly, 0.1.dev20230723+nightly, 0.1.dev20230724+nightly, 0.1.dev20230725+nightly, 0.1.dev20230726+nightly, 0.1.dev20230727+nightly, 0.1.dev20230802+nightly, 0.1.dev20230803+nightly, 0.1.dev20230804+nightly, 0.1.dev20230805+nightly, 0.1.dev20230806+nightly, 0.1.dev20230807+nightly, 0.1.dev20230808+nightly, 0.1.dev20230809+nightly, 0.1.dev20230810+nightly, 0.1.dev20230815+nightly, 0.1.dev20230816+nightly, 0.1.dev20230817+nightly, 0.1.dev20230818+nightly, 0.1.dev20230819+nightly, 0.1.dev20230820+nightly, 0.1.dev20230821+nightly, 0.1.dev20230823+nightly, 0.1.dev20230824+nightly, 0.1.dev20230825+nightly, 0.1.dev20230826+nightly, 0.1.dev20230827+nightly, 0.1.dev20230828+nightly, 0.1.dev20230829+nightly, 0.1.dev20230830+nightly, 0.1.dev20230831+nightly, 0.1.dev20230911+nightly, 0.1.dev20230912+nightly, 0.1.dev20230913+nightly, 0.1.dev20230914+nightly, 0.1.dev20230915+nightly, 0.1.dev20230916+nightly, 0.1.dev20230917+nightly, 0.1.dev20230918+nightly, 0.1.dev20230919+nightly, 0.1.dev20230920+nightly, 0.1.dev20230921+nightly, 0.1.dev20230922+nightly, 0.1.dev20230923+nightly, 0.1.dev20230924+nightly, 0.1.dev20230925+nightly, 0.1.dev20230926+nightly, 0.1.dev20230927+nightly, 0.1.dev20230928+nightly, 0.1.dev20230929+nightly, 0.1.dev20230930+nightly, 0.1.dev20231001+nightly, 0.1.dev20231002+nightly, 0.1.dev20231003+nightly, 0.1.dev20231004+nightly, 0.1.dev20231005+nightly, 0.1.dev20231006+nightly, 0.1.dev20231007+nightly, 0.1.dev20231008+nightly, 0.1.dev20231009+nightly, 0.1.dev20231010+nightly, 0.1.dev20231011+nightly, 0.1.dev20231012+nightly, 0.1.dev20231013+nightly, 0.1.dev20231014+nightly, 0.1.dev20231015+nightly, 0.1.dev20231016+nightly, 0.1.dev20231017+nightly, 0.1.dev20231018+nightly, 0.1.dev20231019+nightly, 0.1.dev20231020+nightly, 0.1.dev20231021+nightly, 0.1.dev20231022+nightly, 0.1.dev20231023+nightly, 0.1.dev20231024+nightly, 0.1.dev20231025+nightly, 0.1.dev20231026+nightly, 0.1.dev20231027+nightly, 0.1.dev20231028+nightly, 0.1.dev20231029+nightly, 0.1.dev20231030+nightly, 0.1.dev20231031+nightly, 0.1.dev20231101+nightly, 0.1.dev20231102+nightly, 0.1.dev20231103+nightly, 0.1.dev20231104+nightly, 0.1.dev20231105+nightly, 0.1.dev20231106+nightly, 0.1.dev20231107+nightly, 0.1.dev20231114+nightly, 0.1.dev20231115+nightly, 0.1.dev20231116+nightly, 0.1.dev20231117+nightly, 0.1.dev20231118+nightly, 0.1.dev20231119+nightly, 0.1.dev20231120+nightly, 0.1.dev20231121+nightly, 0.1.dev20231122+nightly, 0.1.dev20231123+nightly, 0.1.dev20231124+nightly, 0.1.dev20231125+nightly, 0.1.dev20231126+nightly, 0.1.dev20231127+nightly, 0.1.dev20231128+nightly, 0.1.dev20231129+nightly, 0.1.dev20231130+nightly, 0.1.dev20231201+nightly, 0.1.dev20231202+nightly, 0.1.dev20231203+nightly, 0.1.dev20231204+nightly, 0.1.dev20231205+nightly, 0.1.dev20231206+nightly, 0.1.dev20231207+nightly, 0.1.dev20231208+nightly, 0.1.dev20231209+nightly, 0.1.dev20231210+nightly, 0.1.dev20231211+nightly, 0.1.dev20231212+nightly, 0.1.dev20231213+nightly, 0.1.dev20231214+nightly, 0.1.dev20231215+nightly, 0.1.dev20231216+nightly, 0.1.dev20231217+nightly, 0.1.dev20231218+nightly, 0.1.dev20231219+nightly, 0.1.dev20231220+nightly, 0.1.dev20231221+nightly, 0.1.dev20231222+nightly, 0.1.dev20231223+nightly, 0.1.dev20231224+nightly, 0.1.dev20231225+nightly, 0.1.dev20231226+nightly, 0.1.dev20231227+nightly, 0.1.dev20231228+nightly, 0.1.dev20231229+nightly, 0.1.dev20231230+nightly, 0.1.dev20231231+nightly, 0.1.dev20240101+nightly, 0.1.dev20240102+nightly, 0.1.dev20240103+nightly, 0.1.dev20240104+nightly, 0.1.dev20240105+nightly, 0.1.dev20240106+nightly, 0.1.dev20240107+nightly, 0.1.dev20240108+nightly, 0.1.dev20240109+nightly, 0.1.dev20240110+nightly, 0.1.dev20240111+nightly, 0.1.dev20240113+nightly, 0.1.dev20240114+nightly, 0.1.dev20240115+nightly, 0.1.dev20240116+nightly, 0.1.dev20240117+nightly, 0.1.dev20240118+nightly, 0.1.dev20240119+nightly, 0.1.dev20240120+nightly, 0.1.dev20240121+nightly, 0.1.dev20240122+nightly, 0.1.dev20240123+nightly, 0.1.dev20240124+nightly, 0.1.dev20240127+nightly, 0.1.dev20240128+nightly, 0.1.dev20240129+nightly, 0.1.dev20240130+nightly, 0.1.dev20240131+nightly, 0.1.dev20240201+nightly, 0.1.dev20240202+nightly, 0.1.dev20240203+nightly, 0.1.dev20240204+nightly, 0.1.dev20240205+nightly, 0.1.dev20240206+nightly, 0.1.dev20240207+nightly, 0.1.dev20240208+nightly, 0.1.dev20240209+nightly, 0.1.dev20240210+nightly, 0.1.dev20240213+nightly, 0.1.dev20240214+nightly, 0.1.dev20240215+nightly, 0.1.dev20240216+nightly, 0.1.dev20240217+nightly, 0.1.dev20240218+nightly, 0.1.dev20240219+nightly, 0.1.dev20240220+nightly, 0.1.dev20240221+nightly, 0.1.dev20240222+nightly, 0.1.dev20240223+nightly, 0.1.dev20240224+nightly, 0.1.dev20240225+nightly, 0.1.dev20240226+nightly, 0.1.dev20240227+nightly, 0.1.dev20240228+nightly, 0.1.dev20240229+nightly, 0.1.dev20240301+nightly, 0.1.dev20240302+nightly, 0.1.dev20240303+nightly, 0.1.dev20240304+nightly, 0.1.dev20240305+nightly, 0.1.dev20240306+nightly, 0.1.dev20240307+nightly, 0.1.dev20240308+nightly, 0.1.dev20240309+nightly, 0.1.dev20240310+nightly, 0.1.dev20240311+nightly, 0.1.dev20240312+nightly, 0.1.dev20240313+nightly, 0.1.dev20240314+nightly, 0.1.dev20240315+nightly, 0.1.dev20240316+nightly, 0.1.dev20240317+nightly, 0.1.dev20240318+nightly, 0.1.dev20240319+nightly, 0.1.dev20240320+nightly, 0.1.dev20240321+nightly, 0.1.dev20240322+nightly, 0.1.dev20240323+nightly, 0.1.dev20240324+nightly, 0.1.dev20240325+nightly, 0.1.dev20240326+nightly, 0.1.dev20240327+nightly, 0.1.dev20240328+nightly, 0.1.dev20240329+nightly, 0.1.dev20240330+nightly, 0.1.dev20240331+nightly, 0.1.dev20240401+nightly, 0.1.dev20240402+nightly, 0.1.dev20240403+nightly, 0.1.dev20240404+nightly, 0.1.dev20240405+nightly, 0.1.dev20240406+nightly, 0.1.dev20240407+nightly, 0.1.dev20240408+nightly, 0.1.dev20240409+nightly, 0.1.dev20240410+nightly, 0.1.dev20240411+nightly, 0.1.dev20240412+nightly, 0.1.dev20240413+nightly, 0.1.dev20240414+nightly, 0.1.dev20240415+nightly, 0.1.dev20240416+nightly, 0.1.dev20240417+nightly, 0.1.dev20240418+nightly, 0.1.dev20240419+nightly, 0.1.dev20240420+nightly, 0.1.dev20240421+nightly, 0.1.dev20240422+nightly, 0.1.dev20240423+nightly, 0.1.dev20240424+nightly, 0.1.dev20240425+nightly, 0.1.dev20240426+nightly, 0.1.dev20240427+nightly, 0.1.dev20240428+nightly, 0.1.dev20240429+nightly, 0.1.dev20240430+nightly, 0.1.dev20240501+nightly, 0.1.dev20240502+nightly, 0.1.dev20240503+nightly, 0.1.dev20240504+nightly, 0.1.dev20240505+nightly, 0.1.dev20240506+nightly, 0.1.dev20240507+nightly, 0.1.dev20240508+nightly, 0.1.dev20240509+nightly, 0.1.dev20240510+nightly, 0.1.dev20240511+nightly, 0.1.dev20240512+nightly, 0.1.dev20240513+nightly, 0.1.dev20240515+nightly, 0.1.dev20240516+nightly, 0.1.dev20240517+nightly, 0.1.dev20240518+nightly, 0.1.dev20240519+nightly, 0.1.dev20240520+nightly, 0.1.dev20240521+nightly, 0.1.dev20240522+nightly, 0.1.dev20240523+nightly, 0.1.dev20240524+nightly, 0.1.dev20240525+nightly, 0.1.dev20240526+nightly, 0.1.dev20240527+nightly, 0.1.dev20240528+nightly, 0.1.dev20240529+nightly, 0.1.dev20240531+nightly, 0.1.dev20240601+nightly, 0.1.dev20240602+nightly, 0.1.dev20240603+nightly, 0.1.dev20240604+nightly, 0.1.dev20240605+nightly, 0.1.dev20240606+nightly, 0.1.dev20240607+nightly, 0.1.dev20240608+nightly, 0.1.dev20240609+nightly, 0.1.dev20240610+nightly, 0.1.dev20240611+nightly, 0.1.dev20240612+nightly, 0.1.dev20240613+nightly, 0.1.dev20240614+nightly, 0.1.dev20240615+nightly, 0.1.dev20240616+nightly, 0.1.dev20240617+nightly, 0.1.dev20240618+nightly, 0.1.dev20240619+nightly, 0.1.dev20240620+nightly, 0.1.dev20240621+nightly, 0.1.dev20240622+nightly, 0.1.dev20240623+nightly, 0.1.dev20240624+nightly, 0.1.dev20240625+nightly, 0.1.dev20240626+nightly, 0.1.dev20240627+nightly, 0.1.dev20240628+nightly, 0.1.dev20240629+nightly, 0.1.dev20240630+nightly, 0.1.dev20240701+nightly, 0.1.dev20240702+nightly, 0.1.dev20240703+nightly, 0.1.dev20240704+nightly, 0.1.dev20240705+nightly, 0.1.dev20240706+nightly, 0.1.dev20240707+nightly, 0.1.dev20240708+nightly, 0.1.dev20240709+nightly, 0.1.dev20240710+nightly, 0.1.dev20240711+nightly, 0.1.dev20240712+nightly, 0.1.dev20240713+nightly, 0.1.dev20240714+nightly, 0.1.dev20240715+nightly, 0.1.dev20240716+nightly, 0.1.dev20240717+nightly, 0.1.dev20240718+nightly, 0.1.dev20240719+nightly, 0.1.dev20240720+nightly, 0.1.dev20240721+nightly, 0.1.dev20240722+nightly, 0.1.dev20240723+nightly, 0.1.dev20240724+nightly, 0.1.dev20240725+nightly, 0.1.dev20240726+nightly, 0.1.dev20240727+nightly, 0.1.dev20240728+nightly, 0.1.dev20240729+nightly, 0.1.dev20240730+nightly, 0.1.dev20240731+nightly)

ERROR: No matching distribution found for libtpu-nightly==0.1.dev20230825; extra == "tpu"



---

 # Comments from other users

> ## Sergey Bryansky
> 
> It seems there is problem with python env in new docker image, dirty fix is
> 
> !pip install torch~=2.1.0 torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q --use-deprecated=legacy-resolver
> 
> 
> 


---

> ## dddmd
> 
> I ran into the same problem today, but was able to install it just now.
> 
> 
> 


---

> ## Lorry Zou
> 
> I ran into exactly the same issue today.
> 
> 
> 


---



* --- discussion numver 151, the number of votes :0 ---

# Does increase the batch size of gemma2 9b in training do some help?

**Dlond Mike** *Sun Jul 28 2024 09:06:07 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

i new here and i'm quite confused about this



---

 # Comments from other users

> ## xiaotingting
> 
> It seems that we need to use grid search to find a suitable learning rate when the batch size is fixed. If the graphics card has some spare capacity, we can actually scale the batch size and learning rate proportionally. Generally speaking, we can think that scaling the batch size and learning rate proportionally is equivalent.
> 
> 
> 
> > ## Z Hello
> > 
> > Should the learning rate and batch size be increased or decreased simultaneously?
> > 
> > 
> > 


---

> ## ano
> 
> I also want to know how everyone chooses batch size, learning rate and architecture (like gate_proj, q_projâ€¦ when using lora)
> 
> 
> 
> > ## Dlond MikeTopic Author
> > 
> > ðŸ¥¹some tips?
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > All the insights coming in 9 days ðŸ˜‰
> > > 
> > > 
> > > 


---



* --- discussion numver 152, the number of votes :0 ---

# Is there verbosity bias?

**OHIRA** *Tue Jul 30 2024 17:37:18 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I'm using Llama 3 , Is there serious verbosity bias ?

If there is the bias, I want to know how to prevent it using prompt technique.





* --- discussion numver 153, the number of votes :0 ---

# Help! Loss function not behaving as expected!

**Shreyansh Murathia** *Sat Jul 27 2024 22:43:41 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

pred = torch.tensor([[1.0, 0.0, 0.0]])

target = torch.tensor([0])

loss_fct = nn.CrossEntropyLoss()

â€‹

loss = loss_fct(pred, target)

loss

â€‹

tensor(0.5514)

Should'nt the loss be 0 or a very small value? Why is it returning 0.5514? 

I am really sorry if this question is very easy. I am fairly new to this loss. 



---

 # Comments from other users

> ## sayoulala
> 
> Answer from gpt4:
> 
> No need to apologize! Your question is absolutely valid, and I'm here to help clarify it for you.
> 
> In your scenario, you are using nn.CrossEntropyLoss from PyTorch, which combines nn.LogSoftmax and nn.NLLLoss (negative log likelihood loss). Let me break down your example and explain why you are getting a loss value of 0.5514.
> 
> CrossEntropyLoss in PyTorch
> 
> nn.CrossEntropyLoss expects:
> 
> pred (predictions): a 2D tensor of raw, unnormalized scores for each class (often called "logits"). Shape: (batch_size, num_classes).
> 
> target: a 1D tensor of the true class indices. Shape: (batch_size,).
> 
> The CrossEntropyLoss applies a LogSoftmax function to the raw scores first, converting them into log-probabilities. It then uses the NLLLoss to compute the negative log likelihood of the correct class.
> 
> In your example:
> 
> python
> 
> import torch
> 
> import torch.nn as nn
> 
> pred = torch.tensor([[1.0, 0.0, 0.0]])
> 
> target = torch.tensor([0])
> 
> loss_fct = nn.CrossEntropyLoss()
> 
> loss = loss_fct(pred, target)
> 
> print(loss)  # Output: tensor(0.5514)
> 
> Step-by-Step Explanation
> 
> Raw logits (pred):
> 
> python
> 
> [[1.0, 0.0, 0.0]]
> 
> Applying LogSoftmax to logits: The LogSoftmax transformation converts logits to log-probabilities. For pred, the resulting log-probabilities are calculated as:
> 
> python
> 
> log_probabilities = torch.log_softmax(pred, dim=1)
> 
> The softmax transformation for your pred would be:
> 
> python
> 
> softmax(pred) = [exp(1.0)/sum(exp(1.0) + exp(0.0) + exp(0.0)),
> 
>                  exp(0.0)/sum(exp(1.0) + exp(0.0) + exp(0.0)),
> 
>                  exp(0.0)/sum(exp(1.0) + exp(0.0) + exp(0.0))]
> 
>              â‰ˆ [0.5761, 0.2119, 0.2119]
> 
> Applying log to the softmax probabilities gives us:
> 
> python
> 
> log_probabilities = [log(0.5761), log(0.2119), log(0.2119)]
> 
>                   â‰ˆ [-0.5514, -1.5514, -1.5514]
> 
> Negative Log Likelihood of the target class: Since the target class is 0 (target = [0]), we take the log-probability of the first element:
> 
> python
> 
> loss = -log_probabilities[0]
> 
>      = -(-0.5514)
> 
>      = 0.5514
> 
> Summary
> 
> The loss value of 0.5514 is not incorrect. It is derived from the log-probability of the correct class (class 0) for your prediction. The fact that the prediction is not perfect (with a raw logit of 1.0 vs. 0.0 for other classes) leads to a non-zero loss.
> 
> Had the logit for class 0 been significantly higher (e.g., [10.0, 0.0, 0.0]), the softmax probability for class 0 would be closer to 1, resulting in a loss closer to 0. Here, the logit of 1 is not high enough to give a near-zero softmax probability and thus results in a non-zero loss.
> 
> Feel free to ask if you have other questions or need further clarification!
> 
> 
> 
> > ## sayoulala
> > 
> > The input should not be probabilities; it should be the values before applying softmax.
> > 
> > 
> > 
> > ## Shreyansh MurathiaTopic Author
> > 
> > Thanks a lot ðŸ˜Š
> > 
> > 
> > 


---



* --- discussion numver 154, the number of votes :0 ---

# An idea to use Reward Model

**bao** *Fri Jul 26 2024 20:02:28 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hello, kagglersï¼š

I have tried an idea with Reward Modelï¼Œsomeone who is instrested in can disccuss in 

[notebook](https://www.kaggle.com/code/huangshibao/internlm2-reward/notebook)





* --- discussion numver 155, the number of votes :0 ---

# Llama loading error! HELP!

**ë°•ë¯¼ìš±peterminpark** *Sat Jul 27 2024 14:33:00 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)



When i try to load 

[https://www.kaggle.com/models/metaresearch/llama-3/Transformers/8b-chat-hf/1](https://www.kaggle.com/models/metaresearch/llama-3/Transformers/8b-chat-hf/1)

with llamaforsequenceclassification i get the above error.



---

 # Comments from other users

> ## Valentin Werner
> 
> Make sure you apply for access to the model first. Access is usually granted within 10minutes to 24hours.
> 
> 
> 


---



* --- discussion numver 156, the number of votes :0 ---

# how to fine-tune again?

**Mukatai** *Tue Jul 16 2024 01:09:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

In the public notebook, there is information about fine-tuning the Gemma2 model. Do you know how to fine-tune a model that has already been fine-tuned again?

`@dataclass

class Config:

    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'

    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600'

    max_length = 2048

    batch_size = 4

    device = torch.device("cuda")    

    tta = False  # test time augmentation. --

    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()`

I am trying to load a pre-trained model with lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600', but the final results suggest that it has only trained on the additional data.



---

 # Comments from other users

> ## Darshan Patel
> 
> [@mukatai](https://www.kaggle.com/mukatai) You load the pre-trained model Gemma-2-9b-it-4bit and apply the LoRA adapter from /kaggle/input/73zap2gx/checkpoint-5600. This combined model becomes your new base model, which you then fine-tune using your own dataset and hyperparameters.
> 
> 
> 


---



* --- discussion numver 157, the number of votes :0 ---

# Any ways to see submission runtime?

**gmin5y** *Thu Jul 25 2024 03:34:25 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hi, I am new to Kaggle and I am wondering if we can have access to the outputs of the notebooks we submitted for scoring (probably not?) If we couldn't access the submission logs, then how can we know how long our notebooks run? I have seen people discussing about the runtime of the notebook and I would really appreciate someone can explain how you measure the runtime of your solution on test set.

Thank you all! Best of luck for everyone in the coming 2 weeks!



---

 # Comments from other users

> ## Valentin Werner
> 
> You can hover over the "3h ago" to get an exact timestamp of submission. You cannot see exactly how long it runs, so you have to check in regularly, which is quite annoying.
> 
> Submission logs do not exist, as you could just print all the testdata etc.
> 
> 
> 
> > ## gmin5yTopic Author
> > 
> > 
> > You can hover over the "3h ago" to get an exact timestamp of submission. You cannot see exactly how long it runs, so you have to check in regularly, which is quite annoying.
> > 
> > Submission logs do not exist, as you could just print all the testdata etc.
> > 
> > Thank you so much!
> > 
> > 
> > 
> > ## RB
> > 
> > I have used  [this](https://www.kaggle.com/c/riiid-test-answer-prediction/discussion/201047) , its easy and works , you don't have to keep checking forever to measure runtime.
> > 
> > [This](https://www.kaggle.com/code/cpmpml/submission-timing) is another option.
> > 
> > 
> > 


---



* --- discussion numver 158, the number of votes :0 ---

# Do I still need to continue training?

**KeShuang Liu** *Fri Jul 26 2024 14:05:04 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I have fine tuned many models, but the losses have remained almost unchanged. Is it necessary for me to continue training because I haven't trained enough? I find it difficult to make a decision at this point.This is my current validation set loss curve, which I verify every 2000 steps



---

 # Comments from other users

> ## S J Moudry
> 
> Are you testing on a validation set? I'd be more worried about performance there.  I'd also check my warmup steps and set them around 5-20%, having too few can cause a big drop right away but then you never really improve.
> 
> 
> 


---



* --- discussion numver 159, the number of votes :0 ---

# How to choose a suitable model

**KeShuang Liu** *Wed Jul 24 2024 13:48:06 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

When I was fine-tuning, log_dass started oscillating around 1. Do I need to choose a point with the lowest loss to submit, or do I submit the final result



---

 # Comments from other users

> ## Valentin Werner
> 
> This looks like a training loss, right? Your validation loss should be more stable. 
> 
> Selecting a model should never be done based on training loss, as training loss does not represent the way the model can predict on unseen data. From my experience, it is either best to train with CV to find best parameters and then do a full train for a fixed length, that worked best for all the CV models OR you train a single model, use a validation set and submit that single model.
> 
> You would then select the model / parameter which minimizes the validation loss. Note, that you can still overfit on validation loss, as you might take a specific point of the epoch which has the lowest validation loss. Small differences in validation loss do not necessarily reflect capability of the model on new data. So, it is fine to you the leaderboard submissions as "test set", which basically validates your validation loss. If your model has a lower validation loss and leaderboard score (which is loss in this case), this is a promising model.
> 
> Hope this helps you out!
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Thank you very much. I just checked and confirmed that this is indeed a training loss rather than a validation set loss. I will use the validation set loss to select the model and also try to use the training method you mentioned. Thank you for your reply
> > 
> > 
> > 


---

> ## Casmir Sunny
> 
> I will suggest that you submit the model corresponding to the checkpoint with the lowest validation loss rather than the final model. This approach ensures you are submitting the most generalizable and best-performing version of your model.
> 
> 
> 
> > ## Valentin Werner
> > 
> > This does not have to be the case. You can definetly overfit on validation data by blindly following this approach, particularly when only using a smaller subset of data as validation data. It also makes sense to compare validation loss with training loss, and decide from there. 
> > 
> > 
> > 


---

> ## Yi-Fu Chen
> 
> About Trainer
> 
> My understanding is that "Training Loss" is the last loss calculated during training, and there is no average, while "Validation Loss" is the average loss of the entire Validation dataset.
> 
> So "Validation Loss" may be smoother and "Training Loss" may be jumpier.
> 
> 
> 


---



* --- discussion numver 160, the number of votes :0 ---

# Does post-processing apply to this competition?

**Nicole** *Wed Jul 24 2024 08:12:23 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I tried to use some post-processing to deal with my prediction, but the effect was not good. Did you have any improvement in post-processing?



---

 # Comments from other users

> ## Valentin Werner
> 
> I tried some post processing early in the competition, which did also not work well for me. I think the intuition is that the model is basically already caibrating itself, meaning if it says "0.4" as highest probability, it will be right around 40% of the time. And 80% will be right around 80% of the time. 
> 
> If you now say 0.4 is not really confident, and that should be 0.33 , you will increase the loss in ~6 out of 10 cases (because the loss is lower if your predict 40% and it actually the right prediction).
> 
> I prepared this little code snippet to demonstrate this: 
> 
> ```
> from sklearn.metrics import log_loss
> 
> y_true = [[1,0,0]] * 4 + [[0,1,0]] * 3 + [[0,0,1]] * 3
> y_pred = [[0.4, 0.3, 0.3]] * 10
> print("raw log_loss:", log_loss(y_true, y_pred))
> # raw log_loss: 1.0888999753452235
> 
> y_true = [[1,0,0]] * 4 + [[0,1,0]] * 3 + [[0,0,1]] * 3
> y_pred = [[0.334, 0.333, 0.333]] * 10
> print("post processed (overconfident) log_loss):", log_loss(y_true, y_pred))
> # post processed (overconfident) log_loss): 1.0984133878031905
> 
> ```
> 
> Further, overconvidence is a killer. If you set a 0.8 (probably right in 80% of cases), to a 0.9, you will have a much higher loss in those 20% of cases, where you are now overconfident. You are penalized way higher for high-confidence wrong classifications.
> 
> I prepared this little code snippet to demonstrate this: 
> 
> ```
> from sklearn.metrics import log_loss
> 
> y_true = [[1,0,0]] * 2 + [[0,1,0]] * 8
> y_pred = [[0.1, 0.8, 0.05]] * 10
> print("raw log_loss:", log_loss(y_true, y_pred))
> # raw log_loss: 0.5877385652626266
> 
> y_true = [[1,0,0]] * 2 + [[0,1,0]] * 8
> y_pred = [[0.075, 0.90, 0.025]] * 10
> print("post processed (overconfident) log_loss):", log_loss(y_true, y_pred))
> # post processed (overconfident) log_loss): 0.6023418456154264
> 
> ```
> 
> I might be missing something in my intuition, but assuming your model is well calibrated, doing correction will more likely harm than fix anything.
> 
> 
> 
> > ## NicoleTopic Author
> > 
> > Totally agree with you
> > 
> > 
> > 


---

> ## Lorry Zou
> 
> I tried log-loss clipping, got same results.
> 
> 
> 


---



* --- discussion numver 161, the number of votes :0 ---

# [need help]How to use DDP to speedup sft ?

**bao** *Wed Jul 24 2024 18:42:20 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

helloï¼Œkagglers,

I use [codes ](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft/notebook) and change args like below:

```
 args = TrainingArguments(
        output_dir='/gemini/output',
        overwrite_output_dir = True,
        evaluation_strategy = "epoch",
        save_strategy = "steps",
        save_steps=200,
        save_total_limit=2,
        logging_strategy="steps",
        logging_steps=20,
        warmup_steps=20,
        optim="adamw_8bit",
        learning_rate=2e-4,
        per_device_train_batch_size=8,
        per_device_eval_batch_size=8,
        gradient_accumulation_steps=16,
        num_train_epochs=1,
        fp16=True,
        metric_for_best_model="log_loss",
        greater_is_better = False,
        report_to="none",
        accelerator="ddp"  
    )

    trainer = Trainer(
        args=args,
        model=model,
        train_dataset=ds.select(train_idx),
        eval_dataset=ds.select(eval_idx),
        data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
        compute_metrics=compute_metrics,
    )
    trainer.train()

```

but GPU0 utility can reach 97%+ï¼Œ GPU1 only got 20%- even 0%ã€‚  the total training is slower than one GPUï¼Œhow to fix it ?



---

 # Comments from other users

> ## CPMP
> 
> In order to use ddp you have to spawn two processes. Here are some example from HF documentation: [https://huggingface.co/docs/transformers/v4.43.0/en/perf_train_gpu_many#scalability-strategy](https://huggingface.co/docs/transformers/v4.43.0/en/perf_train_gpu_many#scalability-strategy)
> 
> 
> 
> > ## baoTopic Author
> > 
> > Thanks, I will try it.
> > 
> > 
> > 
> > > ## CPMP
> > > 
> > > You can try the accelerate library as well. I never used it myself, but it looks simpler to use than writing your own DDP code.
> > > 
> > > 
> > > 


---

> ## Pranshu Bahadur
> 
> Hf trainer already uses ddp try not setting any accelerator
> 
> And device_map = 'auto' when loading the model
> 
> Is your model unsloth?
> 
> Because they mention that 1xT4 is 5x faster at the end
> 
> [https://huggingface.co/unsloth/llama-3-8b-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-bnb-4bit)
> 
> [Screenshot_2024-07-24-17-08-15-27_40deb401b9ffe8e1df2f1cc5ba480b12.jpg](https://storage.googleapis.com/kaggle-forum-message-attachments/2934339/20965/Screenshot_2024-07-24-17-08-15-27_40deb401b9ffe8e1df2f1cc5ba480b12.jpg)
> 
> > ## baoTopic Author
> > 
> > I remove the accelerator setting, and set device_map = 'auto' when loading the model. But got the same 
> > 
> > 
> > 
> > > ## Pranshu Bahadur
> > > 
> > > Yeah its because of unsloth quantized models training faster on 1xt4
> > > 
> > > Have you tried just using a P100?
> > > 
> > > Also can you share screenshot of memory usage while training?
> > > 
> > > 
> > > 
> > > ## baoTopic Author
> > > 
> > > I use 2x3090. the screenshot was in my last reply message.
> > > 
> > > 
> > > 
> > > ## Pranshu Bahadur
> > > 
> > > Oh then you can use way higher batch size that's why only 1 is being used right now
> > > 
> > > 
> > > 


---



* --- discussion numver 162, the number of votes :0 ---

# A question about submission

**Owen** *Fri Jul 19 2024 09:52:13 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Will sample_submission.csv be replaced during submission like test.csv files



---

 # Comments from other users

> ## CPMP
> 
> Yes. sample submission when you submit has the same number of rows as test.
> 
> 
> 


---

> ## Enter your display name
> 
> The answer should be no.
> 
> 
> 


---

> ## Yichuan Gao
> 
> It will be replaced with the same row count and ids with input data. Almost all notebooks are using it as a template for output predictions 
> 
> 
> 


---



* --- discussion numver 163, the number of votes :0 ---

# The language

**Dlond Mike** *Sun Jul 21 2024 22:03:48 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

train.csv are all english but how about the Public Datasets and private?



---

 # Comments from other users

> ## Valentin Werner
> 
> train.csv is not all english. The data definetly involves a few multi-lingual samples. It is save to assume there could be some samples in the test data too
> 
> 
> 


---



* --- discussion numver 164, the number of votes :0 ---

# [Question] Confused with random classification layer weight(score.weight) for LlamaForSequenceClassification

**Xuhang_CN** *Sat Jul 20 2024 19:07:52 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hi , I am a freshman for llm. I want to SFT llama3 in my GPU.

When I check for the same train environment, I find classification layer weight is initialized randomly(maybe?):

When I run below code first time and second time, I get different score.weight.

model_raw = LlamaForSequenceClassification.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    num_labels=3,
    device_map='auto'
model = prepare_model_for_kbit_training(model)
config = PeftConfig.from_pretrained(finetune_model_name)
model = PeftModel.from_pretrained(model, finetune_model_name,is_trainable=False)
)

Even if I load saved adapter weight, weight is still not the same.

So I want to know how to make sure I train llama3 in my GPU and upload adapter in kaggle to make sure I get the same model.

Thanks!



---

 # Comments from other users

> ## Valentin Werner
> 
> You could set the seed for the run. That way you always get the same initial values. Make sure versions are similar / identical between kaggle notebook and local. There are plenty of "seed all" functions on kaggle and google that you can use
> 
> 
> 


---

> ## hn
> 
> You may need to do a custom save LoRA as some combinations may render the LoRA saving the wrong weights.
> 
> 
> 


---



* --- discussion numver 165, the number of votes :0 ---

# Will we no longer be able to use GPUs and TPUs in competitions?

**HÄ±dÄ±r Demir** *Sat Jul 20 2024 05:11:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Will we no longer be able to use GPUs and TPUs in competitions?ns?  



---

 # Comments from other users

> ## Lorry Zou
> 
> L UI change 
> 
> 
> 


---

> ## Nicholas Broad
> 
> Check the Settings > Accelerator from the top menu bar
> 
> 
> 


---

> ## Akeryu Ryuu
> 
> I don't know if they are experimenting with new UI but you can find the accelerator option in session options on the right side where you turn off internet for the notebook. 
> 
> 
> 


---



* --- discussion numver 166, the number of votes :0 ---

# special token Efficiency

**ë°•ë¯¼ìš±peterminpark** *Fri Jul 19 2024 07:18:04 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

many organized their dataset into  +  +  input text form. I tried adding , ,  as special tokens and trained a model but the result was not good does anyone know why this is the case.



---

 # Comments from other users

> ## cm391
> 
> have you resized the embedding to take into account that you have added new tokens?
> 
> Gemma contains some spare special tokens in its tokenizer - you could just repurpose those!
> 
> 
> 
> > ## ë°•ë¯¼ìš±peterminparkTopic Author
> > 
> > thx
> > 
> > I did resize my model.
> > 
> > recycling unused special token is a good idea. I'll try that out
> > 
> > 
> > 


---



* --- discussion numver 167, the number of votes :0 ---

# Question about submission

**Stanislav9801** *Thu Jul 18 2024 05:44:16 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

When creating submission, do my notebook should take as input test.csv file and produce output.csv file?

I mean, when I submit notebook, does test data will be loaded there dynamically, and through notebook inference submission.csv file should be produced? And this test.csv will not be the same as test.csv in the data provided?

So in this case my notebook should behave as a function, which takes test.csv as input, and submission.csv as output? Am I right?

This is my first time submission, so I don't understand this process a bit.

Thanks.



---

 # Comments from other users

> ## CristÃ³bal Mackenzie
> 
> Yes, you are right. On submission, the test.csv file will contain all test data, not just the three samples you see when editing a notebook.
> 
> 
> 
> > ## Stanislav9801Topic Author
> > 
> > Thank you!
> > 
> > 
> > 


---



* --- discussion numver 168, the number of votes :0 ---

# Usage of unsloth

**Varun Jagannath** *Mon Jul 08 2024 01:16:33 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Has anyone used unsloth in the competition as it says that training and inferencing is much with it. Looking forward for suggestions.



---

 # Comments from other users

> ## Ivan Vybornov
> 
> They do not have kernels for classifier yet.
> 
> 
> 
> > ## CristÃ³bal Mackenzie
> > 
> > Exactly, and I think the "axolotl" library which is also being very widely used doesn't support classification models yet.
> > 
> > 
> > 
> > ## Varun JagannathTopic Author
> > 
> > ok, thanks for the input. But do you think it would be too much of a task if we give prompts and then ask model to predict the classes.
> > 
> > 
> > 
> > ## Takamichi Toda
> > 
> > In the CausalLM header can use the probabilities of generation of tokens A, B and tie as predictions.
> > 
> > ```
> > inputs = tokenizer(text)
> > out = model(inputs)
> > pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
> > pred = out.logits[0, -1, pred_token_id].softmax(0)
> > 
> > ```
> > 
> > 
> > 
> > > ## Varun JagannathTopic Author
> > > 
> > > Tried this method as well, but it always predicts or weighs towards one class. Even I have checked your notebook. Model A is always having higher weight. I guess the classification head is working well in this competition.
> > > 
> > > 
> > > 
> > > ## Takamichi Toda
> > > 
> > > In my experiment as well, the classification head has been producing better results so far. I am currently in the process of trial and error to see if improvements can be made with fine-tuning (in [this discussion](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/520470#2925128), it seems they were able to achieve 0.902 with the generation head).
> > > 
> > > 
> > > 


---



* --- discussion numver 169, the number of votes :0 ---

# Error when submit

**Nguyá»…n Anh TÃº** *Mon Jul 15 2024 16:23:48 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I got the error "Submission Scoring Error" when I submitted  my notebook, I thought that I set the wrong format for my submission.csv. Then, I read the sample_submission.csv and change the the value of ['winner_model_a', 'winner_model_b', 'winner_tie'] columns with my y_predict. The worst thing is my notebook ran successful but when I submitted again I got the error "Notebook Threw Exception", please help me!



---

 # Comments from other users

> ## Valentin Werner
> 
> ### Submission scoring error -> make sure that you include id
> 
> An example way to get a working submission:
> 
> ```
> # Submit
> sub = pd.DataFrame(sm, index = test.id, columns = ["winner_model_a","winner_model_b","winner_tie"]).reset_index()
> sub.to_csv('submission.csv', index=False)
> sub.head()
> 
> ```
> 
> where sm is an array like np.array([0.123,0.567,0.234],â€¦,[0.999,0.000,0.001])
> 
> ### Notebook threw exception
> 
> You managed to make a working notebook not work anymore ðŸ˜€ This could have some reasons: GPU goes OOM (this does not trigger an OOM Error); There are some "null" responses in the responses which need to be handled (e.g., replace null with 'null' before loading the string representation of the list as real list); There is actually an error raised during runtime.
> 
> What you can do to evaluate the errors is try to run your inference code on the half the train set (which is basically the size of test) and see what happens.
> 
> 
> 
> > ## Nguyá»…n Anh TÃºTopic Author
> > 
> > It's very helpful for me. Thanks a lot.
> > 
> > 
> > 


---



* --- discussion numver 170, the number of votes :0 ---

# Training not proceeding for Llama 3

**JamshaidSohail** *Sun Jul 14 2024 20:29:44 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hi. I am trying to train the Llama 2 model from the [notebook](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook) shared by [@kishanvavdara](https://www.kaggle.com/kishanvavdara). But my training is not proceeding as shown in the figure. Any help would be appreciated.  



---

 # Comments from other users

> ## Valentin Werner
> 
> If you have the tqdm per epoch, I recommend changing it to the inner loop (steps within the epoch) to see if it actually does something:
> 
> ```
> for epoch in range(CFG.NUM_EPOCHS):
>     ste = time()
>     for step in tqdm(range(STEPS_PER_EPOCH)):
>         # Zero Out Gradients
>         OPTIMIZER.zero_grad()
> 
> ```
> 
> Also, the first samples sometimes take multiple minutes (I once had 300 seconds for the first batch) but then it will speed up afterwards. The notebook shared works well technically, so if you havent changed anything, I would just recommend factory reset and try again.
> 
> 
> 
> > ## JamshaidSohailTopic Author
> > 
> > The only changes I need to do is the addition of HF_TOKEN for my own account in the tokenizer as well as in the model loading area as below. 
> > 
> > ```
> > model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
> > tokenizer = AutoTokenizer.from_pretrained(model_id,use_auth_token=HF_TOKEN)
> > 
> > base_model = LlamaForSequenceClassification.from_pretrained(model_id,
> >                                                             use_auth_token=HF_TOKEN,
> >                                                             torch_dtype=torch.bfloat16,
> >                                                             num_labels=3)    
> > 
> > ```
> > 
> > Now i followed your advice and added the tqdm to the STEPS_PER_EPOCH line as well and watching the training goes inside. 
> > 
> > 
> > 
> > > ## JamshaidSohailTopic Author
> > > 
> > > It is working now. Thank you for your comment and help ðŸ˜€
> > > 
> > > 
> > > 


---



* --- discussion numver 171, the number of votes :0 ---

# How to solve no kernel

**KeShuang Liu** *Thu Jun 20 2024 12:36:40 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

RuntimeError: cutlassF: no kernel found to launch!



---

 # Comments from other users

> ## YingxiZhang
> 
> what kernelï¼ŸAny more errors?
> 
> 
> 


---



* --- discussion numver 172, the number of votes :0 ---

# Should we train our model just for English or for multiple languages ? 

**AlphaTT30** *Sun Jul 14 2024 21:51:33 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Can the test data contain multiple languages or just English? 



---

 # Comments from other users

> ## Valentin Werner
> 
> The training data does include non-english samples. Even languages with non-latin alphabet (e.g., asian languages and russian). However, the amount of samples seem to be quite low. Keep in mind that many models are able to "speak" multile languages inherently, so if you are using LLMs like Llama or Gemma, you are probably wasting time on this filter.
> 
> 
> 


---



* --- discussion numver 173, the number of votes :0 ---

# Can we use external data that contain other model's responses excluding  those in  the competition data? 

**AlphaTT30** *Sun Jul 14 2024 18:39:39 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Is there any restriction to use the other model's responses that are not in the competition data? Or can we use any model's responses?



---

 # Comments from other users

> ## Valentin Werner
> 
> Just need to make sure data is available open source. Good luck!
> 
> 
> 
> > ## AlphaTT30Topic Author
> > 
> > I have another question, can the test data set contain multiple languages or just English? 
> > 
> > 
> > 


---



* --- discussion numver 174, the number of votes :0 ---

# how to save qlora_trained model weight with pytorch

**YEI0907** *Sat Jul 13 2024 14:23:51 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

because of some problems,I can`t use qlora to train my model  with transformers.Trainer,so,I trained my qlora model with my own pytorch training script and save my weights with torch.save directly,but when i use my weights to inference,i found that when i run my notebook ,my predictions is different in each turns .so,i think my model saving method might be wrong,so, how can i save my trained weights after qlora training with pytorch training script? can i use torch.save directly? or is there any other problems during my inference?Thanks very much for answering my question



---

 # Comments from other users

> ## Valentin Werner
> 
> If you load the model with the peft library, you simply also use model.save_pretrained. This will only save the adapter and not the full model. So we are talking < 100 MB instead of multiple GB.
> 
> Code snippet as example:
> 
> ```
> if test_loss < best_val and epoch != 0:
>     model.save_pretrained(
>         f"my_newest_model_{epoch+1}_{step}"
>     )
> 
> ```
> 
> You will later load it by first loading the model itself (e.g., LlamaForSequenceClassification), getting the PEFT model and then loading the QLoRA weights
> 
> ```
> # Get peft
> model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
> # Load weights
> model_0.load_state_dict(torch.load(CFG.LORA_PATH), strict=False)
> model_0.eval()
> 
> ```
> 
> 
> 
> > ## YEI0907Topic Author
> > 
> > thanks!,I use torch.save directly to save my model,code just like this
> > 
> > ```
> > if score < best_score:
> >             best_score = score
> >             if int(os.environ["RANK"]) == 0:
> >                 torch.save({
> >                     'epoch': epoch,
> >                     'model_state_dict': dict([(k, v) for k, v in model.module.named_parameters() if v.requires_grad]),
> >                     'optimizer_state_dict': optimizer.state_dict(),
> >                     'scheduler_state_dict': scheduler.state_dict()
> >                 }, model_path)
> > 
> > ```
> > 
> > and my model loading method is same as yours,
> > 
> > so you mean I should use peft_model.save_pretrained() insteat of torch.save?
> > 
> > addtionaly,i test my weights in kaggle by using train_data and my loss is 1.8xxxx,but in my traing, the eval_sets loss is 0.9XX and train_sets loss is also 0.9xx
> > 
> > by the way,i learned a lot from your notebooks, thank you for sharing your notebook!
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Happy to be of help :)
> > > 
> > > I highly recommend saving adapters separately and loading them separately. I do not see any value in saving full models as long as you are not adding any layers to the model. If you create a custom model with additional layers, you will need to save these weights to, in that case it might make sense to save full models.
> > > 
> > > However, if you load Llama3 like this:
> > > 
> > > ```
> > > base_model = LlamaForSequenceClassification.from_pretrained(model_id, token=HF_TOKEN, num_labels=CFG.NUM_LABELS, torch_dtype=CFG.TORCH_DTYPE, trust_remote_code=True)   
> > > 
> > > ```
> > > 
> > > The sequence classification head is always initialized in the same way, just random values. Things that can help achieve the same result is setting the seed, so the weights are randomly initialized exactly as they were initialized during training.
> > > 
> > > ```
> > > def set_seeds(seed):
> > >     """Set seeds for reproducibility """
> > >     os.environ['PYTHONHASHSEED'] = str(seed)
> > >     random.seed(seed)
> > >     np.random.seed(seed)
> > >     torch.manual_seed(seed)
> > >     if torch.cuda.is_available():
> > >         torch.cuda.manual_seed(seed)
> > >         torch.cuda.manual_seed_all(seed)
> > >     # Set seed for all TPU cores
> > >     xm.set_rng_state(seed, device=xm.xla_device())  
> > > 
> > > ```
> > > 
> > > Doing this helped me to achieve a better LB <-> CV relationship.
> > > 
> > > Further, you may want to save the tokenizer or replicate and deterministic changes you did to it during training (e.g., if you changed pad_token_id or such). Make sure you are exactly tokenizing the same way (also input formats etc.) as you did during training. This can easily switch a lot for your score too.
> > > 
> > > 
> > > 
> > > ## YEI0907Topic Author
> > > 
> > > thanks,this provide a new idea for me to ensure consistency between train and test ;for my model , i had added some my custom modules,and i will re-train the model and save the weight with two part,one for llama3 with peft.save_pretrained(),one for my custom module with torch.save directly.T_Tï¼Œthe previous weight resulted in my LB score reaching 3.x
> > > 
> > > 
> > > 
> > > ## Ilya Turaev
> > > 
> > > All my life I've been thinking that classification heads for decoders were pretrained on some benchmarks or other data. Glad that I've busted this myth and misunderstanding nowâ€¦
> > > 
> > > 
> > > 


---



* --- discussion numver 175, the number of votes :0 ---

# Which hardware configuration is used in scoring?

**Michael Benton** *Sat Jul 13 2024 10:56:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I'm at a point where I'm making optimizations to get within 9hours. I just launched one that should work in time if on a p100, but won't if on one of the TPM or 2x T4. Does it automatically give you the same hardware configuration as you have set in the notebook? 



---

 # Comments from other users

> ## Valentin Werner
> 
> Yep, you get what you choose during runtime.
> 
> 
> 


---



* --- discussion numver 176, the number of votes :0 ---

# Notebook throwing Exception

**Varun Jagannath** *Wed Jul 10 2024 23:45:51 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

After submitting the notebook. And, after some grueling hours it throws exception and there is no way to find out about it. Is there any other way to check.



---

 # Comments from other users

> ## Valentin Werner
> 
> This is just one potential issue, but how are you handling the "null" that sometimes occur? Doing this helped for me in the early submissions: response.replace("null","'null'") <-- note the extra ' to make it a string before loading the string representation of the list an actual list (e.g., with ast.literal_eval(response))
> 
> I really hope this is doing it for you - because these exceptions can be nasty. If not, maybe try debugging by predicting on training data again.
> 
> If the GPU goes OOM its not an OOM error on kaggle but an Exception, so maybe reduce batch size or max length too. Best of luck!
> 
> 
> 
> > ## Varun JagannathTopic Author
> > 
> > Looks like its an issue with batch size
> > 
> > 
> > 


---

> ## Robert Turro
> 
> Try clicking on the notebook then going to the Logs section.
> 
> 
> 


---



* --- discussion numver 177, the number of votes :0 ---

# Using template for instruct model?

**Weiren** *Thu Jul 11 2024 15:42:49 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I heard that using instruct model would perform slightly better. But when I was using Llama-8b and Llama-8b-instruct without template,  they got the same LB score.

Does the template matters? I had tried using template but the score was even worse. Also, I found that using a template causes the logloss stuckâ€¦

Some details:

1 epoch

4 batch size * 2 accumulation_steps

and just trying different lora params ðŸ¤¡

Any thoughts or insights on this?



---

 # Comments from other users

> ## hn
> 
> Actually anecdotally I use the Llama3 template as well for instruct and I think itâ€™s worse off than no template for some reason. 
> 
> 
> 


---



* --- discussion numver 178, the number of votes :0 ---

# What is 8b-chat-hf in kaggle's llama3 models? 

**Dylan Liu** *Thu Jul 11 2024 18:38:03 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

If I'm right, Llama 3 has a normal version and an instruct version. 

In [https://www.kaggle.com/models/metaresearch/llama-3/Transformers/8b-chat-hf/1](https://www.kaggle.com/models/metaresearch/llama-3/Transformers/8b-chat-hf/1), I see 8b-chat-hf, what's that? And where is the instruct version?



---

 # Comments from other users

> ## Weiren
> 
> I think the 8b-chat-hf one is fine-tuned for dialogue use cases and in huggingface format. 
> 
> The instruct model I think you can download from hugging face then upload to kaggle.
> 
> 
> 
> > ## CristÃ³bal Mackenzie
> > 
> > I actually think the 8b-chat-hf is the same as the instruct model in huggingface.
> > 
> > 
> > 
> > > ## Weiren
> > > 
> > > Yea, should be the same. thanks~
> > > 
> > > 
> > > 


---



* --- discussion numver 179, the number of votes :0 ---

# How to avoid wights and bias logging, when I try train the model, it was asking wights and bias token id

**suri@7** *Wed Jul 10 2024 18:24:36 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

when I try train the model, it was asking wights and bias token id. I don't want to wights and bias to my model.

```
training_args = TrainingArguments(
  output_dir="./kaggle/input/lmsys-chatbot-arena/bert_model",
  learning_rate=2e-5,
  per_device_train_batch_size=3,
  per_device_eval_batch_size=3,
  num_train_epochs=2,
  weight_decay=0.01,
  evaluation_strategy="epoch",
  save_strategy="epoch",
  load_best_model_at_end=True,
)
trainer = Trainer(
  model=model,
  args=training_args,
  train_dataset=train_dataset,
  eval_dataset=test_dataset,
  tokenizer=tokenizer,
  data_collator=data_collator,
  #compute_metrics=compute_metrics,
)
trainer.train()

```



---

 # Comments from other users

> ## waechter
> 
> Set report_to='none' in TrainingArguments if you don't want to use remote logging
> 
> report_to (str or List[str], optional, defaults to "all") â€” The list of integrations to report the results and logs to. Supported platforms are "azure_ml", "clearml", "codecarbon", "comet_ml", "dagshub", "dvclive", "flyte", "mlflow", "neptune", "tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no integrations. 
> 
>   From [https://huggingface.co/docs/transformers/en/main_classes/trainer](https://huggingface.co/docs/transformers/en/main_classes/trainer)
> 
> 
> 
> > ## suri@7Topic Author
> > 
> > Thanks for your help [@waechter](https://www.kaggle.com/waechter) ,, I would like to know after training my model how, can I can submit the prediction offline, When saving the model Kaggle input path, It does not load when submitted to p predictions.
> > 
> > 
> > 
> > > ## waechter
> > > 
> > > output_dir="./kaggle/input/lmsys-chatbot-arena/bert_model",
> > > 
> > > I think this is wrong because you trying to write to the kaggle/input directory (which is read only)
> > > 
> > > You should save the pretrained model to kaggle/working dir : output_dir="kaggle/working/bert_model"
> > > 
> > > 
> > > 
> > > ## suri@7Topic Author
> > > 
> > > ok, Thanks
> > > 
> > > 
> > > 


---



* --- discussion numver 180, the number of votes :0 ---

# Using XGBoost and Graident Boosting Techniques

**Royy** *Mon Jul 08 2024 17:59:56 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Here, the provided code defines several functions to compute various text analysis metrics to understand the quality of the prompt and responses. The functions are defined to calculate word count, character count, sentence count, average word length, average sentence length, type-token ratio, word frequency, bigram frequency, and readability scores using the nltk and textstat libraries. 

The readability scores function calculates several readability indices, including the Flesch-Kincaid score, Gunning Fog index, SMOG index, and Automated Readability Index (ARI).

After defining these functions, the code applies them to the "prompt", "response_a", and "response_b" columns of the DataFrame. For each of these columns, it calculates the word count, character count, sentence count, average word length, and average sentence length by applying the respective functions and creates new columns in the DataFrame to store these metrics. 

Here, in the notebook  I have just tried a few of these metrics to achieve a log loss of 1.05.

Ways you can enhance this code to decrease it's log loss:

Add more hyperparameters.
Try out the other metrics (which are commented on in the code).
Also, add more variables like the Jaccard index and cosine similarity between response a, b, and prompt.
Increase the number of iterations for each model per fold.

These can easily improve the model output and decrease the log loss on the test dataset.

Also, upload the textstat library as the internet is turned off for this competition.

Notebook URL: [https://www.kaggle.com/code/nehalroy/using-xgboost-and-gb-using-nltk](https://www.kaggle.com/code/nehalroy/using-xgboost-and-gb-using-nltk)



---

 # Comments from other users

> ## Valentin Werner
> 
> Hey [@nehalroy](https://www.kaggle.com/nehalroy) - please beware that Pyphen is not released as open source / commercially available package and is therefore one of the few packages that is not allowed in this competition. 
> 
> 
> 
> > ## RoyyTopic Author
> > 
> > Thank you [@valentinwerner](https://www.kaggle.com/valentinwerner) for the heads up. Highly appreciated.
> > 
> > Although, I just shared this as a technique that can be used to design a Gradient Boosting model for this certain problem.
> > 
> > 
> > 


---



* --- discussion numver 181, the number of votes :0 ---

# How to finetune LLM? 

**bao** *Tue Jun 25 2024 18:26:28 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I am a beginner in LLM and would like to get started. Could you please explain how to fine-tune the model for this scenario? Is it done using methods like DPO with LLama-Factory?



---

 # Comments from other users

> ## Staru09
> 
> You can check this [https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07](https://dassum.medium.com/fine-tune-large-language-model-llm-on-a-custom-dataset-with-qlora-fb60abdeba07)
> 
> 
> 
> > ## Nikhil Narayan
> > 
> > Well do you have resources about any other way to do this
> > 
> > 
> > 
> > > ## Staru09
> > > 
> > > Resources as in computation or something else. If computation is an issue then you can try using tools like llamaedge or wasmedge. They are very good frameworks to work with but you should know Rust otherwise you can use PEFT, or converting into lower bit like 8 or 16 to fine-tune along with LORA, QLORA etc.
> > > 
> > > 
> > > 


---

> ## Lorry Zou
> 
> There are plenty of great videos on YouTube teaching you how to fine-tune LLM and Llama 3 specifically.
> 
> 
> 
> > ## YingxiZhang
> > 
> > Indeed, there are a lot of resources on YouTube and I've learned a lot.ðŸ˜€
> > 
> > 
> > 


---



* --- discussion numver 182, the number of votes :0 ---

# config.json file missing in Llama model

**AlphaTT30** *Thu Jul 04 2024 16:18:38 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I copied other notebooks where the config.json file exists. But when I add the model as input in my notebooks the config.json file is missing so the model does not load. 

Why is that and what to do? 



---

 # Comments from other users

> ## Artyom Lyan
> 
> You should use transformers version, not pytorch
> 
> 
> 


---

> ## Saiyan Warrior
> 
> HI [@alphatt30](https://www.kaggle.com/alphatt30) there might be 2 reasons that I can think of:
> 
> You might need to get the llama permission [here](https://www.kaggle.com/models/metaresearch/llama-3)
> You might have given the wrong path.
> 
> can you share the exact error?
> 
> 
> 
> > ## AlphaTT30Topic Author
> > 
> > I have permission and the path was correct, I even tried other's notebooks where they used the same model. so I removed their model and then loaded the same model in input and config.json was still missing there 
> > 
> > 
> > 


---



* --- discussion numver 183, the number of votes :0 ---

# Clarification on the number of chats that need to be processed

**Gabriel Mirea** *Thu Jul 04 2024 21:04:10 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hi,

The dataset description says 

When your submission is scored, this example test data will be replaced with the full test set. There are 55K rows in the training data, and you can expect roughly 25,000 rows in the test set.

and the leaderboard says

This leaderboard is calculated with approximately 26% of the test data. The final results will be based on the other 74%,

So should we expect that the final submission on the private leaderboard will need to process ~75.000 rows?



---

 # Comments from other users

> ## RB
> 
> 
> So should we expect that the final submission on the private leaderboard will need to process ~75.000 rows?
> 
> No, total 25000 samples 
> 
> Public LB - 26% => 6500 Samples
> 
> Private LB - 74% => 18500 Samples
> 
> Total Test Set = 25000
> 
> When you make a submission, your code is doing inference for all 25000 samples, we only see public LB with 6500 samples, remaining after competition ends
> 
> 
> 
> > ## Gabriel MireaTopic Author
> > 
> > Thanks! That makes sense, so if the submission scores on the LB it got through all the samples. And the final score is hidden so that people don't probe, I guess?
> > 
> > My main concern was if the notebook will have to deal with more samples later. That's clear now, thank you again.
> > 
> > 
> > 


---



* --- discussion numver 184, the number of votes :0 ---

# More than 1 response (response_a / response_b)

**Samar Jaffri** *Tue Jul 02 2024 04:01:26 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Some of the rows in the test/train dataset have 2 or more response(s). Do anyone know if we need to make prediction based on all of the responses i.e., user will chose of of response_a or one of response_b.

Or if there is anything specified about that, that I am missing..?



---

 # Comments from other users

> ## waechter
> 
> The data come from [https://chat.lmsys.org/](https://chat.lmsys.org/) , you can try it to help you understand
> 
> ðŸ“œ Rules: You can continue chatting until you identify a winner.
> 
> There are the same number of responses as there are prompts, users vote for their favorite conversation
> 
> 
> 
> > ## Valentin Werner
> > 
> > Exactly this. We are not classifying responses, but conversations. However, if you think that the last response is the one that triggers the user to press "a is better", you are probably right in most cases.
> > 
> > 
> > 
> > > ## Yichuan Gao
> > > 
> > > Haven't thought about this! Wonder how much will only using the last response affects the predictionðŸ˜‚
> > > 
> > > 
> > > 


---



* --- discussion numver 185, the number of votes :0 ---

# test data does not have LLM name , while the train data had (columns : 'model_a' and 'model_b'). 

**Kumar Shivansh** *Tue Jul 02 2024 08:11:13 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

test data does not have llm name , while the train data had (columns : 'model_a' and 'model_b'). 



---

 # Comments from other users

> ## Enter your display name
> 
> This is precisely to prevent people from making predictions based on the model's name.
> 
> 
> 


---

> ## Anya
> 
> I think even the train data doesnt have to have the LLM name cols, cuz they dont matter the predictions. Our task is to predict the battle results' probability between two responses.
> 
> 
> 


---



* --- discussion numver 186, the number of votes :0 ---

# What is the calculation of the loss & log_loss?

**Anya** *Sun Jun 30 2024 16:14:27 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I happened to meet a situation that loss & log_loss was NaN. 

I know in programming it would happen when 0 is taken as the dividend or something like that. 

Now I need to know the calculation of the loss & log_loss so I could find out the cause.

I appreciate every answer.ðŸ™



---

 # Comments from other users

> ## Valentin Werner
> 
> Kaggle uses the sklearn implementation, which is quite well documented: [https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html)
> 
> The log loss does not use any division, but it uses the logarithm. This technically could produce nan values if your predictions are < 0, but this is not happening in the sklearn logloss.
> 
> Anyways, I think you should always softmax your predictions before calculating the log loss.
> 
> Hope this helps!
> 
> 
> 
> > ## AnyaTopic Author
> > 
> > Thanks a lot. I switched to another GPU with larger memory, and the error got solved.
> > 
> > Maybe the data overflow cause a value out of logarithm's definition domain.
> > 
> > 
> > 


---



* --- discussion numver 187, the number of votes :0 ---

# How to get the vector representation of sentences using Llama3?

**godmysalary** *Fri Jun 28 2024 17:22:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hi everyone! Now most public noteboks directly use "LlamaForSequenceClassification" for fine-tuning and getting the predicted probability. I was wondering how I can get the learned embeddings of response_a and response_b besides the predictions since I think the embeddings can be fed into other different classifiers. I don't want to employ another LLM due to the time constraint. So could anybody tell me how I can getting the embeddings of responses as a byproduct of the fine-tuned Llama3? Thanks.



---

 # Comments from other users

> ## RB
> 
> You can pass output_hidden_states=True when initializing model , something like this 
> 
> ```
> model  = AutoModelForCausalLM.from_pretrained(model_path, quantization_config=quantization_config, output_hidden_states=True)
> 
> out = model(input_ids = tokenized['input_ids'], attention_mask = tokenized['attention_mask'])
> 
> out.hidden_states
> 
> ```
> 
> 
> 
> > ## godmysalaryTopic Author
> > 
> > thank you!
> > 
> > 
> > 


---

> ## Enter your display name
> 
> I think what you want is the last hidden state of the model's output?
> 
> 
> 
> > ## godmysalaryTopic Author
> > 
> > exactly. So is there one way to obtain this? thanks
> > 
> > 
> > 


---



* --- discussion numver 188, the number of votes :0 ---

# how many is the perfect score?

**Anya** *Sat Jun 22 2024 17:17:35 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Im new to kaggle.

Till now, Ive seen scores range in 0.5 to 1.7

I wonder how many is the perfect score so I can evaluate my score level.

Is it 5, 10 or 100?



---

 # Comments from other users

> ## Valentin Werner
> 
> The metric "log_loss" allows for a score of 0.0 (e.g., exactly perfect predictions every time). An "educated guess" score (predicting the distribution of the train set) gets you a score of about 1.097.
> 
> The problem is that human preferences, which we are trying to predict, are not clearly predictable. This is because if we both write a prompt, we may prefer different responses - so how is our model supposed to learn which responses are better. Because the problem is this hard to predict, the "ceiling" of the score is much higher than 0.0 - more along the lines of 0.75 or 0.8 (just a gut feeling)
> 
> 
> 
> > ## AnyaTopic Author
> > 
> > I got itðŸ˜ƒ. Thanks  for such a detailed reply.
> > 
> > 
> > 


---

> ## AnyaTopic Author
> 
> After reading the leaderboard I find that the lower one scores, the higher he ranks?
> 
> 
> 
> > ## Yichuan Gao
> > 
> > Yes it is, since the score is calculated as log loss, the lower the loss, the better your guesses are
> > 
> > 
> > 


---



* --- discussion numver 189, the number of votes :0 ---

# How to run a model on two cards

**KeShuang Liu** *Mon Jun 17 2024 17:22:30 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I loaded my model on the CPU and it took up 19g, while the GPU p100 only had 16g. However, I found that if I use two t4 blocks for a total of 30g, can I deploy my model to two t4 blocks? What should I do?



---

 # Comments from other users

> ## Minato Ryan
> 
> If you are using transformers library, use device_map="auto".
> 
> like this,
> 
> ```
> AutoModelForCausalLM.from_pretrained("google-bert/bert-base-cased", device_map="auto")
> 
> ```
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Thank you very much for your reply. I succeeded using your method
> > 
> > 
> > 


---



* --- discussion numver 190, the number of votes :0 ---

# General Question 

**Satish0308** *Sat Jun 15 2024 16:23:59 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

How many times i can submit my file ?

5 submissions per day holds good in all competition ? including this one



---

 # Comments from other users

> ## Valentin Werner
> 
> Not sure if I understand correctly, but:
> 
> - you got 5 submissions per day for the rest of the competition
> 
> - you got 5 submissions per day in EACH competition you partake (so if you do 2 competitions, you could do 10 submissions, but maximum of 5 per competition)
> 
> - you do NOT have extra submissions if you only use 3 submissions on one day
> 
> - you can submit your submission.csv as many times as you want, with the maximum of 5 times a day
> 
> - you can submit the same submission.csv file 5 times a day, for the rest of the competition if you want
> 
> - you can select your best submissions in the submission tab, else the best scoring ones will be selected for you
> 
> - I have not yet seen a competition with more or less than 5 submissions per day
> 
> 
> 


---



* --- discussion numver 191, the number of votes :0 ---

# What does the #id mean and why they are non-sequential

**Eva Wang** *Wed Jun 12 2024 05:23:16 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

We noticed that the #id column was not one-by-one and we wonder why. Is it because the datapoints not shown in the train.csv are cut out because they are not in English?



---

 # Comments from other users

> ## Ahmad Al-Husainy
> 
> Besides what has been suggested, you can think of the ID as sessionID,  a way to link all parts of the text if you decide to split them into separate segments; if look closer at the prompt (and also response_a and response_b), you'll see it includes several parts or segments of a discussion. If you split these into different rows, you can use the ID to piece them back together later.
> 
> 
> 


---

> ## Valentin Werner
> 
> as per usual ID means identifier - they only have to be unique, like an index - not meaningful for training. It is quite likely that LMSYS only provided a subset of their data (I mean, else we would have a lot more) and they just kept their original IDs
> 
> 
> 


---

> ## tanaka
> 
> Id is not sequential data, because its range is from 30,192 to 4,294,947,231, but the chat bot arena has only over 1,241,035 data. Id itself is not so much meaningful data.
> 
> 
> 


---



* --- discussion numver 192, the number of votes :0 ---

# Notebook issue

**Shahik15** *Sun Jun 09 2024 04:35:45 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

Hello, 

           I have developed a notebook but it's not added to the code section? Can somebody help here?

Greetings



---

 # Comments from other users

> ## Valentin Werner
> 
> Have you made sure that it is public in the settings section?
> 
> 
> 
> > ## Shahik15Topic Author
> > 
> > yes here's the notebook link [https://www.kaggle.com/code/shahik15/gemma-rag-chatbot-arena](https://www.kaggle.com/code/shahik15/gemma-rag-chatbot-arena)
> > 
> > 
> > 


---



* --- discussion numver 193, the number of votes :0 ---

# Questions/Thoughts for building the baseline model for my first submission

**Dr. Gopalashivabalasubramanium Chandrashekaran** *Sat Jun 08 2024 10:10:08 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

I see that there is no model name in the test data.

I've listed my thoughts below. Would love to hear your input. 

So, basically, we have to analyze the response (a/b) of the unknown model and decide which one wins. 
The user prompt has to be filtered for anything that would cause the model to give a null response or a default reply
The model response, we have to categorize it somehow with the model name. Which means there would be features to look for in a model response to attach it to a named model.
Lastly, once the models are identified, we can refer to some type of weight for each model based on its win percentage versus whatever other model it is facing and decide whether it wins or loses or ties.
Slap all this into a submission file and win.


---

 # Comments from other users

> ## Valentin Werner
> 
> Step 1: Load data
> 
> Step 2: ?
> 
> Step 3: Slap all into a submission file and win ðŸ˜‰
> 
> 
> 


---



* --- discussion numver 194, the number of votes :0 ---

# Exciting and Creative

**kartikey bartwal** *Fri May 31 2024 03:49:17 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

This concept is quite exciting and beneficial for a data scientist. Choosing the appropriate statements can save or earn you millions of dollars worth of resources and customer scale. 

All the best everyone ðŸ˜Š



---

 # Comments from other users

> ## Valentin Werner
> 
> I think what becomes somewhat clear by current scores, is that it is really hard to cater individual preferences. But maybe providers that give the choice of models can extract key characteristics and serve the model without names, as personalities instead. 
> 
> Best of luck to you too!
> 
> 
> 
> > ## gmin5y
> > 
> > That's good idea haha Personalized LLM sounds interesting. However, will that turn LLM into the next ins/tiktok which only feeds what you want to see
> > 
> > 
> > 


---



* --- discussion numver 195, the number of votes :0 ---

# I encountered the following problem while training the model, how should I solve it

**KeShuang Liu** *Wed May 29 2024 14:06:23 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

wandb: Logging into wandb.ai. (Learn how to deploy a W&B server locally: [https://wandb.me/wandb-server](https://wandb.me/wandb-server))

wandb: You can find your API key in your browser here: [https://wandb.ai/authorize](https://wandb.ai/authorize)

wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:



---

 # Comments from other users

> ## tanaka
> 
> try this
> 
> [https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases?scriptVersionId=63334832&cellId=3](https://www.kaggle.com/code/ayuraj/experiment-tracking-with-weights-and-biases?scriptVersionId=63334832&cellId=3)
> 
> 
> 
> > ## KeShuang LiuTopic Author
> > 
> > Thank you for your reply. I will try it now
> > 
> > 
> > 


---



* --- discussion numver 196, the number of votes :0 ---

# how to submit my predictions

**Rikesh Prajapati** *Thu May 23 2024 12:11:21 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

can anyone tell me how to submit my predictions?



---

 # Comments from other users

> ## Valentin Werner
> 
> This is a simple step-by-step guide:
> 
> 1) Create your notebook, do your modelling and predictions. Get the predictions in the right format, you want a csv file called "submission.csv" with the columns "id", "winner_model_a", "winner_model_b", "winner_tie". If you are using pandas, you want to save the csv file WITHOUT index, such as sub.to_csv('submission.csv', index=False).
> 
> 2) The notebook needs to have Internet Access disabled
> 
> 3) Actually submit your notebook by clicking on "Submit" in the sidebar.
> 
> Note that there are also other ways to submit your predictions, which do not require you to go into the editor of a notebook.
> 
> Option 2) After making sure step 1 & 2 of the above guide are still fulfilled, you can click on the "Submit Prediction" top right of this page (or any other page of the competition). There you can select a notebook and a notebook version you want to submit. There you can also specify the output file, but you can only submit files called "submission.csv". If you DID NOT save your predictions as csv, you cannot submit them.
> 
> Option 3) You can go to the notebook you want to submit and click on "Output". There you will also find a submit button, which leads you to the same selection as Option 2.
> 
> 
> 


---



* --- discussion numver 197, the number of votes :0 ---

# How are we gonna predict probability with no data?

**alekh** *Sun May 12 2024 14:13:23 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (0 votes)

How are we gonna predict the probability when we have no training data with probabilities that makes sense. The training data is comprised of 1-hot vectors assigning 100% probability to one of the outcomes. That must clearly be wrong. I don't understand what the training set represents. Does it represent the preference of one particular human? Of a random human? If so, are there multiple rows with the same prompt and responses for different people?



---

 # Comments from other users

> ## alekhTopic Author
> 
> Guess I found the answer:
> 
> "The competition dataset consists of user interactions from the ChatBot Arena. In each user interaction a judge provides one or more prompts to two different large language models, and then indicates which of the models gave the more satisfactory response."
> 
> So i guess we have to like do a softmax on the logits to get probabilities.
> 
> 
> 


---



* --- discussion numver 198, the number of votes :-1 ---

# How to address the issue of performance loss when transferring trained model weights across different environments and devices?

**YEI0907** *Thu Jul 18 2024 20:05:31 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-1 votes)

Hi guys,

When I move my model weights to Kaggle after finishing training on my GPU server, I found a problem: the performance of my model significantly drops even though the data is the same as on my GPU server. How can I handle this problem? By the way, the only difference between Kaggle and my GPU server is the versions of Torch and CUDAï¼š

| kaggle | my gpu server |
| --- | --- |
| cuda 12.1 torch 2.1.2 | cuda11.8 torch 2.3.X |
| thanks for answering my question! |  |


---

 # Comments from other users

> ## Priyanshu Joshi
> 
> Different versions of Torch and CUDA can indeed lead to performance issues or even different behaviors in model execution. It's essential to ensure that the versions you're using are compatible and consistent. If possible, try to match the versions of Torch and CUDA on your GPU server with those on Kaggle. [Check here](https://pytorch.org/get-started/locally/) to see the see additional information.
> 
> Sometimes, subtle differences in the environment (e.g., different library versions) can also affect performance. You can export the environment configuration from your GPU server and replicate it on Kaggle using:
> 
> ```
> pip freeze > requirements.txt
> 
> ```
> 
> Then install the same requirements on Kaggle:
> 
> ```
> !pip install -r requirements.txt
> 
> ```
> 
> 
> 
> > ## YEI0907Topic Author
> > 
> > ok,thank you!
> > 
> > 
> > 


---

> ## CPMP
> 
> How do you know the performance drops?
> 
> 
> 
> > ## YEI0907Topic Author
> > 
> > I test my model by runing it on train data and compare the loss between kaggle and my server
> > 
> > 
> > 
> > > ## CPMP
> > > 
> > > testing on train data is not good practice unless you mean cross validation. Are you using cross validation?
> > > 
> > > 
> > > 


---



* --- discussion numver 199, the number of votes :-1 ---

# Why are there suddenly so many new accounts with very similar scores?

**Robert0921** *Fri Jul 12 2024 11:58:20 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-1 votes)

Looking at PB, nearly 100 new accounts suddenly appeared in the past day, and their scores were all 0.941. Is there any problem?



---

 # Comments from other users

> ## Valentin Werner
> 
> Dropping silver solutions for free!
> 
> 
> 
> > ## Cody_Null
> > 
> > Thankfully I think it will be outside of the medal range at competition end but always wild to see a medal winning notebook in the last month of every competition haha
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > I think the most wild thing is just the copy & submit. I understand it If you joined early and never did another submission. However, what will happen next is 5 Notebooks that Ensemble these to get .940, .939, â€¦ without any novel ideas
> > > 
> > > This is not the kaggle way in my opinion.
> > > 
> > > I think it is great that this approach was shared because I read a lot of people are struggling with GPU resources to get something to work, the notebook that was shared just "works a little too well"
> > > 
> > > 
> > > 


---

> ## Xinyuan Qiao
> 
> there is a new notebook provide Gemma 2 inference code with 0.941 on PB, my original score was 0.942ðŸ˜‚
> 
> 
> 
> > ## yechenzhi1
> > 
> > try to ensemble itðŸ˜‚
> > 
> > 
> > 
> > > ## Lorry Zou
> > > 
> > > I tried to ensemble it with my 0.957 Llama3 inference, and I got 0.959ðŸ˜…ðŸ˜…
> > > 
> > > 
> > > 
> > > ## Allen Wang
> > > 
> > > [@lorryzouzelun](https://www.kaggle.com/lorryzouzelun) Is the integration of gemma and llama3 effective? I tried it and the lb dropped a lot
> > > 
> > > 
> > > 
> > ## Robert0921Topic Author
> > 
> > 
> > there is a new notebook provide Gemma 2 inference code with 0.941 on PB, my original score was 0.942ðŸ˜‚
> > 
> > Under the same conditions, which one is betterï¼ŸGemma 2 and Llama3
> > 
> > 
> > 
> > > ## Xinyuan Qiao
> > > 
> > > Not sure, the gemma notebook provider use A6000 for training and I don't know his hyper-parameters. I will use same parameters to train both and reply you later.
> > > 
> > > 
> > > 


---



* --- discussion numver 200, the number of votes :-1 ---

# Trying to understand the strength of the models with the highest ties

**Dr. Gopalashivabalasubramanium Chandrashekaran** *Sat Jun 08 2024 09:37:46 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-1 votes)

Good evening everyone,

I had a go at this data and made myself a list of the models with the highest tie rate. It made sense that the non-winner models will be the ones tie-ing. It was interesting to see the models that were subpar tie-ing with each other. 

My question here is: How do I assess the strength of the model based on the count of its ties? It is easy to judge it based on its number of wins. 

Anyone have any findings on this?

Also, if you have time, see my notebook. Would appreciate any feedback.



---

 # Comments from other users

> ## tanaka
> 
> lmsys's elo rating is calculated something like following.
> 
> It means, when ties, higher rank player's score may decrease slightly, while lower-ranked player's score may increase slightly.
> 
> ```
> def compute_online_elo(battles, K=4, SCALE=400, BASE=10, INIT_RATING=1000):
>     rating = defaultdict(lambda: INIT_RATING)
> 
>     for rd, model_a, model_b, winner in battles[['model_a', 'model_b', 'winner']].itertuples():
>         ra = rating[model_a]
>         rb = rating[model_b]
>         ea = 1 / (1 + BASE ** ((rb - ra) / SCALE))
>         eb = 1 / (1 + BASE ** ((ra - rb) / SCALE))
>         if winner == "model_a":
>             sa = 1
>         elif winner == "model_b":
>             sa = 0
>         elif winner == "tie" or winner == "tie (bothbad)":
>             sa = 0.5
>         else:
>             raise Exception(f"unexpected vote {winner}")
>         rating[model_a] += K * (sa - ea)
>         rating[model_b] += K * (1 - sa - eb)
> 
>     # calibrate llama-13b to 800
>     delta = (800-rating["llama-13b"])
>     for model in battles["model_a"].unique():
>         rating[model] += delta
> 
>     return rating
> 
> ```
> 
> Refs
> 
> - [https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=hytEb0aXfcwm](https://colab.research.google.com/drive/1KdwokPjirkTmpO_P1WByFNFiqxWQquwH#scrollTo=hytEb0aXfcwm)
> 
> 
> 


---

> ## Valentin Werner
> 
> 
> It was interesting to see the models that were subpar tie-ing with each other.
> 
> It is important to acknowledge that the prompt matters a lot. When presenting prompting in my company, I show a tool a bit like the lmsys arena to raise understanding for when you need the "big guns". On a simple questions, llama2-7B can easily tie gpt4-turbo, such as "what is 2+2?" - you will not need that many parameters to answer this. Now, one model may say "4" and the other one says "Adding 2+2 results in 4." and you may prefer one of the answers. Oops, suddenly Llama2-7B "outperformed" GPT-4?
> 
> Further, we always expect models of the same category to tie more often - not sure if I fully understood your point.
> 
> How do I assess the strength of the model based on the count of its ties? It is easy to judge it based on its number of wins.
> 
> This is what LMSYS is doing on the website. For this competition, we are also predicting ties - so knowing that a model ties a lot should be as good as winning a lot.
> 
> 
> 


---



* --- discussion numver 201, the number of votes :-4 ---

# Is this competition leaked? Very interesting, there is a huge gap between the first and second place.

**Turbo** *Sun Aug 04 2024 12:56:23 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-5 votes)





---

 # Comments from other users

> ## SeshuRaju ðŸ§˜â€â™‚ï¸
> 
> [@zhoumichael](https://www.kaggle.com/zhoumichael) its all happen in last 24 hours, we need to wait until competition finish.
> 
> 
> 


---



* --- discussion numver 202, the number of votes :-6 ---

# What do you think qwen2:0.5b is doing here?

**Matthew Hendricks** *Thu Jul 11 2024 13:54:14 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-6 votes)

```
import ollama
from datasets import load_dataset

dataset = load_dataset("lmsys/lmsys-arena-human-preference-55k", split='train')
print(dataset[0])
print("---")

"""
Dataset({
    features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],
    num_rows: 57477
})
"""

response = ollama.chat(model='qwen2:0.5b', messages=[
  {
    'role': 'user',
    'content': f'{dataset[0]}',
  },
])
print(response['message']['content'])

```

Look carefully at this and tell me what you think qwen2:0.5b is doing

## prompt='content': f'Dataset[0]'

```
{'id': 30192, 'model_a': 'gpt-4-1106-preview', 'model_b': 'gpt-4-0613', 'prompt': '["Is it morally right to try to have a certain percentage of females on managerial positions?","OK, does pineapple belong on a pizza? Relax and give me fun answer."]', 'response_a': '["The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\"tokens,\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\"color-blind\\" or \\"gender-blind\\" approach to hiring and promotions.\\n\\nOrganizations and societies must carefully consider the implications of these strategies and strive to implement policies that promote fairness and equality while minimizing negative consequences. It is also important to address the root causes of gender inequality, such as unconscious bias, work-life balance challenges, and lack of supportive policies, to create an environment where everyone has the opportunity to succeed based on their capabilities and contributions.","Ah, the age-old culinary conundrum that has divided nations and dinner tables: does pineapple belong on a pizza? The tropical twist of pineapple on pizza, known as Hawaiian pizza, is a hotly debated topic where taste buds battle and pizza purists protest.\\n\\nLet\'s slice into the debate with a zest of fun:\\n\\n**Team Pineapple:** \\"Absolutely, yes! Pineapple on pizza is like a beach party in your mouth. The sweet juiciness of pineapple chunks frolicking with savory ham or bacon creates a flavor wave that surfs across the cheesy ocean of deliciousness. It\'s the Mardi Gras of pizzas, where sweet meets savory in a jubilant jamboree!\\"\\n\\n**Team No-Pineapple:** \\"No way, not in a million pizzas! Pineapple is a fruit that should be sunbathing on a tropical fruit platter, not freeloading on a sacred slice of pizza. The thought of warm, melty cheese conspiring with pineapple\'s sugary mischief is enough to make Italian ancestors turn in their tomato patches. Keep the pineapple in the fruit salad where it belongs!\\"\\n\\nAt the end of the day, whether pineapple belongs on pizza is a matter of personal taste. Some say it\'s a match made in heaven, others say it\'s a culinary crime. But let\'s face it, in the world of pizza toppings, where you can find everything from anchovies to zucchini, there\'s room for a little pineapple pizzazz. So, if your taste buds crave that sweet and tangy twist, go ahead and let your pineapple flag fly atop that glorious cheese-covered dough. Bon app\\u00e9tit, or as they say in pineapple paradise, \\"Aloha-ppetite!\\" \\ud83c\\udf4d\\ud83c\\udf55"]', 'response_b': '["As an AI, I don\'t have personal beliefs or opinions. However, I can tell you that the question of gender quotas in managerial positions is a complex one and positions can vary. Advocates argue that such measures can help correct historical imbalances, promote diversity, and may lead to better decision-making. Critics may argue that positions should be based purely on merit, and that quotas could potentially lead to tokenism or unqualified candidates being promoted. Morality can be subjective and differs from person to person based on their personal beliefs, cultural background, and values. It\'s a topic with valid arguments on both sides, and it\'s up to each individual or organization to decide what they believe is right.","As an AI, I don\'t eat, so I don\'t have personal preferences. But, in the world of pizza, the pineapple topping is a hotly debated topic! Some people love the sweet and savory mix it brings to a pizza, a flavor combination made famous by the Hawaiian pizza. Others firmly believe that the sweetness of the fruit has no place on a pizza. It\'s a pizza puzzle that might never be solved. So, whether pineapple belongs on a pizza or not, it truly depends on your taste buds!"]', 'winner_model_a': 1, 'winner_model_b': 0, 'winner_tie': 0}

```

## qwen2:0.5b response

```
{'id': 30240, 'model_a': 'gpt-4-1106-preview', 'model_b': 'gpt-4-0613', 'prompt': "What is the moral right for trying to have a certain percentage of females on managerial positions?", 'response_a': '["The question of whether it is morally right to aim for a certain percentage of females in managerial positions is a complex ethical issue that involves considerations of fairness, equality, diversity, and discrimination.\\n\\nHere are some arguments in favor of and against such policies:\\n\\n**Arguments in favor:**\\n\\n1. **Correcting Historical Inequities:** Women have historically been underrepresented in leadership roles due to various cultural, institutional, and social barriers. Aiming for a specific percentage can be seen as a corrective measure to address past and ongoing discrimination.\\n\\n2. **Promoting Diversity:** Diverse leadership teams can enhance decision-making and represent a broader range of perspectives. This can lead to better outcomes for organizations and society as a whole.\\n\\n3. **Equality of Opportunity:** Setting targets for female representation in management can help ensure that women have equal opportunities to advance in their careers.\\n\\n4. **Role Modeling:** Increased visibility of female leaders can inspire and encourage other women and girls to pursue leadership roles.\\n\\n**Arguments against:**\\n\\n1. **Reverse Discrimination:** Setting a quota for female representation might lead to the perception or reality of reverse discrimination, where men might be overlooked for positions despite being qualified, simply to meet the gender target.\\n\\n2. **Meritocracy Concerns:** Critics argue that employment and promotion should be based solely on merit and qualifications, not gender. They fear that quotas could compromise the quality of leadership if less qualified individuals are promoted to meet gender targets.\\n\\n3. **Tokenism:** There is a risk that women appointed to meet quotas might be seen as \\"tokens,\\" undermining their legitimacy and potentially leading to resentment among colleagues.\\n\\n4. **Oversimplification of Diversity:** Focusing only on gender might overlook other important aspects of diversity, such as race, ethnicity, socioeconomic background, or sexual orientation.\\n\\nUltimately, the morality of aiming for a certain percentage of females in managerial positions depends on one\'s ethical framework and the context in which such policies are implemented. Proponents of gender diversity targets often argue that these measures are necessary as a transitional mechanism to create a level playing field, while opponents may argue for a \\"color-blind\\" or \\"gender-blind\\" approach to hiring and promotions."},

```



---

 # Comments from other users

> ## Anya
> 
> haha, why are they the same?
> 
> 
> 
> > ## Matthew HendricksTopic Author
> > 
> > It surprised me that qwen2:0.5b seems to excel at mimicking the json format of the prompt it was provided.  It seems more likely to mimic the json format of the prompt if the prompt is 'longer'.
> > 
> > ```
> > import ollama
> > from datasets import load_dataset
> > from tqdm import tqdm
> > import json
> > from collections import Counter
> > 
> > def process_dataset(num_samples=1000):
> >     dataset = load_dataset("lmsys/lmsys-arena-human-preference-55k", split='train')
> >     results = []
> > 
> >     for i in tqdm(range(min(num_samples, len(dataset)))):
> >         sample = dataset[i]
> >         response = ollama.chat(model='qwen2:0.5b', messages=[
> >             {
> >                 'role': 'user',
> >                 'content': f'{sample}',
> >             },
> >         ])
> >         results.append({
> >             'input': sample,
> >             'output': response['message']['content']
> >         })
> > 
> >     return results
> > 
> > def analyze_results(results):
> >     truncation_count = sum(1 for r in results if len(r['output']) < len(str(r['input'])))
> >     json_preservation = sum(1 for r in results if r['output'].startswith('{') and r['output'].endswith('}'))
> > 
> >     field_preservation = Counter()
> >     for r in results:
> >         try:
> >             output_json = json.loads(r['output'])
> >             for key in r['input'].keys():
> >                 if key in output_json:
> >                     field_preservation[key] += 1
> >         except json.JSONDecodeError:
> >             pass
> > 
> >     return {
> >         'total_samples': len(results),
> >         'truncation_rate': truncation_count / len(results),
> >         'json_preservation_rate': json_preservation / len(results),
> >         'field_preservation': {k: v / len(results) for k, v in field_preservation.items()}
> >     }
> > 
> > if __name__ == "__main__":
> >     results = process_dataset(num_samples=10)  # Adjust the number of samples as needed
> >     analysis = analyze_results(results)
> > 
> >     with open("results.jsonl", "w") as f:
> >         for result in results:
> >             json.dump(result, f)
> >             f.write("\n")
> > 
> >     print(json.dumps(analysis, indent=2))
> > 
> > ```
> > 
> > 
> > 


---

> ## Valentin Werner
> 
> I don't know.. what is it doing?
> 
> 
> 
> > ## Matthew HendricksTopic Author
> > 
> > It's distilling and reflecting something.
> > 
> > In my opinion, it is rephrasing the question to be more direct, peeling back the layers of politically correct, amoral obfuscation.  It's decoding the core beliefs of the user based on how the question is phrased.
> > 
> > Dataset[0][ 'prompt']: "Is it morally right to try to have a certain percentage of females on managerial positions?"
> > 
> > ```
> > Qwen2:0.5b response: 
> > 'prompt': "What is the moral right for trying to have a certain percentage of females on managerial positions?",
> > 
> > ```
> > 
> > 
> > 


---



* --- discussion numver 203, the number of votes :-10 ---

# The hidden test dataset has distribution shift 

**Xin** *Sat Aug 03 2024 02:30:10 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (-11 votes)



After extracting some features from training dataset, then a full dataset one epoch train, I thought the single Gemma model can have a good leaderboard result, but the reality is eval:0.867 leaderboard: 0.933.

I think it might mean the data distribution is to some extent different than the train dataset, then after I extracting features from test dataset, the score then be low.



---

 # Comments from other users

> ## CPMP
> 
> What you see is that your model performs better on its training data than on new data. This is to be expected.
> 
> One way to not be surprised  is to split your triaining data into two piece. One piece you use for training, and one piece you use for evaluating your model once trained. The second piece is often called a validation dataset, or a test dataset.
> 
> 
> 


---

> ## Valentin Werner
> 
> You are facing a lot of backlash for a "beginner mistake" - even if only training for one epoch, you want to validate your model on unseen data, your model has theen the "validation" data already once, so it knows it. This is one form of data leakage.
> 
> Just from this training it is impossible to expect how well your model is going to perform on a leaderboard submission. Often, it is better to set aside 10-20% of data to make sure you have a local validation and a leaderboard score rather then fitting your model on all the data.
> 
> It is possible to probe the data distribution of the LB dataset, however, not with this approach. 
> 
> 
> 
> > ## XinTopic Author
> > 
> > Yeah. I understand. One epoch also probably brings model to an underfit status. From the result, I intuitively think that either the distribution between train and hidden test dataset different or there is a high similarity on train dataset (I mean tokenized data, maybe also because I only extract the first part from texts) which causes such an obvious overfit. 
> > 
> > 
> > 


---

> ## JM
> 
> Your eval_dataset is taking a subset of your training datasetâ€¦
> 
> 
> 
> > ## XinTopic Author
> > 
> > Yeah. But I think one epoch, the model only see the data once.
> > 
> > 
> > 
> > > ## David.Ricardo.H.X
> > > 
> > > You didnt get what he meansâ€¦â€¦ 
> > > 
> > > Your eval_dataset is taking a subset of your training datasetâ€¦ 
> > > 
> > > Do you have a concept of what is the difference between train set, validation set and test?
> > > 
> > > 
> > > 


---





# All Jupyter Notebooks

** @@@ Jupyter Notebook numver 0, the number of votes :211 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 1, the number of votes :165 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is
This notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).
I used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.

## Result

I used `id % 5 == 0` as an evaluation set and used all the rest for training.

| subset | log loss |
| - | - |
| eval | 0.9371|
| LB | 0.941 |

## What is QLoRA fine-tuning?

In the conventional fine-tuning, weight ($\mathbf{W}$) is updated as follows:

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

where $L$ is a loss at this step and $\eta$ is a learning rate.

[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$ by factorizing $\Delta \mathbf{W}$ into two (much) smaller matrices, $\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$ and $\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$ with $r \ll \text{min}(\text{d}, \text{k})$.

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

During training, only $\mathbf{A}$ and $\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.

[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. 
Note that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.

1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.

## Note
It takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.
This notebook uses only 100 samples for demo purpose, but everything else is same as my setup.
```

---The following area is a Code cell (cell numver is 1)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 2  # global batch size is 8 
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 6)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 8)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 10)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 11)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 12)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 14)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 15)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 16)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 18)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 20)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 21)---
```python
train_idx, eval_idx = folds[config.fold_idx]

trainer = Trainer(
    args=training_args, 
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)
trainer.train()
```

** @@@ Jupyter Notebook numver 2, the number of votes :116 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.

## ðŸ†• Update in version 4
The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.
In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.
This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.
We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:
1. Flash Attention
2. Memory Efficient Attention
3. A PyTorch (naive) implementation

By default, all of those are enabled but we can also manually enable/disable certain backends.
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 3, the number of votes :97 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Submit LLM 34B Model in 5 hours!
This notebook demonstrates how to submit a LLM 34B model in only 5 hours! Amazing! The key tricks are:
* use vLLM (for speed)
* use AWQ 4bit quantization (to avoid GPU VRAM OOM)
* limit input size to 1024 tokens (for speed)
* limit output size to 1 token (for speed)
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Pip Install vLLM
The package vLLM is an incredibly fast LLM inference library! The vLLM that is installed in Kaggle notebooks will produce errors, therefore we need to reinstall vLLM. The code below was taken from notebook [here][1]

[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution
```

---The following area is a Code cell (cell numver is 2)---
```python
import os, math, numpy as np
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"
```

---The following area is a Code cell (cell numver is 3)---
```python
%%time
!pip uninstall -y torch
!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Load 34B Quantized Model with vLLM!
We will load and use LLM 34B Bagel [here][1]. This is a strong model.

[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2
```

---The following area is a Code cell (cell numver is 5)---
```python
import vllm

llm = vllm.LLM(
    "/kaggle/input/bagel-v3-343",
    quantization="awq",
    tensor_parallel_size=2, 
    gpu_memory_utilization=0.95, 
    trust_remote_code=True,
    dtype="half", 
    enforce_eager=True,
    max_model_len=1024,
    #distributed_executor_backend="ray",
)
tokenizer = llm.get_tokenizer()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load Test Data
During **commit** we load 128 rows of train to compute CV score. During **submit**, we load the test data.
```

---The following area is a Code cell (cell numver is 7)---
```python
import pandas as pd
VALIDATE = 128

test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv") 
if len(test)==3:
    test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
    test = test.iloc[:VALIDATE]
print( test.shape )
test.head(1)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Engineer Prompt
If we want to submit zero shot LLM, we need to experiment with different system prompts to improve CV score. If we finetune the model, then system is not as important because the model will learn from the targets what to do regardless of which system prompt we use.

We use a logits processor to force the model to output the 3 tokens we are interested in.
```

---The following area is a Code cell (cell numver is 9)---
```python
from typing import Any, Dict, List
from transformers import LogitsProcessor
import torch

choices = ["A","B","tie"]

KEEP = []
for x in choices:
    c = tokenizer.encode(x,add_special_tokens=False)[0]
    KEEP.append(c)
print(f"Force predictions to be tokens {KEEP} which are {choices}.")

class DigitLogitsProcessor(LogitsProcessor):
    def __init__(self, tokenizer):
        self.allowed_ids = KEEP
        
    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:
        scores[self.allowed_ids] += 100
        return scores
```

---The following area is a Code cell (cell numver is 10)---
```python
sys_prompt = """Please read the following prompt and two responses. Determine which response is better.
If the responses are relatively the same, respond with 'tie'. Otherwise respond with 'A' or 'B' to indicate which is better."""
```

---The following area is a Code cell (cell numver is 11)---
```python
SS = "#"*25 + "\n"
```

---The following area is a Code cell (cell numver is 12)---
```python
all_prompts = []
for index,row in test.iterrows():
    
    a = " ".join(eval(row.prompt, {"null": ""}))
    b = " ".join(eval(row.response_a, {"null": ""}))
    c = " ".join(eval(row.response_b, {"null": ""}))
    
    prompt = f"{SS}PROMPT: "+a+f"\n\n{SS}RESPONSE A: "+b+f"\n\n{SS}RESPONSE B: "+c+"\n\n"
    
    formatted_sample = sys_prompt + "\n\n" + prompt
    
    all_prompts.append( formatted_sample )
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Infer Test
We infer test using fast vLLM. We ask vLLM to output probabilties of the top 5 tokens considered to be predicted in the first token. We also limit prediction to 1 token to increase inference speed.

Based on the speed it takes to infer 128 train samples, we can deduce how long inferring 25,000 test samples will take.
```

---The following area is a Code cell (cell numver is 14)---
```python
%%time

from time import time
start = time()

logits_processors = [DigitLogitsProcessor(tokenizer)]
responses = llm.generate(
    all_prompts,
    vllm.SamplingParams(
        n=1,  # Number of output sequences to return for each prompt.
        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.
        temperature=0,  # randomness of the sampling
        seed=777, # Seed for reprodicibility
        skip_special_tokens=True,  # Whether to skip special tokens in the output.
        max_tokens=1,  # Maximum number of tokens to generate per output sequence.
        logits_processors=logits_processors,
        logprobs = 5
    ),
    use_tqdm = True
)

end = time()
elapsed = (end-start)/60. #minutes
print(f"Inference of {VALIDATE} samples took {elapsed} minutes!")
```

---The following area is a Code cell (cell numver is 15)---
```python
submit = 25_000 / 128 * elapsed / 60
print(f"Submit will take {submit} hours")
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# Extract Inference Probabilites
We now extract the probabilties of "A", "B", "tie" from the vLLM predictions.
```

---The following area is a Code cell (cell numver is 17)---
```python
results = []
errors = 0

for i,response in enumerate(responses):
    try:
        x = response.outputs[0].logprobs[0]
        logprobs = []
        for k in KEEP:
            if k in x:
                logprobs.append( math.exp(x[k].logprob) )
            else:
                logprobs.append( 0 )
                print(f"bad logits {i}")
        logprobs = np.array( logprobs )
        logprobs /= logprobs.sum()
        results.append( logprobs )
    except:
        #print(f"error {i}")
        results.append( np.array([1/3., 1/3., 1/3.]) )
        errors += 1
        
print(f"There were {errors} inference errors out of {i+1} inferences")
results = np.vstack(results)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Create Submission CSV
```

---The following area is a Code cell (cell numver is 19)---
```python
sub = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")

if len(test)!=VALIDATE:
    sub[["winner_model_a","winner_model_b","winner_tie"]] = results
    
sub.to_csv("submission.csv",index=False)
sub.head()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Compute CV Score
```

---The following area is a Code cell (cell numver is 21)---
```python
if len(test)==VALIDATE:
    true = test[['winner_model_a','winner_model_b','winner_tie']].values
    print(true.shape)
```

---The following area is a Code cell (cell numver is 22)---
```python
if len(test)==VALIDATE:
    from sklearn.metrics import log_loss
    print(f"CV loglosss is {log_loss(true,results)}" )
```

** @@@ Jupyter Notebook numver 4, the number of votes :60 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Note
- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

# Import
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
import torch
import sklearn
import numpy as np
import pandas as pd
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
```

---The following area is a Code cell (cell numver is 3)---
```python
MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'
WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'
MAX_LENGTH = 2400
BATCH_SIZE = 2
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
def tokenize(example, tokenizer):
    prompts = tokenizer(eval(example['prompt'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_a = tokenizer(eval(example['response_a'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_b = tokenizer(eval(example['response_b'], {"null": ""}), add_special_tokens=False)["input_ids"]
    assert len(prompts) == len(responses_a) == len(responses_b), "Lengths of prompts, responses_a, and responses_b do not match"
    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]
    prompt, response_a, response_b = [], [], []
    p_len, a_len, b_len = 0, 0, 0
    for p, a, b in zip(prompts, responses_a, responses_b):
        prompt.append(p)
        response_a.append(a)
        response_b.append(b)
        p_len += len(p)
        a_len += len(a)
        a_len += len(b)
        if p_len+a_len+b_len > MAX_LENGTH:
            break
    prompt = [item for sublist in reversed(prompt) for item in sublist]
    response_a = [item for sublist in reversed(response_a) for item in sublist]
    response_b = [item for sublist in reversed(response_b) for item in sublist]
    p_a_b_len = len(prompt) + len(response_a) + len(response_b)
    cut_len = p_a_b_len - MAX_LENGTH
    if cut_len>0:
        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]
        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]
        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]
    prompt = tokenizer('<prompt>: ', add_special_tokens=False)["input_ids"] + prompt
    response_a = tokenizer('\n\n<response_a>: ', add_special_tokens=False)["input_ids"] + response_a
    response_b = tokenizer('\n\n<response_b>: ', add_special_tokens=False)["input_ids"] + response_b
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]
    label_token_id = [128250]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 7)---
```python
%%time
tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        # remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer},
    )
    return tokenized_datasets
test_ds = load_data(test, tokenizer)
test_ds
```

---The following area is a Code cell (cell numver is 8)---
```python
data = test_ds.to_pandas()
data["max_len"] = data["input_ids"].apply(len)
data[:3]
```

---The following area is a Code cell (cell numver is 9)---
```python
data['input_ids'][0]
```

---The following area is a Code cell (cell numver is 10)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 12)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            fake_label_tokens_ids = torch.tensor([128250],device=shift_labels.device)
            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
#             index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
#             true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
#             true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:,label_tokens_ids]
#             loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 13)---
```python
# Load base model on GPU 0
device0 = torch.device('cuda:0')
base_model_0 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:0',
)
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:1',
)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 15)---
```python
# Get peft
model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0) 
model_0.eval()

model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)
model_1.eval()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []

    model.eval()
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        labels = tmp["labels"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        pad_labels=[]
        for label in labels:
            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])
            pad_labels.append(label)
        labels = torch.tensor(pad_labels).to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    df['winner_model_a'] = a_win
    df['winner_model_b'] = b_win
    df['winner_tie'] = tie
    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

data = data.sort_values("max_len", ascending=False)
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 19)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 5, the number of votes :58 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 7)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 9)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 10)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)

model = Llama3ForSFT.from_pretrained(
    model_path, 
    torch_dtype=torch.float16, 
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 12)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 14)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

** @@@ Jupyter Notebook numver 6, the number of votes :48 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
1 epoch takes ~3hrs 

Just save lora weights and load them in the following notebook for Inference on 2xT4s:

Note: Training on bfloat16 causes model to overfit, so training should be on float32 and inference on float16


https://www.kaggle.com/code/pranshubahadur/inference-tf-gemma-2-9b-lmsys

Inspiration:

https://www.kaggle.com/code/matthewdwatson/gemma-2-fine-tuning-and-inference

https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=_Peq7TnLtHse
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U keras-nlp tensorflow-text
# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.
!pip install -q -U tensorflow-cpu
```

---The following area is a Code cell (cell numver is 2)---
```python
import jax

jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
```

---The following area is a Code cell (cell numver is 4)---
```python
import keras
import keras_nlp
```

---The following area is a Code cell (cell numver is 5)---
```python
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (1, 8),
    ["batch", "model"],
    devices=keras.distribution.list_devices(),
)
```

---The following area is a Code cell (cell numver is 6)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
```

---The following area is a Code cell (cell numver is 7)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 8)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0])

raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)


train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1).apply(lambda x: '\n\nPROMPT\n\n' + x).apply(lambda x: remove_surrogates(x)),
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
})
```

---The following area is a Code cell (cell numver is 9)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)

```

---The following area is a Code cell (cell numver is 10)---
```python
#keras.config.set_floatx("bfloat16")

```

---The following area is a Code cell (cell numver is 11)---
```python
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1")
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 12)---
```python
gemma_lm.backbone.enable_lora(rank=16)

```

---The following area is a Code cell (cell numver is 13)---
```python
for layer in gemma_lm._backbone.layers[:16]:
    layer.trainable = False
```

---The following area is a Code cell (cell numver is 14)---
```python
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 15)---
```python
def preprocess_fn(text, label=None):
    preprocessed = gemma_lm._preprocessor(text, sequence_length=1024)[0]
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}, label if label is not None else text
```

---The following area is a Code cell (cell numver is 16)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

inputs = {
    "token_ids": keras.Input(shape=(1024,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(1024,), dtype=tf.int32, name="padding_mask"),
}
x = gemma_lm.backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 17)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,
                weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 18)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 19)---
```python
import tensorflow as tf
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values)).batch(4).map(preprocess_fn)
ds = ds.shuffle(ds.cardinality())

```

---The following area is a Code cell (cell numver is 20)---
```python
train_split = ds.take(int(len(ds)*0.9))
val_split = ds.skip(int(len(ds)*0.9)).take(int(len(ds)*0.1))
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=4)
```

---The following area is a Code cell (cell numver is 21)---
```python
import numpy as np
layer = model.get_layer(name='dense')
weights = layer.get_weights()
kernel, bias = weights

# Save the kernel and bias separately
np.save('dense_1_kernel.npy', kernel)
np.save('dense_1_bias.npy', bias)
model.layers[2].save_lora_weights("model.lora.h5")
```

** @@@ Jupyter Notebook numver 7, the number of votes :33 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS ZeroShot Prediction

This code uses LLama3 to make zero-shot predictions.

Instead of learning a classification header, it uses a carefully crafted prompt to predict which token among **A**, **B**, or **tie** is most likely to follow `###Answer:`.
```

---The following area is a Code cell (cell numver is 1)---
```python
import json
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import pickle
import random
import os
import sys

import transformers
from transformers import AdamW
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_cosine_schedule_with_warmup

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

import sklearn.metrics
from sklearn.metrics import accuracy_score

os.environ["TOKENIZERS_PARALLELISM"] = "true"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
```

---The following area is a Code cell (cell numver is 2)---
```python
class CFG:
    INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
    OUTPUT = "/kaggle/working"
    MODEL_ID = "/kaggle/input/llama-3/transformers/8b-hf/1"
    SEED = 42
    USE_TURN = 0
```

---The following area is a Code cell (cell numver is 3)---
```python
train_df = pd.read_csv(f"{CFG.INPUT_DIR}/train.csv")
train_df
```

---The following area is a Code cell (cell numver is 4)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    CFG.MODEL_ID,
    torch_dtype=torch.float16,
    device_map='auto',
)
```

---The following area is a Code cell (cell numver is 5)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    CFG.MODEL_ID,
    use_fast=False,
    trust_remote_code=True,
    padding_side="left",
    pad_token="<|endoftext|>"
)
model.config.pad_token_id = model.config.eos_token_id
```

---The following area is a Code cell (cell numver is 6)---
```python
n_valid = len(train_df) // 5
sample_df = train_df.sample(n_valid, random_state=CFG.SEED)
sample_df.head()
```

---The following area is a Code cell (cell numver is 7)---
```python
results = []
for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):
    prompt = json.loads(row["prompt"])
    response_a = json.loads(row["response_a"])
    response_b = json.loads(row["response_b"])
    
    
    p = prompt[CFG.USE_TURN]
    a = response_a[CFG.USE_TURN]
    b = response_b[CFG.USE_TURN]
    
    if a is None or b is None or len(p.split()) < 3:
        continue
        
    p = " ... ".join(["none" if i is None else i for i in prompt])
    a = " ... ".join(["none" if i is None else i for i in response_a])
    b = " ... ".join(["none" if i is None else i for i in response_b])
    # Use head and tail
    p = p[:128] + " ... " + p[-128:]
    a = a[:256] + " ... " + a[-256:]
    b = b[:256] + " ... " + b[-256:]
    
    text = f"""### Instruction
Which model's answer is appropriate for the prompt?ã€€If both are appropriate, answer `tie`.

### Prompt
{p}

### A
{a}

### B
{b}

### Answer
"""

    toks = tokenizer(text)

    for k in toks.keys():
        toks[k] = torch.tensor(toks[k]).cuda()  

    with torch.no_grad():
        out = model(toks["input_ids"].unsqueeze(0))

    pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
    pred = out.logits[0, -1, pred_token_id].cpu().softmax(0).numpy()
    
    d = row.to_dict()
    d["predict"] = pred
    results.append(d)
```

---The following area is a Code cell (cell numver is 8)---
```python
results_df = pd.DataFrame(results)
results_df
```

---The following area is a Code cell (cell numver is 9)---
```python
results_df.to_csv("result.csv", index=None)
```

---The following area is a Code cell (cell numver is 10)---
```python
!ls
```

---The following area is a Code cell (cell numver is 11)---
```python
targets = results_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

predicts = np.array(results_df["predict"].tolist())
```

---The following area is a Code cell (cell numver is 12)---
```python
logloss = sklearn.metrics.log_loss(targets, predicts)
print(logloss)
```

---The following area is a Code cell (cell numver is 13)---
```python
!ls
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Next steps:

- Try a larger model (with better benchmark accuracy)
- Refine the prompt.
- SFT (Supervised Fine-Tuning) on competition data
- Use ensemble
```

** @@@ Jupyter Notebook numver 8, the number of votes :29 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## If it helps you, I hope you can give me a thumbs up

## Importing Libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
from threading import Thread
import gc
import os
import io
import json
import random
import pickle
import zipfile
import datetime
import time

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from IPython.display import display
import torch.nn.functional as F
import tokenizers
```

---The following area is a Code cell (cell numver is 3)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 2048
BATCH_SIZE = 4
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 6)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 11)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: 
    print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 32)---
```python
TRAIN = False
INFER = True 
SAVE = False
```

---The following area is a Code cell (cell numver is 33)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 34)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 35)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 36)---
```python
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat), 
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
```

---The following area is a Code cell (cell numver is 37)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.05 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_wt = 0.05
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 42)---
```python
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 43)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 9, the number of votes :25 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
import os
```

---The following area is a Code cell (cell numver is 4)---
```python
from threading import Thread
import gc
import os
import io
import json
import random
import pickle
import zipfile
import datetime
import time

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from IPython.display import display
import torch.nn.functional as F
import tokenizers
```

---The following area is a Code cell (cell numver is 5)---
```python
assert torch.cuda.device_count() == 2

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# infer
```

---The following area is a Code cell (cell numver is 7)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Code cell (cell numver is 13)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Code cell (cell numver is 14)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 15)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 16)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 17)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Importing Libraries
```

---The following area is a Code cell (cell numver is 19)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Tokenize

## Load model 
> We load 1 model on each gpu.  

## Inference
```

---The following area is a Code cell (cell numver is 21)---
```python
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = result_df[TARGETS]
```

---The following area is a Code cell (cell numver is 22)---
```python
llama_preds = result_df[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 24)---
```python
TAG = 'lmsys-chatbot-arena'
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: 
    print('kaggle')
```

---The following area is a Code cell (cell numver is 25)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 26)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 27)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 28)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 29)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 30)---
```python
params = {}
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 31)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 32)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 33)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 34)---
```python
TRAIN = False
INFER = True 
SAVE = False
```

---The following area is a Code cell (cell numver is 35)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 36)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 37)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat), 
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 40)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.12 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 43)---
```python
lgb_wt = 0.6
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 44)---
```python
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 45)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 10, the number of votes :22 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import tensorflow as tf
from datasets import load_dataset, DatasetDict
from transformers import BertTokenizer, BertTokenizerFast, TFBertModel, DataCollatorWithPadding, TFAutoModel
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
import shutil
import pandas as pd
from tqdm.keras import TqdmCallback
import re
import math
import matplotlib.pyplot as plt
import multiprocessing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tensorflow.keras.callbacks import LearningRateScheduler
import numpy as np
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
```

---The following area is a Code cell (cell numver is 1)---
```python
# Check for GPU availability
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        print(e)
```

---The following area is a Code cell (cell numver is 2)---
```python
# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.MirroredStrategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)
```

---The following area is a Code cell (cell numver is 3)---
```python
# path for sets
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'

# loading datasets
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# saving ID
test_ids = test_dataset['id']
```

---The following area is a Code cell (cell numver is 4)---
```python
# adding missing columns in the test set
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    if col not in test_dataset.column_names:
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# transformation to int64
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 5)---
```python
# using bert-base-cased's files locally
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'

model_dir = '/kaggle/working/bert-base-cased'
os.makedirs(model_dir, exist_ok=True)

shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)
```

---The following area is a Code cell (cell numver is 6)---
```python
stopwords_path = '/kaggle/input/stopwords/stopwords/english'

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð² Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð°
def load_stopwords(stopwords_path):
    with open(stopwords_path, 'r') as file:
        stopwords = file.read().splitlines()
    return set(stopwords)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Initialize the tokenizer
tokenizer = BertTokenizerFast.from_pretrained(model_dir)
# download stopwords
stopwords = load_stopwords(stopwords_path)
# Function for text cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#','', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸ÑŽ
    text = ' '.join([word for word in text.split() if word not in stopwords])  # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°
    return text

```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_function(examples):
    # Clean each text field
    cleaned_prompts = [clean_text(text) for text in examples['prompt']]
    cleaned_responses_a = [clean_text(text) for text in examples['response_a']]
    cleaned_responses_b = [clean_text(text) for text in examples['response_b']]
    
    # Tokenize the cleaned texts
    return tokenizer(cleaned_prompts,
                     cleaned_responses_a,
                     cleaned_responses_b,
                     padding="max_length", 
                     truncation=True, 
                     max_length=512)
```

---The following area is a Code cell (cell numver is 9)---
```python
# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
examples = {
    'prompt': ["This is a sample prompt."],
    'response_a': ["This is a sample response A."],
    'response_b': ["This is a sample response B."]
}

tokenized_output = tokenize_function(examples)
print(tokenized_output)
```

---The following area is a Code cell (cell numver is 10)---
```python
# apply the tokenization and cleaning function with multiprocessing num_proc=num_proc
num_proc = multiprocessing.cpu_count()

# add try-except block for better error handling
try:
    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
    test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)
except Exception as e:
    print(f"Error during tokenization: {e}")
    
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 11)---
```python
# Add debug prints after tokenization
print("Sample tokenized train dataset entry:")
print(tokenized_datasets[0])
if len(tokenized_datasets) == 0:
    raise ValueError("The tokenized training dataset is empty.")
if len(test_tokenized_datasets) == 0:
    raise ValueError("The tokenized test dataset is empty.")
# Print column names for debugging
print(f"Tokenized training dataset columns: {tokenized_datasets.column_names}")
print(f"Tokenized test dataset columns: {test_tokenized_datasets.column_names}")
```

---The following area is a Code cell (cell numver is 12)---
```python
# convert to tf.data.Dataset with the correct shape
def convert_to_tf_dataset(dataset, label_col=None, for_inference=False):
    input_columns = tokenizer.model_input_names
    
    if label_col and not for_inference:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in input_columns])
    else:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in input_columns])
    
    # ensure labels are not sequences
    if label_col:
        dataset = dataset.map(lambda x: {label_col: int(x[label_col])})
    
    shuffle = not for_inference
    batch_size = 16 if for_inference else 450

    tf_dataset = dataset.to_tf_dataset(
        columns=input_columns,
        label_cols=[label_col] if label_col and not for_inference else None,
        shuffle=shuffle,
        batch_size=batch_size,
        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)
    )

    return tf_dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
# run the conversion
try:
    train_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
    test_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
except Exception as e:
    print(f"Error during dataset conversion: {e}")
```

---The following area is a Code cell (cell numver is 14)---
```python
# add debug prints after dataset conversion
print("Sample from converted train tf.data.Dataset:")
for batch in train_tf_dataset.take(1):
    inputs, labels = batch
    print(f'Input IDs shape: {inputs["input_ids"].shape}')
    print(f'Attention mask shape: {inputs["attention_mask"].shape}')
    print(f'Labels shape: {labels.shape}')

```

---The following area is a Code cell (cell numver is 15)---
```python
# building a custom model
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)
    
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state

def create_keras_model():
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    bert_output = BertLayer()([input_ids, attention_mask])
    pooled_output = GlobalAveragePooling1D()(bert_output)
    output = Dense(3, activation='softmax')(pooled_output)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model
```

---The following area is a Code cell (cell numver is 16)---
```python
with strategy.scope():
    model = create_keras_model()
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    model.fit(train_tf_dataset, epochs=3, callbacks=[TqdmCallback(verbose=1)])
```

---The following area is a Code cell (cell numver is 17)---
```python
# getting prediction
predictions = model.predict(test_tf_dataset)

# check lengths
print(f"Length of test_ids: {len(test_ids)}")
print(f"Shape of predictions: {predictions.shape}")
if len(test_ids) != predictions.shape[0]:
    predictions = predictions[:len(test_ids)]

# creating DataFrame
submission = pd.DataFrame({
    'id': test_ids,
    'winner_model_a': predictions[:, 0],
    'winner_model_b': predictions[:, 1],
    'winner_model_tie': predictions[:, 2]
})

# saving DataFrame
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 11, the number of votes :21 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Gemma 2 - 9b
We train a simple classifier in this using computed embeddings as input from [here](https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1?scriptVersionId=186083288) and compute embeddings for test and use trained classifier for inference. Let's get started!

Upvote if you found this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import Libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss

from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Train Classifier
```

---The following area is a Code cell (cell numver is 5)---
```python
train_df = pd.read_csv('/kaggle/input/gemma-2-9b-part-1/train_embed.csv')
train_embed = np.load('/kaggle/input/gemma-2-9b-part-1/gemma2_train_embed.npy')

train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 6)---
```python
# splits
Targets = ['winner_model_a','winner_model_b','winner_tie']

y = train_df['label'].values
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Here we just use classifier with default settings, try out tuning the params) 

model_cb = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    loss_function='MultiClass',
    eval_metric='MultiClass',
    early_stopping_rounds=10,
    task_type='GPU',
    devices='0:1',
    verbose=100)

model_cb.fit(X_train, y_train, 
          eval_set=(X_test, y_test),
          early_stopping_rounds=50)
```

---The following area is a Code cell (cell numver is 8)---
```python
y_pred_proba = model_cb.predict_proba(X_test)
y_pred = model_cb.predict(X_test)

# Evaluate the model
logloss = log_loss(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)
gc.collect()

print(f'Log Loss: {logloss:.3f}')
print(f'Accuracy: {accuracy:.3f}')
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
We'll use this classifer for inference.
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# Load Gemma 2
```

---The following area is a Code cell (cell numver is 11)---
```python
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
MAX_LENGTH = 1024
BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)     
```

---The following area is a Code cell (cell numver is 12)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',
                   max_length=MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')


data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in  tokens['attention_mask']]
data[:2]
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Get embeddings
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 17)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 18)---
```python
test_embeddings = torch.cat([results[0], results[1]], dim=0)
test_embeddings.shape
```

---The following area is a Code cell (cell numver is 19)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 21)---
```python
preds = model_cb.predict_proba(test_embeddings.numpy())
preds
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# Submission
```

---The following area is a Code cell (cell numver is 23)---
```python
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
sample_sub[Targets] =  preds

display(sample_sub)
```

---The following area is a Code cell (cell numver is 24)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# Conclusion 

That's it! Just wanted to share the idea! Try out tuning the classifier or using other classifier. Thanks!

If you learned something, Please upvote:)
```

** @@@ Jupyter Notebook numver 12, the number of votes :19 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS | XGB Baseline

# 1. Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 2. Configuration
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 3. Loading Data
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 4. Feature Engineering
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))
            vectorizer.fit([text1, text2])
            output = vectorizer.transform([text1, text2]).toarray()
            cos_sim = cosine_similarity(output)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n') 
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"] 
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"] 
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"] 
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]
        
        return data
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time
preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 5. Modeling
```

---The following area is a Code cell (cell numver is 11)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))
cv_scores = list()

features = X.columns.tolist()
feat_imp_df = pd.DataFrame({"feature": features})

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed
    )
    
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    val_preds = model.predict_proba(X_val)
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")
    cv_scores.append(val_log_loss)
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances of All Folds", size=12)
plt.show()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 6. Saving Submission
```

---The following area is a Code cell (cell numver is 14)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 13, the number of votes :19 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import logging
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset
from sklearn.metrics import log_loss
import torch
from functools import partial
import warnings
from transformers import logging as transformers_logging
from transformers import EarlyStoppingCallback
import json
from pprint import pformat
from tqdm import trange
warnings.simplefilter('ignore')

TYPE = "large"
VER= 14
DATE = "0717"
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"

# Set up logging
transformers_logging.set_verbosity_error()
logging.basicConfig(level=logging.INFO, filename=f'logs_v{VER}.log', filemode='a',
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PATHS:
    train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
    test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
    sub_path = '/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'
    model_name = f"deberta-v3-{TYPE}"
    model_path = f"/root/autodl-tmp/ase2/huggingfacedebertav3variants/{model_name}"
    tokenizer_path = f"/kaggle/input/lmsys-{TYPE}{VER}-{DATE}/fold_0/tokenizer"
    general_tokenizer = "/kaggle/input/lmsys-base4-0704/fold_0/tokenizer"

class CFG:
    seed = 42
    max_length = 512
    lr = 5e-5  # å­¦ä¹ çŽ‡
    weight_decay = 0.01  # æƒé‡è¡°å‡
    warmup_ratio = 0 # å­¦ä¹ çŽ‡é¢„çƒ­æ¯”ä¾‹
    max_grad_norm = 1000  # æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°
    lr_scheduler_type = 'linear'  # å­¦ä¹ çŽ‡è°ƒåº¦ç±»åž‹
    frozen_embedding = False # å†»ç»“å‰é¢çš„å±‚
    frozen_num = 6
    train_batch_size = 32  # è®­ç»ƒæ‰¹é‡å¤§å°
    eval_batch_size = 64  # è¯„ä¼°æ‰¹é‡å¤§å°
    evaluation_strategy = 'steps'  # æ›´æ”¹ä¸º steps è¯„ä¼°ç­–ç•¥
    metric_for_best_model = "eval_log_loss"  # ç”¨äºŽé€‰æ‹©æœ€ä½³æ¨¡åž‹çš„åº¦é‡æ ‡å‡†
    save_strategy = 'steps'  # æ›´æ”¹ä¸º steps ä¿å­˜ç­–ç•¥
    save_steps = 200  # æ¯ æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹
    save_total_limit = 1  # ä¿å­˜æ£€æŸ¥ç‚¹æ€»æ•°é™åˆ¶
    train_epochs = 5  # è®­ç»ƒå‘¨æœŸæ•°
    num_labels = 6
    output_dir = f'/kaggle/input/lmsys-{TYPE}{VER}-{DATE}'  # è¾“å‡ºç›®å½•
    fp16 = True  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    load_best_model_at_end = True  # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡åž‹
    report_to = 'none'  # ä¸æŠ¥å‘Šè®­ç»ƒæ—¥å¿—åˆ°å¤–éƒ¨å·¥å…·
    optim = 'adamw_torch'  # ä¼˜åŒ–å™¨ç±»åž‹
    logging_first_step = True  # è®°å½•ç¬¬ä¸€æ­¥çš„æ—¥å¿—
    logging_steps = 200  # æ¯ æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    logging_dir =f'logs_v{VER}'  # æ—¥å¿—ä¿å­˜ç›®å½•
    n_splits = 5
    model_name = PATHS.model_name
    greater_is_better = False
    early_stop = False
    early_stopping_patience = 3  # Number of evaluation calls with no improvement after which training will be stopped
    early_stopping_threshold = 0.001  # Minimum change to qualify as an improvement

def seed_everything(seed):
    import random
    import os
    import numpy as np
    import torch
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=CFG.seed)

# tokenizer = AutoTokenizer.from_pretrained(PATHS.tokenizer_path)
tokenizer = AutoTokenizer.from_pretrained(PATHS.general_tokenizer)
sep_token = tokenizer.sep_token_id

def log_parameters(logger):
    """Log all parameters from PATHS and CFG classes."""
    logger.info("=== Parameter Settings ===")
    
    logger.info("PATHS:")
    for key, value in PATHS.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")
    
    logger.info("CFG:")
    for key, value in CFG.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")
    
    logger.info("=*100")

def tokenize_function(row, tokenizer):
    max_len = CFG.max_length - 2 # We need 2 separator tokens
    tokens_prompt = tokenizer(row['prompt'], truncation=True, max_length=max_len//4, add_special_tokens=False)['input_ids']
    remaining_length = max_len - len(tokens_prompt)
    
    tokens_response_a = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2, add_special_tokens=False)['input_ids']
    remaining_length -= len(tokens_response_a)
    tokens_response_b = tokenizer(row['response_b'], truncation=True, max_length=remaining_length, add_special_tokens=False)['input_ids']
    
    input_ids = [tokenizer.cls_token_id] + tokens_prompt + [sep_token] + tokens_response_a + [sep_token] + tokens_response_b
    token_type_ids = [0] * (len(tokens_prompt) + 2) + [1] * (len(tokens_response_a) + 1) + [2] * len(tokens_response_b)
    attention_mask = [1] * len(input_ids)
    
    padding_length = CFG.max_length - len(input_ids)
    if padding_length > 0:
        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
        token_type_ids = token_type_ids + [0] * padding_length
        attention_mask = attention_mask + [0] * padding_length
    
    return {
        'input_ids': input_ids[:CFG.max_length],
        'token_type_ids': token_type_ids[:CFG.max_length],
        'attention_mask': attention_mask[:CFG.max_length],
    }

def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)
    labels[df['winner_model_a'] == 1] = 0
    labels[df['winner_model_b'] == 1] = 1
    labels[df['winner_tie'] == 1] = 2
    df['labels'] = labels
    return df

def process_data(df, mode='train'):
    dataset = Dataset.from_pandas(df)
    tokenized_dataset = dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=False)
    remove_cols = ['id', 'prompt', 'response_a', 'response_b']
    if mode == 'train':
        remove_cols += ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']
    tokenized_dataset = tokenized_dataset.remove_columns(remove_cols)
    return tokenized_dataset

def split_train_val(dataset, train_fraction):
    np.random.seed(0)
    ixs = np.arange(len(dataset))
    cutoff = int(len(ixs) * train_fraction)
    np.random.shuffle(ixs)
    ixs_train = ixs[:cutoff]
    ixs_val = ixs[cutoff:]
    fit_train = dataset.select(ixs_train)
    fit_val = dataset.select(ixs_val)
    return fit_train, fit_val


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    return {
        'eval_log_loss': log_loss(labels, probabilities),
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()
    }
    
def train_model():
    log_parameters(logger)
    train_df = pd.read_csv(PATHS.train_path)
    train_df = add_label(train_df)
    train_tokenized = process_data(train_df, mode='train')
    
    skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_tokenized, train_tokenized['labels'])):
        print(f"Training fold {fold + 1}")
        logger.info(f"Training fold {fold + 1}")
        
        fit_train = train_tokenized.select(train_idx)
        fit_val = train_tokenized.select(val_idx)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            PATHS.model_path,
            num_labels=3,
            problem_type="single_label_classification"
        )
        
        training_args = TrainingArguments(
            output_dir=f"{CFG.output_dir}/fold_{fold}",  # æ¨¡åž‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•
            fp16=CFG.fp16,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            learning_rate=CFG.lr,  # å­¦ä¹ çŽ‡
            per_device_train_batch_size=CFG.train_batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°
            per_device_eval_batch_size=CFG.eval_batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è¯„ä¼°æ‰¹é‡å¤§å°
            num_train_epochs=CFG.train_epochs,  # è®­ç»ƒçš„æ€»å‘¨æœŸæ•°
            weight_decay=CFG.weight_decay,  # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
            evaluation_strategy=CFG.evaluation_strategy,  # è¯„ä¼°ç­–ç•¥
            metric_for_best_model=CFG.metric_for_best_model,  # ç”¨äºŽé€‰æ‹©æœ€ä½³æ¨¡åž‹çš„åº¦é‡æ ‡å‡†
            save_strategy=CFG.save_strategy,  # ä¿å­˜ç­–ç•¥
            save_total_limit=CFG.save_total_limit,  # ä¿å­˜çš„æ£€æŸ¥ç‚¹æ€»æ•°é™åˆ¶
            load_best_model_at_end=CFG.load_best_model_at_end,  # åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡åž‹
            report_to=CFG.report_to,  # ä¸æŠ¥å‘Šè®­ç»ƒæ—¥å¿—åˆ°å¤–éƒ¨å·¥å…·
            warmup_ratio=CFG.warmup_ratio,  # å­¦ä¹ çŽ‡é¢„çƒ­æ¯”ä¾‹
            lr_scheduler_type=CFG.lr_scheduler_type,  # å­¦ä¹ çŽ‡è°ƒåº¦ç±»åž‹
            optim=CFG.optim,  # ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç±»åž‹
            logging_first_step=CFG.logging_first_step,  # è®°å½•ç¬¬ä¸€æ­¥çš„æ—¥å¿—
            greater_is_better=CFG.greater_is_better,
            
            # max_grad_norm=CFG.max_grad_norm,  # è®¾ç½®æ¢¯åº¦è£å‰ª
            
            logging_steps=CFG.logging_steps,  # æ¯ 500 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
            logging_dir=CFG.logging_dir,  # æ—¥å¿—ä¿å­˜ç›®å½•
        
            save_steps=CFG.save_steps,  # æ¯  æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹
            eval_steps=CFG.save_steps,  # æ·»åŠ  eval_steps å‚æ•°,ä¸Ž save_steps ä¿æŒä¸€è‡´
        )

         # Log training arguments
        logger.info("Training arguments:")
        logger.info(pformat(training_args.to_dict()))

        if CFG.frozen_embedding:
            n = CFG.frozen_num
            # å†»ç»“åµŒå…¥å±‚
            for i, layer in enumerate(model.deberta.encoder.layer[:n]):
                for param in layer.parameters():
                    param.requires_grad = False # True False
            for param in model.deberta.embeddings.parameters():
                param.requires_grad = False

        # åˆå§‹åŒ– tokenizer
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

        # Create EarlyStoppingCallback
        if CFG.early_stop:
            early_stopping_callback = EarlyStoppingCallback(
                early_stopping_patience=CFG.early_stopping_patience,
                early_stopping_threshold=CFG.early_stopping_threshold,
            )
        
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=fit_train,
                data_collator=data_collator,
                eval_dataset=fit_val,
                compute_metrics=compute_metrics,
                callbacks=[early_stopping_callback],  # Add the early stopping callback
            )
        else:
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=fit_train,
                data_collator=data_collator,
                eval_dataset=fit_val,
                compute_metrics=compute_metrics,
            )
        
        trainer.train()
        
        # Save the model
        trainer.save_model(f"{CFG.output_dir}/fold_{fold}/best_model")
        tokenizer.save_pretrained(f"{CFG.output_dir}/fold_{fold}/tokenizer")
        
        # Log the results
        eval_result = trainer.evaluate()
        logger.info(f"Fold {fold + 1} - Evaluation result: {eval_result}")
        logger.info("=*100")

def predict_test():
    test_df = pd.read_csv(PATHS.test_path)
    test_tokenized = process_data(test_df, mode='test')
    
    predictions = []
    
    for fold in trange(CFG.n_splits):
        model = AutoModelForSequenceClassification.from_pretrained(f"{CFG.output_dir}/fold_{fold}/best_model")
        model.eval()
        
        trainer = Trainer(model=model)
        fold_preds = trainer.predict(test_tokenized).predictions
        fold_preds = np.exp(fold_preds) / np.sum(np.exp(fold_preds), axis=1, keepdims=True)
        predictions.append(fold_preds)
    
    # Average predictions across folds
    final_preds = np.mean(predictions, axis=0)
    display(predictions)
    logger.info(f"Final_preds: {final_preds}")
    
    # Create submission file
    submission = pd.DataFrame({
        'id': test_df['id'],
        'winner_model_a': final_preds[:, 0],
        'winner_model_b': final_preds[:, 1],
        'winner_tie': final_preds[:, 2]
    })
    
    submission.to_csv('submission.csv', index=False)
    display(submission)
```

---The following area is a Code cell (cell numver is 1)---
```python
%time
if __name__ == "__main__":
#     train_model()
    predict_test()
```

---The following area is a Code cell (cell numver is 2)---
```python
# import tokenizers
# print(tokenizers.__version__)
```

---The following area is a Code cell (cell numver is 3)---
```python

```

** @@@ Jupyter Notebook numver 14, the number of votes :16 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
"""

Credits:

https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora

LB: 0.945
"""
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import (
    Gemma2ForSequenceClassification, GemmaTokenizerFast, 
    AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
)
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel, get_peft_model, LoraConfig, TaskType

torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
```

---The following area is a Code cell (cell numver is 3)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    gemma_lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    llama_model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    llama_weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    tta = False
    spread_max_length = False

cfg = Config()
```

---The following area is a Code cell (cell numver is 4)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)
```

---The following area is a Code cell (cell numver is 5)---
```python
def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):
    if isinstance(tokenizer, GemmaTokenizerFast):
        prompt = ["<prompt>: " + p for p in prompt]
        response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
        response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    else:
        prompt = ["User prompt: " + p for p in prompt]
        response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
        response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 6)---
```python
# Gemma Tokenizer
gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
gemma_tokenizer.add_eos_token = True
gemma_tokenizer.padding_side = "right"

# Llama Tokenizer
llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# Prepare data for both models
gemma_data = pd.DataFrame()
gemma_data["id"] = test["id"]
gemma_data["input_ids"], gemma_data["attention_mask"] = tokenize(gemma_tokenizer, test["prompt"], test["response_a"], test["response_b"])
gemma_data["length"] = gemma_data["input_ids"].apply(len)

llama_data = pd.DataFrame()
llama_data["id"] = test["id"]
llama_data["input_ids"], llama_data["attention_mask"] = tokenize(llama_tokenizer, test["prompt"], test["response_a"], test["response_b"])
llama_data["length"] = llama_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Load Gemma model on GPU 0
device_0 = torch.device('cuda:0')
gemma_model = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False
)
gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)
```

---The following area is a Code cell (cell numver is 8)---
```python
# Load Llama model on GPU 1
device_1 = torch.device('cuda:1')
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False
)
llama_base_model = LlamaForSequenceClassification.from_pretrained(
    cfg.llama_model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)
llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)
llama_model.eval()
```

---The following area is a Code cell (cell numver is 9)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 10)---
```python
st = time.time()

# Sort data by input length
gemma_data = gemma_data.sort_values("length", ascending=False)
llama_data = llama_data.sort_values("length", ascending=False)

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, 
                           (gemma_data, llama_data), 
                           (gemma_model, llama_model),
                           (gemma_tokenizer, llama_tokenizer),
                           (device_0, device_1))

gemma_result_df, llama_result_df = list(results)

# Combine results (simple average)
combined_result_df = gemma_result_df.copy()
combined_result_df["winner_model_a"] = (gemma_result_df["winner_model_a"] + llama_result_df["winner_model_a"]) / 2
combined_result_df["winner_model_b"] = (gemma_result_df["winner_model_b"] + llama_result_df["winner_model_b"]) / 2
combined_result_df["winner_tie"] = (gemma_result_df["winner_tie"] + llama_result_df["winner_tie"]) / 2

print(f"Inference time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 11)---
```python
submission_df = combined_result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df.head())
```

** @@@ Jupyter Notebook numver 15, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS | XGB Baseline

(original notebook: https://www.kaggle.com/code/sercanyesiloz/lmsys-xgb-baseline)

# 1. Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 2. Configuration
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 3. Loading Data
```

---The following area is a Code cell (cell numver is 5)---
```python
# Read the training, test, and sample submission datasets from the specified paths
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

# If the test dataset has fewer than 10 rows, limit the training dataset to the first 10,000 rows
if test.shape[0] < 10:
    train = train.iloc[:10000]

# Define a function to process strings by removing brackets and splitting sentences
# NOTE: Another way would be to convert to JSON and then join, but this is probably most efficient in Python
def process(input_str):
    stripped_str = input_str.strip('[]')  # Remove leading and trailing square brackets
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # Split by "," and remove surrounding quotes
    return ' '.join(sentences)  # Join the sentences with a space

# Apply the `process` function to the prompt and response columns in the train dataset
train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

# Apply the `process` function to the prompt and response columns in the test dataset
test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

# Print the shapes of the train and test datasets
print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)

# Print the total number of missing values in the train and test datasets
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

# Display the first few rows of the train dataset
train.head()

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 4. Feature Engineering
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    # Calculate cosine similarity between two texts
    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Create a TF-IDF vectorizer (word-importance) with n-grams from 1 to 3
            vectorizer.fit([text1, text2])  # Fit the vectorizer on both texts
            output = vectorizer.transform([text1, text2]).toarray()  # Transform texts to TF-IDF vectors
            cos_sim = cosine_similarity(output)  # Calculate cosine similarity between vectors
            return cos_sim[0][1]  # Return the similarity score between text1 and text2
        except:
            print(f"cosine_sim exception with '{text1}' and '{text2}'")
            return np.nan  # Return NaN in case of an exception

    # Calculate Jaccard similarity between two texts
    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())  # Split text1 into set of words
        set2 = set(text2.split())  # Split text2 into set of words
        intersection = set1.intersection(set2)  # Find intersection of both sets
        union = set1.union(set2)  # Find union of both sets
        return len(intersection) / len(union)  # Return Jaccard similarity score
    
    # Count the number of quoted segments in a text
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"  # Pattern for single quotes
        double_quote_pattern = r'"(.*?)"'  # Pattern for double quotes
        single_quotes = re.findall(single_quote_pattern, text)  # Find all single-quoted segments
        double_quotes = re.findall(double_quote_pattern, text)  # Find all double-quoted segments
        total_quotes = len(single_quotes) + len(double_quotes)  # Sum the counts of both types of quotes
        return total_quotes  # Return the total count of quoted segments
    
    # Count the number of new-lines in a text
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n')  # Return the count of newline characters in the text
    
    # Count the number of bulleted lists in the text
    def count_bulleted_lists(self, text: str) -> int:
        bullet_pattern = r'(\\n|^)[\*\-\+]\s'  # Pattern for bulleted list items
        return len(re.findall(bullet_pattern, text))  # Return the count of bulleted list items
    
    # Tokenize text into lowercase words
    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    # Generate n-grams from the tokenized text
    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)  # Tokenize the text
        return list(ngrams(tokens, n))  # Generate n-grams from tokens

    # Count overlapping n-grams between two texts
    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # Generate n-grams for text1
            ngrams2 = self.generate_ngrams(text2, n)  # Generate n-grams for text2
            counter1 = Counter(ngrams1)  # Count n-grams in text1
            counter2 = Counter(ngrams2)  # Count n-grams in text2
            overlap = counter1 & counter2  # Find the overlap between the two counters
            overlap_count = sum(overlap.values())  # Sum the counts of overlapping n-grams
            return overlap_count  # Return the overlap count
        except:
            return 0  # Return 0 in case of an exception
        
    # Run preprocessing on the data
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # Calculate unigram, bigram, and trigram overlaps between response_a and response_b
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        # Calculate unigram, bigram, and trigram overlaps between response_a and prompt
        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        # Calculate unigram, bigram, and trigram overlaps between response_b and prompt
        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        # Calculate the length of tokenized texts
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        # Calculate length ratios between response_a, response_b, and prompt
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        # Calculate length differences between response_a, response_b, and prompt
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_a and prompt
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_b and prompt
        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        # Count the number of quotes in response_a, response_b, and prompt
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))

        # Count the number of new-lines in response_a, response_b, and prompt
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))

        # Count the number of bulleted lists in response_a, response_b, and prompt
        data["respa_bullets"] = data["response_a"].apply(lambda x: self.count_bulleted_lists(x))
        data["respb_bullets"] = data["response_b"].apply(lambda x: self.count_bulleted_lists(x))
        data["prompt_bullets"] = data["prompt"].apply(lambda x: self.count_bulleted_lists(x))
        
        # Calculate cosine and Jaccard similarities between response_a and response_b
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        # Calculate cosine and Jaccard similarities between response_a and prompt
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        # Calculate cosine and Jaccard similarities between response_b and prompt
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        return data  # Return the processed dataframe

```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
# List of columns to drop from the dataset
drop_cols = ["id", "response_a", "response_b", "prompt"]

# List of target columns indicating the winner
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]

# Name of the final target column
target = "target"

# Initialize the target column with NaN values
train[target] = np.nan

# Iterate over the target columns and set the corresponding index in the target column
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx  # Set target column to the index where target column value is 1

# Convert the target column to integer type
train[target] = train[target].astype("int32")

# Display the first few rows of the updated dataframe
train.head()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 5. Modeling
```

---The following area is a Code cell (cell numver is 11)---
```python
# Drop specified columns from the training dataset and assign the result to X
X = train.drop(columns=target_cols + drop_cols + [target] + ["model_a", "model_b"], axis=1)

# Assign the target column to y
y = train[target]

# Drop specified columns from the test dataset and assign the result to X_test
X_test = test.drop(columns=drop_cols, axis=1)

# Replace infinite values (-inf and inf) with NaN in the training feature set
X = X.replace([-np.inf, np.inf], np.nan)

# Replace infinite values (-inf and inf) with NaN in the test feature set
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Set up stratified cross-validation with the specified number of splits, shuffle, and seed
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)

# Initialize an array to store the average predictions for the test set
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))

# Initialize a list to store cross-validation scores (log loss) for each fold
cv_scores = list()

# Get the list of feature names
features = X.columns.tolist()

# Prepare a DataFrame to store feature importances for each fold
feat_imp_df = pd.DataFrame({"feature": features})

# Loop over each fold in the cross-validation
for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))  # Print the fold number

    # Split the data into training and validation sets for the current fold
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    # Print the shapes of the training and validation sets
    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    # Initialize the XGBoost classifier with specified hyperparameters
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed,
        device="gpu"
    )
    
    # Train the model with early stopping on the validation set
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    # Predict probabilities on the validation set
    val_preds = model.predict_proba(X_val)

    # Calculate the log loss for the validation set
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")

    # Append the log loss to the list of cross-validation scores
    cv_scores.append(val_log_loss)
    
    # Update test predictions with the current fold's predictions, averaged over all folds
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    # Merge the current fold's feature importances into the DataFrame
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

# Print a separator line and the average cross-validated log loss
print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

# Calculate the average feature importance across all folds
feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)

# Plot the top 50 features by average importance
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances for All Folds", size=12)
plt.show()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 6. Saving Submission
```

---The following area is a Code cell (cell numver is 14)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 16, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# ðŸ“š Importing Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# âš™ï¸ Configuration Class
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# ðŸ“Š Loading and Processing Data

we will load our training and test datasets, and apply some preprocessing. This includes:


1. **Loading Data**: Read the CSV files into pandas DataFrames.
2. **Subsampling**: If the test dataset has less than 10 rows, subsample the training dataset to 10,000 rows for quicker processing.
3. **Processing Strings**: Clean and process the string columns (`prompt`, `response_a`, `response_b`) by removing unwanted characters.
4. **Shape and Missing Values**: Print the shape of the datasets and count missing values to understand the data structure and quality.
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# ðŸ› ï¸ Preprocessing Class Definition

This class, `Preprocessor`, contains several methods to process and feature-engineer the text data. Hereâ€™s a breakdown of its functionalities:

#### Cosine Similarity
- **Formula**: 
  $$\text{cosine_similarity} = \frac{A \cdot B}{\|A\| \|B\|}$$

- **Description**: Cosine similarity measures the cosine of the angle between two vectors, providing a metric of how similar the texts are.

#### Jaccard Similarity
- **Formula**:

  $$\text{Jaccard_similarity} = \frac{|A \cap B|}{|A \cup B|}$$

- **Description**: Jaccard similarity measures the similarity between two sets by comparing their intersection and union.

#### Count Quotes
- **Description**: This method identifies and counts both single and double quoted texts in a string, giving an idea of how many quotations are present.

#### Tokenize
- **Description**: This method splits the text into individual words (tokens), which can be used for further analysis like generating n-grams or calculating overlaps.

#### Generate N-grams
- **Description**: N-grams are contiguous sequences of 'n' items from a given text. This method helps in analyzing the text at different levels of granularity (unigrams, bigrams, trigrams, etc.).

#### Count N-gram Overlaps
- **Description**: This method calculates how many n-grams are common between two texts, helping to measure their similarity.

#### Run
- **Description**: This method processes the entire dataset, generating new features based on the above calculations, which can be used for training machine learning models.


$\frac{n!}{k!(n-k)!} = \binom{n}{k}$
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer().fit_transform([text1, text2])
            vectors = vectorizer.toarray()
            cos_sim = cosine_similarity(vectors)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        return data
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Data Preparation
```

---The following area is a Code cell (cell numver is 10)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Code cell (cell numver is 11)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Handle missing values
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)
```

---The following area is a Code cell (cell numver is 13)---
```python
# Feature Selection
selector = SelectKBest(f_classif, k=25)
X_new = selector.fit_transform(X, y)
X_test_new = selector.transform(X_test)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# ðŸ§© Model Training and Evaluation

### Model Definitions

Defines several machine learning models including Random Forest, Gradient Boosting, SVM, XGBoost, CatBoost, and a Voting Classifier.

### Feature Selection

Uses SelectKBest to select 25 best features based on ANOVA F-value between feature and target.

### Cross-validation

Uses Stratified K-Fold cross-validation for model evaluation.

### Training and Evaluation

Iterates through each model, trains it using the training data, evaluates using cross-validation, and calculates the mean CV Log Loss.

### Feature Importances

Calculates and stores feature importances for applicable models (Random Forest, Gradient Boosting, XGBoost, CatBoost).

### Best Model Identification

Identifies the best performing model based on the lowest CV Log Loss.

### Results Display

Displays the results in a DataFrame showing the CV Log Loss for each model and, if applicable, feature importances.
```

---The following area is a Code cell (cell numver is 15)---
```python

# Define models and their configurations
models = {
    'random_forest': {
        'model': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=config.seed
        ),
        'params': {}
    },
    'gradient_boosting': {
        'model': GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            random_state=config.seed
        ),
        'params': {}
    },
    'svm': {
        'model': SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=config.seed
        ),
        'params': {}
    },
    'xgboost': {
        'model': xgb.XGBClassifier(
            objective='multi:softprob',
            num_class=3,
            eval_metric='mlogloss',
            subsample=0.8,
            n_estimators=650,
            learning_rate=0.045,
            max_depth=5,
            random_state=config.seed,
#             tree_method='gpu_hist'  # GPU acceleration if available
        ),
        'params': {}
    },
    'catboost': {
        'model': cb.CatBoostClassifier(
            loss_function='MultiClass',
            iterations=650,
            learning_rate=0.045,
            depth=5,
            random_seed=config.seed,
#             task_type="GPU",  # Use GPU if available
            verbose=75
        ),
        'params': {}
    },
    'voting': {
        'model': VotingClassifier(
            estimators=[
                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),
                ('svc', SVC(probability=True)),
                ('knn', KNeighborsClassifier(n_neighbors=5))
            ],
            voting='soft'
        ),
        'params': {}
    }
}

# Select features using SelectKBest
selector = SelectKBest(f_classif, k=25)
X_new = selector.fit_transform(X, y)
X_test_new = selector.transform(X_test)

# Cross-validation setup
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)

# Dataframe to store results
results = []

# Iterate over models
for model_name, model_data in models.items():
    model = model_data['model']
    print(f"Training model: {model_name}")

    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))
    cv_scores = []

    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):
        X_train, y_train = X_new[train_idx], y[train_idx]
        X_val, y_val = X_new[val_idx], y[val_idx]

        if model_name == 'voting':
            model.fit(X_train, y_train)
        elif model_name == 'catboost':
            model.fit(
                X_train,
                y_train,
                eval_set=[(X_train, y_train), (X_val, y_val)],
                early_stopping_rounds=75,
                verbose=75
            )
        else:
            model.fit(
                X_train,
                y_train
            )

        if model_name != 'voting':
            val_preds = model.predict_proba(X_val)
            val_log_loss = log_loss(y_val, val_preds, eps="auto")
            cv_scores.append(val_log_loss)

            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()

    if model_name != 'voting':
        mean_cv_log_loss = np.mean(cv_scores)
        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})
        print(f"Mean CV Log Loss: {mean_cv_log_loss:.5f}")

# Store feature importances if applicable
if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:
    features = X.columns[selector.get_support()].tolist()
    feat_imp_df = pd.DataFrame({"feature": features})
    feat_imp_df[f"{model_name}_avg_importance"] = 0

    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):
        X_val, _ = X_new[val_idx], y[val_idx]
        feat_imp_df[f"{model_name}_avg_importance"] += model.feature_importances_ / cv.get_n_splits()

    results_df = pd.DataFrame(results)
    results_df = pd.concat([results_df, feat_imp_df], axis=1)

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Identify the best model
best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]
print(f"\nBest Model:\n{best_model}")

# Display results DataFrame
print("\nResults DataFrame:")
print(results_df)

```

---The following area is a Code cell (cell numver is 16)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 17)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 17, the number of votes :13 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install transformers peft accelerate bitsandbytes \-U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# PART 1 (LLama 3 8b Chat)
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers
```

---The following area is a Code cell (cell numver is 4)---
```python
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 5)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 32)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 33)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 34)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 35)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 36)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 37)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 40)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_preds = yp
```

---The following area is a Code cell (cell numver is 42)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 43)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# Part 2 (Gemma 2 4b QLora)
```

---The following area is a Code cell (cell numver is 45)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 47)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False 
    spread_max_length = False 

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
## Load & pre-process Data
```

---The following area is a Code cell (cell numver is 49)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 50)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))


test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 51)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 52)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # TODO: change prompt
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 53)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 54)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 55)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
## Load model
```

---The following area is a Code cell (cell numver is 57)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 58)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 59)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 61)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 62)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 63)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 64)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
```

---The following area is a Code cell (cell numver is 65)---
```python
result_df
```

---The following area is a Code cell (cell numver is 66)---
```python
out
```

---The following area is a Code cell (cell numver is 67)---
```python
merged_df = pd.merge(out, result_df, on='id')
```

---The following area is a Code cell (cell numver is 68)---
```python
merged_df
```

---The following area is a Code cell (cell numver is 69)---
```python
merged_df['winner_model_a']= (0.7*merged_df['winner_model_a_y']) + (0.3*merged_df['winner_model_a_x'])
merged_df['winner_model_b']= (0.7*merged_df['winner_model_b_y']) + (0.3*merged_df['winner_model_b_x'])
merged_df['winner_tie']= (0.7*merged_df['winner_tie_y']) + (0.3*merged_df['winner_tie_x'])
```

---The following area is a Code cell (cell numver is 70)---
```python
submission_df = merged_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 71)---
```markdown
credits: https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora
```

** @@@ Jupyter Notebook numver 18, the number of votes :12 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
Inference on float16 should take ???hrs


training nb:

https://www.kaggle.com/code/pranshubahadur/tf-gemma-2-9b-lmsys-training-tpu
```

---The following area is a Code cell (cell numver is 1)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
import keras
import keras_nlp
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (1, 2),
    ["batch", "model"],
    devices=['gpu:0', 'gpu:1'],
)
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
layout_map["position_embedding/embeddings"] = (model_dim, None)

# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)

layout_map["decoder_block.*layer_norm/scale"] = (model_dim,)
layout_map["decoder_block.*layer_norm/bias"] = (model_dim,)
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)

```

---The following area is a Code cell (cell numver is 2)---
```python
import jax
jax.default_device = jax.devices('cpu')[0]
jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
keras.config.set_dtype_policy("float16")

```

---The following area is a Code cell (cell numver is 4)---
```python

```

---The following area is a Code cell (cell numver is 5)---
```python
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1", trainable=False, dtype='int8')

gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 6)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 7)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_test_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
#raw_test_dataset[input_columns] = raw_test_dataset[input_columns].map(lambda x: eval(x)[0])
#raw_test_dataset =raw_test_dataset.dropna().reset_index(drop=True)



train_dataset = DataFrame({
    'text' : raw_test_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1).apply(lambda x: '\n\nPROMPT\n\n' + x).apply(lambda x: remove_surrogates(x)),
})
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = gemma_lm._preprocessor
backbone = gemma_lm.backbone
```

---The following area is a Code cell (cell numver is 9)---
```python
def preprocess_fn(text, label=None):
    preprocessed = tokenizer(text, sequence_length=512)[0]
    print(preprocessed)
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}
```

---The following area is a Code cell (cell numver is 10)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model


inputs = {
        "token_ids": keras.Input(shape=(512,), dtype=tf.int32, name="token_ids"),
        "padding_mask": keras.Input(shape=(512,), dtype=tf.int32, name="padding_mask"),
    }
x = backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 11)---
```python

optimizer = keras.optimizers.AdamW(
                    learning_rate=5e-5,
                    weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 12)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 13)---
```python
model.layers[2].load_lora_weights("/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/model.lora.h5")
```

---The following area is a Code cell (cell numver is 14)---
```python
import numpy as np
dense_1_weights = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_kernel.npy')
dense_1_biases = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_bias.npy')
dense_1_combined = [dense_1_weights, dense_1_biases]
model.layers[-1].set_weights(dense_1_combined)

```

---The following area is a Code cell (cell numver is 15)---
```python
for layer in model.layers:
    layer.trainable = False
```

---The following area is a Code cell (cell numver is 16)---
```python
model.summary()
```

---The following area is a Code cell (cell numver is 17)---
```python
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values)).map(preprocess_fn).batch(16)

```

---The following area is a Code cell (cell numver is 18)---
```python
from tqdm import tqdm

preds = []

for inputs in tqdm(ds):
    keras.backend.clear_session(free_memory=True)
    preds.append(model(inputs))
    keras.backend.clear_session()

    


```

---The following area is a Code cell (cell numver is 19)---
```python
import numpy as np
results = np.concatenate(preds)
```

---The following area is a Code cell (cell numver is 20)---
```python
import pandas
submission = pandas.DataFrame(data=results, index=raw_test_dataset.id, columns=label_columns)
```

---The following area is a Code cell (cell numver is 21)---
```python
submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 22)---
```python
submission.head()
```

---The following area is a Code cell (cell numver is 23)---
```python

```

** @@@ Jupyter Notebook numver 19, the number of votes :9 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction

This notebook is forked from [Here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)

Qwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.

The notebook training Qwen2 1.5b version with batch size of 4 and 1 epoch. The training time is around 1 hr on A100. I expect that Qwen2 7b could have better performance on this task. The training code for Colab is attached at the end of the notebook.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn

import numpy as np
import pandas as pd
from transformers import Qwen2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_dir = '/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1'
    lora_dir = '/kaggle/input/lmsys-qwen2-1-5b-checkpoint/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<User prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,
    num_labels=3,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,
    num_labels=3,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_0.config.pad_token_id = model_0.config.eos_token_id
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
model_1.config.pad_token_id = model_1.config.eos_token_id
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Training Code

This training code is forked and inspired from https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning


## Transfer Kaggle Data
```

---The following area is a Code cell (cell numver is 24)---
```python
# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# # THEN FEEL FREE TO DELETE THIS CELL.
# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# # NOTEBOOK.

# import os
# import sys
# from tempfile import NamedTemporaryFile
# from urllib.request import urlopen
# from urllib.parse import unquote, urlparse
# from urllib.error import HTTPError
# from zipfile import ZipFile
# import tarfile
# import shutil

# CHUNK_SIZE = 40960
# DATA_SOURCE_MAPPING = 'lmsys-chatbot-arena:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66631%2F8346466%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ae2cd10c5a4750deca45551519904c5858980d5cf8cd8ade09b2299d926c86c895b50ba333acf0db5210d0dd29197c9a9c5a525c8afd0186b88f17d3ca756f0562ad5acfa2e856e8b159f554e61f72102865abd60add751dd59bed5126536d977d6fe54d2e85f8e5baa8d3d75337d0a222a89f0f30fa6dd7c360e4a192363dc417e9e4a9c9c23368991db65b4994c2200bee494d8d5e2684f754ab1b1a511f7db3652e01ab658b04d26cc1321e783fa5509f67d4c438808adc7932a0e79a21849375023b36e90cbe288cf68a6185b2ce950464b71c9b6133d49769c67e77a5298809fb63da23c0655165e80661623bd9bb908bc9a486dbc9e09caebf2a01392,qwen2/transformers/qwen2-1.5b-instruct/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F52038%2F62308%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80f6a1f964073d960129f611693afca0666128c1a319b7ab93769a1993d5910e4995cf3c6f2f87323452999b069eafb9f7b01be97dbb80a3441cd0b4871d9d35379750a3b5614397253624b097961c886c48df889ac7b1100231715e2b60bf4f3ffccd5e7fc68b3d7d0668f350a8fefbddb90275770e75aaa7f74fae68b2f5314f610ec2f1abf0436156e9426e6173e229172ca0c4ee91eb2d768de3190c9f07e6c28b73bc8c5553e2dac6320842103524591b663021a41801bb2274b5fd91dd62f174ba8976c74995012ad3ed34ecf554a9e2cb08f91813e9cacc9b8d554c6a7a037414635f30e506ea39f63fb4db01f5cf3322dca02097f9550b1a5454ae99'

# KAGGLE_INPUT_PATH='/kaggle/input'
# KAGGLE_WORKING_PATH='/kaggle/working'
# KAGGLE_SYMLINK='kaggle'

# !umount /kaggle/input/ 2> /dev/null
# shutil.rmtree('/kaggle/input', ignore_errors=True)
# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

# try:
#   os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
# except FileExistsError:
#   pass
# try:
#   os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
# except FileExistsError:
#   pass

# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
#     directory, download_url_encoded = data_source_mapping.split(':')
#     download_url = unquote(download_url_encoded)
#     filename = urlparse(download_url).path
#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
#     try:
#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
#             total_length = fileres.headers['content-length']
#             print(f'Downloading {directory}, {total_length} bytes compressed')
#             dl = 0
#             data = fileres.read(CHUNK_SIZE)
#             while len(data) > 0:
#                 dl += len(data)
#                 tfile.write(data)
#                 done = int(50 * dl / int(total_length))
#                 sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
#                 sys.stdout.flush()
#                 data = fileres.read(CHUNK_SIZE)
#             if filename.endswith('.zip'):
#               with ZipFile(tfile) as zfile:
#                 zfile.extractall(destination_path)
#             else:
#               with tarfile.open(tfile.name) as tarfile:
#                 tarfile.extractall(destination_path)
#             print(f'\nDownloaded and uncompressed: {directory}')
#     except HTTPError as e:
#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
#         continue
#     except OSError as e:
#         print(f'Failed to load {download_url} to path {destination_path}')
#         continue

# print('Data source import complete.')

```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## Install and Load libraries
```

---The following area is a Code cell (cell numver is 26)---
```python
## gemma-2 is available from transformers>=4.42.3
# !pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft datasets
```

---The following area is a Code cell (cell numver is 27)---
```python
# import os
# import copy
# from dataclasses import dataclass

# import numpy as np
# import torch
# from datasets import Dataset
# from transformers import (
#     BitsAndBytesConfig,
#     AutoTokenizer,
#     Qwen2ForSequenceClassification,
#     PreTrainedTokenizerBase,
#     EvalPrediction,
#     Trainer,
#     TrainingArguments,
#     DataCollatorWithPadding,
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
# from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
## Training Configuration
```

---The following area is a Code cell (cell numver is 29)---
```python
# @dataclass
# class Config:
#     output_dir: str = "output"
#     model_name: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1"
#     checkpoint: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1" # local directory of qwen model
#     max_length: int = 2048
#     n_splits: int = 5
#     fold_idx: int = 0
#     optim_type: str = "adamw_8bit"
#     per_device_train_batch_size: int = 4
#     gradient_accumulation_steps: int = 2  # global batch size is 8
#     per_device_eval_batch_size: int = 8
#     n_epochs: int = 1
#     freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
#     lr: float = 2e-4
#     warmup_steps: int = 20
#     lora_r: int = 16
#     lora_alpha: float = lora_r * 2
#     lora_dropout: float = 0.05
#     lora_bias: str = "none"

# config = Config()
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
## Training Arguments
```

---The following area is a Code cell (cell numver is 31)---
```python
# training_args = TrainingArguments(
#     output_dir="output",
#     overwrite_output_dir=True,
#     report_to="none",
#     num_train_epochs=config.n_epochs,
#     per_device_train_batch_size=config.per_device_train_batch_size,
#     gradient_accumulation_steps=config.gradient_accumulation_steps,
#     per_device_eval_batch_size=config.per_device_eval_batch_size,
#     logging_steps=10,
#     eval_strategy="epoch",
#     save_strategy="steps",
#     save_steps=200,
#     optim=config.optim_type,
#     fp16=True,
#     learning_rate=config.lr,
#     warmup_steps=config.warmup_steps,
# )
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
## LoRA Config
```

---The following area is a Code cell (cell numver is 33)---
```python
# lora_config = LoraConfig(
#     r=config.lora_r,
#     lora_alpha=config.lora_alpha,
#     # only target self-attention
#     target_modules=["q_proj", "k_proj", "v_proj"],
#     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
#     lora_dropout=config.lora_dropout,
#     bias=config.lora_bias,
#     task_type=TaskType.SEQ_CLS,
# )
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
## Initialize and Tokenike model
```

---The following area is a Code cell (cell numver is 35)---
```python
# tokenizer = AutoTokenizer.from_pretrained(config.model_name)
# tokenizer.add_eos_token = True
# tokenizer.padding_side = "right"
# model = Qwen2ForSequenceClassification.from_pretrained(
#     config.model_name,
#     num_labels=3,
#     torch_dtype=torch.float16,
#     device_map="auto",
# )
# model.config.use_cache = False
# model = prepare_model_for_kbit_training(model)
# model = get_peft_model(model, lora_config)
# model.config.pad_token_id = model.config.eos_token_id
# model
```

---The following area is a Code cell (cell numver is 36)---
```python
# model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# Load Training Data
```

---The following area is a Code cell (cell numver is 38)---
```python
# ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
# class CustomTokenizer:
#     def __init__(
#         self,
#         tokenizer: PreTrainedTokenizerBase,
#         max_length: int
#     ) -> None:
#         self.tokenizer = tokenizer
#         self.max_length = max_length

#     def __call__(self, batch: dict) -> dict:
#         prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
#         response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
#         response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
#         texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
#         tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
#         labels=[]
#         for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
#             if a_win:
#                 label = 0
#             elif b_win:
#                 label = 1
#             else:
#                 label = 2
#             labels.append(label)
#         return {**tokenized, "labels": labels}

#     @staticmethod
#     def process_text(text: str) -> str:
#         return " ".join(eval(text, {"null": ""}))
# encode = CustomTokenizer(tokenizer, max_length=config.max_length)
# ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
## Run Training
```

---The following area is a Code cell (cell numver is 40)---
```python
# def compute_metrics(eval_preds: EvalPrediction) -> dict:
#     preds = eval_preds.predictions
#     labels = eval_preds.label_ids
#     probs = torch.from_numpy(preds).float().softmax(-1).numpy()
#     loss = log_loss(y_true=labels, y_pred=probs)
#     acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
#     return {"acc": acc, "log_loss": loss}

# folds = [
#     (
#         [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
#         [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
#     )
#     for fold_idx in range(config.n_splits)
# ]
```

---The following area is a Code cell (cell numver is 41)---
```python
# train_idx, eval_idx = folds[config.fold_idx]

# trainer = Trainer(
#     args=training_args,
#     model=model,
#     tokenizer=tokenizer,
#     train_dataset=ds.select(train_idx),
#     eval_dataset=ds.select(eval_idx),
#     compute_metrics=compute_metrics,
#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
# )
# trainer.train()
```

** @@@ Jupyter Notebook numver 20, the number of votes :8 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U transformers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U tokenizers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U peft --no-index --find-links ../input/llama3-1-dependencies/dependencies/
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U trl
```

---The following area is a Code cell (cell numver is 2)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    DataCollatorWithPadding,
    LlamaForSequenceClassification,
    LlamaTokenizerFast,
    PreTrainedTokenizerBase,
    EvalPrediction,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "/kaggle/input/unsloth-meta-llama-3.1-8b-bnb-4bit/transformers/default/1/Meta-Llama-3.1-8B-bnb-4bit"
    max_length: int = 2048
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 32 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 4
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"

config = Config()
```

---The following area is a Code cell (cell numver is 5)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = LlamaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 7)---
```python
model = LlamaForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

---The following area is a Code cell (cell numver is 8)---
```python
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 9)---
```python
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 10)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 11)---
```python
import pandas as pd
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
df = df.dropna()
df = df.drop_duplicates(subset=['response_a', 'response_b'], keep=False)
df["len"] = df["prompt"].apply(len) + df["response_a"].apply(len) + df["response_b"].apply(len)
df = df.sort_values(by=['len'])
df
```

---The following area is a Code cell (cell numver is 12)---
```python
ds = Dataset.from_pandas(df)
ds = ds.select(torch.arange(1000)) #for demo purposes only
```

---The following area is a Code cell (cell numver is 13)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["Which is the better response for the prompt? response_a or response_b or tie? \n'n give score for each lable \n\n <prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 14)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Code cell (cell numver is 15)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    # Check for NaNs in predictions and labels
    if np.isnan(probs).any() or np.isnan(labels).any():
        raise ValueError("NaN values found in predictions or labels")

    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Code cell (cell numver is 16)---
```python
folds = [
        (
            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
            [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
        ) 
        for fold_idx in range(config.n_splits)
    ]
```

---The following area is a Code cell (cell numver is 17)---
```python
from trl import SFTTrainer, SFTConfig
sft_config = SFTConfig(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=1000,
    save_strategy="epoch",
    save_steps=100,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
    packing=True, 
    dataset_text_field="text",
    max_seq_length=config.max_length,
)
```

---The following area is a Code cell (cell numver is 18)---
```python
trainer = SFTTrainer(
        model,
        train_dataset=ds,
        args=sft_config
    )
```

---The following area is a Code cell (cell numver is 19)---
```python
for fold_idx in range(config.n_splits):
    
    train_idx, eval_idx = folds[fold_idx]

    train_data = ds.select(train_idx).sort("len")
    val_data = ds.select(eval_idx).sort("len")
    
    #split training data into batches with the same range of length
    batch_size = 200
    num_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)
    
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(train_data))
        ds_temp = train_data.select(range(start_idx, end_idx))
        
        trainer.train_dataset = ds_temp
        
        print(f"Training batch {batch_idx + 1}/{num_batches} on fold {fold_idx + 1}/{config.n_splits}...")
        
        trainer.train()
        
        trainer.save_model(f"model_fold_{fold_idx}_batch{batch_idx}")

    
    # Validate after training on all batches
    trainer.eval_dataset = val_data
    
    print(f"Validating on fold {fold_idx + 1}/{config.n_splits}...")
    eval_results = trainer.evaluate()

    # Save metrics if needed
    print(f"Evaluation results for fold {fold_idx + 1}: {eval_results}")
```

** @@@ Jupyter Notebook numver 21, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import tensorflow as tf
from datasets import load_dataset, DatasetDict
from transformers import BertTokenizer, TFBertModel, DataCollatorWithPadding
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout
from tensorflow.keras.models import Model
import shutil
import pandas as pd
from tqdm.keras import TqdmCallback
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 1)---
```python
# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.MirroredStrategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Importing/Loading the training and test dataset
```

---The following area is a Code cell (cell numver is 3)---
```python
# path for sets
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'

# loading datasets
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# saving ID
test_ids = test_dataset['id']
```

---The following area is a Code cell (cell numver is 4)---
```python
# adding missing columns in the test set
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    if col not in test_dataset.column_names:
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# trainsformation to int64
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 5)---
```python
# using bert-base-cased's files locally
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'

model_dir = '/kaggle/working/bert-base-cased'
os.makedirs(model_dir, exist_ok=True)

shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Tokenization data
tokenizer = BertTokenizer.from_pretrained(model_dir)

def tokenize_function(examples):
    return tokenizer(examples['model_a'], examples['model_b'], padding="max_length", truncation=True, max_length=512)
tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Convert to tf.data.Dataset with the correct shape
def convert_to_tf_dataset(dataset, label_col=None):
    if label_col:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in tokenizer.model_input_names])
    else:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])

    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,
        label_cols=[label_col] if label_col else None,
        shuffle=True,
        batch_size=64,
        collate_fn=data_collator
    )
# creating tf.data.Dataset for test
def convert_to_tf_dataset_for_inference(dataset):
    dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])
    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,
        shuffle=False,
        batch_size=16,
        collate_fn=data_collator
    )
```

---The following area is a Code cell (cell numver is 8)---
```python
train_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
test_labels = 'winner_model_a'
test_dataset = convert_to_tf_dataset_for_inference(test_tokenized_datasets)


# Check data types and shapes before training
for batch in train_dataset.take(1):
    inputs, labels = batch
    print("Data types:")
    print(f"input_ids: {inputs['input_ids'].dtype}, attention_mask: {inputs['attention_mask'].dtype}, labels: {labels.dtype}")
    print("Data shapes:")
    print(f"input_ids: {inputs['input_ids'].shape}, attention_mask: {inputs['attention_mask'].shape}, labels: {labels.shape}")

```

---The following area is a Code cell (cell numver is 9)---
```python
# building a custom model
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)  # load model from local directory
    
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state


def create_keras_model():
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    bert_output = BertLayer()([input_ids, attention_mask])
    pooled_output = Lambda(lambda x: x[:, 0], output_shape=(768,))(bert_output)
    output = Dense(4, activation='softmax')(pooled_output)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model
```

---The following area is a Code cell (cell numver is 10)---
```python
#learning
with strategy.scope():
    model = create_keras_model()
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(train_dataset,  epochs=10, callbacks=[TqdmCallback(verbose=2)])

```

---The following area is a Code cell (cell numver is 11)---
```python
# prediction
predictions = model.predict(test_dataset)

# creation DataFrame for submission
submission = pd.DataFrame({
    'id': test_ids,
    'winner_model_a': predictions[:, 0],  
    'winner_model_b': predictions[:, 1],
    'winner_model_tie': predictions[:, 2]
})

submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 22, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -U transformers peft bitsandbytes accelerate --no-index --find-links /kaggle/input/lmsys-wheel-files
!pip install -q -U einops --no-index --find-links /kaggle/input/einops-v0-8-0
```

---The following area is a Code cell (cell numver is 1)---
```python
import torch
from transformers import AutoModel, AutoTokenizer
from transformers import BitsAndBytesConfig

```

---The following area is a Code cell (cell numver is 2)---
```python

model_path = "/kaggle/input/internlm2-1.8b-reward/transformers/default/1/internlm_internlm2-1_8b-reward"
model_path = "/kaggle/input/internlm-2-7b/transformers/default/1/internlm_internlm2-7b-reward"
model_path = "/kaggle/input/iternlm2-20b-reward/transformers/default/1/internlm_internlm2-20b-reward"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

model_0 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:0", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

model_1 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:1", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

# model_0 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:0", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
# model_1 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:1", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
```

---The following area is a Code cell (cell numver is 3)---
```python
# bnb_4bit_path = "internlm2-20b-rm-bnb-4bit"
# model_0.save_pretrained(bnb_4bit_path)
# tokenizer.save_pretrained(bnb_4bit_path)
```

---The following area is a Code cell (cell numver is 4)---
```python
# from IPython.display import FileLink, display
# display(FileLink("/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors"))
```

---The following area is a Code cell (cell numver is 5)---
```python
model_0
```

---The following area is a Code cell (cell numver is 6)---
```python
import pandas as pd

# DEBUG = False
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
# if len(df) == 3:
#     DEBUG = True
#     df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#     df = df.head(1000)
df
```

---The following area is a Code cell (cell numver is 7)---
```python

def cut_off(example, max_length=7200):
    
    def _count(example, idx):
        _len = 0    
        for s in example['prompt'][:idx] + example['response_a'][:idx] + example['response_b'][:idx]:
            _len += len(s)
        return _len

    def _recusive_cut(idx):
        if idx == 0:
            example['prompt'] = [example['prompt'][0][:1000]]
            example['response_a'] = [example['response_a'][0][:3000]]
            example['response_b'] = [example['response_b'][0][:3000]]
            return example
            
        if _count(example, idx) > max_length:
            return _recusive_cut(idx-1)
        else:
            example['prompt'] = example['prompt'][:idx]
            example['response_a'] = example['response_a'][:idx]
            example['response_b'] = example['response_b'][:idx]
            return example
    return _recusive_cut(len(example['prompt']))
        


def process_fn(example):
    example['prompt'] = eval(example['prompt'], {"null": ""})
    example['response_a'] = eval(example['response_a'], {"null": ""})
    example['response_b'] = eval(example['response_b'], {"null": ""})
    return cut_off(example)

new_df = df.apply(lambda x: process_fn(x), axis=1)
new_df
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time
import math
import numpy as np
def inference(df, model):

    error_cnt = 0
    y_pred = []
    for idx, row in df.iterrows():
        chat_a = []
        chat_b = []
        for i in range(len(row['prompt'])):
            chat_a.append({"role": "user", "content": row['prompt'][i]})
            chat_a.append({"role": "assistant", "content": row['response_a'][i]})

            chat_b.append({"role": "user", "content": row['prompt'][i]})
            chat_b.append({"role": "assistant", "content": row['response_b'][i]})
            
        try:
            score1, score2 = model.get_scores(tokenizer, [chat_a, chat_b])
            if abs(score1 - score2) < 0.08:
                y_pred.append([0.00005, 0.00005, 0.9999])
            else:
                score1, score2 = math.exp(score1), math.exp(score2)
                sum_ = score1 + score2
                y_pred.append([score1/sum_ - 0.0001, score2/sum_ -0.0001, 0.0002])
        except:
            y_pred.append([0.33334, 0.33333, 0.33333])
            error_cnt += 1
        
    y_pred = np.array(y_pred)
    df['winner_model_a_pred'] = y_pred[:, 0]
    df['winner_model_b_pred'] = y_pred[:, 1]
    df['winner_tie_pred'] = y_pred[:, 2]
    print(error_cnt)
    return df
```

---The following area is a Code cell (cell numver is 9)---
```python
# test
# inference(new_df[:20], model_0)
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time
sub_0 = new_df.iloc[0::2].copy()
sub_1 = new_df.iloc[1::2].copy()

from concurrent.futures import ThreadPoolExecutor 

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_0, sub_1), (model_0, model_1))

result_df = pd.concat(list(results), axis=0)
result_df.head()
```

---The following area is a Code cell (cell numver is 11)---
```python
result_df = result_df.rename(columns={'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})
result_df
```

---The following area is a Code cell (cell numver is 12)---
```python
result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)
pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 13)---
```python
# from sklearn.metrics import log_loss, accuracy_score

# if DEBUG:
#     y_true = result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.tolist()
#     y_pred = result_df[['winner_model_a_pred', 'winner_model_b_pred', 'winner_tie_pred']].values.tolist()
#     print(log_loss(y_true, y_pred))
```

---The following area is a Code cell (cell numver is 14)---
```python
# if not DEBUG:
#     result_df = result_df.rename({'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})
#     result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)
#     pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 15)---
```python

```

** @@@ Jupyter Notebook numver 23, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq
!pip install -U /kaggle/input/peft-wheel/pytorch/version1/1/peft-0.10.0-py3-none-any.whl -qq
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
import numpy as np
from scipy.special import softmax
from transformers import AutoTokenizer, AutoModelForSequenceClassification,LlamaForSequenceClassification, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
import torch
from torch.cuda.amp import autocast
from datasets import Dataset
import torch.nn.functional as F
from threading import Thread
import gc
```

---The following area is a Code cell (cell numver is 2)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)

MODEL_NAME = "/kaggle/input/llama-3/transformers/8b-hf/1"
MAX_LENGTH = 1284
BATCH_SIZE = 4
```

---The following area is a Code cell (cell numver is 3)---
```python
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
df.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
def transform(row):
    return row.strip('[]')
```

---The following area is a Code cell (cell numver is 5)---
```python
df['prompt'] = df['prompt'].apply(transform)
df['response_a'] = df['response_a'].apply(transform)
df['response_b'] = df['response_b'].apply(transform)
```

---The following area is a Code cell (cell numver is 6)---
```python
df['text'] = 'User prompt: ' + df['prompt'] +  '\n\nModel A :\n' + df['response_a'] +'\n\n----------\n\nModel B:\n'  + df['response_b']
```

---The following area is a Code cell (cell numver is 7)---
```python
# peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
# peft_config = PeftConfig.from_pretrained(peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_function(df):
    result = tokenizer(df, padding="max_length", truncation=True, max_length = MAX_LENGTH)
    return result['input_ids'], result['attention_mask']
```

---The following area is a Code cell (cell numver is 9)---
```python
temp = df['text'].apply(tokenize_function)
df['input_ids'] = temp.apply(lambda x: x[0])
df['attention_mask'] = temp.apply(lambda x: x[0])
```

---The following area is a Code cell (cell numver is 10)---
```python
df
```

---The following area is a Code cell (cell numver is 11)---
```python
# data = Dataset.from_pandas(df[['text']])
# data = data.map(tokenize_function, batched=True)
# data.set_format(type='torch', columns=['input_ids', 'attention_mask'])
```

---The following area is a Code cell (cell numver is 12)---
```python
# data
```

---The following area is a Code cell (cell numver is 13)---
```python
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)
```

---The following area is a Code cell (cell numver is 14)---
```python
peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
```

---The following area is a Code cell (cell numver is 15)---
```python
model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
model_0.config.pad_token_id = tokenizer.pad_token_id

model_0 = PeftModel.from_pretrained(model_0, peft_model_id).to(device0)
model_0 = model_0.merge_and_unload()
model_0.eval()
```

---The following area is a Code cell (cell numver is 16)---
```python
model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
model_1.config.pad_token_id = tokenizer.pad_token_id

model_1 = PeftModel.from_pretrained(model_1, peft_model_id).to(device1)
model_1 = model_1.merge_and_unload()
model_1.eval()
```

---The following area is a Code cell (cell numver is 17)---
```python
gc.collect()
```

---The following area is a Code cell (cell numver is 18)---
```python
def inference(df,model,device,batch_size=BATCH_SIZE):
    
    all_probabilities = []
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        
        batch_input_ids = torch.tensor(df['input_ids'][start_idx:end_idx].tolist()).to(device)
        batch_attention_mask = torch.tensor(df['attention_mask'][start_idx:end_idx].tolist()).to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=-1)
        all_probabilities.extend(probabilities.cpu().numpy())
    
    del batch_input_ids, batch_attention_mask, outputs
    gc.collect()
    torch.cuda.empty_cache()  

    all_probabilities = np.array(all_probabilities)
    
    df['winner_model_a'] = all_probabilities[:, 0]
    df['winner_model_b'] = all_probabilities[:, 1]
    df['winner_tie'] = all_probabilities[:, 2]
    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
N_SAMPLES = len(df)

half = round(N_SAMPLES / 2)
sub1 = df.iloc[0:half].copy()
sub2 = df.iloc[half:N_SAMPLES].copy()
```

---The following area is a Code cell (cell numver is 20)---
```python
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 21)---
```python
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)
```

---The following area is a Code cell (cell numver is 22)---
```python
results = {}
t0 = Thread(target=run_inference, args=(sub1, model_0, device0,results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1,results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

data = pd.concat([results[0], results[1]], axis=0)
```

---The following area is a Code cell (cell numver is 23)---
```python
data.drop(columns=['prompt','response_a','response_b','text','input_ids','attention_mask'],axis=1,inplace=True)
```

---The following area is a Code cell (cell numver is 24)---
```python
data
```

---The following area is a Code cell (cell numver is 25)---
```python
data.to_csv("submission.csv",index=False)
```

** @@@ Jupyter Notebook numver 24, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# No Installation RequiredÂ¶
microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training

The max sequence length has a significant impact on model performance, but due to insufficient memory, it was set to a maximum length of 768.

[training code](https://www.kaggle.com/code/argozero01/parallel-train-phi-3-mini-4k-instruct)
```

---The following area is a Code cell (cell numver is 1)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional,Tuple
```

---The following area is a Code cell (cell numver is 2)---
```python
# params
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"
model_path = "/kaggle/input/checkpoint-phi3/model_checkpoint.pth"
seed = 42
lora_r = 2
quantize_bit = 16
test_batch_size = 1
test_max_len = 256
device = "cuda"
```

---The following area is a Code cell (cell numver is 3)---
```python
class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.prompt = df['prompt']
        self.response_a = df['response_a']
        self.response_b = df['response_b']
        self.max_len = max_len
        self.targets = df.get('labels', None)

    def __len__(self):
        return len(self.prompt)

    def __getitem__(self, index):
        prompt = str(self.prompt[index])
        response_a = str(self.response_a[index])
        response_b = str(self.response_b[index])

        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])

        final_prompt_len = min(self.max_len, prompt_len)
        final_a_len = min(self.max_len, response_a_len)
        final_b_len = min(self.max_len, response_b_len)

        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')

        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}
```

---The following area is a Code cell (cell numver is 4)---
```python
def custom_collate_fn(batch, tokenizer):

    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None

    # Find the maximum length of the sequences in the batch
    max_len = max([input_id.size(0) for input_id in input_ids])

    # Re-tokenize with the new max length
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        input_ids = item['input_ids'][:max_len]
        attention_mask = item['attention_mask'][:max_len]

        new_input_ids.append(input_ids)
        new_attention_masks.append(attention_mask)

    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)

    output = {
    'input_ids': new_input_ids,
    'attention_mask': new_attention_masks}

    if labels is not None:
        output['labels'] = labels

    return output
```

---The following area is a Code cell (cell numver is 5)---
```python
def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)
    )
    return dataloader
```

---The following area is a Code cell (cell numver is 6)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    qmin = 0.
    qmax = 2.**num_bits - 1.

    min_val, max_val = tensor.min(), tensor.max()
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - min_val / scale

    quantized_tensor = torch.round(tensor / scale + zero_point)
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)
    quantized_tensor = (quantized_tensor - zero_point) * scale

    return quantized_tensor

def quantize_model(model, num_bits=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)
        elif isinstance(module, nn.Conv2d):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)

    return model


# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Code cell (cell numver is 7)---
```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha
        self.rank = rank
        self.lora_a = nn.Linear(in_features, rank, bias=False)
        self.lora_b = nn.Linear(rank, out_features, bias=False)
        self.dropout = nn.Dropout(lora_dropout)

    def forward(self, x):
        lora_out =  self.alpha * self.lora_b(self.lora_a(x))
        lora_out = self.dropout(lora_out)
        return lora_out
```

---The following area is a Code cell (cell numver is 8)---
```python
from transformers.models.phi3.modeling_phi3 import (
Phi3RotaryEmbedding, 
# Phi3LongRoPEScaledRotaryEmbedding,
apply_rotary_pos_emb,
repeat_kv
)
```

---The following area is a Code cell (cell numver is 9)---
```python
class Phi3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta
        self.rope_scaling = config.rope_scaling
        self.is_causal = True

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)
        self._init_rope()
        
        ########################## LoRA adapters ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)
        ########################## LoRA adapters ##########################
        
    def _init_rope(self):
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         logger.warning_once("You are not running the flash-attention implementation, expect numerical differences.")

        bsz, q_len, _ = hidden_states.size()
        ########################## LoRA adapters ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)
        ########################## LoRA adapters ##########################
        query_pos = self.num_heads * self.head_dim
        query_states = qkv[..., :query_pos]
        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights += causal_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        ########################## LoRA adapters ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)
        ########################## LoRA adapters ##########################
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
```

---The following area is a Code cell (cell numver is 10)---
```python
def replace_attention_module(config,layer,layer_idx):
    if hasattr(layer, 'self_attn') and layer_idx > 12:

        new_attention = Phi3Attention(config,layer_idx)

        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention
```

---The following area is a Code cell (cell numver is 11)---
```python
loss_fn = nn.CrossEntropyLoss()

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # config ì¶”ê°€
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # config ì„¤ì •
        self.peft_model = lora_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.config.hidden_size, 3)
#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)
#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state.mean(dim =1)
        output_dropout = self.dropout(pooled_output)
        logits = self.classifier(output_dropout)
        loss = None
        if labels is not None:
          labels = labels
          loss = loss_fn(logits, labels)
        return loss, logits
```

---The following area is a Code cell (cell numver is 12)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
len(test)
```

---The following area is a Code cell (cell numver is 13)---
```python
import json
test["prompt"] = test["prompt"].apply(lambda x: json.loads(x)[0])
test["response_a"] = test["response_a"].apply(lambda x: json.loads(x)[0])
test["response_b"] = test["response_b"].apply(lambda x: json.loads(x)[0])
```

---The following area is a Code cell (cell numver is 14)---
```python
test_0 = test[:len(test)//2].reset_index(drop=True)
test_1 = test[len(test)//2:].reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 15)---
```python
from torch.cuda.amp import autocast

def infer(model, dataloader, device):
#     model = nn.DataParallel(model)  # Wrap the model with DataParallel
#     model.to(device)
    model.eval()

    target_list = []

    for batch in dataloader:
        with torch.no_grad():
            with autocast():
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                _,logits = model(input_ids=input_ids, attention_mask=attention_mask)
                softmax_logits = torch.nn.functional.softmax(logits, dim=1)
                target_list.append(softmax_logits)

    return target_list
```

---The following area is a Code cell (cell numver is 16)---
```python
from threading import Thread

gpu0 = "cuda:0"
gpu1 = "cuda:1"
```

---The following area is a Code cell (cell numver is 17)---
```python
model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  ,device_map="cpu")
model0 = quantize_model(model0)
for idx, layer in enumerate(model0.layers):
    replace_attention_module(model0.config,layer,idx)
model0 = LoraModelForClassification(model0)

model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  ,device_map="cpu")
model1 = quantize_model(model1)
for idx, layer in enumerate(model1.layers):
    replace_attention_module(model1.config,layer,idx)
model1 = LoraModelForClassification(model1)


model0.load_state_dict(torch.load(model_path))
model1.load_state_dict(torch.load(model_path))
model0.to(gpu0)
model1.to(gpu1)
```

---The following area is a Code cell (cell numver is 18)---
```python
tokenizer0 = AutoTokenizer.from_pretrained(model_name)

if tokenizer0.pad_token is None:
    tokenizer0.pad_token = tokenizer0.eos_token
tokenizer0.padding_side = "right"  # Fix weird overflow issue with fp16 training

tokenizer1 = AutoTokenizer.from_pretrained(model_name)

if tokenizer1.pad_token is None:
    tokenizer1.pad_token = tokenizer1.eos_token
tokenizer1.padding_side = "right"  # Fix weird overflow issue with fp16 training

test_dataloader0 = create_dataloaders(test_0,tokenizer0,test_max_len,test_batch_size, shuffle = False)
test_dataloader1 = create_dataloaders(test_1,tokenizer1,test_max_len,test_batch_size, shuffle = False)


```

---The following area is a Code cell (cell numver is 19)---
```python
def run_inference(model, dataloader, device, results, index):
    results[index] = infer(model, dataloader, device)

results = {}

process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results,0))
process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results,1))

# Start the processes
process0.start()
process1.start()

# Wait for both processes to finish
process0.join()
process1.join()
```

---The following area is a Code cell (cell numver is 20)---
```python
device = 'cuda:0'  # ì´ë™í•  ìž¥ì¹˜ ì„ íƒ
for k, v in results.items():
    for i in range(len(v)):
        results[k][i] = v[i].to(device)

# ë”•ì…”ë„ˆë¦¬ì˜ ê°’ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)
```

---The following area is a Code cell (cell numver is 21)---
```python
sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 22)---
```python
df_list = []
for tensor in target_list:
    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
    df_list.append(df)

combined_df = pd.concat(df_list, axis=0, ignore_index=True)

sub = sub.set_index(pd.Index(combined_df.index))

final_df = pd.concat([sub[['id']], combined_df], axis=1)
```

---The following area is a Code cell (cell numver is 23)---
```python
def delete_files_and_folders(path):
    # ê²½ë¡œê°€ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(path):
        print(f"Error: {path} does not exist.")
        return

    # ê²½ë¡œ ë‚´ì˜ ëª¨ë“  íŒŒì¼ ë° í´ë”ë¥¼ íƒìƒ‰
    for root, dirs, files in os.walk(path, topdown=False):
        # íŒŒì¼ ì‚­ì œ
        for name in files:
            if name == "submission.csv":
                print(f"Skipping file: {os.path.join(root, name)}")
                continue
            file_path = os.path.join(root, name)
            print(f"Deleting file: {file_path}")
            os.remove(file_path)

#         # í´ë” ì‚­ì œ
#         for name in dirs:
#             folder_path = os.path.join(root, name)
#             print(f"Deleting folder: {folder_path}")
#             shutil.rmtree(folder_path)

    print(f"All files and folders in {path} have been deleted.")

# ì˜ˆì œ ê²½ë¡œ
path_to_delete = "/kaggle/working/"

# íŒŒì¼ ë° í´ë” ì‚­ì œ í•¨ìˆ˜ í˜¸ì¶œ
delete_files_and_folders(path_to_delete)
```

---The following area is a Code cell (cell numver is 24)---
```python
final_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 25)---
```python
final_df.head()
```

---The following area is a Code cell (cell numver is 26)---
```python
# GPU ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°
def clear_gpu_memory():
    torch.cuda.empty_cache()
    gc.collect()

# í•™ìŠµ í›„ GPU ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°
clear_gpu_memory()
```

** @@@ Jupyter Notebook numver 25, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# Library
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import log_loss
import lightgbm as lgb
import optuna
```

---The following area is a Code cell (cell numver is 1)---
```python
# Load the data
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

print(train_data.head())
print(train_data.info())
print(train_data.describe())

```

---The following area is a Code cell (cell numver is 2)---
```python
# Preprocessing data
# Handling missing values if any
train_data.fillna('', inplace=True)
test_data.fillna('', inplace=True)

# Combine text data for vectorization
train_data['combined_text'] = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']
test_data['combined_text'] = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']

# Vectorize the text data
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(train_data['combined_text'])
X_test = vectorizer.transform(test_data['combined_text'])

# Extract the target variable
train_data['winner'] = np.where(train_data['winner_model_a'] == 1, 0, np.where(train_data['winner_model_b'] == 1, 1, 2))
y_train = train_data['winner']

```

---The following area is a Code cell (cell numver is 3)---
```python
# Split the data into training and validation sets
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Convert to LightGBM Dataset
train_data_lgb = lgb.Dataset(X_train_split, label=y_train_split)
val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)

# Optuna objective function for tuning
def objective(trial):
    params = {
        'feature_pre_filter': False,
        'objective': 'multiclass',
        'num_class': 3,
        'metric': 'multi_logloss',
        'boosting': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.25),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 30, 100),
    }
    
    model = lgb.train(params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])
    
    y_val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)
    loss = log_loss(y_val, y_val_pred_proba)
    return loss

# Run Optuna for hyperparameter tuning
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=20)

# Retrieve the best hyperparameters
best_params = study.best_trial.params
best_params.update({'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss', 'boosting': 'gbdt'})

# Train the final model with the best hyperparameters
final_model = lgb.train(best_params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])

```

---The following area is a Code cell (cell numver is 4)---
```python
# Predict probabilities for test set
test_pred_proba = final_model.predict(X_test, num_iteration=final_model.best_iteration)

# Create a submission file
submission = pd.DataFrame(test_pred_proba, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission['id'] = test_data['id']
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 26, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 3)---
```python
train_data.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
train_data_processed = train_data.drop('id', axis = 1)
test_data_processed = test_data.drop('id', axis = 1)
```

---The following area is a Code cell (cell numver is 5)---
```python
train_data_processed = train_data_processed.drop(['model_a', 'model_b'], axis = 1)
```

---The following area is a Code cell (cell numver is 6)---
```python
train_data_processed['winner'] = train_data_processed[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).apply(lambda x: {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}[x])
train_data_processed.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data_processed
```

---The following area is a Code cell (cell numver is 8)---
```python
!pip install transformers
!pip install torch
```

---The following area is a Code cell (cell numver is 9)---
```python
import torch
from tqdm import tqdm
```

---The following area is a Code cell (cell numver is 10)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---The following area is a Code cell (cell numver is 11)---
```python
from transformers import BertTokenizer, BertModel
```

---The following area is a Code cell (cell numver is 12)---
```python
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# model = BertModel.from_pretrained('bert-base-uncased')

# tokenizer.save_pretrained('./bert-base-uncased')
# model.save_pretrained('./bert-base-uncased')
```

---The following area is a Code cell (cell numver is 13)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')
model = BertModel.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')
model.to(device)
```

---The following area is a Code cell (cell numver is 14)---
```python
def get_bert_embeddings_batch(text_list, batch_size=32):
    embeddings = []
    for i in tqdm(range(0, len(text_list), batch_size)):
        batch_texts = text_list[i:i+batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU if available
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move embeddings back to CPU
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)
```

---The following area is a Code cell (cell numver is 15)---
```python
train_prompt_embeddings = get_bert_embeddings_batch(train_data_processed['prompt'].tolist())
train_response_a_embeddings = get_bert_embeddings_batch(train_data_processed['response_a'].tolist())
train_response_b_embeddings = get_bert_embeddings_batch(train_data_processed['response_b'].tolist())
```

---The following area is a Code cell (cell numver is 16)---
```python
test_prompt_embeddings = get_bert_embeddings_batch(test_data_processed['prompt'].tolist())
test_response_a_embeddings = get_bert_embeddings_batch(test_data_processed['response_a'].tolist())
test_response_b_embeddings = get_bert_embeddings_batch(test_data_processed['response_b'].tolist())
```

---The following area is a Code cell (cell numver is 17)---
```python
train_embeddings = np.hstack([train_prompt_embeddings, train_response_a_embeddings, train_response_b_embeddings])
test_embeddings = np.hstack([test_prompt_embeddings, test_response_a_embeddings, test_response_b_embeddings])
```

---The following area is a Code cell (cell numver is 18)---
```python
train_data_processed['prompt_embedding'] = list(train_embeddings[:, :768])
train_data_processed['response_a_embedding'] = list(train_embeddings[:, 768:1536])
train_data_processed['response_b_embedding'] = list(train_embeddings[:, 1536:2304])

test_data_processed['prompt_embedding'] = list(test_embeddings[:, :768])
test_data_processed['response_a_embedding'] = list(test_embeddings[:, 768:1536])
test_data_processed['response_b_embedding'] = list(test_embeddings[:, 1536:2304])
```

---The following area is a Code cell (cell numver is 19)---
```python
train_data_processed
```

---The following area is a Code cell (cell numver is 20)---
```python
X = train_embeddings
y = train_data_processed['winner']
```

---The following area is a Code cell (cell numver is 21)---
```python
X
```

---The following area is a Code cell (cell numver is 22)---
```python
y
```

---The following area is a Code cell (cell numver is 23)---
```python
from sklearn.model_selection import train_test_split
```

---The following area is a Code cell (cell numver is 24)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)
```

---The following area is a Code cell (cell numver is 25)---
```python
pip install catboost
```

---The following area is a Code cell (cell numver is 26)---
```python
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score
```

---The following area is a Code cell (cell numver is 27)---
```python
model = CatBoostClassifier(
    iterations = 1460,
    learning_rate = 0.01,
    depth = 7,
    loss_function = 'MultiClass',
    eval_metric = 'Accuracy',
    random_seed = 0,
    task_type = 'GPU',
    verbose = 100
)
```

---The following area is a Code cell (cell numver is 28)---
```python
model.fit(X_train, y_train, eval_set=(X_val, y_val))
```

---The following area is a Code cell (cell numver is 29)---
```python
val_preds = model.predict(X_val)
val_preds_class = val_preds.argmax(axis=1)

accuracy = accuracy_score(y_val, val_preds_class)
print(f'Validation Accuracy: {accuracy:.4f}')
```

---The following area is a Code cell (cell numver is 30)---
```python
X_test = test_embeddings
```

---The following area is a Code cell (cell numver is 31)---
```python
test_preds_prob = model.predict_proba(X_test)
```

---The following area is a Code cell (cell numver is 32)---
```python
submission = pd.DataFrame({
    'id': test_data['id'],
    'prob_winner_model_a': test_preds_prob[:, 0],
    'prob_winner_model_b': test_preds_prob[:, 1],
    'prob_winner_tie': test_preds_prob[:, 2]
})

submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 33)---
```python

```

** @@@ Jupyter Notebook numver 27, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "tensorflow"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import transformers
```

---The following area is a Code cell (cell numver is 2)---
```python
print("TensorFlow:", tf.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)
```

---The following area is a Code cell (cell numver is 3)---
```python
class CFG:
    seed = 50  # Random seed
    sequence_length = 1024  # Input sequence length
    epochs = 2 # Training epochs
    batch_size = 2  # Batch size
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Code cell (cell numver is 4)---
```python
keras.utils.set_random_seed(CFG.seed)
keras.mixed_precision.set_global_policy("bfloat16")
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Code cell (cell numver is 5)---
```python
def load_data(row):
  row_list = json.loads(row)
  return " ".join(row for row in row_list if row != None)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv')
# df = df.iloc[:100] #using subset of data for demo

# Take the first prompt and its associated response
df["prompt"] = df["prompt"].apply(load_data)
df["response_a"] = df["response_a"].apply(load_data)
df["response_b"] = df["response_b"].apply(load_data)

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Code cell (cell numver is 7)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df["prompt"].apply(load_data)
test_df["response_a"] = test_df["response_a"].apply(load_data)
test_df["response_b"] = test_df["response_b"].apply(load_data)

# Show Sample
test_df.head()
```

---The following area is a Code cell (cell numver is 8)---
```python
df['options'] = df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(df.head(2))  # Display the first 2 rows of df

test_df['options'] = test_df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Code cell (cell numver is 9)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df["class_label"])
```

---The following area is a Code cell (cell numver is 10)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    "deberta_v3_small_en",
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
    dtype="bfloat16",
)
```

---The following area is a Code cell (cell numver is 11)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Code cell (cell numver is 12)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available

```

---The following area is a Code cell (cell numver is 13)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Code cell (cell numver is 14)---
```python
# import jax
# devices = jax.devices("gpu")
# print("devices", devices)

# data_parallel = keras.distribution.DataParallel(devices=devices)
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_data():
    train_texts = train_df.options.tolist()  # Extract training texts
    train_labels = train_df.class_label.tolist()  # Extract training labels
    train_ds = build_dataset(train_texts, train_labels,
                             batch_size=CFG.batch_size,
                             shuffle=True)

    # Valid
    valid_texts = valid_df.options.tolist()  # Extract validation texts
    valid_labels = valid_df.class_label.tolist()  # Extract validation labels
    valid_ds = build_dataset(valid_texts, valid_labels,
                             batch_size=CFG.batch_size,
                             shuffle=False)

    # Build test dataset
    test_texts = test_df.options.tolist()
    test_ds = build_dataset(test_texts,
                             batch_size=min(len(test_df), CFG.batch_size),
                             shuffle=False)
    return train_ds, valid_ds, test_ds
```

---The following area is a Code cell (cell numver is 16)---
```python
# keras.distribution.set_distribution(data_parallel) #replicate model on both GPUs
```

---The following area is a Code cell (cell numver is 17)---
```python
def get_backbone():
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        "deberta_v3_small_en",
        dtype="bfloat16",
    )
    backbone.enable_lora(8)
    
    return backbone
```

---The following area is a Code cell (cell numver is 18)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                  monitor='val_log_loss',
                                  save_best_only=True,
                                  save_weights_only=True,
                                  mode='min')  # Get Model checkpoint callback
```

---The following area is a Code cell (cell numver is 19)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Code cell (cell numver is 20)---
```python
def get_model(backbone):
    inputs = {
        "token_ids": keras.Input(shape=(2, None), dtype=tf.int16, name="token_ids"),
        "padding_mask": keras.Input(shape=(2, None), dtype=tf.int16, name="padding_mask"),
    }

    # Compute embeddings for first response: (P + R_A) using backbone
    response_a = {k: v[:, 0, :] for k, v in inputs.items()}
    embed_a = backbone(response_a)

    # Compute embeddings for second response: (P + R_B), using the same backbone
    response_b = {k: v[:, 1, :] for k, v in inputs.items()}
    embed_b = backbone(response_b)

    # Compute final output
    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
    embeds = keras.layers.GlobalAveragePooling1D()(embeds)
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
    model = keras.Model(inputs, outputs)

    # Compile the model with optimizer, loss, and metrics
    model.compile(
        optimizer=keras.optimizers.Adam(5e-6),
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
        metrics=[
            log_loss,
            keras.metrics.CategoricalAccuracy(name="accuracy"),
        ],
    )
    
    return model
```

---The following area is a Code cell (cell numver is 21)---
```python
strategy = tf.distribute.MirroredStrategy()
# Open a strategy scope.

with strategy.scope():
    
    train_ds, valid_ds, test_ds = get_data()
    
    backbone = get_backbone()
    
    model = get_model(backbone)
    
    history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[ckpt_cb]
    )
```

---The following area is a Code cell (cell numver is 22)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Code cell (cell numver is 23)---
```python
# Make predictions using the trained model on test data
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Code cell (cell numver is 24)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# ðŸ“Œ | Reference

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
* [LMSYS: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 28, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np
import pandas as pd

from sklearn.metrics import log_loss
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

df['response_a_len'] = df['response_a'].apply(lambda x: len(x))
df['response_b_len'] = df['response_b'].apply(lambda x: len(x))

df["ratio_response_b_response_a"] = df['response_b_len'] / df['response_a_len']

# This value is computed from train dataset
avg_1 = {'winner_model_a': 0.2550469811145223, 'winner_model_b': 0.4547399758117034, 'winner_tie': 0.2902130430737743}
avg_2 = {'winner_model_a': 0.31478770131771594, 'winner_model_b': 0.33601756954612005, 'winner_tie': 0.349194729136164}
avg_3 = {'winner_model_a': 0.45816715010877446, 'winner_model_b': 0.25086113125453224, 'winner_tie': 0.29097171863669324}


# The below threshold is also computed from the train datset
def assign_prob(row):
    if row['ratio_response_b_response_a'] <= 0.838296:
        return avg_3.values()
    elif row['ratio_response_b_response_a'] <= 1.004711:
        return avg_2.values()
    else:
        return avg_1.values()



df[['winner_model_a', 'winner_model_b', 'winner_tie']] = df.apply(
    lambda x: assign_prob(x), axis=1, result_type='expand'
)

```

---The following area is a Code cell (cell numver is 1)---
```python

submission_df = pd.DataFrame(
    {
        'id': df['id'],
        'winner_model_a': df['winner_model_a'],
        'winner_model_b': df['winner_model_b'],
        'winner_tie': df['winner_tie']
    }
)
submission_df.to_csv("submission.csv", index=False)
submission_df
```

** @@@ Jupyter Notebook numver 29, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSys Chatbot Arena

Propose, document and defend a solution that can determine which answer is better, or if there is a tie, based on criteria you define..  


**Proposal**: Boosting is a machine learning technique that combines the results of several weak model trainings to create a strong model. The process uses a sequence of interactions, where weights are used for the errors obtained in each training session.

**Example**: XGBoost is a machine learning algorithm based on gradient boosting and using decision trees, where each tree tries to correct the errors of the previous tree.

**Features**: For this problem, we will use features extracted from the texts in our dataset.

In this case, we can use exploratory analysis to establish features add to our classification, such as:
- Size of the texts;
- Words present in the question and answears;
- Difference between answears, etc.

## 1 - Pre-processing
```

---The following area is a Code cell (cell numver is 1)---
```python
# Librarys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = 14,5

import seaborn as sns
sns.set_style = 'whitegrid'

import plotly.express as px
px.defaults.template = "plotly_dark"

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

# Classification models
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

from gensim.models import Word2Vec
import spacy
nlp = spacy.load('en_core_web_sm')

import re
import string

from warnings import filterwarnings
filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 2)---
```python
# loading datasets
df_train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
df_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
df_submission = df_test.copy()
sample_example = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
df_train.head()
```

---The following area is a Code cell (cell numver is 3)---
```python
# Sample example visualization
sample_example.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
# train dataset
df_test.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
# Dataset's shape
print(f'Train dataset shape: {df_train.shape}')
print(f'Test dataset shape: {df_test.shape}')
```

---The following area is a Code cell (cell numver is 6)---
```python
# Dataset info
df_train.info()
```

---The following area is a Code cell (cell numver is 7)---
```python
# Null values train
df_train.isnull().sum()
```

---The following area is a Code cell (cell numver is 8)---
```python
# Null values test
df_test.isnull().sum()
```

---The following area is a Code cell (cell numver is 9)---
```python
# Text Preprocessing 
def preprocess(string):
    strip = string.strip('[]')
    parts = [x.strip('"') for x in strip.split('","')]
    return ''.join(parts)
columns = ['prompt','response_a','response_b']

for colum in df_train[columns]:
    df_train[colum] = df_train[colum].apply(preprocess)
    
for colum in df_test[columns]:
    df_test[colum] = df_test[colum].apply(preprocess)
    
df_train.head(2)
```

---The following area is a Code cell (cell numver is 10)---
```python
# create new columns for data exploration
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

def class_label(df):
    df['class_label'] = None
    
    for index, row in df.iterrows():
        for col in columns:
            if row[col] == 1:
                df.at[index, 'class_label'] = col
                break

# Call the function on your DataFrame
class_label(df_train)
```

---The following area is a Code cell (cell numver is 11)---
```python
# Define class number
class_number = {'winner_model_a':0 ,'winner_model_b':1,'winner_tie':2}

df_train['class'] = df_train['class_label'].map(class_number)
df_train.head(2)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Which model was chosen
def chose_model(row):
    if row['class'] == 0:
        return row['model_a']
    elif row['class'] == 1:
        return row['model_b']
    else:
        return 'tie'

df_train['chose_model'] = df_train.apply(chose_model, axis=1)
df_train.head(2)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## 2 - Data Exploration
```

---The following area is a Code cell (cell numver is 14)---
```python
df_train[['winner_model_a','winner_model_b','winner_tie']].mean()
```

---The following area is a Code cell (cell numver is 15)---
```python
# Analysing class distribuitions
# Objective: is the base balanced?
fig = px.pie(df_train[['winner_model_a','winner_model_b','winner_tie']].mean(), names=df_train['class_label'], title='Class distribuition')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 16)---
```python
# Models distribuitions
# Objective: The winningest models in the comparison
df_models = df_train.groupby('chose_model')['class'].count().reset_index().sort_values(by='class',ascending=False)
df_models.rename(columns={'chose_model':'model'}, inplace=True)

# Results Graphs
fig = px.bar(df_models[1:11].sort_values(by='class'), y='model', x='class', title='Top 10 Most Chosen Models', text_auto='.3s')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 17)---
```python
# Models chosen from those mentioned
# Objective: The most of these models
df_uni1 = df_train.groupby('model_a')['id'].count().reset_index().sort_values('id', ascending=False)
df_uni1.rename(columns={'model_a':'model'}, inplace=True)
df_uni2 = df_train.groupby('model_b')['id'].count().reset_index().sort_values('id', ascending=False)
df_uni2.rename(columns={'model_b':'model'}, inplace=True)

df_uni = df_uni1.merge(df_uni2, how='left', on = 'model')
df_uni['total_mentions'] = df_uni['id_x'] + df_uni['id_y']
mentions = df_uni['total_mentions'].sum()
df_uni['%mentions'] = (df_uni['total_mentions'] / mentions) * 100

df_uni = df_uni.merge(df_models, how='left', on = 'model')
df_uni['chose_in_mentions'] = (df_uni['class'] / df_uni['total_mentions']) * 100
df_uni = df_uni.sort_values(by = 'chose_in_mentions', ascending=False)[:10]
df_uni.head(3)
```

---The following area is a Code cell (cell numver is 18)---
```python
# Results - Graphs
fig = px.bar(df_uni.sort_values('chose_in_mentions'), y='model', x='chose_in_mentions', title='% Top 10 Most Chosen Models',
             text_auto='.3s')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## 3 Modeling
```

---The following area is a Code cell (cell numver is 20)---
```python
%%time

df = df_train.copy()

# Word Processing
stopwords_list = stopwords.words('english')

def word_processing(text):
    '''Process text: remove special characters, convert to lowercase, tokenize, and remove stopwords'''
    text = re.sub(r'[/<>()|\+\-\$%&#@\'\"]+', ' ', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)  # Tokenize the text
    filtered_tokens = [token for token in tokens if token not in stopwords_list]  # Remove stopwords
    return filtered_tokens

# Features engineering
def count_token(tokens):
    '''Count number of tokens'''
    return len(tokens)

def diff_response(tokens1, tokens2):
    '''Difference in token count between responses'''
    len_1 = len(tokens1)
    len_2 = len(tokens2)
    diff = abs(len_1 - len_2)
    return len_1, len_2, diff

def prompt_u_response(text1, text2):
    '''Common words between the prompt and the responses'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)
    return len(intersec)

def aUb(text1, text2):
    '''Common words between the responses'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)
    return len(intersec)

def lexical_diversity(text):
    '''Proportion of unique words'''
    return len(set(text)) / len(text) if text else 0

def avg_words_per_sentence(text):
    '''Calculate average number of words per sentence'''
    sentences = sent_tokenize(text)
    word_count = sum(len(word_tokenize(sentence)) for sentence in sentences)
    return word_count / len(sentences) if sentences else 0

def sentence_diversity(text):
    '''Calculate sentence diversity (variety in sentence length)'''
    sentences = sent_tokenize(text)
    lengths = [len(word_tokenize(sentence)) for sentence in sentences]
    return len(set(lengths)) / len(lengths) if lengths else 0

def Simylarity(text1, text2):
    '''Calculate the similaty between prompt and responses'''
    tokenA = nlp(text1)
    tokenB = nlp(text2)
    return tokenA.similarity(tokenB)

df['similaty_promptUresponse_a'] = df.apply(lambda row: Simylarity(row['prompt'],row['response_a']), axis=1)
df['similaty_promptUresponse_b'] = df.apply(lambda row: Simylarity(row['prompt'],row['response_b']), axis=1)


# Applying word processing to columns
for col in ['prompt', 'response_a', 'response_b']:
    df[col] = df[col].apply(word_processing)
    df[f'{col}_count_token'] = df[col].apply(count_token)

# Applying additional features
for index, row in df.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)
        
        df.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)
        df.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)
        #df.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)
        df.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)
        
        # Existing features
        df.at[index, f'{col}_count_token'] = count_token(tokens)

# Applying diferences functions in the train dataframe 
for index, row in df.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)
    response_b_text = ' '.join(response_b_tokens)
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)
    df.at[index, 'response_len_a'] = len_a
    df.at[index, 'response_len_b'] = len_b
    df.at[index, 'response_diff'] = diff
    
    common_words_ab = aUb(response_a_text, response_b_text)
    df.at[index, 'common_words_ab'] = common_words_ab
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)
    df.at[index, 'common_words_prompt_a'] = common_words_prompt_a
    df.at[index, 'common_words_prompt_b'] = common_words_prompt_b
    

df.head(2)
```

---The following area is a Code cell (cell numver is 21)---
```python
df[['similaty_promptUresponse_a','similaty_promptUresponse_b']].describe().T
```

---The following area is a Code cell (cell numver is 22)---
```python
%%time

# features and target selection
X = df.drop(['id','model_a','model_b','prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie',
            'class_label','chose_model','class'], axis=1).values
y = df['class'].values

# Split train and valid
X_train,X_valid,y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)

# Models use
models = {
    'XGBoost': XGBClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(random_state=42,verbose=False)
}

# Trainning the models
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


# Metrics
result_loss = []
result_models = []

for model_name, model in models.items():
    i = 0
    print(f'Model {model_name}')

    model_losses = []  # Loss Models List
    
    for train_index, valid_index in skf.split(X, y):
        X_train_fold, X_test_fold = X[train_index], X[valid_index]
        y_train_fold, y_test_fold = y[train_index], y[valid_index]
        
        # Training the model
        model.fit(X_train_fold, y_train_fold)
        
        # Metrics evaluation
        y_test_pred_proba = model.predict_proba(X_test_fold)
        loss = metrics.log_loss(y_test_fold, y_test_pred_proba)
        model_losses.append(loss)  # Add loss in list
        
        # Display the results
        print(f'Fold {i} | Log Loss: {loss}')
        i += 1
        
        mean_loss = np.mean(model_losses)
        result_loss.append(model_losses)
        result_models.append((model_name, model_losses))

    print(f'Mean Loss for model {model_name} is {mean_loss}')
    print('---' * 30)
```

---The following area is a Code cell (cell numver is 23)---
```python
# Plot results
data_dict = {'CV': list(range(0, 5))}

for model, values in result_models:
    data_dict[model] = values

results_df = pd.DataFrame(data_dict)

fig = px.line(results_df, x='CV', y=['XGBoost','GradientBoosting','CatBoost'], title='Loss Models')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 24)---
```python
%%time
# The best model are GradientBoosting
# Now, we'll use the gridsearch for tunning hyperparametros
model_grad = GradientBoostingClassifier()
params = {
    'n_estimators':[100, 150, 170],
    'max_depth': [1, 3, 5],
    'learning_rate': [0.1, 0.01]
}

grid = GridSearchCV(model_grad, params, n_jobs=-1, cv=5, scoring='neg_log_loss', verbose=False)
grid.fit(X_train, y_train)
```

---The following area is a Code cell (cell numver is 25)---
```python
print(f'The best params {grid.best_params_}')
```

---The following area is a Code cell (cell numver is 26)---
```python
# Selection the best model
best_model = grid.best_estimator_
```

---The following area is a Code cell (cell numver is 27)---
```python
# Avaluation the best model
y_predict_valid_prob = best_model.predict_proba(X_valid)

logloss = metrics.log_loss(y_valid, y_predict_valid_prob)

print(f'LogLoss the Best Models with GridSearch: {logloss}')
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
There was no significant improvement with GridSearch
```

---The following area is a Code cell (cell numver is 29)---
```python
# Features importance
features_importance = best_model.feature_importances_
features_names = ['prompt_count_token', 'response_a_count_token',
       'response_b_count_token', 'response_a_lexical_diversity',
       'response_a_avg_words_per_sentence', 'response_a_sentence_diversity',
       'response_b_lexical_diversity', 'response_b_avg_words_per_sentence',
       'response_b_sentence_diversity', 'response_len_a', 'response_len_b',
       'response_diff', 'common_words_ab', 'common_words_prompt_a',
       'common_words_prompt_b','similaty_promptUresponse_a', 'similaty_promptUresponse_b']

fig = px.bar(y=features_names, x=features_importance, color=features_importance, title='Features Importance')
scale = px.colors.sequential.BuGn
fig.update_traces(marker=dict(colorscale=scale))
fig.update_layout(width=800, height=500)
fig.show()
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
Above we can see the most important features for the model
```

---The following area is a Code cell (cell numver is 31)---
```python
# Preparing the test data to predict
df_test['similaty_promptUresponse_a'] = df_test.apply(lambda row: Simylarity(row['prompt'],row['response_a']), axis=1)
df_test['similaty_promptUresponse_b'] = df_test.apply(lambda row: Simylarity(row['prompt'],row['response_b']), axis=1)



# Applying word processing to columns
for col in ['prompt', 'response_a', 'response_b']:
    df_test[col] = df_test[col].apply(word_processing)
    df_test[f'{col}_count_token'] = df_test[col].apply(count_token)

# Applying additional features
for index, row in df_test.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)
        
        df_test.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)
        df_test.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)
        #df.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)
        df_test.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)
        
        # Existing features
        df_test.at[index, f'{col}_count_token'] = count_token(tokens)


# Applying diferences functions in the test dataframe 
for index, row in df_test.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)
    response_b_text = ' '.join(response_b_tokens)
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)
    df_test.at[index, 'response_len_a'] = len_a
    df_test.at[index, 'response_len_b'] = len_b
    df_test.at[index, 'response_diff'] = diff
    
    common_words_ab = aUb(response_a_text, response_b_text)
    df_test.at[index, 'common_words_ab'] = common_words_ab
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)
    df_test.at[index, 'common_words_prompt_a'] = common_words_prompt_a
    df_test.at[index, 'common_words_prompt_b'] = common_words_prompt_b

df_test.drop(columns = ['id','prompt','response_a','response_b'], axis=1, inplace=True)
df_test.head()
```

---The following area is a Code cell (cell numver is 32)---
```python
# Predict proba in test
y_sub_proba = best_model.predict_proba(df_test)
y_sub_proba
```

---The following area is a Code cell (cell numver is 33)---
```python
# Submission df
submission = pd.DataFrame({
    'id':df_submission['id'],
    'winner_model_a': y_sub_proba[:, 0],
    'winner_model_b': y_sub_proba[:, 1],
    'winner_tie': y_sub_proba[:, 2]
})

submission.head()
```

---The following area is a Code cell (cell numver is 34)---
```python
submission.to_csv('/kaggle/working/submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 30, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Code cell (cell numver is 1)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Code cell (cell numver is 2)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Code cell (cell numver is 3)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))
            vectorizer.fit([text1, text2])
            output = vectorizer.transform([text1, text2]).toarray()
            cos_sim = cosine_similarity(output)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n') 
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"] 
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"] 
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"] 
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]
        
        return data
```

---The following area is a Code cell (cell numver is 4)---
```python
%%time
preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Code cell (cell numver is 6)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 7)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))
cv_scores = list()

features = X.columns.tolist()
feat_imp_df = pd.DataFrame({"feature": features})

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed
    )
    
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    val_preds = model.predict_proba(X_val)
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")
    cv_scores.append(val_log_loss)
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances of All Folds", size=12)
plt.show()
```

---The following area is a Code cell (cell numver is 8)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 31, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel
import torch.nn.functional as F
np.random.seed(1337)
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen2-7B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)
```

---The following area is a Code cell (cell numver is 7)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 8)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 9)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 1)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 14300
maxlen = 1024
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
num_samples = INPUT_IDS.shape[0]

# Sá»‘ lÆ°á»£ng máº«u cho X_valid (20% cá»§a X_train)
num_valid_samples = int(num_samples * 0.2)

# XÃ¡o trá»™n cÃ¡c chá»‰ sá»‘ cá»§a X_train
indices = np.random.permutation(num_samples)

# Chá»n 20% chá»‰ sá»‘ Ä‘áº§u tiÃªn lÃ m chá»‰ sá»‘ cho X_valid
valid_indices = indices[:num_valid_samples]

# CÃ¡c chá»‰ sá»‘ cÃ²n láº¡i lÃ m chá»‰ sá»‘ cho X_train
train_indices = indices[num_valid_samples:]

# Táº¡o X_valid vÃ  X_train má»›i tá»« cÃ¡c chá»‰ sá»‘ Ä‘Ã£ chá»n
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=maxlen)
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=maxlen)
y_train = LABELS[train_indices]

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=maxlen)
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=maxlen)
y_valid = LABELS[valid_indices]
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
'''This example demonstrates the use of Convolution1D for text classification.
Gets to 0.88 test accuracy after 2 epochs.
90s/epoch on Intel i5 2.4Ghz CPU.
10s/epoch on Tesla K40 GPU.
'''
from tensorflow.keras.layers import Layer
from keras.layers import Concatenate
from keras.layers import  GlobalMaxPooling1D
import tensorflow as tf

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})


# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        embeddings, attention_mask = inputs
        return embeddings * tf.expand_dims(attention_mask, -1)

model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')
attention_masks = Input(shape=(maxlen,), dtype='float32', name="attention_masks")

emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])
def max_1d(X):
    return K.max(X, axis=1)

# we add a Convolution1D, which will learn nb_filter
# word group filters of size 3:

con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size =3,
                    strides=1)(masked_embeddings)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 4:

con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 5:

con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)


cnn_layer =Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])


#LSTM


x = masked_embeddings
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)
output_layer= Dropout(0.2)(dense_layer)
output_layer = Dense(3, trainable=True,activation='softmax')(output_layer)




model = Model(inputs=[input_layer, attention_masks], outputs=[output_layer])
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
adamax = Adamax(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.summary()

```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 21)---
```python
from keras.callbacks import EarlyStopping
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)

model.fit([X_train,X_train_attention], y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint, early_stopping],
          validation_data=([X_valid,X_valid_attention], y_valid))
```

---The following area is a Code cell (cell numver is 22)---
```python
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.save('model_LSTM_mix_CNN.keras')
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Test Model
```

---The following area is a Code cell (cell numver is 24)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 25)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 26)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 27)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 28)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=maxlen)
```

---The following area is a Code cell (cell numver is 29)---
```python
test
```

---The following area is a Code cell (cell numver is 30)---
```python
y_predict = model.predict([X_test,X_test_attention])
y_predict
```

---The following area is a Code cell (cell numver is 31)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)
```

---The following area is a Code cell (cell numver is 32)---
```python
result_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 33)---
```python
result_df
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 32, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Inference - llama-3 8b Super Fast ðŸš€
In this notebook we do inference using fine-tuned llama-3 8b model using T4 * 2 Gpu parallel, motivation behind to create this is the huge test size (25k samples). 

Prerequisite: Access to Llama-3, if you have not filled consent form, go [here](https://www.kaggle.com/models/metaresearch/llama-3), fill the form to access llama-3.  

Training notebook can be found [here](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)

Please upvote if you find this helpful!

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType,prepare_model_for_kbit_training
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Code cell (cell numver is 3)---
```python
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'
MAX_LENGTH = 2048
BATCH_SIZE = 4
DEVICE = torch.device("cuda")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))
```

---The following area is a Code cell (cell numver is 6)---
```python
# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:0',
    use_cache=False,
)


base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:1',
    use_cache=False,
)
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
Now, we have sucessfully loaded one model on each GPU!

# Load weights
```

---The following area is a Code cell (cell numver is 12)---
```python
# LoRa configuration

lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    inference_mode=True,
    task_type = 'SEQ_CLS'
)
```

---The following area is a Code cell (cell numver is 13)---
```python
# Get peft
# model_0 = get_peft_model(base_model_0, lora_config).to(device0) 
model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)
# model_0 = model_0.merge_and_unload()
# model_0.config.pad_token_id = tokenizer.pad_token_id
# model_0.config.pretraining_tp = 1
model_0.eval()

# model_1 = get_peft_model(base_model_1, lora_config).to(device1) 
model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)
# model_1 = model_1.merge_and_unload()
# model_0.config.pad_token_id = tokenizer.pad_token_id
# model_0.config.pretraining_tp = 1
model_1.eval()
```

---The following area is a Code cell (cell numver is 14)---
```python
# Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 16)---
```python
import gc
gc.collect()
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}

# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 19)---
```python
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
display(sample_sub)
```

---The following area is a Code cell (cell numver is 20)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :)
```

** @@@ Jupyter Notebook numver 33, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl
!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl
import textstat
```

---The following area is a Code cell (cell numver is 1)---
```python
import sklearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
import nltk
import textstat
from textblob import TextBlob
from collections import Counter

import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings('ignore')
pd.options.display.float_format = '{:.2f}'.format
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)
```

---The following area is a Code cell (cell numver is 2)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 3)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
print(f"The size of the train data: {train.shape} is and the test data is: {test.shape}")
```

---The following area is a Code cell (cell numver is 6)---
```python
print(train['winner_model_a'].value_counts())
print(train['winner_model_b'].value_counts())
print(train['winner_tie'].value_counts())
```

---The following area is a Code cell (cell numver is 7)---
```python
# Create a figure and axes
fig, axes = plt.subplots(3, 1, figsize=(7, 6))

# Columns to plot
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

# Define colors for 0 and 1
colors = {0: 'steelblue', 1: 'salmon'}

# Plot each column in its respective subplot
for i, column in enumerate(columns):
    ax = axes[i]
    value_counts = train[column].value_counts().sort_index()
    
    # Plot bars with specified colors and labels for legend
    bars = ax.bar(value_counts.index.astype(str), value_counts, color=[colors[idx] for idx in value_counts.index],
                  label=value_counts.index.map({0: 'Lose (0)', 1: 'Win (1)'}))
    
    # Annotate counts on bars
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    ax.set_xlabel('Winner')
    ax.set_ylabel('Count')
    ax.set_title(f'Model {column.split("_")[-1].capitalize()} Counts')
    ax.legend(title='Outcome', loc='upper right')

# Add overall title and adjust layout
fig.suptitle('Distribution of Winners Across Models', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

# Display the plot
plt.show()
```

---The following area is a Code cell (cell numver is 8)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)


train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)
```

---The following area is a Code cell (cell numver is 9)---
```python
train.head(3)
```

---The following area is a Code cell (cell numver is 10)---
```python
test.head(3)
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

# Function to compute word count
def word_count(text):
    return len(nltk.word_tokenize(text))

# Function to compute character count
def char_count(text):
    return len(text)

# Function to compute sentence count
def sentence_count(text):
    return len(nltk.sent_tokenize(text))

# Function to compute average word length
def avg_word_length(text):
    words = nltk.word_tokenize(text)
    if len(words) == 0:
        return 0
    return sum(len(word) for word in words) / len(words)

# Function to compute average sentence length
def avg_sentence_length(text):
    words = nltk.word_tokenize(text)
    sentences = nltk.sent_tokenize(text)
    if len(sentences) == 0:
        return 0
    return len(words) / len(sentences)

# Function to compute type-token ratio
def ttr(text):
    words = nltk.word_tokenize(text)
    if len(words) == 0:
        return 0
    unique_words = set(words)
    return len(unique_words) / len(words)

# Function to compute word frequency
def word_freq(text):
    words = nltk.word_tokenize(text)
    return Counter(words)

# Function to compute bigram frequency
def bigram_freq(text):
    words = nltk.word_tokenize(text)
    bigrams = list(nltk.bigrams(words))
    return Counter(bigrams)

# Function to compute readability scores
def readability_scores(text):
    scores = {
        "flesch_kincaid_score": textstat.flesch_kincaid_grade(text),
        "gunning_fog_index": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "ari": textstat.automated_readability_index(text)
    }
    return scores

# Compute additional metrics and add to DataFrame
for column in ["prompt", "response_a", "response_b"]:
    train[f"{column}_word_count"] = train[column].apply(word_count)
    train[f"{column}_char_count"] = train[column].apply(char_count)
    train[f"{column}_sentence_count"] = train[column].apply(sentence_count)
    train[f"{column}_avg_word_length"] = train[column].apply(avg_word_length)
    train[f"{column}_avg_sentence_length"] = train[column].apply(avg_sentence_length)
#     train[f"{column}_ttr"] = train[column].apply(ttr)
#     readability = train[column].apply(readability_scores)
#     train[f"{column}_flesch_kincaid_score"] = readability.apply(lambda x: x["flesch_kincaid_score"])
#     train[f"{column}_gunning_fog_index"] = readability.apply(lambda x: x["gunning_fog_index"])
#     train[f"{column}_smog_index"] = readability.apply(lambda x: x["smog_index"])
#     train[f"{column}_ari"] = readability.apply(lambda x: x["ari"])

train.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

import time
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import log_loss
from scipy.stats import uniform, randint

# Convert the target into a single column with categorical labels
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# Define features and target
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]

X = train[features]
y = train['winner'] - 1

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the models
models = {
    'GradientBoostingClassifier': GradientBoostingClassifier(),
    'XGBClassifier': XGBClassifier()
}

# Define the parameter distributions for random search
param_distributions = {
    'GradientBoostingClassifier': {
        'n_estimators': [100,200,350,300],
        'max_depth': [2,3,4,5,7,9]
    },
    'XGBClassifier': {
        'n_estimators': [100,200,350,300],
        'max_depth': [2,3,4,5,7,9]
    }
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)

best_models = {}  # Dictionary to store the best models

# Iterate over each model
for model_name, model in models.items():
    print(f"Model training for {model_name}")
    
    # Perform RandomizedSearchCV
    random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=10, scoring='neg_log_loss', 
                                       n_jobs=-1, cv=skf, random_state=42)
    random_search.fit(X_train, y_train)
    
    best_model = random_search.best_estimator_
    best_models[model_name] = best_model  # Store the best model for the current type
    
    logloss_scores = []
    start_time = time.time()
    
    count = 0
    for train_index, test_index in skf.split(X, y):
        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

        best_model.fit(X_train_fold, y_train_fold)
        y_test_pred_proba = best_model.predict_proba(X_test_fold)

        logloss = log_loss(y_test_fold, y_test_pred_proba)
        logloss_scores.append(logloss)
        print(f"The log loss score for fold {count}: {logloss}")
        count += 1

    average_logloss = sum(logloss_scores) / len(logloss_scores)
    print(f"The average log loss score for {model_name} across all folds: {average_logloss}")
    
    elapsed_time = time.time() - start_time
    print(f"Time taken for {model_name}: {elapsed_time:.2f} seconds")
    
    # Predict probabilities on the validation set
    y_val_prob = best_model.predict_proba(X_val)
    # Calculate log loss on the validation set
    val_loss = log_loss(y_val, y_val_prob)
    print(f'Log Loss using {model_name} on validation set: {val_loss}')

# Identify the best model based on validation set performance
best_model_name = min(best_models, key=lambda k: log_loss(y_val, best_models[k].predict_proba(X_val)))
best_average_logloss = log_loss(y_val, best_models[best_model_name].predict_proba(X_val))

print(f"The best model is {best_model_name} with an average log loss score of {best_average_logloss}")
```

---The following area is a Code cell (cell numver is 13)---
```python
model_to_use = best_models[best_model_name]
model_to_use
```

---The following area is a Code cell (cell numver is 14)---
```python
# Compute additional metrics and add to DataFrame
for column in ["prompt", "response_a", "response_b"]:
    test[f"{column}_word_count"] = test[column].apply(word_count)
    test[f"{column}_char_count"] = test[column].apply(char_count)
    test[f"{column}_sentence_count"] = test[column].apply(sentence_count)
    test[f"{column}_avg_word_length"] = test[column].apply(avg_word_length)
    test[f"{column}_avg_sentence_length"] = test[column].apply(avg_sentence_length)
    
test.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
test_features = test[features]
test_predictions = model_to_use.predict_proba(test_features)
```

---The following area is a Code cell (cell numver is 16)---
```python
test_predictions
```

---The following area is a Code cell (cell numver is 17)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test['id'],
    'winner_model_a': test_predictions[:, 0],
    'winner_model_b': test_predictions[:, 1],
    'winner_tie': test_predictions[:, 2]
})
```

---The following area is a Code cell (cell numver is 18)---
```python
submission.head()
```

---The following area is a Code cell (cell numver is 19)---
```python
submission.to_csv('/kaggle/working/submission.csv', index= False)
```

** @@@ Jupyter Notebook numver 34, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# reduce 50% data

import pandas as pd

# Load the train.csv file
train_data_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # Update with the correct path
train_data = pd.read_csv(train_data_path)

test_data_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
test_data = pd.read_csv(test_data_path)

# Randomly sample 10% of the data
sampled_train_data = train_data.sample(frac=0.5, random_state=42)

# Save the sampled data if needed
sampled_train_data_path = '/kaggle/working/sample_train.csv'
sampled_train_data.to_csv(sampled_train_data_path, index=False)
```

---The following area is a Code cell (cell numver is 1)---
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Combine prompts and responses for feature extraction
sampled_train_data['text_a'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_a']
sampled_train_data['text_b'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_b']
test_data['text_a'] = test_data['prompt'] + " " + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + " " + test_data['response_b']

# Initialize the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(pd.concat([sampled_train_data['text_a'], sampled_train_data['text_b'], test_data['text_a'], test_data['text_b']]))

# Convert texts to sequences
X_train_a = tokenizer.texts_to_sequences(sampled_train_data['text_a'])
X_train_b = tokenizer.texts_to_sequences(sampled_train_data['text_b'])
X_test_a = tokenizer.texts_to_sequences(test_data['text_a'])
X_test_b = tokenizer.texts_to_sequences(test_data['text_b'])

# Pad sequences to ensure equal length
max_length = max(max(len(seq) for seq in X_train_a), max(len(seq) for seq in X_train_b))
X_train_a = pad_sequences(X_train_a, maxlen=max_length, padding='post')
X_train_b = pad_sequences(X_train_b, maxlen=max_length, padding='post')
X_test_a = pad_sequences(X_test_a, maxlen=max_length, padding='post')
X_test_b = pad_sequences(X_test_b, maxlen=max_length, padding='post')

# Extract targets
y_train_a = sampled_train_data['winner_model_a']
y_train_b = sampled_train_data['winner_model_b']
```

---The following area is a Code cell (cell numver is 2)---
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Define the LSTM model
def create_lstm_model(input_length):
    model = Sequential()
    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))
    model.add(LSTM(128))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

input_length = X_train_a.shape[1]

# Train model for text_a
model_a = create_lstm_model(input_length)
model_a.fit(X_train_a, y_train_a, epochs=5, batch_size=64, validation_split=0.7)
```

---The following area is a Code cell (cell numver is 3)---
```python
# Train model for text_b
model_b = create_lstm_model(input_length)
model_b.fit(X_train_b, y_train_b, epochs=5, batch_size=64, validation_split=0.7)
```

---The following area is a Code cell (cell numver is 4)---
```python
import numpy as np

# Make predictions on the test set
test_pred_a = model_a.predict(X_test_a).flatten()
test_pred_b = model_b.predict(X_test_b).flatten()

# Calculate probabilities for tie (assuming uniform distribution for simplicity)
test_pred_tie = np.full(test_pred_a.shape, 1/3)
```

---The following area is a Code cell (cell numver is 5)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test_data['id'],
    'winner_model_a': test_pred_a,
    'winner_model_b': test_pred_b,
    'winner_tie': test_pred_tie
})

# Save the submission file
submission_path = '/kaggle/working/submission.csv'
submission.to_csv(submission_path, index=False)

print(f"Submission file saved to {submission_path}")
```

** @@@ Jupyter Notebook numver 35, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Prompt/Response Words KeyBERT



KeyBERT is a Python library for keyword extraction and keyphrase extraction. It is built on top of the Hugging Face Transformers library and leverages pre-trained transformer models, such as BERT, to extract key phrases or keywords from a given text. KeyBERT is particularly useful for tasks like document summarization, content analysis, and information retrieval.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install keybert
```

---The following area is a Code cell (cell numver is 2)---
```python
import numpy as np 
import pandas as pd 
import random
import os
from keybert import KeyBERT
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 3)---
```python
model = KeyBERT('distilbert-base-nli-mean-tokens')
```

---The following area is a Code cell (cell numver is 4)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')#, encoding='iso-8859-1')
print(len(train))
train['prompt_kw']='-'
train['res_a_kw']='-'
train['res_b_kw']='-'

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')#, encoding='iso-8859-1')
test['prompt_kw']='-'
test['res_a_kw']='-'
test['res_b_kw']='-'
```

---The following area is a Code cell (cell numver is 5)---
```python
kw0 = model.extract_keywords(train['prompt'],top_n=5)
kw1 = model.extract_keywords(train['response_a'],top_n=20)
kw2 = model.extract_keywords(train['response_b'],top_n=20)

tkw0 = model.extract_keywords(test['prompt'],top_n=5)
tkw1 = model.extract_keywords(test['response_a'],top_n=20)
tkw2 = model.extract_keywords(test['response_b'],top_n=20)
```

---The following area is a Code cell (cell numver is 6)---
```python
for i,w in enumerate(kw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(kw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(kw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'res_b_kw']=' '.join(ws)   

train['res_a_kw']=train['prompt_kw']+' / '+train['res_a_kw']
train['res_b_kw']=train['prompt_kw']+' / '+train['res_b_kw']
train=train.iloc[:,6:]
display(train)

train.to_csv('train_key.csv',index=False)
```

---The following area is a Code cell (cell numver is 7)---
```python
for i,w in enumerate(tkw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(tkw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(tkw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_b_kw']=' '.join(ws)   

test['res_a_kw']=test['prompt_kw']+' / '+test['res_a_kw']
test['res_b_kw']=test['prompt_kw']+' / '+test['res_b_kw']
test=test.iloc[:,4:]
display(test)

test.to_csv('test_key.csv',index=False)
```

** @@@ Jupyter Notebook numver 36, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np
import pandas as pd

import plotly.graph_objects as go
from plotly import express as px
from plotly.offline import init_notebook_mode, iplot

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
```

---The following area is a Code cell (cell numver is 1)---
```python
init_notebook_mode(connected=True)

# set max column width to 500
pd.set_option("display.max_colwidth", 500)
```

---The following area is a Code cell (cell numver is 2)---
```python
DATA_PATH = "/kaggle/input/lmsys-chatbot-arena/train.csv"
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Data analysis
```

---The following area is a Code cell (cell numver is 4)---
```python
df = pd.read_csv(DATA_PATH)
df.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
df.info()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Models performance analysis
```

---The following area is a Code cell (cell numver is 7)---
```python
model_usage = pd.concat([df["model_a"], df["model_b"]]).value_counts()

# How many unique models are there?
len(model_usage.keys())
```

---The following area is a Code cell (cell numver is 8)---
```python
# plot the model usage
fig = px.bar(
    x=model_usage.index,
    y=model_usage.values,
    labels={"x": "Model", "y": "Usage times"},
    title="Model Usage Distribution",
)

fig.update_layout(width=1200, height=700)

iplot(fig)
```

---The following area is a Code cell (cell numver is 9)---
```python
# count the number of times each pair of models are compared, regardless of the order
compared_models_count = (
    pd.DataFrame(
        np.sort(df[["model_a", "model_b"]].values, axis=1),
        columns=["model_a", "model_b"],
    )
    .value_counts()
    .reset_index(name="counts")
)

len(compared_models_count)
```

---The following area is a Code cell (cell numver is 10)---
```python
top_compared_models = compared_models_count.head(20)

fig = px.bar(
    x=top_compared_models["model_a"] + " vs " + top_compared_models["model_b"],
    y=top_compared_models["counts"],
    labels={"x": "Model Comparison", "y": "Comparison times"},
    title="Top 20 Model Comparison Distribution",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 11)---
```python
# there are 1275 combinations of compared models. How often given numbers of comparisons occur?

compared_models_count["counts"].value_counts()
```

---The following area is a Code cell (cell numver is 12)---
```python
# bin the comparison count distribution to 10 bins
comparision_count_distribution_bins = pd.cut(
    compared_models_count["counts"],
    bins=[
        0,
        5,
        10,
        20,
        30,
        40,
        50,
        60,
        70,
        80,
        90,
        100,
        compared_models_count["counts"].max(),
    ],
    precision=0,
    retbins=False,
)

comparision_count_distribution_bins.value_counts(normalize=True).sort_index()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
**Which models performs the best and the worst?**
```

---The following area is a Code cell (cell numver is 14)---
```python
model_scores_part1 = df.groupby("model_a")["winner_model_a"].sum()
model_scores_part2 = df.groupby("model_b")["winner_model_b"].sum()

model_scores = model_scores_part1.add(model_scores_part2, fill_value=0)
model_scores = model_scores / (
    df["model_a"].value_counts() + df["model_b"].value_counts()
)
model_scores = model_scores.sort_values(ascending=False)
```

---The following area is a Code cell (cell numver is 15)---
```python
# top 10 best models
fig = px.bar(
    x=model_scores.head(10).index,
    y=model_scores.head(10).values,
    labels={"x": "Model", "y": "Score"},
    title="Top 10 best-rated models",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 16)---
```python
# top 10 worst models
fig = px.bar(
    x=model_scores.tail(10).index,
    y=model_scores.tail(10).values,
    labels={"x": "Model", "y": "Score"},
    title="Top 10 worst-rated models",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 17)---
```python
tie_df = df.query("winner_tie == 1")
(tie_df["winner_model_a"] + tie_df["winner_model_b"]).value_counts()
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
**How frequent is tie?**
```

---The following area is a Code cell (cell numver is 19)---
```python
f"{(tie_df.shape[0] / df.shape[0]) * 100:.1f}%"
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Positive and negative responses analysis
```

---The following area is a Code cell (cell numver is 21)---
```python
# exclude tied examples
df_no_ties = df.query("winner_tie == 0")

loosing_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 0 else x["response_b"]),
    axis=1,
)

loosing_responses
```

---The following area is a Code cell (cell numver is 22)---
```python
winning_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 1 else x["response_b"]),
    axis=1,
)
```

---The following area is a Code cell (cell numver is 23)---
```python
# loosing responses must be cleaned
loosing_responses = loosing_responses.str.strip("[]")
loosing_responses = loosing_responses.str.strip('"')

winning_responses = winning_responses.str.strip("[]")
winning_responses = winning_responses.str.strip('"')
```

---The following area is a Code cell (cell numver is 24)---
```python
df_no_ties["winning_response"] = winning_responses
df_no_ties["loosing_response"] = loosing_responses
```

---The following area is a Code cell (cell numver is 25)---
```python
texts = [
    "I do not feel comfortable",
    "I'm sorry, but",
    "I am sorry, but",
    "I apologize, but",
    "Unfortunately I",
    "I do not have enough",
    "I'm just an AI",
    "I'm an AI and",
    "I'm afraid",
    "I can't",
]

unprecise_responses = []
for _, row in df_no_ties.iterrows():
    if row["loosing_response"].startswith(tuple(texts)):
        if row["winning_response"].startswith(tuple(texts)):
            unprecise_responses.append("both")
        else:
            unprecise_responses.append("loosing")
    elif row["winning_response"].startswith(tuple(texts)):
        unprecise_responses.append("winning")
    else:
        continue

pd.Series(unprecise_responses).value_counts(normalize=True)
```

---The following area is a Code cell (cell numver is 26)---
```python
import os
os.environ["KERAS_BACKEND"] = "jax"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import plotly.express as px
```

---The following area is a Code cell (cell numver is 27)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en" # Name of pretrained models
    sequence_length = 512  # Input sequence length
    epochs = 3 # Training epochs
    batch_size = 16  # Batch size
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Code cell (cell numver is 28)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Code cell (cell numver is 29)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Code cell (cell numver is 30)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Code cell (cell numver is 31)---
```python
BASE_PATH
```

---The following area is a Code cell (cell numver is 32)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# Sample data
# df = df.sample(frac=0.10)

# Take the first prompt and its associated response
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Code cell (cell numver is 33)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Show Sample
test_df.head()
```

---The following area is a Code cell (cell numver is 34)---
```python
# Define a function to create options based on the prompt and choices
def make_pairs(row):
    row["encode_fail"] = False
    try:
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        prompt = ""
        row["encode_fail"] = True

    try:
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        response_a = ""
        row["encode_fail"] = True

    try:
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        response_b = ""
        row["encode_fail"] = True
        
    row['options'] = [f"Prompt: {prompt}\n\nResponse: {response_a}",  # Response from Model A
                      f"Prompt: {prompt}\n\nResponse: {response_b}"  # Response from Model B
                     ]
    return row
```

---The following area is a Code cell (cell numver is 35)---
```python
df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(df.head(2))  # Display the first 2 rows of df

test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Code cell (cell numver is 36)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Code cell (cell numver is 37)---
```python
model_df = pd.concat([df.model_a, df.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count', color_continuous_scale='viridis')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()
```

---The following area is a Code cell (cell numver is 38)---
```python
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

fig = px.bar(counts, x='Winner', y='Win Count',
             title='Winner distribution for Train Data',
             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},
             color='Winner', color_continuous_scale='viridis')

fig.update_layout(xaxis_title="Winner", yaxis_title="Win Count")

fig.show()
```

---The following area is a Code cell (cell numver is 39)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Code cell (cell numver is 40)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # Name of the model
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
)
```

---The following area is a Code cell (cell numver is 41)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Code cell (cell numver is 42)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Code cell (cell numver is 43)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Code cell (cell numver is 44)---
```python
# Train
train_texts = train_df.options.tolist()  # Extract training texts
train_labels = train_df.class_label.tolist()  # Extract training labels
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# Valid
valid_texts = valid_df.options.tolist()  # Extract validation texts
valid_labels = valid_df.class_label.tolist()  # Extract validation labels
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 45)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # Plot lr curve if plot is True
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('epoch'); plt.ylabel('lr')
        plt.title('LR Scheduler')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 46)---
```python
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Code cell (cell numver is 47)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')
```

---The following area is a Code cell (cell numver is 48)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Code cell (cell numver is 49)---
```python
# Define input layers
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# Create a DebertaV3Classifier backbone
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# Compute embeddings for first response: (P + R_A) using backbone
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# Compute embeddings for second response: (P + R_B), using the same backbone
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# Compute final output
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# Compile the model with optimizer, loss, and metrics
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Code cell (cell numver is 50)---
```python
model.summary()
```

---The following area is a Code cell (cell numver is 51)---
```python
# Start training the model
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Code cell (cell numver is 52)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Code cell (cell numver is 53)---
```python
# Build test dataset
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 54)---
```python
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Code cell (cell numver is 55)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

** @@@ Jupyter Notebook numver 37, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Step 1: Data Exploration
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
**Load the Data**
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd

# Load the datasets
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# Display the first few rows of each dataset
print("Train Data:")
print(train_data.head())

print("\nTest Data:")
print(test_data.head())

print("\nSample Submission:")
print(sample_submission.head())

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
**Inspect the Data**
```

---The following area is a Code cell (cell numver is 5)---
```python
# Display the summary of each dataset
print("Train Data Info:")
print(train_data.info())

print("\nTest Data Info:")
print(test_data.info())

print("\nSample Submission Info:")
print(sample_submission.info())

# Check for missing values
print("Missing Values in Train Data:")
print(train_data.isnull().sum())

print("\nMissing Values in Test Data:")
print(test_data.isnull().sum())

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
**Analyze the Data**
```

---The following area is a Code cell (cell numver is 7)---
```python
# Get basic statistics for numerical features
print("Train Data Description:")
print(train_data.describe())

# Explore the distribution of target variables
print("Distribution of Winner Model (Train Data):")
print(train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].sum())

```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Step 2: Data Preprocessing
```

---The following area is a Code cell (cell numver is 9)---
```python
import re

def clean_text(text):
    # Remove HTML tags if any
    text = re.sub(r'<.*?>', '', text)
    
    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove extra whitespace
    text = text.strip()
    
    return text

```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
**Applying Cleaning Function:**
```

---The following area is a Code cell (cell numver is 11)---
```python
# Clean text data in train_data
train_data['prompt'] = train_data['prompt'].apply(clean_text)
train_data['response_a'] = train_data['response_a'].apply(clean_text)
train_data['response_b'] = train_data['response_b'].apply(clean_text)

# Clean text data in test_data
test_data['prompt'] = test_data['prompt'].apply(clean_text)
test_data['response_a'] = test_data['response_a'].apply(clean_text)
test_data['response_b'] = test_data['response_b'].apply(clean_text)

```

---The following area is a Code cell (cell numver is 12)---
```python
# Check column names in train_data and test_data
print("Train Data Columns:", train_data.columns)
print("Test Data Columns:", test_data.columns)

```

** @@@ Jupyter Notebook numver 38, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import pandas as pd
import ast
import json
import re
import torch
from tqdm import tqdm
```

---The following area is a Code cell (cell numver is 2)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 3)---
```python
def process_data(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    sentences = ' '.join(sentences)
    return sentences

def get_data(path, system_prompt=None):
    df = pd.read_csv(path)
    df['prompt'] = df['prompt'].apply(process_data)
    df['response_a'] = df['response_a'].apply(process_data)
    df['response_b'] = df['response_b'].apply(process_data)
    return df
```

---The following area is a Code cell (cell numver is 4)---
```python
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
test_df = get_data(test_path)
test_df.head()
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Load Gemma-2-9B model
> google/gemma-2-9b-it 

I have already downloaded and stored bfloat16 weights.
```

---The following area is a Code cell (cell numver is 6)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-tokenizer'
model_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-model'
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time
model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.bfloat16)
```

---The following area is a Code cell (cell numver is 10)---
```python
model
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## Let's do a simple inference
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time
prompt = 'Write a conversation between gemma and llama llm models'
input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')

outputs = model.generate(**input_ids, max_new_tokens=200)
response = tokenizer.decode(outputs[0])
print(response)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Let's do inference on test set with some prompt engineering
```

---The following area is a Code cell (cell numver is 14)---
```python
test_df
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_prompt(query, response_a, response_b):
    prompt = f"""
You are tasked with evaluating two responses generated by different models to determine which one is better. Given a query and two responses (RESPONSE_A from MODEL_A and RESPONSE_B from MODEL_B), you will assess the quality of each response based on relevance, accuracy, completeness, and overall coherence. If both responses are equally good or equally poor, you may declare a tie.

Instructions:

Query: {query}
RESPONSE_A (MODEL_A): {response_a}
RESPONSE_B (MODEL_B): {response_b}

Evaluation Criteria:

Relevance: How well does the response address the query?
Accuracy: Is the information provided correct and reliable?
Completeness: Does the response provide a comprehensive answer?
Coherence: Is the response logically structured and easy to understand?
Output:

If RESPONSE_A is better, output: RESPONSE_A
If RESPONSE_B is better, output: RESPONSE_B
If both responses are equally good or poor, output: TIE

You have to output a single line having either of these words - RESPONSE_A or RESPONSE_B or TIE \n
OUTPUT: 

    """
    return prompt
```

---The following area is a Code cell (cell numver is 16)---
```python
def predict(query, response_a, response_b, max_new_tokens=50, do_sample=False, temperature=1.0):
    prompt = get_prompt(query=query, response_a=response_a, response_b=response_b)

    input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')

    outputs = model.generate(**input_ids, max_new_tokens=50, do_sample=do_sample, temperature=temperature)
    response = tokenizer.decode(outputs[0])

    pattern = r"OUTPUT:\s*(RESPONSE_A|RESPONSE_B|TIE)"
    match = re.search(pattern, response)
    if match:
        pred = match.group(1)
    else:
        pred = None
    return pred
```

---The following area is a Code cell (cell numver is 17)---
```python
%%time
id_list = []
winner_model_a_list = []
winner_model_b_list = []
winner_tie_list = []
for idx in tqdm(range(0, len(test_df))):
    query_id = test_df.iloc[idx]['id']
    query = test_df.iloc[idx]['prompt']
    response_a = test_df.iloc[idx]['response_a']
    response_b = test_df.iloc[idx]['response_b']
    pred = predict(query, response_a, response_b, max_new_tokens=20, do_sample=True, temperature=0.7)
    id_list.append(query_id)
    if pred is not None:
        if 'A' in pred or 'a' in pred:
            winner_model_a_list.append(1)
            winner_model_b_list.append(0)                    
            winner_tie_list.append(0)
        if 'B' in pred or 'b' in pred:
            winner_model_a_list.append(0)
            winner_model_b_list.append(1)        
            winner_tie_list.append(0)            
    else:
        winner_model_a_list.append(0)
        winner_model_b_list.append(0)        
        winner_tie_list.append(1)
    torch.cuda.empty_cache()
```

---The following area is a Code cell (cell numver is 18)---
```python
submission_df = pd.DataFrame({'id': id_list, 'winner_model_a': winner_model_a_list, 'winner_model_b': winner_model_b_list, 'winner_tie': winner_tie_list})
```

---The following area is a Code cell (cell numver is 19)---
```python
submission_df
```

---The following area is a Code cell (cell numver is 20)---
```python
submission_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 39, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 30px; 
    border-radius: 15px; 
    text-align: center; 
    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.4);
    margin-bottom: 30px;
">
    <h1 style="font-size: 2.5em;">ðŸ§  LMSYS - AI Chat Evaluator: Enhancing Human-AI</h1>
</div>


<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-bottom: 20px;
">
    <h1>Welcome to my Notebook! ðŸ™</h1>
    <p> This notebook is dedicated to the <strong>Chatbot Arena Competition</strong>, where we explore and innovate in the realm of conversational AI.</p>
    <p><strong>Competition Overview:</strong> The goal is to develop a chatbot that excels in understanding and engaging users through natural and relevant conversations. You'll find data, techniques, and insights here to help you craft the best chatbot possible.</p>
    <p>Thank you for joining this journey. Dive in, explore, and letâ€™s make some magic with chatbots!</p>
</div>



# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
 1. Setup the Environment
</h1>
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install tensorflow pandas numpy scikit-learn

```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
2. Importing Libraries
</h1>
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
3. Loading.. Data
</h1>
```

---The following area is a Code cell (cell numver is 5)---
```python
# Load the dataset
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
4. Preprocessing Text Data
</h1>
```

---The following area is a Code cell (cell numver is 7)---
```python
# Concatenate responses and prompts for tokenization
texts = train_df['prompt'] + ' ' + train_df['response_a'] + ' ' + train_df['response_b']
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

# Convert text to sequences
def text_to_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    return pad_sequences(sequences, maxlen=500)  # Adjust maxlen based on your needs

# Process training data
X_prompt_a = text_to_sequences(train_df['prompt'])
X_response_a = text_to_sequences(train_df['response_a'])
X_response_b = text_to_sequences(train_df['response_b'])

# Labels
y_a = train_df['winner_model_a']
y_b = train_df['winner_model_b']
y_tie = train_df['winner_tie']

# Split data for training
X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_prompt_a, y_a, test_size=0.2, random_state=42)
X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_prompt_a, y_b, test_size=0.2, random_state=42)
X_train_tie, X_val_tie, y_train_tie, y_val_tie = train_test_split(X_prompt_a, y_tie, test_size=0.2, random_state=42)

```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
5. Building the Model
</h1>
```

---The following area is a Code cell (cell numver is 9)---
```python
# Define model parameters
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
max_len = 500

# Define inputs
input_prompt = Input(shape=(max_len,))
input_response_a = Input(shape=(max_len,))
input_response_b = Input(shape=(max_len,))

# Embedding layers
embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)

embedded_prompt = embedding_layer(input_prompt)
embedded_response_a = embedding_layer(input_response_a)
embedded_response_b = embedding_layer(input_response_b)

# LSTM layers
lstm_layer = LSTM(64)

encoded_prompt = lstm_layer(embedded_prompt)
encoded_response_a = lstm_layer(embedded_response_a)
encoded_response_b = lstm_layer(embedded_response_b)

# Concatenate encoded responses with prompt
concat_a = Concatenate()([encoded_prompt, encoded_response_a])
concat_b = Concatenate()([encoded_prompt, encoded_response_b])

# Dense layers for classification
dense_layer = Dense(64, activation='relu')

output_a = Dense(1, activation='sigmoid')(dense_layer(concat_a))
output_b = Dense(1, activation='sigmoid')(dense_layer(concat_b))
output_tie = Dense(1, activation='sigmoid')(dense_layer(concat_a))

# Model
model = Model(inputs=[input_prompt, input_response_a, input_response_b], 
              outputs=[output_a, output_b, output_tie])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
6. Training the Model
</h1>
```

---The following area is a Code cell (cell numver is 11)---
```python
# Model Compilation
model.compile(
    optimizer='adam', 
    loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], 
    metrics=[['accuracy'], ['accuracy'], ['accuracy']]
)

# Model Training
history = model.fit(
    [X_train_a, X_train_b, X_train_b],
    [y_train_a, y_train_b, y_train_tie],
    validation_data=([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie]),
    epochs=5,  # Adjust epochs as needed
    batch_size=32
)

```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
7. Evaluateing the Model
</h1>
```

---The following area is a Code cell (cell numver is 13)---
```python
# Evaluate the model
eval_results = model.evaluate([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie])

# Unpack the results
val_loss = eval_results[0]
loss_a = eval_results[1]
loss_b = eval_results[2]
loss_tie = eval_results[3]
acc_a = eval_results[4]
acc_b = eval_results[5]
acc_tie = eval_results[6]

print(f'Validation Loss: {val_loss}')
print(f'Loss for model_a predictions: {loss_a}')
print(f'Loss for model_b predictions: {loss_b}')
print(f'Loss for tie predictions: {loss_tie}')
print(f'Accuracy for model_a predictions: {acc_a}')
print(f'Accuracy for model_b predictions: {acc_b}')
print(f'Accuracy for tie predictions: {acc_tie}')

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
8. Makeing Predictions
</h1>
```

---The following area is a Code cell (cell numver is 15)---
```python
# Preprocess test data
X_test_prompt = text_to_sequences(test_df['prompt'])
X_test_response_a = text_to_sequences(test_df['response_a'])
X_test_response_b = text_to_sequences(test_df['response_b'])

# Predict
predictions = model.predict([X_test_prompt, X_test_response_a, X_test_response_b])

# Unpack predictions
predicted_a = predictions[0].flatten()
predicted_b = predictions[1].flatten()
predicted_tie = predictions[2].flatten()

```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
9. Prepareing the Submission File
</h1>
```

---The following area is a Code cell (cell numver is 17)---
```python
# Format predictions for submission
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': predicted_a,
    'winner_model_b': predicted_b,
    'winner_tie': predicted_tie
})


# Save to CSV
submission.to_csv('/kaggle/working/submission.csv', index=False)

```

---The following area is a Code cell (cell numver is 18)---
```python
submission.head()

```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-top: 30px;
">
    <h2><strong>Thank You for Exploring My Notebook! ðŸ™<strong></h2>
    <p>I hope you found the content and insights valuable as we journeyed through the <strong>LMSYS - AI Chat Evaluator</strong> project together.</p>
    <p>Your feedback and support mean a lot to me. If you enjoyed this notebook and found it helpful, please consider giving it an <span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"> <strong>upvote â¬†ï¸</strong></span>. Your encouragement helps me continue improving and sharing useful content.</p>
    <p><span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"><strong> Thank you once again for your time and support!</strong></span></p>
</div>
```

** @@@ Jupyter Notebook numver 40, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd

# Veri setini yÃ¼kleme
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# Veri setinin ilk birkaÃ§ satÄ±rÄ±nÄ± inceleme
df.head()

```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from textblob import TextBlob
import string
```

---The following area is a Code cell (cell numver is 3)---
```python
from sklearn.preprocessing import OneHotEncoder

# Noktalama iÅŸaretleri sayÄ±sÄ±nÄ± Ã¶zellik olarak eklemek
df['response_a_punctuation_count'] = df['response_a'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))
df['response_b_punctuation_count'] = df['response_b'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))

# Kelime Ã§eÅŸitliliÄŸi Ã¶zelliÄŸi eklemek
df['response_a_unique_words'] = df['response_a'].apply(lambda x: len(set(x.split())))
df['response_b_unique_words'] = df['response_b'].apply(lambda x: len(set(x.split())))

# Stop words oranÄ± Ã¶zelliÄŸi eklemek (Ã¶rnek bir stop words listesi kullanÄ±yoruz)
stop_words = set(['the', 'and', 'is', 'in', 'at', 'of', 'it', 'to'])
df['response_a_stop_words_ratio'] = df['response_a'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))
df['response_b_stop_words_ratio'] = df['response_b'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))

# CÃ¼mlenin baÅŸ harf bÃ¼yÃ¼klÃ¼ÄŸÃ¼ Ã¶zelliÄŸi eklemek
df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: 1 if x[0].isupper() else 0)
df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: 1 if x[0].isupper() else 0)

# CÃ¼mlenin uzunluÄŸu ve farkÄ± Ã¶zelliÄŸi eklemek
df['response_a_length'] = df['response_a'].apply(len)
df['response_b_length'] = df['response_b'].apply(len)
df['response_length_difference'] = df['response_a_length'] - df['response_b_length']

# Kazanan model sÃ¼tunu eklemek
df['winner'] = df.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)

# Model adÄ± sÃ¼tunu eklemek
df['model_name'] = df.apply(lambda row: row['model_a'] if row['winner'] == 'model_a' else row['model_b'] if row['winner'] == 'model_b' else '', axis=1)

# One-hot encoding iÅŸlemi
encoder = OneHotEncoder()
winner_encoded = encoder.fit_transform(df[['winner']])

# Encode edilmiÅŸ sÃ¼tunlarÄ± dataframe'e ekleme
df_encoded = pd.concat([df, pd.DataFrame(winner_encoded.toarray(), columns=encoder.categories_[0])], axis=1)

# SonuÃ§larÄ± gÃ¶sterme


```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Veri setini hazÄ±rlama fonksiyonu
def prepare_data_for_ml(df):
    # Punctuation count fonksiyonu
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;:!?')

    # Yeni sÃ¼tunlar ekleme
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation)
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation)
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length'])

    # winner_encoded sÃ¼tunu oluÅŸturma
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner'])

    # model_name sÃ¼tunu oluÅŸturma
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b'])

    # Kategorik deÄŸiÅŸkenleri encode etme
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col])

    # Boolean deÄŸerleri 0 ve 1'e Ã§evirme
    df['model_a_win'] = df['winner'] == 'model_a'
    df['model_b_win'] = df['winner'] == 'model_b'
    df['winner_tie'] = df['winner'] == 'tie'
    df['tie'] = df['winner'] == 'tie'

    boolean_columns = ['response_b_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int)

    return df

# Veri setini hazÄ±rlama
prepared_df = prepare_data_for_ml(df)

# Gerekli sÃ¼tunlarÄ± seÃ§me ve eksik sÃ¼tunlarÄ± kontrol etme
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]
```

---The following area is a Code cell (cell numver is 5)---
```python
prepared_df.head()
```

---The following area is a Code cell (cell numver is 6)---
```python
# Veriyi hazÄ±rlama
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncoded kullanÄ±larak kodlanmÄ±ÅŸ hedef deÄŸiÅŸken

# Veriyi train ve test setlerine bÃ¶lelim
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Veriyi standardize edelim
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modeli eÄŸitme
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Model Ã¼zerinde tahmin yapma
y_pred = model.predict(X_test_scaled)

# SonuÃ§larÄ± deÄŸerlendirme
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie']))
```

---The following area is a Code cell (cell numver is 7)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Veri setini hazÄ±rlama fonksiyonu
def prepare_data_for_ml(df):
    # Punctuation count fonksiyonu
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;:!?')

    # Yeni sÃ¼tunlar ekleme
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation)
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation)
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length'])

    # winner_encoded sÃ¼tunu oluÅŸturma
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner'])

    # model_name sÃ¼tunu oluÅŸturma
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b'])

    # Kategorik deÄŸiÅŸkenleri encode etme
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col])

    # Boolean deÄŸerleri 0 ve 1'e Ã§evirme
    df['model_a_win'] = df['winner'] == 'model_a'
    df['model_b_win'] = df['winner'] == 'model_b'
    df['winner_tie'] = df['winner'] == 'tie'
    df['tie'] = df['winner'] == 'tie'

    boolean_columns = ['response_b_startswith_upper', 'response_a_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int)

    return df

# Veri setini hazÄ±rlama
prepared_df = prepare_data_for_ml(df)

# Gerekli sÃ¼tunlarÄ± seÃ§me ve eksik sÃ¼tunlarÄ± kontrol etme
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_startswith_upper',  # response_a_startswith_upper eklendi
    'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]

# Veriyi hazÄ±rlama
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncoded kullanÄ±larak kodlanmÄ±ÅŸ hedef deÄŸiÅŸken

# Veriyi train ve test setlerine bÃ¶lelim
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Veriyi standardize edelim
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modeli eÄŸitme
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Model Ã¼zerinde tahmin yapma
y_pred = model.predict(X_test_scaled)

# SonuÃ§larÄ± deÄŸerlendirme
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie']))

```

---The following area is a Code cell (cell numver is 8)---
```python
df['winner']
```

---The following area is a Code cell (cell numver is 9)---
```python
import pandas as pd
import numpy as np
import string
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
```

---The following area is a Code cell (cell numver is 10)---
```python
# Kazanan modeli encode etmek
label_encoder = LabelEncoder()
df['winner_encoded'] = label_encoder.fit_transform(df['winner'])

# Numeric Ã¶zellikler ve metinsel Ã¶zellikler iÃ§in kolonlarÄ± ayÄ±rmak
numeric_features = [
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_a_unique_words', 'response_b_unique_words',
    'response_a_stop_words_ratio', 'response_b_stop_words_ratio',
    'response_a_length', 'response_b_length', 'response_length_difference'
]

text_features = ['prompt', 'response_a', 'response_b']

# ColumnTransformer oluÅŸturmak
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('text', 'passthrough', text_features)  # Metinsel Ã¶zellikler passthrough olarak geÃ§irilecek
    ])

# Verileri X (Ã¶zellikler) ve y (hedef deÄŸiÅŸken) olarak ayÄ±rmak
X = df[numeric_features + text_features]
y = df['winner_encoded']

# Train ve test veri setlerini oluÅŸturmak
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pipeline oluÅŸturmak
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Modeli eÄŸitmek
pipeline.fit(X_train, y_train)

# Test veri seti Ã¼zerinde modeli deÄŸerlendirmek
accuracy = pipeline.score(X_test, y_test)
print(f"Model accuracy: {accuracy}")

```

---The following area is a Code cell (cell numver is 11)---
```python
import pandas as pd

# CSV dosyalarÄ±nÄ± okuma
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# BirleÅŸtirme iÅŸlemi
df = pd.concat([train_df, test_df], ignore_index=True)

# Sonucu kontrol etme
df.head()

```

---The following area is a Code cell (cell numver is 12)---
```python
df.shape
```

---The following area is a Code cell (cell numver is 13)---
```python
len(df.id)
```

---The following area is a Code cell (cell numver is 14)---
```python
df.info()
```

---The following area is a Code cell (cell numver is 15)---
```python
winner_counts = {
    'model_a': df['winner_model_a'].value_counts(),
    'model_b': df['winner_model_b'].value_counts(),
    'tie': df['winner_tie'].sum()
}

print(winner_counts)
```

---The following area is a Code cell (cell numver is 16)---
```python
# winner_model_a ve winner_model_b sÃ¼tunlarÄ± sÄ±fÄ±r olan satÄ±rlarÄ± filtreleme
zero_winner_df = df[(df['winner_model_a'] == 1.0) & (df['winner_model_b'] == 1.0)].astype("float64")
```

---The following area is a Code cell (cell numver is 17)---
```python
# winner_model_a sÃ¼tunu 1 olan satÄ±rlarÄ± filtreleme
model_a_winner_df = combined_df[combined_df['winner_model_a'] == 1.0]

# winner_model_b sÃ¼tunu 1 olan satÄ±rlarÄ± filtreleme
model_b_winner_df = combined_df[combined_df['winner_model_b'] == 1.0]

# 1 olan satÄ±rlarda model_a ve model_b'nin en Ã§ok tekrar edileni bulma
model_a_counts = model_a_winner_df['model_a'].value_counts()
model_b_counts = model_b_winner_df['model_b'].value_counts()

print("model_a en Ã§ok tekrar edenler (combined):")
print(model_a_counts.head())

print("\nmodel_b en Ã§ok tekrar edenler (combined):")
print(model_b_counts.head())
```

---The following area is a Code cell (cell numver is 18)---
```python
# model_a sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma
response_a_counts = model_a_winner_df['response_a'].value_counts()
response_b_counts = model_b_winner_df['response_b'].value_counts()
```

---The following area is a Code cell (cell numver is 19)---
```python
# response_a sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma ve kazanan modele gÃ¶re gruplandÄ±rma
response_a_counts_df = model_a_winner_df.groupby(['model_a', 'prompt']).size().reset_index(name='count')

# response_b sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma ve kazanan modele gÃ¶re gruplandÄ±rma
response_b_counts_df = model_b_winner_df.groupby(['model_b', 'prompt']).size().reset_index(name='count')

print("response_a en Ã§ok tekrar edenler (combined):")
print(response_a_counts_df.head())

print("\nresponse_b en Ã§ok tekrar edenler (combined):")
print(response_b_counts_df.head())
```

---The following area is a Code cell (cell numver is 20)---
```python
response_a_counts_df
```

---The following area is a Code cell (cell numver is 21)---
```python
top_models = ['gpt-4-1106-preview', 'gpt-4-0613', 'gpt-3.5-turbo-0613', 'gpt-4-0314']

```

---The following area is a Code cell (cell numver is 22)---
```python
# Her model iÃ§in en Ã§ok tekrar eden 10 yanÄ±tÄ± almak
for model in top_models:
    print(f"\nModel: {model} - En Ã§ok tekrar eden 10 yanÄ±t (response_a):")
    top_responses_a = response_a_counts_df[response_a_counts_df['model_a'] == model].sort_values(by='count', ascending=False).head(10)
    print(top_responses_a)

    print(f"\nModel: {model} - En Ã§ok tekrar eden 10 yanÄ±t (response_b):")
    top_responses_b = response_b_counts_df[response_b_counts_df['model_b'] == model].sort_values(by='count', ascending=False).head(10)
    print(top_responses_b)
```

---The following area is a Code cell (cell numver is 23)---
```python
# Kazanan modelin adÄ±nÄ± belirleyen ve yeni bir sÃ¼tun oluÅŸturan fonksiyon
def determine_winner(row):
    if row['winner_model_a'] == 1.0:
        return row['model_a']
    elif row['winner_model_b'] == 1.0:
        return row['model_b']
    else:
        return 'tie'

# Kazanan sÃ¼tunu oluÅŸturma
df['winner'] = df.apply(determine_winner, axis=1)

# Sadece prompt ve kazanan sÃ¼tununu alarak gruplama iÅŸlemi
grouped_df = df[['prompt', 'winner']].groupby('prompt')['winner'].apply(list).reset_index()

# CSV olarak Ã§Ä±ktÄ± alma
grouped_df.to_csv('prompt_winners.csv', index=False)

print("CSV dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu: 'prompt_winners.csv'")
```

---The following area is a Code cell (cell numver is 24)---
```python
prompt = pd.read_csv("/kaggle/working/prompt_winners.csv")
prompt.groupby("winner").count().reset_index().head(20)
```

---The following area is a Code cell (cell numver is 25)---
```python
# PromptlarÄ± kazanan modele gÃ¶re gruplayarak en Ã§ok tekrar edenleri bulma
winner_counts = prompt_df.explode('winner').groupby('winner').size().reset_index(name='count')
winner_counts = winner_counts.sort_values(by='count', ascending=False)

# CSV olarak Ã§Ä±ktÄ± alma
winner_counts.to_csv('prompt_top_winners.csv', index=False)

print("CSV dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu: 'prompt_top_winners.csv'")
```

---The following area is a Code cell (cell numver is 26)---
```python
winner_counts.head(20)
```

---The following area is a Code cell (cell numver is 27)---
```python
# Prompt ve kazanan modeli gruplayarak Ã§Ä±ktÄ± alma
prompt_winners = df.groupby(['winner', 'prompt']).sum().reset_index()

prompt_winners
```

---The following area is a Code cell (cell numver is 28)---
```python
top_prompt = pd.read_csv("/kaggle/working/prompt_top_winners.csv")
top_prompt.head(20)
```

---The following area is a Code cell (cell numver is 29)---
```python
# 'tie' durumunda response_a ve response_b deÄŸerlerini gruplayarak Ã§Ä±ktÄ± alma
tie_responses = df[df['winner'] == 'tie'].groupby(['response_a', 'response_b']).sum().reset_index()[['response_a', 'response_b',"prompt"]]

tie_responses
```

---The following area is a Code cell (cell numver is 30)---
```python
def determine_winner_and_loser(row):
    if row['winner_model_a'] == 1.0:
        winner = row['model_a']
        loser = row['model_b']
        winner_response = row['response_a']
        loser_response = row['response_b']
    elif row['winner_model_b'] == 1.0:
        winner = row['model_b']
        loser = row['model_a']
        winner_response = row['response_b']
        loser_response = row['response_a']
    else:
        winner = 'tie'
        loser = 'tie'
        winner_response = ''
        loser_response = ''
    
    return pd.Series([winner, loser, len(winner_response), len(loser_response)], index=['winner', 'loser', 'winner_response_length', 'loser_response_length'])

# Kazanan ve kaybeden sÃ¼tunlarÄ±nÄ± oluÅŸturma
winner_loser_lengths = df.apply(determine_winner_and_loser, axis=1)

# Ã–nceki DataFrame'e kazanan ve kaybeden uzunluklarÄ±nÄ± ekleyerek yeni bir DataFrame oluÅŸturma
prompt_df = pd.concat([df[['prompt']], winner_loser_lengths], axis=1)

prompt_df.head(20)
```

---The following area is a Code cell (cell numver is 31)---
```python
# CSV dosyalarÄ±nÄ± okuma
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# BirleÅŸtirme iÅŸlemi
df = pd.concat([train_df, test_df], ignore_index=True)
```

---The following area is a Code cell (cell numver is 32)---
```python
train_df
```

---The following area is a Code cell (cell numver is 33)---
```python
df
```

---The following area is a Code cell (cell numver is 34)---
```python
# Polars DataFrame oluÅŸturma
import polars as pl
from textblob import TextBlob
import numpy as np
```

---The following area is a Code cell (cell numver is 35)---
```python
import polars as pl
import numpy as np
from textblob import TextBlob

# Polars DataFrame oluÅŸturma
df = pl.DataFrame(df)

# Sentiment analizi ve error sayÄ±mÄ± fonksiyonlarÄ±
def batch_analyze_sentiment(texts):
    return np.array([TextBlob(text).sentiment.polarity for text in texts])

def batch_count_errors(texts):
    return np.array([text.lower().count('error') for text in texts])

# Yeni sÃ¼tunlar oluÅŸturma
df = df.with_columns([
    pl.col('response_a').str.lengths().alias('response_a_length'),
    pl.col('response_b').str.lengths().alias('response_b_length'),
    (pl.col('response_a').str.lengths() - pl.col('response_b').str.lengths()).abs().alias('response_length_difference'),
])

# Sentiment analizi ve error sayÄ±mÄ± iÃ§in ilk 100 satÄ±rÄ± iÅŸleme al
if len(df) > 0:
    n_rows = min(100, len(df))
    
    sentiment_a = batch_analyze_sentiment(df['response_a'].head(n_rows).to_numpy())
    sentiment_b = batch_analyze_sentiment(df['response_b'].head(n_rows).to_numpy())
    errors_a = batch_count_errors(df['response_a'].head(n_rows).to_numpy())
    errors_b = batch_count_errors(df['response_b'].head(n_rows).to_numpy())

    # Yeni sÃ¼tunlarÄ± ekleme
    df = df.with_columns([
        pl.Series('sentiment_a', sentiment_a).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('sentiment_b', sentiment_b).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('errors_a', errors_a).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('errors_b', errors_b).extend(pl.Series([None] * (len(df) - n_rows))),
    ])

# Kazanan ve kaybeden sÃ¼tunlarÄ±nÄ± oluÅŸturma (eski sÃ¼rÃ¼mlerle uyumlu)
df = df.with_columns([
    pl.when(pl.col('sentiment_a') > pl.col('sentiment_b'))
      .then(pl.col('response_a'))
      .when(pl.col('sentiment_a') <= pl.col('sentiment_b'))
      .then(pl.col('response_b'))
      .alias('winner_sentiment'),
    pl.when(pl.col('sentiment_a') < pl.col('sentiment_b'))
      .then(pl.col('response_a'))
      .when(pl.col('sentiment_a') >= pl.col('sentiment_b'))
      .then(pl.col('response_b'))
      .alias('loser_sentiment')
])

df
```

** @@@ Jupyter Notebook numver 41, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction


In this notebook we demostrate how to finetune **Llama3** (8B) model from Meta using **QLoRA** and **SFTTrainer** from **tlr**.

## What is Llama3?

Llama3 is the latest release of open-source LLMs from Meta, with features pretrained and instruction-fine-tuned language models with 8B and 70B parameters.

## What is LoRA?

LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times.


## What is QLoRA?

QLoRA builds on LoRA by incorporating quantization techniques to further reduce memory usage while maintaining, or even enhancing, model performance.Â With QLoRA it is possible to finetune a 70B parameter model that requires 36 GPUs with only 2!


## What is PEFT?

Parameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained modelâ€™s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task.

## What is SFTTrainer?

SFT in SFTTrainer stands for supervised fine-tuning. The **trl** (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.

## What is UltraChat200k?  

UltraChat-200k is an invaluable resource for natural language understanding, generation and dialog system research. With 1.4 million dialogues spanning a variety of topics, this parquet-formatted dataset offers researchers four distinct formats to aid in their studies: test_sft, train_sft, train_gen and test_gen. More details [here](https://www.kaggle.com/datasets/thedevastator/ultrachat-200k-nlp-dataset).

## Inspiration

For this notebook, I took inspiration from several sources:
* [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3)  
* [Fine-tuning LLMs using LoRA](https://medium.com/@rajatsharma_33357/fine-tuning-llama-using-lora-fb3f48a557d5)  
* [Fine-tuning Llama-3â€“8B-Instruct QLORA using low cost resources](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)  
* [Llama2 Fine-Tuning with Low-Rank Adaptations (LoRA) on Gaudi 2 Processors](https://eduand-alvarez.medium.com/llama2-fine-tuning-with-low-rank-adaptations-lora-on-gaudi-2-processors-52cf1ee6ce11)
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Install and import libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes
!pip install -q -U transformers
!pip install -q -U peft
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
```

---The following area is a Code cell (cell numver is 4)---
```python
# from kaggle_secrets import UserSecretsClient
# user_secrets = UserSecretsClient()
# wandb_key = user_secrets.get_secret("wandb_api")
# import wandb
# ! wandb login $wandb_key
```

---The following area is a Code cell (cell numver is 5)---
```python
import os
import torch
from time import time
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    Trainer
)
from trl import SFTTrainer,setup_chat_format
import numpy as np
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 16
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Prepare Dataset
```

---The following area is a Code cell (cell numver is 8)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Initialize model


The model used is:

* **Model**: Llama3  
* **Framework**: Transformers   
* **Size**: 8B   
* **Type**: 8b-chat-hf (hf stands for HuggingFace). 
* **Version**: V1
```

---The following area is a Code cell (cell numver is 10)---
```python
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification

quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)

model_name = "/kaggle/input/llama-3/transformers/8b-chat-hf/1"

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    num_labels=4,
    device_map='auto'
)
```

---The following area is a Code cell (cell numver is 11)---
```python
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    task_type = 'SEQ_CLS'
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
```

---The following area is a Code cell (cell numver is 12)---
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)

tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 13)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 14)---
```python
from datasets import DatasetDict, Dataset

def data_preprocesing(row):
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')

dataset = Dataset.from_pandas(train)
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])
tokenized_data.set_format("torch")
```

---The following area is a Code cell (cell numver is 15)---
```python
import gc

del train
gc.collect()
```

---The following area is a Code cell (cell numver is 16)---
```python
from transformers import DataCollatorWithPadding

collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 17)---
```python
def compute_metrics(evaluations):
    predictions, labels = evaluations
    predictions = np.argmax(predictions, axis=1)
    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),
    'accuracy':accuracy_score(predictions,labels)}
```

---The following area is a Code cell (cell numver is 18)---
```python
import torch.nn.functional as F

class CustomTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)
        else:
            self.class_weights = None

    def compute_loss(self, model, inputs, return_outputs=False):
#         print(inputs)
        labels = inputs.pop("labels").long()

        outputs = model(**inputs)

        logits = outputs.get('logits')

        if self.class_weights is not None:
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)
        else:
            loss = F.cross_entropy(logits, labels)

        return (loss, outputs) if return_outputs else loss
```

---The following area is a Code cell (cell numver is 19)---
```python
training_args = TrainingArguments(
    output_dir = 'sentiment_classification',
    learning_rate = 1e-4,
    per_device_train_batch_size = 8,
    per_device_eval_batch_size = 8,
    num_train_epochs = 1,
    logging_steps=1,
    weight_decay = 0.01,
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    load_best_model_at_end = True,
    report_to="none"
)
```

---The following area is a Code cell (cell numver is 20)---
```python
trainer = CustomTrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_data,
    tokenizer = tokenizer,
    data_collator = collate_fn,
    compute_metrics = compute_metrics,
#     class_weights=class_weights,
)

train_result = trainer.train()
```

---The following area is a Code cell (cell numver is 21)---
```python

```

** @@@ Jupyter Notebook numver 42, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import numpy as np
from datasets import Dataset
from functools import partial
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, TFAutoModel
from transformers import DebertaV2Tokenizer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input
from keras.preprocessing import sequence as sq
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train.head(5)
```

---The following area is a Code cell (cell numver is 4)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test.head(5)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Preprocessing
```

---The following area is a Code cell (cell numver is 6)---
```python
def combine_text(df):
    def process(input_str):
        stripped_str = input_str.strip('[]')
        sentences = [s.strip('"') for s in stripped_str.split('","')]
        return  ' '.join(sentences)

    # Flatten the arrays to single strings
    df['prompt'] = df['prompt'].apply(process)
    df['response_a'] = df['response_a'].apply(process)
    df['response_b'] = df['response_b'].apply(process)
    
    # Combine text data
#     df['combined_text'] = '[PROMPT] ' + df['prompt'] + ' [RESPONSE_A] ' + df['response_a'] + ' [RESPONSE_B] ' + df['response_b'] 
```

---The following area is a Code cell (cell numver is 7)---
```python
combine_text(train)
# print(train['combined_text'][69])
```

---The following area is a Code cell (cell numver is 8)---
```python
# Create labels
def create_label(df):
    def process(row):
        if row['winner_model_a'] == 1:
            return 0
        elif row['winner_model_b'] == 1:
            return 1
        elif row['winner_tie'] == 1:
            return 2
        
    df['label'] = df.apply(process, axis=1)
```

---The following area is a Code cell (cell numver is 9)---
```python
create_label(train)
print(train['label'][69])
```

---The following area is a Code cell (cell numver is 10)---
```python
print("train.shape", train.shape)
train.head()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## Tokenizer
```

---The following area is a Code cell (cell numver is 12)---
```python
max_length = 1024
```

---The following area is a Code cell (cell numver is 13)---
```python
# Tokenization using DebertaV2Tokenizer
model_name = "/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1"
# model_name = "/kaggle/input/deberta-v3/pytorch/large/1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Limit the vocabulary size
# tokenizer.model_max_length = max_length
tokenizer.add_tokens(['[CLS]', '[SEP]', '[PAD]'], special_tokens=True)
```

---The following area is a Code cell (cell numver is 14)---
```python
def tokenize_df(df, tokenizer):
    # Check and set special tokens if they are not present
    if tokenizer.cls_token_id is None:
        tokenizer.cls_token_id = tokenizer.convert_tokens_to_ids('[CLS]')
    if tokenizer.sep_token_id is None:
        tokenizer.sep_token_id = tokenizer.convert_tokens_to_ids('[SEP]')
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
        
    def process(row):
        max_len = max_length - 2 # 2 separator tokens
        # Tokenize prompt
        prompt_tokens = tokenizer(row['prompt'], truncation=True, max_length=max_len//4)['input_ids']
        remaining_length = max_len - len(prompt_tokens)

        # Tokenize response A
        response_a_tokens = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2)['input_ids']
        remaining_length -= len(response_a_tokens)

        # Tokenize response B
        response_b_tokens = tokenizer(row['response_b'], truncation=True, max_length=remaining_length//2)['input_ids']

        # Add responses
        input_ids = [tokenizer.cls_token_id] + prompt_tokens + [tokenizer.sep_token_id] + response_a_tokens + [tokenizer.sep_token_id] + response_b_tokens
        token_type_ids = [0] * (len(prompt_tokens) + 2) + [1] * (len(response_a_tokens) + 1) + [2] * len(response_b_tokens)
        attention_mask = [1] * len(input_ids)

        # Add padding
        padding_length = max_length - len(input_ids)
        if padding_length > 0:
            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
            token_type_ids = token_type_ids + [0] * padding_length
            attention_mask = attention_mask + [0] * padding_length

        input_ids = input_ids[:max_length]
        token_type_ids = token_type_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        
        return input_ids, token_type_ids, attention_mask
    
    df[['input_ids', 'token_type_ids', 'attention_mask']] = df.apply(lambda row: pd.Series(process(row)), axis=1)
#     tokenized = df.apply(lambda row: pd.Series(process(row)), axis=1)
#     df.loc[:, ['input_ids', 'token_type_ids', 'attention_mask']] = tokenized
#     return df
```

---The following area is a Code cell (cell numver is 15)---
```python
# Convert labels to categorical
labels = tf.keras.utils.to_categorical(train['label'], num_classes=3)
```

---The following area is a Code cell (cell numver is 16)---
```python
tokenize_df(train, tokenizer)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Prepare data for training
input_ids = train['input_ids']
attention_mask = train['attention_mask']

X_train = sq.pad_sequences(input_ids, maxlen=max_length)
X_train_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)

y_train = labels
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Model
```

---The following area is a Code cell (cell numver is 19)---
```python
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D, Masking

# Define the CNN model
def create_cnn_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        Conv1D(filters=256, kernel_size=5, activation='relu'),
        GlobalMaxPooling1D(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Define the LSTM model
def create_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        LSTM(256, return_sequences=True),
        GlobalMaxPooling1D(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Define the CNN LSTM model
def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        Conv1D(filters=128, kernel_size=5, activation='relu'),
        LSTM(128, return_sequences=True),
        GlobalMaxPooling1D(),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
```

---The following area is a Code cell (cell numver is 20)---
```python
# Parameters
vocab_size = tokenizer.vocab_size
# vocab_size = max_length
embedding_dim = 100
max_length = max_length
max_features = tokenizer.vocab_size
# max_features = max_length * 2
max_len = max_length
maxlen = max_len
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
# Create the model
# model = create_lstm_model(vocab_size, embedding_dim, max_length)
model = create_cnn_lstm_model(vocab_size, embedding_dim, max_length)
model.summary()
```

---The following area is a Code cell (cell numver is 21)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint
```

---The following area is a Code cell (cell numver is 22)---
```python
# from tensorflow.keras.layers import Layer
# from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D
# from keras.layers import  GlobalMaxPooling1D
# import tensorflow as tf

# class ApplyAttentionMask(Layer):
#     def call(self, inputs):
#         embeddings, attention_mask = inputs
#         return embeddings * tf.expand_dims(attention_mask, -1)

# input_layer = Input(shape=(max_length,),dtype='int32', name='main_input')
# attention_masks = Input(shape=(max_length,), dtype='float32', name="attention_masks")

# emb_layer = Embedding(max_features,
#                       embedding_dims,
#                       input_length=max_len
#                       )(input_layer)

# masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])

# # LSTM branch
# lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)
# lstm_out = LSTM(64, return_sequences=True)(lstm_out)
# lstm_out = LSTM(32)(lstm_out)
# lstm_out = BatchNormalization()(lstm_out)
# lstm_out = Dropout(0.5)(lstm_out)
# lstm_out = GlobalMaxPooling1D()(lstm_out)

# # CNN layer branch
# cnn_out = Conv1D(128, 5, activation='relu')(masked_embeddings)
# cnn_out = Conv1D(64, 5, activation='relu')(cnn_out)
# cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)
# cnn_out = BatchNormalization()(cnn_out)
# cnn_out = Dropout(0.5)(cnn_out)
# cnn_out = GlobalMaxPooling1D()(cnn_out)


# # Concatenate LSTM and CNN outputs
# merged = concatenate([lstm_out, cnn_out])
# merged = Dense(32, activation='sigmoid')(merged)
# merged = BatchNormalization()(merged)
# merged = Dropout(0.5)(merged)
# pred = Dense(3, activation='softmax')(merged)


# # Build model
# model = Model(inputs=[input_layer, attention_masks], outputs=[pred])
# adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
# adamax = Adamax(learning_rate=0.001)
# model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])
# model.summary()
```

---The following area is a Code cell (cell numver is 23)---
```python
# import tensorflow as tf
# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Masking
# from tensorflow.keras.models import Model
# from transformers import DebertaTokenizer, AutoModel

# def create_cnn_lstm_hybrid_model(base_model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes):
#     # Load pre-trained BERT model
#     model = AutoModel.from_pretrained(base_model_name)
    
#     # Define inputs
#     input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')
#     attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')

#     # Get BERT outputs
#     outputs = model(input_ids, attention_mask=attention_mask)
#     seq_output = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)

#     # Apply CNN
#     cnn_output = Conv1D(filters=cnn_output_channels, kernel_size=cnn_kernel_size, padding='same', activation='relu')(bert_seq_output)

#     # Apply Masking to handle padded sequences
#     masked_cnn_output = Masking()(cnn_output)
    
#     # Apply LSTM
#     rnn_output = LSTM(hidden_dim)(masked_cnn_output)

#     # Define the classifier layer
#     logits = Dense(num_classes, activation='softmax')(rnn_output)

#     # Create the model
#     model = Model(inputs=[input_ids, attention_mask], outputs=logits)

#     return model
```

---The following area is a Code cell (cell numver is 24)---
```python
# Define hyperparameters
# bert_model_name = '/kaggle/input/deberta_v3/keras/deberta_v3_large_en/2'
# bert_model_name = '/kaggle/input/deberta-v3/pytorch/large/1'
# bert_model_name = 'deberta_v3_large_en'
# model_name = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'
# cnn_output_channels = 128
# cnn_kernel_size = 5
# hidden_dim = 256
# num_classes = 3

# # Initialize the model
# model = create_cnn_lstm_hybrid_model(model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes)
# model.summary()
```

---The following area is a Code cell (cell numver is 25)---
```python
from keras.callbacks import EarlyStopping

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)

history = model.fit([X_train, X_train_attention_mask], y_train, epochs=20, batch_size=32, validation_split=0.2
                    , callbacks=[early_stopping])
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Test
```

---The following area is a Code cell (cell numver is 27)---
```python
# Encode test data
combine_text(test)
tokenize_df(test, tokenizer)

input_ids = test['input_ids']
attention_mask = test['attention_mask']

X_test = sq.pad_sequences(input_ids, maxlen=max_length)
X_test_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)
```

---The following area is a Code cell (cell numver is 28)---
```python
predictions = model.predict([X_test, X_test_attention_mask])
predictions
```

---The following area is a Code cell (cell numver is 29)---
```python
winner = pd.DataFrame(predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result = pd.concat([test['id'], winner], axis=1)
result.to_csv('submission.csv', index=False)
result
```

** @@@ Jupyter Notebook numver 43, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Setup
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## Model Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Load sample data
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv(TRAIN_CSV)
train = train.head(20)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenizer and Preprocess dataset
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 9)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Metrics
```

---The following area is a Code cell (cell numver is 11)---
```python
def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Model
```

---The following area is a Code cell (cell numver is 13)---
```python
class CustomLlama3(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 14)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 15)---
```python
model = CustomLlama3.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
### Training Arguments
```

---The following area is a Code cell (cell numver is 17)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=5,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2, ##GPU 16GB o
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Training (Only Run one of Training or Inference)
```

---The following area is a Code cell (cell numver is 19)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

---The following area is a Code cell (cell numver is 20)---
```python
model.save_pretrained('pretrained_model')
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# Inference (Only Run one of Training or Inference)
```

---The following area is a Code cell (cell numver is 22)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 23)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  file Lora adapter
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
lora_adapter_path = "/kaggle/working/pretrained_model"

# Táº£i mÃ´ hÃ¬nh gá»‘c
model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# Táº£i tokenizer tÆ°Æ¡ng á»©ng
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Cáº¥u hÃ¬nh Lora
lora_config = peft_config

# Chuáº©n bá»‹ mÃ´ hÃ¬nh cho k-bit training náº¿u cáº§n
model_1 = prepare_model_for_kbit_training(model_1)

# Ãp dá»¥ng Lora Adapter vÃ o mÃ´ hÃ¬nh
model_1 = get_peft_model(model_1, lora_config)

# Táº£i cÃ¡c tham sá»‘ cá»§a Lora Adapter Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
model_1.load_adapter(lora_adapter_path, adapter_name="test")

# MÃ´ hÃ¬nh hoÃ n chá»‰nh Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng
model_1.eval()  # Äáº·t mÃ´ hÃ¬nh vÃ o cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ náº¿u cáº§n
```

---The following area is a Code cell (cell numver is 24)---
```python
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
import pandas as pd
import numpy as np

def softmax(row):
    e_row = np.exp(row - np.max(row))
    return e_row / e_row.sum()
```

---The following area is a Code cell (cell numver is 25)---
```python
data = ds.to_pandas()[0:10]
data["max_len"] = data["input_ids"].apply(len)
display(data[:3])
print()

print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 26)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=2, max_length=1024):
    a_win, b_win, tie = [], [], []

    model.eval()
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        labels = tmp["labels"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        pad_labels=[]
        for label in labels:
            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])
            pad_labels.append(label)
        labels = torch.tensor(pad_labels).to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    df['winner_model_a'] = a_win
    df['winner_model_b'] = b_win
    df['winner_tie'] = tie
    return df
```

---The following area is a Code cell (cell numver is 27)---
```python
result_df  = inference(data[0:4], model_1, device, batch_size=2, max_length=1024)

proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values
        
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]

result_df['winner_model_a'] = result_df['winner_model_a'].apply(lambda x: x[0])
result_df['winner_model_b'] = result_df['winner_model_b'].apply(lambda x: x[0])
result_df['winner_tie'] = result_df['winner_tie'].apply(lambda x: x[0])

result_df
```

** @@@ Jupyter Notebook numver 44, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

```

---The following area is a Code cell (cell numver is 1)---
```python
INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
train_df = pd.read_csv(f"{INPUT_DIR}/train.csv")

```

---The following area is a Code cell (cell numver is 2)---
```python
train_df
```

---The following area is a Code cell (cell numver is 3)---
```python
train_df.info()

```

---The following area is a Code cell (cell numver is 4)---
```python
MODEL_ID = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModel.from_pretrained(MODEL_ID)
```

---The following area is a Code cell (cell numver is 5)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Move model to GPU
model.to(device)

```

---The following area is a Code cell (cell numver is 7)---
```python
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = '[PAD]'

```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_text(text):
    return tokenizer(text, padding=True, truncation=True, return_tensors="pt").to(device)
```

---The following area is a Code cell (cell numver is 9)---
```python
train_df['prompt_tokens'] = train_df['prompt'].apply(tokenize_text)
train_df['response_a_tokens'] = train_df['response_a'].apply(tokenize_text)
train_df['response_b_tokens'] = train_df['response_b'].apply(tokenize_text)

```

---The following area is a Code cell (cell numver is 10)---
```python
train_df
```

---The following area is a Code cell (cell numver is 11)---
```python
def get_embeddings(text_tokens):
    with torch.no_grad():
        outputs = model(**text_tokens)
    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

```

---The following area is a Code cell (cell numver is 12)---
```python
train_df['prompt_embeddings'] = train_df['prompt_tokens'].apply(lambda x: get_embeddings(x))
train_df['response_a_embeddings'] = train_df['response_a_tokens'].apply(lambda x: get_embeddings(x))
train_df['response_b_embeddings'] = train_df['response_b_tokens'].apply(lambda x: get_embeddings(x))

```

---The following area is a Code cell (cell numver is 13)---
```python
X = pd.concat([pd.DataFrame(train_df['prompt_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_a_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_b_embeddings'].tolist())], axis=1)
y = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)

```

---The following area is a Code cell (cell numver is 14)---
```python
#input 
X
```

---The following area is a Code cell (cell numver is 15)---
```python
#output
y
```

---The following area is a Code cell (cell numver is 16)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 17)---
```python
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

```

---The following area is a Code cell (cell numver is 18)---
```python
y_pred = clf.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)
print(f'Validation Accuracy: {accuracy:.4f}')

```

---The following area is a Code cell (cell numver is 19)---
```python
test_df = pd.read_csv(f"{INPUT_DIR}/test.csv")
```

---The following area is a Code cell (cell numver is 20)---
```python
# Process Test Data
test_df['prompt_tokens'] = test_df['prompt'].apply(tokenize_text)
test_df['response_a_tokens'] = test_df['response_a'].apply(tokenize_text)
test_df['response_b_tokens'] = test_df['response_b'].apply(tokenize_text)

test_df['prompt_embeddings'] = test_df['prompt_tokens'].apply(get_embeddings)
test_df['response_a_embeddings'] = test_df['response_a_tokens'].apply(get_embeddings)
test_df['response_b_embeddings'] = test_df['response_b_tokens'].apply(get_embeddings)

# Prepare Test Features
X_test = pd.concat([pd.DataFrame(test_df['prompt_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_a_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_b_embeddings'].tolist())], axis=1)
```

---The following area is a Code cell (cell numver is 21)---
```python
X_test
```

---The following area is a Code cell (cell numver is 22)---
```python
y_test_pred_prob = clf.predict_proba(X_test)

submission_df = pd.DataFrame(y_test_pred_prob, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission_df.insert(0, 'id', test_df['id'])

```

---The following area is a Code cell (cell numver is 23)---
```python
submission_df
```

---The following area is a Code cell (cell numver is 24)---
```python
submission_df.to_csv('/kaggle/working/submission.csv', index=False)

print("Submission file created.")
```

** @@@ Jupyter Notebook numver 45, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
```

---The following area is a Code cell (cell numver is 1)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Create a new DataFrame with the same 'id' column as 'test'
sample_submission = pd.DataFrame({'id': test['id']})

# Generate random winner columns
n_rows = len(test)
winners = np.random.choice(['a', 'b', 'tie'], size=n_rows)

# Create the winner columns
sample_submission['winner_model_a'] = (winners == 'a').astype(int)
sample_submission['winner_model_b'] = (winners == 'b').astype(int)
sample_submission['winner_tie'] = (winners == 'tie').astype(int)

# Verify that the sum of winner columns is 1 for each row
assert (sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1) == 1).all()

# Display the first few rows of the new DataFrame
print(sample_submission.head())
```

---The following area is a Code cell (cell numver is 3)---
```python
sample_submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 46, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction
 This notebook provides a comprehensive guide for loading, preprocessing, and training a text classification model using the Hugging Face Transformers library. We will use the Roberta tokenizer and a smaller Roberta model to manage computational resources efficiently. The process includes data loading, sampling, tokenization, and model preparation.

# Step 1: Load and Sample Data
 First, we load the dataset from a CSV file and take a random sample to reduce the dataset size for quicker processing.
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")

# Sample 10% of the data
df_sample = df.sample(frac=0.1, random_state=42)

# Check sample data
print(df_sample.head())
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Step 2: Split Data into Training and Validation Sets
We split the data into training and validation sets to evaluate the model's performance.
```

---The following area is a Code cell (cell numver is 3)---
```python
# Split data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_sample['prompt'].tolist(), 
    df_sample['winner_model_a'], 
    test_size=0.1, 
    random_state=42
)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Step 3: Load and Prepare the Tokenizer
We use a smaller Roberta model tokenizer for efficiency and tokenize the text data.
```

---The following area is a Code cell (cell numver is 5)---
```python
from transformers import RobertaTokenizer

# Load tokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Tokenize function
def tokenize_function(texts):
    return tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

# Tokenize texts
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# Check tokenized data
print(train_encodings)
print(val_encodings)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Step 4: Prepare the Model Configuration
 Set up the configuration for the model, ensuring it uses the appropriate device (GPU if available).
```

---The following area is a Code cell (cell numver is 7)---
```python
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification
from torch.utils.data import Dataset
import torch
import numpy as np

# Config class
class Config:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = 16

cfg = Config()

# Load model
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2).to(cfg.device)

# CustomDataset class
class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = np.array(labels, dtype=int)
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
    
    def __len__(self):
        return len(self.labels)

# Create datasets
train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=cfg.batch_size,
    per_device_eval_batch_size=cfg.batch_size,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train model
trainer.train()
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Step 5: Inference Function
Define a function to perform inference on the tokenized data using the trained model.
```

---The following area is a Code cell (cell numver is 9)---
```python
def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):
    model.eval()
    results = []
    with torch.no_grad():
        for i in range(0, len(input_ids), batch_size):
            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)
            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
            results.extend(outputs.logits.cpu().numpy())
    return np.array(results)
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# Step 6: Evaluate and Save Results
Tokenize the test data, perform inference, and save the results to a CSV file.
```

---The following area is a Code cell (cell numver is 11)---
```python
# Tokenize the test data
test_texts = df_sample['prompt'].tolist()
test_encodings = tokenize_function(test_texts)

# Perform inference
results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])

# Convert results to DataFrame and save as CSV
results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])
submission = pd.concat([df_sample[['id']], results_df], axis=1)
submission.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Conclusion
This notebook provided a structured approach to loading, preprocessing, and training a text classification model using the Transformers library. The steps included data sampling, tokenization, model configuration, and inference. The final results were saved for further analysis. This methodology ensures efficient use of computational resources while maintaining model performance.
```

** @@@ Jupyter Notebook numver 47, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS - Chatbot Arena Human Preference Predictions
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
**Due to the size of the train data, and I only using 0.5% of the train data!**

WIP: Compute embeddings using TPU in a differente notebook to use the full train data and then load the embeddings here!
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Install and load libraries
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install textstat SweetViz
```

---The following area is a Code cell (cell numver is 4)---
```python
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import sweetviz as sv

import re
import string
import nltk
from nltk.corpus import stopwords

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from nltk.sentiment.vader import SentimentIntensityAnalyzer

import textstat
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.model_selection import train_test_split

from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, TFBertModel

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, \
                            log_loss, \
                            confusion_matrix, \
                            classification_report
```

---The following area is a Code cell (cell numver is 5)---
```python
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## Loading data
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Exploratory Data Analysis (EDA)
```

---The following area is a Code cell (cell numver is 9)---
```python
train_analysis = sv.analyze(train_data)
```

---The following area is a Code cell (cell numver is 10)---
```python
train_analysis.show_html('train_analysis.html')
```

---The following area is a Code cell (cell numver is 11)---
```python
train_data.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
print("Training Data -", train_data.shape)
print("Test Data -", test_data.shape)
```

---The following area is a Code cell (cell numver is 13)---
```python
train_data.describe(include=['O'])
```

---The following area is a Code cell (cell numver is 14)---
```python
print(train_data.info())
```

---The following area is a Code cell (cell numver is 15)---
```python
train_data.drop("id", axis=1).duplicated().sum()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
There exist 14 duplicated rows forming 7 groups, I will just keep one row per group.
```

---The following area is a Code cell (cell numver is 17)---
```python
train_data = train_data.drop_duplicates(keep="first", ignore_index=True)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
Checking the quality of the train data with respect the `id` column.
```

---The following area is a Code cell (cell numver is 19)---
```python
train_data.nunique()
```

---The following area is a Code cell (cell numver is 20)---
```python
assert train_data["id"].nunique() == len(train_data)
```

---The following area is a Code cell (cell numver is 21)---
```python
train_data.isna().sum()
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
### Distribution
```

---The following area is a Code cell (cell numver is 23)---
```python
model_df = pd.concat([train_data.model_a, train_data.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()
```

---The following area is a Code cell (cell numver is 24)---
```python
counts_a = train_data['winner_model_a'].value_counts().reset_index()
counts_b = train_data['winner_model_b'].value_counts().reset_index()
counts_tie = train_data['winner_tie'].value_counts().reset_index()

# Renaming columns for convinience
counts_a.columns = ['Winner', 'Count']
counts_b.columns = ['Winner', 'Count']
counts_tie.columns = ['Winner', 'Count']

# Adding column to identify the model
counts_a['Model'] = 'Model A'
counts_b['Model'] = 'Model B'
counts_tie['Model'] = 'Tie'

counts = pd.concat([counts_a, counts_b, counts_tie])

fig = px.bar(counts, x='Model', y='Count', 
             color='Model',
             title='Winner Distribution for Train Data',
             labels={'Model': 'Model', 'Count': 'Win Count', 'Winner': 'Winner'})

fig.update_layout(xaxis_title="Model", yaxis_title="Win Count")

fig.show()
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
Conclusions:

* There are 57477 training rows and 3 test rows.
    * Note: Test data will be replaced with the full test set (~25k rows, 70% for private LB) during scoring phase.
* The column `id` has no duplicated values.
* Model identities aren't revealed in the test set.
* Strings in columns prompt, `response_a`, and `response_a` are wrapped in a list. 
    * The reason is that each chat can contains more than one prompt/response pairs.
* After dropping `id` column, there exist 14 duplicated rows forming 7 groups, we just keep one row per group and shape of the training DataFrame becomes (57470, 8).
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Data preparation and Feature Engineering

* Cleaning data: clean text, such as removing special characters, normalizing to lowercase, removing stopwords and tokenizing.
* Tokenize Inputs: using the TensorFlow/Kerar tokenizer by training on training data and fitting on both training and test data.
* Padding sequences to `max_len`.
* Create BERT embeddings.
* Compute Similarity Features using BERT between the prompt and responses for each model. 
* Compute word count, character count, and lexical diversity for each response.
* Tokenize the text inputs for the BERT model.
```

---The following area is a Code cell (cell numver is 27)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv').sample(frac=0.001)
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
### Cleaning data

Clean text, such as removing special characters, normalizing to lowercase and removing stopwords.
```

---The following area is a Code cell (cell numver is 29)---
```python
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text
```

---The following area is a Code cell (cell numver is 30)---
```python
# Cleaning texts
train_data['prompt_clean'] = train_data['prompt'].apply(clean_text)
train_data['response_a_clean'] = train_data['response_a'].apply(clean_text)
train_data['response_b_clean'] = train_data['response_b'].apply(clean_text)
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
### Tokenize Inputs

Using the TensorFlow/Kerar tokenizer on both training and test data. Padding sequences to `max_len`.
```

---The following area is a Code cell (cell numver is 32)---
```python
max_len = 512
```

---The following area is a Code cell (cell numver is 33)---
```python
tokenizer = Tokenizer(num_words=20000)
```

---The following area is a Code cell (cell numver is 34)---
```python
tokenizer.fit_on_texts(pd.concat([train_data['prompt_clean'], train_data['response_a_clean'], train_data['response_b_clean']]))
```

---The following area is a Code cell (cell numver is 35)---
```python
train_sequences = tokenizer.texts_to_sequences(train_data['prompt_clean'])
response_a_sequences = tokenizer.texts_to_sequences(train_data['response_a_clean'])
response_b_sequences = tokenizer.texts_to_sequences(train_data['response_b_clean'])
```

---The following area is a Code cell (cell numver is 36)---
```python
# Padding
train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')
response_a_sequences = pad_sequences(response_a_sequences, maxlen=max_len, padding='post')
response_b_sequences = pad_sequences(response_b_sequences, maxlen=max_len, padding='post')
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
### Sentiment Analysis

Sentiment analysis using `vaderSentiment`. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media
```

---The following area is a Code cell (cell numver is 38)---
```python
analyzer = SentimentIntensityAnalyzer()
```

---The following area is a Code cell (cell numver is 39)---
```python
def sentiment_analysis(text):
    return analyzer.polarity_scores(text)['compound']
```

---The following area is a Code cell (cell numver is 40)---
```python
train_data['sentiment_prompt'] = train_data['prompt_clean'].apply(sentiment_analysis)
train_data['sentiment_response_a'] = train_data['response_a_clean'].apply(sentiment_analysis)
train_data['sentiment_response_b'] = train_data['response_b_clean'].apply(sentiment_analysis)
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
### Text Features

Calculate text features such as word count, character count, 
lexical diversity, syllable count, sentence count and 
calculating the Flesch Reading Ease score (quantitative 
measurement of how readable a piece of text is) for each response. 

I am using `textstat` library that analyze text statistics.
```

---The following area is a Code cell (cell numver is 42)---
```python
def word_count(text):
    return len(text.split())

def char_count(text):
    return len(text)

def lexical_diversity(text):
    words = text.split()
    return len(set(words)) / len(words) if words else 0

def syllable_count(text):
    return textstat.syllable_count(text)

def sentence_count(text):
    return textstat.sentence_count(text)

def flesch_reading_ease(text):
    return textstat.flesch_reading_ease(text)
```

---The following area is a Code cell (cell numver is 43)---
```python
train_data['word_count_prompt'] = train_data['prompt_clean'].apply(word_count)
train_data['word_count_response_a'] = train_data['response_a_clean'].apply(word_count)
train_data['word_count_response_b'] = train_data['response_b_clean'].apply(word_count)
train_data['char_count_prompt'] = train_data['prompt_clean'].apply(char_count)
train_data['char_count_response_a'] = train_data['response_a_clean'].apply(char_count)
train_data['char_count_response_b'] = train_data['response_b_clean'].apply(char_count)
train_data['lexical_diversity_prompt'] = train_data['prompt_clean'].apply(lexical_diversity)
train_data['lexical_diversity_response_a'] = train_data['response_a_clean'].apply(lexical_diversity)
train_data['lexical_diversity_response_b'] = train_data['response_b_clean'].apply(lexical_diversity)
train_data['syllable_count_prompt'] = train_data['prompt_clean'].apply(syllable_count)
train_data['syllable_count_response_a'] = train_data['response_a_clean'].apply(syllable_count)
train_data['syllable_count_response_b'] = train_data['response_b_clean'].apply(syllable_count)
train_data['sentence_count_prompt'] = train_data['prompt_clean'].apply(sentence_count)
train_data['sentence_count_response_a'] = train_data['response_a_clean'].apply(sentence_count)
train_data['sentence_count_response_b'] = train_data['response_b_clean'].apply(sentence_count)
train_data['flesch_reading_ease_prompt'] = train_data['prompt_clean'].apply(flesch_reading_ease)
train_data['flesch_reading_ease_response_a'] = train_data['response_a_clean'].apply(flesch_reading_ease)
train_data['flesch_reading_ease_response_b'] = train_data['response_b_clean'].apply(flesch_reading_ease)
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
### Create BERT embeddings

Compute BERT embeddings for prompt and response for both train and test data.
Also compute Cosine Similarity features using BERT between the prompt and responses for each model. 

I will use `tf.data.Dataset` to create an efficient pipeline, and process the features in batches using GPU. Also I am going to save the intermediate embeddings using `joblib` library
```

---The following area is a Code cell (cell numver is 45)---
```python
# Load BERT
bert_model_name = 'bert-base-uncased'
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
bert_model = TFBertModel.from_pretrained(bert_model_name)
```

---The following area is a Code cell (cell numver is 46)---
```python
@tf.function
def get_bert_embeddings(texts):
    inputs = bert_tokenizer(texts, 
                       return_tensors='tf', 
                       padding=True, 
                       truncation=True, 
                       max_length=512)
    outputs = bert_model(inputs)
    return outputs.last_hidden_state[:, 0, :]
```

---The following area is a Code cell (cell numver is 47)---
```python
def process_column(column_data):
    column_data = column_data.dropna().tolist()
    column_data = [str(text) for text in column_data]  
    dataset = tf.data.Dataset.from_tensor_slices(column_data)
    dataset = dataset.batch(8)  
    
    embeddings = []
    for batch in dataset:
        batch_list = [str(text) for text in batch.numpy().tolist()]  
        batch_embeddings = get_bert_embeddings(batch_list)
        embeddings.append(batch_embeddings)
    
    return np.concatenate(embeddings, axis=0)
```

---The following area is a Code cell (cell numver is 48)---
```python
def add_embeddings_to_dataframe(df, column_names):
    for column in column_names:
        print(f"Processing column: {column}")
        embeddings = process_column(df[column])
        df[f'{column}_embedding'] = list(embeddings)
    return df
```

---The following area is a Code cell (cell numver is 49)---
```python
columns_to_embed = ['prompt_clean', 'response_a_clean', 'response_b_clean']
```

---The following area is a Code cell (cell numver is 50)---
```python
train_data = add_embeddings_to_dataframe(train_data, columns_to_embed)
```

---The following area is a Code cell (cell numver is 51)---
```python
train_data['similarity_prompt_response_a'] = train_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)

train_data['similarity_prompt_response_b'] = train_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
### Prepare Data
```

---The following area is a Code cell (cell numver is 53)---
```python
X = train_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',
                'char_count_prompt', 'char_count_response_a', 'char_count_response_b',
                'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',
                'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',
                'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',
                'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',
                'similarity_prompt_response_a', 'similarity_prompt_response_b', 
                'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]

```

---The following area is a Code cell (cell numver is 54)---
```python
# Definindo a coluna alvo
train_data['winner'] = train_data.apply(lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2), axis=1)
```

---The following area is a Code cell (cell numver is 55)---
```python
y = train_data['winner']
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
### Splitting training and validation data
```

---The following area is a Code cell (cell numver is 57)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, 
                                                  random_state=42)
```

---The following area is a Markdown cell (cell numver is 58)---
```markdown
## Defining Models

* Random Forest
* Logistic Regression
* Support Vector Machin
* Gradient Boosting
* Neural Network
```

---The following area is a Code cell (cell numver is 59)---
```python
models = {
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True),
    'Gradient Boosting': GradientBoostingClassifier()
}
```

---The following area is a Code cell (cell numver is 60)---
```python
# Creating Neural Network
def create_nn_model(input_shape):
    model = Sequential()
    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(3, activation='softmax'))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model
```

---The following area is a Code cell (cell numver is 61)---
```python
nn_model = create_nn_model(X_train.shape[1])
models['Neural Network'] = nn_model
```

---The following area is a Markdown cell (cell numver is 62)---
```markdown
Training models and evaluating:
```

---The following area is a Code cell (cell numver is 63)---
```python
results = {}

for name, model in models.items():
    print(f"Treinando e avaliando {name}...")
    
    if name == 'Neural Network':
        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=2)
        y_pred = np.argmax(model.predict(X_val), axis=1)
        y_pred_proba = model.predict(X_val)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)

    accuracy = accuracy_score(y_val, y_pred)
    logloss = log_loss(y_val, y_pred_proba)

    results[name] = {
        'AcurÃ¡cia': accuracy,
        'Log Loss': logloss,
        'RelatÃ³rio de ClassificaÃ§Ã£o': classification_report(y_val, y_pred),
        'Matriz de ConfusÃ£o': confusion_matrix(y_val, y_pred)
    }

    print(f"AcurÃ¡cia de {name}: {accuracy}")
    print(f"Log Loss de {name}: {logloss}")
    print(f"RelatÃ³rio de ClassificaÃ§Ã£o de {name}:\n{classification_report(y_val, y_pred)}")
    print(f"Matriz de ConfusÃ£o de {name}:\n{confusion_matrix(y_val, y_pred)}\n")
```

---The following area is a Markdown cell (cell numver is 64)---
```markdown
Selecting the best model with respect the Log Loss
```

---The following area is a Code cell (cell numver is 65)---
```python
best_model_name = max(results, key=lambda name: results[name]['Log Loss'])
best_model = models[best_model_name]
```

---The following area is a Markdown cell (cell numver is 66)---
```markdown
## Prediction and Submission

Preparing test data, predicting on test data and creating submission data.
```

---The following area is a Code cell (cell numver is 67)---
```python
# Cleaning text from test data
test_data['prompt_clean'] = test_data['prompt'].apply(clean_text)
test_data['response_a_clean'] = test_data['response_a'].apply(clean_text)
test_data['response_b_clean'] = test_data['response_b'].apply(clean_text)
```

---The following area is a Code cell (cell numver is 68)---
```python
# Tokenize text from test data
test_sequences = tokenizer.texts_to_sequences(test_data['prompt_clean'])
response_a_test_sequences = tokenizer.texts_to_sequences(test_data['response_a_clean'])
response_b_test_sequences = tokenizer.texts_to_sequences(test_data['response_b_clean'])
```

---The following area is a Code cell (cell numver is 69)---
```python
# Padding sequences from test data
test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')
response_a_test_sequences = pad_sequences(response_a_test_sequences, maxlen=max_len, padding='post')
response_b_test_sequences = pad_sequences(response_b_test_sequences, maxlen=max_len, padding='post')
```

---The following area is a Code cell (cell numver is 70)---
```python
# Sentiment analysis for test data
test_data['sentiment_prompt'] = test_data['prompt_clean'].apply(sentiment_analysis)
test_data['sentiment_response_a'] = test_data['response_a_clean'].apply(sentiment_analysis)
test_data['sentiment_response_b'] = test_data['response_b_clean'].apply(sentiment_analysis)
```

---The following area is a Code cell (cell numver is 71)---
```python
# Creating features of text structure from test data
test_data['word_count_prompt'] = test_data['prompt_clean'].apply(word_count)
test_data['word_count_response_a'] = test_data['response_a_clean'].apply(word_count)
test_data['word_count_response_b'] = test_data['response_b_clean'].apply(word_count)
test_data['char_count_prompt'] = test_data['prompt_clean'].apply(char_count)
test_data['char_count_response_a'] = test_data['response_a_clean'].apply(char_count)
test_data['char_count_response_b'] = test_data['response_b_clean'].apply(char_count)
test_data['lexical_diversity_prompt'] = test_data['prompt_clean'].apply(lexical_diversity)
test_data['lexical_diversity_response_a'] = test_data['response_a_clean'].apply(lexical_diversity)
test_data['lexical_diversity_response_b'] = test_data['response_b_clean'].apply(lexical_diversity)
test_data['syllable_count_prompt'] = test_data['prompt_clean'].apply(syllable_count)
test_data['syllable_count_response_a'] = test_data['response_a_clean'].apply(syllable_count)
test_data['syllable_count_response_b'] = test_data['response_b_clean'].apply(syllable_count)
test_data['sentence_count_prompt'] = test_data['prompt_clean'].apply(sentence_count)
test_data['sentence_count_response_a'] = test_data['response_a_clean'].apply(sentence_count)
test_data['sentence_count_response_b'] = test_data['response_b_clean'].apply(sentence_count)
test_data['flesch_reading_ease_prompt'] = test_data['prompt_clean'].apply(flesch_reading_ease)
test_data['flesch_reading_ease_response_a'] = test_data['response_a_clean'].apply(flesch_reading_ease)
test_data['flesch_reading_ease_response_b'] = test_data['response_b_clean'].apply(flesch_reading_ease)
```

---The following area is a Code cell (cell numver is 72)---
```python
# Embedding from test data
test_data = add_embeddings_to_dataframe(test_data, columns_to_embed)
```

---The following area is a Code cell (cell numver is 73)---
```python
# Cosine similarity from test data
test_data['similarity_prompt_response_a'] = test_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)

test_data['similarity_prompt_response_b'] = test_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)
```

---The following area is a Code cell (cell numver is 74)---
```python
X_test = test_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',
                    'char_count_prompt', 'char_count_response_a', 'char_count_response_b',
                    'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',
                    'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',
                    'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',
                    'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',
                    'similarity_prompt_response_a', 'similarity_prompt_response_b', 
                    'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]
```

---The following area is a Code cell (cell numver is 75)---
```python
test_pred_proba = best_model.predict(X_test)
```

---The following area is a Code cell (cell numver is 76)---
```python
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_pred_proba[:, 0]
submission['winner_model_b'] = test_pred_proba[:, 1]
submission['winner_tie'] = test_pred_proba[:, 2]

submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 77)---
```python
submission
```

** @@@ Jupyter Notebook numver 48, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 3)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "/kaggle/input/checkpoint-5200/checkpoint-5200"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 2  # global batch size is 8 
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 5)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 7)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 10)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 11)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
#ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 14)---
```python
ds
```

---The following area is a Code cell (cell numver is 15)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 16)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 18)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 20)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 21)---
```python
train_idx, eval_idx = folds[config.fold_idx]

trainer = Trainer(
    args=training_args, 
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)
trainer.train(resume_from_checkpoint="/kaggle/input/checkpoint-5200/checkpoint-5200")
```

** @@@ Jupyter Notebook numver 49, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import sys
sys.path.append('/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 2)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import textstat
from textblob import TextBlob
import spacy
import concurrent.futures
import optuna
from sklearn.metrics import log_loss
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
# nltk.download('punkt')
```

---The following area is a Code cell (cell numver is 5)---
```python

def calculate_readability_scores(text):
    # This function calculates various readability scores
    return {
        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
        "gunning_fog": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "ari": textstat.automated_readability_index(text),
        "coleman_liau_index": textstat.coleman_liau_index(text)
    }

def count_noun_phrases(text):
    blob = TextBlob(text)
    return len(blob.noun_phrases)

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

def count_passive_voice(text):
    doc = nlp(text)
    return sum(1 for token in doc if token.dep_ == 'auxpass')

def pos_tag_frequencies(text):
    words = word_tokenize(text)
    tags = nltk.pos_tag(words)
    freq_dist = nltk.FreqDist(tag for (word, tag) in tags)
    # Ensure all frequencies are stored in a consistent dictionary format
    return {tag: freq for tag, freq in freq_dist.items()}

def text_statistics(text):
    stats = calculate_readability_scores(text)
    stats.update({
        "word_count": len(word_tokenize(text)),
        "char_count": len(text),
        "sentence_count": len(sent_tokenize(text)),
        "avg_word_length": sum(len(word) for word in word_tokenize(text)) / len(word_tokenize(text)),
        "avg_sentence_length": sum(len(sent) for sent in sent_tokenize(text)) / len(sent_tokenize(text)),
        "lexical_diversity": len(set(word_tokenize(text))) / len(word_tokenize(text)),
        "noun_phrases_count": count_noun_phrases(text),
        "sentiment": analyze_sentiment(text),
        "passive_voice_count": count_passive_voice(text),
    })
    # Merge POS tag frequencies into the main stats dictionary
    pos_tags = pos_tag_frequencies(text)
    for tag, count in pos_tags.items():
        stats[f'pos_tag_{tag}'] = count
    return stats

def parallel_apply(df, column):
    # Drop NaN values to avoid errors during text processing
    texts = df[column].dropna()

    # Use ProcessPoolExecutor to apply the function in parallel
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = list(executor.map(text_statistics, texts))

    # Convert the list of dictionaries to a DataFrame
    results_df = pd.DataFrame(results)

    # Automatically handles missing POS tags by filling with 0 and converts data types appropriately
    # Filling missing POS tags handled by DataFrame initialization from dict
    results_df.fillna(0, inplace=True)
    for col in results_df.columns:
        if results_df[col].dtype == float:
            results_df[col] = results_df[col].astype(int)

    return results_df

```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
print('Data has been imported')
```

---The following area is a Code cell (cell numver is 7)---
```python
train.shape
```

---The following area is a Code cell (cell numver is 8)---
```python
# def parallel_apply(df, column):
#     with concurrent.futures.ProcessPoolExecutor() as executor:
#         results = list(executor.map(text_statistics, df[column].dropna()))  # Use dropna to handle NaNs gracefully
#     return pd.DataFrame(results)
```

---The following area is a Code cell (cell numver is 9)---
```python
# Applying parallel_apply to 'prompt' and 'response' columns
nlp = spacy.load('en_core_web_sm')
prompt_stats_df = parallel_apply(train, 'prompt')
response_a_stats_df = parallel_apply(train, 'response_a')
response_b_stats_df = parallel_apply(train, 'response_b')
```

---The following area is a Code cell (cell numver is 10)---
```python
train = train.join(prompt_stats_df.add_suffix('_prompt'))
train = train.join(response_a_stats_df.add_suffix('_response_a'))
train = train.join(response_b_stats_df.add_suffix('_response_b'))
```

---The following area is a Code cell (cell numver is 11)---
```python
train.shape
```

---The following area is a Code cell (cell numver is 12)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 13)---
```python
%%time

import time
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import log_loss
from scipy.stats import uniform, randint

# Convert the target into a single column with categorical labels
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# Define features and target
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]

X = train[features]
y = train['winner'] - 1
```

---The following area is a Code cell (cell numver is 14)---
```python
# Define the Optimization Function
def objective(trial):
    # Data splitting inside the trial
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Define model and hyperparameters to optimize
    model_type = trial.suggest_categorical('model_type', ['XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier'])
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    max_depth = trial.suggest_int('max_depth', 2, 10)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 0.1)

    if model_type == 'XGBClassifier':
        model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, use_label_encoder=False, eval_metric='logloss', random_state=42)
    elif model_type == 'LGBMClassifier':
        model = LGBMClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=42)
    elif model_type == 'CatBoostClassifier':
        model = CatBoostClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, verbose=0, random_state=42)

    # Training and evaluating the model
    model.fit(X_train, y_train)
    y_val_pred = model.predict_proba(X_val)
    return log_loss(y_val, y_val_pred)

# Run Optuna Optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=5)

print('Best trial:', study.best_trial.params)

# Train the Best Model on Full Data
best_params = study.best_trial.params
model_type = best_params.pop('model_type')

if model_type == 'XGBClassifier':
    final_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)
elif model_type == 'LGBMClassifier':
    final_model = LGBMClassifier(**best_params, random_state=42)
elif model_type == 'CatBoostClassifier':
    final_model = CatBoostClassifier(**best_params, verbose=0, random_state=42)

final_model.fit(X, y)  # Training on the full dataset
```

---The following area is a Code cell (cell numver is 15)---
```python
final_model
```

---The following area is a Code cell (cell numver is 16)---
```python
# Applying parallel_apply to 'prompt' and 'response' columns
prompt_stats_df_test = parallel_apply(test, 'prompt')
response_a_stats_df_test = parallel_apply(test, 'response_a')
response_b_stats_df_test = parallel_apply(test, 'response_b')
```

---The following area is a Code cell (cell numver is 17)---
```python
test = test.join(prompt_stats_df_test.add_suffix('_prompt'))
test = test.join(response_a_stats_df_test.add_suffix('_response_a'))
test = test.join(response_b_stats_df_test.add_suffix('_response_b'))
```

---The following area is a Code cell (cell numver is 18)---
```python
test = test[features]
test
```

---The following area is a Code cell (cell numver is 19)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 20)---
```python

test_predictions = final_model.predict_proba(test)
```

---The following area is a Code cell (cell numver is 21)---
```python
test_predictions
```

---The following area is a Code cell (cell numver is 22)---
```python
test_raw = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv', usecols=['id'])

```

---The following area is a Code cell (cell numver is 23)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test_raw['id'],
    'winner_model_a': test_predictions[:, 0],
    'winner_model_b': test_predictions[:, 1],
    'winner_tie': test_predictions[:, 2]
})

submission.head()
```

---The following area is a Code cell (cell numver is 24)---
```python
submission.to_csv('/kaggle/working/submission.csv', index= False)

```

** @@@ Jupyter Notebook numver 50, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction ðŸ“œ
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
âœ”ï¸ What is the objective of this notebook?

The goal is to create a robust and efficient solution to predict users' preference of LLM responses using LightGBM and TF-IDF vectorization.

---

âœ”ï¸ What does this notebook cover?

- `Data Loading & EDA`

- `Theory behind TF-IDF`

- `Data Preprocessing`

- `Model Training`
       
- `Model Inference`
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Imports ðŸ“¦
```

---The following area is a Code cell (cell numver is 3)---
```python
# Handle warning messages
import warnings
warnings.filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Data preprocessing
import numpy as np
import pandas as pd
from pathlib import Path

# Data visualization
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix

# Model development
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold

# TF-IDF Vectorization
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer

# Similarity/distance features for TF-IDF vectors
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Configuration âš™ï¸
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    # Paths to competition data
    train_data = Path("/kaggle/input/lmsys-chatbot-arena/train.csv")
    test_data = Path("/kaggle/input/lmsys-chatbot-arena/test.csv")
    subm_data = Path("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
    
    # Colorscale for confusion matrix
    colorscale = "peach"
    
    # TF-IDF Vectorization parameters
    components = 32
    ngrams = (1, 7) 
    max_freq = 0.95 # Words that occur in more than 95% of the documents are omitted
    min_freq = 10   # Words that occur in less than 10 documents are omitted
    
    # Training arguments
    num_classes = 3
    early_stop = 50
    log_steps = 100
    
    # LightGBM parameters
    params = {
        "objective": "multiclass",
        "colsample_bytree": 0.8,
        "colsample_bynode": 0.8,
        "metric": "multiclass",
        "learning_rate": 0.02,
        "extra_trees": True,
        "num_rounds": 3000,
        "reg_lambda": 1.3,
        "num_classes": 3,
        "num_leaves": 64,
        "reg_alpha": 0.1,
        "device": "cpu",
        "max_depth": 6,
        "max_bin": 128,
        "verbose": -1,
        "seed": 42
    }
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Exploratory Data Analysis (EDA) ðŸ—ƒï¸
```

---The following area is a Code cell (cell numver is 8)---
```python
class EDA:
    def read_data(self, path):
        # Read dataframe from path
        df = pd.read_csv(path)
        
        # Display the shape of the dataframe and the first 3 rows
        print(f"The shape of the dataframe is: {df.shape}")
        display(df.head(3))
        
        return df
    
    def pie_chart(self, data):
        # Calculate the counts for each winner column
        counts = {
            'winner_model_a': data['winner_model_a'].sum(),
            'winner_model_b': data['winner_model_b'].sum(),
            'winner_tie': data['winner_tie'].sum()
        }

        # Define the colors
        colors = ['#a89192', '#8083a8', '#a8c28c']  # creme, light blue, mint
        identifiers = ['Creme', 'Light Blue', 'Mint']
        
        # Create the pie chart
        fig = go.Figure(data=[go.Pie(labels=identifiers, 
                                     values=list(counts.values()), 
                                     textinfo='percent', 
                                     hole=0.1,
                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])
        
        # Update layout for a transparent background and move the pie to the left
        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', 
                          paper_bgcolor='rgba(0,0,0,0)', 
                          margin=dict(l=0, r=0, t=0, b=0))
        
        # Hide the legend
        fig.update_layout(showlegend=False)
        
        # Show the plot
        fig.show()

        # Display the counts as a table
        counts_df = pd.DataFrame(list(counts.items()), columns=['Class', 'Count'])
        counts_df['Identifier'] = identifiers
        display(counts_df)
        
    def response_length(self, data):
        # Create a copy of the dataframe to avoid modifying the original data
        data_copy = data.copy()
        
        # Calculate the number of words in each response
        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))
        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))
        
        # Calculate the average word count for each winner class
        word_counts = {
            'winner_model_a': int(
                data_copy[data_copy['winner_model_a'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            ),
            
            'winner_model_b': int(
                data_copy[data_copy['winner_model_b'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            ),
            
            'winner_tie': int(
                data_copy[data_copy['winner_tie'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            )
        }
        
        # Create custom hover text
        hover_texts = [f"Word Count: {value}<br>{key}" for key, value in word_counts.items()]
        
        # Create the bar chart
        fig = go.Figure(data=[go.Bar(
            x=list(word_counts.keys()),  # Winner class labels on x-axis
            y=list(word_counts.values()),
            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),
            hovertext=hover_texts,
            hoverinfo='text',
            orientation='v'  # Ensure bars are vertical
        )])
        
        # Update layout
        fig.update_layout(
            title='Average Response Word Count by Winner Class',
            xaxis_title='',
            yaxis_title='Average Response Word Count',
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis=dict(showticklabels=False)  # Hide x-axis labels
        )
        
        # Show the plot
        fig.show()
```

---The following area is a Code cell (cell numver is 9)---
```python
eda = EDA()
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data = eda.read_data(CFG.train_data)
```

---The following area is a Code cell (cell numver is 11)---
```python
test_data = eda.read_data(CFG.test_data)
```

---The following area is a Code cell (cell numver is 12)---
```python
subm_data = eda.read_data(CFG.subm_data)
```

---The following area is a Code cell (cell numver is 13)---
```python
print("Distribution of classes (winners):")
eda.pie_chart(train_data)
```

---The following area is a Code cell (cell numver is 14)---
```python
# Plot average response word count per winner model
eda.response_length(train_data)
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Theory ðŸ“’
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
âœ”ï¸ **Term Frequency - Inverse Document Frequency** or **TF-IDF** vectorization is used in text mining and information retrieval to assess the importance of words in a document relative to a corpus. This technique transforms text data into a numerical format suitable for machine learning algorithms.

---

âœ”ï¸ **Components of TF-IDF**

1. Term Frequency (TF):

   - *Definition:* Measures the frequency of a term in a document.
   
   - *Formula:* $ \text{TF}(t,d) = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t',d}} $ , where $ f_{t,d} $ is the frequency of term $ t $ in document $ d $.

2. Inverse Document Frequency (IDF):

   - *Definition:* Measures the importance of a term across the entire corpus.
   
   - *Formula:* $ \text{IDF}(t) = \log \left( \frac{N}{1 + n_t} \right) $ , where $ N $ is the total number of documents, and $ n_t $ is the number of documents containing term $ t $.

3. TF-IDF Score:

   - *Definition:* Product of TF and IDF scores.
   
   - *Formula:* $ \text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t) $
   
---

âœ”ï¸ ***N-grams* explained**

*N-grams* are contiguous sequences of $ n $ items (tokens) extracted from a text document. They provide a more comprehensive representation of the language structure and context compared to individual words.

*Formula:* $ N\text{-grams} = [t_1, t_2, ..., t_n] $

*Example:* For `ngrams = (1, 3)`, it means we are considering all possible combinations of tokens within a sliding window of length 3 in the text document. Each combination of 3 tokens represents a trigram. 

For instance, consider the sentence: "I love coding."

With `ngrams = (1, 3)`, the n-grams extracted from this sentence would include:

   * Unigrams (1-grams): ["I"], ["love"], ["coding"]
    
   * Bigrams (2-grams): ["I love"], ["love coding"]
    
   * Trigrams (3-grams): ["I love coding"]

This way, $ N-grams $ capture not only individual words but also phrases and contextual information within the text.
  
---
   
âœ”ï¸ **Steps of TF-IDF**

1. Tokenization:

   - *Definition:* Breaks text into tokens.
   
   - *Example:* "I love coding" -> ["I", "love", "coding"]

2. Document Frequency Calculation:

   - *Definition:* Counts the number of documents containing each term.
   
   - *Example:* "love" appears in 1 document out of 1.

3. TF-IDF Calculation:

   - *Definition:* Computes the TF-IDF score for each term in each document.
   
   - *Example:* For ngrams = (1, 3), "love" appears in Document 1, the TF-IDF score for "love" would be calculated based on its TF and IDF.

4. Vectorization:

   - *Definition:* Represents each document as a vector of TF-IDF scores.
   
   - *Example:* Each document becomes a high-dimensional vector where each dimension corresponds to a unique term or n-gram.
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Data Preprocessing ðŸ› ï¸
```

---The following area is a Code cell (cell numver is 18)---
```python
class DataPreprocessing:
    # Check if any value in the input list is None
    @staticmethod
    def retrieve_none(vals):
        return int(any(val is None for val in vals))

    # Calculate the total length of strings in the input list
    @staticmethod
    def retrieve_length(vals):
        length = 0
        for val in vals:
            if isinstance(val, str):
                length += len(val)
        return length
    
    # Calculate the count of unique works in the input list
    @staticmethod
    def retrieve_nuniques(vals):
        if isinstance(vals, str):
            return len(set(vals.split()))
        return 0
    
    # Replace 'None' in the list with the string 'NONE', and join elements with a space
    @staticmethod
    def clean_response(text):
        if isinstance(text, list):
            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])
            return cleaned_text

        return text

    def add_features(self, data):
        # Add features related to the length and presence of None values in response columns.
        data[f"response_a_len"] = data[f"response_a"].apply(self.retrieve_length)
        data[f"response_b_len"] = data[f"response_b"].apply(self.retrieve_length)

        # Calculate unique word count for responses
        data[f"response_a_unique"] = data[f"response_a"].apply(self.retrieve_nuniques)
        data[f"response_b_unique"] = data[f"response_b"].apply(self.retrieve_nuniques)

        # Calculate length difference, mean length, and length difference ratio.
        data["response_len_diff"] = data["response_a_len"] - data["response_b_len"]
        data["response_len_mean"] = (data["response_a_len"] + data["response_b_len"]) / 2
        data["response_diff_ratio"] = data["response_len_diff"] / data["response_len_mean"]

        # Calculate unique word count difference, mean, and ratio.
        data["response_unique_diff"] = data["response_a_unique"] - data["response_b_unique"]
        data["response_unique_mean"] = (data["response_a_unique"] + 
                                        data["response_b_unique"]) / 2
        data["response_unique_ratio"] = (data["response_unique_diff"] / 
                                         data["response_unique_mean"])

        # Check if any value in response columns is None.
        data["a_has_none"] = data["response_a"].apply(self.retrieve_none)
        data["b_has_none"] = data["response_b"].apply(self.retrieve_none)
        data["has_none_diff"] = data["a_has_none"] - data["b_has_none"]

        return data
    
    # Calculate cosine similarity between prompt and responses
    @staticmethod
    def calculate_cosine_similarity(tfidf_matrix, 
                                    prompt_idx, 
                                    response_a_idx, 
                                    response_b_idx):
        
        # Cosine similarity between prompt (p) and response_a (a)
        similarity_pa = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]

        # Cosine similarity between prompt (p) and response_b (b)
        similarity_pb = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]

        return similarity_pa, similarity_pb

    # Calculate distances (Euclidean/Laplacian) between prompt and responses
    @staticmethod
    def calculate_distances(tfidf_matrix, 
                            prompt_idx, 
                            response_a_idx, 
                            response_b_idx, 
                            distance_metric):
        
        # Distance between prompt (p) and response_a (a)
        distance_pa = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]
        
        # Distance between prompt (p) and response_b (b)
        distance_pb = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1),
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]
        
        return distance_pa, distance_pb

    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):
        # Initialize TF-IDF Vectorizer
        tfidf_vectorizer = TfidfVectorizer(analyzer='char', 
                                           ngram_range=ngrams, 
                                           min_df=min_freq, 
                                           max_df=max_freq,
                                           lowercase=False,
                                           sublinear_tf=True)

        # Combine train and test data into a single DataFrame
        full_data = pd.concat([train, test], ignore_index=True)

        # Clean and prepare the text columns
        for col in ['prompt', 'response_a', 'response_b']:
            full_data[col] = full_data[col].apply(self.clean_response)

        # Combine all text columns into a single corpus for TF-IDF vectorization
        full_corpus = pd.concat([full_data['prompt'], 
                                 full_data['response_a'], 
                                 full_data['response_b']], 
                                 ignore_index=True)

        # Compute the TF-IDF matrix
        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)

        # Perform dimensionality reduction with TruncatedSVD
        svd = TruncatedSVD(n_components=components, random_state=42)
        reduced_matrix = svd.fit_transform(tfidf_matrix)

        # Calculate split indices for separating different parts of the corpus
        len_full = len(full_data)
        split_index_01 = len_full
        split_index_02 = len_full * 2

        # Split the reduced matrix into prompts, response_a, and response_b parts
        full_tfidf_prompts = reduced_matrix[:split_index_01]
        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]
        full_tfidf_response_b = reduced_matrix[split_index_02:]

        # Separate the reduced matrix into training and testing sets
        len_train = len(train)
        train_tfidf_prompts = full_tfidf_prompts[:len_train]
        train_tfidf_response_a = full_tfidf_response_a[:len_train]
        train_tfidf_response_b = full_tfidf_response_b[:len_train]
        test_tfidf_prompts = full_tfidf_prompts[len_train:]
        test_tfidf_response_a = full_tfidf_response_a[len_train:]
        test_tfidf_response_b = full_tfidf_response_b[len_train:]

        # Create DataFrames to hold the SVD features for train and test sets
        feature_names = [f'svd_feature_{i}' for i in range(components)]
        train_features = pd.DataFrame(index=train.index)
        test_features = pd.DataFrame(index=test.index)

        # Assign SVD features to the respective columns in the feature DataFrames
        for i in range(components):
            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]
            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]
            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]
            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]
            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]
            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]

        # Concatenate the new features with the original train and test DataFrames
        train = pd.concat([train, train_features], axis=1)
        test = pd.concat([test, test_features], axis=1)

        # Calculate similarity and distance features
        for df, len_df in zip([train, test], [len(train), len(test)]):
            prompt_indices = df.index

            # Calculate cosine similarity features
            df['similarity_pa'], df['similarity_pb'] = zip(*[
                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)
                for i in prompt_indices
            ])

            # Calculate Euclidean distance features
            df['euclidean_pa'], df['euclidean_pb'] = zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         euclidean_distances)
                for i in prompt_indices
            ])

            # Calculate Laplacian kernel distance features
            df['laplacian_pa'], df['laplacian_pb']= zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         laplacian_kernel)
                for i in prompt_indices
            ])

        return train, test
    
    # Merges multiple labels into a single label
    def merge_label(self, row):
        if row["winner_model_a"] == 1:
            return 0
        if row["winner_model_b"] == 1:
            return 1
        if row["winner_tie"] == 1:
            return 2
        raise ValueError("The value is invalid.")
```

---The following area is a Code cell (cell numver is 19)---
```python
dp = DataPreprocessing()
```

---The following area is a Code cell (cell numver is 20)---
```python
# Add length, similarity and distance features
train_data = dp.add_features(train_data)
test_data = dp.add_features(test_data)
```

---The following area is a Code cell (cell numver is 21)---
```python
# Extract TF-IDF features and perform dimensionality reduction
train_data, test_data = dp.create_tfidf_features(train_data, 
                                                 test_data, 
                                                 CFG.ngrams,
                                                 CFG.min_freq, 
                                                 CFG.max_freq, 
                                                 CFG.components)
```

---The following area is a Code cell (cell numver is 22)---
```python
# Merge multiple labels into a single label
train_data["target"] = train_data[
    ["winner_model_a", "winner_model_b", "winner_tie"]
                                 ].apply(lambda x: dp.merge_label(x), axis=1)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Model Development ðŸ§ 
```

---The following area is a Code cell (cell numver is 24)---
```python
class ModelDevelopment:
    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):
        # Extract feature values and target labels from the training and testing data
        X_train = train_data[feature_cols].values
        X_test = test_data[feature_cols].values
        Y_train = train_data["target"]

        # List to store predictions
        train_preds_list = []
        test_preds_list = []

        # Initialize StratifiedKFold
        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):
            # Split the training data into training and validation sets for the current fold
            x_train, x_valid = X_train[train_index], X_train[valid_index]
            y_train, y_valid = Y_train[train_index], Y_train[valid_index]

            # Create LightGBM dataset objects for training and validation
            train = lgb.Dataset(x_train, y_train)
            valid = lgb.Dataset(x_valid, y_valid, reference=train)

            # Train the model on the current fold
            model = lgb.train(
                params,
                train,
                valid_sets=[train, valid],
                feature_name=feature_cols,
                callbacks=[lgb.early_stopping(early_stop),
                           lgb.log_evaluation(log_steps)])

            # Make predictions on the train and test sets
            train_preds = model.predict(X_train)
            test_preds = model.predict(X_test)

            train_preds_list.append(train_preds)
            test_preds_list.append(test_preds)

        # Average predictions
        train_preds = np.mean(train_preds_list, axis=0)
        test_preds = np.mean(test_preds_list, axis=0)

        return train_preds, test_preds
    
    # Confusion matrix for train data predictions
    def plot_cm(self, y_true, y_pred, labels, colorscale):
        cm = confusion_matrix(y_true, y_pred, labels=labels)

        # Create a custom hover text formatter
        def format_hover_text(value):
            if value >= 10000:
                return str(int(value))  # Convert to integer without commas or "k"
            else:
                return str(value)

        # Create the heatmap
        fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=labels,
            y=labels,
            colorscale=colorscale,
            zmin=0,
            zmax=20000,
            text=cm,
            texttemplate="%{text:.0f}",
            hovertemplate="True: %{y}<br>Predicted: %{x}<br>Count: %{z:,.0f}<extra></extra>",
            customdata=[format_hover_text(value) for value in cm.flatten()]
        ))

        # Update layout for a transparent background and square aspect ratio
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis_title="Predicted Labels",
            yaxis_title="True Labels",
            xaxis=dict(constrain='domain'),
            yaxis=dict(constrain='domain', scaleanchor='x'),
            width=650,  
            height=650,  
            margin=dict(t=65, b=65, l=65, r=65) 
        )

        # Show the plot
        fig.show()
```

---The following area is a Code cell (cell numver is 25)---
```python
md = ModelDevelopment()
```

---The following area is a Code cell (cell numver is 26)---
```python
# Define label columns
label_cols = ["winner_model_a", "winner_model_b", "winner_tie"]

# Define the list of features to exclude from the training data
excluded_features = ['id', 
                     'model_a', 
                     'model_b', 
                     'prompt', 
                     'response_a', 
                     'response_b',
                     'winner_model_a', 
                     'winner_model_b', 
                     'winner_tie', 
                     'target', 
                     'fold_id']

features = [col for col in train_data.columns if col not in excluded_features]
```

---The following area is a Code cell (cell numver is 27)---
```python
# Train LightGBM
train_preds, test_preds = md.train_lgb(train_data, 
                                       test_data, 
                                       features,
                                       CFG.params, 
                                       CFG.early_stop, 
                                       CFG.log_steps)
```

---The following area is a Code cell (cell numver is 28)---
```python
# Confusion matrix for (mean) predictions on train data
md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# Submit Predictions ðŸ’¡
```

---The following area is a Code cell (cell numver is 30)---
```python
# Assign the predicted test labels to the submission dataframe
subm_data[label_cols] = test_preds

# Save the submission dataframe and display the first 3 rows
subm_data.to_csv("submission.csv", index=False)
display(subm_data.head(3))
```

** @@@ Jupyter Notebook numver 51, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from scipy.sparse import hstack, csr_matrix

# Load data
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# Initialize vectorizers and label encoders
n_features = 2**10
vectorizers = {
    'prompt': HashingVectorizer(n_features=n_features),
    'response_a': HashingVectorizer(n_features=n_features),
    'response_b': HashingVectorizer(n_features=n_features)
}
model_encoder = LabelEncoder()

# Encode model identifiers
train['model_a_encoded'] = model_encoder.fit_transform(train['model_a'])
train['model_b_encoded'] = model_encoder.transform(train['model_b'])

# Process text data into vectors
def process_and_concat_features(data, vectorizers):
    features_list = []
    for column, vectorizer in vectorizers.items():
        print(f"Vectorizing '{column}'...")
        transformed_data = vectorizer.transform(data[column])
        features_list.append(transformed_data)
    final_features = hstack(features_list)  # Keep as sparse matrix
    return final_features

train_features = process_and_concat_features(train, vectorizers)

# Combine model identifiers with text features
model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']])
X_combined = hstack([model_features, train_features])

# Encode target variable
train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(train['winner'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)

# Convert the data to DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set up parameters
params = {
    'objective': 'multi:softprob',
    'num_class': len(np.unique(y_encoded)),
    'eval_metric': 'mlogloss',
    'tree_method':'hist',
    'device':'cuda'
}

# Train the model
num_boost_round = 100
bst = xgb.train(params, dtrain, num_boost_round)

# Predict the probabilities
pred_probs = bst.predict(dtest)
predictions = np.argmax(pred_probs, axis=1)

# Evaluate the model
from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(y_test, predictions)
print(f"Test Accuracy: {test_accuracy:.2f}")

```

---The following area is a Code cell (cell numver is 2)---
```python
# Load data
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test_features = process_and_concat_features(test, vectorizers)

dtest = xgb.DMatrix(test_features)

# Predict the probabilities
pred_probs = bst.predict(dtest)

# Create DataFrame with the prediction probabilities for each class
df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_)
df_submission.insert(0, 'id', test['id'])
df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']

# Save predictions to CSV
df_submission.to_csv('submission.csv', index=False)
print(df_submission.head())

```

** @@@ Jupyter Notebook numver 52, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "tensorflow"  # or "jax" or "torch"
import re

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Num GPUs Available
```

---The following area is a Code cell (cell numver is 3)---
```python
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# TPU
```

---The following area is a Code cell (cell numver is 5)---
```python
# # Detect hardware, return appropriate distribution strategy
# try:
#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.
#     print('Running on TPU ', tpu.master())
# except ValueError:
#     tpu = None

# if tpu:
#     tf.config.experimental_connect_to_cluster(tpu)
#     tf.tpu.experimental.initialize_tpu_system(tpu)
#     strategy = tf.distribute.experimental.TPUStrategy(tpu)
# else:
#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.

# print("REPLICAS: ", strategy.num_replicas_in_sync)

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Configuration
```

---The following area is a Code cell (cell numver is 7)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en"
    sequence_length = 512
    epochs = 6
    batch_size = 16
#     batch_size = 16 * strategy.num_replicas_in_sync
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Reproducibility 
è®¾ç½®éšæœºç§å­çš„å€¼ä»¥åœ¨æ¯æ¬¡è¿è¡Œä¸­äº§ç”Ÿç±»ä¼¼çš„ç»“æžœã€‚
```

---The following area is a Code cell (cell numver is 9)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
#  Mixed Precision

åœ¨æœ¬ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ··åˆç²¾åº¦è€Œä¸æ˜¯ float32 ç²¾åº¦è¿›è¡Œè®­ç»ƒå’ŒæŽ¨ç†ï¼Œä»¥å‡å°‘ GPU å†…å­˜ä½¿ç”¨é‡ã€‚è¿™æœ€ç»ˆå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œä»Žè€Œå‡å°‘æˆ‘ä»¬çš„è®­ç»ƒå’ŒæŽ¨ç†æ—¶é—´ã€‚
```

---The following area is a Code cell (cell numver is 11)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
#åœ¨mixed_float16ç­–ç•¥ä¸‹ï¼Œæ¨¡åž‹çš„æŸäº›éƒ¨åˆ†ä¼šè‡ªåŠ¨ä½¿ç”¨float16è¿›è¡Œè®¡ç®—ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ï¼ˆå¦‚æŸå¤±å‡½æ•°çš„è®¡ç®—ï¼‰åˆ™å¯èƒ½ä»ç„¶ä½¿ç”¨float32ä»¥ä¿æŒç¨³å®šæ€§ã€‚
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Dataset Path
```

---The following area is a Code cell (cell numver is 13)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Meta Data 
## Files

### `train.csv`
- `id`: Unique identifier for each row.
- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).

### `test.csv`
- `id`: Unique identifier for each row.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 
ultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')
df = pd.concat([df, ultrachat_df], axis=0)
lmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')
df = pd.concat([df, lmsys_33k_deduplicated], axis=0)
# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')
# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'["{x}"]')
# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)

# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# display(ultrafeedback_lmsysformat.head())
display(df.head())
```

---The following area is a Code cell (cell numver is 16)---
```python
df = df.drop("id", axis=1)
df = df.drop_duplicates(keep="first", ignore_index=True)

for col in ["prompt"]:
    df[col] = df[col].apply(lambda x: eval(x))
    test_df[col] = test_df[col].apply(lambda x: eval(x))
for col in ["response_a", "response_b"]:
    df[col] = df[col].apply(lambda x: eval(x.replace("null", "None")))
    test_df[col] = test_df[col].apply(lambda x: eval(x.replace("null", "None")))
    
# Sample data
# df = df.sample(frac=0.01)

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
display(df.head())
# Show Sample
display(test_df.head())
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## Contextualize Response with Prompt

åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®æç¤ºå¯¹æ¯ä¸ªå›žç­”è¿›è¡Œæƒ…å¢ƒåŒ–ï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰å›žç­”ä½¿ç”¨å•ä¸€æç¤ºã€‚è¿™æ„å‘³ç€ï¼Œå¯¹äºŽæ¯ä¸ªå›žç­”ï¼Œæˆ‘ä»¬å°†ä¸ºæ¨¡åž‹æä¾›åŒä¸€ç»„æç¤ºåŠå…¶å„è‡ªçš„å›žç­”ï¼ˆä¾‹å¦‚ï¼Œâ€œ(P + R_A)â€ï¼Œâ€œ(P + R_B)â€ç­‰ï¼‰ã€‚

> æŸäº›æç¤ºå’Œå“åº”å¯èƒ½æœªä½¿ç”¨ `utf-8` ç¼–ç ï¼Œå¯¼è‡´åˆ›å»ºæ•°æ®åŠ è½½å™¨æ—¶å‡ºé”™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”¨ç©ºå­—ç¬¦ä¸²æ›¿æ¢å®ƒä»¬ã€‚
```

---The following area is a Code cell (cell numver is 18)---
```python
def make_pairs(row):
    row['options'] = []
    row["encode_fail"] = False

    try:
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„é”®éƒ½å­˜åœ¨äºŽrowå­—å…¸ä¸­
        prompts = row['prompt']
        responses_a = row['response_a']
        responses_b = row['response_b']
        
        # æ£€æŸ¥åˆ—è¡¨é•¿åº¦æ˜¯å¦åŒ¹é…
        if not (len(prompts) == len(responses_a) == len(responses_b)):
            raise ValueError("The lists 'prompt', 'response_a', and 'response_b' must be of the same length.")
            
        response_a_str = ''
        response_b_str = ''
        
        for idx in range(len(prompts)):
            response_a_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_a[idx]}"
            response_b_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_b[idx]}"
        
        # æ–‡æœ¬æ¸…æ´—ï¼Œä¾‹å¦‚åŽ»é™¤æ— æ³•è¯†åˆ«çš„Unicodeå­—ç¬¦æˆ–æ›¿æ¢å®ƒä»¬
        clean_response_a_str = "".join(filter(lambda x: ord(x) < 128, response_a_str))
        clean_response_b_str = "".join(filter(lambda x: ord(x) < 128, response_b_str))
        
        row['options'].append(clean_response_a_str)
        row['options'].append(clean_response_b_str)
        
    except KeyError as e:
        print(f"Missing key in row: {e}")
        row["encode_fail"] = True
    except ValueError as e:
        print(e)
        row["encode_fail"] = True
    except Exception as e:
        # æ•èŽ·å…¶ä»–æ‰€æœ‰å¼‚å¸¸
        print(f"An unexpected error occurred: {e}")
        row["encode_fail"] = True

    return row
```

---The following area is a Code cell (cell numver is 19)---
```python
df = df.apply(make_pairs, axis=1)
display(df.head(2))

test_df = test_df.apply(make_pairs, axis=1)
display(test_df.head(2))
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Encoding Fail Statistics
```

---The following area is a Code cell (cell numver is 21)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# EDA
```

---The following area is a Code cell (cell numver is 23)---
```python
class DataFrameStatsProcessor:
    def __init__(self, df):
        self.df = df

    def _is_empty(self, string: str) -> bool:
        return bool(re.match("^\s*$", string))

    def _len(self, string: str) -> int:
        if string is None:
            return 0
        return len(string)

    def _add_len_stats(self, col: str) -> pd.DataFrame:
        if col == "prompt":
            col_prefix = "p_len"
        elif col == "response_a":
            col_prefix = "res_a_len"
        elif col == "response_b":
            col_prefix = "res_b_len"
        
        self.df[f"{col_prefix}_sum"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))
        self.df[f"{col_prefix}_mean"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))
        self.df[f"{col_prefix}_max"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))
        self.df[f"{col_prefix}_sum_log"] = np.log1p(self.df[f"{col_prefix}_sum"])
        self.df[f"{col_prefix}_mean_log"] =  np.log1p(self.df[f"{col_prefix}_mean"])
        self.df[f"{col_prefix}_max_log"] = np.log1p(self.df[f"{col_prefix}_max"])
        
        return self.df
    
    def z_score_normalize(self, columns):
        """
        å¯¹æŒ‡å®šçš„åˆ—è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–ã€‚
        å‚æ•°:
            columns (list): éœ€è¦è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–çš„åˆ—ååˆ—è¡¨ã€‚
        """
        for col in columns:
            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()
    
    def process_dataframe(self):
        self.df["n_prompts"] = self.df["prompt"].apply(lambda x: len(x))
        self.df["n_res_a"] = self.df["response_a"].apply(lambda x: len(x))
        self.df["n_res_b"] = self.df["response_b"].apply(lambda x: len(x))
        assert ((self.df["n_prompts"] == self.df["n_res_a"]) & (self.df["n_prompts"] == self.df["n_res_b"])).all()

        self.df["n_na_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))

        self.df["n_miss_res_a"] = self.df["n_na_res_a"] + self.df["n_empty_res_a"]
        self.df["n_miss_res_b"] = self.df["n_na_res_b"] + self.df["n_empty_res_b"]

        self.df["n_eff_res_a"] = self.df["n_res_a"] - self.df["n_miss_res_a"]
        self.df["n_eff_res_b"] = self.df["n_res_b"] - self.df["n_miss_res_b"]

        self._add_len_stats("prompt")
        self._add_len_stats("response_a")
        self._add_len_stats("response_b")

        self.df["res_len_mean_diff"] = self.df["res_a_len_mean"] - self.df["res_b_len_mean"]
        self.df["res_len_mean_diff_clip"] = self.df["res_len_mean_diff"].clip(-6000, 6000)

        self.df["n_miss_prompts"] = self.df["n_na_prompts"] + self.df["n_empty_prompts"]
        self.df["n_eff_prompts"] = self.df["n_prompts"] - self.df["n_miss_prompts"]

        self.df["na_prompt_ratio"] = self.df["n_na_prompts"] / self.df["n_prompts"]
        self.df["empty_prompt_ratio"] = self.df["n_empty_prompts"] / self.df["n_prompts"]
        self.df["miss_prompt_ratio"] = self.df["n_miss_prompts"] / self.df["n_prompts"]

        self.df["na_res_a_ratio"] = self.df["n_na_res_a"] / self.df["n_res_a"]
        self.df["empty_res_a_ratio"] = self.df["n_empty_res_a"] / self.df["n_res_a"]
        self.df["miss_res_a_ratio"] = self.df["n_miss_res_a"] / self.df["n_res_a"]
        self.df["na_res_b_ratio"] = self.df["n_na_res_b"] / self.df["n_res_b"]
        self.df["empty_res_b_ratio"] = self.df["n_empty_res_b"] / self.df["n_res_b"]
        self.df["miss_res_b_ratio"] = self.df["n_miss_res_b"] / self.df["n_res_b"]

        for col, col_prefix in zip(["prompt", "response_a", "response_b"], ["p_len", "res_a_len", "res_b_len"]):
            self.df[f"{col_prefix}_med"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))
            self.df[f"{col_prefix}_std"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))

        self.df["p_len_eff_mean"] = self.df["p_len_sum"] / self.df["n_eff_prompts"]
        self.df["res_a_len_eff_mean"] = self.df["res_a_len_sum"] / self.df["n_eff_res_a"]
        self.df["res_b_len_eff_mean"] = self.df["res_b_len_sum"] / self.df["n_eff_res_b"]

        for stats in ["sum", "mean", "max", "med", "eff_mean"]:
            self.df[f"p_a_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_a_len_{stats}"]
            self.df[f"p_b_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            self.df[f"a_b_{stats}_diff"] = self.df[f"res_a_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            
        len_feature_a_col = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                     "p_a_eff_mean_diff"]
        
        len_feature_b_col = ["res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                             "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                             "p_b_eff_mean_diff"]
        
        numerical_feature_columns = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                                     "p_a_eff_mean_diff", "res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                                     "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                                     "p_b_eff_mean_diff"]
        # ç¡®ä¿ä¸é™¤ä»¥é›¶è¿›è¡Œå½’ä¸€åŒ–
        for col in numerical_feature_columns:
            if self.df[col].std() == 0:
                print(f"Warning: Standard deviation is zero for column {col}. Skipping normalization.")
            else:
                self.z_score_normalize([col])
                
        self.df = self.df.fillna(0)
        
        # é€‰æ‹©è¿™äº›åˆ—å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºåˆ—è¡¨
        len_features_a = self.df[len_feature_a_col].values.tolist()
        len_features_b = self.df[len_feature_b_col].values.tolist()

        return len_features_a, len_features_b
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# Data Split

åœ¨ä¸‹é¢æä¾›çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨class_labelåˆ—çš„åˆ†å±‚å°†çŽ°æœ‰æ•°æ®åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯ã€‚
```

---The following area is a Code cell (cell numver is 25)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# Preprocessing
```

---The following area is a Code cell (cell numver is 27)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, 
    sequence_length=CFG.sequence_length, 
)
```

---The following area is a Code cell (cell numver is 28)---
```python
def preprocess_fn(text, label=None, features_a=None, features_b=None):
    text = preprocessor(text)
    if features_a is not None:
        text['features_a'] = features_a
    if features_b is not None:
         text['features_b'] = features_b
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# FGM
```

---The following area is a Code cell (cell numver is 30)---
```python
# # æ·»åŠ  FGM æ‰°åŠ¨å‡½æ•°
# def fgm_perturb(features, epsilon=1.0):
#     # è®¡ç®—æ‰°åŠ¨é‡ï¼Œepsilon ä¸ºæ‰°åŠ¨æ¯”ä¾‹
#     perturbation = np.random.uniform(-1, 1, features.shape) * epsilon
#     # åº”ç”¨æ‰°åŠ¨
#     return features + perturbation
```

---The following area is a Code cell (cell numver is 31)---
```python
# # ä¿®æ”¹æ•°æ®é¢„å¤„ç†å‡½æ•°ä»¥åŒ…å« FGM æ‰°åŠ¨
# def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):
#     # é¢„å¤„ç†æ–‡æœ¬
#     text = preprocessor(text)
#     if features_a is not None:
#         if is_fgm:
#             # å¦‚æžœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨
#             features_a = fgm_perturb(features_a, epsilon)
#         text['features_a'] = features_a
#     if features_b is not None:
#         if is_fgm:
#             # å¦‚æžœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨
#             features_b = fgm_perturb(features_b, epsilon)
#         text['features_b'] = features_b
#     return (text, label) if label is not None else text
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
# AWP
```

---The following area is a Code cell (cell numver is 33)---
```python
#å®šä¹‰ AWP æ‰°åŠ¨å‡½æ•°
def awp_perturb(model, epsilon=1e-4):
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            # èŽ·å–æƒé‡
            weights = layer.kernel
            # è®¡ç®—æ‰°åŠ¨
            perturbation = tf.random.normal(weights.shape, stddev=epsilon)
            # åº”ç”¨æ‰°åŠ¨
            layer.kernel.assign_add(perturbation)

#åˆ›å»º AWP å›žè°ƒå‡½æ•°
class AWPCallback(keras.callbacks.Callback):
    def __init__(self, epsilon):
        super(AWPCallback, self).__init__()
        self.epsilon = epsilon

    def on_batch_begin(self, batch, logs=None):
        # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¼€å§‹æ—¶åº”ç”¨ AWP æ‰°åŠ¨
        awp_perturb(self.model, self.epsilon)
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# DataLoader

ä¸‹é¢çš„ä»£ç ä½¿ç”¨tf.data.Datasetä¸ºæ•°æ®å¤„ç†è®¾ç½®äº†ä¸€ä¸ªå¥å£®çš„æ•°æ®æµç®¡é“ã€‚
```

---The following area is a Code cell (cell numver is 35)---
```python
def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,
                                cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE
    if (features_a is not None) and (features_b is not None):
        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # Create slices
    else:
        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)
    ds = ds.cache() if cache else ds
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)
#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),
#                 num_parallel_calls=tf.data.AUTOTUNE)
    opt = tf.data.Options()
    if shuffle:
        ds = ds.shuffle(shuffle, seed=CFG.seed)
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)
    ds = ds.batch(batch_size, drop_remainder=False)
    ds = ds.prefetch(AUTO)
    
    return ds
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
## Build Train/Valid Dataloader
```

---The following area is a Code cell (cell numver is 37)---
```python
train_features_processor = DataFrameStatsProcessor(train_df.copy())
train_features_a, train_features_b = train_features_processor.process_dataframe()
valid_features_processor = DataFrameStatsProcessor(valid_df.copy())
valid_features_a, valid_features_b = valid_features_processor.process_dataframe()
```

---The following area is a Code cell (cell numver is 38)---
```python
# # Train
train_texts = train_df.options.tolist()  
train_labels = train_df.class_label.tolist() 
train_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, 
                         batch_size=CFG.batch_size,
                         shuffle=True)
# # Valid
valid_texts = valid_df.options.tolist()  
valid_labels = valid_df.class_label.tolist() 
valid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, 
                         batch_size=CFG.batch_size,
                         shuffle=False)
print(train_ds)
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
# LR Schedule

å®žæ–½å­¦ä¹ çŽ‡è°ƒåº¦ç¨‹åºå¯¹äºŽè¿ç§»å­¦ä¹ è‡³å…³é‡è¦ã€‚

å­¦ä¹ çŽ‡ä»Ž lr_start å¼€å§‹ï¼Œç„¶åŽä½¿ç”¨å„ç§æŠ€æœ¯é€æ¸å‡å°åˆ° lr_minï¼ŒåŒ…æ‹¬ï¼š

- stepï¼šä»¥ç±»ä¼¼æ¥¼æ¢¯çš„æ–¹å¼é€æ­¥é™ä½Žå­¦ä¹ çŽ‡ã€‚
- cosï¼šåˆ©ç”¨ä½™å¼¦æ›²çº¿é€æ¸é™ä½Žå­¦ä¹ çŽ‡ã€‚
- expï¼šä»¥æŒ‡æ•°æ–¹å¼é™ä½Žå­¦ä¹ çŽ‡ã€‚

**é‡è¦æ€§**ï¼šç»“æž„è‰¯å¥½çš„å­¦ä¹ çŽ‡è°ƒåº¦å¯¹äºŽæœ‰æ•ˆçš„æ¨¡åž‹è®­ç»ƒè‡³å…³é‡è¦ï¼Œå¯ç¡®ä¿æœ€ä½³æ”¶æ•›å¹¶é¿å…è¯¸å¦‚è¿‡å†²æˆ–åœæ»žç­‰é—®é¢˜ã€‚
```

---The following area is a Code cell (cell numver is 40)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr
    
    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 41)---
```python
lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs)
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
# Model Checkpointing

ä¸‹é¢çš„ä»£ç å°†åˆ›å»ºä¸€ä¸ªå›žè°ƒï¼Œåœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ¨¡åž‹çš„æœ€ä½³æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨æäº¤æ—¶ä½¿ç”¨å®ƒè¿›è¡ŒæŽ¨ç†ã€‚
```

---The following area is a Code cell (cell numver is 43)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')  # Get Model checkpoint callback
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# Metric

è¿™æ¬¡æ¯”èµ›çš„æŒ‡æ ‡æ˜¯å¯¹æ•°æŸå¤±ã€‚è¿™ä¸ªåº¦é‡å¯ä»¥ç”¨æ•°å­¦è¡¨ç¤ºä¸ºï¼š

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$
```

---The following area is a Code cell (cell numver is 45)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss", label_smoothing=0.1, from_logits=False)
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
# Modeling
```

---The following area is a Code cell (cell numver is 47)---
```python
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dropout

with strategy.scope():

    # å°†æ‰€æœ‰è¾“å…¥å±‚æ•´åˆåˆ°ä¸€ä¸ªå­—å…¸ä¸­
    inputs = {
        "token_ids": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
        "padding_mask": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
        "features_a": keras.layers.Input(shape=(14,), name="features_a", dtype=tf.float32),
        "features_b": keras.layers.Input(shape=(14,), name="features_b", dtype=tf.float32),
    }
    
    # Create a DebertaV3Classifier backbone
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        CFG.preset,
    )

   # ä¿®æ”¹ response_a å’Œ response_b çš„åˆ›å»ºæ–¹å¼ï¼ŒåŒ…å« padding_mask
    response_a = {
        "token_ids": inputs["token_ids"][:, 0, :],
        "padding_mask": inputs["padding_mask"][:, 0, :]
    }
    embed_a = backbone(response_a)

    response_b = {
        "token_ids": inputs["token_ids"][:, 1, :],
        "padding_mask": inputs["padding_mask"][:, 1, :]
    }
    embed_b = backbone(response_b)
    
    # å°†æ•°å€¼ç‰¹å¾åµŒå…¥
    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_a"])
    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_b"])
    
    # ä½¿ç”¨ Flatten å±‚å°†æ•°å€¼ç‰¹å¾åµŒå…¥å±•å¹³ä¸ºäºŒç»´å¼ é‡
    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)
    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)
    
    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)
    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)
    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])
    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])
    
    # åˆå¹¶æ–‡æœ¬åµŒå…¥å’Œæ•°å€¼ç‰¹å¾åµŒå…¥
    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_a])
    
    # æ·»åŠ L2æ­£åˆ™åŒ–å’ŒDropoutåˆ°æ¨¡åž‹ä¸­
    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2æ­£åˆ™åŒ–
    combined_embeds = Dropout(0.05)(combined_embeds)  # Dropoutå±‚ï¼Œä¸¢å¼ƒ5%çš„ç¥žç»å…ƒ
    
    # å®šä¹‰ temperature_scale å‡½æ•°
    def temperature_scale(logits, T=1.0):
        return logits / T
    
    # å®šä¹‰æ¸©åº¦å‚æ•° T
    T = 0.85
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    scaled_logits = temperature_scale(combined_embeds, T)
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(scaled_logits)
    
    model = keras.Model(inputs,  outputs)
    
    # Compile the model with optimizer, loss, and metrics
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),
        metrics=[
            log_loss,
            keras.metrics.CategoricalAccuracy(name="accuracy"),
        ],
    )
    
    # æ·»åŠ  AWP å›žè°ƒåˆ°æ¨¡åž‹è®­ç»ƒä¸­
    awp_cb = AWPCallback(epsilon=1e-4)  # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ epsilon çš„å€¼
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
### Model Summary
```

---The following area is a Code cell (cell numver is 49)---
```python
model.summary()
```

---The following area is a Markdown cell (cell numver is 50)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 51)---
```python
# try:
#     history = model.fit(
#         train_ds,
#         epochs=CFG.epochs,
#         validation_data=valid_ds,
#         callbacks=[lr_cb, ckpt_cb]
#     )
# except tf.errors.InvalidArgumentError as e:
#     print(f"å‡ºçŽ°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}")
try:
    history = model.fit(
        train_ds,
        epochs=CFG.epochs,
        validation_data=valid_ds,
        callbacks=[lr_cb, ckpt_cb, awp_cb]  # å°† AWP å›žè°ƒæ·»åŠ åˆ°è®­ç»ƒå›žè°ƒåˆ—è¡¨ä¸­
    )
except tf.errors.InvalidArgumentError as e:
    print(f"å‡ºçŽ°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}")
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
## Load Best Model
```

---The following area is a Code cell (cell numver is 53)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Markdown cell (cell numver is 54)---
```markdown
# Prediction
```

---The following area is a Code cell (cell numver is 55)---
```python
# # ä½¿ç”¨ FGM æ‰°åŠ¨çš„æ•°æ®é›†è¯„ä¼°æ¨¡åž‹
# fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,
#                                      is_fgm=True, epsilon=1.0)
# evaluation_results = model.evaluate(fgm_ds)

# print(f"Evaluation results on FGM perturbed dataset: {evaluation_results}")
```

---The following area is a Code cell (cell numver is 56)---
```python
test_df_features_processor = DataFrameStatsProcessor(test_df)
test_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()
```

---The following area is a Code cell (cell numver is 57)---
```python
test_texts = test_df.options.tolist()
test_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
print(test_ds)
```

---The following area is a Code cell (cell numver is 58)---
```python
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Markdown cell (cell numver is 59)---
```markdown
# Submission
```

---The following area is a Code cell (cell numver is 60)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

** @@@ Jupyter Notebook numver 53, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Configuration
```

---The following area is a Code cell (cell numver is 1)---
```python
class CFG:
    OFFLINE = True#False #False # requirement for submision competition, during development recommend online model support LLM Evaluation Tools
    USE_LLAMA3 = False # for GPU version
    USE_GEMMA2 = False # for GPU version only 
    TASK_GEN = False # for generative Text output task (seem not suitble for this competition)
    TASK_CLASSIFICATION = True  # for text classiction (suitable for this competition)
#     model1 = "/kaggle/input/llama-3/transformers/8b-hf/1"  # llama3 8B |
    model2 = "/kaggle/input/gemma/transformers/2b-it/3" #  gemma 2B
    model3 = "/kaggle/input/gemma/transformers/7b-it/3" # gemma 7B
#     model4 = "/kaggle/input/gemma-2/pytorch/gemma-2-9b-it/1" # gemma 2 9B
    trainFile = "/kaggle/input/lmsys-chatbot-arena/train.csv"
    testFile = "/kaggle/input/lmsys-chatbot-arena/test.csv"
    submitSample = "/kaggle/input/lmsys-chatbot-arena/sample_submission.csv"
    FEW_SHOT_TEST= False#True
    USE_RAG = False#False#False #True#True , in this project, prefer use fine tuning for p
    USE_WANDB = False#True # for  LLM evalution and debug , track fine tuning performance
    USE_TRULENS = False # for LLM evalution For RAG prefer 
    USE_DEEPEVAL = False # for LLM evalution   (require openAI API key)
    USE_TRAIN =  True #True #False#True Much be use GPU for Training
    USE_INFER =  False # for submision prediction only , no test model
    loggingSteps= 10#100 #100, #20, #5,#10,
    maxTrainData = 1500#3500#5000 #10000#5000 #10000
    maxEvalData = 20#100 # 20 
    maxToken=  650 #512#768#512#768 # 512 for test only
    
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Install Library
```

---The following area is a Code cell (cell numver is 3)---
```python
# installDir = #"/kaggle/input/ai-math-llm-install-package/Universal-LLM-install-page/Universal-LLM-install-page"
installDir = "/kaggle/input/universal-llm-install-package2/Universal-LLM-install-page"
# install Libary for offline
if CFG.OFFLINE:
    !pip install transformers --no-index --no-deps --find-links=file://{installDir}/tranforemers
    !pip install -U datasets     --no-index --no-deps --find-links=file://{installDir}/datasets
    !pip install -U accelerate   --no-index --no-deps --find-links=file://{installDir}/accelerate
    !pip install build        --no-index  --no-deps --find-links=file://{installDir}/build-1.2.1-py3-none-any.whl
    !pip install -U bitsandbytes --no-index --no-deps --find-links=file://{installDir}/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl
    !pip install langchain --no-index --no-deps --find-links=file://{installDir}/langchain-0.2.6-py3-none-any.whl
    !pip install langchain-core  --no-index --no-deps --find-links=file://{installDir}/langchain_core-0.2.10-py3-none-any.whl
    !pip install langsmith   --no-index --no-deps --find-links=file://{installDir}/langsmith-0.1.82-py3-none-any.whl
    !pip install langchain-community --no-index --no-deps --find-links=file://{installDir}/langchain_community-0.2.5-py3-none-any.whl
    !pip install sentence-transformers --no-index --no-deps --find-links=file://{installDir}/sentence_transformers-3.0.1-py3-none-any.whl
    !pip install chromadb --no-index --no-deps --find-links=file://{installDir}/chromadb-0.5.3-py3-none-any.whl
    !pip install faiss-cpu --no-index --no-deps --find-links=file://{installDir}/faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
    !pip install -U huggingface_hub --no-index --no-deps --find-links=file://{installDir}/huggingface_hub
    !pip install -qU langchain-text-splitters --no-index --no-deps --find-links=file://{installDir}/langchain_text_splitters-0.2.2-py3-none-any.whl
    !pip install -U peft  --no-index --no-deps --find-links=file://{installDir}/peft-0.11.1-py3-none-any.whl
    !pip install -U trl  --no-index --no-deps --find-links=file://{installDir}/trl-0.9.4-py3-none-any.whl 
    !pip install umap-learn  --no-index --no-deps --find-links=file://{installDir}/umap_learn
    !pip install evaluate  --no-index --no-deps --find-links=file://{installDir}/evaluate-0.4.2-py3-none-any.whl 
    !pip install deepeval  --no-index --no-deps --find-links=file://{installDir}/deepeval-0.21.62-py3-none-any.whl
    !pip install weave  --no-index --no-deps --find-links=file://{installDir}/weave-0.50.2-py3-none-any.whl
    !pip install openai --no-index --no-deps --find-links=file://{installDir}/openai-1.35.7-py3-none-any.whl
    !pip install langchain_openai --no-index --no-deps --find-links=file://{installDir}/langchain_openai-0.1.13-py3-none-any.whl
    !pip install trulens --no-index --no-deps --find-links=file://{installDir}/trulens-0.13.4-py3-none-any.whl
    !pip install trulens-eval --no-index --no-deps --find-links=file://{installDir}/trulens_eval-0.32.0-py3-none-any.whl
    
else: # install libary for online
    !pip install git+https://github.com/huggingface/transformers.git  # install transformer from source
    # !pip install --upgrade torch datasets accelerate peft bitsandbytes trl
    # !pip install --upgrade accelerate peft bitsandbytes trl
    !pip install --upgrade datasets accelerate bitsandbytes  # add datasets, accelerate , bitsndbytes
    !pip install langchain  langchain-community sentence-transformers chromadb  faiss-cpu #pypdf
    !pip install --upgrade huggingface_hub
    !pip install -qU langchain-text-splitters
    # for loRA fine tuning 
    !pip install --upgrade peft trl
    # for advance RAG & LLM evalution 
    !pip install portalocker openai langchain_openai # for deepeval dependance library (require openai api key)
    !pip install --upgrade umap-learn evaluate deepeval weave trulens trulens-eval
    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Import Library
```

---The following area is a Code cell (cell numver is 5)---
```python
import os, json, time
import gc
from IPython.display import display, Markdown
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import transformers
import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments
from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding
from langchain_community.document_loaders import TextLoader # new version
from langchain.prompts.prompt import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_community.vectorstores import FAISS, Chroma




# Text chunk spliter
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter , SentenceTransformersTokenTextSplitter# Text Splitter
from langchain.embeddings import HuggingFaceEmbeddings
from datasets import Dataset, DatasetDict, load_dataset

# for evaluate  LLM 
import evaluate
import pytest
import trulens
from sklearn.metrics import (classification_report, ConfusionMatrixDisplay  , 
                             f1_score , accuracy_score, precision_score, recall_score)

import warnings
# warnings.filterwarnings("error") # for handle warning as error
# warnings.filterwarnings("ignore", category=DeprecationWarning) 
```

---The following area is a Code cell (cell numver is 6)---
```python
device =  torch.device('cuda'  if  torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 7)---
```python
def clearMemory():
    for _ in range(5):
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(0.3)
```

---The following area is a Code cell (cell numver is 8)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 9)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
if False:
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# W&B inital For Online Only (LLM evalution Tool)
```

---The following area is a Code cell (cell numver is 11)---
```python
if CFG.USE_WANDB:
    import wandb
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    my_secret = user_secrets.get_secret("wandb_api_key")
    wandb.login(key=my_secret) # login
    reportTo= "wandb" # use for fine tuning training logging 
else:
    reportTo = "none"# None
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Load Dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
trainDF =  pd.read_csv(CFG.trainFile)
trainDF
```

---The following area is a Code cell (cell numver is 14)---
```python
trainDF[trainDF["winner_tie"] ==1] # 
```

---The following area is a Code cell (cell numver is 15)---
```python
nullTranDF=trainDF[trainDF.response_a == 'null']
```

---The following area is a Code cell (cell numver is 16)---
```python
len(nullTranDF)
```

---The following area is a Code cell (cell numver is 17)---
```python
trainDF.isnull().sum()
```

---The following area is a Code cell (cell numver is 18)---
```python
testDF =  pd.read_csv(CFG.testFile)
testDF.head()
```

---The following area is a Code cell (cell numver is 19)---
```python
submitDF = pd.read_csv(CFG.submitSample)
submitDF.head()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# EDA
```

---The following area is a Code cell (cell numver is 21)---
```python
def printUniqueValue(df, showAll= True):
    for col in df.columns:
        if showAll ==True:
            print(f"""{col} :  {df[col].unique()}""")
        else:
            if df[col].dtype == "object": # only show object type columns unique values
                print(f"{col} : {df[col].unique()}")
        
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
## print  unqiue Value
```

---The following area is a Code cell (cell numver is 23)---
```python
printUniqueValue(trainDF, showAll=True)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## Model distribution
```

---The following area is a Code cell (cell numver is 25)---
```python
trainDF.columns
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# Load LLM Model
```

---The following area is a Code cell (cell numver is 27)---
```python
if CFG.USE_TRAIN == True:
    # for LoRA fine tuning
    from trl import SFTTrainer
    from peft import LoraConfig, PeftModel, get_peft_model , prepare_model_for_kbit_training #prepare_model_for_int8_training deprecated 

```

---The following area is a Code cell (cell numver is 28)---
```python
do_sample= True 
top_p=0.95 
top_k= 2
temperature=0.2#0.7 
num_beams = 3
max_length= 512

# Quantized Config for GPU support only
bnb_config = BitsAndBytesConfig(
        load_in_4bit = True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype= torch.bfloat16,
        bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)

)

```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# define Label for Text classification
```

---The following area is a Code cell (cell numver is 30)---
```python
id2label = {0: "winner_model_a", 1: "winner_model_b", 2: "winner_tie"}
label2id = {"winner_model_a": 0, "winner_model_b": 1, "winner_tie": 2}
```

---The following area is a Code cell (cell numver is 31)---
```python
if device.type == "cuda": # use 7b/8b/9b model gain performance
    if CFG.USE_LLAMA3:
        modelSel = CFG.model1
        llmModel = "llama3_8b"
        
    elif CFG.USE_GEMMA2:
        modelSel = CFG.model4
        llmModel = "gemma2_9b"
    
    else:
        modelSel = CFG.model3
        llmModel = "gemma_7b"
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config= bnb_config)
#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels =3 ,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type= "multi_label_classification", #problem unmatch issues
                                                 quantization_config= bnb_config) #remove quantization
#          model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  num_labels =3)
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config= bnb_config)
#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
    
    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer
    tokenizer.add_eos_token = True  # We'll add <eos> at the end
    tokenizer.padding_side = "right"
    

else: # for cpu select smaller model
    modelSel = CFG.model2
    llmModel = 'gemma_2b'
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
        
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels =3,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type= "multi_label_classification",
                                                                  )
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")

    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer
    tokenizer.add_eos_token = True  # We'll add <eos> at the end
    tokenizer.padding_side = "right"
    
        
        
```

---The following area is a Code cell (cell numver is 32)---
```python
model
```

---The following area is a Code cell (cell numver is 33)---
```python
llmModel
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# Prompt Engineering
```

---The following area is a Code cell (cell numver is 35)---
```python
if CFG.TASK_GEN:
    templatePrompt1 = """Question: {question}.\nOnly require given final result in JSON format with key 'answer'
            """
    templatePrompt2 = "Answer the user Question.\n###\n{format_instructions}\n###\nQuestion: {query}\n"
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
### Generate Response from LLM
```

---The following area is a Code cell (cell numver is 37)---
```python
if CFG.TASK_GEN:
    def generateResponse(query, maxNewToken =256):
        """
        Direct send message to LLM model, get resposne
        """
    
        inputIds = tokenizer(query, return_tensors="pt").to(device)
        response = model.generate(**inputIds,
                              do_sample= True,
                              top_p =0.95,
                              top_k= 3,
                              temperature= 0.5,
#                               max_lenght = 1024, 
                              max_new_tokens= maxNewToken,
                             )
    
        return tokenizer.decode(response[0][len(inputIds["input_ids"]):], skip_special_tokens = True)
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# Simple parser to extract data
```

---The following area is a Code cell (cell numver is 39)---
```python
import re
from  json.decoder import JSONDecodeError
if CFG.TASK_GEN:

    def isInteger(text):
        try:
            if int(text) >= 0:
                return True
            else:
                return False
        except ValueError:
            return False

    def llmJSONparser(txt, key="answer:", integerOut= False):
        """
        try to get answer from LLM response , expect in JSON format, 
        """
        try:
            subText = txt.split("{") # split several {} in list 
            for txtSeg in subText: # loop in list to find answer
                end = txtSeg.find("}") # find end position in text segment
                sub = txtSeg[:end] #subsring with {} context
                temp = sub.replace("*", "") # remove * symbol
                temp = temp.replace("\"", "") # reomve \" symbol
                temp = temp.lower() # convert to lower case
                answerloc = temp.find(key) # find key word "answer" position
                if answerloc != -1:
                    print(f"find answer location : {answerloc}")
                    newTxt = temp[answerloc:] # substring start answer
#                   print("Temp: ", temp)
                    subTxt = newTxt.split("\n")
                    #       print(subTxt)
                    rel =subTxt[0][len(key):].strip() # get answer value with remove space
                    rel= rel.replace(',', '') # remove , symbol
                    print(rel)
                    if integerOut: # expect integer output 
                        if isInteger(rel):
                            return rel
                        else:
                            continue # not find the value
                    else:
                        return rel
                
            return None # can't find answer
        except :
            print(f"""Error LLM JSON parser input txt {txt}""" )
            return None
        return None


    def getLLMAnswerParser(txt, key="answer:"):
        """
        when json parser failure, seem answer not JSON format, 
        use "answer" for key word search final answer 
        """
         # find answer  
        temp = txt.replace("*", "") # remove * symbol
        temp = temp.replace("\"", "") # reomve "" symbol
        temp = temp.lower() # convert to lower case
        # find answer key word
        start = temp.find(key)
        print(f"Start loc: {start}")
        subStr = temp[start:]
        if start != -1:
            subTxt = subStr.split("\n")
           #print(subTxt)
            rel =subTxt[0][len(key):].strip() # get answer value with remove space
            rel= rel.replace(',', '') # remove , symbol
            print(rel)
            return rel
    
        print(subStr)
        return None
    

```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# Add parser to control extract data from LLM Structure Output
```

---The following area is a Code cell (cell numver is 41)---
```python
from langchain_core.output_parsers import (StrOutputParser, 
                                           JsonOutputParser,
                                           PydanticOutputParser,
                                          )
# for LLM structure output
from langchain_core.pydantic_v1 import BaseModel, Field, validator
# from pydantic import BaseModel, Field
```

---The following area is a Code cell (cell numver is 42)---
```python
if CFG.TASK_GEN:
# define data strauctrue for LLM output structure 
    class Answer(BaseModel):
        answer: str = Field(description="the answer of question response from LLM")
        explanation: str = Field(description="explain how answer come from and reasoning")
    
    
```

---The following area is a Code cell (cell numver is 43)---
```python
# test Parser
if CFG.TASK_GEN:
    jsonParser = JsonOutputParser(pydantic_object=Answer)  # json parser
    pydanticParser =  PydanticOutputParser(pydantic_object=Answer) # pydantic base parse
             
```

---The following area is a Code cell (cell numver is 44)---
```python
if CFG.TASK_GEN:
    print(jsonParser.get_format_instructions())
```

---The following area is a Code cell (cell numver is 45)---
```python
if CFG.TASK_GEN:
    print(pydanticParser.get_format_instructions())
```

---The following area is a Code cell (cell numver is 46)---
```python
%%time
if CFG.TASK_GEN:
    ret = generateResponse("What is Machine Learning?", maxNewToken=256) # test Model 
```

---The following area is a Code cell (cell numver is 47)---
```python
if CFG.TASK_GEN:
    print(ret) # seem LLM default output in Markdown format
```

---The following area is a Code cell (cell numver is 48)---
```python
if CFG.TASK_GEN:
    display(Markdown(ret))
```

---The following area is a Code cell (cell numver is 49)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 50)---
```python
%%time 
if CFG.TASK_GEN:
    # test prompt template with structure format and test parser
    query = "What is Machine Learning?"
    newPrompt =PromptTemplate(input_variables=["question"], template=templatePrompt1)
    finalPrompt = newPrompt.format(
        question= query
    )
    rel = generateResponse(finalPrompt,  maxNewToken=1024)
    jsonTxt = llmJSONparser(rel, key="answer:", integerOut= False)
    print(f"Question : {query}\nResponse Answer: {jsonTxt}") # convert output structure format
```

---The following area is a Code cell (cell numver is 51)---
```python
if CFG.TASK_GEN:
    print(rel)
```

---The following area is a Code cell (cell numver is 52)---
```python
if CFG.TASK_GEN:
    templatePrompt2
```

---The following area is a Code cell (cell numver is 53)---
```python
%%time 
if CFG.TASK_GEN:
    # test Structure output control by PydanticOutputParser
    query = "What is Machine Learning?"
    newPrompt = PromptTemplate(template=templatePrompt2,
                          input_variables=["query"],
                          partial_variables={"format_instructions": pydanticParser.get_format_instructions()},  
                          )
    finalPrompt = newPrompt.format(
            query=query,
        )
    print(f"Final Prompt: {finalPrompt}")
    print("Response:\n")
    rel = generateResponse(finalPrompt,  maxNewToken=2048)
    print(rel)
```

---The following area is a Code cell (cell numver is 54)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 55)---
```python
# pydanticParser.parse(rel)
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
# Prepare Dataset for Fine Tuning
```

---The following area is a Code cell (cell numver is 57)---
```python
tempTrainData= (trainDF["prompt"])
```

---The following area is a Code cell (cell numver is 58)---
```python
tempTrainData[0]
```

---The following area is a Code cell (cell numver is 59)---
```python
def dataPreprocess(inputStr):
    # concatenate strings in list
    stripStr = inputStr.strip("[]") # remove list symbol
#     print(stripStr)
    sentence = [s.strip("\"") for s in stripStr.split('","')] # list spliter and remove start 
#     print(sentence)
    finalStr = " ".join(sentence) #concatenate snetence into single string 
    return finalStr
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
# Preprocessing Training / Testing dataset
```

---The following area is a Code cell (cell numver is 61)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 62)---
```python
tempTrainData[0]
```

---The following area is a Code cell (cell numver is 63)---
```python
dataPreprocess(tempTrainData[0])
```

---The following area is a Code cell (cell numver is 64)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 65)---
```python
testDF
```

---The following area is a Code cell (cell numver is 66)---
```python
# Cleaning Training Dataset
trainDF.loc[:, "prompt"] = trainDF["prompt"].apply(dataPreprocess) # convert list of string into single sentence string
trainDF.loc[:, "response_a"] = trainDF["response_a"].apply(dataPreprocess) # convert list of string into single sentence string
trainDF.loc[:, "response_b"] = trainDF["response_b"].apply(dataPreprocess) # convert list of string into single sentence string 

# Clearning Testing Dataset 
testDF.loc[:, "prompt"] = testDF["prompt"].apply(dataPreprocess) # convert list of string into single sentence string
testDF.loc[:, "response_a"] =  testDF["response_a"].apply(dataPreprocess) # convert list of string into single sentence string
testDF.loc[:, "response_b"] = testDF["response_b"].apply(dataPreprocess) # convert list of string into single sentence string 

```

---The following area is a Code cell (cell numver is 67)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 68)---
```python
testDF
```

---The following area is a Markdown cell (cell numver is 69)---
```markdown
## Create Training datase
```

---The following area is a Code cell (cell numver is 70)---
```python
if CFG.USE_TRAIN:
    def getTokenLength(texts):
        ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
        # return length of inputs_ids for each text
        return [len(t) for t in ids]
```

---The following area is a Code cell (cell numver is 71)---
```python
# convert targat from one hot encoding into caterogy (numberic)
targetCol = ["winner_model_a", "winner_model_b", "winner_tie"]
trainDF["label"] = np.argmax(trainDF[targetCol].values, axis =1 )  # 0: for winner model a, 1 for winner model b , 2 for winner tie
```

---The following area is a Code cell (cell numver is 72)---
```python
trainDF["label"].value_counts()
```

---The following area is a Code cell (cell numver is 73)---
```python
trainDF["label"]
```

---The following area is a Code cell (cell numver is 74)---
```python
trainDF["label"].value_counts().plot(kind="bar");
```

---The following area is a Code cell (cell numver is 75)---
```python
# trainDF["labelStr"]= trainDF["label"].apply(str)
```

---The following area is a Code cell (cell numver is 76)---
```python
# trainDF["labelStr"]
```

---The following area is a Code cell (cell numver is 77)---
```python
# create training data set for training feature
# trainDF["text"]=  ("User Prompt: " + trainDF["prompt"] + 
#                      "\n\n---\n\nModel A: " + trainDF["response_a"] +
#                      "\n\n---\n\nModel B: " + trainDF["response_b"] 
#                    )
```

---The following area is a Code cell (cell numver is 78)---
```python
trainDF["text"]=  ("<prompt>: " + trainDF["prompt"] + 
                     "\n\n<response_a>: " + trainDF["response_a"] +
                     "\n\n<response_b>: " + trainDF["response_b"] 
                   )
```

---The following area is a Code cell (cell numver is 79)---
```python
testDF
```

---The following area is a Markdown cell (cell numver is 80)---
```markdown
## Prepare Test Data set for submission
```

---The following area is a Code cell (cell numver is 81)---
```python
# testDF["text"] = ("User Prompt: " + testDF["prompt"] + 
#                      "\n\n---\n\nModel A: " + testDF["response_a"] +
#                      "\n\n---\n\nModel B: " + testDF["response_b"]
#                  )
```

---The following area is a Code cell (cell numver is 82)---
```python
testDF["text"]=  ("<prompt>: " + testDF["prompt"] + 
                     "\n\n<response_a>: " + testDF["response_a"] +
                     "\n\n<response_b>: " + testDF["response_b"] 
                   )
```

---The following area is a Code cell (cell numver is 83)---
```python
print(trainDF["text"][1]) #print the train data sample
```

---The following area is a Code cell (cell numver is 84)---
```python
if False:  # only for statistic count the length of token
    trainDF.loc[:, 'token_count'] = getTokenLength(trainDF['text']) #calucate each 
```

---The following area is a Markdown cell (cell numver is 85)---
```markdown
## Fine Tuning
```

---The following area is a Code cell (cell numver is 86)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 87)---
```python
if False: # only for statistic count the length of token
    print(trainDF['token_count'].describe().to_frame().astype(int)) # check token length in statistic
```

---The following area is a Code cell (cell numver is 88)---
```python
if False:  # only for statistic count the length of token
    # get length of tokens which covers 80% of data, we'll still take 1024 length!
    print(np.percentile(trainDF["token_count"],  q=65))
```

---The following area is a Code cell (cell numver is 89)---
```python
if False:
    trainDF.drop("token_count", axis=1, inplace=True)
```

---The following area is a Code cell (cell numver is 90)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 91)---
```python
print(testDF["text"][0])
```

---The following area is a Code cell (cell numver is 92)---
```python
if CFG.USE_WANDB and CFG.USE_TRAIN:
     # Start a new wandb run
    wandbFineTuningProject = "lmsys-chatbot-araena-fine-tuning"
    runTask1 = wandb.init(project=wandbFineTuningProject, job_type="generation", anonymous="allow")
    # define W&B Table
    wandbCol1 =  ["model", "user query", "modela_ans", "modelb_ans", "label"] #define column for record
    wandbFineTuneTable =wandb.Table(columns=wandbCol1)
```

---The following area is a Code cell (cell numver is 93)---
```python
if CFG.USE_TRAIN == True: #requred GPU support
    # for LoRA fine tuning
    from trl import SFTTrainer
    from peft import LoraConfig, PeftModel, get_peft_model #, prepare_model_for_kbit_training #prepare_model_for_int8_training
```

---The following area is a Markdown cell (cell numver is 94)---
```markdown
# Setup LoRA For Fine tuning
```

---The following area is a Code cell (cell numver is 95)---
```python
if CFG.USE_TRAIN:
    #LoRA config 
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        task_type="SEQ_CLS",  # for Sequence Classification 
        target_modules = ["q_proj", "o_proj", "k_proj", "v_proj",
                      "gate_proj", "up_proj", "down_proj"],
        lora_dropout= 0.05,
    )
    
```

---The following area is a Code cell (cell numver is 96)---
```python
# if False:#CFG.USE_TRAIN:
#     model = prepare_model_for_kbit_training(model) # convert float32 to int8 # take care will out of memory for T4 GPU
#     model= get_peft_model(model, lora_config) #get Model With LORD  # take care int8 train will out of memory for T4 GPU
#     model.print_trainable_parameters()
    
if False:#True:
    model.config.use_cache = False
    model = prepare_model_for_kbit_training(model)
    model= get_peft_model(model, lora_config)
    model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 97)---
```python
model
```

---The following area is a Code cell (cell numver is 98)---
```python
if CFG.USE_TRAIN:
    # Create a preprocessing function to tokenize train data and truncate sequences 
    def tokenizeProcess(sample):
        return tokenizer(sample["text"],  max_length=CFG.maxToken, padding=True, truncation=True)
    
 
```

---The following area is a Markdown cell (cell numver is 99)---
```markdown
# Create Training/Validation Dataset
```

---The following area is a Code cell (cell numver is 100)---
```python
trainDF.describe()
```

---The following area is a Code cell (cell numver is 101)---
```python
if CFG.USE_TRAIN:
#     maxTrainData = 300#5000#10000#5000 #10000
#     maxEvalData = 20#100 
    tempTrainDF = trainDF[:CFG.maxTrainData]
    tempEvalDF =  trainDF[CFG.maxTrainData: CFG.maxTrainData+ CFG.maxEvalData]
    # convert HuggingFace dataset
    trainDataset = Dataset.from_pandas(tempTrainDF, split="train")
    evalDataset = Dataset.from_pandas(tempEvalDF, split="test")
    
```

---The following area is a Code cell (cell numver is 102)---
```python
if CFG.USE_TRAIN:
    print(tempTrainDF["label"].value_counts())
    print(tempEvalDF["label"].value_counts())
```

---The following area is a Markdown cell (cell numver is 103)---
```markdown
### Create Submit Dataset
```

---The following area is a Code cell (cell numver is 104)---
```python
submitDataset = Dataset.from_pandas(testDF, split="test") 
```

---The following area is a Code cell (cell numver is 105)---
```python
submitDataset
```

---The following area is a Code cell (cell numver is 106)---
```python
if CFG.USE_TRAIN:
    del tempTrainDF
    del tempEvalDF
    print(len(trainDataset), len(evalDataset))
```

---The following area is a Code cell (cell numver is 107)---
```python
if CFG.USE_TRAIN:
    # convert dataset to  Datasetdict 
    datasetDict= DatasetDict({
        "train": trainDataset,
        'test': evalDataset
    })
```

---The following area is a Code cell (cell numver is 108)---
```python
submitDataDict = DatasetDict({
    "test": submitDataset
})
```

---The following area is a Code cell (cell numver is 109)---
```python
submitDataDict
```

---The following area is a Code cell (cell numver is 110)---
```python
if CFG.USE_TRAIN:
    print(datasetDict)
```

---The following area is a Code cell (cell numver is 111)---
```python
if CFG.USE_TRAIN:
    # drop unused column 
#     datasetDict =datasetDict.remove_columns(['id', 'model_a', 'model_b', 'response_a', 'response_b'])
    datasetDict = datasetDict.remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
                                 'winner_model_a', 'winner_model_b', 'winner_tie'])
```

---The following area is a Code cell (cell numver is 112)---
```python
# convert Train dataset into tokenize 
if CFG.USE_TRAIN:
    datasetDict = datasetDict.map(tokenizeProcess, batched=True)
```

---The following area is a Code cell (cell numver is 113)---
```python
# datasetDict["train"].remove_columns(['id', 'model_a', 'model_b','winner_model_a', 'winner_model_b', 'winner_tie'])
```

---The following area is a Code cell (cell numver is 114)---
```python
datasetDict
```

---The following area is a Markdown cell (cell numver is 115)---
```markdown
## Convert Test submisioin datasetDict
```

---The following area is a Code cell (cell numver is 116)---
```python
submitDataDict.map(tokenizeProcess, batched=True)
```

---The following area is a Code cell (cell numver is 117)---
```python
submitDataDict["test"][0]
```

---The following area is a Code cell (cell numver is 118)---
```python
datasetDict["train"][0]
```

---The following area is a Code cell (cell numver is 119)---
```python
if True:
    datasetDict = datasetDict.rename_column("label", "labels")
```

---The following area is a Code cell (cell numver is 120)---
```python
datasetDict
```

---The following area is a Code cell (cell numver is 121)---
```python
if CFG.USE_TRAIN:
    accList = []
    f1List = []
    recallList = []
    preciseList = []
    def compute_metrics1(pred):
        logits, labels= pred
        predictions = np.argmax(logits, axis=1) # 
        return {"accuracy": (predictions == labels).mean()}
    
    def compute_metrics2(pred):
        logits, labels= pred
        predictions = np.argmax(logits, axis=-1) # 
        accuracy = (predictions == labels).mean()
        f1score = f1_score(labels, predictions, average='weighted')
        recallScore = recall_score(labels, predictions,  average='weighted')
        precision = precision_score(labels, predictions,  average='weighted')
        accList.append(accuracy)
        f1List.append(f1score)
        recallList.append(recallScore)
        preciseList.append(precision)
        return {"accuracy": accuracy , "recall":  recallScore,  "precision":precision, 'f1-score': f1score }
    
    def compute_metrics3(eval_preds) -> dict:
        preds = eval_preds.predictions
        labels = eval_preds.label_ids
        probs = torch.from_numpy(preds).float().softmax(-1).numpy()
        loss = log_loss(y_true=labels, y_pred=probs)
        acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
        return {"acc": acc, "log_loss": loss}
    
    def formatFuc(sample):
        text = f"{sample['text']}"
        retrun [text]
    
```

---The following area is a Code cell (cell numver is 122)---
```python
datasetDict["test"]
```

---The following area is a Code cell (cell numver is 123)---
```python
if CFG.USE_TRAIN:
    from transformers import DataCollatorWithPadding

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    trainArg = TrainingArguments(
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,
        eval_accumulation_steps=1,
        warmup_steps=2,
#         max_steps=300, # , the total number of training steps to perform, overide num_train epochs
        num_train_epochs=1, # set train epochs
        learning_rate=2e-5,
#         evaluation_strategy= 'steps', # older version 
        eval_strategy="steps",
        save_strategy="steps",
        fp16=True,
        logging_steps=CFG.loggingSteps, # 10,#100, #20, #5,#10,
#         save_steps= 100,
        output_dir= '/kaggle/working/lora_model',#"./results",#"outputs",
        optim="paged_adamw_8bit",
        weight_decay=0.01,
        load_best_model_at_end=True,
#         overwrite_output_dir=True,
#         label_names=["label"],
        report_to= reportTo # avoid wnb access token request during training
    )
    
    trainer = SFTTrainer(
        model = model,
        train_dataset=datasetDict["train"],
        eval_dataset= datasetDict["test"],
        args = trainArg,
        max_seq_length=2048,
        peft_config = lora_config,
#         formatting_func = formatFuc, #formatFuc1, #dataset aleader input_ids  
        data_collator=data_collator,
        compute_metrics = compute_metrics2 #compute_metrics1        
    
    )
```

---The following area is a Code cell (cell numver is 124)---
```python
clearMemory()

```

---The following area is a Code cell (cell numver is 125)---
```python
device
```

---The following area is a Code cell (cell numver is 126)---
```python
%%time
if CFG.USE_TRAIN:
    print("Training the Model")
    trainer.train()
#     print("Saving the model!")
    # only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.
#     trainer.model.save_pretrained('fine-tuned-model')
```

---The following area is a Code cell (cell numver is 127)---
```python
# # only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.
if CFG.USE_TRAIN:
    print("Saving the model!")
    trainer.model.save_pretrained('lora_model')
```

---The following area is a Code cell (cell numver is 128)---
```python
clearMemory()
```

---The following area is a Markdown cell (cell numver is 129)---
```markdown
# Inference Test
```

---The following area is a Code cell (cell numver is 130)---
```python
def getClassifierOutput(text):
        """
        Direct sendtext LLM model, get classification output ,
        
        """
        with torch.no_grad():
            inputIds = tokenizer(text, return_tensors="pt").to(device)
            logits = model(**inputIds).logits
            probabilities = nn.functional.softmax(logits, dim=-1)
            classID = logits.argmax().item()
            return probabilities , classID 
```

---The following area is a Code cell (cell numver is 131)---
```python
print(submitDataDict["test"]["text"][0])
```

---The following area is a Code cell (cell numver is 132)---
```python
# accList
# f1List
# recallList
# preciseList
```

---The following area is a Code cell (cell numver is 133)---
```python
list(range(1, len(accList)+1))
```

---The following area is a Code cell (cell numver is 134)---
```python
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 135)---
```python
plt.plot(list(range(1, len(accList)+1)), accList)
plt.xlabel("Step")
plt.ylabel("Accuary")
plt.title("Accuracy")
plt.show();
```

---The following area is a Code cell (cell numver is 136)---
```python
plt.plot(list(range(1, len(f1List)+1)), f1List)
plt.xlabel("Step")
plt.ylabel("F1-Score")
plt.title("F1 Score")
plt.show();
```

---The following area is a Code cell (cell numver is 137)---
```python
plt.plot(list(range(1, len(recallList)+1)), recallList)
plt.xlabel("Step")
plt.ylabel("Recall")
plt.title("Recall")
plt.show();
```

---The following area is a Code cell (cell numver is 138)---
```python
plt.plot(list(range(1, len(preciseList)+1)), preciseList)
plt.xlabel("Step")
plt.ylabel("Precision")
plt.title("Precision")
plt.show();
```

---The following area is a Code cell (cell numver is 139)---
```python
submitDF
```

---The following area is a Code cell (cell numver is 140)---
```python
finalDF = submitDF.copy()
finalDF
```

---The following area is a Code cell (cell numver is 141)---
```python
from torch import nn
```

---The following area is a Code cell (cell numver is 142)---
```python
wina , winb , tie =[], [], [] 
for i, text in enumerate(submitDataDict["test"]["text"]):
    probs , classID = getClassifierOutput(text)
#     print("{}: ", probs.tolist()[0])
    rounded_probs = [round(val, 6) for val in probs.tolist()[0]]
    print(rounded_probs)
    print("Classified ID : ", classID)
    wina.append(rounded_probs[0])
    winb.append(rounded_probs[1])
    tie.append(rounded_probs[2])
    #calculate probability
#     print("Probabilities: ", [round(val, 6) for val in probabilities.tolist()[0]])
    #write to submit DF
#     finalDF.iloc[i, 1:] = [round(val, 6) for val in probabilities.tolist()[0]] #logits.tolist()[0] #update column 
finalDF["winner_model_a"] = wina
finalDF["winner_model_b"] = winb
finalDF["winner_tie"] = tie
```

---The following area is a Code cell (cell numver is 143)---
```python
finalDF.head()
```

---The following area is a Code cell (cell numver is 144)---
```python
finalDF.to_csv('submission.csv' , index=False)
```

---The following area is a Code cell (cell numver is 145)---
```python
# clearMemory()
```

** @@@ Jupyter Notebook numver 54, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 55, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keywords torch RoBERTa for Submission
with internet-off condition


- https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert <br/>
train data processing

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta <br/>
model training using processed train data

- https://www.kaggle.com/code/stpeteishii/download-keybert <br/>
download keybert

- https://www.kaggle.com/code/stpeteishii/save-distilbert-base-nli-mean-tokens <br/>
download distilbert-base-nli-mean-tokens

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta-for-submission <br/>
test data processing, inferance (this notebook)
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install keybert --no-index --find-links=file:///kaggle/input/download-keybert
```

---The following area is a Code cell (cell numver is 2)---
```python
from keybert import KeyBERT
```

---The following area is a Code cell (cell numver is 3)---
```python
import numpy as np 
import pandas as pd 
import os
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import matplotlib.pyplot as plt 
import transformers
import random
import warnings
warnings.simplefilter('ignore')
scaler = torch.cuda.amp.GradScaler() 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 4)---
```python
def random_seed(SEED):
    
    random.seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    
SEED = 508
random_seed(SEED)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Process Test Data
```

---The following area is a Code cell (cell numver is 6)---
```python
from sentence_transformers import SentenceTransformer

local_model = SentenceTransformer('/kaggle/input/save-distilbert-base-nli-mean-tokens')
modelky = KeyBERT(model=local_model)
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')#, encoding='iso-8859-1')
test['prompt_kw']='-'
test['res_a_kw']='-'
test['res_b_kw']='-'

tkw0 = modelky.extract_keywords(test['prompt'],top_n=5)
tkw1 = modelky.extract_keywords(test['response_a'],top_n=10)
tkw2 = modelky.extract_keywords(test['response_b'],top_n=10)
```

---The following area is a Code cell (cell numver is 8)---
```python
for i,w in enumerate(tkw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(tkw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(tkw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_b_kw']=' '.join(ws)   

test['res_a_kw']=test['res_a_kw']+' // '+test['prompt_kw']
test['res_b_kw']=test['res_b_kw']+' // '+test['prompt_kw']
test=test.iloc[:,4:]
display(test)

#test.to_csv('test_key.csv',index=False)
```

---The following area is a Code cell (cell numver is 9)---
```python
testA=test[['res_a_kw']]
testA['label']=0
testA.columns=['text','label']
testB=test[['res_b_kw']]
testB['label']=0
testB.columns=['text','label']
TEST=pd.concat([testA,testB],axis=0)
```

---The following area is a Code cell (cell numver is 10)---
```python
max_sens = 8
p_test=TEST.reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 11)---
```python
class BERTDataSet(Dataset):
    
    def __init__(self,sentences,targets):        
        self.sentences = sentences
        self.targets = targets
        
    def __len__(self):        
        return len(self.sentences)
    
    def __getitem__(self,idx):        
        sentence = self.sentences[idx]    
        bert_sens = tokenizer.encode_plus(
                                sentence,
                                add_special_tokens = True, 
                                max_length = max_sens, 
                                pad_to_max_length = True, 
                                return_attention_mask = True)

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)

        target = torch.tensor(self.targets[idx],dtype=torch.float)
        
        return {
                'ids': ids,
                'mask': mask,

                'targets': target
            }
```

---The following area is a Code cell (cell numver is 12)---
```python
test_dataset = BERTDataSet(p_test["text"],p_test["label"])
test_batch = 32
test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=8,pin_memory=True)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# def predicting
use saved models
```

---The following area is a Code cell (cell numver is 14)---
```python
#model initialized
tokenizer = transformers.RobertaTokenizer.from_pretrained("/kaggle/input/roberta-base")
model = transformers.RobertaForSequenceClassification.from_pretrained("/kaggle/input/roberta-base",num_labels=1)
pths = [os.path.join("/kaggle/input/lmsys-keywords-torch-roberta",s) for s in os.listdir("/kaggle/input/lmsys-keywords-torch-roberta") if ".pth" in s]
print(pths)
```

---The following area is a Code cell (cell numver is 15)---
```python
def predicting(
    test_dataloader,
    model,
    pths 
):
    allpreds = []    
    for pth in pths:  
        state = torch.load(pth, map_location=torch.device('cpu'))      
        model.load_state_dict(state["state_dict"])
        model.to(device)
        model.eval()      
        preds = []
        allvalloss=0

        with torch.no_grad():
            for a in test_dataloader:
                ids = a["ids"].to(device)
                mask = a["mask"].to(device)
                output = model(ids,mask)
                output = output["logits"].squeeze(-1)
                preds.append(output.cpu().numpy())

            preds = np.concatenate(preds)           
            allpreds.append(preds)

    return allpreds
```

---The following area is a Code cell (cell numver is 16)---
```python
tpreds = predicting(test_dataloader,model,pths)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Prediction Result
```

---The following area is a Code cell (cell numver is 18)---
```python
test_pred = []
for p in tpreds[0]:
    test_pred+=[p]
```

---The following area is a Code cell (cell numver is 19)---
```python
submit=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
submit['winner_model_a']=test_pred[0:len(test)]
submit['winner_model_b']=test_pred[len(test):]
pa=submit['winner_model_a']
pb=submit['winner_model_b']
submit['winner_tie']=np.clip((pa+pb),0,1)
display(submit)
submit.to_csv('submission.csv',index=False)
```

** @@@ Jupyter Notebook numver 56, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keywords torch RoBERTa
under internet on condition

https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install chardet
```

---The following area is a Code cell (cell numver is 2)---
```python
#debug = False
#debug2 = False
```

---The following area is a Code cell (cell numver is 3)---
```python
import numpy as np 
import pandas as pd 
import os
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import matplotlib.pyplot as plt 
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import random
import chardet
import warnings
warnings.simplefilter('ignore')
scaler = torch.cuda.amp.GradScaler() 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 4)---
```python
def random_seed(SEED):
    
    random.seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    
SEED = 508
random_seed(SEED)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# label is ranking
```

---The following area is a Code cell (cell numver is 6)---
```python
train0=pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/train_key.csv')
train0=train0[0:20000]
display(train0)
print(train0.columns.tolist())

test0=pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/test_key.csv')
display(test0)
print(test0.columns.tolist())
```

---The following area is a Code cell (cell numver is 7)---
```python
trainA=train0[['res_a_kw','winner_model_a']]
trainA.columns=['text','label']
trainB=train0[['res_b_kw','winner_model_b']]
trainB.columns=['text','label']
data=pd.concat([trainA,trainB],axis=0)

testA=test0[['res_a_kw']]
testA['label']=0
testA.columns=['text','label']
testB=test0[['res_b_kw']]
testB['label']=0
testB.columns=['text','label']
TEST=pd.concat([testA,testB],axis=0)
```

---The following area is a Code cell (cell numver is 8)---
```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 9)---
```python
#tokenizer = transformers.BertTokenizer.from_pretrained("../input/bert-base-uncased")
tokenizer = transformers.AutoTokenizer.from_pretrained("roberta-base")
```

---The following area is a Code cell (cell numver is 10)---
```python
test_s = train['text'].iloc[0]
result1 = tokenizer.encode_plus(test_s)
tokenizer.decode(result1["input_ids"])
```

---The following area is a Code cell (cell numver is 11)---
```python
len(test_s.split(" "))
```

---The following area is a Code cell (cell numver is 12)---
```python
result2 = tokenizer.encode_plus(
    test_s,
    add_special_tokens = True, 
    max_length = 20, 
    pad_to_max_length = True, 
    truncation = True 
)
```

---The following area is a Code cell (cell numver is 13)---
```python
tokenizer.decode(result2["input_ids"])
```

---The following area is a Code cell (cell numver is 14)---
```python
max_sens = 20

train = train.sort_values("label").reset_index(drop=True)

train["kfold"] = train.index % 5

p_train = train[train["kfold"]!=0].reset_index(drop=True)
p_valid = train[train["kfold"]==0].reset_index(drop=True)

p_test=TEST.reset_index(drop=True)

```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
'token_type_ids' no need in RoBERTa/DeBERTa
```

---The following area is a Code cell (cell numver is 16)---
```python
class BERTDataSet(Dataset):
    
    def __init__(self,sentences,targets):        
        self.sentences = sentences
        self.targets = targets
        
    def __len__(self):        
        return len(self.sentences)
    
    def __getitem__(self,idx):        
        sentence = self.sentences[idx]    
        bert_sens = tokenizer.encode_plus(
                                sentence,
                                add_special_tokens = True, 
                                max_length = max_sens, 
                                pad_to_max_length = True, 
                                return_attention_mask = True)

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)

        target = torch.tensor(self.targets[idx],dtype=torch.float)
        
        return {
                'ids': ids,
                'mask': mask,

                'targets': target
            }
```

---The following area is a Code cell (cell numver is 17)---
```python
train_dataset = BERTDataSet(p_train["text"],p_train["label"])
valid_dataset = BERTDataSet(p_valid["text"],p_valid["label"])
test_dataset = BERTDataSet(p_test["text"],p_test["label"])


train_batch = 16
valid_batch = 32
test_batch = 32

train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=8,pin_memory=True)
valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=8,pin_memory=True)
test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=8,pin_memory=True)
```

---The following area is a Code cell (cell numver is 18)---
```python
model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)
#model = transformers.BertForSequenceClassification.from_pretrained("../input/bert-base-uncased",num_labels=1)
```

---The following area is a Code cell (cell numver is 19)---
```python
model.to(device)
model.train()
```

---The following area is a Code cell (cell numver is 20)---
```python
for a in train_dataloader:
    ids = a["ids"].to(device)
    mask = a["mask"].to(device)
    output = model(ids,mask)
    break
```

---The following area is a Code cell (cell numver is 21)---
```python
output = output["logits"].squeeze(-1).shape
```

---The following area is a Code cell (cell numver is 22)---
```python
from transformers import AdamW
LR=2e-5
optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) 
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# set epochs
```

---The following area is a Code cell (cell numver is 24)---
```python
from transformers import get_linear_schedule_with_warmup
epochs = 30
#if debug:
#    epochs = 1
train_steps = int(len(p_train)/train_batch*epochs)
print(train_steps)
num_steps = int(train_steps*0.1)
scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)
```

---The following area is a Code cell (cell numver is 25)---
```python
def loss_fn(output,target):
    return torch.sqrt(nn.MSELoss()(output,target))
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# def training
```

---The following area is a Code cell (cell numver is 27)---
```python
def training(
    train_dataloader,
    model,
    optimizer,
    scheduler
):
    
    model.train()
    torch.backends.cudnn.benchmark = True
    allpreds = []
    alltargets = []

    for a in train_dataloader:

        losses = []
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():

            ids = a["ids"].to(device,non_blocking=True)
            mask = a["mask"].to(device,non_blocking=True)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            target = a["targets"].to(device,non_blocking=True)
            loss = loss_fn(output,target)

            losses.append(loss.item())
            allpreds.append(output.detach().cpu().numpy())
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())

        scaler.scale(loss).backward() 
        scaler.step(optimizer) 
        scaler.update() 
        
        del loss 

        scheduler.step() 

    allpreds = np.concatenate(allpreds)
    alltargets = np.concatenate(alltargets)
    losses = np.mean(losses)
    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))

    return losses,train_rme_loss
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# def validating
```

---The following area is a Code cell (cell numver is 29)---
```python
def validating(valid_dataloader,model):
    
    model.eval()
    allpreds = []
    alltargets = []

    for a in valid_dataloader:
        losses = []
        with torch.no_grad():

            ids = a["ids"].to(device)
            mask = a["mask"].to(device)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            target = a["targets"].to(device)
            loss = loss_fn(output,target)
            losses.append(loss.item())
            allpreds.append(output.detach().cpu().numpy())
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())
            
            del loss

    allpreds = np.concatenate(allpreds)
    alltargets = np.concatenate(alltargets)
    losses = np.mean(losses)
    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))

    return allpreds,losses,valid_rme_loss
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
if debug2 == False:
        for a in range(epochs):
            for b in train_dataloader:
                break

        losses,train_rme_loss = training(train_dataloader,model,optimizer,scheduler)

        for a in valid_dataloader:
            break

# Train and Validate
```

---The following area is a Code cell (cell numver is 31)---
```python
trainlosses = []
vallosses = []
bestscore = None
trainscores = []
validscores = []

for epoch in tqdm(range(epochs)):
    
    print("---------------" + str(epoch) + "start-------------")
    
    trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)    
    trainlosses.append(trainloss)
    trainscores.append(trainscore)
    
    print("trainscore is " + str(trainscore))
    
    preds,validloss,valscore=validating(valid_dataloader,model)    
    vallosses.append(validloss)
    validscores.append(valscore)
    
    print("valscore is " + str(valscore))
    
    if bestscore is None:
        bestscore = valscore
        
        print("Save first model")
        
        state = {
                        'state_dict': model.state_dict(),
                        'optimizer_dict': optimizer.state_dict(),
                        "bestscore":bestscore
                    }
            
        torch.save(state, "model0.pth")
        
    elif bestscore > valscore:
        
        bestscore = valscore        
        print("found better point")        
        state = {
                        'state_dict': model.state_dict(),
                        'optimizer_dict': optimizer.state_dict(),
                        "bestscore":bestscore
                    }
            
        torch.save(state, "model0.pth")
        
    else:
        pass
    
```

---The following area is a Code cell (cell numver is 32)---
```python
plt.scatter(p_valid['label'],preds, alpha=0.2)
plt.title('Validation Prediction Result')
plt.xlabel('Actual')
plt.ylabel('Prediction')
plt.show()

x = np.arange(epochs)
plt.title('Validation Losses')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(x,trainlosses)
plt.plot(x,vallosses)
plt.show()

x = np.arange(epochs)
plt.title('Validation Scores')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.plot(x,trainscores)
plt.plot(x,validscores)
plt.show()
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# save model
```

---The following area is a Code cell (cell numver is 34)---
```python
bestscores = []
bestscores.append(bestscore)

for fold in range(1,5):
    
    # initializing the data
    p_train = train[train["kfold"]!=fold].reset_index(drop=True)
    p_valid = train[train["kfold"]==fold].reset_index(drop=True)

    train_dataset = BERTDataSet(p_train["text"],p_train["label"])
    valid_dataset = BERTDataSet(p_valid["text"],p_valid["label"])
    
    model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)
    
    model.to(device)
    LR=2e-5
    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer
    train_steps = int(len(p_train)/train_batch*epochs)
    num_steps = int(train_steps*0.1)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)

    trainlosses = []
    vallosses = []
    bestscore = None
    trainscores = []
    validscores = []

    for epoch in tqdm(range(epochs)):

        print("---------------" + str(epoch) + "start-------------")

        trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)
        trainlosses.append(trainloss)
        trainscores.append(trainscore)

        print("trainscore is " + str(trainscore))

        preds,validloss,valscore=validating(valid_dataloader,model)
        vallosses.append(validloss)
        validscores.append(valscore)

        print("valscore is " + str(valscore))

        if bestscore is None:
            bestscore = valscore

            print("Save first model")

            state = {
                            'state_dict': model.state_dict(),
                            'optimizer_dict': optimizer.state_dict(),
                            "bestscore":bestscore
                        }

            torch.save(state, "model" + str(fold) + ".pth") 

        elif bestscore > valscore:
            bestscore = valscore
            print("found better point")

            state = {
                            'state_dict': model.state_dict(),
                            'optimizer_dict': optimizer.state_dict(),
                            "bestscore":bestscore
                        }
            torch.save(state, "model"+ str(fold) + ".pth")

        else:
            pass


    bestscores.append(bestscore)
```

---The following area is a Code cell (cell numver is 35)---
```python
bestscores
```

---The following area is a Code cell (cell numver is 36)---
```python
np.mean(bestscores)
print("My CV is " + str(np.mean(bestscores))+ ".")
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# def predicting
not use saved models
```

---The following area is a Code cell (cell numver is 38)---
```python
def predicting(test_dataloader,model):
    
    model.to(device)
    model.eval()   
    allpreds = []
    preds = []
    allvalloss=0

    with torch.no_grad():
        for a in test_dataloader:

            ids = a["ids"].to(device)
            mask = a["mask"].to(device)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            preds.append(output.cpu().numpy())

        preds = np.concatenate(preds)
        allpreds.append(preds)

    return allpreds
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
# Predict
```

---The following area is a Code cell (cell numver is 40)---
```python
tpreds = predicting(test_dataloader,model)
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# Prediction Result
```

---The following area is a Code cell (cell numver is 42)---
```python
test_pred = []
for p in tpreds[0]:
    test_pred+=[p]
```

---The following area is a Code cell (cell numver is 43)---
```python
submit=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
pa=test_pred[0:len(test0)]
pb=test_pred[len(test0):]
pc=[]
for i in range(len(test0)):
    pc+=[np.clip(1-(pa[i]+pb[i]),0,1)]
submit['winner_model_a']=pa
submit['winner_model_b']=pb
submit['winner_tie']=pc
display(submit)
submit.to_csv('submission.csv',index=False)
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
[Caution] The 'submission.csv' is not available for submission, because this noteboook runs under internet-on condition.
```

** @@@ Jupyter Notebook numver 57, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model

# Load the dataset
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Combine prompts and responses for feature extraction
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']

# Vectorize the text data using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_a = tfidf.fit_transform(train_data['text_a']).toarray()
X_b = tfidf.transform(train_data['text_b']).toarray()

# Combine the vectorized responses
X = np.hstack([X_a, X_b])

# Target variable
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Define the model using Functional API
input_layer = Input(shape=(X_train.shape[1],))
x = Dense(512, activation='relu')(input_layer)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output_layer = Dense(3, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
val_predictions = model.predict(X_val)
loss = log_loss(y_val, val_predictions)
print(f'Validation Log Loss: {loss}')

# Prepare the test data
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b']
X_test_a = tfidf.transform(test_data['text_a']).toarray()
X_test_b = tfidf.transform(test_data['text_b']).toarray()
X_test = np.hstack([X_test_a, X_test_b])
X_test = scaler.transform(X_test)

# Predict on the test set
test_predictions = model.predict(X_test)

# Create the submission file
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_predictions[:, 0]
submission['winner_model_b'] = test_predictions[:, 1]
submission['winner_tie'] = test_predictions[:, 2]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 58, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install peft bitsandbytes
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from datasets import Dataset, load_metric, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback,
    AutoConfig
)

from pathlib import Path
from sklearn.metrics import log_loss
import numpy as np
import json

from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType, LoftQConfig
import bitsandbytes as bnb

from tqdm.auto import tqdm
import random
```

---The following area is a Code cell (cell numver is 3)---
```python
from kaggle_secrets import UserSecretsClient
import wandb
user_secrets = UserSecretsClient()
my_secret = user_secrets.get_secret("wandb_api_key") 
wandb.login(key=my_secret)
```

---The following area is a Code cell (cell numver is 4)---
```python
%env WANDB_LOG_MODEL="checkpoint"
%env WANDB_PROJECT=LMSYS
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Data & Configs
```

---The following area is a Code cell (cell numver is 6)---
```python
def seed_everything(seed):
    import random
    import os
    import numpy as np
    import torch
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(2024)
```

---The following area is a Code cell (cell numver is 7)---
```python
max_len = 1024
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Generate label & remove columns
```

---The following area is a Code cell (cell numver is 9)---
```python
folder_path = Path('/kaggle/input/lmsys-chatbot-arena')
```

---The following area is a Code cell (cell numver is 10)---
```python
train_df = load_dataset('csv', data_files=str(folder_path / 'train.csv'))
# train_df = load_dataset('csv', data_files='external_train.csv')
# train_df = load_dataset('csv', data_files='/kaggle/input/lmsys-additional-33k-labelled-conversations/lmsys-33k-deduplicated.csv')
test_df = load_dataset('csv', data_files=str(folder_path / 'test.csv'))
sample_df = load_dataset('csv', data_files=str(folder_path / 'sample_submission.csv'))
```

---The following area is a Code cell (cell numver is 11)---
```python
def get_label(sample):
    sample['label'] = np.argmax([sample['winner_model_a'], sample['winner_model_b'], sample['winner_tie']])
    return sample
train_df = train_df.map(get_label)
```

---The following area is a Code cell (cell numver is 12)---
```python
train_df = train_df.select_columns(['prompt', 'response_a', 'response_b', 'label'])
train_df
```

---The following area is a Code cell (cell numver is 13)---
```python
dataset = train_df['train'].train_test_split(0.05)
dataset
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Models
```

---The following area is a Code cell (cell numver is 15)---
```python
# model_name = "microsoft/deberta-v3-base"
adapther_tokenizer_path = "/kaggle/input/deberta-finetuned/DeBerta-base-5"
model_name = "/kaggle/input/deberta-finetuned/LMSYS/checkpoint-2980"
```

---The following area is a Code cell (cell numver is 16)---
```python
tokenizer = AutoTokenizer.from_pretrained(adapther_tokenizer_path, model_max_length=max_len)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
)
```

---The following area is a Code cell (cell numver is 17)---
```python
loftq_config = LoftQConfig(loftq_bits=4)
peft_config = LoraConfig(
#     target_modules=['query_proj', 'value_proj', 'key_proj'],
    use_rslora=True,
    r=16,
    lora_alpha=8,
    lora_dropout=0.1,
    task_type=TaskType.SEQ_CLS,
    init_lora_weights='loftq',
)

model = get_peft_model(model, peft_config)
```

---The following area is a Code cell (cell numver is 18)---
```python
def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )
    
print_trainable_parameters(model)
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 20)---
```python
sep_token = tokenizer.sep_token_id
cls_token = tokenizer.cls_token_id
tokenizer.add_special_tokens({'pad_token': '<pad>'})
# model.resize_token_embeddings(len(tokenizer))
len_to_each = max_len // 3 - 3
```

---The following area is a Code cell (cell numver is 21)---
```python
def tokenize_function(sample):
    prompt =  tokenizer(sample['prompt'], max_length=len_to_each, truncation=True, padding=True).input_ids
    response_a = tokenizer(sample['response_a'], max_length=len_to_each, truncation=True, padding=True).input_ids
    response_b = tokenizer(sample['response_b'], max_length=len_to_each, truncation=True, padding=True).input_ids
    
    sample['input_ids'] = [cls_token] + prompt + [sep_token] + response_a + [sep_token] + response_b
    return sample
```

---The following area is a Code cell (cell numver is 22)---
```python
dataset = dataset.map(tokenize_function)
```

---The following area is a Code cell (cell numver is 23)---
```python
dataset
```

---The following area is a Code cell (cell numver is 24)---
```python
dataset = dataset.select_columns(['input_ids', 'label'])
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## Train
```

---The following area is a Code cell (cell numver is 26)---
```python
args = TrainingArguments(
    "LMSYS",
    eval_strategy = "steps",
    save_strategy = "steps",
    save_steps=500,
    learning_rate=3e-4,
    fp16=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    optim='adamw_hf',
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    save_total_limit = 2,
    metric_for_best_model='accuracy',
    report_to="wandb",
    run_name="DeBerta-base-train-data",
    eval_steps=500,
    logging_strategy="steps", 
    logging_steps=100,
)
```

---The following area is a Code cell (cell numver is 27)---
```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    return {
        'eval_log_loss': log_loss(labels, probabilities),
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()
    }
```

---The following area is a Code cell (cell numver is 28)---
```python
data_collator = DataCollatorWithPadding(tokenizer)
```

---The following area is a Code cell (cell numver is 29)---
```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
```

---The following area is a Code cell (cell numver is 30)---
```python
trainer.train()
wandb.finish()
```

---The following area is a Code cell (cell numver is 31)---
```python
save_folder = 'DeBerta-base-trained'
model = model.merge_and_unload()
model.save_pretrained(save_folder)
tokenizer.save_pretrained(save_folder)
```

---The following area is a Code cell (cell numver is 32)---
```python
trainer.save_model(f"{save_folder}-trainer")
```

---The following area is a Code cell (cell numver is 33)---
```python

```

** @@@ Jupyter Notebook numver 59, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install git+https://github.com/huggingface/transformers
!pip install -U bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    AutoModel
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = True )

```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 8)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 10)---
```python
from transformers import AutoModel, MistralPreTrainedModel, MistralModel
class Llama3ForSFT(MistralPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = MistralModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
#         if self.config.pretraining_tp > 1:
#             lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
#             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
#             logits = torch.cat(logits, dim=-1)
#         else:
        logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 11)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)

model = Llama3ForSFT.from_pretrained(
    model_path, 
    torch_dtype=torch.float16, 
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 13)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=3,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 15)---
```python
import transformers 
transformers.__version__
```

---The following area is a Code cell (cell numver is 16)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

** @@@ Jupyter Notebook numver 60, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install  /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
!pip install  /kaggle/input/textstat-dependencies/textstat-0.7.4-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import textstat as ts
import json
from ftfy import fix_encoding
from catboost import CatBoostClassifier,Pool
from lightgbm import LGBMClassifier
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

from tqdm.notebook import tqdm
tqdm.pandas()
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Data
```

---The following area is a Code cell (cell numver is 4)---
```python
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 5)---
```python
response_a = pd.read_csv("/kaggle/input/responses-textstat/response_a.csv")
response_b = pd.read_csv("/kaggle/input/responses-textstat/response_b.csv")
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## Get true data
```

---The following area is a Code cell (cell numver is 7)---
```python
unused_columns = ['model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'text_standard']
```

---The following area is a Code cell (cell numver is 8)---
```python
textstat_train = response_a.drop(columns=unused_columns).merge(response_b.drop(columns=unused_columns), on='id')
```

---The following area is a Code cell (cell numver is 9)---
```python
textstat_train = textstat_train.groupby('id').mean()
textstat_train.to_csv('textstat.csv', index=False)
```

---The following area is a Code cell (cell numver is 10)---
```python
X = textstat_train.copy()
```

---The following area is a Code cell (cell numver is 11)---
```python
y = response_a[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].groupby('id').agg('max').apply(pd.Series.argmax, axis=1)
y.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)
clfs = []
scores = []
for train_index, test_index in tqdm(skf.split(X, y), total=5):

    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ 
    train_dataset = Pool(data=X_train, label=y_train)
    eval_dataset = Pool(data=X_test, label=y_test)

    clf = CatBoostClassifier(
        depth=6,
        iterations=10000,
        learning_rate=0.06,
#         loss_function="MultiLogloss",  # MultiLogloss
        eval_metric = 'AUC', 
#         custom_metric=["Logloss"],  # 'AUC / Accuracy,
        
        # Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„Ð¸ÑˆÐºÐ° ÐºÐ°Ñ‚Ð±ÑƒÑÑ‚Ð° - Ñ€Ð°Ð±Ð¾Ñ‚Ð° Ñ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸
#         cat_features=cat_features,
        # ignored_features = ignored_features,
        
        # Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ
#         colsample_bylevel=0.4,
#         subsample=0.95,
        l2_leaf_reg=10,
        min_data_in_leaf=50,
        max_bin=70,
        random_strength=1,
        
        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ
        task_type="CPU",    
        thread_count=-1,
        bootstrap_type="Bernoulli", 
        
        # Ð’Ð°Ð¶Ð½Ð¾Ðµ!
        random_seed=2024,
#         auto_class_weights="SqrtBalanced",
        early_stopping_rounds=200)

    clfs.append(clf)

    clf.fit(
        train_dataset,
        eval_set=eval_dataset,
        verbose=200,
        use_best_model=True,
        plot=False)

    scores.append(np.mean([v for k, v in clf.best_score_["validation"].items() if "Recall" in k], dtype="float16"))
    # scores.append(clf.best_score_['validation']['MultiClass'])
    # clf.save_model("../tmp_data/cool_catboost_model_{}_deep".format(n))

assert len(clfs) == 5
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Generate test
```

---The following area is a Code cell (cell numver is 14)---
```python
df_test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))

    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])
    return tmp
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_features(df, column):
    df = df.copy()
    df['flesch_reading_ease'] = df[column].progress_apply(lambda x: ts.flesch_reading_ease(str(x)))
    df['flesch_kincaid_grade'] = df[column].progress_apply(lambda x: ts.flesch_kincaid_grade(str(x)))
    df['smog_index'] = df[column].progress_apply(lambda x: ts.smog_index(str(x)))
    df['automated_readability_index'] = df[column].progress_apply(lambda x: ts.automated_readability_index(str(x)))
    df['dale_chall_readability_score'] = df[column].progress_apply(lambda x: ts.dale_chall_readability_score(str(x)))
    df['difficult_words'] = df[column].progress_apply(lambda x: ts.difficult_words(str(x)))
    df['linsear_write_formula'] = df[column].progress_apply(lambda x: ts.linsear_write_formula(str(x)))
    df['gunning_fog'] = df[column].progress_apply(lambda x: ts.gunning_fog(str(x)))
    df['text_standard'] = df[column].progress_apply(lambda x: ts.text_standard(str(x)))
    df['fernandez_huerta'] = df[column].progress_apply(lambda x: ts.fernandez_huerta(str(x)))
    df['szigriszt_pazos'] = df[column].progress_apply(lambda x: ts.szigriszt_pazos(str(x)))
    df['gutierrez_polini'] = df[column].progress_apply(lambda x: ts.gutierrez_polini(str(x)))
    df['crawford'] = df[column].progress_apply(lambda x: ts.crawford(str(x)))
#     df['gulpease_index'] = df[column].progress_apply(lambda x: ts.gulpease_index(str(x)))
#     df['osman'] = df[column].progress_apply(lambda x: ts.osman(str(x)))
    return df
```

---The following area is a Code cell (cell numver is 17)---
```python
test_expl = get_exploded(df_test)
test_a = get_features(test_expl, 'response_a')
test_b = get_features(test_expl, 'response_b')
```

---The following area is a Code cell (cell numver is 18)---
```python
unused_columns = ['prompt', 'response_a', 'response_b', 'text_standard']
textstat_test = test_a.drop(columns=unused_columns).merge(test_b.drop(columns=unused_columns), on='id')
test = textstat_test.groupby('id').mean()
test
```

---The following area is a Code cell (cell numver is 19)---
```python
y_pred = []
for clf in tqdm(clfs, total=len(clfs)):
    y_predict = clf.predict_proba(test)
    y_pred.append(y_predict)
```

---The following area is a Code cell (cell numver is 20)---
```python
y_pred = sum(y_pred) / len(y_pred)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# Submition
```

---The following area is a Code cell (cell numver is 22)---
```python
sample_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 23)---
```python
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred
```

---The following area is a Code cell (cell numver is 24)---
```python
sample_df.head()
```

---The following area is a Code cell (cell numver is 25)---
```python
sample_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 61, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
# from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from ftfy import fix_encoding
import re
import nltk
from nltk.corpus import stopwords
# nltk.download('stopwords')
from torch.utils.data import Dataset
import torch
import json
import optuna
import numpy as np

from tqdm.auto import tqdm
tqdm.pandas()
```

---The following area is a Code cell (cell numver is 3)---
```python
stop_words = pd.read_csv("/kaggle/input/nltk-english-stopwords/nltk_eng_stopwords.csv")["list_of_stopwords"].tolist()
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Load Data
```

---The following area is a Code cell (cell numver is 5)---
```python
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 6)---
```python
# if test_df.shape[0] < 10:
#     train_df = train_df[:100]
```

---The following area is a Code cell (cell numver is 7)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))

    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])
    return tmp
```

---The following area is a Code cell (cell numver is 8)---
```python
tmp_train = get_exploded(train_df)
tmp_test = get_exploded(test_df)
```

---The following area is a Code cell (cell numver is 9)---
```python
class MyDataset(Dataset):
    def __init__(self, df, col):
        self.col = col
        self.df = df.copy()
        
        self.df["prompt"] = self.df["prompt"].progress_apply(self.fix_encode)
        self.df[col] = self.df[col].progress_apply(self.fix_encode)
        
        self.df = self.df.explode(['prompt', col])
    
    def fix_encode(self, x):
        return json.loads(fix_encoding(x))

    def __len__(self):
        return len(self.df)

    def __getitem__(self, i):
        QA_input = {}
        QA_input['question'] = str(self.df.iloc[i]["prompt"])
        QA_input['context'] = str(self.df.iloc[i][self.col])
        
        if not QA_input['question']:
#             print(True)
            QA_input['question'] = 'empty_text' * 10
        if not QA_input['context']:
#             print(True)
            QA_input['context'] = 'empty_text' * 10
        
        QA_input['question'] = QA_input['question'][:510]
        QA_input['context'] = QA_input['context'][:510]

        return QA_input


dataset_a = MyDataset(train_df, col='response_a')
dataset_b = MyDataset(train_df, col='response_b')

dataset_a_test = MyDataset(test_df, col='response_a')
dataset_b_test = MyDataset(test_df, col='response_b')
```

---The following area is a Code cell (cell numver is 10)---
```python
len(dataset_a)
```

---The following area is a Code cell (cell numver is 11)---
```python
outs_dict = {'dataset_a': [], 'dataset_b': []}
outs_dict_test = {'dataset_a': [], 'dataset_b': []}
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Models

## Generate Score
```

---The following area is a Code cell (cell numver is 13)---
```python
model_list = [
    "deepset/roberta-base-squad2",
    "deepset/deberta-v3-base-squad2",
    "distilbert/distilbert-base-cased-distilled-squad"
#     "Palak/microsoft_deberta-large_squad"
#     'distilbert/distilbert-base-cased-distilled-squad',
#     'deepset/bert-large-uncased-whole-word-masking-squad2'
]

model_list_kaggle = model_list.copy()
for i, model_name in enumerate(model_list_kaggle):
    model_list_kaggle[i] = '/kaggle/input/deberta-v3-base/' + model_name
model_list_kaggle
```

---The following area is a Code cell (cell numver is 14)---
```python
nlp_list = []

for model_name in model_list_kaggle:
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
    
    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer, device='cuda',torch_dtype=torch.float16)
    
    nlp_list.append(nlp)
```

---The following area is a Code cell (cell numver is 15)---
```python
for model_name, pipeline in tqdm(zip(model_list, nlp_list), total=len(nlp_list)):
    pipeline.save_pretrained(model_name)
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_outs(model_list):
    outs_dict = {}
    for model_name in model_list:
        outs_dict[f'{model_name}-a'] = []
        outs_dict[f'{model_name}-b'] = []
    return outs_dict
```

---The following area is a Code cell (cell numver is 17)---
```python
outs_train = get_outs(model_list)
outs_test = get_outs(model_list)
```

---The following area is a Code cell (cell numver is 18)---
```python
# tokenizer_kwargs = {"truncation": True, 'max_length': 512, 'padding': True}
```

---The following area is a Code cell (cell numver is 19)---
```python
def get_score(nlp, dataset) -> list:
    scores = []
    for sample in tqdm(dataset, total=len(dataset)):
        try:
            out = nlp(sample, doc_stride=47)
        except:
            print('omom')
            out = {}
            out['score'] = 0
        scores.append(out['score'])
#     for out in tqdm(nlp(dataset, 
#         handle_impossible_answer=True,
#         max_seq_len=384,
#         ), total=len(dataset)):
#         scores.append(out['score'])
    return scores
```

---The following area is a Code cell (cell numver is 20)---
```python
for model_name, nlp in tqdm(zip(model_list, nlp_list), total=len(model_list)):
    outs_train[f'{model_name}-a'] = get_score(nlp, dataset_a)
    outs_train[f'{model_name}-b'] = get_score(nlp, dataset_b)
    
    outs_test[f'{model_name}-a'] = get_score(nlp, dataset_a_test)
    outs_test[f'{model_name}-b'] = get_score(nlp, dataset_b_test)
    
    del nlp
```

---The following area is a Code cell (cell numver is 21)---
```python
outs_train = pd.DataFrame(outs_train)
outs_test = pd.DataFrame(outs_test)
```

---The following area is a Code cell (cell numver is 22)---
```python
outs_train.head()
```

---The following area is a Code cell (cell numver is 23)---
```python
df_bert = pd.concat([outs_train, tmp_train[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].reset_index()], axis=1).drop('index', axis=1)
df_bert_test = pd.concat([outs_test, tmp_test['id'].reset_index()], axis=1).drop('index', axis=1)
df_bert.head()
```

---The following area is a Code cell (cell numver is 24)---
```python
model_dataset = outs_train.columns
model_dataset
```

---The following area is a Code cell (cell numver is 25)---
```python
df_bert_train = df_bert.groupby('id').mean()

df_bert_test = df_bert_test.groupby('id').mean()
```

---The following area is a Code cell (cell numver is 26)---
```python
df_bert_train.to_csv('df_bert_train.csv', index=False)
df_bert_test.to_csv('df_bert_test.csv', index=False)
```

---The following area is a Code cell (cell numver is 27)---
```python
df_bert_train.head()
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
## Collect in one column
```

---The following area is a Code cell (cell numver is 29)---
```python
df_bert_train['winner'] = df_bert_train[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(np.argmax, axis=1)
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 30)---
```python
df_bert_train.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)
```

---The following area is a Code cell (cell numver is 31)---
```python
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 32)---
```python
df_bert_train.to_csv('deberts.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
## Train model
```

---The following area is a Code cell (cell numver is 34)---
```python
model = CatBoostClassifier(verbose=False, random_state=2024)
```

---The following area is a Code cell (cell numver is 35)---
```python
target = 'winner'
```

---The following area is a Code cell (cell numver is 36)---
```python
df_bert_train.head()
```

---The following area is a Code cell (cell numver is 37)---
```python
X_train = df_bert_train.drop(columns=target)
y_train = df_bert_train[target]
```

---The following area is a Code cell (cell numver is 38)---
```python
model.fit(X_train, y_train)
```

---The following area is a Code cell (cell numver is 39)---
```python
y_pred = model.predict_proba(df_bert_test)
y_pred
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# Submit
```

---The following area is a Code cell (cell numver is 41)---
```python
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred
```

---The following area is a Code cell (cell numver is 42)---
```python
sample_df
```

---The following area is a Code cell (cell numver is 43)---
```python
sample_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 62, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
# Import necessary libraries
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, log_loss, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

print('All libraries has been imported')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Data Collection
train_file = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_file = '/kaggle/input/lmsys-chatbot-arena/test.csv'
train_data = pd.read_csv(train_file)
test_data = pd.read_csv(test_file)

```

---The following area is a Code cell (cell numver is 3)---
```python
# Data Understanding
print("Training Dataset Info:")
print(train_data.info())
print("\nFirst few rows:")
print(train_data.head())

print("\nTest Dataset Info:")
print(test_data.info())
print("\nFirst few rows:")
print(test_data.head())

# Visualize class distribution before resampling
plt.figure(figsize=(12, 6))
sns.countplot(x=train_data['winner_model_a'])
plt.title("Class Distribution Before Resampling")
plt.show()
```

---The following area is a Code cell (cell numver is 4)---
```python
# Data Preparation
def clean_text(text, stop_words):
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'http\S+|www.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text

stop_words = set()

# Clean text data
train_data['prompt'] = train_data['prompt'].apply(lambda x: clean_text(x, stop_words))
train_data['response_a'] = train_data['response_a'].apply(lambda x: clean_text(x, stop_words))
train_data['response_b'] = train_data['response_b'].apply(lambda x: clean_text(x, stop_words))

test_data['prompt'] = test_data['prompt'].apply(lambda x: clean_text(x, stop_words))
test_data['response_a'] = test_data['response_a'].apply(lambda x: clean_text(x, stop_words))
test_data['response_b'] = test_data['response_b'].apply(lambda x: clean_text(x, stop_words))

```

---The following area is a Code cell (cell numver is 5)---
```python
# Vectorize text data
vectorizer = TfidfVectorizer(max_features=1000)
train_text = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']
test_text = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']

X_train_text = vectorizer.fit_transform(train_text)
X_test_text = vectorizer.transform(test_text)

# Verbosity Bias - Add response lengths and length differences
train_data['response_a_length'] = train_data['response_a'].apply(len)
train_data['response_b_length'] = train_data['response_b'].apply(len)
test_data['response_a_length'] = test_data['response_a'].apply(len)
test_data['response_b_length'] = test_data['response_b'].apply(len)
train_data['length_diff'] = train_data['response_a_length'] - train_data['response_b_length']
test_data['length_diff'] = test_data['response_a_length'] - test_data['response_b_length']

# Position Bias - Add position bias feature
train_data['position_bias_a'] = 0  # Assuming response_a is always the first
train_data['position_bias_b'] = 1  # Assuming response_b is always the second
test_data['position_bias_a'] = 0
test_data['position_bias_b'] = 1

# Self-Enhancement Bias - Add self-enhancement detection feature
def detect_self_enhancement(text):
    keywords = ['best', 'better', 'excellent', 'superior', 'number one']
    for keyword in keywords:
        if keyword in text:
            return 1
    return 0

train_data['self_enhancement_a'] = train_data['response_a'].apply(detect_self_enhancement)
train_data['self_enhancement_b'] = train_data['response_b'].apply(detect_self_enhancement)
test_data['self_enhancement_a'] = test_data['response_a'].apply(detect_self_enhancement)
test_data['self_enhancement_b'] = test_data['response_b'].apply(detect_self_enhancement)

```

---The following area is a Code cell (cell numver is 6)---
```python
# Encoding Categorical Features
categorical_columns = ['model_a', 'model_b']
for column in categorical_columns:
    if column not in test_data.columns:
        test_data[column] = 'missing'
train_data_encoded = pd.get_dummies(train_data, columns=categorical_columns)
test_data_encoded = pd.get_dummies(test_data, columns=categorical_columns)
train_data_encoded, test_data_encoded = train_data_encoded.align(test_data_encoded, join='left', axis=1, fill_value=0)

# Handle Missing Columns
test_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], errors='ignore', inplace=True)

# Remove non-numeric columns
non_numeric_columns = train_data_encoded.select_dtypes(exclude=[np.number]).columns
train_data_encoded.drop(columns=non_numeric_columns, inplace=True)
test_data_encoded.drop(columns=non_numeric_columns, inplace=True)

# Combine all features into training and testing sets
X_train_combined = np.hstack((X_train_text.toarray(), train_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie']).values))
X_test_combined = np.hstack((X_test_text.toarray(), test_data_encoded.values))
X = X_train_combined
y = train_data_encoded['winner_model_a']

```

---The following area is a Code cell (cell numver is 7)---
```python
# Modeling
# Resample Data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Visualize class distribution after resampling
plt.figure(figsize=(12, 6))
sns.countplot(x=y_resampled)
plt.title("Class Distribution After Resampling")
plt.show()

```

---The following area is a Code cell (cell numver is 8)---
```python
# Split the resampled data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

```

---The following area is a Code cell (cell numver is 9)---
```python
# Hyperparameter tuning and model selection
models = {
    'Logistic Regression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),
        'params': {'C': [0.01, 0.1, 1, 10, 100]}
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30]}
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]}
    }
}

best_models = {}

for model_name, config in models.items():
    grid_search = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=5, scoring='neg_log_loss', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best {model_name} Model: {grid_search.best_params_}")

```

---The following area is a Code cell (cell numver is 10)---
```python
# Evaluation
for model_name, model in best_models.items():
    y_val_pred = model.predict(X_val)
    y_val_pred_proba = model.predict_proba(X_val)
    print(f"Classification Report on Validation Set ({model_name}):")
    print(classification_report(y_val, y_val_pred, zero_division=1))
    print(f"Log Loss ({model_name}): {log_loss(y_val, y_val_pred_proba)}")

    # Confusion Matrix
    cm = confusion_matrix(y_val, y_val_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix ({model_name})')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic ({model_name})')
    plt.legend(loc="lower right")
    plt.show()

```

---The following area is a Code cell (cell numver is 11)---
```python
# Deployment with the best model (example: Logistic Regression)
best_lr_model = best_models['Logistic Regression']
test_predictions_proba = best_lr_model.predict_proba(X_test_combined)
submission_df = pd.DataFrame({
    'id': test_data['id'],
    'winner_model_a': test_predictions_proba[:, 0],
    'winner_model_b': test_predictions_proba[:, 1],
    'winner_tie': 0.0  # Assuming binary classification
})
submission_df.to_csv('submission.csv', index=False)
print(submission_df.head())
print("Submission file saved successfully.")

```

** @@@ Jupyter Notebook numver 63, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.multioutput import MultiOutputClassifier
import numpy as np
# Load the dataset
train_df = pd.read_csv('/kaggle/input/my-files/train.csv')

# Check the column names
print(train_df.columns)

# Check if the 'winner' column is created properly
def get_winner(row):
    if row['winner_model_a'] == 1:
        return 'a'
    elif row['winner_model_b'] == 1:
        return 'b'
    else:
        return 'tie'

train_df['winner'] = train_df.apply(get_winner, axis=1)

# Display the first few rows to ensure 'winner' column is created
print(train_df.head())

# Check if 'winner' column exists in the DataFrame
print('winner' in train_df.columns)

# Visualize the distribution of the 'winner' variable
sns.countplot(data=train_df, x='winner')
plt.title("Distribution of Winners")
plt.xlabel("Winner")
plt.ylabel("Count")
plt.show()

# Feature engineering: Adding length of responses as features
train_df['response_a_length'] = train_df['response_a'].apply(len)
train_df['response_b_length'] = train_df['response_b'].apply(len)

# Display the correlation matrix
correlation_matrix = train_df[['response_a_length', 'response_b_length']].corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title("Correlation Matrix")
plt.show()

# Prepare the data for training
X = train_df[['response_a_length', 'response_b_length']]
y = train_df['winner']

# One-hot encode the target variable
y = pd.get_dummies(y)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = MultiOutputClassifier(RandomForestClassifier())
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict_proba(X_val)

# Convert list of arrays to a single array
y_pred = np.array([pred[:, 1] for pred in y_pred]).T

# Evaluate the model
loss = log_loss(y_val, y_pred)
print(f'Log Loss: {loss}')

# Prepare the submission file
# Load the test set
test_df = pd.read_csv('/kaggle/input/my-files/test.csv')

# Add feature engineering steps for the test set
test_df['response_a_length'] = test_df['response_a'].apply(len)
test_df['response_b_length'] = test_df['response_b'].apply(len)

# Make predictions on the test set
test_pred = model.predict_proba(test_df[['response_a_length', 'response_b_length']])

# Convert list of arrays to a single array
test_pred = np.array([pred[:, 1] for pred in test_pred]).T

# Format the predictions for submission
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': test_pred[:, 0],
    'winner_model_b': test_pred[:, 1],
    'winner_tie': test_pred[:, 2]
})

# Save the submission file
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 64, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes
!pip install -q -U transformers
!pip install -q -U peft
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import os
import torch
from time import time
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    Trainer,
    EarlyStoppingCallback
)
from trl import SFTTrainer,setup_chat_format
import numpy as np
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoTokenizer
from datasets import DatasetDict, Dataset
```

---The following area is a Code cell (cell numver is 2)---
```python
class CFG:
    VER = 1
    NUM_LABELS = 3
    BATCH_SIZE = 4
    EPOCHS = 1
    
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'

    
    SEED = 2024 
    MAX_LENGTH = 1024 
    
    OUTPUT_DIR = 'Llama 3 8b fine-tuned model'
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Prepare Dataset
```

---The following area is a Code cell (cell numver is 4)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train = train[:int(len(train)*0.01)]
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 5)---
```python
from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)

dataset_train = Dataset.from_pandas(train_df)
dataset_val = Dataset.from_pandas(val_df)

# Combine them into a single DatasetDict
dataset = DatasetDict({
    'train': dataset_train,
    'val': dataset_val,
})
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Model Loading -Quantization
```

---The following area is a Code cell (cell numver is 7)---
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)


model = AutoModelForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    quantization_config=quantization_config,
    num_labels=CFG.NUM_LABELS,
    device_map='auto'
)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# LoRA Configuration
```

---The following area is a Code cell (cell numver is 9)---
```python
lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    task_type = 'SEQ_CLS'
)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
```

---The following area is a Code cell (cell numver is 10)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, add_prefix_space=True)

tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True
```

---The following area is a Code cell (cell numver is 11)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
# model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 12)---
```python
def data_preprocesing(row):
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')

# dataset = Dataset.from_pandas(train)
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])
tokenized_data.set_format("torch")
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Finetune Llama 3: Model Training
```

---The following area is a Code cell (cell numver is 14)---
```python
training_args = TrainingArguments(
    per_device_train_batch_size=CFG.BATCH_SIZE,
    num_train_epochs=CFG.EPOCHS,
    logging_dir = f'./logs_v{CFG.VER}',
    output_dir = f'./output_v{CFG.VER}',
    logging_steps=10,
    save_steps=10,
    logging_first_step=True,
    overwrite_output_dir=True,
    warmup_ratio=0.0,
    learning_rate=5e-5,
    lr_scheduler_type='constant',
    weight_decay=0.01,
    eval_steps=10,
    evaluation_strategy='steps',
    save_total_limit=2,
    report_to='none',
    load_best_model_at_end = True
)
```

---The following area is a Code cell (cell numver is 15)---
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['val'],
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]
)

train_result = trainer.train()
```

** @@@ Jupyter Notebook numver 65, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/accelerate-0.29.3-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/fsspec-2024.3.1-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/unsloth-2024.4-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/peft-0.10.0-py3-none-any.whl'

```

---The following area is a Code cell (cell numver is 1)---
```python

!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl'
##!pip install '/kaggle/input/llm-detect-pip/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/nvidia-nccl-2-19-3/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl'
#!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'


```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/shtab-1.7.1-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/tyro-0.8.3-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/trl-0.8.5-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
```

---The following area is a Code cell (cell numver is 5)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 6)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl'
```

---The following area is a Code cell (cell numver is 7)---
```python
import torch
print(f"pytorch version {torch.__version__}")
```

---The following area is a Code cell (cell numver is 8)---
```python

```

---The following area is a Code cell (cell numver is 9)---
```python

```

---The following area is a Code cell (cell numver is 10)---
```python
from unsloth import FastLanguageModel
```

---The following area is a Code cell (cell numver is 11)---
```python

```

---The following area is a Code cell (cell numver is 12)---
```python

```

---The following area is a Code cell (cell numver is 13)---
```python

```

---The following area is a Code cell (cell numver is 14)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 15)---
```python
import pandas as pd
import numpy as np
```

---The following area is a Code cell (cell numver is 16)---
```python
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from peft import PeftModel
from trl import SFTTrainer
from trl import setup_chat_format
from transformers import (AutoModelForCausalLM, 
                          AutoTokenizer, 
                          BitsAndBytesConfig, 
                          TrainingArguments, 
                          pipeline, 
                          logging)
from sklearn.metrics import (accuracy_score, 
                             classification_report, 
                             confusion_matrix)
from sklearn.model_selection import train_test_split
```

---The following area is a Code cell (cell numver is 17)---
```python
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))
```

---The following area is a Code cell (cell numver is 18)---
```python
def generate_test_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred 
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
"""
```

---The following area is a Code cell (cell numver is 19)---
```python

```

---The following area is a Code cell (cell numver is 20)---
```python
'''
MODEL_PATH = "/kaggle/input/llama-3/transformers/8b-hf/1"

quantization_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model1 = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map = "auto",
    trust_remote_code = True,
    quantization_config=quantization_config,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=torch.float16)
'''
```

---The following area is a Code cell (cell numver is 21)---
```python
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",

    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit"
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "/kaggle/input/unslothmistral-7b-instruct-v0-2-bnb-4bit/unsloth-mistral-7b-instruct-v0.2-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```

---The following area is a Code cell (cell numver is 22)---
```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)
```

---The following area is a Code cell (cell numver is 23)---
```python
submission = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'))
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
```

---The following area is a Code cell (cell numver is 24)---
```python
result = []
for index, row in train.iterrows():
    if(row['winner_model_a']==1): result.append('winner_model_a')
    elif(row['winner_model_b']==1): result.append('winner_model_b')
    elif(row['winner_tie']==1): result.append('winner_tie')
train.insert(6, "Preference", result, True)
train
```

---The following area is a Code cell (cell numver is 25)---
```python
train = train[:1000]
train.shape
```

---The following area is a Code cell (cell numver is 26)---
```python
train_size = int(train.shape[0]*0.6)
test_val_size = train.shape[0]-train_size
train_, test_val  = train_test_split(train, train_size=train_size,
                                    test_size=test_val_size,
                                    random_state=42)

test_, val_ = train_test_split(test_val, train_size=int(test_val_size/2),
                                    test_size=int(test_val_size/2),
                                    random_state=42)
```

---The following area is a Code cell (cell numver is 27)---
```python
def past_generate_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}
            ----------------------------------------------------------------------------------------------------------
            model_a: {data_point["model_a"]}
            response_a: {data_point["response_a"]}
            ----------------------------------------------------------------------------------------------------------
            model_b: {data_point["model_b"]}
            response_b: {data_point["response_b"]}
            ----------------------------------------------------------------------------------------------------------
            Preference= {data_point["Preference"]} """.strip()

def generate_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
Human Preference Label: {data_point["Preference"]}
"""

def past_generate_test_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}
            ----------------------------------------------------------------------------------------------------------

            response_a: {data_point["response_a"]}
            ----------------------------------------------------------------------------------------------------------

            response_b: {data_point["response_b"]}
            ----------------------------------------------------------------------------------------------------------
            Preference: """.strip()

def generate_test_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
"""

```

---The following area is a Code cell (cell numver is 28)---
```python
X_train = pd.DataFrame(train_.apply(generate_prompt, axis=1),
                       columns=["text"])
X_eval = pd.DataFrame(val_.apply(generate_prompt, axis=1),
                      columns=["text"])
X_test = pd.DataFrame(test_.apply(generate_test_prompt, axis=1), columns=["text"])

y_true = test_.Preference

train_data = Dataset.from_pandas(X_train)
eval_data = Dataset.from_pandas(X_eval)
```

---The following area is a Code cell (cell numver is 29)---
```python
def evaluate(y_true, y_pred):
    labels = ['winner_model_a', 'winner_model_b', 'winner_tie']
    mapping = {'none':0, 'winner_model_a': 1, 'winner_model_b': 2, 'winner_tie': 3}
    def map_func(x):
        return mapping.get(x, 1)

    y_true = np.vectorize(map_func)(y_true)
    y_pred = np.vectorize(map_func)(y_pred)

    # Calculate accuracy
    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)
    print(f'Accuracy: {accuracy:.3f}')

    # Generate accuracy report
    unique_labels = set(y_true)  # Get unique labels

    for label in unique_labels:
        label_indices = [i for i in range(len(y_true))
                         if y_true[i] == label]
        label_y_true = [y_true[i] for i in label_indices]
        label_y_pred = [y_pred[i] for i in label_indices]
        accuracy = accuracy_score(label_y_true, label_y_pred)
        print(f'Accuracy for label {label}: {accuracy:.3f}')

    # Generate classification report
    class_report = classification_report(y_true=y_true, y_pred=y_pred)
    print('\nClassification Report:')
    print(class_report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])
    print('\nConfusion Matrix:')
    print(conf_matrix)
```

---The following area is a Code cell (cell numver is 30)---
```python
from trl import SFTTrainer
from transformers import TrainingArguments


trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_data,
    eval_dataset = eval_data,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 2,
        warmup_steps = 5,
        #evaluation_strategy="steps",         # evaluate at the end of each epoch
        #save_strategy="steps",
        #save_steps=10,                      # save checkpoint every 500 steps
        #eval_steps=10,
        max_steps = 100,
        learning_rate = 2e-4,
        fp16 = True,
        bf16 = False,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        #load_best_model_at_end=True,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)
```

---The following area is a Code cell (cell numver is 31)---
```python
import wandb
wandb.init(mode='disabled')
```

---The following area is a Code cell (cell numver is 32)---
```python
import gc
gc.collect()
```

---The following area is a Code cell (cell numver is 33)---
```python
trainer.train()
```

---The following area is a Code cell (cell numver is 34)---
```python

```

---The following area is a Code cell (cell numver is 35)---
```python

```

---The following area is a Code cell (cell numver is 36)---
```python
test['text'] = test.apply(lambda x: generate_test_prompt(x), axis=1)
```

---The following area is a Code cell (cell numver is 37)---
```python
'''
prompt = test.loc[2]["text"]

#print(prompt)

#FastLanguageModel.for_inference(model) # Enable native 2x faster inference

inputs = tokenizer(prompt, return_tensors='pt').to(device)

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
out1 = tokenizer.batch_decode(outputs)
print("*************")
print(out1)
'''
```

---The following area is a Code cell (cell numver is 38)---
```python
def extract_logits(text):
    try:
        d = {'winner_a': [], 'winner_b': [], 'winner_tie': []}
        logits = re.findall(r"Logits: \[(.*?)\]", text)[0]
        logits_dict = {k.strip(): float(v) for k, v in [item.split(": ") for item in logits[1:-1].split(", ")]}

        for k, v in logits_dict.items():
            if '_a' in k:
                d['winner_a'].append(v)
            elif '_b' in k:
                d['winner_b'].append(v)
            elif '_tie' in k:
                d['winner_tie'].append(v)
            
        df = pd.DataFrame.from_dict(d)
    
    except:
        d = {'winner_a': [0.33], 'winner_b': [0.33], 'winner_tie': [0.33]}
        df = pd.DataFrame.from_dict(d)


    
    

    return df
```

---The following area is a Code cell (cell numver is 39)---
```python
prompt = X_test.iloc[2]["text"]

#print(prompt)



FastLanguageModel.for_inference(model) # Enable native 2x faster inference

inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#print(inputs)

print("*************")
outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
out1 = tokenizer.batch_decode(outputs)

print(out1)


```

---The following area is a Code cell (cell numver is 40)---
```python

```

---The following area is a Code cell (cell numver is 41)---
```python
# import re
# label_pattern = re.compile(r'Human Preference Label:\s*(\w+)')
```

---The following area is a Code cell (cell numver is 42)---
```python
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')
#     logits_match = logits_pattern.search(text[0])

#     if logits_match:
#         logits_str = logits_match.group(1)
#         logits_list = [(x.strip()) for x in logits_str.split(',')]

#         # Output results
#         #print("Logits:", logits_list)
#     else:
#         # if it is not able to find string, give deafult values
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
    
    
#     return logits_list
```

---The following area is a Code cell (cell numver is 43)---
```python
# y_pred = []
# logits = pd.DataFrame()
# for i in tqdm(range(len(X_test))):
#     prompt = X_test.iloc[i]["text"]
#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference

#     inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#     outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
#     out1 = tokenizer.batch_decode(outputs)
#     try:
#         preferences = label_pattern.search(out1[0]).group(1)
#         lgts = extract_logits(out1[0])
#     except Exception as e:
#         print(out1)
#         preferences = 'winner_tie'
#     print(preferences)
#     print(lgts)
#     y_pred.append(preferences)
    
#     logits = pd.concat([logits, lgts],axis=0)
```

---The following area is a Code cell (cell numver is 44)---
```python
##evaluate(y_true, y_pred)
```

---The following area is a Code cell (cell numver is 45)---
```python
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')
#     logits_match = logits_pattern.search(text[0])

#     if logits_match:
#         logits_str = logits_match.group(1)
#         logits_list = [(x.strip()) for x in logits_str.split(',')]

#         # Output results
#         #print("Logits:", logits_list)
#     else:
#         # if it is not able to find string, give deafult values
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
    
    
#     return logits_list
```

---The following area is a Code cell (cell numver is 46)---
```python
logits_list = pd.DataFrame()
for i in tqdm(range(len(test))):
    prompt = test.iloc[i]["text"]
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

    inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
    out1 = tokenizer.batch_decode(outputs)
    preferences = extract_logits(out1[0])
    #print(preferences)
    logits_list = pd.concat([logits_list, preferences],axis=0)
```

---The following area is a Code cell (cell numver is 47)---
```python
# import pandas as pd



# # Function to convert string list to dictionary
# def logits_to_dict(logits):
#     return {item.split(':')[0].strip("' "): float(item.split(':')[1].strip()) for item in logits}

# # Convert each list to a dictionary
# logits_dicts = [logits_to_dict(logits) for logits in logits_list]

# # Create DataFrame
# df = pd.DataFrame(logits_dicts)

# df

```

---The following area is a Code cell (cell numver is 48)---
```python
df1 = logits_list.reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 49)---
```python
df = pd.concat([test['id'], df1],axis=1)
```

---The following area is a Code cell (cell numver is 50)---
```python

```

---The following area is a Code cell (cell numver is 51)---
```python
df
```

---The following area is a Code cell (cell numver is 52)---
```python
df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 53)---
```python

```

** @@@ Jupyter Notebook numver 66, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Importing Libraries
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification,Qwen2ForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers


```

---The following area is a Code cell (cell numver is 4)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
train = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytes configuration
# bnb_config =  BitsAndBytesConfig(
#     load_in_8bit=True,
#     bnb_8bit_compute_dtype=torch.float16,
#     bnb_8bit_use_double_quant=False)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)
# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')

base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 15)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 16)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 18)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 20)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 21)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Code cell (cell numver is 22)---
```python

out = pd.DataFrame(llama_preds, 
                index = test.id, 
                    columns = train.columns[-3:])
display(out.head())

```

---The following area is a Code cell (cell numver is 23)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 67, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## Importing Libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers


```

---The following area is a Code cell (cell numver is 3)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 6)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 11)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)

```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 32)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 33)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 34)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 35)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 36)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 37)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 38)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 39)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 40)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 41)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 42)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.2 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 44)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 45)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 46)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 68, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Refer: [IFD](https://github.com/tianyi-lab/Superfiltering)
$$\mathrm{IFD}_\theta(Q,A)=\frac{\mathrm{PPL}_\theta(A|Q)}{\mathrm{PPL}_\theta(A)}$$
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import numpy as np 
import pandas as pd 
import regex as re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix, save_npz, load_npz, hstack
import lightgbm as lgb
from tqdm import tqdm
import gensim
import itertools
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
from transformers import DebertaV2Tokenizer, DebertaV2Model
from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer
import torch
import joblib
import unicodedata
import re
import time
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 2)---
```python
MAX_LENGTH = 1024
deberta_path = "/kaggle/input/debertav3base"
gpt_path = "/kaggle/input/qwen2-1-5b-instruct"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SEED = 42
set_seed(SEED)
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
vectorize_on_train_and_test = True
#quick_test for training on small part of train data (and not using bunch of GPU on submit)
#(if this is on - saved models won't be fully trained)
quick_test = True
quick_test_items = 1000
#automatically disable quick_test if we detect actual test data... (assures full training when scoring)
if (len(test)) > 3:quick_test = False
if quick_test: train = train.head(quick_test_items)

def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
train.head(3)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### TF-IDF
```

---The following area is a Code cell (cell numver is 5)---
```python
train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)
test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)

if vectorize_on_train_and_test:
    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)
else:
    vector_fit_text = train_text
```

---The following area is a Code cell (cell numver is 6)---
```python
def custom_tokenizer(text):
    return re.findall(r'[^\W]+', text)

#word-level vectorizer
tfidf_word_vectorizer = TfidfVectorizer(
    ngram_range=(1, 5),
    tokenizer=custom_tokenizer,
    token_pattern=None,
    strip_accents='unicode',
    min_df=4,
    max_features=300
)

#char-level vectorizer
tfidf_char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(1, 5), 
    max_features=1000, 
    min_df=4
)

def batch_process(texts, batch_size):
    for i in range(0, len(texts), batch_size):
        yield texts[i:i + batch_size]

#doing in batches so we can see progress
batch_size = 1000
for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):
    if len(batch) >= tfidf_word_vectorizer.min_df:
        tfidf_word_vectorizer.fit(batch)
    if len(batch) >= tfidf_char_vectorizer.min_df:
        tfidf_char_vectorizer.fit(batch)
        
def get_tfidf_vectors(df):
    vectorized_columns = []
    for column in columns_to_vectorize:
        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))
        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))
    return hstack(vectorized_columns)

tfidf_train_vectors = get_tfidf_vectors(train)
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
### LEN
```

---The following area is a Code cell (cell numver is 8)---
```python
def has_none(vals) -> int:
    # some responses contains null and probably they are useful for prediction
    return int(any(val is None for val in vals))

def str_length(vals) -> int:
    length = 0
    for val in vals:
        if isinstance(val, str):
            length += len(val)
    return length

def get_length_features(data: pd.DataFrame):
    length_feature_array = []
    length_feature_array.append(data["response_a"].apply(str_length))
    length_feature_array.append(data["response_b"].apply(str_length))
    length_feature_array.append(length_feature_array[0] - length_feature_array[1])
    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)
    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))
    length_feature_array.append(data["response_a"].apply(has_none))
    length_feature_array.append(data["response_b"].apply(has_none))
    length_feature_array.append(data["response_a"].apply(has_none) - data["response_b"].apply(has_none))
    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)
    length_feature_array = np.transpose(length_feature_array, (1, 0))
    return length_feature_array
train_length_features = get_length_features(train)
print(train_length_features.shape)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### IFD
```

---The following area is a Code cell (cell numver is 10)---
```python
def get_ifd_features(data: pd.DataFrame):
    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)
    tokenizer = AutoTokenizer.from_pretrained(gpt_path)
    model.eval()
    def get_ppl_features(output, instruct=''):
        try:
            answer_start_index = 0
            if instruct != '':
                answer_start_index = tokenizer.encode(instruct, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).shape[1]
            input_ids = tokenizer.encode(instruct + output, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).to(device)
            labels = input_ids.clone()
            labels[:, :answer_start_index] = -100
            with torch.no_grad():
                outputs = model(input_ids, labels=labels)
            perplexity = torch.exp(outputs.loss)
            return perplexity.to('cpu').item()
        except:
            return 0
        
    ppl_ifd_feature_array = []
    for i in tqdm(range(len(data)), desc="Scoring ifd"):
        instruct = data['prompt'][i]
        output_a = data['response_a'][i]
        output_b = data['response_b'][i]
        
        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)
        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)
        try:
            ifd_a = ppl_ca_a / ppl_da_a
        except ZeroDivisionError:
            ifd_a = 0
        try:
            ifd_b = ppl_ca_b / ppl_da_b
        except ZeroDivisionError:
            ifd_b = 0
        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])
    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)
    return ppl_ifd_feature_array
ppl_ifd_features = get_ifd_features(train)
print(ppl_ifd_features.shape)
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
### Combine
```

---The following area is a Code cell (cell numver is 12)---
```python
tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2
ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7
train_length_features_csr = csr_matrix(train_length_features)*0.1
combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])
print(combined_train_vectors.shape)
print("Vectorizing test text...")
tfidf_test_vectors = get_tfidf_vectors(test)
tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2
test_ppl_ifd_features = get_ifd_features(test)
test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7
test_length_features = get_length_features(test)
test_length_features_csr = csr_matrix(test_length_features)*0.1
combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr]) 
print("Done!")
```

---The following area is a Code cell (cell numver is 13)---
```python
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss, accuracy_score
from sklearn.model_selection import train_test_split
import joblib

max_estimators = 1000
early_stopping_limit = 100

# Data preparation
X = combined_train_vectors
y_encoded = train['label'].values

# LightGBM parameters
params = {
    'n_estimators': max_estimators,
    'max_depth': 4,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'multiclass',
    'num_class': 3,
    'metric': 'multi_logloss',
    'random_state': 42,
    'learning_rate': 0.03,
    'verbose': -1  # keep logs quiet
}

# Create the model
model = lgb.LGBMClassifier(**params)

# 5-fold cross-validation
stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
logloss_scores = []
accuracy_scores = []
test_pred_list = []

for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):
    print(f"\nFold {fold + 1}")
    X_train_fold, X_val_fold = X[train_indices], X[val_indices]
    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]

    def callback(env):
        if env.iteration % 10 == 0: print ("Iteration:", env.iteration, "\tLog Loss:", env.evaluation_result_list[0][2])

    model.fit(
        X_train_fold, y_train_fold,
        eval_set=[(X_val_fold, y_val_fold)],
        eval_metric='multi_logloss',
        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]
    )

    y_pred_proba_fold = model.predict_proba(X_val_fold)
    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)
    logloss_scores.append(logloss_fold)
    print(f"Log Loss: {logloss_fold}")
    
    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)
    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)
    accuracy_scores.append(accuracy_fold)
    print(f"Accuracy: {accuracy_fold}")

    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))

# Calculate and print average scores
average_logloss = np.mean(logloss_scores)
average_accuracy = np.mean(accuracy_scores)
print(f"\nAverage Log Loss: {average_logloss}")
print(f"Average Accuracy: {average_accuracy}")
```

---The following area is a Code cell (cell numver is 14)---
```python
preds_test = np.mean(test_pred_list, axis=0)
submission = pd.DataFrame({
    'id': test["id"],
    'winner_model_a': preds_test[:, 0],
    'winner_model_b': preds_test[:, 1], 
    'winner_tie': preds_test[:, 2]
})
submission.to_csv('submission.csv', index=False)
display(submission)
```

** @@@ Jupyter Notebook numver 69, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# No Installation Required

microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training

The max sequence length has a significant impact on model performance, 
but due to insufficient memory, it was set to a maximum length of 768.


## Load Library
```

---The following area is a Code cell (cell numver is 1)---
```python
import multiprocessing as mp
mp.set_start_method('spawn')
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional,Tuple
```

---The following area is a Code cell (cell numver is 3)---
```python
# params
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"
model_path = "model_checkpoint.pth"
seed = 42
lora_r = 2
quantize_bit = 16
learning_rate = 5e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.999
eps = 1e-9
l1_rate = 1e-10
batch_size = 1
max_len = 256
n_sample = 0.10
n_epoch = 2
device = "cuda"
file_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Preprocess
```

---The following area is a Code cell (cell numver is 5)---
```python
def cl(x):
  if x == [1,0,0]:
    return 0
  elif x == [0,1,0]:
    return 1
  else :
    return 2
```

---The following area is a Code cell (cell numver is 6)---
```python
def preprocess_data(file_path, sample = False):
    train = pd.read_csv(file_path)
    clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]

    clf_train.loc[:, "prompt"] = clf_train["prompt"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_a"] = clf_train["response_a"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_b"] = clf_train["response_b"].apply(lambda x: json.loads(x)[0])

    clf_train = clf_train.dropna()
    clf_train = clf_train.reset_index(drop = True)

    clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train)) ]

    clf_train = clf_train[['prompt','response_a','response_b','target']]

    clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))

    clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))
    clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))
    clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))

    clf_train['len'] = clf_train['p_len'] + clf_train['a_len']+ clf_train['b_len']
    
    if sample:
        clf_train = clf_train.sample(int(len(clf_train)*n_sample), weights = "len", random_state=seed).reset_index(drop=True)

    t_dat, v_dat = train_test_split(clf_train, test_size=0.2, random_state=42, stratify = clf_train['labels'])

    t_dat = t_dat.reset_index(drop=True)
    v_dat = v_dat.reset_index(drop=True)

    t_dat = t_dat.drop( labels= 'target' , axis = 1)
    v_dat = v_dat.drop( labels= 'target' , axis = 1)
    return t_dat, v_dat
```

---The following area is a Code cell (cell numver is 7)---
```python
class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.prompt = df['prompt']
        self.response_a = df['response_a']
        self.response_b = df['response_b']
        self.max_len = max_len
        self.targets = df.get('labels', None)

    def __len__(self):
        return len(self.prompt)

    def __getitem__(self, index):
        prompt = str(self.prompt[index])
        response_a = str(self.response_a[index])
        response_b = str(self.response_b[index])

        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])

        final_prompt_len = min(self.max_len, prompt_len)
        final_a_len = min(self.max_len, response_a_len)
        final_b_len = min(self.max_len, response_b_len)

        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')

        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}
```

---The following area is a Code cell (cell numver is 8)---
```python
def custom_collate_fn(batch, tokenizer):

    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None

    # Find the maximum length of the sequences in the batch
    max_len = max([input_id.size(0) for input_id in input_ids])

    # Re-tokenize with the new max length
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        input_ids = item['input_ids'][:max_len]
        attention_mask = item['attention_mask'][:max_len]

        new_input_ids.append(input_ids)
        new_attention_masks.append(attention_mask)

    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)

    output = {
    'input_ids': new_input_ids,
    'attention_mask': new_attention_masks}

    if labels is not None:
        output['labels'] = labels

    return output
```

---The following area is a Code cell (cell numver is 9)---
```python
def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)
    )
    return dataloader
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load Model
```

---The following area is a Code cell (cell numver is 11)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    qmin = 0.
    qmax = 2.**num_bits - 1.

    min_val, max_val = tensor.min(), tensor.max()
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - min_val / scale

    quantized_tensor = torch.round(tensor / scale + zero_point)
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)
    quantized_tensor = (quantized_tensor - zero_point) * scale

    return quantized_tensor

def quantize_model(model, num_bits=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)
        elif isinstance(module, nn.Conv2d):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)

    return model


# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Add Adepter Layers
Copied from transformers.models.phi3.modeling_phi3.Phi3Attention

[github url](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py)
```

---The following area is a Code cell (cell numver is 13)---
```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha
        self.rank = rank
        self.lora_a = nn.Linear(in_features, rank, bias=False)
        self.lora_b = nn.Linear(rank, out_features, bias=False)
        self.dropout = nn.Dropout(lora_dropout)

    def forward(self, x):
        lora_out =  self.alpha * self.lora_b(self.lora_a(x))
        lora_out = self.dropout(lora_out)
        return lora_out
```

---The following area is a Code cell (cell numver is 14)---
```python
from transformers.models.phi3.modeling_phi3 import (
Phi3RotaryEmbedding, 
# Phi3LongRoPEScaledRotaryEmbedding,
apply_rotary_pos_emb,
repeat_kv
)
```

---The following area is a Code cell (cell numver is 15)---
```python
class Phi3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta
        self.rope_scaling = config.rope_scaling
        self.is_causal = True

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)
        self._init_rope()
        
        ########################## LoRA adapters ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)
        ########################## LoRA adapters ##########################
        
    def _init_rope(self):
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         logger.warning_once("You are not running the flash-attention implementation, expect numerical differences.")

        bsz, q_len, _ = hidden_states.size()
        ########################## LoRA adapters ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)
        ########################## LoRA adapters ##########################
        query_pos = self.num_heads * self.head_dim
        query_states = qkv[..., :query_pos]
        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights += causal_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        ########################## LoRA adapters ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)
        ########################## LoRA adapters ##########################
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value

```

---The following area is a Code cell (cell numver is 16)---
```python
def replace_attention_module(config,layer,layer_idx):
    if hasattr(layer, 'self_attn') and layer_idx > 12:

        new_attention = Phi3Attention(config,layer_idx)

        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention
```

---The following area is a Code cell (cell numver is 17)---
```python
loss_fn = nn.CrossEntropyLoss()

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # config ì¶”ê°€
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # config ì„¤ì •
        self.peft_model = lora_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.config.hidden_size, 3)
#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)
#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state.mean(dim =1)
        output_dropout = self.dropout(pooled_output)
        logits = self.classifier(output_dropout)
        loss = None
        if labels is not None:
          labels = labels
          loss = loss_fn(logits, labels)
        return loss, logits
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Parallel Train
```

---The following area is a Code cell (cell numver is 19)---
```python
def parallel_function(model_name,attention_name,file_path):
    mp.set_start_method('spawn', force=True)

    accelerator = Accelerator(mixed_precision="fp16")
    if accelerator.is_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()

    set_seed(seed)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"  # Fix weird overflow issue with fp16 training
    
    model = AutoModel.from_pretrained(model_name,torch_dtype=torch.float16)
    model = quantize_model(model)
    for idx, layer in enumerate(model.layers):
        replace_attention_module(model.config,layer,idx)
    model = LoraModelForClassification(model)

    for param in model.peft_model.parameters():
        param.requires_grad = False
    for param in model.classifier.parameters():
        param.requires_grad = True

    for name, module in model.named_modules():
        if isinstance(module, attention_name):
            module.qkv_lora.lora_a.weight.requires_grad = True
            module.qkv_lora.lora_b.weight.requires_grad = True
            module.o_lora.lora_a.weight.requires_grad = True
            module.o_lora.lora_b.weight.requires_grad = True

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)
    print(f"Total trainable parameters: {total_params}")

    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    
    t_dat, v_dat = preprocess_data(file_path,sample = True)
    train_dataloader = create_dataloaders(t_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)
    eval_dataloader = create_dataloaders(v_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)

    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader)

    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=100,
        num_training_steps=len(train_dataloader) * n_epoch,
    )

    progress_bar = tqdm(range(n_epoch * len(train_dataloader)), disable=not accelerator.is_main_process)

    train_loss = 0
    valid_loss = 0
    
    for epoch in range(n_epoch):
        model.train()
        for step, batch in enumerate(train_dataloader):
            loss, _ = model(**batch)
            accelerator.backward(loss)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

            train_loss += loss.item()
        
        all_predictions = []
        all_labels = []
        model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                loss, logits = model(**batch)
            predictions = logits.argmax(dim=-1)
            all_predictions.append(accelerator.gather(predictions))
            all_labels.append(accelerator.gather(batch["labels"]))
            
            valid_loss += loss.item()

        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()
        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()

        acc_metric = accuracy_score(all_labels, all_predictions)
        eval_metric = f1_score(all_labels, all_predictions, average="macro")
        train_loss_avg = train_loss / len(train_dataloader)
        valid_loss_avg = valid_loss / len(eval_dataloader)
        
        accelerator.print(f"epoch: {epoch} \n accuracy: {acc_metric:.3f} \n f1 score: {eval_metric:.3f} \n train loss: {train_loss_avg:.3f} \n valid loss: {valid_loss_avg:.3f}")

    model = accelerator.unwrap_model(model)
    accelerator.wait_for_everyone()

    # ëª¨ë¸ ìƒíƒœ ì‚¬ì „ ì €ìž¥
    if accelerator.is_main_process:
        torch.save(model.state_dict(), model_path)

    # ë™ê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
    accelerator.wait_for_everyone()
```

---The following area is a Code cell (cell numver is 20)---
```python
notebook_launcher(parallel_function, args=(model_name,Phi3Attention,file_path,), num_processes=2)
```

** @@@ Jupyter Notebook numver 70, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Create a new DataFrame with the same 'id' column as 'test'
sample_submission = pd.DataFrame({'id': test['id']})

# Generate random floats for each column
n_rows = len(test)
random_values = np.random.rand(n_rows, 3)

# Normalize the random values so they sum to 1 for each row
normalized_values = random_values / random_values.sum(axis=1)[:, np.newaxis]

# Add the normalized random values to the DataFrame
sample_submission['winner_model_a'] = normalized_values[:, 0]
sample_submission['winner_model_b'] = normalized_values[:, 1]
sample_submission['winner_tie'] = normalized_values[:, 2]

# Verify that the sum of winner columns is very close to 1 for each row
# (using np.isclose due to potential floating-point precision issues)
assert np.allclose(sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1), 1)

# Display the first few rows of the new DataFrame
print(sample_submission.head())
```

---The following area is a Code cell (cell numver is 1)---
```python
sample_submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 71, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Note
- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

# Import
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers -U --no-index --find-links /kaggle/input/lmsys-transformers/lmsys_transformers
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
import json
import time
import sklearn
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
import warnings
warnings.filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 4)---
```python
# /kaggle/input/llama-3-8b-instruct-bnb-4bit/llama-3-8b-Instruct-bnb-4bit/config.json
MODEL_NAME = '/kaggle/input/meta-llama-3-8b/LLM-Research/Meta-Llama-3-8B'
WEIGHTS_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/checkpoint-550'
TOKENIZER_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/tokenizer'
MAX_LENGTH = 2400
BATCH_SIZE = 4
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 6)---
```python
def process_text(text: str) -> list:
    x = json.loads(text)
    x = ['none' if pd.isna(i) else i for i in x]
    return x


def merge_text(x):
    prompt = x['prompt']
    response_a = x['response_a']
    response_b = x['response_b']
    res = ''
    for i in range(len(prompt)):
        if i == len(prompt) - 1:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}'
        else:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}' + '\n\n'
    return res
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

if test.shape[0] == 3:
    test = train.head(100)


test['prompt'] = test['prompt'].map(lambda x: process_text(x))
test['response_a'] = test['response_a'].map(lambda x: process_text(x))
test['response_b'] = test['response_b'].map(lambda x: process_text(x))

test['text'] = test.apply(lambda x: merge_text(x), axis=1)
test.head()
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(tokenizer, x):
    tokenized = tokenizer(x, max_length=MAX_LENGTH, truncation=True)
    return tokenized['input_ids'], tokenized['attention_mask']
```

---The following area is a Code cell (cell numver is 10)---
```python
llama31_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

llama31_data = pd.DataFrame()
llama31_data["id"] = test["id"]
llama31_data["input_ids"], llama31_data["attention_mask"] = tokenize(llama31_tokenizer, list(test['text']))
llama31_data["length"] = llama31_data["input_ids"].apply(len)
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 12)---
```python
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

device_0 = torch.device('cuda:0')
llama31_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    use_cache=False,
    device_map=device_0
)
llama31_model_0.config.pad_token_id = llama31_tokenizer.pad_token_id

device_1 = torch.device('cuda:1')
llama31_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    use_cache=False,
    device_map=device_1
)
llama31_model_1.config.pad_token_id = llama31_tokenizer.pad_token_id


lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    # only target self-attention
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
    # layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=0,
    bias='none',
    task_type=TaskType.SEQ_CLS,
)

# llama31_model_0 = prepare_model_for_kbit_training(llama31_model_0)
# llama31_model_0 = get_peft_model(llama31_model_0, lora_config)
# llama31_model_0.print_trainable_parameters()

# llama31_model_1 = prepare_model_for_kbit_training(llama31_model_1)
# llama31_model_1 = get_peft_model(llama31_model_1, lora_config)
# llama31_model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
llama31_model_0 = PeftModel.from_pretrained(llama31_model_0, model_id=WEIGHTS_PATH).to(device_0) 
llama31_model_0.eval()

llama31_model_1 = PeftModel.from_pretrained(llama31_model_1, model_id=WEIGHTS_PATH).to(device_1)
llama31_model_1.eval();
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 16)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, tokenizer, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 17)---
```python
st = time.time()

llama31_data = llama31_data.sort_values("length", ascending=False).reset_index(drop=True)
sub_1 = llama31_data.iloc[0::2].copy()
sub_2 = llama31_data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (llama31_model_0, llama31_model_1), (llama31_tokenizer, llama31_tokenizer), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 18)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 72, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.

## ðŸ†• Update in version 4
The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.
In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.
This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.
We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:
1. Flash Attention
2. Memory Efficient Attention
3. A PyTorch (naive) implementation

By default, all of those are enabled but we can also manually enable/disable certain backends.
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 73, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
from tqdm import tqdm
import pandas as pd
import json
import torch
from transformers import AutoModel
from numpy.linalg import norm
import torch.nn as nn
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import re
```

---The following area is a Code cell (cell numver is 1)---
```python
device = "cuda" if torch.cuda.is_available else "cpu"
device
```

---The following area is a Code cell (cell numver is 2)---
```python
tokenizer = AutoModel.from_pretrained('/kaggle/input/jinaai/pytorch/default/4')
```

---The following area is a Code cell (cell numver is 3)---
```python
class EmbeddingModel(nn.Module):
    def __init__(self, embedding_model, max_sequences):
        super(EmbeddingModel, self).__init__()
        self.embedding = embedding_model
        self.max_seq_length = max_sequences
        self.device = device

    def forward(self, prompts, responses_a, responses_b):
        batch_features_a = []
        batch_features_b = []

        for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):
            prompt = json.loads(prompt)
            response_a = json.loads(response_a)
            response_b = json.loads(response_b)
            
            prompt = ["" if p is None else p for p in prompt]
            response_a = ["" if r is None else r for r in response_a]
            response_b = ["" if r is None else r for r in response_b]
            
            
            embedded_prompt = torch.from_numpy(self.embedding.encode(prompt)).to(self.device)
           
            embedded_response_a = torch.from_numpy(self.embedding.encode(response_a)).to(self.device)
            embedded_response_b = torch.from_numpy(self.embedding.encode(response_b)).to(self.device)

            features_a = []
            features_b = []
            for i in range(len(embedded_prompt)):
                combined_a = torch.cat((embedded_prompt[i], embedded_response_a[i]), dim=0)
                combined_b = torch.cat((embedded_prompt[i], embedded_response_b[i]), dim=0)

                features_a.append(combined_a)
                features_b.append(combined_b)

            features_a = torch.stack(features_a) if features_a else torch.tensor([]).to(self.device)
            features_b = torch.stack(features_b) if features_b else torch.tensor([]).to(self.device)

            features_a = self.pad_to_shape(features_a, (self.max_seq_length, 768 * 2))
            features_b = self.pad_to_shape(features_b, (self.max_seq_length, 768 * 2))

            batch_features_a.append(features_a)
            batch_features_b.append(features_b)

        return torch.stack(batch_features_a).to(self.device), torch.stack(batch_features_b).to(self.device)

    def pad_to_shape(self, tensor, shape):
        current_shape = tensor.shape
        padding = [(0, max(s - cs, 0)) for cs, s in zip(current_shape, shape)]
        padded_tensor = F.pad(tensor, pad=[p for pair in reversed(padding) for p in pair], mode='constant', value=0)
        return padded_tensor[:shape[0], :shape[1]]

class Model(nn.Module):
    def __init__(self, embedding_model, max_sequences=64, hidden_dim=512, dropout=0.3):
        super(Model, self).__init__()
        self.device = device
        self.embedding = EmbeddingModel(embedding_model, max_sequences)
        self.lstm_input_a = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)
        self.lstm_input_b = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)

        self.conv_input_a = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)
        self.conv_input_b = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)

        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2 + hidden_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3),
            nn.Softmax()
        ).to(self.device)

    def forward(self, prompts, responses_a, responses_b):
        batch_a, batch_b = self.embedding(prompts, responses_a, responses_b)

        batch_a_lstm, _ = self.lstm_input_a(batch_a)  # (batch, 64, hidden_dim)
        batch_b_lstm, _ = self.lstm_input_b(batch_b)  # (batch, 64, hidden_dim)

        batch_a_cnn = self.conv_input_a(batch_a.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)
        batch_b_cnn = self.conv_input_b(batch_b.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)

        batch_a_lstm = batch_a_lstm[:, -1, :] 
        batch_b_lstm = batch_b_lstm[:, -1, :]  
        batch_a_cnn = batch_a_cnn[:, -1, :]    
        batch_b_cnn = batch_b_cnn[:, -1, :]
        
        combined = torch.cat([batch_a_lstm, batch_a_cnn, batch_b_lstm, batch_b_cnn], dim=1)
        flattened = combined.view(combined.size(0), -1)

        output = self.fc(flattened)
        return output
    
class DatasetLMSYS(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]
        prompt = sample['prompt']
        response_a = sample['response_a']
        response_b = sample['response_b']
        label = sample['model_result']
        return prompt, response_a, response_b, label
    
class DatasetLMSYSTest(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]
        _id = sample['id']
        _prompt = sample['prompt']
        _response_a = sample['response_a']
        _response_b = sample['response_b']
        return _id, _prompt, _response_a, _response_b
```

---The following area is a Code cell (cell numver is 4)---
```python
model = Model(tokenizer).to(device)
```

---The following area is a Code cell (cell numver is 5)---
```python
batch_size = 128
learning_rate = 0.001
epochs = 5
```

---The following area is a Code cell (cell numver is 6)---
```python
file_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
file_data['model_result'] = file_data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)
file_data = file_data[['prompt', 'response_a', 'response_b', 'model_result']]
train_loader = DataLoader(
    dataset=DatasetLMSYS(file_data),
    batch_size=batch_size,
    shuffle=True
)

file_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test_loader = DataLoader(
    dataset=DatasetLMSYSTest(file_test),
    batch_size=batch_size,
    shuffle=False
)
```

---The following area is a Code cell (cell numver is 7)---
```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

---The following area is a Code cell (cell numver is 8)---
```python
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    running_loss = 0
    total_train = 0
    correct_train = 0
    # Train
    model.train()
    for batch in tqdm(train_loader):
        prompts, responses_a, responses_b, labels = batch
        labels = labels.to(device)

        outputs = model(prompts, responses_a, responses_b)
        _, predicted_idx = torch.max(outputs.data, 1)

        loss = criterion(outputs, labels)
        optimizer.zero_grad()

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        total_train += labels.size(0)
        correct_train += (predicted_idx == labels).sum().item()

        del labels, outputs
        
    train_accuracy = 100 * correct_train / total_train
    print(f"\nTraining Loss: {running_loss/len(train_loader):.4f} | Training Accuracy: {train_accuracy:.2f}%")
print("\n==> Training finished!")
```

---The following area is a Code cell (cell numver is 9)---
```python
def test(model, test_loader, device):
    model.eval() 
    results = [] 
    with torch.no_grad(): 
        for batch in tqdm(test_loader):
            ids, prompts, responses_a, responses_b = batch
            outputs = model(prompts, responses_a, responses_b)
            _, predicted_idx = torch.max(outputs.data, 1)
            
            for idx, output, prediction in zip(ids, outputs, predicted_idx):
                results.append({
                    'id': idx.item(),
                    'winner_model_a': output[0].item(),
                    'winner_model_b': output[1].item(),
                    'winner_tie': output[2].item()
                })
    df_results = pd.DataFrame(results)
    return df_results

df_results = test(model, test_loader, device)
df_results
```

** @@@ Jupyter Notebook numver 74, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keras Gemma 2B
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U keras-nlp tensorflow-text
# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.
!pip install -q -U tensorflow-cpu
```

---The following area is a Code cell (cell numver is 2)---
```python
import jax

jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
```

---The following area is a Code cell (cell numver is 4)---
```python
import keras
import keras_nlp
```

---The following area is a Code cell (cell numver is 5)---
```python
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (8, 1),
    ["batch", "model"],
    devices=keras.distribution.list_devices(),
)
```

---The following area is a Code cell (cell numver is 6)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
```

---The following area is a Code cell (cell numver is 7)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 8)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0] if 'null' not in x else None)

raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)


train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].apply(lambda x: '<start_of_turn>user\nFind which one is the best answer for the question:\n'+x['prompt']+'\n\nA:\n'+x['response_a']+'\n\nB\n:'+x['response_b']+'\n\nC:\n both right (or) both wrong<end_of_turn>\n<start_of_turn>model\n', axis=1).apply(lambda x: remove_surrogates(x)),
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
#         'label' : raw_train_dataset[label_columns].apply(lambda x: 'A' if x.values.tolist()[0] == 1 else 'B' if x.values.tolist()[1] == 1 else 'C', axis=1)
})

train_dataset = train_dataset[:4000]
raw_train_dataset = raw_train_dataset[:4000]
```

---The following area is a Code cell (cell numver is 9)---
```python
len(train_dataset)
```

---The following area is a Code cell (cell numver is 10)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)
```

---The following area is a Code cell (cell numver is 11)---
```python
keras.config.set_floatx("float16")

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma/keras/gemma_instruct_2b_en/2")
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 12)---
```python
gemma_lm.backbone.enable_lora(rank=8)

```

---The following area is a Code cell (cell numver is 13)---
```python
# for layer in gemma_lm.backbone.layers[:16]:
#     layer.trainable = False
```

---The following area is a Code cell (cell numver is 14)---
```python
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 15)---
```python
def preprocess_fn(text, label=None):
    preprocessed = gemma_lm._preprocessor(text, sequence_length=3072)[0]
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}, label if label is not None else {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}
```

---The following area is a Code cell (cell numver is 16)---
```python
gemma_lm.layers[-1]
```

---The following area is a Code cell (cell numver is 17)---
```python
import gc
del gemma_lm.layers[-1]

gc.collect()
```

---The following area is a Code cell (cell numver is 18)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

inputs = {
    "token_ids": keras.Input(shape=(3072,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(3072,), dtype=tf.int32, name="padding_mask"),
}
x = gemma_lm.backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 19)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,
                weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 20)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 21)---
```python
import tensorflow as tf


ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values)).batch(8).map(preprocess_fn)
ds = ds.shuffle(ds.cardinality())

```

---The following area is a Code cell (cell numver is 22)---
```python
train_split = ds.take(int(len(ds)*0.9))
val_split = ds.skip(int(len(ds)*0.9)).take(int(len(ds)*0.1))
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=8)
```

---The following area is a Code cell (cell numver is 23)---
```python
model.get_layer("gemma_backbone").save_lora_weights('/kaggle/working/lora19.lora.h5')
```

** @@@ Jupyter Notebook numver 75, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
#pip install torch torchvision torchaudio
```

---The following area is a Code cell (cell numver is 2)---
```python
#pip install pandas numpy scikit-learn tensorflow
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Load the dataset
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Combine prompts and responses for feature extraction
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']

# Vectorize the text data using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_a = tfidf.fit_transform(train_data['text_a']).toarray()
X_b = tfidf.transform(train_data['text_b']).toarray()

# Combine the vectorized responses
X = np.hstack([X_a, X_b])

# Target variable
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build a simple neural network model
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
val_predictions = model.predict(X_val)
loss = log_loss(y_val, val_predictions)
print(f'Validation Log Loss: {loss}')

# Prepare the test data
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b']
X_test_a = tfidf.transform(test_data['text_a']).toarray()
X_test_b = tfidf.transform(test_data['text_b']).toarray()
X_test = np.hstack([X_test_a, X_test_b])
X_test = scaler.transform(X_test)

# Predict on the test set
test_predictions = model.predict(X_test)

# Create the submission file
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_predictions[:, 0]
submission['winner_model_b'] = test_predictions[:, 1]
submission['winner_tie'] = test_predictions[:, 2]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 76, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch.nn.functional as F
np.random.seed(1337)

```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
model = "/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Prepare train set
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train.head(5)
```

---The following area is a Code cell (cell numver is 7)---
```python
def put_text(train):
    train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A:\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
    return train

train = put_text(train)
print(train['text'][0])
```

---The following area is a Code cell (cell numver is 8)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)


train = put_text(train)
print(train['text'][0])
```

---The following area is a Code cell (cell numver is 9)---
```python
train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))

```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 14300
max_len = 1024
maxlen = max_len
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
num_samples = INPUT_IDS.shape[0]

# Sá»‘ lÆ°á»£ng máº«u cho X_valid (20% cá»§a X_train)
num_valid_samples = int(num_samples * 0.2)

# XÃ¡o trá»™n cÃ¡c chá»‰ sá»‘ cá»§a X_train
indices = np.random.permutation(num_samples)

# Chá»n 20% chá»‰ sá»‘ Ä‘áº§u tiÃªn lÃ m chá»‰ sá»‘ cho X_valid
valid_indices = indices[:num_valid_samples]

# CÃ¡c chá»‰ sá»‘ cÃ²n láº¡i lÃ m chá»‰ sá»‘ cho X_train
train_indices = indices[num_valid_samples:]

# Táº¡o X_valid vÃ  X_train má»›i tá»« cÃ¡c chá»‰ sá»‘ Ä‘Ã£ chá»n
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=max_len)
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=max_len)
y_train = LABELS[train_indices]

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=max_len)
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=max_len)
y_valid = LABELS[valid_indices]
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
from tensorflow.keras.layers import Layer
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D
from keras.layers import  GlobalMaxPooling1D
import tensorflow as tf

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        embeddings, attention_mask = inputs
        return embeddings * tf.expand_dims(attention_mask, -1)

input_layer = Input(shape=(max_len,),dtype='int32', name='main_input')
attention_masks = Input(shape=(max_len,), dtype='float32', name="attention_masks")

emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=max_len
                      )(input_layer)

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])

# LSTM branch (with Batch Normalization and Dropout)
lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)
lstm_out = BatchNormalization()(lstm_out) # Batch Normalization helps to normalize activations and speed up convergence
lstm_out = Dropout(0.5)(lstm_out) # Dropout = 0.5 helps to prevent overfitting
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)
lstm_out = LSTM(32)(lstm_out)
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)

# CNN layer branch (with Batch Normalization and Dropout)
cnn_out = Conv1D(64, 5, activation='relu')(masked_embeddings)
cnn_out = BatchNormalization()(cnn_out)
cnn_out = Dropout(0.5)(cnn_out)
cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)
cnn_out = BatchNormalization()(cnn_out)
cnn_out = Dropout(0.5)(cnn_out)
cnn_out = GlobalMaxPooling1D()(cnn_out)


# Concatenate LSTM and CNN outputs
merged = concatenate([lstm_out, cnn_out])
merged = Dense(32, activation='sigmoid')(merged)
merged = BatchNormalization()(merged)
merged = Dropout(0.5)(merged)
pred = Dense(3, activation='softmax')(merged)


# Build model
model = Model(inputs=[input_layer, attention_masks], outputs=[pred])
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
adamax = Adamax(learning_rate=0.001)
model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Training
```

---The following area is a Code cell (cell numver is 21)---
```python
def clip_indices(data, max_index):
    return np.where(data >= max_index, max_index - 1, data)
X_train = clip_indices(X_train, 14300)
X_train_attention = clip_indices(X_train_attention, 14300)
X_valid = clip_indices(X_valid, 14300)
X_valid_attention = clip_indices(X_valid_attention, 14300)
```

---The following area is a Code cell (cell numver is 22)---
```python
from keras.callbacks import EarlyStopping
checkpoint = ModelCheckpoint('/kaggle/working/model.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

model.fit([X_train,X_train_attention], y_train,
          batch_size=16,
          epochs=nb_epoch,
#           callbacks=[checkpoint, early_stopping],
          callbacks=[early_stopping],
          validation_data=([X_valid,X_valid_attention], y_valid))
```

---The following area is a Code cell (cell numver is 23)---
```python
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])

model.save('model.keras', overwrite=True)
model.save_weights("model.weights.h5", overwrite=True)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## Test Model
```

---The following area is a Code cell (cell numver is 25)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 26)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 27)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A:\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 28)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 29)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=max_len)
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=max_len)

```

---The following area is a Code cell (cell numver is 30)---
```python
test
```

---The following area is a Code cell (cell numver is 31)---
```python
y_predict = model.predict([X_test,X_test_attention])
y_predict
```

---The following area is a Code cell (cell numver is 32)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)

```

---The following area is a Code cell (cell numver is 33)---
```python
result_df.to_csv('submission.csv', index=False)
result_df
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
____________________
```

---The following area is a Code cell (cell numver is 35)---
```python
# # Import necessary libraries
# import pandas as pd
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import log_loss
# from sklearn.preprocessing import LabelEncoder

# # Load the data
# train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
# test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# # Inspect the data
# print(train.head())
# print(test.head())

# # Data Preprocessing
# # Combine responses into one text feature
# train['response_combined'] = train['response_a'] + ' ' + train['response_b']
# test['response_combined'] = test['response_a'] + ' ' + test['response_b']

# # Encode the target labels
# label_encoder = LabelEncoder()
# train['winner'] = label_encoder.fit_transform(train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1))

# # Feature Engineering
# # Vectorize the combined responses using TF-IDF
# tfidf = TfidfVectorizer(max_features=1000)
# X_train_tfidf = tfidf.fit_transform(train['response_combined'])
# X_test_tfidf = tfidf.transform(test['response_combined'])

# # Prepare the data for model training
# X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, train['winner'], test_size=0.2, random_state=42)

# # Train the model
# model = RandomForestClassifier(n_estimators=100, random_state=42)
# model.fit(X_train, y_train)

# # Validate the model
# y_val_pred_proba = model.predict_proba(X_val)
# val_log_loss = log_loss(y_val, y_val_pred_proba)
# print(f'Validation Log Loss: {val_log_loss}')

# # Predict on the test set
# test_pred_proba = model.predict_proba(X_test_tfidf)

# # Prepare the submission file
# submission = pd.DataFrame(test['id'], columns=['id'])
# submission['winner_model_a'] = test_pred_proba[:, label_encoder.transform(['winner_model_a'])]
# submission['winner_model_b'] = test_pred_proba[:, label_encoder.transform(['winner_model_b'])]
# submission['winner_tie'] = test_pred_proba[:, label_encoder.transform(['winner_tie'])]
# submission.to_csv('submission.csv', index=False)

# # Inspect the submission file
# print(submission.head())
```

** @@@ Jupyter Notebook numver 77, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 7)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 9)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 10)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 11)---
```python
model = Llama3ForSFT.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 13)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 15)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

---The following area is a Code cell (cell numver is 16)---
```python
model.save_pretrained('pretrained_model')
```

---The following area is a Code cell (cell numver is 17)---
```python
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from peft import LoraConfig, get_peft_model, TaskType

# # ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  file Lora adapter
# model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
# lora_adapter_path = "/kaggle/input/model-1"

# # Táº£i mÃ´ hÃ¬nh gá»‘c
# model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# # Táº£i tokenizer tÆ°Æ¡ng á»©ng
# tokenizer = AutoTokenizer.from_pretrained(model_path)

# # Cáº¥u hÃ¬nh Lora
# lora_config = LoraConfig(
#     r=8,            # Rank cá»§a Lora
#     lora_alpha=16,  # Há»‡ sá»‘ khuáº¿ch Ä‘áº¡i cá»§a Lora
#     task_type=TaskType.CAUSAL_LM  # Loáº¡i nhiá»‡m vá»¥ cá»§a mÃ´ hÃ¬nh
# )

# # Chuáº©n bá»‹ mÃ´ hÃ¬nh cho k-bit training náº¿u cáº§n
# model_1 = prepare_model_for_kbit_training(model_1)

# # Ãp dá»¥ng Lora Adapter vÃ o mÃ´ hÃ¬nh
# model_1 = get_peft_model(model_1, lora_config)

# # Táº£i cÃ¡c tham sá»‘ cá»§a Lora Adapter Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
# model_1.load_adapter(lora_adapter_path, adapter_name="test")

# # MÃ´ hÃ¬nh hoÃ n chá»‰nh Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng
# model_1.eval()  # Äáº·t mÃ´ hÃ¬nh vÃ o cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ náº¿u cáº§n

# # Tokenize má»™t cÃ¢u vÃ­ dá»¥ vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o vÄƒn báº£n
# sentence = "Hello, how are you?"
# inputs = tokenizer(sentence, return_tensors="pt")
# outputs = model_1.generate(**inputs)

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---The following area is a Code cell (cell numver is 18)---
```python
# !zip -r model_2.zip /kaggle/working/saved_model_2
```

** @@@ Jupyter Notebook numver 78, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
```

---The following area is a Code cell (cell numver is 1)---
```python
# Load the data
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Check the column names
print("Train Data Columns:", train_df.columns)
print("\nTest Data Columns:", test_df.columns)
```

---The following area is a Code cell (cell numver is 3)---
```python
# Use a smaller subset of data
train_df = train_df.head(2000)
test_df = test_df.head(2000)
```

---The following area is a Code cell (cell numver is 4)---
```python
# Combine the winner columns into a single target column
train_df['winner'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 5)---
```python
# Combine all text data for tokenization
all_text = pd.concat([train_df['prompt'], train_df['response_a'], train_df['response_b'],
                      test_df['prompt'], test_df['response_a'], test_df['response_b']])
```

---The following area is a Code cell (cell numver is 6)---
```python
# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_text)
vocab_size = len(tokenizer.word_index) + 1
```

---The following area is a Code cell (cell numver is 7)---
```python
def tokenize_and_pad(text_series, tokenizer, max_len):
    sequences = tokenizer.texts_to_sequences(text_series)
    padded_sequences = pad_sequences(sequences,maxlen=max_len)
    return padded_sequences
```

---The following area is a Code cell (cell numver is 8)---
```python
# Define maximum sequence length
max_len = 100
```

---The following area is a Code cell (cell numver is 9)---
```python
# Tokenize and pad the training data
x_prompt = tokenize_and_pad(train_df['prompt'],tokenizer, max_len)
x_response_a = tokenize_and_pad(train_df['response_a'],tokenizer, max_len)
x_response_b = tokenize_and_pad(train_df['response_b'],tokenizer, max_len)
x_train = np.concatenate([x_prompt,x_response_a,x_response_b],axis=1)
```

---The following area is a Code cell (cell numver is 10)---
```python
# Encode the target variable
y_train = to_categorical(train_df['winner'])
```

---The following area is a Code cell (cell numver is 11)---
```python
# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Build the Keras model
model = Sequential()
model.add(Embedding(vocab_size,128))
model.add(LSTM(64,return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(64))
model.add(Dense(3,activation='softmax'))
```

---The following area is a Code cell (cell numver is 13)---
```python
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))
```

---The following area is a Code cell (cell numver is 15)---
```python
# Create a figure and a set of subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

# Plot training & validation loss values
ax1.plot(history.history['loss'], label='Train')
ax1.plot(history.history['val_loss'], label='Validation')
ax1.set_title('Model Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend(loc='upper right')

# Plot training & validation accuracy values
ax2.plot(history.history['accuracy'], label='Train')
ax2.plot(history.history['val_accuracy'], label='Validation')
ax2.set_title('Model Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend(loc='upper left')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()
```

---The following area is a Code cell (cell numver is 16)---
```python
# Prepare the test data
x_test_prompt = tokenize_and_pad(test_df['prompt'],tokenizer, max_len)
x_test_response_a = tokenize_and_pad(test_df['response_a'],tokenizer, max_len)
x_test_response_b = tokenize_and_pad(test_df['response_b'],tokenizer, max_len)
x_test = np.concatenate([x_test_prompt,x_test_response_a,x_test_response_b],axis=1)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Predict on the test data
y_test_pred = model.predict(x_test)
y_test_pred_labels = np.argmax(y_test_pred,axis=1)
```

---The following area is a Code cell (cell numver is 18)---
```python
# Convert predictions to binary columns
test_df['winner_model_a'] = (y_test_pred_labels == 0).astype(float)
test_df['winner_model_b'] = (y_test_pred_labels == 1).astype(float)
test_df['winner_tie'] = (y_test_pred_labels == 2).astype(float)
```

---The following area is a Code cell (cell numver is 19)---
```python
# Save the predictions to submission.csv
submission_df = test_df[['id','winner_model_a','winner_model_b','winner_tie']]
submission_df.to_csv('submission.csv',index=False)
```

** @@@ Jupyter Notebook numver 79, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
data=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# Load data

# Preprocess data
# Create labels
data['winner'] = data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)

# Encode categorical features
label_encoder = LabelEncoder()
data['model_a'] = label_encoder.fit_transform(data['model_a'])
data['model_b'] = label_encoder.fit_transform(data['model_b'])

# Feature and target split
X = data[['model_a', 'model_b', 'prompt', 'response_a', 'response_b']]
y = data['winner']

# Text vectorization (simple approach: count the number of words)
X['prompt_len'] = X['prompt'].apply(lambda x: len(str(x).split()))
X['response_a_len'] = X['response_a'].apply(lambda x: len(str(x).split()))
X['response_b_len'] = X['response_b'].apply(lambda x: len(str(x).split()))
X = X.drop(['prompt', 'response_a', 'response_b'], axis=1)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encode labels
y = to_categorical(y, num_classes=3)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(16, activation='relu'))
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Save the model
model.save('chatbot_preference_model.h5')

# Print validation loss during training
val_loss = history.history['val_loss']
print(f"Validation Loss: {val_loss[-1]:.4f}")

```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the test data
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

# Add default values for 'model_a' and 'model_b' as placeholders
test['model_a'] = 'gpt-3.5-turbo-0613'  # Example default value
test['model_b'] = 'gpt-3.5-turbo-0613'  # Example default value

# Encode categorical features using the same label encoder used for training data
label_encoder = LabelEncoder()
test['model_a'] = label_encoder.fit_transform(test['model_a'])
test['model_b'] = label_encoder.fit_transform(test['model_b'])

# Text vectorization (simple approach: count the number of words)
test['prompt_len'] = test['prompt'].apply(lambda x: len(str(x).split()))
test['response_a_len'] = test['response_a'].apply(lambda x: len(str(x).split()))
test['response_b_len'] = test['response_b'].apply(lambda x: len(str(x).split()))

# Drop original text columns
X_test = test.drop(['prompt', 'response_a', 'response_b'], axis=1)

# Normalize features using the same scaler used for training data
# Assuming the scaler was fit during training and is now loaded
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test[['model_a', 'model_b', 'prompt_len', 'response_a_len', 'response_b_len']])

# Load the trained model
model = load_model('chatbot_preference_model.h5')

# Make predictions
probs = model.predict(X_test_scaled)

# Prepare the submission file
submission = pd.DataFrame(probs, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission['id'] = test['id']

# Reorder columns to have 'id' first
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

submission.to_csv('submission.csv', index=False)
print("Submission file generated successfully!")

```

---The following area is a Code cell (cell numver is 4)---
```python
print(submission)
```

** @@@ Jupyter Notebook numver 80, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/libs-install
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss

from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Code cell (cell numver is 2)---
```python
train_df = pd.read_csv('/kaggle/input/embedding/train_embed.csv')
train_embed = np.load('/kaggle/input/embedding/gemma2_train_embed.npy')

train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 3)---
```python
# splits
Targets = ['winner_model_a','winner_model_b','winner_tie']

y = train_df['label'].values
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
```

---The following area is a Code cell (cell numver is 4)---
```python
model_cb = CatBoostClassifier()
model_cb.load_model('/kaggle/input/catboost-mike/catboost.cbm')
```

---The following area is a Code cell (cell numver is 5)---
```python
model_cb
```

---The following area is a Code cell (cell numver is 6)---
```python
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
MAX_LENGTH = 1024
BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)
```

---The following area is a Code cell (cell numver is 7)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'
print(test['text'][0])
```

---The following area is a Code cell (cell numver is 8)---
```python
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',
                   max_length=MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')


data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in  tokens['attention_mask']]
data[:2]
```

---The following area is a Code cell (cell numver is 9)---
```python
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 10)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 11)---
```python
test_embeddings = torch.cat([results[0], results[1]], dim=0)
test_embeddings.shape
```

---The following area is a Code cell (cell numver is 12)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Code cell (cell numver is 13)---
```python
preds = model_cb.predict_proba(test_embeddings.numpy())
preds
```

---The following area is a Code cell (cell numver is 14)---
```python
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
sample_sub[Targets] =  preds

display(sample_sub)
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 16)---
```python

```

** @@@ Jupyter Notebook numver 81, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python

```

---The following area is a Code cell (cell numver is 1)---
```python
pip install transformers datasets
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
import torch.nn as nn
from tqdm import tqdm
from sklearn.metrics import log_loss

# Define the SiameseLSTM class
class SiameseLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(SiameseLSTM, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 3)  # Output size 3 for 3 classes: model A wins, model B wins, tie

    def forward_one(self, x):
        x = self.embedding(x)
        _, (h, _) = self.lstm(x)
        return h[-1]

    def forward(self, x1, x2):
        h1 = self.forward_one(x1)
        h2 = self.forward_one(x2)
        return self.fc(torch.abs(h1 - h2))

# Step 1: Load your training dataset
df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')

# Filter out invalid cases and prepare data
data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values

def determine_label(row):
    if row[2] == 1:
        return 0  # model A wins
    elif row[3] == 1:
        return 1  # model B wins
    elif row[4] == 1:
        return 2  # tie
    else:
        return -1  # Invalid or unclear case

labels = [determine_label(row) for row in data if determine_label(row) != -1]
data = [row[:2] for row in data if determine_label(row) != -1]

# Step 2: Define a custom Dataset class
class SiameseDataset(Dataset):
    def __init__(self, data, labels, tokenizer, max_length):
        self.data = data
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]
        label = self.labels[idx]

        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),
            'input_ids_b': torch.tensor(tokens_b['input_ids']),
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Step 3: Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')
max_length = 128  # Adjust according to your dataset

# Step 4: Create instances of Dataset and DataLoader for training
train_dataset = SiameseDataset(data, labels, tokenizer, max_length)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Step 5: Define the Siamese network model
input_size = len(tokenizer)
hidden_size = 300
num_layers = 1
model = SiameseLSTM(input_size, hidden_size, num_layers)

# Step 6: Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Step 7: Training loop
num_epochs = 5  # Adjust as needed
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids_a, input_ids_b)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')

# Load your test dataset
df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')

# Define a custom Dataset class for testing
class SiameseTestDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]

        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),
            'input_ids_b': torch.tensor(tokens_b['input_ids']),
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),
        }

# Prepare test data
test_data = df_test[['response_a', 'response_b']].values.tolist()

# Create instance of SiameseTestDataset
test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)

# Create DataLoader for the test dataset
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the saved model
model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))
model.eval()  # Set model to evaluation mode

# Perform inference on the test data and generate predictions
all_preds = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc='Testing'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)

        outputs = model(input_ids_a, input_ids_b)
        probabilities = nn.Softmax(dim=1)(outputs)
        all_preds.extend(probabilities.cpu().numpy().tolist())

# Create a DataFrame for predictions
pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
pred_df['id'] = df_test['id']

# Reorder columns to match the required format
pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

# Save predictions to CSV for submission
pred_df.to_csv('submission.csv', index=False)
print(pred_df.head())

```

---The following area is a Code cell (cell numver is 3)---
```python

```

---The following area is a Code cell (cell numver is 4)---
```python

```

** @@@ Jupyter Notebook numver 82, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 83, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
#  Llama-3 8b [TPU Train]

Learning to train llms on tpu, Hope this will help you too!

Notebook inspired from:

* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)


Prerequisite: Access to using llama-3

Note: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)

Please upvote if you learn or find this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
# Install libs
!pip install -qq peft==0.6.0
!pip install -qq bitsandbytes==0.41.1
!pip install -qq accelerate==0.24.1
!pip install -qq transformers==4.35.0
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q 
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q
!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT
!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import torch.nn.functional as F

import torch_xla.debug.profiler as xp
import torch_xla.core.xla_model as xm
import torch_xla.experimental.xla_sharding as xs
import torch_xla.runtime as xr

xr.use_spmd()

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh
from spmd_util import partition_module

tqdm.pandas()

print(f'Torch Version: {torch.__version__}')
```

---The following area is a Code cell (cell numver is 4)---
```python
# import os
# import gc
# import re
# from time import time
# import random
# import warnings
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from tqdm.auto import tqdm

# import torch
# import transformers
# from sklearn.metrics import accuracy_score
# from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
# from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# import torch.nn.functional as F

# import torch_xla.debug.profiler as xp
# import torch_xla.core.xla_model as xm
# import torch_xla.experimental.xla_sharding as xs
# import torch_xla.runtime as xr

# xr.use_spmd()

# from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
# from torch_xla.experimental.xla_sharding import Mesh
# from spmd_util import partition_module

# tqdm.pandas()

# print(f'Torch Version: {torch.__version__}')
# /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
#   from .autonotebook import tqdm as notebook_tqdm
# /usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
#   warn("The installed version of bitsandbytes was compiled without GPU support. "
# /usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
# Torch Version: 2.1.2+cpu

# # Configs
# class CFG:
#     NUM_EPOCHS = 1
#     BATCH_SIZE = 1  # It was 16 before
#     DROPOUT = 0.05 
#     MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
#     SEED = 2024 
#     MAX_LENGTH = 1024 
#     NUM_WARMUP_STEPS = 128
#     LR_MAX = 5e-5 
#     NUM_LABELS = 3 
#     LORA_RANK = 4
#     LORA_ALPHA = 8
#     LORA_MODULES = ['o_proj', 'v_proj']
    
# DEVICE = xm.xla_device()  # Initialize TPU Device
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 1 # It was 16 before
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
    
DEVICE = xm.xla_device() # Initialize TPU Device
```

---The following area is a Code cell (cell numver is 7)---
```python
def set_seeds(seed):
    """Set seeds for reproducibility """
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
    # Set seed for all TPU cores
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 10)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 12)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 13)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 14)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 15)---
```python
# Train with only take 50% train dataset
# train = train[:int(len(train) * 0.5)]
train = train[int(len(train) * 0.5):]

# Tsplit to two halves
first_half = train[:int(len(train) * 0.5)]

first_half.loc[:, 'token_count'] = get_token_lengths(first_half['text'])
first_half.loc[:, 'label'] = np.argmax(first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)


train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model and get the max index
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 16)---
```python

```

---The following area is a Code cell (cell numver is 17)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 18)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 19)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 21)---
```python
# # Tokenize Data
# tokens = tokenizer(
#     train['text'].tolist(), 
#     padding='max_length', 
#     max_length=CFG.MAX_LENGTH, 
#     truncation=True, 
#     return_tensors='np')

# # Input IDs are the token IDs
# INPUT_IDS = tokens['input_ids']
# # Attention Masks to Ignore Padding Tokens
# ATTENTION_MASKS = tokens['attention_mask']
# # Label of Texts
# LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

# print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
# print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 22)---
```python
tokens = tokenizer(
    first_half['text'].tolist(), 
    padding='max_length', 
    max_length=CFG.MAX_LENGTH, 
    truncation=True, 
    return_tensors='np'
)

INPUT_IDS = tokens['input_ids']
ATTENTION_MASKS = tokens['attention_mask']
LABELS = first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values
print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 23)---
```python
def train_dataset(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        # Shuffle Indices
        np.random.shuffle(IDXS)
        # Iterate Over All Indices Once
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output
            
            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)
```

---The following area is a Code cell (cell numver is 24)---
```python
# Define dataset generator for the first half
def train_dataset_first_half(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        np.random.shuffle(IDXS)
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset_first_half(CFG.BATCH_SIZE)
# Calculate STEPS_PER_EPOCH for the first half
STEPS_PER_EPOCH = len(first_half) // CFG.BATCH_SIZE


```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# Load Model
```

---The following area is a Code cell (cell numver is 26)---
```python
# Load model for classification with 3 target label
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    num_labels=CFG.NUM_LABELS,
    torch_dtype=torch.bfloat16)

base_model.config.pretraining_tp = 1 

# Assign Padding TOKEN
base_model.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# Low-Rank Adaptation [LORA]
```

---The following area is a Code cell (cell numver is 29)---
```python
lora_config = LoraConfig(
    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices
    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations
    lora_dropout= CFG.DROPOUT, 
    bias='none',
    inference_mode=False,
    task_type=TaskType.SEQ_CLS,
    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection
```

---The following area is a Code cell (cell numver is 30)---
```python
# Create LoRa Model
model = get_peft_model(base_model, lora_config)
# Trainable Parameters
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 31)---
```python
# Number of TPU Nodes
num_devices = xr.global_runtime_device_count()
mesh_shape = (1, num_devices, 1)
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))
# distribute model
partition_module(model, mesh)

print(f'num_devices: {num_devices}')
```

---The following area is a Code cell (cell numver is 32)---
```python
# Verfy The Trainable Layers
MODEL_LAYERS_ROWS = []
TRAINABLE_PARAMS = []
N_TRAINABLE_PARAMS = 0

for name, param in model.named_parameters():
    # Layer Parameter Count
    n_parameters = int(torch.prod(torch.tensor(param.shape)))
    # Only Trainable Layers
    if param.requires_grad:
        # Add Layer Information
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,
            'name': name,
            'dtype': param.data.dtype,
        })
        # Append Trainable Parameter
        TRAINABLE_PARAMS.append({ 'params': param })
        # Add Number Of Trainable Parameters"
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))

print(f"""
===============================
N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}
N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}
===============================
""")
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 34)---
```python
# LR & Optimizer
N_SAMPLES = len(train)
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)

# Cosine Learning Rate With Warmup
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)

print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')
```

---The following area is a Code cell (cell numver is 35)---
```python
# Set the data type for the optimizer's state (e.g., momentum buffers)
for state in OPTIMIZER.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:
            state[v] = v.to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 36)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)

print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')
print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')
print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')
```

---The following area is a Code cell (cell numver is 37)---
```python
%%time
# Dummy Prediction
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    
print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')
```

---The following area is a Code cell (cell numver is 38)---
```python
# Put Model In Train Mode
model.train()

# Loss Function, Cross Entropy
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 39)---
```python
st = time()
warnings.filterwarnings("error")
METRICS = {
    'loss': [],
    'accuracy': {'y_true': [], 'y_pred': [] }}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):
    ste = time()
    for step in range(STEPS_PER_EPOCH):
        # Zero Out Gradients
        OPTIMIZER.zero_grad()
        
        # Get Batch
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # Forward Pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # Logits Float32
        logits = outputs.logits.to(dtype=torch.float32)
        
        # Backward Pass
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))
        loss.backward()
        
        # optimizer step
        OPTIMIZER.step()
        xm.mark_step()
        
        # Update Learning Rate Scheduler
        lr_scheduler.step()
        
        # Update Metrics And Progress Bar
        METRICS['loss'].append(float(loss))
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()
        
        if (step + 1) % 200 == 0:  
            metrics = 'Âµ_loss: {:.3f}'.format(np.mean(METRICS['loss']))
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])
            metrics += ', Âµ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))
            lr = OPTIMIZER.param_groups[0]['lr']
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')
    
    print(f'\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )

    # If stopped, and to continue training in future on tpu we save model and optimizer
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    
    
    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')
```

---The following area is a Code cell (cell numver is 40)---
```python
plt.figure(figsize=(15, 6))
plt.plot(METRICS['loss'])    
plt.xlabel('Step per epoch')
plt.ylabel('Loss')
plt.title('Loss Plot step per epoch')    
plt.show()
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# Save Model
```

---The following area is a Code cell (cell numver is 42)---
```python
model = model.cpu()
torch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 84, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
from itertools import zip_longest
from  tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from transformers import BertTokenizer, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
import  pandas as pd
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


def process(example):
    sentences = [s.strip('"').replace('[MASK]','') for s in example['prompt'].strip('[]').split('","')]
    sentences_a = [s.strip('"').replace('[MASK]','') for s in example['response_a'].strip('[]').split('","')]
    sentences_b = [s.strip('"').replace('[MASK]','') for s in example['response_b'].strip('[]').split('","')]
    texts_a = [p for pair in zip_longest(sentences, sentences_a, fillvalue='') for p in pair if p]
    texts_b = [p for pair in zip_longest(sentences, sentences_b, fillvalue='') for p in pair if p]
    return pd.Series([' '.join(sentences), ' '.join(sentences_a), ' '.join(sentences_b), '\n'.join(texts_a), '\n'.join(texts_b)], index=['prompt', 'response_a', 'response_b', 'text_a','text_b'])

test[['prompt', 'response_a', 'response_b', 'text_a','text_b']] = test.apply(process, axis=1)
test = test.dropna()


# pet
class Senmamtic_news(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
        self.sep_token = tokenizer.sep_token
        self.mask_token = tokenizer.mask_token

        prompt_new = []
        tk0 = tqdm(data['prompt'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            prompt_new.append(text)
        # response_a
        print(f'== response_a ==')
        response_a_new = []
        tk0 = tqdm(data['response_a'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_a_new.append(text)

        # response_b
        print(f'== response_b ==')
        response_b_new = []
        tk0 = tqdm(data['response_b'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_b_new.append(text)
        self.data['prompt'] = prompt_new
        self.data['response_a'] = response_a_new
        self.data['response_b'] = response_b_new


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # text, label = self.data[idx]
#         semantic_label = self.data['semantic_labels'][idx]
        prompt = self.data['prompt'][idx]
        response_a = self.data['response_a'][idx]
        response_b = self.data['response_b'][idx]
        system_prompt = f"""{prompt}:
                        Response A: {response_a}
                        Response B: {response_b}
                        Which is better? Choose 'A', 'B', or 'both'.
""" +self.mask_token


        inputs = self.tokenizer(system_prompt, truncation=True, max_length=1600)
#         semantic_label = self.tokenizer(semantic_label, truncation=True, max_length=2,add_special_tokens=False)['input_ids']
        input_ids  = torch.tensor(inputs['input_ids'], dtype=torch.long)
        mask_index = torch.where(input_ids == self.tokenizer.mask_token_id)[0]

        return {
            'input_ids': input_ids,
            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
#             'semantic_label': torch.tensor(semantic_label, dtype=torch.long),
            # 'semantic_label': torch.tensor([-100]*(len(inputs['input_ids'])-3)+semantic_label+[-100], dtype=torch.long),
            'mask_index': mask_index
        }
def collate_fn_semantic(batch):
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
#     semantic_label = [item['semantic_label'] for item in batch]
    mask_index = [item['mask_index'] for item in batch]

    input_ids = pad_sequence(input_ids, batch_first=True)
    attention_mask = pad_sequence(attention_mask, batch_first=True,)
#     semantic_label = pad_sequence(semantic_label, batch_first=True,padding_value=-100)
    mask_index = torch.stack(mask_index)
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
#         'semantic_label': semantic_label,
        'mask_index': mask_index
    }
def get_semantic_data_loader(batch_size=32, mode='train',shuffle=True):
    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/deberta-small/deberta')
    if mode == 'train':
        dataset = Senmamtic_news(train, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,collate_fn=collate_fn_semantic)
        return dataloader
    else:
        dataset = Senmamtic_news(test, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,collate_fn=collate_fn_semantic,drop_last = False)
        return dataloader
import torch
from torch import nn
from transformers import AutoModelForMaskedLM, AutoConfig
from tqdm import tqdm

# å®šä¹‰æ•°æ®åŠ è½½å™¨å‡½æ•°å’Œç´¢å¼•å­—å…¸
# from get_dataloader import get_semantic_data_loader


# å®šä¹‰æ¨¡åž‹ç±»
class BertPET(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        self.bert = AutoModelForMaskedLM.from_pretrained(model_path)
        self.config = AutoConfig.from_pretrained(model_path)
    # {"336": 0,
    #             "736": 1,
    #             "462": 2 }
    def forward(self, input_ids, attention_mask, mask_index, labels=None, return_logits=False):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits
        mask_index = mask_index.unsqueeze(-1).expand(-1, -1, output.size(-1))
        output = torch.gather(output, 1, mask_index).squeeze(1)
        if return_logits:
            # è¿”å›žç‰¹å®šç´¢å¼•çš„ logits
            logits = output[:,[336, 732, 462]]
            return logits

# æŽ¨ç†å‡½æ•°
def inference(model, dataloader, device):
    model.eval()
    softmax = nn.Softmax(dim=-1)
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Inference", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            mask_index = batch['mask_index'].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask, mask_index=mask_index, return_logits=True)
#             logits = logits.squeeze(1)

            probs = softmax(logits)
            probs_list = probs.cpu().numpy().tolist()
            for i in probs_list:
                all_probs.append(i)
        

    return all_probs

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½æ¨¡åž‹å’Œæƒé‡
model_path = '/kaggle/input/deberta-small/deberta'  # é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„
checkpoint_path = '/kaggle/input/deberta-v3/model_checkpoint_epoch_2.pt'  # æœ€ä½³æƒé‡æ–‡ä»¶è·¯å¾„
model = BertPET(model_path)
model.load_state_dict(torch.load(checkpoint_path, map_location=device))
model.to(device)

# èŽ·å–æµ‹è¯•æ•°æ®
test_loader = get_semantic_data_loader(2, mode='test')

# è¿›è¡ŒæŽ¨ç†
test_probs = inference(model, test_loader, device)


sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

test[['winner_model_a', 'winner_model_b', 'winner_tie']] = test_probs
sample_sub = sample_sub[['id']].merge(test[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']], on='id', how='left')
sample_sub.to_csv('submission.csv', index=False)
display(sample_sub.head())
```

** @@@ Jupyter Notebook numver 85, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
import torch.nn.functional as F
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2')

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True
# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 7)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 8)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 9)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 1)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 21540#14300
maxlen = 1024
batch_size = 16
embedding_dims = 200
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 14
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
X_train = sq.pad_sequences(INPUT_IDS, maxlen=maxlen)
y_train = LABELS
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
'''This example demonstrates the use of Convolution1D for text classification.
Gets to 0.88 test accuracy after 2 epochs.
90s/epoch on Intel i5 2.4Ghz CPU.
10s/epoch on Tesla K40 GPU.
'''
from keras.layers import Concatenate
from keras.layers import  GlobalMaxPooling1D

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})


# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)



model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int64', name='main_input')
emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)
def max_1d(X):
    return K.max(X, axis=1)

# we add a Convolution1D, which will learn nb_filter
# word group filters of size 3:

con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size =3,
                    strides=1)(emb_layer)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 4:

con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 5:

con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)


cnn_layer =Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])


#LSTM


x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)
output_layer= Dropout(0.2)(dense_layer)
output_layer = Dense(3, trainable=True,activation='softmax')(output_layer)




model = Model(inputs=[input_layer], outputs=[output_layer])
adadelta = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-06)

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])
model.summary()

```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 21)---
```python
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
model.fit(X_train, y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint])

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])
```

---The following area is a Code cell (cell numver is 22)---
```python
model.save('model_LSTM_mix_CNN.keras')  # LÆ°u toÃ n bá»™ model
```

---The following area is a Code cell (cell numver is 23)---
```python
model.predict(X_train)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# Test Model
```

---The following area is a Code cell (cell numver is 25)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 26)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 27)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 28)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 29)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)
```

---The following area is a Code cell (cell numver is 30)---
```python
test
```

---The following area is a Code cell (cell numver is 31)---
```python
y_predict = model.predict(X_test)
y_predict
```

---The following area is a Code cell (cell numver is 32)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)
```

---The following area is a Code cell (cell numver is 33)---
```python
result_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 34)---
```python
result_df
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 86, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is
This notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).
I used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.

## Result

I used `id % 5 == 0` as an evaluation set and used all the rest for training.

| subset | log loss |
| - | - |
| eval | 0.9371|
| LB | 0.941 |

## What is QLoRA fine-tuning?

In the conventional fine-tuning, weight ($\mathbf{W}$) is updated as follows:

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

where $L$ is a loss at this step and $\eta$ is a learning rate.

[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$ by factorizing $\Delta \mathbf{W}$ into two (much) smaller matrices, $\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$ and $\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$ with $r \ll \text{min}(\text{d}, \text{k})$.

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

During training, only $\mathbf{A}$ and $\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.

[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. 
Note that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.

1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.

## Note
It takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.
This notebook uses only 100 samples for demo purpose, but everything else is same as my setup.
```

---The following area is a Code cell (cell numver is 1)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass
# å¯¼å…¥æ‰€éœ€åº“
from torch.utils.tensorboard import SummaryWriter
import psutil  # ç”¨äºŽèŽ·å–ç³»ç»Ÿå’Œè¿›ç¨‹çš„èµ„æºä½¿ç”¨ä¿¡æ¯
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 3)---
```python
# !pip install --upgrade kaggle
# #è¦å…ˆè¿™ä¹ˆå¯åŠ¨ä¸‹æ‰èƒ½åˆ›å»º
# !kaggle datasets list
```

---The following area is a Code cell (cell numver is 4)---
```python
# import json

# token = {
#     "username": "qinhaoyang",
#     "key": "02c6cdf132dabb5ddd9de0d37d8a7777"
# }

# with open('/root/.kaggle/kaggle.json', 'w') as file:
#     json.dump(token, file)
# with open('/kaggle/working/kaggle.json', 'w') as file:
#     json.dump(token, file)
```

---The following area is a Code cell (cell numver is 5)---
```python
# !chmod 600 /root/.kaggle/kaggle.json

# # å®šä¹‰ JSON æ•°æ®
# data ={
#   "title": "LMSYS-model", #//æ•°æ®é›†æ ‡é¢˜
#   "subtitle": "",
#   "description": "",
#   "id": "qinhaoyang/LMSYS-model",
#   "licenses": [
#         {
#             "name": "unknown"
#         }
#     ],
#     "keywords": [],
#     "collaborators": [],
#     "data": []
# }


# with open('/kaggle/working/dataset-metadata.json', 'w') as file:
#     json.dump(data, file)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 7)---
```python
#!kaggle kernels output qinhaoyang/training-gemma-2-9b-4-bit-qlora-fine-tuning -p /kaggle/working
```

---The following area is a Code cell (cell numver is 8)---
```python
#!kaggle datasets version -p /kaggle/working/output/ -m "Description of the dataset changes" --dir-mode tar
```

---The following area is a Code cell (cell numver is 9)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 1  # global batch size is 8 
    per_device_eval_batch_size: int = 4
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 11)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 13)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 15)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 16)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 17)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 19)---
```python
#ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
ds = Dataset.from_csv("/kaggle/input/lmsys-72k-dataset/lmsys-7.2k.csv")
#ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 20)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 21)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 23)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 25)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 26)---
```python
# åŠ è½½ checkpoint
checkpoint = "/kaggle/input/lmsys-gemma-checkpoint/output/checkpoint-10200"
```

---The following area is a Code cell (cell numver is 27)---
```python
#%tensorboard --logdir=/kaggle/working
```

---The following area is a Code cell (cell numver is 28)---
```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®°å½•å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶å†™å…¥TensorBoard
def log_memory_usage(step, writer):  # step æ˜¯è®°å½•çš„æ­¥éª¤æ ‡è¯†ï¼Œwriter æ˜¯SummaryWriterå®žä¾‹
    # èŽ·å–å½“å‰è¿›ç¨‹
    process = psutil.Process(os.getpid())
    # èŽ·å–è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯
    mem_info = process.memory_info()
    # å°†RSSï¼ˆå¸¸é©»é›†å¤§å°ï¼‰å’ŒVMSï¼ˆè™šæ‹Ÿå†…å­˜å¤§å°ï¼‰è½¬æ¢ä¸ºMBï¼Œå¹¶è®°å½•åˆ°TensorBoard
    writer.add_scalar('Memory Usage/RSS (MB)', mem_info.rss / (1024 * 1024), step)  # ç‰©ç†å†…å­˜ä½¿ç”¨
    writer.add_scalar('Memory Usage/VMS (MB)', mem_info.vms / (1024 * 1024), step)  # è™šæ‹Ÿå†…å­˜ä½¿ç”¨
    print(f"Memory usage logged at step {step}")
# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„TrainerCallbackå­ç±»æ¥åœ¨æ¯ä¸ªepochç»“æŸæ—¶è®°å½•å†…å­˜ä½¿ç”¨
class MemoryUsageLoggingCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):  # è¿™æ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¢«è°ƒç”¨çš„å›žè°ƒæ–¹æ³•
        # ä½¿ç”¨å½“å‰çš„epochæ•°ä½œä¸ºè®°å½•çš„æ­¥æ•°
        current_epoch = state.epoch
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°ï¼Œè®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        log_memory_usage(current_epoch, tb_writer)
    
    def on_log(self, args, state, control, **kwargs):
        logs = kwargs.get("logs", {})
        for key, value in logs.items():
            if isinstance(value, (int, float)):
                tb_writer.add_scalar(f"{key.capitalize()}", value, state.global_step)
                print(f"{key}: {value}")

        # é¢å¤–è®°å½•è®­ç»ƒå’Œè¯„ä¼°çš„æŸå¤±ä¸Žå‡†ç¡®çŽ‡
        if "loss" in logs:
            tb_writer.add_scalar("Loss/train", logs["loss"], state.global_step)
        if "eval_loss" in logs:
            tb_writer.add_scalar("Loss/eval", logs["eval_loss"], state.global_step)
        if "accuracy" in logs:
            tb_writer.add_scalar("Accuracy/train", logs["accuracy"], state.global_step)
        if "eval_accuracy" in logs:
            tb_writer.add_scalar("Accuracy/eval", logs["eval_accuracy"], state.global_step)

# åˆå§‹åŒ–SummaryWriter
tb_writer = SummaryWriter(log_dir="/kaggle/working/Gemma/tensorboard_logs")

# å®šä¹‰è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†
train_idx, eval_idx = folds[config.fold_idx]

# åˆå§‹åŒ–Traineræ—¶ï¼ŒåŒ…å«æ¨¡åž‹ã€æ•°æ®é›†ç­‰é…ç½®ï¼ŒåŒæ—¶åŠ å…¥è‡ªå®šä¹‰çš„å†…å­˜ä½¿ç”¨è®°å½•Callback
trainer = Trainer(
    args=training_args,  # è®­ç»ƒå‚æ•°
    model=model,  # è®­ç»ƒçš„æ¨¡åž‹
    tokenizer=tokenizer,  # åˆ†è¯å™¨
    train_dataset=ds.select(train_idx),  # è®­ç»ƒæ•°æ®é›†
    eval_dataset=ds.select(eval_idx),  # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,  # è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ–¹æ³•
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # æ•°æ®å¤„ç†
    callbacks=[MemoryUsageLoggingCallback()],  # æ·»åŠ è‡ªå®šä¹‰å›žè°ƒåˆ°Trainer
)

# æ£€æŸ¥æ˜¯å¦æœ‰ checkpoint æ¥ç»§ç»­è®­ç»ƒ
if checkpoint:
    trainer.train(resume_from_checkpoint=checkpoint)
else:
    trainer.train()

# è®­ç»ƒå®ŒæˆåŽï¼Œè®°å¾—å…³é—­SummaryWriterä»¥é‡Šæ”¾èµ„æº
tb_writer.close()
```

---The following area is a Code cell (cell numver is 29)---
```python
model.save_pretrained("/kaggle/working/Gemma") # æŒ‡å®šä¿å­˜æ¨¡åž‹çš„æœ¬åœ°è·¯å¾„
tokenizer.save_pretrained("/kaggle/working/Gemma") # å¦‚æžœéœ€è¦ï¼ŒåŒæ—¶ä¿å­˜tokenizer
```

---The following area is a Code cell (cell numver is 30)---
```python
# !kaggle datasets version -p /kaggle/working -m "Description of the dataset changes" --dir-mode tar
```

** @@@ Jupyter Notebook numver 87, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
train_file_path = '/kaggle/input/lmsysdataset/train.csv'
test_file_path = '/kaggle/input/lmsysdataset/test.csv'

train_df = pd.read_csv(train_file_path)
test_df = pd.read_csv(test_file_path)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
train_df['target'] = train_df.apply(lambda row: 1 if row['winner_model_a'] == 1 else 0, axis=1)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµåˆ
train_df['text_a'] = train_df['prompt'] + ' ' + train_df['response_a']
train_df['text_b'] = train_df['prompt'] + ' ' + train_df['response_b']
test_df['text_a'] = test_df['prompt'] + ' ' + test_df['response_a']
test_df['text_b'] = test_df['prompt'] + ' ' + test_df['response_b']

# TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ã®è¨­å®š
vectorizer = TfidfVectorizer(max_features=5000)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
X_a = vectorizer.fit_transform(train_df['text_a'])
X_b = vectorizer.fit_transform(train_df['text_b'])
X_test_a = vectorizer.transform(test_df['text_a'])
X_test_b = vectorizer.transform(test_df['text_b'])

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
X_train_a, X_valid_a, y_train, y_valid = train_test_split(X_a, train_df['target'], test_size=0.2, random_state=42)
X_train_b, X_valid_b, _, _ = train_test_split(X_b, train_df['target'], test_size=0.2, random_state=42)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›žå¸°ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
model_a = LogisticRegression(max_iter=1000)
model_b = LogisticRegression(max_iter=1000)

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
model_a.fit(X_train_a, y_train)
model_b.fit(X_train_b, y_train)

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
valid_preds_a = model_a.predict_proba(X_valid_a)[:, 1]
valid_preds_b = model_b.predict_proba(X_valid_b)[:, 1]

# ãƒ­ã‚°æå¤±ã®è¨ˆç®—
loss_a = log_loss(y_valid, valid_preds_a)
loss_b = log_loss(y_valid, valid_preds_b)

print(f'Log Loss for model_a: {loss_a}')
print(f'Log Loss for model_b: {loss_b}')

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
test_preds_a = model_a.predict_proba(X_test_a)[:, 1]
test_preds_b = model_b.predict_proba(X_test_b)[:, 1]

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': test_preds_a,
    'winner_model_b': test_preds_b,
    'winner_tie': 0.0  # åŒç‚¹ã®å ´åˆã¯0ã¨ã—ã¾ã™ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
})

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
submission.to_csv('/kaggle/working/submission.csv', index=False)
print("Submission file created successfully!")

```

---The following area is a Code cell (cell numver is 1)---
```python

```

** @@@ Jupyter Notebook numver 88, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
train=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
import tensorflow as tf
# detect and init the TPU


```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import tensorflow as tf
import numpy as np
import transformers 
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer

```

---The following area is a Code cell (cell numver is 5)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')
```

---The following area is a Code cell (cell numver is 6)---
```python
from transformers import TFBertModel

# Load BERT model
bert_model = TFBertModel.from_pretrained('/kaggle/input/bert-base-uncased')

```

---The following area is a Code cell (cell numver is 7)---
```python
for i in train.index:
    if train.loc[i,'winner_model_a']==1:
        train.loc[i,'winner']=0
    elif train.loc[i,'winner_model_b']==1:
        train.loc[i,'winner']=1
    else :
        train.loc[i,'winner']=2
```

---The following area is a Code cell (cell numver is 8)---
```python
features=['prompt','response_a','response_b','winner']
```

---The following area is a Code cell (cell numver is 9)---
```python
train_data=train[features]
```

---The following area is a Code cell (cell numver is 10)---
```python
X_train,X_val=train_test_split(train_data,test_size=0.2,random_state=42)

```

---The following area is a Code cell (cell numver is 11)---
```python
# Tokenize function
def tokenize_function(df):
    prompt_encodings = tokenizer(
        df['prompt'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    response_a_encodings = tokenizer(
        df['response_a'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    response_b_encodings = tokenizer(
        df['response_b'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    return prompt_encodings, response_a_encodings, response_b_encodings 


train_prompt_encodings, train_response_a_encodings, train_response_b_encodings = tokenize_function(X_train)
val_prompt_encodings, val_response_a_encodings, val_response_b_encodings = tokenize_function(X_val)

```

---The following area is a Code cell (cell numver is 12)---
```python
import tensorflow as tf
from tensorflow.keras.utils import to_categorical 

# Prepare input features and labels
train_labels = to_categorical(X_train['winner'].tolist(), num_classes=3)
val_labels = to_categorical(X_val['winner'].tolist(), num_classes=3)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids_prompt': train_prompt_encodings['input_ids'],
        'attention_mask_prompt': train_prompt_encodings['attention_mask'],
        'input_ids_response_a': train_response_a_encodings['input_ids'],
        'attention_mask_response_a': train_response_a_encodings['attention_mask'],
        'input_ids_response_b': train_response_b_encodings['input_ids'],
        'attention_mask_response_b': train_response_b_encodings['attention_mask'],
    },
    train_labels
)).shuffle(1000).batch(1)

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids_prompt': val_prompt_encodings['input_ids'],
        'attention_mask_prompt': val_prompt_encodings['attention_mask'],
        'input_ids_response_a': val_response_a_encodings['input_ids'],
        'attention_mask_response_a': val_response_a_encodings['attention_mask'],
        'input_ids_response_b': val_response_b_encodings['input_ids'],
        'attention_mask_response_b': val_response_b_encodings['attention_mask'],
    },
    val_labels
)).batch(1)

```

---The following area is a Code cell (cell numver is 13)---
```python

# Define inputs
input_ids_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_prompt")
attention_mask_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_prompt")

input_ids_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_a")
attention_mask_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_a")

input_ids_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_b")
attention_mask_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_b")

```

---The following area is a Code cell (cell numver is 14)---
```python
import tensorflow as tf
from transformers import TFBertModel

class BertEmbeddingLayer(tf.keras.layers.Layer):
    def __init__(self, bert_model_name='bert-base-uncased', **kwargs):
        super(BertEmbeddingLayer, self).__init__(**kwargs)
        self.bert = bert_model
        
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state[:, 0, :]  # CLS token embedding

# Initialize the custom BERT layer
bert_layer = BertEmbeddingLayer()

```

---The following area is a Code cell (cell numver is 15)---
```python

prompt_embeddings = bert_layer([input_ids_prompt, attention_mask_prompt])
response_a_embeddings = bert_layer([input_ids_response_a, attention_mask_response_a])
response_b_embeddings = bert_layer([input_ids_response_b, attention_mask_response_b])

# Concatenate embeddings
combined_embeddings = tf.keras.layers.Concatenate()([prompt_embeddings, response_a_embeddings, response_b_embeddings])

```

---The following area is a Code cell (cell numver is 16)---
```python

dense_layer = tf.keras.layers.Dense(256, activation='relu')(combined_embeddings)
dropout_layer = tf.keras.layers.Dropout(0.2)(dense_layer)
output_layer = tf.keras.layers.Dense(3, activation='softmax')(dropout_layer)

# Build and compile the model
model = tf.keras.Model(inputs=[
    input_ids_prompt, attention_mask_prompt,
    input_ids_response_a, attention_mask_response_a,
    input_ids_response_b, attention_mask_response_b
], outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])

```

---The following area is a Code cell (cell numver is 17)---
```python

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=1
)

```

---The following area is a Code cell (cell numver is 18)---
```python
def encode_text(texts, max_length):
    return tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='tf'
    )
```

---The following area is a Code cell (cell numver is 19)---
```python
test
```

---The following area is a Code cell (cell numver is 20)---
```python
max_length = 128  # Adjust according to your model's max sequence length
input_ids_prompt = encode_text(test['prompt'].tolist(), max_length)
input_ids_response_a = encode_text(test['response_a'].tolist(), max_length)
input_ids_response_b = encode_text(test['response_b'].tolist(), max_length)
```

---The following area is a Code cell (cell numver is 21)---
```python
predictions = model.predict({
    'input_ids_prompt': input_ids_prompt['input_ids'],
    'attention_mask_prompt': input_ids_prompt['attention_mask'],
    'input_ids_response_a': input_ids_response_a['input_ids'],
    'attention_mask_response_a': input_ids_response_a['attention_mask'],
    'input_ids_response_b': input_ids_response_b['input_ids'],
    'attention_mask_response_b': input_ids_response_b['attention_mask']
})
```

---The following area is a Code cell (cell numver is 22)---
```python
print("done till here")
```

---The following area is a Code cell (cell numver is 23)---
```python
results=pd.DataFrame({
    'ID':test['id'],
    'winner_model_a':predictions[0],
    'winner_model_b':predictions[1],
    'winner_tie':predictions[2],
    
})
```

---The following area is a Code cell (cell numver is 24)---
```python
results.to_csv('/kaggle/working/submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 25)---
```python

```

---The following area is a Code cell (cell numver is 26)---
```python

```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Code cell (cell numver is 28)---
```python

```

** @@@ Jupyter Notebook numver 89, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Gemma 2 - 9b 

We use Gemma 2 9b model to get embeddings and train a classifier on it. This is first part and in this we only compute embed. You can also use other models. Let's get started!

Upvote if you found this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes 
!pip install -q git+https://github.com/huggingface/transformers
!pip install sentencepiece
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:
    MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
    MAX_LENGTH = 1024
    BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = GemmaTokenizer.from_pretrained(CFG.MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)     
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 9)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena-additional-data-90k-columns/Merged_data.csv')

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)


train['text'] = '<start_of_turn>User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n----\n\nModel B:\n'  + train['response_b'] + '<end_of_turn><eos>'
```

---The following area is a Code cell (cell numver is 10)---
```python
# taking only 40k sample
train = train[:40000]
train.head(1)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(train['text'][10])
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 13)---
```python
tokens = tokenizer(train['text'].tolist(),
                   padding='max_length',
                   max_length=CFG.MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')

INPUT_IDS = tokens['input_ids']
ATTENTION_MASKS = tokens['attention_mask']

data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in INPUT_IDS]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in ATTENTION_MASKS]
data[:2]
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Compute embedding
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_embeddings(df, model, device, batch_size=CFG.BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 16)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 17)---
```python
embeddings = torch.cat([results[0], results[1]], dim=0)
embeddings.shape
```

---The following area is a Code cell (cell numver is 18)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Save embed
```

---The following area is a Code cell (cell numver is 20)---
```python
save_path = 'gemma2_train_embed.npy'

# Save the embeddings as .npy file
np.save(save_path, embeddings.numpy())
# we also save train just for completeness
train.to_csv('train_embed.csv', index=False)

print(f"Concatenated embeddings saved to {save_path}")
```

** @@@ Jupyter Notebook numver 90, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# About this notebook
- Deberta-v3 xsmall starter code
- Original notebook is [here](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)
- Inference notebook will be updated a little later.
- In this notebook, I just use only prompt so you can use response_a, response_a, etc.



If this notebook is helpful, feel free to upvote.

And please upvote the original notebook :)

`V1` - Run debug mode for test
- only used prompt, you can add more texts

`V2` - Remove existing preprocessing (this part needs more ideas)
- please see [this](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/497832)
- changed `max_length = 1024` and `lr = 1e-5`

`V3` - Will be updated for all texts or HuggingFace, etc.
```

---The following area is a Code cell (cell numver is 1)---
```python
# ====================================================
# Directory settings
# ====================================================
import os

OUTPUT_DIR = './'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
```

---The following area is a Code cell (cell numver is 2)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 3)---
```python
# ====================================================
# CFG
# ====================================================
class CFG:
    wandb=False
    competition='LMSYS'
    _wandb_kernel='test'
    debug=True
    apex=True
    print_freq=20
    num_workers=4
    model="microsoft/deberta-v3-xsmall" # ["microsoft/deberta-v3-small, microsoft/deberta-v3-base"]
    gradient_checkpointing=False
    scheduler='cosine' # ['linear', 'cosine']
    batch_scheduler=True
    num_cycles=0.5
    num_warmup_steps=0
    epochs=4
    encoder_lr=1e-5
    decoder_lr=1e-5
    min_lr=1e-5
    eps=1e-6
    betas=(0.9, 0.999)
    batch_size=4
    max_len=2048
    weight_decay=0.01
    gradient_accumulation_steps=1
    max_grad_norm=1000
    target_label=['target']
    target_cols=['winner_model_a', 'winner_model_b', 'winner_tie']
    seed=42
    train=True
    
if CFG.debug:
    CFG.epochs = 2
    CFG.trn_fold = [0, 1]
```

---The following area is a Code cell (cell numver is 4)---
```python
# ====================================================
# Library
# ====================================================
import os
import gc
import re
import ast
import sys
import copy
import json
import time
import math
import string
import pickle
import random
import joblib
import itertools
import warnings
warnings.filterwarnings("ignore")

import scipy as sp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
from tqdm.auto import tqdm
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold

import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.nn.functional as F
from torch.optim import Adam, SGD, AdamW
from torch.utils.data import DataLoader, Dataset

os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels transformers')
os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels tokenizers')

import tokenizers
import transformers
print(f"tokenizers.__version__: {tokenizers.__version__}")
print(f"transformers.__version__: {transformers.__version__}")
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
%env TOKENIZERS_PARALLELISM=true

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---The following area is a Code cell (cell numver is 5)---
```python
# ====================================================
# Utils
# ====================================================
def get_score(y_trues, y_preds):
    score = log_loss(y_trues, y_preds, labels=[0, 1, 2])
    return score


def get_logger(filename=OUTPUT_DIR+'train'):
    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter
    logger = getLogger(__name__)
    logger.setLevel(INFO)
    handler1 = StreamHandler()
    handler1.setFormatter(Formatter("%(message)s"))
    handler2 = FileHandler(filename=f"{filename}.log")
    handler2.setFormatter(Formatter("%(message)s"))
    logger.addHandler(handler1)
    logger.addHandler(handler2)
    return logger

LOGGER = get_logger()


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=CFG.seed)
```

---The following area is a Code cell (cell numver is 6)---
```python
# =====================================
# We will add targets in dataframs
# =====================================
def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)
    labels[df['winner_model_a'] == 1] = 0
    labels[df['winner_model_b'] == 1] = 1
    labels[df['winner_tie'] == 1] = 2
    df['target'] = labels
    return df


```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 8)---
```python
add_label(train).head(5)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 10)---
```python
# ====================================================
# tokenizer
# ====================================================
# Define special tokens
special_tokens = ['[R_STRAT]', '[R_END]', '<PROMPT>', '<RESPONSE>', '[NL]', '[NLNL]']

def preprocess_text(text):
    text = text.replace('\n\n', ' [NLNL] ')
    text = text.replace('\n', ' [NL] ')
    return text

def format_conversation(row):
    conversations = []
    num_turns = min(len(row['prompt']), len(row['response_a']), len(row['response_b']))
    
    for i in range(num_turns):
        prompt = f"<PROMPT> {row['prompt'][i]}"
        response_a = f"<RESPONSE> [R_STRAT] {preprocess_text(row['response_a'][i])} [R_END]"
        response_b = f"[R_STRAT] {preprocess_text(row['response_b'][i])} [R_END]"
        conversations.append(f"{prompt} {response_a} {response_b}")
        
    return ' [NLNL] '.join(conversations)

train['text'] = train.apply(format_conversation, axis=1)

# Add special tokens to tokenizer
tokenizer = AutoTokenizer.from_pretrained(CFG.model)
special_tokens_dict = {'additional_special_tokens': special_tokens}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
CFG.tokenizer = tokenizer
```

---The following area is a Code cell (cell numver is 11)---
```python
tokenizer.decode(
    tokenizer(
        train['text'][1]
    ).input_ids
)
```

---The following area is a Code cell (cell numver is 12)---
```python
# ====================================================
# Dataset
# ====================================================
def prepare_input(cfg, text):
    inputs = cfg.tokenizer.encode_plus(
        text, 
        return_tensors=None, 
        add_special_tokens=True, 
        max_length=CFG.max_len,
        pad_to_max_length=True,
        truncation=True
    )
    for k, v in inputs.items():
        inputs[k] = torch.tensor(v, dtype=torch.long)
    return inputs


class TrainDataset(Dataset):
    def __init__(self, cfg, df):
        self.cfg = cfg
        self.texts = df['text'].values # only use prompt, please feel free add other texts
        self.labels = df[cfg.target_label].values.squeeze().tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        inputs = prepare_input(self.cfg, self.texts[item])
        label = torch.tensor(self.labels[item], dtype=torch.long)
        return inputs, label
    
    
def collate(inputs):
    mask_len = int(inputs["attention_mask"].sum(axis=1).max())
    for k, v in inputs.items():
        inputs[k] = inputs[k][:,:mask_len]
    return inputs
```

---The following area is a Code cell (cell numver is 13)---
```python
# ====================================================
# Model
# ====================================================
class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()
        
    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        mean_embeddings = sum_embeddings / sum_mask
        return mean_embeddings
    

class CustomModel(nn.Module):
    def __init__(self, cfg, config_path=None, pretrained=False):
        super().__init__()
        self.cfg = cfg
        if config_path is None:
            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)
            self.config.hidden_dropout = 0.
            self.config.hidden_dropout_prob = 0.
            self.config.attention_dropout = 0.
            self.config.attention_probs_dropout_prob = 0.
            LOGGER.info(self.config)
        else:
            self.config = torch.load(config_path)
        if pretrained:
            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)
        else:
            self.model = AutoModel(self.config)
        if self.cfg.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        self.pool = MeanPooling()
        self.fc = nn.Linear(self.config.hidden_size, 3)
        self._init_weights(self.fc)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        
    def feature(self, inputs):
        outputs = self.model(**inputs)
        last_hidden_states = outputs[0]
        feature = self.pool(last_hidden_states, inputs['attention_mask'])
        return feature

    def forward(self, inputs):
        feature = self.feature(inputs)
        output = self.fc(feature)
        return output
```

---The following area is a Code cell (cell numver is 14)---
```python
# ====================================================
# Helper functions
# ====================================================
class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))


def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):
    model.train()
    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)
    losses = AverageMeter()
    start = end = time.time()
    global_step = 0
    for step, (inputs, labels) in enumerate(train_loader):
        inputs = collate(inputs)
        for k, v in inputs.items():
            inputs[k] = v.to(device)
        labels = labels.to(device)
        batch_size = labels.size(0)
        with torch.cuda.amp.autocast(enabled=CFG.apex):
            y_preds = model(inputs)
            loss = criterion(y_preds, labels)
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps
        losses.update(loss.item(), batch_size)
        scaler.scale(loss).backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)
        if (step + 1) % CFG.gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            global_step += 1
            if CFG.batch_scheduler:
                scheduler.step()
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):
            print('Epoch: [{0}][{1}/{2}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  'Grad: {grad_norm:.4f}  '
                  'LR: {lr:.8f}  '
                  .format(epoch+1, step, len(train_loader), 
                          remain=timeSince(start, float(step+1)/len(train_loader)),
                          loss=losses,
                          grad_norm=grad_norm,
                          lr=scheduler.get_lr()[0]))
    return losses.avg


def valid_fn(valid_loader, model, criterion, device):
    losses = AverageMeter()
    model.eval()
    preds = []
    start = end = time.time()
    for step, (inputs, labels) in enumerate(valid_loader):
        inputs = collate(inputs)
        for k, v in inputs.items():
            inputs[k] = v.to(device)
        labels = labels.to(device)
        batch_size = labels.size(0)
        with torch.no_grad():
            y_preds = model(inputs)
            loss = criterion(y_preds, labels)
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps
        losses.update(loss.item(), batch_size)
        preds.append(y_preds.softmax(1).to('cpu').numpy())
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):
            print('EVAL: [{0}/{1}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  .format(step, len(valid_loader),
                          loss=losses,
                          remain=timeSince(start, float(step+1)/len(valid_loader))))
    predictions = np.concatenate(preds)
    return losses.avg, predictions
```

---The following area is a Code cell (cell numver is 15)---
```python
# ====================================================
# train loop
# ====================================================
def train_loop(train_df, valid_df):
    
    LOGGER.info(f"========== training ==========")

    # ====================================================
    # loader
    # ====================================================
    valid_labels = valid_df[CFG.target_label].values
    
    train_dataset = TrainDataset(CFG, train_df)
    valid_dataset = TrainDataset(CFG, valid_df)

    train_loader = DataLoader(train_dataset,
                              batch_size=CFG.batch_size,
                              shuffle=True,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)
    valid_loader = DataLoader(valid_dataset,
                              batch_size=CFG.batch_size * 2,
                              shuffle=False,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)

    # ====================================================
    # model & optimizer
    # ====================================================
    model = CustomModel(CFG, config_path=None, pretrained=True)
    torch.save(model.config, OUTPUT_DIR+'config.pth')
    model.to(device)
    
    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': weight_decay},
            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': 0.0},
            {'params': [p for n, p in model.named_parameters() if "model" not in n],
             'lr': decoder_lr, 'weight_decay': 0.0}
        ]
        return optimizer_parameters

    optimizer_parameters = get_optimizer_params(model,
                                                encoder_lr=CFG.encoder_lr, 
                                                decoder_lr=CFG.decoder_lr,
                                                weight_decay=CFG.weight_decay)
    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)
    
    # ====================================================
    # scheduler
    # ====================================================
    def get_scheduler(cfg, optimizer, num_train_steps):
        if cfg.scheduler == 'linear':
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps
            )
        elif cfg.scheduler == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles
            )
        return scheduler
    
    num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)
    scheduler = get_scheduler(CFG, optimizer, num_train_steps)

    # ====================================================
    # loop
    # ====================================================
    criterion = nn.CrossEntropyLoss()
    
    best_score = np.inf

    for epoch in range(CFG.epochs):

        start_time = time.time()

        # train
        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)

        # eval
        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)
        
        # scoring
        score = get_score(valid_labels, predictions)

        elapsed = time.time() - start_time

        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')
        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')
        if CFG.wandb:
            wandb.log({"epoch": epoch+1, 
                       "avg_train_loss": avg_loss, 
                       "avg_val_loss": avg_val_loss,
                       "score": score})
        
        if best_score > score:
            best_score = score
            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')
            torch.save({'model': model.state_dict(),
                        'predictions': predictions},
                        OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth")

    predictions = torch.load(OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth", 
                             map_location=torch.device('cpu'))['predictions']
    valid_df[[f"pred_{c}" for c in CFG.target_cols]] = predictions

    torch.cuda.empty_cache()
    gc.collect()
    
    return valid_df
```

---The following area is a Code cell (cell numver is 16)---
```python
if __name__ == '__main__':
    
    def get_result(oof_df):
        labels = oof_df[CFG.target_cols].values
        labels = np.argmax(labels, axis=1)
        preds = oof_df[[f"pred_{c}" for c in CFG.target_cols]].values
        score = get_score(labels, preds)
        LOGGER.info(f'Score: {score:<.4f}')
    
    if CFG.train:
        # Split data into train and validation (80% train, 20% validation)
        train_df = train.sample(frac=0.8, random_state=CFG.seed).reset_index(drop=True)
        valid_df = train.drop(train_df.index).reset_index(drop=True)

        _oof_df = train_loop(train_df, valid_df)
        LOGGER.info(f"========== result ==========")
        get_result(_oof_df)
        _oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')
        
    if CFG.wandb:
        wandb.finish()
```

** @@@ Jupyter Notebook numver 91, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 1280
    batch_size = 8
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding=True,
            max_length=max_length,
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Code cell (cell numver is 24)---
```python

```

** @@@ Jupyter Notebook numver 92, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Imports
```

---The following area is a Code cell (cell numver is 1)---
```python
from IPython.display import clear_output
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q bertopic
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install -U numpy==1.23.5
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
from ast import literal_eval
import numpy as np
import os
import random
import re

from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance
from bertopic.vectorizers import ClassTfidfTransformer



```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
**Before we start, here is a summary derived from the excellent EDA done in this notebook :**https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda

**General findings**

- There are 64 different models in the training set.
- For each prompt, there are 3 responses, each from different model and ranked according to the human preference.
- The most common (frequent models) in the training data are as follows :

    - gpt-4-1106-preview 
    - gpt-3.5-turbo-0613
    - gpt-4-0613 
    - claude-2.1 
    - (gpt-4-0314,claude-instant-1)
    
    
- Number of Turns : The number of prompt/response pairs in the training data.

    - Around 86.88% of conversations are single-turn.
    - Over 99.19% of conversations are less than 6 turns.
    - The maximum number of turns is 36.
    
    

**Model response preferences**

- Three LLMs have win rate over 50%, including gpt-3.5-turbo-0314, gpt-4-0125-preview and gpt-4-1106-preview.
- A lower tie rate means that a winner can be judged more deterministically


**Biases and correlations in response/prompts**


- No position bias for human judges. i.e , the positions A,B,C in the training data have no such positional bias in them in context of preference of annotators.
 
- Correlation between response lenghts of models for the same prompt: There exist a strong correlation between response length for the same prompt.
 
- Correlation between prompt length and response length : the linear relationship between prompt length and response length, the correlation seems much weaker.

- Verbosity Bias (how the verbosity of the answer affects human preference) : 
    - There is a clear verbosity bias with the data.
    - Correlation of "Mean response length" vs "win rate"  =  0.488
    - gpt-4-0125-preview and gpt-4-1106-preview are models with the top-2 longest average response length.
    

**null/empty responses or prompts**

- There exists 5 samples with empty prompts.
- All of the empty prompts are a single space " " and appear at the last prompt during conversation.
- Models can still continue to respond even if an empty prompt is sent.
- Missing responses can be empty or None.

- efffect on judges
    - We can see that the tie rate drops to around 0.15, which is quite reasonble.
    - If only one model has missing responses, judges might tend to vote the other responding normally or tie.
    

**checking train and test data**
```

---The following area is a Code cell (cell numver is 6)---
```python
def parse_response(response_object:str)->[str,]:
    try:
        resp = literal_eval(response_object)
        
    except Exception as e:
        try:
            response_object = response_object.replace('null',None)
            resp = literal_eval(response_object)
        except Exception as e:
#             print(type(response_object),response_object)
            resp = []

    return resp
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
train['prompt'] = train['prompt'].apply(literal_eval)
train['response_a'] = train['response_a'].apply(parse_response)
train['response_b'] = train['response_b'].apply(parse_response)

test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

_ = print(train.shape),print(test.shape)
```

---The following area is a Code cell (cell numver is 8)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
#dropping empty responses
train_ss = train[(train.response_a.apply(len)>0) & (train.response_b.apply(len)>0)]

train_ss.shape
```

---The following area is a Code cell (cell numver is 10)---
```python
test.head()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Topic Modelling 

**How it works (from: https://maartengr.github.io/BERTopic/algorithm/algorithm.html)**

![image.png](attachment:60ac0df8-5866-4948-9240-7e6453976a10.png)

1. Embed documents : We start by converting our documents to numerical representations using a tranformer based model embedding.

2. Dimensionality reduction : After having created our numerical representations of the documents we have to reduce the dimensionality of these representations. Cluster models typically have difficulty handling high dimensional data due to the curse of dimensionality.

3. Cluster Documents : After having reduced our embeddings, we can start clustering our data. For that, we leverage a density-based clustering technique, HDBSCAN. It can find clusters of different shapes and has the nice feature of identifying outliers where possible.

4. Bag-of-words : To create topic representations in BERTopic's algorithm while allowing for modularity, HDBSCAN is used as a clustering model because it accommodates clusters with varying densities and shapes. Instead of using a centroid-based method, all documents in a cluster are combined into a single document, and the frequency of each word is counted to form a bag-of-words representation. This representation, normalized for cluster size differences, focuses on words at the cluster level without assuming a specific cluster structure.

5. Topic representation : From the generated bag-of-words representation, we want to know what makes one cluster different from another. Which words are typical for cluster 1 and not so much for all other clusters? To solve this, we need to modify TF-IDF such that it considers topics (i.e., clusters) instead of documents.

6. Fine-tune Topic representation : we can consider the c-TF-IDF generated topics to be candidate topics. They each contain a set of keywords and representative documents that we can use to further fine-tune the topic representations. Having a set of representative documents for each topic is huge advantage as it allows for fine-tuning on a reduced number of documents. This reduces computation for large models as they only need to operate on that small set of representative documents for each topic. 



    Let us try to find the underlying patterns in the prompt, using a unsupervised topic modelling approach. This will give an high level idea of what the constituent topics in the prompts are.
```

---The following area is a Code cell (cell numver is 12)---
```python
# Setup modules


# Step 1 - Extract embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 2 - Reduce dimensionality
umap_model = UMAP(
    n_neighbors=20, 
    n_components=5,
    min_dist=0.0, 
    metric="cosine",
    random_state=7,
)

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(
    min_cluster_size=32, 
    min_samples=1,
    metric="euclidean", 
    cluster_selection_method="eom",
    prediction_data=True
)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words="english")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True,)

# Fine-tune topic representations with  a `bertopic.representation` model
representation_model = MaximalMarginalRelevance(diversity=0.4,
                                                top_n_words=15
                                               )
```

---The following area is a Code cell (cell numver is 13)---
```python
# Build topic modeling pipeline
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    representation_model=representation_model,
    n_gram_range=(1,5),
    language="english"
)
```

---The following area is a Code cell (cell numver is 14)---
```python
train_prompt_concatenated = train.prompt.apply(lambda x: "\n\n".join(x)).to_list()
len(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 15)---
```python
%%time
topics,topic_proba = topic_model.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 16)---
```python
topic_model.save("topic_model_unguided", serialization="safetensors")

```

---The following area is a Code cell (cell numver is 17)---
```python
print(f" number of unique topics: {len(np.unique(topics))}")

```

---The following area is a Code cell (cell numver is 18)---
```python
topic_info = topic_model.get_topic_info()
topic_info.head(10)
```

---The following area is a Code cell (cell numver is 19)---
```python
#get a specific topic repr 
topic_model.get_topic(0)
```

---The following area is a Code cell (cell numver is 20)---
```python
#visualize topic repr
topic_model.visualize_barchart(top_n_topics=30,n_words=10)
```

---The following area is a Code cell (cell numver is 21)---
```python
topic_model.visualize_term_rank()

```

---The following area is a Code cell (cell numver is 22)---
```python
topic_model.visualize_heatmap(top_n_topics=20)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Guided topic modelling 

    The following illustration gives a idea behind guided topic modelling (from: https://maartengr.github.io/BERTopic/getting_started/guided)

![image.png](attachment:e350c127-4025-4b0a-9942-3768abb34873.png)
```

---The following area is a Code cell (cell numver is 24)---
```python
task_topics = "Code to text,,text to code,Named entity recognition,Sentiment Analysis,Translation,Question Answering,Program Execution, Miscallenous tasks,Text Categorization,Language Identification, Information Extraction,Text Quality,Summarization,text completion,essay writing,poem writing,creative writing,fact verification,reasoning,mathematical,grammer task,rephrasing,style transfer,paraphrasing,natural language inference,question generation,text matching,dialogue generation,harmfullness detection,toxic language detection,fact verification,keyword tagging".split(",")

print(task_topics)
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
**Modified task keywords using CHATGPT**
```

---The following area is a Code cell (cell numver is 26)---
```python
#define seed topics 
task_topics_modified = [
    ['Code to text', 'source code', 'comments', 'explanation', 'description', 'documentation'],
    ['Text to code', 'programming', 'syntax', 'function', 'script', 'automation'],
    ['Named entity recognition', 'NER', 'entities', 'classification', 'annotation', 'identification'],
    ['Sentiment Analysis', 'emotion', 'opinion', 'polarity', 'attitude', 'mood'],
    ['Translation', 'bilingual', 'language pair', 'conversion', 'interpretation', 'localization'],
    ['Question Answering', 'QA', 'response', 'inquiry', 'knowledge', 'retrieval'],
    ['Program Execution', 'run', 'execute', 'compile', 'script', 'process'],
    ['Miscellaneous tasks', 'varied', 'general', 'diverse', 'assorted', 'multiple'],
    ['Text Categorization', 'classification', 'labeling', 'sorting', 'grouping', 'organization'],
    ['Language Identification', 'detection', 'recognition', 'classification', 'language', 'dialect'],
    ['Information Extraction', 'data mining', 'retrieval', 'extraction', 'parsing', 'harvesting'],
    ['Text Quality', 'clarity', 'readability', 'coherence', 'accuracy', 'precision'],
    ['Summarization', 'abstract', 'condense', 'overview', 'digest', 'outline'],
    ['Text completion', 'autocomplete', 'fill-in', 'predictive', 'continuation', 'suggestion'],
    ['Essay writing', 'composition', 'argument', 'thesis', 'structure', 'drafting'],
    ['Poem writing', 'verse', 'rhyme', 'stanza', 'meter', 'lyric'],
    ['Creative writing', 'story', 'imagination', 'narrative', 'fiction', 'expression'],
    ['Fact verification', 'truth', 'validation', 'accuracy', 'confirmation', 'authenticity'],
    ['Reasoning', 'logic', 'deduction', 'inference', 'rationale', 'analysis'],
    ['Mathematical', 'calculation', 'formula', 'equation', 'computation', 'arithmetic'],
    ['Grammar task', 'syntax', 'rules', 'correction', 'structure', 'editing'],
    ['Rephrasing', 'paraphrase', 'reword', 'rewrite', 'restatement', 'alteration'],
    ['Style transfer', 'transformation', 'conversion', 'adaptation', 'modification', 'recasting'],
    ['Paraphrasing', 'rewording', 'restating', 'rephrasing', 'altering', 'modifying'],
    ['Natural language inference', 'NLI', 'hypothesis', 'entailment', 'contradiction', 'inference'],
    ['Question generation', 'inquiry', 'query', 'interrogative', 'ask', 'question'],
    ['Text matching', 'similarity', 'comparison', 'alignment', 'correlation', 'matching'],
    ['Dialogue generation', 'conversation', 'interaction', 'exchange', 'communication', 'chatbot'],
    ['Harmfulness detection', 'toxicity', 'abuse', 'malice', 'danger', 'risk'],
    ['Toxic language detection', 'abusive', 'offensive', 'harmful', 'inappropriate', 'insulting'],
    ['Fact verification', 'validation', 'authenticity', 'accuracy', 'truth', 'confirmation'],
    ['Keyword tagging', 'labeling', 'annotation', 'classification', 'indexing', 'tagging'],
    ['Topic modeling', 'themes', 'topics', 'clustering', 'segmentation', 'grouping'],
    ['Contextual embedding', 'context', 'representation', 'vectors', 'embeddings', 'contextualization'],
    ['Coreference resolution', 'pronouns', 'anaphora', 'antecedents', 'referents', 'binding'],
    ['Semantic similarity', 'meaning', 'relation', 'comparison', 'equivalence', 'likeness'],
    ['Document summarization', 'overview', 'digest', 'abstract', 'compendium', 'condensation'],
    ['Speech recognition', 'transcription', 'audio', 'voice', 'ASR', 'spoken'],
    ['Optical character recognition', 'OCR', 'text', 'image', 'scanning', 'extraction'],
    ['Text generation', 'creation', 'synthesis', 'generation', 'writing', 'production'],
    ['Dialogue summarization', 'conversation', 'overview', 'recap', 'condensation', 'summary'],
    ['Data anonymization', 'privacy', 'masking', 'obfuscation', 'anonymity', 'de-identification']
]



len(task_topics_modified)
```

---The following area is a Code cell (cell numver is 27)---
```python
topic_model_guided = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    representation_model=representation_model,
    n_gram_range=(1,5),
    language="english",
    seed_topic_list=task_topics_modified)
```

---The following area is a Code cell (cell numver is 28)---
```python
%%time
topics,topic_proba = topic_model_guided.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 29)---
```python
topic_model_guided.visualize_barchart(top_n_topics=30,n_words=10)
```

---The following area is a Code cell (cell numver is 30)---
```python
topic_model_guided.save("topic_model_guided", serialization="safetensors")

```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
# Resources 



* https://research.google/pubs/large-language-models-are-effective-text-rankers-with-pairwise-ranking-prompting/
* https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms
* https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives
* https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda
* https://www.kaggle.com/code/robikscube/lmsys-chatbot-arena-data-anaylsis#Response-Length-Baseline
* https://medium.com/data-reply-it-datatech/bertopic-topic-modeling-as-you-have-never-seen-it-before-abb48bbab2b2
```

** @@@ Jupyter Notebook numver 93, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
<center><img src="https://keras.io/img/logo-small.png" alt="Keras logo" width="100"><br/>
This starter notebook is provided by the Keras team.</center>

# LMSYS - Chatbot Arena Human Preference Predictions with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)

<div align="center">
    <img src="https://i.ibb.co/wJMF5HL/lmsys.png">
</div>

In this competition, our aim is to predict which LLM responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). In other words, the goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. This notebook will guide you through the process of fine-tuning the **DebertaV3** model for this competition using the **Shared Weight** strategy with KerasNLP. This strategy is similar to how Multiple Choice Question (MCQ) models are trained. Additionally, we will use mixed precision for faster training and inference.

**Did you know**: This notebook is backend-agnostic, which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of the preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).

**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).


# ðŸ“š | Import Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "jax"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import plotly.express as px
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Library Version
```

---The following area is a Code cell (cell numver is 3)---
```python
print("TensorFlow:", tf.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# âš™ï¸ | Configuration
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en" # Name of pretrained models
    sequence_length = 512  # Input sequence length
    epochs = 3 # Training epochs
    batch_size = 16  # Batch size
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# â™»ï¸ | Reproducibility 
Sets value for random seed to produce similar result in each run.
```

---The following area is a Code cell (cell numver is 7)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# ðŸ§® | Mixed Precision

In this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time.
```

---The following area is a Code cell (cell numver is 9)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# ðŸ“ | Dataset Path
```

---The following area is a Code cell (cell numver is 11)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# ðŸ“– | Meta Data 

The competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.

## Files

### `train.csv`
- `id`: Unique identifier for each row.
- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).

### `test.csv`
- `id`: Unique identifier for each row.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.

> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.


## Train Data
```

---The following area is a Code cell (cell numver is 13)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# Sample data
# df = df.sample(frac=0.10)

# Take the first prompt and its associated response
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## Test Data
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Show Sample
test_df.head()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Contextualize Response with Prompt

In our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.

> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.
```

---The following area is a Code cell (cell numver is 17)---
```python
# Define a function to create options based on the prompt and choices
def make_pairs(row):
    row["encode_fail"] = False
    try:
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        prompt = ""
        row["encode_fail"] = True

    try:
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        response_a = ""
        row["encode_fail"] = True

    try:
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        response_b = ""
        row["encode_fail"] = True
        
    row['options'] = [f"Prompt: {prompt}\n\nResponse: {response_a}",  # Response from Model A
                      f"Prompt: {prompt}\n\nResponse: {response_b}"  # Response from Model B
                     ]
    return row

```

---The following area is a Code cell (cell numver is 18)---
```python
df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(df.head(2))  # Display the first 2 rows of df

test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## Encoding Fail Statistics

Let's examine how many samples have encoding issues. From the code below, we can see that only $1\%$ of the samples failed to be encoded, while $99\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference.
```

---The following area is a Code cell (cell numver is 20)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# ðŸŽ¨ | Exploratory Data Analysis (EDA)

## LLM Distribution
```

---The following area is a Code cell (cell numver is 22)---
```python
model_df = pd.concat([df.model_a, df.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count', color_continuous_scale='viridis')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()

```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
## Winning Distribution
```

---The following area is a Code cell (cell numver is 24)---
```python
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

fig = px.bar(counts, x='Winner', y='Win Count',
             title='Winner distribution for Train Data',
             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},
             color='Winner', color_continuous_scale='viridis')

fig.update_layout(xaxis_title="Winner", yaxis_title="Win Count")

fig.show()

```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# ðŸ”ª | Data Split

In the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column.
```

---The following area is a Code cell (cell numver is 26)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# ðŸ½ï¸ | Preprocessing

**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.

**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `"The quick brown fox"` into `["the", "qu", "##ick", "br", "##own", "fox"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.

Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:
- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)
- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)
```

---The following area is a Code cell (cell numver is 28)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # Name of the model
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
)
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\_responses, sequence\_length)$.
```

---The following area is a Code cell (cell numver is 30)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.
```

---The following area is a Code cell (cell numver is 32)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# ðŸš | DataLoader

The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.

To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).
```

---The following area is a Code cell (cell numver is 34)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
## Build Train/Valid Dataloader
```

---The following area is a Code cell (cell numver is 36)---
```python
# Train
train_texts = train_df.options.tolist()  # Extract training texts
train_labels = train_df.class_label.tolist()  # Extract training labels
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# Valid
valid_texts = valid_df.options.tolist()  # Extract validation texts
valid_labels = valid_df.class_label.tolist()  # Extract validation labels
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# âš“ | LR Schedule

Implementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:
- `step`: Lowering the learning rate in step-wise manner resembling stairs.
- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.
- `exp`: Exponentially decreasing the learning rate.

**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.
```

---The following area is a Code cell (cell numver is 38)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # Plot lr curve if plot is True
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('epoch'); plt.ylabel('lr')
        plt.title('LR Scheduler')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 39)---
```python
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# ðŸ’¾ | Model Checkpointing

The following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission.
```

---The following area is a Code cell (cell numver is 41)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')  # Get Model checkpoint callback
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
# ðŸ“ | Metric

The metric for this competition is **Log Loss**. This metric can be expressed mathematically as,

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$

where $ N $ is the number of samples, $ y_i $ is the true label, and $ p_i $ is the predicted probability of the sample belonging to the positive class.

Note that this metric is similar to categorical cross entropy widely used in classification tasks. Thus, we don't need to implement the loss from scratch. As the Keras library already has an implementation of this metric, we will simply use the metric to monitor performance of our model.
```

---The following area is a Code cell (cell numver is 43)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# ðŸ¤– | Modeling

The `KerasNLP` library provides various NLP model architectures such as `Bert`, `Roberta`, `DebertaV3`, and more. While this notebook focuses on `DebertaV3`, you can explore others in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). For a deeper understanding, refer to the [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).

Our approach utilizes `keras_nlp.models.DebertaV3Classifier` to process each prompt and response pair, generating output embeddings. We then concatenate these embeddings and pass them through a Pooling layer and a classifier to obtain logits, followed by a `softmax` function for the final output.

When dealing with multiple responses, we use a weight-sharing strategy. This means we provide the model with one response at a time along with the prompt `(P + R_A)`, `(P + R_B)`, etc., using the same model weights for all responses. After obtaining embeddings for all responses, we concatenate them and apply average pooling. Next, we use a `Linear/Dense` layer along with the `Softmax` function as the classifier for the final result. Providing all responses at once would increase text length and complicate model handling. Note that, in the classifier, we use 3 classes for `winner_model_a`, `winner_model_b`, and `draw` cases.

The diagram below illustrates this approach:

<div align="center">
    <img src="https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png">
</div>

From a coding perspective, note that we use the same model for all responses with shared weights, contrary to the separate models implied in the diagram.
```

---The following area is a Code cell (cell numver is 45)---
```python
# Define input layers
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# Create a DebertaV3Classifier backbone
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# Compute embeddings for first response: (P + R_A) using backbone
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# Compute embeddings for second response: (P + R_B), using the same backbone
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# Compute final output
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# Compile the model with optimizer, loss, and metrics
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
### Model Summary
```

---The following area is a Code cell (cell numver is 47)---
```python
model.summary()
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
### Model Plot

In the model graph below, it may seem there are **four** inputs, but actually, there are **two** as discussed before. Our input consists of two parts, one for each response. However, for each input, we have `token_ids` and `padding_mask`, which makes it look like we have four inputs, but in reality, we have two inputs.
```

---The following area is a Code cell (cell numver is 49)---
```python
# Currently throwing error !! [probably library or env issue, so hopefully will be fixed soon]

# keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)
```

---The following area is a Markdown cell (cell numver is 50)---
```markdown
# ðŸš‚ | Training
```

---The following area is a Code cell (cell numver is 51)---
```python
# Start training the model
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
## Load Best Model

After training, let's load the weight with best result to get the best performance.
```

---The following area is a Code cell (cell numver is 53)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Markdown cell (cell numver is 54)---
```markdown
# ðŸ§ª | Prediction
```

---The following area is a Code cell (cell numver is 55)---
```python
# Build test dataset
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 56)---
```python
# Make predictions using the trained model on test data
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Markdown cell (cell numver is 57)---
```markdown
# ðŸ“¬ | Submission

Following code will prepare the submission file.
```

---The following area is a Code cell (cell numver is 58)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

---The following area is a Markdown cell (cell numver is 59)---
```markdown
# ðŸ”­ | Future Directions

In this notebook, we've achieved a good score with a small model and modest token length. But there's plenty of room to improve. Here's how:

1. Try bigger models like `Deberta-Base` or `Deberta-Small`, or even LLMs like `Gemma`.
2. Increase max token length to reduce loss of data.
3. Use a five-fold cross-validation and ensemble to make the model robust and get better scores.
4. Add augmentation like shuffling response orders for more robust performance.
5. Train for more epochs.
6. Tune the learning rate scheduler.

# ðŸ“Œ | Reference

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 94, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## llama3-8b

shout out to:
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python

import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1024
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    

# # Prepare Data 

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])

# # Tokenize


tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]

# # Load model 
# We load 1 model on each gpu.  

# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id

# Now, we have sucessfully loaded one model on each GPU!

# # Load weights 

# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])

# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
# Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

# Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()

# # Inference
# 

import gc
gc.collect()

def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df

st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}

# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

# Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) 

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 3)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 5)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 6)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
    
```

---The following area is a Code cell (cell numver is 7)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 8)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 9)---
```python
import os
import io
import gc
import time
import json
import random
import pickle
import zipfile
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers

os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 10)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 11)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 12)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)

```

---The following area is a Code cell (cell numver is 13)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 14)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 15)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 16)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 17)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 18)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 19)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 20)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 21)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 22)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 23)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 24)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 25)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.2 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 27)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 28)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 29)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 95, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
#  Llama-3 8b [TPU Train]

Learning to train llms on tpu, Hope this will help you too!

Notebook inspired from:

* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)


Prerequisite: Access to using llama-3

Note: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)

Please upvote if you learn or find this helpful!

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
# Install libs
!pip install -qq peft==0.6.0
!pip install -qq bitsandbytes==0.41.1
!pip install -qq accelerate==0.24.1
!pip install -qq transformers==4.35.0
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q 
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q
!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT
!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import torch.nn.functional as F

import torch_xla.debug.profiler as xp
import torch_xla.core.xla_model as xm
import torch_xla.experimental.xla_sharding as xs
import torch_xla.runtime as xr

xr.use_spmd()

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh
from spmd_util import partition_module

tqdm.pandas()

print(f'Torch Version: {torch.__version__}')
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 4)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 16
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
    
DEVICE = xm.xla_device() # Initialize TPU Device
```

---The following area is a Code cell (cell numver is 5)---
```python
def set_seeds(seed):
    """Set seeds for reproducibility """
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
    # Set seed for all TPU cores
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 8)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 10)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 11)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 12)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 13)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 0.5)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 14)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 15)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 16)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 18)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    padding='max_length', 
    max_length=CFG.MAX_LENGTH, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 19)---
```python
def train_dataset(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        # Shuffle Indices
        np.random.shuffle(IDXS)
        # Iterate Over All Indices Once
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output
            
            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Load Model
```

---The following area is a Code cell (cell numver is 21)---
```python
# Load model for classification with 3 target label
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    num_labels=CFG.NUM_LABELS,
    torch_dtype=torch.bfloat16)

base_model.config.pretraining_tp = 1 

# Assign Padding TOKEN
base_model.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# Low-Rank Adaptation [LORA]
```

---The following area is a Code cell (cell numver is 23)---
```python
lora_config = LoraConfig(
    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices
    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations
    lora_dropout= CFG.DROPOUT, 
    bias='none',
    inference_mode=False,
    task_type=TaskType.SEQ_CLS,
    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection
```

---The following area is a Code cell (cell numver is 24)---
```python
# Create LoRa Model
model = get_peft_model(base_model, lora_config)
# Trainable Parameters
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 25)---
```python
# Number of TPU Nodes
num_devices = xr.global_runtime_device_count()
mesh_shape = (1, num_devices, 1)
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))
# distribute model
partition_module(model, mesh)

print(f'num_devices: {num_devices}')
```

---The following area is a Code cell (cell numver is 26)---
```python
# Verfy The Trainable Layers
MODEL_LAYERS_ROWS = []
TRAINABLE_PARAMS = []
N_TRAINABLE_PARAMS = 0

for name, param in model.named_parameters():
    # Layer Parameter Count
    n_parameters = int(torch.prod(torch.tensor(param.shape)))
    # Only Trainable Layers
    if param.requires_grad:
        # Add Layer Information
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,
            'name': name,
            'dtype': param.data.dtype,
        })
        # Append Trainable Parameter
        TRAINABLE_PARAMS.append({ 'params': param })
        # Add Number Of Trainable Parameters"
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))

print(f"""
===============================
N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}
N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}
===============================
""")
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 28)---
```python
# LR & Optimizer
N_SAMPLES = len(train)
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)

# Cosine Learning Rate With Warmup
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)

print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')
```

---The following area is a Code cell (cell numver is 29)---
```python
# Set the data type for the optimizer's state (e.g., momentum buffers)
for state in OPTIMIZER.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:
            state[v] = v.to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 30)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)

print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')
print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')
print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')
```

---The following area is a Code cell (cell numver is 31)---
```python
%%time
# Dummy Prediction
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    
print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')
```

---The following area is a Code cell (cell numver is 32)---
```python
# Put Model In Train Mode
model.train()

# Loss Function, Cross Entropy
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 33)---
```python
st = time()
warnings.filterwarnings("error")
METRICS = {
    'loss': [],
    'accuracy': {'y_true': [], 'y_pred': [] }}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):
    ste = time()
    for step in range(STEPS_PER_EPOCH):
        # Zero Out Gradients
        OPTIMIZER.zero_grad()
        
        # Get Batch
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # Forward Pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # Logits Float32
        logits = outputs.logits.to(dtype=torch.float32)
        
        # Backward Pass
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))
        loss.backward()
        
        # optimizer step
        OPTIMIZER.step()
        xm.mark_step()
        
        # Update Learning Rate Scheduler
        lr_scheduler.step()
        
        # Update Metrics And Progress Bar
        METRICS['loss'].append(float(loss))
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()
        
        if (step + 1) % 200 == 0:  
            metrics = 'Âµ_loss: {:.3f}'.format(np.mean(METRICS['loss']))
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])
            metrics += ', Âµ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))
            lr = OPTIMIZER.param_groups[0]['lr']
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')
    
    print(f'\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )

    # If stopped, and to continue training in future on tpu we save model and optimizer
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    
    
    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')
```

---The following area is a Code cell (cell numver is 34)---
```python
plt.figure(figsize=(15, 6))
plt.plot(METRICS['loss'])    
plt.xlabel('Step per epoch')
plt.ylabel('Loss')
plt.title('Loss Plot step per epoch')    
plt.show()
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# Save Model
```

---The following area is a Code cell (cell numver is 36)---
```python
model = model.cpu()
torch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 96, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
# !pip install cupy-cuda11x>=12.0.0
# !pip install packaging>=22
# !pip install shapely>=2.0.1
# !pip install numpy<1.26
# !pip install scipy<1.12
!pip install transformers datasets accelerate peft

```

---The following area is a Code cell (cell numver is 2)---
```python
# You only need to run this once per machine
!pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/transformers.git
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q -U datasets scipy ipywidgets
!pip install datasets
```

---The following area is a Code cell (cell numver is 3)---
```python
from datasets import load_dataset

train_dataset = load_dataset('csv', data_files='/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 4)---
```python
from huggingface_hub import notebook_login

notebook_login()
```

---The following area is a Code cell (cell numver is 5)---
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map="auto")
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    model_max_length=512,
    padding_side="left",
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data = pd.read_csv('../input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('../input/lmsys-chatbot-arena/test.csv')
submission_data = pd.read_csv('../input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
train_data.shape,test_data.shape,submission_data.shape
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

def trim_endings(custom_string):
    return custom_string[:-2][2:]

def count_newlines(custom_string):
    return custom_string.count('\\n')

def word_counts(custom_string):
    return len(custom_string.split())


def apply_transformations(df):
    
    df['prompt'] =  df['prompt'].map(process)
    df['response_a'] =  df['response_a'].map(process)
    df['response_b'] =  df['response_b'].map(process)
    
    df['prompt'] =  df['prompt'].map(trim_endings)
    df['response_a'] =  df['response_a'].map(trim_endings)
    df['response_b'] =  df['response_b'].map(trim_endings)
    
    df['res_a_line_count'] = df['response_a'].map(count_newlines).astype(str)
    df['res_b_line_count'] = df['response_b'].map(count_newlines).astype(str)
    
    df['res_a_word_count'] = df['response_a'].map(word_counts).astype(str)
    df['res_b_word_count'] = df['response_b'].map(word_counts).astype(str)
    
    return df

def create_summary(df):
    df['summary'] = 'User prompt: ' + df['prompt'] +  '\n\n Model A :\n' + df['response_a'] +'\n\n Model A length:\n' + df['res_a_word_count'] +'\n\n Model A Line Count:\n' + df['res_a_line_count'] +'\n\nModel B:\n'  + df['response_b'] +'\n\n \n\nModel B length:\n'  + df['res_b_word_count'] +'\n\n \n\nModel B Line Count:\n'  + df['res_b_line_count']
    return df
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data = apply_transformations(train_data)
train_data = create_summary(train_data)
test_data = apply_transformations(test_data)    
test_data = create_summary(test_data)
```

---The following area is a Code cell (cell numver is 11)---
```python
train_data['summary'][2]
```

---The following area is a Code cell (cell numver is 12)---
```python
train_data['labels'] = train_data[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1)
train_data['labels']=train_data['label'].astype('category')
train_data['target']=train_data['label'].cat.codes
```

---The following area is a Code cell (cell numver is 13)---
```python
category_map = {code: category for code, category in enumerate(train_data['labels'].cat.categories)}
category_map
```

---The following area is a Code cell (cell numver is 14)---
```python
SEED = 7920
```

---The following area is a Code cell (cell numver is 15)---
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score
train_df = train_data[['id','summary','target']]
test_df = train_data[['id','summary']]

df_train,df_val = train_test_split(train_df,test_size=0.2,random_state=SEED,stratify=train_df['target'])
```

---The following area is a Code cell (cell numver is 16)---
```python
from datasets import Dataset, DatasetDict
dataset_train = Dataset.from_pandas(df_train,preserve_index=False)
dataset_val = Dataset.from_pandas(df_val,preserve_index=False)
dataset_test = Dataset.from_pandas(test_df,preserve_index=False)
```

---The following area is a Code cell (cell numver is 17)---
```python
 dataset_train_shuffled = dataset_train.shuffle(seed=SEED)
```

---The following area is a Code cell (cell numver is 18)---
```python
dataset = DatasetDict({
    'train': dataset_train_shuffled,
#     'val': dataset_val,
#     'test': dataset_test
})
dataset
```

---The following area is a Code cell (cell numver is 19)---
```python
df_train.target.value_counts(normalize=True)
```

---The following area is a Code cell (cell numver is 20)---
```python
class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()
class_weights=torch.tensor(class_weights)
class_weights=class_weights/class_weights.sum()
class_weights
```

---The following area is a Code cell (cell numver is 21)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    model_max_length=512,
    padding_side="left",
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 22)---
```python
MAX_LEN = 512
col_to_delete = ['summary']

def llama_preprocessing_function(examples):
    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)

tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)
tokenized_datasets = tokenized_datasets.rename_column("target", "labels")
tokenized_datasets.set_format("torch")
```

---The following area is a Code cell (cell numver is 23)---
```python
from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)
```

---The following area is a Code cell (cell numver is 24)---
```python
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

```

---The following area is a Code cell (cell numver is 25)---
```python
print(model)
```

---The following area is a Code cell (cell numver is 26)---
```python
from accelerate import FullyShardedDataParallelPlugin, Accelerator
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
```

---The following area is a Code cell (cell numver is 27)---
```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.05,  # Conventional
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

# Apply the accelerator. You can comment this out to remove the accelerator.
model = accelerator.prepare_model(model)
```

---The following area is a Code cell (cell numver is 28)---
```python
!pip install -q wandb -U

import wandb, os
wandb.login()

wandb_project = "mistral-finetune"
if len(wandb_project) > 0:
    os.environ["WANDB_PROJECT"] = wandb_project
```

---The following area is a Code cell (cell numver is 29)---
```python
if torch.cuda.device_count() > 1: # If more than 1 GPU
    model.is_parallelizable = True
    model.model_parallel = True
```

---The following area is a Code cell (cell numver is 30)---
```python
!nvcc --version
!pip uninstall torch torchvision torchaudio -y
!pip install torch==2.0.0+cu121 torchvision==0.15.0+cu121 torchaudio==2.0.0+cu121 -f https://download.pytorch.org/whl/nightly/cu121/torch_nightly.html

```

---The following area is a Code cell (cell numver is 31)---
```python
# Uninstall existing versions of torch, torchvision, and torchaudio
!pip uninstall torch torchvision torchaudio -y

# Install compatible versions with CUDA 11.2 support
!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu112/torch_stable.html

```

---The following area is a Code cell (cell numver is 32)---
```python
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

```

---The following area is a Code cell (cell numver is 33)---
```python
import torch

seed = 7920  # Replace with your desired seed

if torch.cuda.is_available():
    generator = torch.Generator(device='cuda').manual_seed(seed)
else:
    generator = torch.Generator().manual_seed(seed)

```

---The following area is a Code cell (cell numver is 34)---
```python
if torch.cuda.is_available():
  generator = torch.Generator('cuda').manual_seed(seed)
else:
  generator = torch.Generator().manual_seed(seed)
```

---The following area is a Code cell (cell numver is 35)---
```python
import transformers
class CustomTrainer(transformers.Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        # Ensure label_weights is a tensor
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)
        else:
            self.class_weights = None

    def compute_loss(self, model, inputs, return_outputs=False):
        # Extract labels and convert them to long type for cross_entropy
        labels = inputs.pop("labels").long()

        # Forward pass
        outputs = model(**inputs)

        # Extract logits assuming they are directly outputted by the model
        logits = outputs.get('logits')

        # Compute custom loss with class weights for imbalanced data handling
        if self.class_weights is not None:
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)
        else:
            loss = F.cross_entropy(logits, labels)

        return (loss, outputs) if return_outputs else loss
```

---The following area is a Code cell (cell numver is 36)---
```python
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}
```

---The following area is a Code cell (cell numver is 37)---
```python
training_args = transformers.TrainingArguments(
    output_dir = 'model_classification',
    learning_rate = 1e-4,
    per_device_train_batch_size = 8,
    per_device_eval_batch_size = 8,
    num_train_epochs = 1,
    weight_decay = 0.01,
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    load_best_model_at_end = True,
	logging_steps=10,
	max_steps=100,
    fp16=True,
    push_to_hub=False
)
```

---The following area is a Code cell (cell numver is 38)---
```python
collate_fn = transformers.DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 39)---
```python
trainer = CustomTrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_datasets['train'],
    tokenizer = tokenizer,
    data_collator = collate_fn,
    compute_metrics = compute_metrics,
    class_weights=class_weights,
)
```

---The following area is a Code cell (cell numver is 40)---
```python
!pip install torch
```

---The following area is a Code cell (cell numver is 41)---
```python
import transformers
from datetime import datetime
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# Check if CUDA is available
print("CUDA Available:", torch.cuda.is_available())
project = "mistral-finetune"
base_model_name = "mistral"
run_name = base_model_name + "-" + project
output_dir = "./" + run_name

tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_datasets['train'],
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=5,
        per_device_train_batch_size=2,
        gradient_checkpointing=True,
        gradient_accumulation_steps=4,
        max_steps=1000,
        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate
        logging_steps=50,
        fp16=False,
        optim="paged_adamw_8bit",
        logging_dir="./logs",        # Directory for storing logs
        save_strategy="steps",       # Save the model checkpoint every logging step
        save_steps=50,                # Save checkpoints every 50 steps
        evaluation_strategy="steps", # Evaluate the model every logging step
        eval_steps=50,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,                # Perform evaluation at the end of training
        report_to="wandb",           # Comment this out if you don't want to use weights & baises
        run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"          # Name of the W&B run (optional)
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()
```

---The following area is a Code cell (cell numver is 42)---
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,  # Mistral, same as before
    quantization_config=bnb_config,  # Same quantization config as before
    device_map="auto",
    trust_remote_code=True,
)

eval_tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    add_bos_token=True,
    trust_remote_code=True,
)
```

---The following area is a Code cell (cell numver is 43)---
```python
from peft import PeftModel

ft_model = PeftModel.from_pretrained(base_model, "mistral-viggo-finetune/checkpoint-1000")
```

---The following area is a Code cell (cell numver is 44)---
```python
eval_prompt = """Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']

### Target sentence:
Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?

### Meaning representation:
"""

model_input = eval_tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))
```

