** @@@ Jupyter Notebook numver 0, the number of votes :211 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 1, the number of votes :165 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is
This notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).
I used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.

## Result

I used `id % 5 == 0` as an evaluation set and used all the rest for training.

| subset | log loss |
| - | - |
| eval | 0.9371|
| LB | 0.941 |

## What is QLoRA fine-tuning?

In the conventional fine-tuning, weight ($\mathbf{W}$) is updated as follows:

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

where $L$ is a loss at this step and $\eta$ is a learning rate.

[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$ by factorizing $\Delta \mathbf{W}$ into two (much) smaller matrices, $\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$ and $\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$ with $r \ll \text{min}(\text{d}, \text{k})$.

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

During training, only $\mathbf{A}$ and $\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.

[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. 
Note that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.

1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.

## Note
It takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.
This notebook uses only 100 samples for demo purpose, but everything else is same as my setup.
```

---The following area is a Code cell (cell numver is 1)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 2  # global batch size is 8 
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 6)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 8)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 10)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 11)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 12)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 14)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 15)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 16)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 18)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 20)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 21)---
```python
train_idx, eval_idx = folds[config.fold_idx]

trainer = Trainer(
    args=training_args, 
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)
trainer.train()
```

** @@@ Jupyter Notebook numver 2, the number of votes :116 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.

## ðŸ†• Update in version 4
The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.
In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.
This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.
We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:
1. Flash Attention
2. Memory Efficient Attention
3. A PyTorch (naive) implementation

By default, all of those are enabled but we can also manually enable/disable certain backends.
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 3, the number of votes :97 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Submit LLM 34B Model in 5 hours!
This notebook demonstrates how to submit a LLM 34B model in only 5 hours! Amazing! The key tricks are:
* use vLLM (for speed)
* use AWQ 4bit quantization (to avoid GPU VRAM OOM)
* limit input size to 1024 tokens (for speed)
* limit output size to 1 token (for speed)
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Pip Install vLLM
The package vLLM is an incredibly fast LLM inference library! The vLLM that is installed in Kaggle notebooks will produce errors, therefore we need to reinstall vLLM. The code below was taken from notebook [here][1]

[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution
```

---The following area is a Code cell (cell numver is 2)---
```python
import os, math, numpy as np
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"
```

---The following area is a Code cell (cell numver is 3)---
```python
%%time
!pip uninstall -y torch
!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Load 34B Quantized Model with vLLM!
We will load and use LLM 34B Bagel [here][1]. This is a strong model.

[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2
```

---The following area is a Code cell (cell numver is 5)---
```python
import vllm

llm = vllm.LLM(
    "/kaggle/input/bagel-v3-343",
    quantization="awq",
    tensor_parallel_size=2, 
    gpu_memory_utilization=0.95, 
    trust_remote_code=True,
    dtype="half", 
    enforce_eager=True,
    max_model_len=1024,
    #distributed_executor_backend="ray",
)
tokenizer = llm.get_tokenizer()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load Test Data
During **commit** we load 128 rows of train to compute CV score. During **submit**, we load the test data.
```

---The following area is a Code cell (cell numver is 7)---
```python
import pandas as pd
VALIDATE = 128

test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv") 
if len(test)==3:
    test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
    test = test.iloc[:VALIDATE]
print( test.shape )
test.head(1)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Engineer Prompt
If we want to submit zero shot LLM, we need to experiment with different system prompts to improve CV score. If we finetune the model, then system is not as important because the model will learn from the targets what to do regardless of which system prompt we use.

We use a logits processor to force the model to output the 3 tokens we are interested in.
```

---The following area is a Code cell (cell numver is 9)---
```python
from typing import Any, Dict, List
from transformers import LogitsProcessor
import torch

choices = ["A","B","tie"]

KEEP = []
for x in choices:
    c = tokenizer.encode(x,add_special_tokens=False)[0]
    KEEP.append(c)
print(f"Force predictions to be tokens {KEEP} which are {choices}.")

class DigitLogitsProcessor(LogitsProcessor):
    def __init__(self, tokenizer):
        self.allowed_ids = KEEP
        
    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:
        scores[self.allowed_ids] += 100
        return scores
```

---The following area is a Code cell (cell numver is 10)---
```python
sys_prompt = """Please read the following prompt and two responses. Determine which response is better.
If the responses are relatively the same, respond with 'tie'. Otherwise respond with 'A' or 'B' to indicate which is better."""
```

---The following area is a Code cell (cell numver is 11)---
```python
SS = "#"*25 + "\n"
```

---The following area is a Code cell (cell numver is 12)---
```python
all_prompts = []
for index,row in test.iterrows():
    
    a = " ".join(eval(row.prompt, {"null": ""}))
    b = " ".join(eval(row.response_a, {"null": ""}))
    c = " ".join(eval(row.response_b, {"null": ""}))
    
    prompt = f"{SS}PROMPT: "+a+f"\n\n{SS}RESPONSE A: "+b+f"\n\n{SS}RESPONSE B: "+c+"\n\n"
    
    formatted_sample = sys_prompt + "\n\n" + prompt
    
    all_prompts.append( formatted_sample )
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Infer Test
We infer test using fast vLLM. We ask vLLM to output probabilties of the top 5 tokens considered to be predicted in the first token. We also limit prediction to 1 token to increase inference speed.

Based on the speed it takes to infer 128 train samples, we can deduce how long inferring 25,000 test samples will take.
```

---The following area is a Code cell (cell numver is 14)---
```python
%%time

from time import time
start = time()

logits_processors = [DigitLogitsProcessor(tokenizer)]
responses = llm.generate(
    all_prompts,
    vllm.SamplingParams(
        n=1,  # Number of output sequences to return for each prompt.
        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.
        temperature=0,  # randomness of the sampling
        seed=777, # Seed for reprodicibility
        skip_special_tokens=True,  # Whether to skip special tokens in the output.
        max_tokens=1,  # Maximum number of tokens to generate per output sequence.
        logits_processors=logits_processors,
        logprobs = 5
    ),
    use_tqdm = True
)

end = time()
elapsed = (end-start)/60. #minutes
print(f"Inference of {VALIDATE} samples took {elapsed} minutes!")
```

---The following area is a Code cell (cell numver is 15)---
```python
submit = 25_000 / 128 * elapsed / 60
print(f"Submit will take {submit} hours")
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# Extract Inference Probabilites
We now extract the probabilties of "A", "B", "tie" from the vLLM predictions.
```

---The following area is a Code cell (cell numver is 17)---
```python
results = []
errors = 0

for i,response in enumerate(responses):
    try:
        x = response.outputs[0].logprobs[0]
        logprobs = []
        for k in KEEP:
            if k in x:
                logprobs.append( math.exp(x[k].logprob) )
            else:
                logprobs.append( 0 )
                print(f"bad logits {i}")
        logprobs = np.array( logprobs )
        logprobs /= logprobs.sum()
        results.append( logprobs )
    except:
        #print(f"error {i}")
        results.append( np.array([1/3., 1/3., 1/3.]) )
        errors += 1
        
print(f"There were {errors} inference errors out of {i+1} inferences")
results = np.vstack(results)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Create Submission CSV
```

---The following area is a Code cell (cell numver is 19)---
```python
sub = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")

if len(test)!=VALIDATE:
    sub[["winner_model_a","winner_model_b","winner_tie"]] = results
    
sub.to_csv("submission.csv",index=False)
sub.head()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Compute CV Score
```

---The following area is a Code cell (cell numver is 21)---
```python
if len(test)==VALIDATE:
    true = test[['winner_model_a','winner_model_b','winner_tie']].values
    print(true.shape)
```

---The following area is a Code cell (cell numver is 22)---
```python
if len(test)==VALIDATE:
    from sklearn.metrics import log_loss
    print(f"CV loglosss is {log_loss(true,results)}" )
```

** @@@ Jupyter Notebook numver 4, the number of votes :60 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Note
- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

# Import
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
import torch
import sklearn
import numpy as np
import pandas as pd
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
```

---The following area is a Code cell (cell numver is 3)---
```python
MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'
WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'
MAX_LENGTH = 2400
BATCH_SIZE = 2
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
def tokenize(example, tokenizer):
    prompts = tokenizer(eval(example['prompt'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_a = tokenizer(eval(example['response_a'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_b = tokenizer(eval(example['response_b'], {"null": ""}), add_special_tokens=False)["input_ids"]
    assert len(prompts) == len(responses_a) == len(responses_b), "Lengths of prompts, responses_a, and responses_b do not match"
    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]
    prompt, response_a, response_b = [], [], []
    p_len, a_len, b_len = 0, 0, 0
    for p, a, b in zip(prompts, responses_a, responses_b):
        prompt.append(p)
        response_a.append(a)
        response_b.append(b)
        p_len += len(p)
        a_len += len(a)
        a_len += len(b)
        if p_len+a_len+b_len > MAX_LENGTH:
            break
    prompt = [item for sublist in reversed(prompt) for item in sublist]
    response_a = [item for sublist in reversed(response_a) for item in sublist]
    response_b = [item for sublist in reversed(response_b) for item in sublist]
    p_a_b_len = len(prompt) + len(response_a) + len(response_b)
    cut_len = p_a_b_len - MAX_LENGTH
    if cut_len>0:
        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]
        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]
        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]
    prompt = tokenizer('<prompt>: ', add_special_tokens=False)["input_ids"] + prompt
    response_a = tokenizer('\n\n<response_a>: ', add_special_tokens=False)["input_ids"] + response_a
    response_b = tokenizer('\n\n<response_b>: ', add_special_tokens=False)["input_ids"] + response_b
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]
    label_token_id = [128250]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 7)---
```python
%%time
tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        # remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer},
    )
    return tokenized_datasets
test_ds = load_data(test, tokenizer)
test_ds
```

---The following area is a Code cell (cell numver is 8)---
```python
data = test_ds.to_pandas()
data["max_len"] = data["input_ids"].apply(len)
data[:3]
```

---The following area is a Code cell (cell numver is 9)---
```python
data['input_ids'][0]
```

---The following area is a Code cell (cell numver is 10)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 12)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            fake_label_tokens_ids = torch.tensor([128250],device=shift_labels.device)
            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
#             index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
#             true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
#             true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:,label_tokens_ids]
#             loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 13)---
```python
# Load base model on GPU 0
device0 = torch.device('cuda:0')
base_model_0 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:0',
)
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:1',
)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 15)---
```python
# Get peft
model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0) 
model_0.eval()

model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)
model_1.eval()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []

    model.eval()
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        labels = tmp["labels"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        pad_labels=[]
        for label in labels:
            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])
            pad_labels.append(label)
        labels = torch.tensor(pad_labels).to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    df['winner_model_a'] = a_win
    df['winner_model_b'] = b_win
    df['winner_tie'] = tie
    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

data = data.sort_values("max_len", ascending=False)
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 19)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 5, the number of votes :58 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 7)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 9)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 10)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)

model = Llama3ForSFT.from_pretrained(
    model_path, 
    torch_dtype=torch.float16, 
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 12)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 14)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

** @@@ Jupyter Notebook numver 6, the number of votes :48 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
1 epoch takes ~3hrs 

Just save lora weights and load them in the following notebook for Inference on 2xT4s:

Note: Training on bfloat16 causes model to overfit, so training should be on float32 and inference on float16


https://www.kaggle.com/code/pranshubahadur/inference-tf-gemma-2-9b-lmsys

Inspiration:

https://www.kaggle.com/code/matthewdwatson/gemma-2-fine-tuning-and-inference

https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=_Peq7TnLtHse
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U keras-nlp tensorflow-text
# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.
!pip install -q -U tensorflow-cpu
```

---The following area is a Code cell (cell numver is 2)---
```python
import jax

jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
```

---The following area is a Code cell (cell numver is 4)---
```python
import keras
import keras_nlp
```

---The following area is a Code cell (cell numver is 5)---
```python
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (1, 8),
    ["batch", "model"],
    devices=keras.distribution.list_devices(),
)
```

---The following area is a Code cell (cell numver is 6)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
```

---The following area is a Code cell (cell numver is 7)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 8)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0])

raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)


train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1).apply(lambda x: '\n\nPROMPT\n\n' + x).apply(lambda x: remove_surrogates(x)),
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
})
```

---The following area is a Code cell (cell numver is 9)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)

```

---The following area is a Code cell (cell numver is 10)---
```python
#keras.config.set_floatx("bfloat16")

```

---The following area is a Code cell (cell numver is 11)---
```python
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1")
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 12)---
```python
gemma_lm.backbone.enable_lora(rank=16)

```

---The following area is a Code cell (cell numver is 13)---
```python
for layer in gemma_lm._backbone.layers[:16]:
    layer.trainable = False
```

---The following area is a Code cell (cell numver is 14)---
```python
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 15)---
```python
def preprocess_fn(text, label=None):
    preprocessed = gemma_lm._preprocessor(text, sequence_length=1024)[0]
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}, label if label is not None else text
```

---The following area is a Code cell (cell numver is 16)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

inputs = {
    "token_ids": keras.Input(shape=(1024,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(1024,), dtype=tf.int32, name="padding_mask"),
}
x = gemma_lm.backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 17)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,
                weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 18)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 19)---
```python
import tensorflow as tf
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values)).batch(4).map(preprocess_fn)
ds = ds.shuffle(ds.cardinality())

```

---The following area is a Code cell (cell numver is 20)---
```python
train_split = ds.take(int(len(ds)*0.9))
val_split = ds.skip(int(len(ds)*0.9)).take(int(len(ds)*0.1))
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=4)
```

---The following area is a Code cell (cell numver is 21)---
```python
import numpy as np
layer = model.get_layer(name='dense')
weights = layer.get_weights()
kernel, bias = weights

# Save the kernel and bias separately
np.save('dense_1_kernel.npy', kernel)
np.save('dense_1_bias.npy', bias)
model.layers[2].save_lora_weights("model.lora.h5")
```

** @@@ Jupyter Notebook numver 7, the number of votes :33 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS ZeroShot Prediction

This code uses LLama3 to make zero-shot predictions.

Instead of learning a classification header, it uses a carefully crafted prompt to predict which token among **A**, **B**, or **tie** is most likely to follow `###Answer:`.
```

---The following area is a Code cell (cell numver is 1)---
```python
import json
import pandas as pd
import numpy as np
from tqdm.auto import tqdm
import pickle
import random
import os
import sys

import transformers
from transformers import AdamW
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_cosine_schedule_with_warmup

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset

import sklearn.metrics
from sklearn.metrics import accuracy_score

os.environ["TOKENIZERS_PARALLELISM"] = "true"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)
```

---The following area is a Code cell (cell numver is 2)---
```python
class CFG:
    INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
    OUTPUT = "/kaggle/working"
    MODEL_ID = "/kaggle/input/llama-3/transformers/8b-hf/1"
    SEED = 42
    USE_TURN = 0
```

---The following area is a Code cell (cell numver is 3)---
```python
train_df = pd.read_csv(f"{CFG.INPUT_DIR}/train.csv")
train_df
```

---The following area is a Code cell (cell numver is 4)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained(
    CFG.MODEL_ID,
    torch_dtype=torch.float16,
    device_map='auto',
)
```

---The following area is a Code cell (cell numver is 5)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    CFG.MODEL_ID,
    use_fast=False,
    trust_remote_code=True,
    padding_side="left",
    pad_token="<|endoftext|>"
)
model.config.pad_token_id = model.config.eos_token_id
```

---The following area is a Code cell (cell numver is 6)---
```python
n_valid = len(train_df) // 5
sample_df = train_df.sample(n_valid, random_state=CFG.SEED)
sample_df.head()
```

---The following area is a Code cell (cell numver is 7)---
```python
results = []
for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):
    prompt = json.loads(row["prompt"])
    response_a = json.loads(row["response_a"])
    response_b = json.loads(row["response_b"])
    
    
    p = prompt[CFG.USE_TURN]
    a = response_a[CFG.USE_TURN]
    b = response_b[CFG.USE_TURN]
    
    if a is None or b is None or len(p.split()) < 3:
        continue
        
    p = " ... ".join(["none" if i is None else i for i in prompt])
    a = " ... ".join(["none" if i is None else i for i in response_a])
    b = " ... ".join(["none" if i is None else i for i in response_b])
    # Use head and tail
    p = p[:128] + " ... " + p[-128:]
    a = a[:256] + " ... " + a[-256:]
    b = b[:256] + " ... " + b[-256:]
    
    text = f"""### Instruction
Which model's answer is appropriate for the prompt?ã€€If both are appropriate, answer `tie`.

### Prompt
{p}

### A
{a}

### B
{b}

### Answer
"""

    toks = tokenizer(text)

    for k in toks.keys():
        toks[k] = torch.tensor(toks[k]).cuda()  

    with torch.no_grad():
        out = model(toks["input_ids"].unsqueeze(0))

    pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
    pred = out.logits[0, -1, pred_token_id].cpu().softmax(0).numpy()
    
    d = row.to_dict()
    d["predict"] = pred
    results.append(d)
```

---The following area is a Code cell (cell numver is 8)---
```python
results_df = pd.DataFrame(results)
results_df
```

---The following area is a Code cell (cell numver is 9)---
```python
results_df.to_csv("result.csv", index=None)
```

---The following area is a Code cell (cell numver is 10)---
```python
!ls
```

---The following area is a Code cell (cell numver is 11)---
```python
targets = results_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

predicts = np.array(results_df["predict"].tolist())
```

---The following area is a Code cell (cell numver is 12)---
```python
logloss = sklearn.metrics.log_loss(targets, predicts)
print(logloss)
```

---The following area is a Code cell (cell numver is 13)---
```python
!ls
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Next steps:

- Try a larger model (with better benchmark accuracy)
- Refine the prompt.
- SFT (Supervised Fine-Tuning) on competition data
- Use ensemble
```

** @@@ Jupyter Notebook numver 8, the number of votes :29 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## If it helps you, I hope you can give me a thumbs up

## Importing Libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
from threading import Thread
import gc
import os
import io
import json
import random
import pickle
import zipfile
import datetime
import time

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from IPython.display import display
import torch.nn.functional as F
import tokenizers
```

---The following area is a Code cell (cell numver is 3)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 2048
BATCH_SIZE = 4
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 6)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 11)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: 
    print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 32)---
```python
TRAIN = False
INFER = True 
SAVE = False
```

---The following area is a Code cell (cell numver is 33)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 34)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 35)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 36)---
```python
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat), 
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
```

---The following area is a Code cell (cell numver is 37)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.05 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_wt = 0.05
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 42)---
```python
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 43)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 9, the number of votes :25 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
import os
```

---The following area is a Code cell (cell numver is 4)---
```python
from threading import Thread
import gc
import os
import io
import json
import random
import pickle
import zipfile
import datetime
import time

import torch
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from IPython.display import display
import torch.nn.functional as F
import tokenizers
```

---The following area is a Code cell (cell numver is 5)---
```python
assert torch.cuda.device_count() == 2

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# infer
```

---The following area is a Code cell (cell numver is 7)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Code cell (cell numver is 13)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Code cell (cell numver is 14)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 15)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 16)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 17)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Importing Libraries
```

---The following area is a Code cell (cell numver is 19)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Tokenize

## Load model 
> We load 1 model on each gpu.  

## Inference
```

---The following area is a Code cell (cell numver is 21)---
```python
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = result_df[TARGETS]
```

---The following area is a Code cell (cell numver is 22)---
```python
llama_preds = result_df[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 24)---
```python
TAG = 'lmsys-chatbot-arena'
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: 
    print('kaggle')
```

---The following area is a Code cell (cell numver is 25)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 26)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 27)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 28)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 29)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 30)---
```python
params = {}
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 31)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 32)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 33)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 34)---
```python
TRAIN = False
INFER = True 
SAVE = False
```

---The following area is a Code cell (cell numver is 35)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 36)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 37)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat), 
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 40)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.12 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 43)---
```python
lgb_wt = 0.6
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 44)---
```python
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 45)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 10, the number of votes :22 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import tensorflow as tf
from datasets import load_dataset, DatasetDict
from transformers import BertTokenizer, BertTokenizerFast, TFBertModel, DataCollatorWithPadding, TFAutoModel
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
import shutil
import pandas as pd
from tqdm.keras import TqdmCallback
import re
import math
import matplotlib.pyplot as plt
import multiprocessing
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from tensorflow.keras.callbacks import LearningRateScheduler
import numpy as np
from tensorflow import keras
from tensorflow.keras.optimizers import Adam
```

---The following area is a Code cell (cell numver is 1)---
```python
# Check for GPU availability
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        print(e)
```

---The following area is a Code cell (cell numver is 2)---
```python
# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.MirroredStrategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)
```

---The following area is a Code cell (cell numver is 3)---
```python
# path for sets
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'

# loading datasets
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# saving ID
test_ids = test_dataset['id']
```

---The following area is a Code cell (cell numver is 4)---
```python
# adding missing columns in the test set
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    if col not in test_dataset.column_names:
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# transformation to int64
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 5)---
```python
# using bert-base-cased's files locally
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'

model_dir = '/kaggle/working/bert-base-cased'
os.makedirs(model_dir, exist_ok=True)

shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)
```

---The following area is a Code cell (cell numver is 6)---
```python
stopwords_path = '/kaggle/input/stopwords/stopwords/english'

# Ð¤ÑƒÐ½ÐºÑ†Ð¸Ñ Ð´Ð»Ñ Ð·Ð°Ð³Ñ€ÑƒÐ·ÐºÐ¸ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð² Ð¸Ð· Ñ„Ð°Ð¹Ð»Ð°
def load_stopwords(stopwords_path):
    with open(stopwords_path, 'r') as file:
        stopwords = file.read().splitlines()
    return set(stopwords)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Initialize the tokenizer
tokenizer = BertTokenizerFast.from_pretrained(model_dir)
# download stopwords
stopwords = load_stopwords(stopwords_path)
# Function for text cleaning
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@\w+|\#','', text)
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ Ð¿ÑƒÐ½ÐºÑ‚ÑƒÐ°Ñ†Ð¸ÑŽ
    text = ' '.join([word for word in text.split() if word not in stopwords])  # Ð£Ð´Ð°Ð»Ð¸Ñ‚ÑŒ ÑÑ‚Ð¾Ð¿-ÑÐ»Ð¾Ð²Ð°
    return text

```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_function(examples):
    # Clean each text field
    cleaned_prompts = [clean_text(text) for text in examples['prompt']]
    cleaned_responses_a = [clean_text(text) for text in examples['response_a']]
    cleaned_responses_b = [clean_text(text) for text in examples['response_b']]
    
    # Tokenize the cleaned texts
    return tokenizer(cleaned_prompts,
                     cleaned_responses_a,
                     cleaned_responses_b,
                     padding="max_length", 
                     truncation=True, 
                     max_length=512)
```

---The following area is a Code cell (cell numver is 9)---
```python
# ÐŸÑ€Ð¸Ð¼ÐµÑ€ Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ð½Ð¸Ñ Ñ„ÑƒÐ½ÐºÑ†Ð¸Ð¸
examples = {
    'prompt': ["This is a sample prompt."],
    'response_a': ["This is a sample response A."],
    'response_b': ["This is a sample response B."]
}

tokenized_output = tokenize_function(examples)
print(tokenized_output)
```

---The following area is a Code cell (cell numver is 10)---
```python
# apply the tokenization and cleaning function with multiprocessing num_proc=num_proc
num_proc = multiprocessing.cpu_count()

# add try-except block for better error handling
try:
    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
    test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)
except Exception as e:
    print(f"Error during tokenization: {e}")
    
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 11)---
```python
# Add debug prints after tokenization
print("Sample tokenized train dataset entry:")
print(tokenized_datasets[0])
if len(tokenized_datasets) == 0:
    raise ValueError("The tokenized training dataset is empty.")
if len(test_tokenized_datasets) == 0:
    raise ValueError("The tokenized test dataset is empty.")
# Print column names for debugging
print(f"Tokenized training dataset columns: {tokenized_datasets.column_names}")
print(f"Tokenized test dataset columns: {test_tokenized_datasets.column_names}")
```

---The following area is a Code cell (cell numver is 12)---
```python
# convert to tf.data.Dataset with the correct shape
def convert_to_tf_dataset(dataset, label_col=None, for_inference=False):
    input_columns = tokenizer.model_input_names
    
    if label_col and not for_inference:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in input_columns])
    else:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in input_columns])
    
    # ensure labels are not sequences
    if label_col:
        dataset = dataset.map(lambda x: {label_col: int(x[label_col])})
    
    shuffle = not for_inference
    batch_size = 16 if for_inference else 450

    tf_dataset = dataset.to_tf_dataset(
        columns=input_columns,
        label_cols=[label_col] if label_col and not for_inference else None,
        shuffle=shuffle,
        batch_size=batch_size,
        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)
    )

    return tf_dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
# run the conversion
try:
    train_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
    test_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
except Exception as e:
    print(f"Error during dataset conversion: {e}")
```

---The following area is a Code cell (cell numver is 14)---
```python
# add debug prints after dataset conversion
print("Sample from converted train tf.data.Dataset:")
for batch in train_tf_dataset.take(1):
    inputs, labels = batch
    print(f'Input IDs shape: {inputs["input_ids"].shape}')
    print(f'Attention mask shape: {inputs["attention_mask"].shape}')
    print(f'Labels shape: {labels.shape}')

```

---The following area is a Code cell (cell numver is 15)---
```python
# building a custom model
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)
    
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state

def create_keras_model():
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    bert_output = BertLayer()([input_ids, attention_mask])
    pooled_output = GlobalAveragePooling1D()(bert_output)
    output = Dense(3, activation='softmax')(pooled_output)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model
```

---The following area is a Code cell (cell numver is 16)---
```python
with strategy.scope():
    model = create_keras_model()
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    model.fit(train_tf_dataset, epochs=3, callbacks=[TqdmCallback(verbose=1)])
```

---The following area is a Code cell (cell numver is 17)---
```python
# getting prediction
predictions = model.predict(test_tf_dataset)

# check lengths
print(f"Length of test_ids: {len(test_ids)}")
print(f"Shape of predictions: {predictions.shape}")
if len(test_ids) != predictions.shape[0]:
    predictions = predictions[:len(test_ids)]

# creating DataFrame
submission = pd.DataFrame({
    'id': test_ids,
    'winner_model_a': predictions[:, 0],
    'winner_model_b': predictions[:, 1],
    'winner_model_tie': predictions[:, 2]
})

# saving DataFrame
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 11, the number of votes :21 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Gemma 2 - 9b
We train a simple classifier in this using computed embeddings as input from [here](https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1?scriptVersionId=186083288) and compute embeddings for test and use trained classifier for inference. Let's get started!

Upvote if you found this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import Libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss

from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Train Classifier
```

---The following area is a Code cell (cell numver is 5)---
```python
train_df = pd.read_csv('/kaggle/input/gemma-2-9b-part-1/train_embed.csv')
train_embed = np.load('/kaggle/input/gemma-2-9b-part-1/gemma2_train_embed.npy')

train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 6)---
```python
# splits
Targets = ['winner_model_a','winner_model_b','winner_tie']

y = train_df['label'].values
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Here we just use classifier with default settings, try out tuning the params) 

model_cb = CatBoostClassifier(
    iterations=1000,
    learning_rate=0.03,
    loss_function='MultiClass',
    eval_metric='MultiClass',
    early_stopping_rounds=10,
    task_type='GPU',
    devices='0:1',
    verbose=100)

model_cb.fit(X_train, y_train, 
          eval_set=(X_test, y_test),
          early_stopping_rounds=50)
```

---The following area is a Code cell (cell numver is 8)---
```python
y_pred_proba = model_cb.predict_proba(X_test)
y_pred = model_cb.predict(X_test)

# Evaluate the model
logloss = log_loss(y_test, y_pred_proba)
accuracy = accuracy_score(y_test, y_pred)
gc.collect()

print(f'Log Loss: {logloss:.3f}')
print(f'Accuracy: {accuracy:.3f}')
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
We'll use this classifer for inference.
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# Load Gemma 2
```

---The following area is a Code cell (cell numver is 11)---
```python
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
MAX_LENGTH = 1024
BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)     
```

---The following area is a Code cell (cell numver is 12)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',
                   max_length=MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')


data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in  tokens['attention_mask']]
data[:2]
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Get embeddings
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 17)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 18)---
```python
test_embeddings = torch.cat([results[0], results[1]], dim=0)
test_embeddings.shape
```

---The following area is a Code cell (cell numver is 19)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 21)---
```python
preds = model_cb.predict_proba(test_embeddings.numpy())
preds
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# Submission
```

---The following area is a Code cell (cell numver is 23)---
```python
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
sample_sub[Targets] =  preds

display(sample_sub)
```

---The following area is a Code cell (cell numver is 24)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# Conclusion 

That's it! Just wanted to share the idea! Try out tuning the classifier or using other classifier. Thanks!

If you learned something, Please upvote:)
```

** @@@ Jupyter Notebook numver 12, the number of votes :19 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS | XGB Baseline

# 1. Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 2. Configuration
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 3. Loading Data
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 4. Feature Engineering
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))
            vectorizer.fit([text1, text2])
            output = vectorizer.transform([text1, text2]).toarray()
            cos_sim = cosine_similarity(output)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n') 
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"] 
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"] 
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"] 
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]
        
        return data
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time
preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 5. Modeling
```

---The following area is a Code cell (cell numver is 11)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))
cv_scores = list()

features = X.columns.tolist()
feat_imp_df = pd.DataFrame({"feature": features})

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed
    )
    
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    val_preds = model.predict_proba(X_val)
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")
    cv_scores.append(val_log_loss)
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances of All Folds", size=12)
plt.show()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 6. Saving Submission
```

---The following area is a Code cell (cell numver is 14)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 13, the number of votes :19 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import logging
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset
from sklearn.metrics import log_loss
import torch
from functools import partial
import warnings
from transformers import logging as transformers_logging
from transformers import EarlyStoppingCallback
import json
from pprint import pformat
from tqdm import trange
warnings.simplefilter('ignore')

TYPE = "large"
VER= 14
DATE = "0717"
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"

# Set up logging
transformers_logging.set_verbosity_error()
logging.basicConfig(level=logging.INFO, filename=f'logs_v{VER}.log', filemode='a',
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PATHS:
    train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
    test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
    sub_path = '/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'
    model_name = f"deberta-v3-{TYPE}"
    model_path = f"/root/autodl-tmp/ase2/huggingfacedebertav3variants/{model_name}"
    tokenizer_path = f"/kaggle/input/lmsys-{TYPE}{VER}-{DATE}/fold_0/tokenizer"
    general_tokenizer = "/kaggle/input/lmsys-base4-0704/fold_0/tokenizer"

class CFG:
    seed = 42
    max_length = 512
    lr = 5e-5  # å­¦ä¹ çŽ‡
    weight_decay = 0.01  # æƒé‡è¡°å‡
    warmup_ratio = 0 # å­¦ä¹ çŽ‡é¢„çƒ­æ¯”ä¾‹
    max_grad_norm = 1000  # æ¢¯åº¦è£å‰ªæœ€å¤§èŒƒæ•°
    lr_scheduler_type = 'linear'  # å­¦ä¹ çŽ‡è°ƒåº¦ç±»åž‹
    frozen_embedding = False # å†»ç»“å‰é¢çš„å±‚
    frozen_num = 6
    train_batch_size = 32  # è®­ç»ƒæ‰¹é‡å¤§å°
    eval_batch_size = 64  # è¯„ä¼°æ‰¹é‡å¤§å°
    evaluation_strategy = 'steps'  # æ›´æ”¹ä¸º steps è¯„ä¼°ç­–ç•¥
    metric_for_best_model = "eval_log_loss"  # ç”¨äºŽé€‰æ‹©æœ€ä½³æ¨¡åž‹çš„åº¦é‡æ ‡å‡†
    save_strategy = 'steps'  # æ›´æ”¹ä¸º steps ä¿å­˜ç­–ç•¥
    save_steps = 200  # æ¯ æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹
    save_total_limit = 1  # ä¿å­˜æ£€æŸ¥ç‚¹æ€»æ•°é™åˆ¶
    train_epochs = 5  # è®­ç»ƒå‘¨æœŸæ•°
    num_labels = 6
    output_dir = f'/kaggle/input/lmsys-{TYPE}{VER}-{DATE}'  # è¾“å‡ºç›®å½•
    fp16 = True  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
    load_best_model_at_end = True  # è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡åž‹
    report_to = 'none'  # ä¸æŠ¥å‘Šè®­ç»ƒæ—¥å¿—åˆ°å¤–éƒ¨å·¥å…·
    optim = 'adamw_torch'  # ä¼˜åŒ–å™¨ç±»åž‹
    logging_first_step = True  # è®°å½•ç¬¬ä¸€æ­¥çš„æ—¥å¿—
    logging_steps = 200  # æ¯ æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
    logging_dir =f'logs_v{VER}'  # æ—¥å¿—ä¿å­˜ç›®å½•
    n_splits = 5
    model_name = PATHS.model_name
    greater_is_better = False
    early_stop = False
    early_stopping_patience = 3  # Number of evaluation calls with no improvement after which training will be stopped
    early_stopping_threshold = 0.001  # Minimum change to qualify as an improvement

def seed_everything(seed):
    import random
    import os
    import numpy as np
    import torch
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=CFG.seed)

# tokenizer = AutoTokenizer.from_pretrained(PATHS.tokenizer_path)
tokenizer = AutoTokenizer.from_pretrained(PATHS.general_tokenizer)
sep_token = tokenizer.sep_token_id

def log_parameters(logger):
    """Log all parameters from PATHS and CFG classes."""
    logger.info("=== Parameter Settings ===")
    
    logger.info("PATHS:")
    for key, value in PATHS.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")
    
    logger.info("CFG:")
    for key, value in CFG.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")
    
    logger.info("=*100")

def tokenize_function(row, tokenizer):
    max_len = CFG.max_length - 2 # We need 2 separator tokens
    tokens_prompt = tokenizer(row['prompt'], truncation=True, max_length=max_len//4, add_special_tokens=False)['input_ids']
    remaining_length = max_len - len(tokens_prompt)
    
    tokens_response_a = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2, add_special_tokens=False)['input_ids']
    remaining_length -= len(tokens_response_a)
    tokens_response_b = tokenizer(row['response_b'], truncation=True, max_length=remaining_length, add_special_tokens=False)['input_ids']
    
    input_ids = [tokenizer.cls_token_id] + tokens_prompt + [sep_token] + tokens_response_a + [sep_token] + tokens_response_b
    token_type_ids = [0] * (len(tokens_prompt) + 2) + [1] * (len(tokens_response_a) + 1) + [2] * len(tokens_response_b)
    attention_mask = [1] * len(input_ids)
    
    padding_length = CFG.max_length - len(input_ids)
    if padding_length > 0:
        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
        token_type_ids = token_type_ids + [0] * padding_length
        attention_mask = attention_mask + [0] * padding_length
    
    return {
        'input_ids': input_ids[:CFG.max_length],
        'token_type_ids': token_type_ids[:CFG.max_length],
        'attention_mask': attention_mask[:CFG.max_length],
    }

def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)
    labels[df['winner_model_a'] == 1] = 0
    labels[df['winner_model_b'] == 1] = 1
    labels[df['winner_tie'] == 1] = 2
    df['labels'] = labels
    return df

def process_data(df, mode='train'):
    dataset = Dataset.from_pandas(df)
    tokenized_dataset = dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=False)
    remove_cols = ['id', 'prompt', 'response_a', 'response_b']
    if mode == 'train':
        remove_cols += ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']
    tokenized_dataset = tokenized_dataset.remove_columns(remove_cols)
    return tokenized_dataset

def split_train_val(dataset, train_fraction):
    np.random.seed(0)
    ixs = np.arange(len(dataset))
    cutoff = int(len(ixs) * train_fraction)
    np.random.shuffle(ixs)
    ixs_train = ixs[:cutoff]
    ixs_val = ixs[cutoff:]
    fit_train = dataset.select(ixs_train)
    fit_val = dataset.select(ixs_val)
    return fit_train, fit_val


def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    return {
        'eval_log_loss': log_loss(labels, probabilities),
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()
    }
    
def train_model():
    log_parameters(logger)
    train_df = pd.read_csv(PATHS.train_path)
    train_df = add_label(train_df)
    train_tokenized = process_data(train_df, mode='train')
    
    skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_tokenized, train_tokenized['labels'])):
        print(f"Training fold {fold + 1}")
        logger.info(f"Training fold {fold + 1}")
        
        fit_train = train_tokenized.select(train_idx)
        fit_val = train_tokenized.select(val_idx)
        
        model = AutoModelForSequenceClassification.from_pretrained(
            PATHS.model_path,
            num_labels=3,
            problem_type="single_label_classification"
        )
        
        training_args = TrainingArguments(
            output_dir=f"{CFG.output_dir}/fold_{fold}",  # æ¨¡åž‹å’Œæ£€æŸ¥ç‚¹çš„è¾“å‡ºç›®å½•
            fp16=CFG.fp16,  # ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ
            learning_rate=CFG.lr,  # å­¦ä¹ çŽ‡
            per_device_train_batch_size=CFG.train_batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è®­ç»ƒæ‰¹é‡å¤§å°
            per_device_eval_batch_size=CFG.eval_batch_size,  # æ¯ä¸ªè®¾å¤‡ä¸Šçš„è¯„ä¼°æ‰¹é‡å¤§å°
            num_train_epochs=CFG.train_epochs,  # è®­ç»ƒçš„æ€»å‘¨æœŸæ•°
            weight_decay=CFG.weight_decay,  # æƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
            evaluation_strategy=CFG.evaluation_strategy,  # è¯„ä¼°ç­–ç•¥
            metric_for_best_model=CFG.metric_for_best_model,  # ç”¨äºŽé€‰æ‹©æœ€ä½³æ¨¡åž‹çš„åº¦é‡æ ‡å‡†
            save_strategy=CFG.save_strategy,  # ä¿å­˜ç­–ç•¥
            save_total_limit=CFG.save_total_limit,  # ä¿å­˜çš„æ£€æŸ¥ç‚¹æ€»æ•°é™åˆ¶
            load_best_model_at_end=CFG.load_best_model_at_end,  # åœ¨è®­ç»ƒç»“æŸæ—¶åŠ è½½æœ€ä½³æ¨¡åž‹
            report_to=CFG.report_to,  # ä¸æŠ¥å‘Šè®­ç»ƒæ—¥å¿—åˆ°å¤–éƒ¨å·¥å…·
            warmup_ratio=CFG.warmup_ratio,  # å­¦ä¹ çŽ‡é¢„çƒ­æ¯”ä¾‹
            lr_scheduler_type=CFG.lr_scheduler_type,  # å­¦ä¹ çŽ‡è°ƒåº¦ç±»åž‹
            optim=CFG.optim,  # ä½¿ç”¨çš„ä¼˜åŒ–å™¨ç±»åž‹
            logging_first_step=CFG.logging_first_step,  # è®°å½•ç¬¬ä¸€æ­¥çš„æ—¥å¿—
            greater_is_better=CFG.greater_is_better,
            
            # max_grad_norm=CFG.max_grad_norm,  # è®¾ç½®æ¢¯åº¦è£å‰ª
            
            logging_steps=CFG.logging_steps,  # æ¯ 500 æ­¥è®°å½•ä¸€æ¬¡æ—¥å¿—
            logging_dir=CFG.logging_dir,  # æ—¥å¿—ä¿å­˜ç›®å½•
        
            save_steps=CFG.save_steps,  # æ¯  æ­¥ä¿å­˜ä¸€æ¬¡æ¨¡åž‹
            eval_steps=CFG.save_steps,  # æ·»åŠ  eval_steps å‚æ•°,ä¸Ž save_steps ä¿æŒä¸€è‡´
        )

         # Log training arguments
        logger.info("Training arguments:")
        logger.info(pformat(training_args.to_dict()))

        if CFG.frozen_embedding:
            n = CFG.frozen_num
            # å†»ç»“åµŒå…¥å±‚
            for i, layer in enumerate(model.deberta.encoder.layer[:n]):
                for param in layer.parameters():
                    param.requires_grad = False # True False
            for param in model.deberta.embeddings.parameters():
                param.requires_grad = False

        # åˆå§‹åŒ– tokenizer
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

        # Create EarlyStoppingCallback
        if CFG.early_stop:
            early_stopping_callback = EarlyStoppingCallback(
                early_stopping_patience=CFG.early_stopping_patience,
                early_stopping_threshold=CFG.early_stopping_threshold,
            )
        
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=fit_train,
                data_collator=data_collator,
                eval_dataset=fit_val,
                compute_metrics=compute_metrics,
                callbacks=[early_stopping_callback],  # Add the early stopping callback
            )
        else:
            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=fit_train,
                data_collator=data_collator,
                eval_dataset=fit_val,
                compute_metrics=compute_metrics,
            )
        
        trainer.train()
        
        # Save the model
        trainer.save_model(f"{CFG.output_dir}/fold_{fold}/best_model")
        tokenizer.save_pretrained(f"{CFG.output_dir}/fold_{fold}/tokenizer")
        
        # Log the results
        eval_result = trainer.evaluate()
        logger.info(f"Fold {fold + 1} - Evaluation result: {eval_result}")
        logger.info("=*100")

def predict_test():
    test_df = pd.read_csv(PATHS.test_path)
    test_tokenized = process_data(test_df, mode='test')
    
    predictions = []
    
    for fold in trange(CFG.n_splits):
        model = AutoModelForSequenceClassification.from_pretrained(f"{CFG.output_dir}/fold_{fold}/best_model")
        model.eval()
        
        trainer = Trainer(model=model)
        fold_preds = trainer.predict(test_tokenized).predictions
        fold_preds = np.exp(fold_preds) / np.sum(np.exp(fold_preds), axis=1, keepdims=True)
        predictions.append(fold_preds)
    
    # Average predictions across folds
    final_preds = np.mean(predictions, axis=0)
    display(predictions)
    logger.info(f"Final_preds: {final_preds}")
    
    # Create submission file
    submission = pd.DataFrame({
        'id': test_df['id'],
        'winner_model_a': final_preds[:, 0],
        'winner_model_b': final_preds[:, 1],
        'winner_tie': final_preds[:, 2]
    })
    
    submission.to_csv('submission.csv', index=False)
    display(submission)
```

---The following area is a Code cell (cell numver is 1)---
```python
%time
if __name__ == "__main__":
#     train_model()
    predict_test()
```

---The following area is a Code cell (cell numver is 2)---
```python
# import tokenizers
# print(tokenizers.__version__)
```

---The following area is a Code cell (cell numver is 3)---
```python

```

** @@@ Jupyter Notebook numver 14, the number of votes :16 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
"""

Credits:

https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora

LB: 0.945
"""
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import (
    Gemma2ForSequenceClassification, GemmaTokenizerFast, 
    AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
)
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel, get_peft_model, LoraConfig, TaskType

torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
```

---The following area is a Code cell (cell numver is 3)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    gemma_lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    llama_model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    llama_weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    tta = False
    spread_max_length = False

cfg = Config()
```

---The following area is a Code cell (cell numver is 4)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)
```

---The following area is a Code cell (cell numver is 5)---
```python
def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):
    if isinstance(tokenizer, GemmaTokenizerFast):
        prompt = ["<prompt>: " + p for p in prompt]
        response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
        response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    else:
        prompt = ["User prompt: " + p for p in prompt]
        response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
        response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 6)---
```python
# Gemma Tokenizer
gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
gemma_tokenizer.add_eos_token = True
gemma_tokenizer.padding_side = "right"

# Llama Tokenizer
llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# Prepare data for both models
gemma_data = pd.DataFrame()
gemma_data["id"] = test["id"]
gemma_data["input_ids"], gemma_data["attention_mask"] = tokenize(gemma_tokenizer, test["prompt"], test["response_a"], test["response_b"])
gemma_data["length"] = gemma_data["input_ids"].apply(len)

llama_data = pd.DataFrame()
llama_data["id"] = test["id"]
llama_data["input_ids"], llama_data["attention_mask"] = tokenize(llama_tokenizer, test["prompt"], test["response_a"], test["response_b"])
llama_data["length"] = llama_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Load Gemma model on GPU 0
device_0 = torch.device('cuda:0')
gemma_model = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False
)
gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)
```

---The following area is a Code cell (cell numver is 8)---
```python
# Load Llama model on GPU 1
device_1 = torch.device('cuda:1')
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False
)
llama_base_model = LlamaForSequenceClassification.from_pretrained(
    cfg.llama_model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id

peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)
llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)
llama_model.eval()
```

---The following area is a Code cell (cell numver is 9)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 10)---
```python
st = time.time()

# Sort data by input length
gemma_data = gemma_data.sort_values("length", ascending=False)
llama_data = llama_data.sort_values("length", ascending=False)

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, 
                           (gemma_data, llama_data), 
                           (gemma_model, llama_model),
                           (gemma_tokenizer, llama_tokenizer),
                           (device_0, device_1))

gemma_result_df, llama_result_df = list(results)

# Combine results (simple average)
combined_result_df = gemma_result_df.copy()
combined_result_df["winner_model_a"] = (gemma_result_df["winner_model_a"] + llama_result_df["winner_model_a"]) / 2
combined_result_df["winner_model_b"] = (gemma_result_df["winner_model_b"] + llama_result_df["winner_model_b"]) / 2
combined_result_df["winner_tie"] = (gemma_result_df["winner_tie"] + llama_result_df["winner_tie"]) / 2

print(f"Inference time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 11)---
```python
submission_df = combined_result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df.head())
```

** @@@ Jupyter Notebook numver 15, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS | XGB Baseline

(original notebook: https://www.kaggle.com/code/sercanyesiloz/lmsys-xgb-baseline)

# 1. Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 2. Configuration
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 3. Loading Data
```

---The following area is a Code cell (cell numver is 5)---
```python
# Read the training, test, and sample submission datasets from the specified paths
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

# If the test dataset has fewer than 10 rows, limit the training dataset to the first 10,000 rows
if test.shape[0] < 10:
    train = train.iloc[:10000]

# Define a function to process strings by removing brackets and splitting sentences
# NOTE: Another way would be to convert to JSON and then join, but this is probably most efficient in Python
def process(input_str):
    stripped_str = input_str.strip('[]')  # Remove leading and trailing square brackets
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # Split by "," and remove surrounding quotes
    return ' '.join(sentences)  # Join the sentences with a space

# Apply the `process` function to the prompt and response columns in the train dataset
train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

# Apply the `process` function to the prompt and response columns in the test dataset
test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

# Print the shapes of the train and test datasets
print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)

# Print the total number of missing values in the train and test datasets
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

# Display the first few rows of the train dataset
train.head()

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 4. Feature Engineering
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    # Calculate cosine similarity between two texts
    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # Create a TF-IDF vectorizer (word-importance) with n-grams from 1 to 3
            vectorizer.fit([text1, text2])  # Fit the vectorizer on both texts
            output = vectorizer.transform([text1, text2]).toarray()  # Transform texts to TF-IDF vectors
            cos_sim = cosine_similarity(output)  # Calculate cosine similarity between vectors
            return cos_sim[0][1]  # Return the similarity score between text1 and text2
        except:
            print(f"cosine_sim exception with '{text1}' and '{text2}'")
            return np.nan  # Return NaN in case of an exception

    # Calculate Jaccard similarity between two texts
    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())  # Split text1 into set of words
        set2 = set(text2.split())  # Split text2 into set of words
        intersection = set1.intersection(set2)  # Find intersection of both sets
        union = set1.union(set2)  # Find union of both sets
        return len(intersection) / len(union)  # Return Jaccard similarity score
    
    # Count the number of quoted segments in a text
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"  # Pattern for single quotes
        double_quote_pattern = r'"(.*?)"'  # Pattern for double quotes
        single_quotes = re.findall(single_quote_pattern, text)  # Find all single-quoted segments
        double_quotes = re.findall(double_quote_pattern, text)  # Find all double-quoted segments
        total_quotes = len(single_quotes) + len(double_quotes)  # Sum the counts of both types of quotes
        return total_quotes  # Return the total count of quoted segments
    
    # Count the number of new-lines in a text
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n')  # Return the count of newline characters in the text
    
    # Count the number of bulleted lists in the text
    def count_bulleted_lists(self, text: str) -> int:
        bullet_pattern = r'(\\n|^)[\*\-\+]\s'  # Pattern for bulleted list items
        return len(re.findall(bullet_pattern, text))  # Return the count of bulleted list items
    
    # Tokenize text into lowercase words
    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    # Generate n-grams from the tokenized text
    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)  # Tokenize the text
        return list(ngrams(tokens, n))  # Generate n-grams from tokens

    # Count overlapping n-grams between two texts
    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # Generate n-grams for text1
            ngrams2 = self.generate_ngrams(text2, n)  # Generate n-grams for text2
            counter1 = Counter(ngrams1)  # Count n-grams in text1
            counter2 = Counter(ngrams2)  # Count n-grams in text2
            overlap = counter1 & counter2  # Find the overlap between the two counters
            overlap_count = sum(overlap.values())  # Sum the counts of overlapping n-grams
            return overlap_count  # Return the overlap count
        except:
            return 0  # Return 0 in case of an exception
        
    # Run preprocessing on the data
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # Calculate unigram, bigram, and trigram overlaps between response_a and response_b
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        # Calculate unigram, bigram, and trigram overlaps between response_a and prompt
        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        # Calculate unigram, bigram, and trigram overlaps between response_b and prompt
        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        # Calculate the length of tokenized texts
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        # Calculate length ratios between response_a, response_b, and prompt
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        # Calculate length differences between response_a, response_b, and prompt
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_a and prompt
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        # Calculate overlap ratios for unigrams, bigrams, and trigrams between response_b and prompt
        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        # Count the number of quotes in response_a, response_b, and prompt
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))

        # Count the number of new-lines in response_a, response_b, and prompt
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))

        # Count the number of bulleted lists in response_a, response_b, and prompt
        data["respa_bullets"] = data["response_a"].apply(lambda x: self.count_bulleted_lists(x))
        data["respb_bullets"] = data["response_b"].apply(lambda x: self.count_bulleted_lists(x))
        data["prompt_bullets"] = data["prompt"].apply(lambda x: self.count_bulleted_lists(x))
        
        # Calculate cosine and Jaccard similarities between response_a and response_b
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        # Calculate cosine and Jaccard similarities between response_a and prompt
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        # Calculate cosine and Jaccard similarities between response_b and prompt
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        return data  # Return the processed dataframe

```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
# List of columns to drop from the dataset
drop_cols = ["id", "response_a", "response_b", "prompt"]

# List of target columns indicating the winner
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]

# Name of the final target column
target = "target"

# Initialize the target column with NaN values
train[target] = np.nan

# Iterate over the target columns and set the corresponding index in the target column
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx  # Set target column to the index where target column value is 1

# Convert the target column to integer type
train[target] = train[target].astype("int32")

# Display the first few rows of the updated dataframe
train.head()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 5. Modeling
```

---The following area is a Code cell (cell numver is 11)---
```python
# Drop specified columns from the training dataset and assign the result to X
X = train.drop(columns=target_cols + drop_cols + [target] + ["model_a", "model_b"], axis=1)

# Assign the target column to y
y = train[target]

# Drop specified columns from the test dataset and assign the result to X_test
X_test = test.drop(columns=drop_cols, axis=1)

# Replace infinite values (-inf and inf) with NaN in the training feature set
X = X.replace([-np.inf, np.inf], np.nan)

# Replace infinite values (-inf and inf) with NaN in the test feature set
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Set up stratified cross-validation with the specified number of splits, shuffle, and seed
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)

# Initialize an array to store the average predictions for the test set
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))

# Initialize a list to store cross-validation scores (log loss) for each fold
cv_scores = list()

# Get the list of feature names
features = X.columns.tolist()

# Prepare a DataFrame to store feature importances for each fold
feat_imp_df = pd.DataFrame({"feature": features})

# Loop over each fold in the cross-validation
for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))  # Print the fold number

    # Split the data into training and validation sets for the current fold
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    # Print the shapes of the training and validation sets
    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    # Initialize the XGBoost classifier with specified hyperparameters
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed,
        device="gpu"
    )
    
    # Train the model with early stopping on the validation set
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    # Predict probabilities on the validation set
    val_preds = model.predict_proba(X_val)

    # Calculate the log loss for the validation set
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")

    # Append the log loss to the list of cross-validation scores
    cv_scores.append(val_log_loss)
    
    # Update test predictions with the current fold's predictions, averaged over all folds
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    # Merge the current fold's feature importances into the DataFrame
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

# Print a separator line and the average cross-validated log loss
print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

# Calculate the average feature importance across all folds
feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)

# Plot the top 50 features by average importance
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances for All Folds", size=12)
plt.show()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 6. Saving Submission
```

---The following area is a Code cell (cell numver is 14)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 16, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# ðŸ“š Importing Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# âš™ï¸ Configuration Class
```

---The following area is a Code cell (cell numver is 3)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# ðŸ“Š Loading and Processing Data

we will load our training and test datasets, and apply some preprocessing. This includes:


1. **Loading Data**: Read the CSV files into pandas DataFrames.
2. **Subsampling**: If the test dataset has less than 10 rows, subsample the training dataset to 10,000 rows for quicker processing.
3. **Processing Strings**: Clean and process the string columns (`prompt`, `response_a`, `response_b`) by removing unwanted characters.
4. **Shape and Missing Values**: Print the shape of the datasets and count missing values to understand the data structure and quality.
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# ðŸ› ï¸ Preprocessing Class Definition

This class, `Preprocessor`, contains several methods to process and feature-engineer the text data. Hereâ€™s a breakdown of its functionalities:

#### Cosine Similarity
- **Formula**: 
  $$\text{cosine_similarity} = \frac{A \cdot B}{\|A\| \|B\|}$$

- **Description**: Cosine similarity measures the cosine of the angle between two vectors, providing a metric of how similar the texts are.

#### Jaccard Similarity
- **Formula**:

  $$\text{Jaccard_similarity} = \frac{|A \cap B|}{|A \cup B|}$$

- **Description**: Jaccard similarity measures the similarity between two sets by comparing their intersection and union.

#### Count Quotes
- **Description**: This method identifies and counts both single and double quoted texts in a string, giving an idea of how many quotations are present.

#### Tokenize
- **Description**: This method splits the text into individual words (tokens), which can be used for further analysis like generating n-grams or calculating overlaps.

#### Generate N-grams
- **Description**: N-grams are contiguous sequences of 'n' items from a given text. This method helps in analyzing the text at different levels of granularity (unigrams, bigrams, trigrams, etc.).

#### Count N-gram Overlaps
- **Description**: This method calculates how many n-grams are common between two texts, helping to measure their similarity.

#### Run
- **Description**: This method processes the entire dataset, generating new features based on the above calculations, which can be used for training machine learning models.


$\frac{n!}{k!(n-k)!} = \binom{n}{k}$
```

---The following area is a Code cell (cell numver is 7)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer().fit_transform([text1, text2])
            vectors = vectorizer.toarray()
            cos_sim = cosine_similarity(vectors)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        return data
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Data Preparation
```

---The following area is a Code cell (cell numver is 10)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Code cell (cell numver is 11)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Handle missing values
imputer = SimpleImputer(strategy='mean')
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)
```

---The following area is a Code cell (cell numver is 13)---
```python
# Feature Selection
selector = SelectKBest(f_classif, k=25)
X_new = selector.fit_transform(X, y)
X_test_new = selector.transform(X_test)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# ðŸ§© Model Training and Evaluation

### Model Definitions

Defines several machine learning models including Random Forest, Gradient Boosting, SVM, XGBoost, CatBoost, and a Voting Classifier.

### Feature Selection

Uses SelectKBest to select 25 best features based on ANOVA F-value between feature and target.

### Cross-validation

Uses Stratified K-Fold cross-validation for model evaluation.

### Training and Evaluation

Iterates through each model, trains it using the training data, evaluates using cross-validation, and calculates the mean CV Log Loss.

### Feature Importances

Calculates and stores feature importances for applicable models (Random Forest, Gradient Boosting, XGBoost, CatBoost).

### Best Model Identification

Identifies the best performing model based on the lowest CV Log Loss.

### Results Display

Displays the results in a DataFrame showing the CV Log Loss for each model and, if applicable, feature importances.
```

---The following area is a Code cell (cell numver is 15)---
```python

# Define models and their configurations
models = {
    'random_forest': {
        'model': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=config.seed
        ),
        'params': {}
    },
    'gradient_boosting': {
        'model': GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            random_state=config.seed
        ),
        'params': {}
    },
    'svm': {
        'model': SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=config.seed
        ),
        'params': {}
    },
    'xgboost': {
        'model': xgb.XGBClassifier(
            objective='multi:softprob',
            num_class=3,
            eval_metric='mlogloss',
            subsample=0.8,
            n_estimators=650,
            learning_rate=0.045,
            max_depth=5,
            random_state=config.seed,
#             tree_method='gpu_hist'  # GPU acceleration if available
        ),
        'params': {}
    },
    'catboost': {
        'model': cb.CatBoostClassifier(
            loss_function='MultiClass',
            iterations=650,
            learning_rate=0.045,
            depth=5,
            random_seed=config.seed,
#             task_type="GPU",  # Use GPU if available
            verbose=75
        ),
        'params': {}
    },
    'voting': {
        'model': VotingClassifier(
            estimators=[
                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),
                ('svc', SVC(probability=True)),
                ('knn', KNeighborsClassifier(n_neighbors=5))
            ],
            voting='soft'
        ),
        'params': {}
    }
}

# Select features using SelectKBest
selector = SelectKBest(f_classif, k=25)
X_new = selector.fit_transform(X, y)
X_test_new = selector.transform(X_test)

# Cross-validation setup
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)

# Dataframe to store results
results = []

# Iterate over models
for model_name, model_data in models.items():
    model = model_data['model']
    print(f"Training model: {model_name}")

    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))
    cv_scores = []

    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):
        X_train, y_train = X_new[train_idx], y[train_idx]
        X_val, y_val = X_new[val_idx], y[val_idx]

        if model_name == 'voting':
            model.fit(X_train, y_train)
        elif model_name == 'catboost':
            model.fit(
                X_train,
                y_train,
                eval_set=[(X_train, y_train), (X_val, y_val)],
                early_stopping_rounds=75,
                verbose=75
            )
        else:
            model.fit(
                X_train,
                y_train
            )

        if model_name != 'voting':
            val_preds = model.predict_proba(X_val)
            val_log_loss = log_loss(y_val, val_preds, eps="auto")
            cv_scores.append(val_log_loss)

            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()

    if model_name != 'voting':
        mean_cv_log_loss = np.mean(cv_scores)
        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})
        print(f"Mean CV Log Loss: {mean_cv_log_loss:.5f}")

# Store feature importances if applicable
if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:
    features = X.columns[selector.get_support()].tolist()
    feat_imp_df = pd.DataFrame({"feature": features})
    feat_imp_df[f"{model_name}_avg_importance"] = 0

    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):
        X_val, _ = X_new[val_idx], y[val_idx]
        feat_imp_df[f"{model_name}_avg_importance"] += model.feature_importances_ / cv.get_n_splits()

    results_df = pd.DataFrame(results)
    results_df = pd.concat([results_df, feat_imp_df], axis=1)

# Convert results to DataFrame
results_df = pd.DataFrame(results)

# Identify the best model
best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]
print(f"\nBest Model:\n{best_model}")

# Display results DataFrame
print("\nResults DataFrame:")
print(results_df)

```

---The following area is a Code cell (cell numver is 16)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 17)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 17, the number of votes :13 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install transformers peft accelerate bitsandbytes \-U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# PART 1 (LLama 3 8b Chat)
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers
```

---The following area is a Code cell (cell numver is 4)---
```python
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 5)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 32)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 33)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 34)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 35)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 36)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 37)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 38)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 39)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 40)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 41)---
```python
lgb_preds = yp
```

---The following area is a Code cell (cell numver is 42)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 43)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# Part 2 (Gemma 2 4b QLora)
```

---The following area is a Code cell (cell numver is 45)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 47)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False 
    spread_max_length = False 

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
## Load & pre-process Data
```

---The following area is a Code cell (cell numver is 49)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 50)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))


test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 51)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 52)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # TODO: change prompt
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 53)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 54)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 55)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
## Load model
```

---The following area is a Code cell (cell numver is 57)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 58)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 59)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 61)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 62)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 63)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 64)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
```

---The following area is a Code cell (cell numver is 65)---
```python
result_df
```

---The following area is a Code cell (cell numver is 66)---
```python
out
```

---The following area is a Code cell (cell numver is 67)---
```python
merged_df = pd.merge(out, result_df, on='id')
```

---The following area is a Code cell (cell numver is 68)---
```python
merged_df
```

---The following area is a Code cell (cell numver is 69)---
```python
merged_df['winner_model_a']= (0.7*merged_df['winner_model_a_y']) + (0.3*merged_df['winner_model_a_x'])
merged_df['winner_model_b']= (0.7*merged_df['winner_model_b_y']) + (0.3*merged_df['winner_model_b_x'])
merged_df['winner_tie']= (0.7*merged_df['winner_tie_y']) + (0.3*merged_df['winner_tie_x'])
```

---The following area is a Code cell (cell numver is 70)---
```python
submission_df = merged_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 71)---
```markdown
credits: https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora
```

** @@@ Jupyter Notebook numver 18, the number of votes :12 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
Inference on float16 should take ???hrs


training nb:

https://www.kaggle.com/code/pranshubahadur/tf-gemma-2-9b-lmsys-training-tpu
```

---The following area is a Code cell (cell numver is 1)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
import keras
import keras_nlp
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (1, 2),
    ["batch", "model"],
    devices=['gpu:0', 'gpu:1'],
)
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
layout_map["position_embedding/embeddings"] = (model_dim, None)

# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)

layout_map["decoder_block.*layer_norm/scale"] = (model_dim,)
layout_map["decoder_block.*layer_norm/bias"] = (model_dim,)
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)

```

---The following area is a Code cell (cell numver is 2)---
```python
import jax
jax.default_device = jax.devices('cpu')[0]
jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
keras.config.set_dtype_policy("float16")

```

---The following area is a Code cell (cell numver is 4)---
```python

```

---The following area is a Code cell (cell numver is 5)---
```python
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1", trainable=False, dtype='int8')

gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 6)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 7)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_test_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
#raw_test_dataset[input_columns] = raw_test_dataset[input_columns].map(lambda x: eval(x)[0])
#raw_test_dataset =raw_test_dataset.dropna().reset_index(drop=True)



train_dataset = DataFrame({
    'text' : raw_test_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1).apply(lambda x: '\n\nPROMPT\n\n' + x).apply(lambda x: remove_surrogates(x)),
})
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = gemma_lm._preprocessor
backbone = gemma_lm.backbone
```

---The following area is a Code cell (cell numver is 9)---
```python
def preprocess_fn(text, label=None):
    preprocessed = tokenizer(text, sequence_length=512)[0]
    print(preprocessed)
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}
```

---The following area is a Code cell (cell numver is 10)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model


inputs = {
        "token_ids": keras.Input(shape=(512,), dtype=tf.int32, name="token_ids"),
        "padding_mask": keras.Input(shape=(512,), dtype=tf.int32, name="padding_mask"),
    }
x = backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 11)---
```python

optimizer = keras.optimizers.AdamW(
                    learning_rate=5e-5,
                    weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 12)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 13)---
```python
model.layers[2].load_lora_weights("/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/model.lora.h5")
```

---The following area is a Code cell (cell numver is 14)---
```python
import numpy as np
dense_1_weights = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_kernel.npy')
dense_1_biases = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_bias.npy')
dense_1_combined = [dense_1_weights, dense_1_biases]
model.layers[-1].set_weights(dense_1_combined)

```

---The following area is a Code cell (cell numver is 15)---
```python
for layer in model.layers:
    layer.trainable = False
```

---The following area is a Code cell (cell numver is 16)---
```python
model.summary()
```

---The following area is a Code cell (cell numver is 17)---
```python
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values)).map(preprocess_fn).batch(16)

```

---The following area is a Code cell (cell numver is 18)---
```python
from tqdm import tqdm

preds = []

for inputs in tqdm(ds):
    keras.backend.clear_session(free_memory=True)
    preds.append(model(inputs))
    keras.backend.clear_session()

    


```

---The following area is a Code cell (cell numver is 19)---
```python
import numpy as np
results = np.concatenate(preds)
```

---The following area is a Code cell (cell numver is 20)---
```python
import pandas
submission = pandas.DataFrame(data=results, index=raw_test_dataset.id, columns=label_columns)
```

---The following area is a Code cell (cell numver is 21)---
```python
submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 22)---
```python
submission.head()
```

---The following area is a Code cell (cell numver is 23)---
```python

```

** @@@ Jupyter Notebook numver 19, the number of votes :9 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction

This notebook is forked from [Here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)

Qwen2 is a language model series including decoder language models of different model sizes. For each size, we release the base language model and the aligned chat model. It is based on the Transformer architecture with SwiGLU activation, attention QKV bias, group query attention, mixture of sliding window attention and full attention, etc. Additionally, we have an improved tokenizer adaptive to multiple natural languages and codes.

The notebook training Qwen2 1.5b version with batch size of 4 and 1 epoch. The training time is around 1 hr on A100. I expect that Qwen2 7b could have better performance on this task. The training code for Colab is attached at the end of the notebook.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn

import numpy as np
import pandas as pd
from transformers import Qwen2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_dir = '/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1'
    lora_dir = '/kaggle/input/lmsys-qwen2-1-5b-checkpoint/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<User prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,
    num_labels=3,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,
    num_labels=3,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_0.config.pad_token_id = model_0.config.eos_token_id
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
model_1.config.pad_token_id = model_1.config.eos_token_id
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Training Code

This training code is forked and inspired from https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning


## Transfer Kaggle Data
```

---The following area is a Code cell (cell numver is 24)---
```python
# # IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES
# # TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,
# # THEN FEEL FREE TO DELETE THIS CELL.
# # NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON
# # ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR
# # NOTEBOOK.

# import os
# import sys
# from tempfile import NamedTemporaryFile
# from urllib.request import urlopen
# from urllib.parse import unquote, urlparse
# from urllib.error import HTTPError
# from zipfile import ZipFile
# import tarfile
# import shutil

# CHUNK_SIZE = 40960
# DATA_SOURCE_MAPPING = 'lmsys-chatbot-arena:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66631%2F8346466%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ae2cd10c5a4750deca45551519904c5858980d5cf8cd8ade09b2299d926c86c895b50ba333acf0db5210d0dd29197c9a9c5a525c8afd0186b88f17d3ca756f0562ad5acfa2e856e8b159f554e61f72102865abd60add751dd59bed5126536d977d6fe54d2e85f8e5baa8d3d75337d0a222a89f0f30fa6dd7c360e4a192363dc417e9e4a9c9c23368991db65b4994c2200bee494d8d5e2684f754ab1b1a511f7db3652e01ab658b04d26cc1321e783fa5509f67d4c438808adc7932a0e79a21849375023b36e90cbe288cf68a6185b2ce950464b71c9b6133d49769c67e77a5298809fb63da23c0655165e80661623bd9bb908bc9a486dbc9e09caebf2a01392,qwen2/transformers/qwen2-1.5b-instruct/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F52038%2F62308%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80f6a1f964073d960129f611693afca0666128c1a319b7ab93769a1993d5910e4995cf3c6f2f87323452999b069eafb9f7b01be97dbb80a3441cd0b4871d9d35379750a3b5614397253624b097961c886c48df889ac7b1100231715e2b60bf4f3ffccd5e7fc68b3d7d0668f350a8fefbddb90275770e75aaa7f74fae68b2f5314f610ec2f1abf0436156e9426e6173e229172ca0c4ee91eb2d768de3190c9f07e6c28b73bc8c5553e2dac6320842103524591b663021a41801bb2274b5fd91dd62f174ba8976c74995012ad3ed34ecf554a9e2cb08f91813e9cacc9b8d554c6a7a037414635f30e506ea39f63fb4db01f5cf3322dca02097f9550b1a5454ae99'

# KAGGLE_INPUT_PATH='/kaggle/input'
# KAGGLE_WORKING_PATH='/kaggle/working'
# KAGGLE_SYMLINK='kaggle'

# !umount /kaggle/input/ 2> /dev/null
# shutil.rmtree('/kaggle/input', ignore_errors=True)
# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

# try:
#   os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
# except FileExistsError:
#   pass
# try:
#   os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
# except FileExistsError:
#   pass

# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
#     directory, download_url_encoded = data_source_mapping.split(':')
#     download_url = unquote(download_url_encoded)
#     filename = urlparse(download_url).path
#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
#     try:
#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
#             total_length = fileres.headers['content-length']
#             print(f'Downloading {directory}, {total_length} bytes compressed')
#             dl = 0
#             data = fileres.read(CHUNK_SIZE)
#             while len(data) > 0:
#                 dl += len(data)
#                 tfile.write(data)
#                 done = int(50 * dl / int(total_length))
#                 sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
#                 sys.stdout.flush()
#                 data = fileres.read(CHUNK_SIZE)
#             if filename.endswith('.zip'):
#               with ZipFile(tfile) as zfile:
#                 zfile.extractall(destination_path)
#             else:
#               with tarfile.open(tfile.name) as tarfile:
#                 tarfile.extractall(destination_path)
#             print(f'\nDownloaded and uncompressed: {directory}')
#     except HTTPError as e:
#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
#         continue
#     except OSError as e:
#         print(f'Failed to load {download_url} to path {destination_path}')
#         continue

# print('Data source import complete.')

```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## Install and Load libraries
```

---The following area is a Code cell (cell numver is 26)---
```python
## gemma-2 is available from transformers>=4.42.3
# !pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft datasets
```

---The following area is a Code cell (cell numver is 27)---
```python
# import os
# import copy
# from dataclasses import dataclass

# import numpy as np
# import torch
# from datasets import Dataset
# from transformers import (
#     BitsAndBytesConfig,
#     AutoTokenizer,
#     Qwen2ForSequenceClassification,
#     PreTrainedTokenizerBase,
#     EvalPrediction,
#     Trainer,
#     TrainingArguments,
#     DataCollatorWithPadding,
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
# from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
## Training Configuration
```

---The following area is a Code cell (cell numver is 29)---
```python
# @dataclass
# class Config:
#     output_dir: str = "output"
#     model_name: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1"
#     checkpoint: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1" # local directory of qwen model
#     max_length: int = 2048
#     n_splits: int = 5
#     fold_idx: int = 0
#     optim_type: str = "adamw_8bit"
#     per_device_train_batch_size: int = 4
#     gradient_accumulation_steps: int = 2  # global batch size is 8
#     per_device_eval_batch_size: int = 8
#     n_epochs: int = 1
#     freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
#     lr: float = 2e-4
#     warmup_steps: int = 20
#     lora_r: int = 16
#     lora_alpha: float = lora_r * 2
#     lora_dropout: float = 0.05
#     lora_bias: str = "none"

# config = Config()
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
## Training Arguments
```

---The following area is a Code cell (cell numver is 31)---
```python
# training_args = TrainingArguments(
#     output_dir="output",
#     overwrite_output_dir=True,
#     report_to="none",
#     num_train_epochs=config.n_epochs,
#     per_device_train_batch_size=config.per_device_train_batch_size,
#     gradient_accumulation_steps=config.gradient_accumulation_steps,
#     per_device_eval_batch_size=config.per_device_eval_batch_size,
#     logging_steps=10,
#     eval_strategy="epoch",
#     save_strategy="steps",
#     save_steps=200,
#     optim=config.optim_type,
#     fp16=True,
#     learning_rate=config.lr,
#     warmup_steps=config.warmup_steps,
# )
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
## LoRA Config
```

---The following area is a Code cell (cell numver is 33)---
```python
# lora_config = LoraConfig(
#     r=config.lora_r,
#     lora_alpha=config.lora_alpha,
#     # only target self-attention
#     target_modules=["q_proj", "k_proj", "v_proj"],
#     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
#     lora_dropout=config.lora_dropout,
#     bias=config.lora_bias,
#     task_type=TaskType.SEQ_CLS,
# )
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
## Initialize and Tokenike model
```

---The following area is a Code cell (cell numver is 35)---
```python
# tokenizer = AutoTokenizer.from_pretrained(config.model_name)
# tokenizer.add_eos_token = True
# tokenizer.padding_side = "right"
# model = Qwen2ForSequenceClassification.from_pretrained(
#     config.model_name,
#     num_labels=3,
#     torch_dtype=torch.float16,
#     device_map="auto",
# )
# model.config.use_cache = False
# model = prepare_model_for_kbit_training(model)
# model = get_peft_model(model, lora_config)
# model.config.pad_token_id = model.config.eos_token_id
# model
```

---The following area is a Code cell (cell numver is 36)---
```python
# model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# Load Training Data
```

---The following area is a Code cell (cell numver is 38)---
```python
# ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
# class CustomTokenizer:
#     def __init__(
#         self,
#         tokenizer: PreTrainedTokenizerBase,
#         max_length: int
#     ) -> None:
#         self.tokenizer = tokenizer
#         self.max_length = max_length

#     def __call__(self, batch: dict) -> dict:
#         prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
#         response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
#         response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
#         texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
#         tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
#         labels=[]
#         for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
#             if a_win:
#                 label = 0
#             elif b_win:
#                 label = 1
#             else:
#                 label = 2
#             labels.append(label)
#         return {**tokenized, "labels": labels}

#     @staticmethod
#     def process_text(text: str) -> str:
#         return " ".join(eval(text, {"null": ""}))
# encode = CustomTokenizer(tokenizer, max_length=config.max_length)
# ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
## Run Training
```

---The following area is a Code cell (cell numver is 40)---
```python
# def compute_metrics(eval_preds: EvalPrediction) -> dict:
#     preds = eval_preds.predictions
#     labels = eval_preds.label_ids
#     probs = torch.from_numpy(preds).float().softmax(-1).numpy()
#     loss = log_loss(y_true=labels, y_pred=probs)
#     acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
#     return {"acc": acc, "log_loss": loss}

# folds = [
#     (
#         [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
#         [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
#     )
#     for fold_idx in range(config.n_splits)
# ]
```

---The following area is a Code cell (cell numver is 41)---
```python
# train_idx, eval_idx = folds[config.fold_idx]

# trainer = Trainer(
#     args=training_args,
#     model=model,
#     tokenizer=tokenizer,
#     train_dataset=ds.select(train_idx),
#     eval_dataset=ds.select(eval_idx),
#     compute_metrics=compute_metrics,
#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
# )
# trainer.train()
```

** @@@ Jupyter Notebook numver 20, the number of votes :8 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U transformers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U tokenizers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U peft --no-index --find-links ../input/llama3-1-dependencies/dependencies/
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U trl
```

---The following area is a Code cell (cell numver is 2)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    DataCollatorWithPadding,
    LlamaForSequenceClassification,
    LlamaTokenizerFast,
    PreTrainedTokenizerBase,
    EvalPrediction,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "/kaggle/input/unsloth-meta-llama-3.1-8b-bnb-4bit/transformers/default/1/Meta-Llama-3.1-8B-bnb-4bit"
    max_length: int = 2048
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 4
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 32 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 4
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"

config = Config()
```

---The following area is a Code cell (cell numver is 5)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = LlamaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 7)---
```python
model = LlamaForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto"
)
```

---The following area is a Code cell (cell numver is 8)---
```python
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 9)---
```python
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 10)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 11)---
```python
import pandas as pd
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
df = df.dropna()
df = df.drop_duplicates(subset=['response_a', 'response_b'], keep=False)
df["len"] = df["prompt"].apply(len) + df["response_a"].apply(len) + df["response_b"].apply(len)
df = df.sort_values(by=['len'])
df
```

---The following area is a Code cell (cell numver is 12)---
```python
ds = Dataset.from_pandas(df)
ds = ds.select(torch.arange(1000)) #for demo purposes only
```

---The following area is a Code cell (cell numver is 13)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["Which is the better response for the prompt? response_a or response_b or tie? \n'n give score for each lable \n\n <prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 14)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Code cell (cell numver is 15)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    # Check for NaNs in predictions and labels
    if np.isnan(probs).any() or np.isnan(labels).any():
        raise ValueError("NaN values found in predictions or labels")

    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Code cell (cell numver is 16)---
```python
folds = [
        (
            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
            [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
        ) 
        for fold_idx in range(config.n_splits)
    ]
```

---The following area is a Code cell (cell numver is 17)---
```python
from trl import SFTTrainer, SFTConfig
sft_config = SFTConfig(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=1000,
    save_strategy="epoch",
    save_steps=100,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
    packing=True, 
    dataset_text_field="text",
    max_seq_length=config.max_length,
)
```

---The following area is a Code cell (cell numver is 18)---
```python
trainer = SFTTrainer(
        model,
        train_dataset=ds,
        args=sft_config
    )
```

---The following area is a Code cell (cell numver is 19)---
```python
for fold_idx in range(config.n_splits):
    
    train_idx, eval_idx = folds[fold_idx]

    train_data = ds.select(train_idx).sort("len")
    val_data = ds.select(eval_idx).sort("len")
    
    #split training data into batches with the same range of length
    batch_size = 200
    num_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)
    
    for batch_idx in range(num_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, len(train_data))
        ds_temp = train_data.select(range(start_idx, end_idx))
        
        trainer.train_dataset = ds_temp
        
        print(f"Training batch {batch_idx + 1}/{num_batches} on fold {fold_idx + 1}/{config.n_splits}...")
        
        trainer.train()
        
        trainer.save_model(f"model_fold_{fold_idx}_batch{batch_idx}")

    
    # Validate after training on all batches
    trainer.eval_dataset = val_data
    
    print(f"Validating on fold {fold_idx + 1}/{config.n_splits}...")
    eval_results = trainer.evaluate()

    # Save metrics if needed
    print(f"Evaluation results for fold {fold_idx + 1}: {eval_results}")
```

** @@@ Jupyter Notebook numver 21, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import os
import tensorflow as tf
from datasets import load_dataset, DatasetDict
from transformers import BertTokenizer, TFBertModel, DataCollatorWithPadding
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout
from tensorflow.keras.models import Model
import shutil
import pandas as pd
from tqdm.keras import TqdmCallback
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 1)---
```python
# Detect hardware, return appropriate distribution strategy
try:
    # TPU detection. No parameters necessary if TPU_NAME environment variable is
    # set: this is always the case on Kaggle.
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('Running on TPU ', tpu.master())
except ValueError:
    tpu = None

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    tf.tpu.experimental.initialize_tpu_system(tpu)
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # Default distribution strategy in Tensorflow. Works on CPU and single GPU.
    strategy = tf.distribute.MirroredStrategy()

print("REPLICAS: ", strategy.num_replicas_in_sync)
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Importing/Loading the training and test dataset
```

---The following area is a Code cell (cell numver is 3)---
```python
# path for sets
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'

# loading datasets
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# saving ID
test_ids = test_dataset['id']
```

---The following area is a Code cell (cell numver is 4)---
```python
# adding missing columns in the test set
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    if col not in test_dataset.column_names:
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# trainsformation to int64
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 5)---
```python
# using bert-base-cased's files locally
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'

model_dir = '/kaggle/working/bert-base-cased'
os.makedirs(model_dir, exist_ok=True)

shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Tokenization data
tokenizer = BertTokenizer.from_pretrained(model_dir)

def tokenize_function(examples):
    return tokenizer(examples['model_a'], examples['model_b'], padding="max_length", truncation=True, max_length=512)
tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 7)---
```python
# Convert to tf.data.Dataset with the correct shape
def convert_to_tf_dataset(dataset, label_col=None):
    if label_col:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in tokenizer.model_input_names])
    else:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])

    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,
        label_cols=[label_col] if label_col else None,
        shuffle=True,
        batch_size=64,
        collate_fn=data_collator
    )
# creating tf.data.Dataset for test
def convert_to_tf_dataset_for_inference(dataset):
    dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])
    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,
        shuffle=False,
        batch_size=16,
        collate_fn=data_collator
    )
```

---The following area is a Code cell (cell numver is 8)---
```python
train_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
test_labels = 'winner_model_a'
test_dataset = convert_to_tf_dataset_for_inference(test_tokenized_datasets)


# Check data types and shapes before training
for batch in train_dataset.take(1):
    inputs, labels = batch
    print("Data types:")
    print(f"input_ids: {inputs['input_ids'].dtype}, attention_mask: {inputs['attention_mask'].dtype}, labels: {labels.dtype}")
    print("Data shapes:")
    print(f"input_ids: {inputs['input_ids'].shape}, attention_mask: {inputs['attention_mask'].shape}, labels: {labels.shape}")

```

---The following area is a Code cell (cell numver is 9)---
```python
# building a custom model
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)  # load model from local directory
    
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state


def create_keras_model():
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    bert_output = BertLayer()([input_ids, attention_mask])
    pooled_output = Lambda(lambda x: x[:, 0], output_shape=(768,))(bert_output)
    output = Dense(4, activation='softmax')(pooled_output)

    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model
```

---The following area is a Code cell (cell numver is 10)---
```python
#learning
with strategy.scope():
    model = create_keras_model()
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-5),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    history = model.fit(train_dataset,  epochs=10, callbacks=[TqdmCallback(verbose=2)])

```

---The following area is a Code cell (cell numver is 11)---
```python
# prediction
predictions = model.predict(test_dataset)

# creation DataFrame for submission
submission = pd.DataFrame({
    'id': test_ids,
    'winner_model_a': predictions[:, 0],  
    'winner_model_b': predictions[:, 1],
    'winner_model_tie': predictions[:, 2]
})

submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 22, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -U transformers peft bitsandbytes accelerate --no-index --find-links /kaggle/input/lmsys-wheel-files
!pip install -q -U einops --no-index --find-links /kaggle/input/einops-v0-8-0
```

---The following area is a Code cell (cell numver is 1)---
```python
import torch
from transformers import AutoModel, AutoTokenizer
from transformers import BitsAndBytesConfig

```

---The following area is a Code cell (cell numver is 2)---
```python

model_path = "/kaggle/input/internlm2-1.8b-reward/transformers/default/1/internlm_internlm2-1_8b-reward"
model_path = "/kaggle/input/internlm-2-7b/transformers/default/1/internlm_internlm2-7b-reward"
model_path = "/kaggle/input/iternlm2-20b-reward/transformers/default/1/internlm_internlm2-20b-reward"
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

model_0 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:0", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

model_1 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:1", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

# model_0 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:0", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
# model_1 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:1", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
```

---The following area is a Code cell (cell numver is 3)---
```python
# bnb_4bit_path = "internlm2-20b-rm-bnb-4bit"
# model_0.save_pretrained(bnb_4bit_path)
# tokenizer.save_pretrained(bnb_4bit_path)
```

---The following area is a Code cell (cell numver is 4)---
```python
# from IPython.display import FileLink, display
# display(FileLink("/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors"))
```

---The following area is a Code cell (cell numver is 5)---
```python
model_0
```

---The following area is a Code cell (cell numver is 6)---
```python
import pandas as pd

# DEBUG = False
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
# if len(df) == 3:
#     DEBUG = True
#     df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#     df = df.head(1000)
df
```

---The following area is a Code cell (cell numver is 7)---
```python

def cut_off(example, max_length=7200):
    
    def _count(example, idx):
        _len = 0    
        for s in example['prompt'][:idx] + example['response_a'][:idx] + example['response_b'][:idx]:
            _len += len(s)
        return _len

    def _recusive_cut(idx):
        if idx == 0:
            example['prompt'] = [example['prompt'][0][:1000]]
            example['response_a'] = [example['response_a'][0][:3000]]
            example['response_b'] = [example['response_b'][0][:3000]]
            return example
            
        if _count(example, idx) > max_length:
            return _recusive_cut(idx-1)
        else:
            example['prompt'] = example['prompt'][:idx]
            example['response_a'] = example['response_a'][:idx]
            example['response_b'] = example['response_b'][:idx]
            return example
    return _recusive_cut(len(example['prompt']))
        


def process_fn(example):
    example['prompt'] = eval(example['prompt'], {"null": ""})
    example['response_a'] = eval(example['response_a'], {"null": ""})
    example['response_b'] = eval(example['response_b'], {"null": ""})
    return cut_off(example)

new_df = df.apply(lambda x: process_fn(x), axis=1)
new_df
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time
import math
import numpy as np
def inference(df, model):

    error_cnt = 0
    y_pred = []
    for idx, row in df.iterrows():
        chat_a = []
        chat_b = []
        for i in range(len(row['prompt'])):
            chat_a.append({"role": "user", "content": row['prompt'][i]})
            chat_a.append({"role": "assistant", "content": row['response_a'][i]})

            chat_b.append({"role": "user", "content": row['prompt'][i]})
            chat_b.append({"role": "assistant", "content": row['response_b'][i]})
            
        try:
            score1, score2 = model.get_scores(tokenizer, [chat_a, chat_b])
            if abs(score1 - score2) < 0.08:
                y_pred.append([0.00005, 0.00005, 0.9999])
            else:
                score1, score2 = math.exp(score1), math.exp(score2)
                sum_ = score1 + score2
                y_pred.append([score1/sum_ - 0.0001, score2/sum_ -0.0001, 0.0002])
        except:
            y_pred.append([0.33334, 0.33333, 0.33333])
            error_cnt += 1
        
    y_pred = np.array(y_pred)
    df['winner_model_a_pred'] = y_pred[:, 0]
    df['winner_model_b_pred'] = y_pred[:, 1]
    df['winner_tie_pred'] = y_pred[:, 2]
    print(error_cnt)
    return df
```

---The following area is a Code cell (cell numver is 9)---
```python
# test
# inference(new_df[:20], model_0)
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time
sub_0 = new_df.iloc[0::2].copy()
sub_1 = new_df.iloc[1::2].copy()

from concurrent.futures import ThreadPoolExecutor 

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_0, sub_1), (model_0, model_1))

result_df = pd.concat(list(results), axis=0)
result_df.head()
```

---The following area is a Code cell (cell numver is 11)---
```python
result_df = result_df.rename(columns={'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})
result_df
```

---The following area is a Code cell (cell numver is 12)---
```python
result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)
pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 13)---
```python
# from sklearn.metrics import log_loss, accuracy_score

# if DEBUG:
#     y_true = result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.tolist()
#     y_pred = result_df[['winner_model_a_pred', 'winner_model_b_pred', 'winner_tie_pred']].values.tolist()
#     print(log_loss(y_true, y_pred))
```

---The following area is a Code cell (cell numver is 14)---
```python
# if not DEBUG:
#     result_df = result_df.rename({'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})
#     result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)
#     pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 15)---
```python

```

** @@@ Jupyter Notebook numver 23, the number of votes :6 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq
!pip install -U /kaggle/input/peft-wheel/pytorch/version1/1/peft-0.10.0-py3-none-any.whl -qq
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
import numpy as np
from scipy.special import softmax
from transformers import AutoTokenizer, AutoModelForSequenceClassification,LlamaForSequenceClassification, BitsAndBytesConfig
from peft import PeftModel, PeftConfig
import torch
from torch.cuda.amp import autocast
from datasets import Dataset
import torch.nn.functional as F
from threading import Thread
import gc
```

---The following area is a Code cell (cell numver is 2)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)

MODEL_NAME = "/kaggle/input/llama-3/transformers/8b-hf/1"
MAX_LENGTH = 1284
BATCH_SIZE = 4
```

---The following area is a Code cell (cell numver is 3)---
```python
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
df.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
def transform(row):
    return row.strip('[]')
```

---The following area is a Code cell (cell numver is 5)---
```python
df['prompt'] = df['prompt'].apply(transform)
df['response_a'] = df['response_a'].apply(transform)
df['response_b'] = df['response_b'].apply(transform)
```

---The following area is a Code cell (cell numver is 6)---
```python
df['text'] = 'User prompt: ' + df['prompt'] +  '\n\nModel A :\n' + df['response_a'] +'\n\n----------\n\nModel B:\n'  + df['response_b']
```

---The following area is a Code cell (cell numver is 7)---
```python
# peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
# peft_config = PeftConfig.from_pretrained(peft_model_id)
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_function(df):
    result = tokenizer(df, padding="max_length", truncation=True, max_length = MAX_LENGTH)
    return result['input_ids'], result['attention_mask']
```

---The following area is a Code cell (cell numver is 9)---
```python
temp = df['text'].apply(tokenize_function)
df['input_ids'] = temp.apply(lambda x: x[0])
df['attention_mask'] = temp.apply(lambda x: x[0])
```

---The following area is a Code cell (cell numver is 10)---
```python
df
```

---The following area is a Code cell (cell numver is 11)---
```python
# data = Dataset.from_pandas(df[['text']])
# data = data.map(tokenize_function, batched=True)
# data.set_format(type='torch', columns=['input_ids', 'attention_mask'])
```

---The following area is a Code cell (cell numver is 12)---
```python
# data
```

---The following area is a Code cell (cell numver is 13)---
```python
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)
```

---The following area is a Code cell (cell numver is 14)---
```python
peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
```

---The following area is a Code cell (cell numver is 15)---
```python
model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
model_0.config.pad_token_id = tokenizer.pad_token_id

model_0 = PeftModel.from_pretrained(model_0, peft_model_id).to(device0)
model_0 = model_0.merge_and_unload()
model_0.eval()
```

---The following area is a Code cell (cell numver is 16)---
```python
model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
model_1.config.pad_token_id = tokenizer.pad_token_id

model_1 = PeftModel.from_pretrained(model_1, peft_model_id).to(device1)
model_1 = model_1.merge_and_unload()
model_1.eval()
```

---The following area is a Code cell (cell numver is 17)---
```python
gc.collect()
```

---The following area is a Code cell (cell numver is 18)---
```python
def inference(df,model,device,batch_size=BATCH_SIZE):
    
    all_probabilities = []
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        
        batch_input_ids = torch.tensor(df['input_ids'][start_idx:end_idx].tolist()).to(device)
        batch_attention_mask = torch.tensor(df['attention_mask'][start_idx:end_idx].tolist()).to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask)
        logits = outputs.logits
        probabilities = F.softmax(logits, dim=-1)
        all_probabilities.extend(probabilities.cpu().numpy())
    
    del batch_input_ids, batch_attention_mask, outputs
    gc.collect()
    torch.cuda.empty_cache()  

    all_probabilities = np.array(all_probabilities)
    
    df['winner_model_a'] = all_probabilities[:, 0]
    df['winner_model_b'] = all_probabilities[:, 1]
    df['winner_tie'] = all_probabilities[:, 2]
    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
N_SAMPLES = len(df)

half = round(N_SAMPLES / 2)
sub1 = df.iloc[0:half].copy()
sub2 = df.iloc[half:N_SAMPLES].copy()
```

---The following area is a Code cell (cell numver is 20)---
```python
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 21)---
```python
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)
```

---The following area is a Code cell (cell numver is 22)---
```python
results = {}
t0 = Thread(target=run_inference, args=(sub1, model_0, device0,results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1,results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

data = pd.concat([results[0], results[1]], axis=0)
```

---The following area is a Code cell (cell numver is 23)---
```python
data.drop(columns=['prompt','response_a','response_b','text','input_ids','attention_mask'],axis=1,inplace=True)
```

---The following area is a Code cell (cell numver is 24)---
```python
data
```

---The following area is a Code cell (cell numver is 25)---
```python
data.to_csv("submission.csv",index=False)
```

** @@@ Jupyter Notebook numver 24, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# No Installation RequiredÂ¶
microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training

The max sequence length has a significant impact on model performance, but due to insufficient memory, it was set to a maximum length of 768.

[training code](https://www.kaggle.com/code/argozero01/parallel-train-phi-3-mini-4k-instruct)
```

---The following area is a Code cell (cell numver is 1)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional,Tuple
```

---The following area is a Code cell (cell numver is 2)---
```python
# params
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"
model_path = "/kaggle/input/checkpoint-phi3/model_checkpoint.pth"
seed = 42
lora_r = 2
quantize_bit = 16
test_batch_size = 1
test_max_len = 256
device = "cuda"
```

---The following area is a Code cell (cell numver is 3)---
```python
class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.prompt = df['prompt']
        self.response_a = df['response_a']
        self.response_b = df['response_b']
        self.max_len = max_len
        self.targets = df.get('labels', None)

    def __len__(self):
        return len(self.prompt)

    def __getitem__(self, index):
        prompt = str(self.prompt[index])
        response_a = str(self.response_a[index])
        response_b = str(self.response_b[index])

        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])

        final_prompt_len = min(self.max_len, prompt_len)
        final_a_len = min(self.max_len, response_a_len)
        final_b_len = min(self.max_len, response_b_len)

        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')

        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}
```

---The following area is a Code cell (cell numver is 4)---
```python
def custom_collate_fn(batch, tokenizer):

    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None

    # Find the maximum length of the sequences in the batch
    max_len = max([input_id.size(0) for input_id in input_ids])

    # Re-tokenize with the new max length
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        input_ids = item['input_ids'][:max_len]
        attention_mask = item['attention_mask'][:max_len]

        new_input_ids.append(input_ids)
        new_attention_masks.append(attention_mask)

    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)

    output = {
    'input_ids': new_input_ids,
    'attention_mask': new_attention_masks}

    if labels is not None:
        output['labels'] = labels

    return output
```

---The following area is a Code cell (cell numver is 5)---
```python
def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)
    )
    return dataloader
```

---The following area is a Code cell (cell numver is 6)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    qmin = 0.
    qmax = 2.**num_bits - 1.

    min_val, max_val = tensor.min(), tensor.max()
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - min_val / scale

    quantized_tensor = torch.round(tensor / scale + zero_point)
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)
    quantized_tensor = (quantized_tensor - zero_point) * scale

    return quantized_tensor

def quantize_model(model, num_bits=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)
        elif isinstance(module, nn.Conv2d):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)

    return model


# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Code cell (cell numver is 7)---
```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha
        self.rank = rank
        self.lora_a = nn.Linear(in_features, rank, bias=False)
        self.lora_b = nn.Linear(rank, out_features, bias=False)
        self.dropout = nn.Dropout(lora_dropout)

    def forward(self, x):
        lora_out =  self.alpha * self.lora_b(self.lora_a(x))
        lora_out = self.dropout(lora_out)
        return lora_out
```

---The following area is a Code cell (cell numver is 8)---
```python
from transformers.models.phi3.modeling_phi3 import (
Phi3RotaryEmbedding, 
# Phi3LongRoPEScaledRotaryEmbedding,
apply_rotary_pos_emb,
repeat_kv
)
```

---The following area is a Code cell (cell numver is 9)---
```python
class Phi3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta
        self.rope_scaling = config.rope_scaling
        self.is_causal = True

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)
        self._init_rope()
        
        ########################## LoRA adapters ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)
        ########################## LoRA adapters ##########################
        
    def _init_rope(self):
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         logger.warning_once("You are not running the flash-attention implementation, expect numerical differences.")

        bsz, q_len, _ = hidden_states.size()
        ########################## LoRA adapters ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)
        ########################## LoRA adapters ##########################
        query_pos = self.num_heads * self.head_dim
        query_states = qkv[..., :query_pos]
        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights += causal_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        ########################## LoRA adapters ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)
        ########################## LoRA adapters ##########################
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
```

---The following area is a Code cell (cell numver is 10)---
```python
def replace_attention_module(config,layer,layer_idx):
    if hasattr(layer, 'self_attn') and layer_idx > 12:

        new_attention = Phi3Attention(config,layer_idx)

        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention
```

---The following area is a Code cell (cell numver is 11)---
```python
loss_fn = nn.CrossEntropyLoss()

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # config ì¶”ê°€
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # config ì„¤ì •
        self.peft_model = lora_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.config.hidden_size, 3)
#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)
#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state.mean(dim =1)
        output_dropout = self.dropout(pooled_output)
        logits = self.classifier(output_dropout)
        loss = None
        if labels is not None:
          labels = labels
          loss = loss_fn(logits, labels)
        return loss, logits
```

---The following area is a Code cell (cell numver is 12)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
len(test)
```

---The following area is a Code cell (cell numver is 13)---
```python
import json
test["prompt"] = test["prompt"].apply(lambda x: json.loads(x)[0])
test["response_a"] = test["response_a"].apply(lambda x: json.loads(x)[0])
test["response_b"] = test["response_b"].apply(lambda x: json.loads(x)[0])
```

---The following area is a Code cell (cell numver is 14)---
```python
test_0 = test[:len(test)//2].reset_index(drop=True)
test_1 = test[len(test)//2:].reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 15)---
```python
from torch.cuda.amp import autocast

def infer(model, dataloader, device):
#     model = nn.DataParallel(model)  # Wrap the model with DataParallel
#     model.to(device)
    model.eval()

    target_list = []

    for batch in dataloader:
        with torch.no_grad():
            with autocast():
                input_ids = batch["input_ids"].to(device)
                attention_mask = batch["attention_mask"].to(device)
                _,logits = model(input_ids=input_ids, attention_mask=attention_mask)
                softmax_logits = torch.nn.functional.softmax(logits, dim=1)
                target_list.append(softmax_logits)

    return target_list
```

---The following area is a Code cell (cell numver is 16)---
```python
from threading import Thread

gpu0 = "cuda:0"
gpu1 = "cuda:1"
```

---The following area is a Code cell (cell numver is 17)---
```python
model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  ,device_map="cpu")
model0 = quantize_model(model0)
for idx, layer in enumerate(model0.layers):
    replace_attention_module(model0.config,layer,idx)
model0 = LoraModelForClassification(model0)

model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  ,device_map="cpu")
model1 = quantize_model(model1)
for idx, layer in enumerate(model1.layers):
    replace_attention_module(model1.config,layer,idx)
model1 = LoraModelForClassification(model1)


model0.load_state_dict(torch.load(model_path))
model1.load_state_dict(torch.load(model_path))
model0.to(gpu0)
model1.to(gpu1)
```

---The following area is a Code cell (cell numver is 18)---
```python
tokenizer0 = AutoTokenizer.from_pretrained(model_name)

if tokenizer0.pad_token is None:
    tokenizer0.pad_token = tokenizer0.eos_token
tokenizer0.padding_side = "right"  # Fix weird overflow issue with fp16 training

tokenizer1 = AutoTokenizer.from_pretrained(model_name)

if tokenizer1.pad_token is None:
    tokenizer1.pad_token = tokenizer1.eos_token
tokenizer1.padding_side = "right"  # Fix weird overflow issue with fp16 training

test_dataloader0 = create_dataloaders(test_0,tokenizer0,test_max_len,test_batch_size, shuffle = False)
test_dataloader1 = create_dataloaders(test_1,tokenizer1,test_max_len,test_batch_size, shuffle = False)


```

---The following area is a Code cell (cell numver is 19)---
```python
def run_inference(model, dataloader, device, results, index):
    results[index] = infer(model, dataloader, device)

results = {}

process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results,0))
process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results,1))

# Start the processes
process0.start()
process1.start()

# Wait for both processes to finish
process0.join()
process1.join()
```

---The following area is a Code cell (cell numver is 20)---
```python
device = 'cuda:0'  # ì´ë™í•  ìž¥ì¹˜ ì„ íƒ
for k, v in results.items():
    for i in range(len(v)):
        results[k][i] = v[i].to(device)

# ë”•ì…”ë„ˆë¦¬ì˜ ê°’ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ê¸°
target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)
```

---The following area is a Code cell (cell numver is 21)---
```python
sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 22)---
```python
df_list = []
for tensor in target_list:
    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
    df_list.append(df)

combined_df = pd.concat(df_list, axis=0, ignore_index=True)

sub = sub.set_index(pd.Index(combined_df.index))

final_df = pd.concat([sub[['id']], combined_df], axis=1)
```

---The following area is a Code cell (cell numver is 23)---
```python
def delete_files_and_folders(path):
    # ê²½ë¡œê°€ ì¡´ìž¬í•˜ëŠ”ì§€ í™•ì¸
    if not os.path.exists(path):
        print(f"Error: {path} does not exist.")
        return

    # ê²½ë¡œ ë‚´ì˜ ëª¨ë“  íŒŒì¼ ë° í´ë”ë¥¼ íƒìƒ‰
    for root, dirs, files in os.walk(path, topdown=False):
        # íŒŒì¼ ì‚­ì œ
        for name in files:
            if name == "submission.csv":
                print(f"Skipping file: {os.path.join(root, name)}")
                continue
            file_path = os.path.join(root, name)
            print(f"Deleting file: {file_path}")
            os.remove(file_path)

#         # í´ë” ì‚­ì œ
#         for name in dirs:
#             folder_path = os.path.join(root, name)
#             print(f"Deleting folder: {folder_path}")
#             shutil.rmtree(folder_path)

    print(f"All files and folders in {path} have been deleted.")

# ì˜ˆì œ ê²½ë¡œ
path_to_delete = "/kaggle/working/"

# íŒŒì¼ ë° í´ë” ì‚­ì œ í•¨ìˆ˜ í˜¸ì¶œ
delete_files_and_folders(path_to_delete)
```

---The following area is a Code cell (cell numver is 24)---
```python
final_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 25)---
```python
final_df.head()
```

---The following area is a Code cell (cell numver is 26)---
```python
# GPU ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°
def clear_gpu_memory():
    torch.cuda.empty_cache()
    gc.collect()

# í•™ìŠµ í›„ GPU ë©”ëª¨ë¦¬ ë¹„ìš°ê¸°
clear_gpu_memory()
```

** @@@ Jupyter Notebook numver 25, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# Library
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import log_loss
import lightgbm as lgb
import optuna
```

---The following area is a Code cell (cell numver is 1)---
```python
# Load the data
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

print(train_data.head())
print(train_data.info())
print(train_data.describe())

```

---The following area is a Code cell (cell numver is 2)---
```python
# Preprocessing data
# Handling missing values if any
train_data.fillna('', inplace=True)
test_data.fillna('', inplace=True)

# Combine text data for vectorization
train_data['combined_text'] = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']
test_data['combined_text'] = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']

# Vectorize the text data
vectorizer = TfidfVectorizer(max_features=10000)
X_train = vectorizer.fit_transform(train_data['combined_text'])
X_test = vectorizer.transform(test_data['combined_text'])

# Extract the target variable
train_data['winner'] = np.where(train_data['winner_model_a'] == 1, 0, np.where(train_data['winner_model_b'] == 1, 1, 2))
y_train = train_data['winner']

```

---The following area is a Code cell (cell numver is 3)---
```python
# Split the data into training and validation sets
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Convert to LightGBM Dataset
train_data_lgb = lgb.Dataset(X_train_split, label=y_train_split)
val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)

# Optuna objective function for tuning
def objective(trial):
    params = {
        'feature_pre_filter': False,
        'objective': 'multiclass',
        'num_class': 3,
        'metric': 'multi_logloss',
        'boosting': 'gbdt',
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.25),
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 30, 100),
    }
    
    model = lgb.train(params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])
    
    y_val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)
    loss = log_loss(y_val, y_val_pred_proba)
    return loss

# Run Optuna for hyperparameter tuning
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=20)

# Retrieve the best hyperparameters
best_params = study.best_trial.params
best_params.update({'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss', 'boosting': 'gbdt'})

# Train the final model with the best hyperparameters
final_model = lgb.train(best_params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])

```

---The following area is a Code cell (cell numver is 4)---
```python
# Predict probabilities for test set
test_pred_proba = final_model.predict(X_test, num_iteration=final_model.best_iteration)

# Create a submission file
submission = pd.DataFrame(test_pred_proba, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission['id'] = test_data['id']
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 26, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 3)---
```python
train_data.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
train_data_processed = train_data.drop('id', axis = 1)
test_data_processed = test_data.drop('id', axis = 1)
```

---The following area is a Code cell (cell numver is 5)---
```python
train_data_processed = train_data_processed.drop(['model_a', 'model_b'], axis = 1)
```

---The following area is a Code cell (cell numver is 6)---
```python
train_data_processed['winner'] = train_data_processed[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).apply(lambda x: {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}[x])
train_data_processed.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data_processed
```

---The following area is a Code cell (cell numver is 8)---
```python
!pip install transformers
!pip install torch
```

---The following area is a Code cell (cell numver is 9)---
```python
import torch
from tqdm import tqdm
```

---The following area is a Code cell (cell numver is 10)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---The following area is a Code cell (cell numver is 11)---
```python
from transformers import BertTokenizer, BertModel
```

---The following area is a Code cell (cell numver is 12)---
```python
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
# model = BertModel.from_pretrained('bert-base-uncased')

# tokenizer.save_pretrained('./bert-base-uncased')
# model.save_pretrained('./bert-base-uncased')
```

---The following area is a Code cell (cell numver is 13)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')
model = BertModel.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')
model.to(device)
```

---The following area is a Code cell (cell numver is 14)---
```python
def get_bert_embeddings_batch(text_list, batch_size=32):
    embeddings = []
    for i in tqdm(range(0, len(text_list), batch_size)):
        batch_texts = text_list[i:i+batch_size]
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
        inputs = {key: value.to(device) for key, value in inputs.items()}  # Move inputs to GPU if available
        with torch.no_grad():
            outputs = model(**inputs)
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # Move embeddings back to CPU
        embeddings.append(batch_embeddings)
    return np.vstack(embeddings)
```

---The following area is a Code cell (cell numver is 15)---
```python
train_prompt_embeddings = get_bert_embeddings_batch(train_data_processed['prompt'].tolist())
train_response_a_embeddings = get_bert_embeddings_batch(train_data_processed['response_a'].tolist())
train_response_b_embeddings = get_bert_embeddings_batch(train_data_processed['response_b'].tolist())
```

---The following area is a Code cell (cell numver is 16)---
```python
test_prompt_embeddings = get_bert_embeddings_batch(test_data_processed['prompt'].tolist())
test_response_a_embeddings = get_bert_embeddings_batch(test_data_processed['response_a'].tolist())
test_response_b_embeddings = get_bert_embeddings_batch(test_data_processed['response_b'].tolist())
```

---The following area is a Code cell (cell numver is 17)---
```python
train_embeddings = np.hstack([train_prompt_embeddings, train_response_a_embeddings, train_response_b_embeddings])
test_embeddings = np.hstack([test_prompt_embeddings, test_response_a_embeddings, test_response_b_embeddings])
```

---The following area is a Code cell (cell numver is 18)---
```python
train_data_processed['prompt_embedding'] = list(train_embeddings[:, :768])
train_data_processed['response_a_embedding'] = list(train_embeddings[:, 768:1536])
train_data_processed['response_b_embedding'] = list(train_embeddings[:, 1536:2304])

test_data_processed['prompt_embedding'] = list(test_embeddings[:, :768])
test_data_processed['response_a_embedding'] = list(test_embeddings[:, 768:1536])
test_data_processed['response_b_embedding'] = list(test_embeddings[:, 1536:2304])
```

---The following area is a Code cell (cell numver is 19)---
```python
train_data_processed
```

---The following area is a Code cell (cell numver is 20)---
```python
X = train_embeddings
y = train_data_processed['winner']
```

---The following area is a Code cell (cell numver is 21)---
```python
X
```

---The following area is a Code cell (cell numver is 22)---
```python
y
```

---The following area is a Code cell (cell numver is 23)---
```python
from sklearn.model_selection import train_test_split
```

---The following area is a Code cell (cell numver is 24)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)
```

---The following area is a Code cell (cell numver is 25)---
```python
pip install catboost
```

---The following area is a Code cell (cell numver is 26)---
```python
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score
```

---The following area is a Code cell (cell numver is 27)---
```python
model = CatBoostClassifier(
    iterations = 1460,
    learning_rate = 0.01,
    depth = 7,
    loss_function = 'MultiClass',
    eval_metric = 'Accuracy',
    random_seed = 0,
    task_type = 'GPU',
    verbose = 100
)
```

---The following area is a Code cell (cell numver is 28)---
```python
model.fit(X_train, y_train, eval_set=(X_val, y_val))
```

---The following area is a Code cell (cell numver is 29)---
```python
val_preds = model.predict(X_val)
val_preds_class = val_preds.argmax(axis=1)

accuracy = accuracy_score(y_val, val_preds_class)
print(f'Validation Accuracy: {accuracy:.4f}')
```

---The following area is a Code cell (cell numver is 30)---
```python
X_test = test_embeddings
```

---The following area is a Code cell (cell numver is 31)---
```python
test_preds_prob = model.predict_proba(X_test)
```

---The following area is a Code cell (cell numver is 32)---
```python
submission = pd.DataFrame({
    'id': test_data['id'],
    'prob_winner_model_a': test_preds_prob[:, 0],
    'prob_winner_model_b': test_preds_prob[:, 1],
    'prob_winner_tie': test_preds_prob[:, 2]
})

submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 33)---
```python

```

** @@@ Jupyter Notebook numver 27, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "tensorflow"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import transformers
```

---The following area is a Code cell (cell numver is 2)---
```python
print("TensorFlow:", tf.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)
```

---The following area is a Code cell (cell numver is 3)---
```python
class CFG:
    seed = 50  # Random seed
    sequence_length = 1024  # Input sequence length
    epochs = 2 # Training epochs
    batch_size = 2  # Batch size
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Code cell (cell numver is 4)---
```python
keras.utils.set_random_seed(CFG.seed)
keras.mixed_precision.set_global_policy("bfloat16")
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Code cell (cell numver is 5)---
```python
def load_data(row):
  row_list = json.loads(row)
  return " ".join(row for row in row_list if row != None)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv')
# df = df.iloc[:100] #using subset of data for demo

# Take the first prompt and its associated response
df["prompt"] = df["prompt"].apply(load_data)
df["response_a"] = df["response_a"].apply(load_data)
df["response_b"] = df["response_b"].apply(load_data)

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Code cell (cell numver is 7)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df["prompt"].apply(load_data)
test_df["response_a"] = test_df["response_a"].apply(load_data)
test_df["response_b"] = test_df["response_b"].apply(load_data)

# Show Sample
test_df.head()
```

---The following area is a Code cell (cell numver is 8)---
```python
df['options'] = df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(df.head(2))  # Display the first 2 rows of df

test_df['options'] = test_df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Code cell (cell numver is 9)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df["class_label"])
```

---The following area is a Code cell (cell numver is 10)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    "deberta_v3_small_en",
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
    dtype="bfloat16",
)
```

---The following area is a Code cell (cell numver is 11)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Code cell (cell numver is 12)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available

```

---The following area is a Code cell (cell numver is 13)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Code cell (cell numver is 14)---
```python
# import jax
# devices = jax.devices("gpu")
# print("devices", devices)

# data_parallel = keras.distribution.DataParallel(devices=devices)
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_data():
    train_texts = train_df.options.tolist()  # Extract training texts
    train_labels = train_df.class_label.tolist()  # Extract training labels
    train_ds = build_dataset(train_texts, train_labels,
                             batch_size=CFG.batch_size,
                             shuffle=True)

    # Valid
    valid_texts = valid_df.options.tolist()  # Extract validation texts
    valid_labels = valid_df.class_label.tolist()  # Extract validation labels
    valid_ds = build_dataset(valid_texts, valid_labels,
                             batch_size=CFG.batch_size,
                             shuffle=False)

    # Build test dataset
    test_texts = test_df.options.tolist()
    test_ds = build_dataset(test_texts,
                             batch_size=min(len(test_df), CFG.batch_size),
                             shuffle=False)
    return train_ds, valid_ds, test_ds
```

---The following area is a Code cell (cell numver is 16)---
```python
# keras.distribution.set_distribution(data_parallel) #replicate model on both GPUs
```

---The following area is a Code cell (cell numver is 17)---
```python
def get_backbone():
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        "deberta_v3_small_en",
        dtype="bfloat16",
    )
    backbone.enable_lora(8)
    
    return backbone
```

---The following area is a Code cell (cell numver is 18)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                  monitor='val_log_loss',
                                  save_best_only=True,
                                  save_weights_only=True,
                                  mode='min')  # Get Model checkpoint callback
```

---The following area is a Code cell (cell numver is 19)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Code cell (cell numver is 20)---
```python
def get_model(backbone):
    inputs = {
        "token_ids": keras.Input(shape=(2, None), dtype=tf.int16, name="token_ids"),
        "padding_mask": keras.Input(shape=(2, None), dtype=tf.int16, name="padding_mask"),
    }

    # Compute embeddings for first response: (P + R_A) using backbone
    response_a = {k: v[:, 0, :] for k, v in inputs.items()}
    embed_a = backbone(response_a)

    # Compute embeddings for second response: (P + R_B), using the same backbone
    response_b = {k: v[:, 1, :] for k, v in inputs.items()}
    embed_b = backbone(response_b)

    # Compute final output
    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
    embeds = keras.layers.GlobalAveragePooling1D()(embeds)
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
    model = keras.Model(inputs, outputs)

    # Compile the model with optimizer, loss, and metrics
    model.compile(
        optimizer=keras.optimizers.Adam(5e-6),
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
        metrics=[
            log_loss,
            keras.metrics.CategoricalAccuracy(name="accuracy"),
        ],
    )
    
    return model
```

---The following area is a Code cell (cell numver is 21)---
```python
strategy = tf.distribute.MirroredStrategy()
# Open a strategy scope.

with strategy.scope():
    
    train_ds, valid_ds, test_ds = get_data()
    
    backbone = get_backbone()
    
    model = get_model(backbone)
    
    history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[ckpt_cb]
    )
```

---The following area is a Code cell (cell numver is 22)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Code cell (cell numver is 23)---
```python
# Make predictions using the trained model on test data
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Code cell (cell numver is 24)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# ðŸ“Œ | Reference

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
* [LMSYS: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 28, the number of votes :5 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np
import pandas as pd

from sklearn.metrics import log_loss
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

df['response_a_len'] = df['response_a'].apply(lambda x: len(x))
df['response_b_len'] = df['response_b'].apply(lambda x: len(x))

df["ratio_response_b_response_a"] = df['response_b_len'] / df['response_a_len']

# This value is computed from train dataset
avg_1 = {'winner_model_a': 0.2550469811145223, 'winner_model_b': 0.4547399758117034, 'winner_tie': 0.2902130430737743}
avg_2 = {'winner_model_a': 0.31478770131771594, 'winner_model_b': 0.33601756954612005, 'winner_tie': 0.349194729136164}
avg_3 = {'winner_model_a': 0.45816715010877446, 'winner_model_b': 0.25086113125453224, 'winner_tie': 0.29097171863669324}


# The below threshold is also computed from the train datset
def assign_prob(row):
    if row['ratio_response_b_response_a'] <= 0.838296:
        return avg_3.values()
    elif row['ratio_response_b_response_a'] <= 1.004711:
        return avg_2.values()
    else:
        return avg_1.values()



df[['winner_model_a', 'winner_model_b', 'winner_tie']] = df.apply(
    lambda x: assign_prob(x), axis=1, result_type='expand'
)

```

---The following area is a Code cell (cell numver is 1)---
```python

submission_df = pd.DataFrame(
    {
        'id': df['id'],
        'winner_model_a': df['winner_model_a'],
        'winner_model_b': df['winner_model_b'],
        'winner_tie': df['winner_tie']
    }
)
submission_df.to_csv("submission.csv", index=False)
submission_df
```

** @@@ Jupyter Notebook numver 29, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSys Chatbot Arena

Propose, document and defend a solution that can determine which answer is better, or if there is a tie, based on criteria you define..  


**Proposal**: Boosting is a machine learning technique that combines the results of several weak model trainings to create a strong model. The process uses a sequence of interactions, where weights are used for the errors obtained in each training session.

**Example**: XGBoost is a machine learning algorithm based on gradient boosting and using decision trees, where each tree tries to correct the errors of the previous tree.

**Features**: For this problem, we will use features extracted from the texts in our dataset.

In this case, we can use exploratory analysis to establish features add to our classification, such as:
- Size of the texts;
- Words present in the question and answears;
- Difference between answears, etc.

## 1 - Pre-processing
```

---The following area is a Code cell (cell numver is 1)---
```python
# Librarys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.rcParams['figure.figsize'] = 14,5

import seaborn as sns
sns.set_style = 'whitegrid'

import plotly.express as px
px.defaults.template = "plotly_dark"

from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV

# Classification models
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from catboost import CatBoostClassifier

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer

from gensim.models import Word2Vec
import spacy
nlp = spacy.load('en_core_web_sm')

import re
import string

from warnings import filterwarnings
filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 2)---
```python
# loading datasets
df_train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
df_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
df_submission = df_test.copy()
sample_example = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
df_train.head()
```

---The following area is a Code cell (cell numver is 3)---
```python
# Sample example visualization
sample_example.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
# train dataset
df_test.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
# Dataset's shape
print(f'Train dataset shape: {df_train.shape}')
print(f'Test dataset shape: {df_test.shape}')
```

---The following area is a Code cell (cell numver is 6)---
```python
# Dataset info
df_train.info()
```

---The following area is a Code cell (cell numver is 7)---
```python
# Null values train
df_train.isnull().sum()
```

---The following area is a Code cell (cell numver is 8)---
```python
# Null values test
df_test.isnull().sum()
```

---The following area is a Code cell (cell numver is 9)---
```python
# Text Preprocessing 
def preprocess(string):
    strip = string.strip('[]')
    parts = [x.strip('"') for x in strip.split('","')]
    return ''.join(parts)
columns = ['prompt','response_a','response_b']

for colum in df_train[columns]:
    df_train[colum] = df_train[colum].apply(preprocess)
    
for colum in df_test[columns]:
    df_test[colum] = df_test[colum].apply(preprocess)
    
df_train.head(2)
```

---The following area is a Code cell (cell numver is 10)---
```python
# create new columns for data exploration
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

def class_label(df):
    df['class_label'] = None
    
    for index, row in df.iterrows():
        for col in columns:
            if row[col] == 1:
                df.at[index, 'class_label'] = col
                break

# Call the function on your DataFrame
class_label(df_train)
```

---The following area is a Code cell (cell numver is 11)---
```python
# Define class number
class_number = {'winner_model_a':0 ,'winner_model_b':1,'winner_tie':2}

df_train['class'] = df_train['class_label'].map(class_number)
df_train.head(2)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Which model was chosen
def chose_model(row):
    if row['class'] == 0:
        return row['model_a']
    elif row['class'] == 1:
        return row['model_b']
    else:
        return 'tie'

df_train['chose_model'] = df_train.apply(chose_model, axis=1)
df_train.head(2)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## 2 - Data Exploration
```

---The following area is a Code cell (cell numver is 14)---
```python
df_train[['winner_model_a','winner_model_b','winner_tie']].mean()
```

---The following area is a Code cell (cell numver is 15)---
```python
# Analysing class distribuitions
# Objective: is the base balanced?
fig = px.pie(df_train[['winner_model_a','winner_model_b','winner_tie']].mean(), names=df_train['class_label'], title='Class distribuition')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 16)---
```python
# Models distribuitions
# Objective: The winningest models in the comparison
df_models = df_train.groupby('chose_model')['class'].count().reset_index().sort_values(by='class',ascending=False)
df_models.rename(columns={'chose_model':'model'}, inplace=True)

# Results Graphs
fig = px.bar(df_models[1:11].sort_values(by='class'), y='model', x='class', title='Top 10 Most Chosen Models', text_auto='.3s')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 17)---
```python
# Models chosen from those mentioned
# Objective: The most of these models
df_uni1 = df_train.groupby('model_a')['id'].count().reset_index().sort_values('id', ascending=False)
df_uni1.rename(columns={'model_a':'model'}, inplace=True)
df_uni2 = df_train.groupby('model_b')['id'].count().reset_index().sort_values('id', ascending=False)
df_uni2.rename(columns={'model_b':'model'}, inplace=True)

df_uni = df_uni1.merge(df_uni2, how='left', on = 'model')
df_uni['total_mentions'] = df_uni['id_x'] + df_uni['id_y']
mentions = df_uni['total_mentions'].sum()
df_uni['%mentions'] = (df_uni['total_mentions'] / mentions) * 100

df_uni = df_uni.merge(df_models, how='left', on = 'model')
df_uni['chose_in_mentions'] = (df_uni['class'] / df_uni['total_mentions']) * 100
df_uni = df_uni.sort_values(by = 'chose_in_mentions', ascending=False)[:10]
df_uni.head(3)
```

---The following area is a Code cell (cell numver is 18)---
```python
# Results - Graphs
fig = px.bar(df_uni.sort_values('chose_in_mentions'), y='model', x='chose_in_mentions', title='% Top 10 Most Chosen Models',
             text_auto='.3s')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## 3 Modeling
```

---The following area is a Code cell (cell numver is 20)---
```python
%%time

df = df_train.copy()

# Word Processing
stopwords_list = stopwords.words('english')

def word_processing(text):
    '''Process text: remove special characters, convert to lowercase, tokenize, and remove stopwords'''
    text = re.sub(r'[/<>()|\+\-\$%&#@\'\"]+', ' ', text)  # Remove special characters
    text = text.lower()  # Convert to lowercase
    tokens = word_tokenize(text)  # Tokenize the text
    filtered_tokens = [token for token in tokens if token not in stopwords_list]  # Remove stopwords
    return filtered_tokens

# Features engineering
def count_token(tokens):
    '''Count number of tokens'''
    return len(tokens)

def diff_response(tokens1, tokens2):
    '''Difference in token count between responses'''
    len_1 = len(tokens1)
    len_2 = len(tokens2)
    diff = abs(len_1 - len_2)
    return len_1, len_2, diff

def prompt_u_response(text1, text2):
    '''Common words between the prompt and the responses'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)
    return len(intersec)

def aUb(text1, text2):
    '''Common words between the responses'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)
    return len(intersec)

def lexical_diversity(text):
    '''Proportion of unique words'''
    return len(set(text)) / len(text) if text else 0

def avg_words_per_sentence(text):
    '''Calculate average number of words per sentence'''
    sentences = sent_tokenize(text)
    word_count = sum(len(word_tokenize(sentence)) for sentence in sentences)
    return word_count / len(sentences) if sentences else 0

def sentence_diversity(text):
    '''Calculate sentence diversity (variety in sentence length)'''
    sentences = sent_tokenize(text)
    lengths = [len(word_tokenize(sentence)) for sentence in sentences]
    return len(set(lengths)) / len(lengths) if lengths else 0

def Simylarity(text1, text2):
    '''Calculate the similaty between prompt and responses'''
    tokenA = nlp(text1)
    tokenB = nlp(text2)
    return tokenA.similarity(tokenB)

df['similaty_promptUresponse_a'] = df.apply(lambda row: Simylarity(row['prompt'],row['response_a']), axis=1)
df['similaty_promptUresponse_b'] = df.apply(lambda row: Simylarity(row['prompt'],row['response_b']), axis=1)


# Applying word processing to columns
for col in ['prompt', 'response_a', 'response_b']:
    df[col] = df[col].apply(word_processing)
    df[f'{col}_count_token'] = df[col].apply(count_token)

# Applying additional features
for index, row in df.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)
        
        df.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)
        df.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)
        #df.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)
        df.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)
        
        # Existing features
        df.at[index, f'{col}_count_token'] = count_token(tokens)

# Applying diferences functions in the train dataframe 
for index, row in df.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)
    response_b_text = ' '.join(response_b_tokens)
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)
    df.at[index, 'response_len_a'] = len_a
    df.at[index, 'response_len_b'] = len_b
    df.at[index, 'response_diff'] = diff
    
    common_words_ab = aUb(response_a_text, response_b_text)
    df.at[index, 'common_words_ab'] = common_words_ab
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)
    df.at[index, 'common_words_prompt_a'] = common_words_prompt_a
    df.at[index, 'common_words_prompt_b'] = common_words_prompt_b
    

df.head(2)
```

---The following area is a Code cell (cell numver is 21)---
```python
df[['similaty_promptUresponse_a','similaty_promptUresponse_b']].describe().T
```

---The following area is a Code cell (cell numver is 22)---
```python
%%time

# features and target selection
X = df.drop(['id','model_a','model_b','prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie',
            'class_label','chose_model','class'], axis=1).values
y = df['class'].values

# Split train and valid
X_train,X_valid,y_train, y_valid = train_test_split(X, y, test_size = 0.2, random_state=42, stratify=y)

# Models use
models = {
    'XGBoost': XGBClassifier(random_state=42),
    'GradientBoosting': GradientBoostingClassifier(random_state=42),
    'CatBoost': CatBoostClassifier(random_state=42,verbose=False)
}

# Trainning the models
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)


# Metrics
result_loss = []
result_models = []

for model_name, model in models.items():
    i = 0
    print(f'Model {model_name}')

    model_losses = []  # Loss Models List
    
    for train_index, valid_index in skf.split(X, y):
        X_train_fold, X_test_fold = X[train_index], X[valid_index]
        y_train_fold, y_test_fold = y[train_index], y[valid_index]
        
        # Training the model
        model.fit(X_train_fold, y_train_fold)
        
        # Metrics evaluation
        y_test_pred_proba = model.predict_proba(X_test_fold)
        loss = metrics.log_loss(y_test_fold, y_test_pred_proba)
        model_losses.append(loss)  # Add loss in list
        
        # Display the results
        print(f'Fold {i} | Log Loss: {loss}')
        i += 1
        
        mean_loss = np.mean(model_losses)
        result_loss.append(model_losses)
        result_models.append((model_name, model_losses))

    print(f'Mean Loss for model {model_name} is {mean_loss}')
    print('---' * 30)
```

---The following area is a Code cell (cell numver is 23)---
```python
# Plot results
data_dict = {'CV': list(range(0, 5))}

for model, values in result_models:
    data_dict[model] = values

results_df = pd.DataFrame(data_dict)

fig = px.line(results_df, x='CV', y=['XGBoost','GradientBoosting','CatBoost'], title='Loss Models')
fig.update_layout(width=600, height=400)
fig.show()
```

---The following area is a Code cell (cell numver is 24)---
```python
%%time
# The best model are GradientBoosting
# Now, we'll use the gridsearch for tunning hyperparametros
model_grad = GradientBoostingClassifier()
params = {
    'n_estimators':[100, 150, 170],
    'max_depth': [1, 3, 5],
    'learning_rate': [0.1, 0.01]
}

grid = GridSearchCV(model_grad, params, n_jobs=-1, cv=5, scoring='neg_log_loss', verbose=False)
grid.fit(X_train, y_train)
```

---The following area is a Code cell (cell numver is 25)---
```python
print(f'The best params {grid.best_params_}')
```

---The following area is a Code cell (cell numver is 26)---
```python
# Selection the best model
best_model = grid.best_estimator_
```

---The following area is a Code cell (cell numver is 27)---
```python
# Avaluation the best model
y_predict_valid_prob = best_model.predict_proba(X_valid)

logloss = metrics.log_loss(y_valid, y_predict_valid_prob)

print(f'LogLoss the Best Models with GridSearch: {logloss}')
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
There was no significant improvement with GridSearch
```

---The following area is a Code cell (cell numver is 29)---
```python
# Features importance
features_importance = best_model.feature_importances_
features_names = ['prompt_count_token', 'response_a_count_token',
       'response_b_count_token', 'response_a_lexical_diversity',
       'response_a_avg_words_per_sentence', 'response_a_sentence_diversity',
       'response_b_lexical_diversity', 'response_b_avg_words_per_sentence',
       'response_b_sentence_diversity', 'response_len_a', 'response_len_b',
       'response_diff', 'common_words_ab', 'common_words_prompt_a',
       'common_words_prompt_b','similaty_promptUresponse_a', 'similaty_promptUresponse_b']

fig = px.bar(y=features_names, x=features_importance, color=features_importance, title='Features Importance')
scale = px.colors.sequential.BuGn
fig.update_traces(marker=dict(colorscale=scale))
fig.update_layout(width=800, height=500)
fig.show()
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
Above we can see the most important features for the model
```

---The following area is a Code cell (cell numver is 31)---
```python
# Preparing the test data to predict
df_test['similaty_promptUresponse_a'] = df_test.apply(lambda row: Simylarity(row['prompt'],row['response_a']), axis=1)
df_test['similaty_promptUresponse_b'] = df_test.apply(lambda row: Simylarity(row['prompt'],row['response_b']), axis=1)



# Applying word processing to columns
for col in ['prompt', 'response_a', 'response_b']:
    df_test[col] = df_test[col].apply(word_processing)
    df_test[f'{col}_count_token'] = df_test[col].apply(count_token)

# Applying additional features
for index, row in df_test.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)
        
        df_test.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)
        df_test.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)
        #df.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)
        df_test.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)
        
        # Existing features
        df_test.at[index, f'{col}_count_token'] = count_token(tokens)


# Applying diferences functions in the test dataframe 
for index, row in df_test.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)
    response_b_text = ' '.join(response_b_tokens)
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)
    df_test.at[index, 'response_len_a'] = len_a
    df_test.at[index, 'response_len_b'] = len_b
    df_test.at[index, 'response_diff'] = diff
    
    common_words_ab = aUb(response_a_text, response_b_text)
    df_test.at[index, 'common_words_ab'] = common_words_ab
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)
    df_test.at[index, 'common_words_prompt_a'] = common_words_prompt_a
    df_test.at[index, 'common_words_prompt_b'] = common_words_prompt_b

df_test.drop(columns = ['id','prompt','response_a','response_b'], axis=1, inplace=True)
df_test.head()
```

---The following area is a Code cell (cell numver is 32)---
```python
# Predict proba in test
y_sub_proba = best_model.predict_proba(df_test)
y_sub_proba
```

---The following area is a Code cell (cell numver is 33)---
```python
# Submission df
submission = pd.DataFrame({
    'id':df_submission['id'],
    'winner_model_a': y_sub_proba[:, 0],
    'winner_model_b': y_sub_proba[:, 1],
    'winner_tie': y_sub_proba[:, 2]
})

submission.head()
```

---The following area is a Code cell (cell numver is 34)---
```python
submission.to_csv('/kaggle/working/submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 30, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Code cell (cell numver is 1)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Code cell (cell numver is 2)---
```python
train = pd.read_csv(config.train_path)
test = pd.read_csv(config.test_path)
sample_submission = pd.read_csv(config.sample_submission_path)

if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()
```

---The following area is a Code cell (cell numver is 3)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))
            vectorizer.fit([text1, text2])
            output = vectorizer.transform([text1, text2]).toarray()
            cos_sim = cosine_similarity(output)
            return cos_sim[0][1]
        except:
            return np.nan

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())
        set2 = set(text2.split())
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union)
    
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n') 
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"
        double_quote_pattern = r'"(.*?)"'
        single_quotes = re.findall(single_quote_pattern, text)
        double_quotes = re.findall(double_quote_pattern, text)
        total_quotes = len(single_quotes) + len(double_quotes)
        return len(single_quotes) + len(double_quotes)

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)
        return list(ngrams(tokens, n))

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)
            ngrams2 = self.generate_ngrams(text2, n)
            counter1 = Counter(ngrams1)
            counter2 = Counter(ngrams2)
            overlap = counter1 & counter2
            overlap_count = sum(overlap.values())
            return overlap_count
        except:
            return 0
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"] 
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"] 
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"] 
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]
        
        return data
```

---The following area is a Code cell (cell numver is 4)---
```python
%%time
preprocessor = Preprocessor()
train = preprocessor.run(train)
test = preprocessor.run(test)
train.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]
target = "target"

train[target] = np.nan
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx
train[target] = train[target].astype("int32")
    
train.head()
```

---The following area is a Code cell (cell numver is 6)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)
y = train[target]
X_test = test.drop(columns=drop_cols, axis=1)

X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 7)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))
cv_scores = list()

features = X.columns.tolist()
feat_imp_df = pd.DataFrame({"feature": features})

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]

    print(f'train: {X_train.shape}')
    print(f'val: {X_val.shape}')
    
    model = xgb.XGBClassifier(
        objective='multi:softprob',
        num_class=3,
        eval_metric='mlogloss',
        subsample=0.8,
        n_estimators=650,
        learning_rate=0.045,
        max_depth=5,
        random_state=config.seed
    )
    
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],
        early_stopping_rounds=75,
        verbose=75
    )
    
    val_preds = model.predict_proba(X_val)
    val_log_loss = log_loss(y_val, val_preds, eps="auto")
    print(f"val log loss: {val_log_loss:.5f}")
    cv_scores.append(val_log_loss)
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()
    
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)
plt.figure(figsize=(12, 10))
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50
    ],
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances of All Folds", size=12)
plt.show()
```

---The following area is a Code cell (cell numver is 8)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]
sample_submission.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
sample_submission.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 31, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel
import torch.nn.functional as F
np.random.seed(1337)
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen2-7B-Instruct")
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)
```

---The following area is a Code cell (cell numver is 7)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 8)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 9)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 1)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 14300
maxlen = 1024
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
num_samples = INPUT_IDS.shape[0]

# Sá»‘ lÆ°á»£ng máº«u cho X_valid (20% cá»§a X_train)
num_valid_samples = int(num_samples * 0.2)

# XÃ¡o trá»™n cÃ¡c chá»‰ sá»‘ cá»§a X_train
indices = np.random.permutation(num_samples)

# Chá»n 20% chá»‰ sá»‘ Ä‘áº§u tiÃªn lÃ m chá»‰ sá»‘ cho X_valid
valid_indices = indices[:num_valid_samples]

# CÃ¡c chá»‰ sá»‘ cÃ²n láº¡i lÃ m chá»‰ sá»‘ cho X_train
train_indices = indices[num_valid_samples:]

# Táº¡o X_valid vÃ  X_train má»›i tá»« cÃ¡c chá»‰ sá»‘ Ä‘Ã£ chá»n
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=maxlen)
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=maxlen)
y_train = LABELS[train_indices]

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=maxlen)
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=maxlen)
y_valid = LABELS[valid_indices]
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
'''This example demonstrates the use of Convolution1D for text classification.
Gets to 0.88 test accuracy after 2 epochs.
90s/epoch on Intel i5 2.4Ghz CPU.
10s/epoch on Tesla K40 GPU.
'''
from tensorflow.keras.layers import Layer
from keras.layers import Concatenate
from keras.layers import  GlobalMaxPooling1D
import tensorflow as tf

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})


# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        embeddings, attention_mask = inputs
        return embeddings * tf.expand_dims(attention_mask, -1)

model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')
attention_masks = Input(shape=(maxlen,), dtype='float32', name="attention_masks")

emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])
def max_1d(X):
    return K.max(X, axis=1)

# we add a Convolution1D, which will learn nb_filter
# word group filters of size 3:

con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size =3,
                    strides=1)(masked_embeddings)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 4:

con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 5:

con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)


cnn_layer =Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])


#LSTM


x = masked_embeddings
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)
output_layer= Dropout(0.2)(dense_layer)
output_layer = Dense(3, trainable=True,activation='softmax')(output_layer)




model = Model(inputs=[input_layer, attention_masks], outputs=[output_layer])
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
adamax = Adamax(learning_rate=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.summary()

```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 21)---
```python
from keras.callbacks import EarlyStopping
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)

model.fit([X_train,X_train_attention], y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint, early_stopping],
          validation_data=([X_valid,X_valid_attention], y_valid))
```

---The following area is a Code cell (cell numver is 22)---
```python
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])
model.save('model_LSTM_mix_CNN.keras')
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Test Model
```

---The following area is a Code cell (cell numver is 24)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 25)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 26)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 27)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 28)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=maxlen)
```

---The following area is a Code cell (cell numver is 29)---
```python
test
```

---The following area is a Code cell (cell numver is 30)---
```python
y_predict = model.predict([X_test,X_test_attention])
y_predict
```

---The following area is a Code cell (cell numver is 31)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)
```

---The following area is a Code cell (cell numver is 32)---
```python
result_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 33)---
```python
result_df
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 32, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Inference - llama-3 8b Super Fast ðŸš€
In this notebook we do inference using fine-tuned llama-3 8b model using T4 * 2 Gpu parallel, motivation behind to create this is the huge test size (25k samples). 

Prerequisite: Access to Llama-3, if you have not filled consent form, go [here](https://www.kaggle.com/models/metaresearch/llama-3), fill the form to access llama-3.  

Training notebook can be found [here](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)

Please upvote if you find this helpful!

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType,prepare_model_for_kbit_training
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Code cell (cell numver is 3)---
```python
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'
MAX_LENGTH = 2048
BATCH_SIZE = 4
DEVICE = torch.device("cuda")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))
```

---The following area is a Code cell (cell numver is 6)---
```python
# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:0',
    use_cache=False,
)


base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:1',
    use_cache=False,
)
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
Now, we have sucessfully loaded one model on each GPU!

# Load weights
```

---The following area is a Code cell (cell numver is 12)---
```python
# LoRa configuration

lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    inference_mode=True,
    task_type = 'SEQ_CLS'
)
```

---The following area is a Code cell (cell numver is 13)---
```python
# Get peft
# model_0 = get_peft_model(base_model_0, lora_config).to(device0) 
model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)
# model_0 = model_0.merge_and_unload()
# model_0.config.pad_token_id = tokenizer.pad_token_id
# model_0.config.pretraining_tp = 1
model_0.eval()

# model_1 = get_peft_model(base_model_1, lora_config).to(device1) 
model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)
# model_1 = model_1.merge_and_unload()
# model_0.config.pad_token_id = tokenizer.pad_token_id
# model_0.config.pretraining_tp = 1
model_1.eval()
```

---The following area is a Code cell (cell numver is 14)---
```python
# Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 16)---
```python
import gc
gc.collect()
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}

# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 19)---
```python
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
display(sample_sub)
```

---The following area is a Code cell (cell numver is 20)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :)
```

** @@@ Jupyter Notebook numver 33, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl
!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl
import textstat
```

---The following area is a Code cell (cell numver is 1)---
```python
import sklearn
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
from xgboost import XGBClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
import nltk
import textstat
from textblob import TextBlob
from collections import Counter

import warnings
warnings.filterwarnings("ignore")
warnings.filterwarnings('ignore')
pd.options.display.float_format = '{:.2f}'.format
pd.set_option('display.max_rows', None)  # Show all rows
pd.set_option('display.max_columns', None)
```

---The following area is a Code cell (cell numver is 2)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 3)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 4)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
print(f"The size of the train data: {train.shape} is and the test data is: {test.shape}")
```

---The following area is a Code cell (cell numver is 6)---
```python
print(train['winner_model_a'].value_counts())
print(train['winner_model_b'].value_counts())
print(train['winner_tie'].value_counts())
```

---The following area is a Code cell (cell numver is 7)---
```python
# Create a figure and axes
fig, axes = plt.subplots(3, 1, figsize=(7, 6))

# Columns to plot
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

# Define colors for 0 and 1
colors = {0: 'steelblue', 1: 'salmon'}

# Plot each column in its respective subplot
for i, column in enumerate(columns):
    ax = axes[i]
    value_counts = train[column].value_counts().sort_index()
    
    # Plot bars with specified colors and labels for legend
    bars = ax.bar(value_counts.index.astype(str), value_counts, color=[colors[idx] for idx in value_counts.index],
                  label=value_counts.index.map({0: 'Lose (0)', 1: 'Win (1)'}))
    
    # Annotate counts on bars
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    ax.set_xlabel('Winner')
    ax.set_ylabel('Count')
    ax.set_title(f'Model {column.split("_")[-1].capitalize()} Counts')
    ax.legend(title='Outcome', loc='upper right')

# Add overall title and adjust layout
fig.suptitle('Distribution of Winners Across Models', fontsize=16)
plt.tight_layout(rect=[0, 0.03, 1, 0.95])

# Display the plot
plt.show()
```

---The following area is a Code cell (cell numver is 8)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)


train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)
```

---The following area is a Code cell (cell numver is 9)---
```python
train.head(3)
```

---The following area is a Code cell (cell numver is 10)---
```python
test.head(3)
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

# Function to compute word count
def word_count(text):
    return len(nltk.word_tokenize(text))

# Function to compute character count
def char_count(text):
    return len(text)

# Function to compute sentence count
def sentence_count(text):
    return len(nltk.sent_tokenize(text))

# Function to compute average word length
def avg_word_length(text):
    words = nltk.word_tokenize(text)
    if len(words) == 0:
        return 0
    return sum(len(word) for word in words) / len(words)

# Function to compute average sentence length
def avg_sentence_length(text):
    words = nltk.word_tokenize(text)
    sentences = nltk.sent_tokenize(text)
    if len(sentences) == 0:
        return 0
    return len(words) / len(sentences)

# Function to compute type-token ratio
def ttr(text):
    words = nltk.word_tokenize(text)
    if len(words) == 0:
        return 0
    unique_words = set(words)
    return len(unique_words) / len(words)

# Function to compute word frequency
def word_freq(text):
    words = nltk.word_tokenize(text)
    return Counter(words)

# Function to compute bigram frequency
def bigram_freq(text):
    words = nltk.word_tokenize(text)
    bigrams = list(nltk.bigrams(words))
    return Counter(bigrams)

# Function to compute readability scores
def readability_scores(text):
    scores = {
        "flesch_kincaid_score": textstat.flesch_kincaid_grade(text),
        "gunning_fog_index": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "ari": textstat.automated_readability_index(text)
    }
    return scores

# Compute additional metrics and add to DataFrame
for column in ["prompt", "response_a", "response_b"]:
    train[f"{column}_word_count"] = train[column].apply(word_count)
    train[f"{column}_char_count"] = train[column].apply(char_count)
    train[f"{column}_sentence_count"] = train[column].apply(sentence_count)
    train[f"{column}_avg_word_length"] = train[column].apply(avg_word_length)
    train[f"{column}_avg_sentence_length"] = train[column].apply(avg_sentence_length)
#     train[f"{column}_ttr"] = train[column].apply(ttr)
#     readability = train[column].apply(readability_scores)
#     train[f"{column}_flesch_kincaid_score"] = readability.apply(lambda x: x["flesch_kincaid_score"])
#     train[f"{column}_gunning_fog_index"] = readability.apply(lambda x: x["gunning_fog_index"])
#     train[f"{column}_smog_index"] = readability.apply(lambda x: x["smog_index"])
#     train[f"{column}_ari"] = readability.apply(lambda x: x["ari"])

train.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

import time
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import log_loss
from scipy.stats import uniform, randint

# Convert the target into a single column with categorical labels
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# Define features and target
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]

X = train[features]
y = train['winner'] - 1

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Define the models
models = {
    'GradientBoostingClassifier': GradientBoostingClassifier(),
    'XGBClassifier': XGBClassifier()
}

# Define the parameter distributions for random search
param_distributions = {
    'GradientBoostingClassifier': {
        'n_estimators': [100,200,350,300],
        'max_depth': [2,3,4,5,7,9]
    },
    'XGBClassifier': {
        'n_estimators': [100,200,350,300],
        'max_depth': [2,3,4,5,7,9]
    }
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)

best_models = {}  # Dictionary to store the best models

# Iterate over each model
for model_name, model in models.items():
    print(f"Model training for {model_name}")
    
    # Perform RandomizedSearchCV
    random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=10, scoring='neg_log_loss', 
                                       n_jobs=-1, cv=skf, random_state=42)
    random_search.fit(X_train, y_train)
    
    best_model = random_search.best_estimator_
    best_models[model_name] = best_model  # Store the best model for the current type
    
    logloss_scores = []
    start_time = time.time()
    
    count = 0
    for train_index, test_index in skf.split(X, y):
        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]
        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]

        best_model.fit(X_train_fold, y_train_fold)
        y_test_pred_proba = best_model.predict_proba(X_test_fold)

        logloss = log_loss(y_test_fold, y_test_pred_proba)
        logloss_scores.append(logloss)
        print(f"The log loss score for fold {count}: {logloss}")
        count += 1

    average_logloss = sum(logloss_scores) / len(logloss_scores)
    print(f"The average log loss score for {model_name} across all folds: {average_logloss}")
    
    elapsed_time = time.time() - start_time
    print(f"Time taken for {model_name}: {elapsed_time:.2f} seconds")
    
    # Predict probabilities on the validation set
    y_val_prob = best_model.predict_proba(X_val)
    # Calculate log loss on the validation set
    val_loss = log_loss(y_val, y_val_prob)
    print(f'Log Loss using {model_name} on validation set: {val_loss}')

# Identify the best model based on validation set performance
best_model_name = min(best_models, key=lambda k: log_loss(y_val, best_models[k].predict_proba(X_val)))
best_average_logloss = log_loss(y_val, best_models[best_model_name].predict_proba(X_val))

print(f"The best model is {best_model_name} with an average log loss score of {best_average_logloss}")
```

---The following area is a Code cell (cell numver is 13)---
```python
model_to_use = best_models[best_model_name]
model_to_use
```

---The following area is a Code cell (cell numver is 14)---
```python
# Compute additional metrics and add to DataFrame
for column in ["prompt", "response_a", "response_b"]:
    test[f"{column}_word_count"] = test[column].apply(word_count)
    test[f"{column}_char_count"] = test[column].apply(char_count)
    test[f"{column}_sentence_count"] = test[column].apply(sentence_count)
    test[f"{column}_avg_word_length"] = test[column].apply(avg_word_length)
    test[f"{column}_avg_sentence_length"] = test[column].apply(avg_sentence_length)
    
test.head()
```

---The following area is a Code cell (cell numver is 15)---
```python
test_features = test[features]
test_predictions = model_to_use.predict_proba(test_features)
```

---The following area is a Code cell (cell numver is 16)---
```python
test_predictions
```

---The following area is a Code cell (cell numver is 17)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test['id'],
    'winner_model_a': test_predictions[:, 0],
    'winner_model_b': test_predictions[:, 1],
    'winner_tie': test_predictions[:, 2]
})
```

---The following area is a Code cell (cell numver is 18)---
```python
submission.head()
```

---The following area is a Code cell (cell numver is 19)---
```python
submission.to_csv('/kaggle/working/submission.csv', index= False)
```

** @@@ Jupyter Notebook numver 34, the number of votes :4 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# reduce 50% data

import pandas as pd

# Load the train.csv file
train_data_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # Update with the correct path
train_data = pd.read_csv(train_data_path)

test_data_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
test_data = pd.read_csv(test_data_path)

# Randomly sample 10% of the data
sampled_train_data = train_data.sample(frac=0.5, random_state=42)

# Save the sampled data if needed
sampled_train_data_path = '/kaggle/working/sample_train.csv'
sampled_train_data.to_csv(sampled_train_data_path, index=False)
```

---The following area is a Code cell (cell numver is 1)---
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Combine prompts and responses for feature extraction
sampled_train_data['text_a'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_a']
sampled_train_data['text_b'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_b']
test_data['text_a'] = test_data['prompt'] + " " + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + " " + test_data['response_b']

# Initialize the tokenizer
tokenizer = Tokenizer()
tokenizer.fit_on_texts(pd.concat([sampled_train_data['text_a'], sampled_train_data['text_b'], test_data['text_a'], test_data['text_b']]))

# Convert texts to sequences
X_train_a = tokenizer.texts_to_sequences(sampled_train_data['text_a'])
X_train_b = tokenizer.texts_to_sequences(sampled_train_data['text_b'])
X_test_a = tokenizer.texts_to_sequences(test_data['text_a'])
X_test_b = tokenizer.texts_to_sequences(test_data['text_b'])

# Pad sequences to ensure equal length
max_length = max(max(len(seq) for seq in X_train_a), max(len(seq) for seq in X_train_b))
X_train_a = pad_sequences(X_train_a, maxlen=max_length, padding='post')
X_train_b = pad_sequences(X_train_b, maxlen=max_length, padding='post')
X_test_a = pad_sequences(X_test_a, maxlen=max_length, padding='post')
X_test_b = pad_sequences(X_test_b, maxlen=max_length, padding='post')

# Extract targets
y_train_a = sampled_train_data['winner_model_a']
y_train_b = sampled_train_data['winner_model_b']
```

---The following area is a Code cell (cell numver is 2)---
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# Define the LSTM model
def create_lstm_model(input_length):
    model = Sequential()
    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))
    model.add(LSTM(128, return_sequences=True))
    model.add(Dropout(0.5))
    model.add(LSTM(128))
    model.add(Dense(1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

input_length = X_train_a.shape[1]

# Train model for text_a
model_a = create_lstm_model(input_length)
model_a.fit(X_train_a, y_train_a, epochs=5, batch_size=64, validation_split=0.7)
```

---The following area is a Code cell (cell numver is 3)---
```python
# Train model for text_b
model_b = create_lstm_model(input_length)
model_b.fit(X_train_b, y_train_b, epochs=5, batch_size=64, validation_split=0.7)
```

---The following area is a Code cell (cell numver is 4)---
```python
import numpy as np

# Make predictions on the test set
test_pred_a = model_a.predict(X_test_a).flatten()
test_pred_b = model_b.predict(X_test_b).flatten()

# Calculate probabilities for tie (assuming uniform distribution for simplicity)
test_pred_tie = np.full(test_pred_a.shape, 1/3)
```

---The following area is a Code cell (cell numver is 5)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test_data['id'],
    'winner_model_a': test_pred_a,
    'winner_model_b': test_pred_b,
    'winner_tie': test_pred_tie
})

# Save the submission file
submission_path = '/kaggle/working/submission.csv'
submission.to_csv(submission_path, index=False)

print(f"Submission file saved to {submission_path}")
```

** @@@ Jupyter Notebook numver 35, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Prompt/Response Words KeyBERT



KeyBERT is a Python library for keyword extraction and keyphrase extraction. It is built on top of the Hugging Face Transformers library and leverages pre-trained transformer models, such as BERT, to extract key phrases or keywords from a given text. KeyBERT is particularly useful for tasks like document summarization, content analysis, and information retrieval.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install keybert
```

---The following area is a Code cell (cell numver is 2)---
```python
import numpy as np 
import pandas as pd 
import random
import os
from keybert import KeyBERT
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 3)---
```python
model = KeyBERT('distilbert-base-nli-mean-tokens')
```

---The following area is a Code cell (cell numver is 4)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')#, encoding='iso-8859-1')
print(len(train))
train['prompt_kw']='-'
train['res_a_kw']='-'
train['res_b_kw']='-'

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')#, encoding='iso-8859-1')
test['prompt_kw']='-'
test['res_a_kw']='-'
test['res_b_kw']='-'
```

---The following area is a Code cell (cell numver is 5)---
```python
kw0 = model.extract_keywords(train['prompt'],top_n=5)
kw1 = model.extract_keywords(train['response_a'],top_n=20)
kw2 = model.extract_keywords(train['response_b'],top_n=20)

tkw0 = model.extract_keywords(test['prompt'],top_n=5)
tkw1 = model.extract_keywords(test['response_a'],top_n=20)
tkw2 = model.extract_keywords(test['response_b'],top_n=20)
```

---The following area is a Code cell (cell numver is 6)---
```python
for i,w in enumerate(kw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(kw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(kw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    train.loc[i,'res_b_kw']=' '.join(ws)   

train['res_a_kw']=train['prompt_kw']+' / '+train['res_a_kw']
train['res_b_kw']=train['prompt_kw']+' / '+train['res_b_kw']
train=train.iloc[:,6:]
display(train)

train.to_csv('train_key.csv',index=False)
```

---The following area is a Code cell (cell numver is 7)---
```python
for i,w in enumerate(tkw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(tkw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(tkw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_b_kw']=' '.join(ws)   

test['res_a_kw']=test['prompt_kw']+' / '+test['res_a_kw']
test['res_b_kw']=test['prompt_kw']+' / '+test['res_b_kw']
test=test.iloc[:,4:]
display(test)

test.to_csv('test_key.csv',index=False)
```

** @@@ Jupyter Notebook numver 36, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np
import pandas as pd

import plotly.graph_objects as go
from plotly import express as px
from plotly.offline import init_notebook_mode, iplot

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
```

---The following area is a Code cell (cell numver is 1)---
```python
init_notebook_mode(connected=True)

# set max column width to 500
pd.set_option("display.max_colwidth", 500)
```

---The following area is a Code cell (cell numver is 2)---
```python
DATA_PATH = "/kaggle/input/lmsys-chatbot-arena/train.csv"
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Data analysis
```

---The following area is a Code cell (cell numver is 4)---
```python
df = pd.read_csv(DATA_PATH)
df.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
df.info()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Models performance analysis
```

---The following area is a Code cell (cell numver is 7)---
```python
model_usage = pd.concat([df["model_a"], df["model_b"]]).value_counts()

# How many unique models are there?
len(model_usage.keys())
```

---The following area is a Code cell (cell numver is 8)---
```python
# plot the model usage
fig = px.bar(
    x=model_usage.index,
    y=model_usage.values,
    labels={"x": "Model", "y": "Usage times"},
    title="Model Usage Distribution",
)

fig.update_layout(width=1200, height=700)

iplot(fig)
```

---The following area is a Code cell (cell numver is 9)---
```python
# count the number of times each pair of models are compared, regardless of the order
compared_models_count = (
    pd.DataFrame(
        np.sort(df[["model_a", "model_b"]].values, axis=1),
        columns=["model_a", "model_b"],
    )
    .value_counts()
    .reset_index(name="counts")
)

len(compared_models_count)
```

---The following area is a Code cell (cell numver is 10)---
```python
top_compared_models = compared_models_count.head(20)

fig = px.bar(
    x=top_compared_models["model_a"] + " vs " + top_compared_models["model_b"],
    y=top_compared_models["counts"],
    labels={"x": "Model Comparison", "y": "Comparison times"},
    title="Top 20 Model Comparison Distribution",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 11)---
```python
# there are 1275 combinations of compared models. How often given numbers of comparisons occur?

compared_models_count["counts"].value_counts()
```

---The following area is a Code cell (cell numver is 12)---
```python
# bin the comparison count distribution to 10 bins
comparision_count_distribution_bins = pd.cut(
    compared_models_count["counts"],
    bins=[
        0,
        5,
        10,
        20,
        30,
        40,
        50,
        60,
        70,
        80,
        90,
        100,
        compared_models_count["counts"].max(),
    ],
    precision=0,
    retbins=False,
)

comparision_count_distribution_bins.value_counts(normalize=True).sort_index()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
**Which models performs the best and the worst?**
```

---The following area is a Code cell (cell numver is 14)---
```python
model_scores_part1 = df.groupby("model_a")["winner_model_a"].sum()
model_scores_part2 = df.groupby("model_b")["winner_model_b"].sum()

model_scores = model_scores_part1.add(model_scores_part2, fill_value=0)
model_scores = model_scores / (
    df["model_a"].value_counts() + df["model_b"].value_counts()
)
model_scores = model_scores.sort_values(ascending=False)
```

---The following area is a Code cell (cell numver is 15)---
```python
# top 10 best models
fig = px.bar(
    x=model_scores.head(10).index,
    y=model_scores.head(10).values,
    labels={"x": "Model", "y": "Score"},
    title="Top 10 best-rated models",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 16)---
```python
# top 10 worst models
fig = px.bar(
    x=model_scores.tail(10).index,
    y=model_scores.tail(10).values,
    labels={"x": "Model", "y": "Score"},
    title="Top 10 worst-rated models",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 17)---
```python
tie_df = df.query("winner_tie == 1")
(tie_df["winner_model_a"] + tie_df["winner_model_b"]).value_counts()
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
**How frequent is tie?**
```

---The following area is a Code cell (cell numver is 19)---
```python
f"{(tie_df.shape[0] / df.shape[0]) * 100:.1f}%"
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Positive and negative responses analysis
```

---The following area is a Code cell (cell numver is 21)---
```python
# exclude tied examples
df_no_ties = df.query("winner_tie == 0")

loosing_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 0 else x["response_b"]),
    axis=1,
)

loosing_responses
```

---The following area is a Code cell (cell numver is 22)---
```python
winning_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 1 else x["response_b"]),
    axis=1,
)
```

---The following area is a Code cell (cell numver is 23)---
```python
# loosing responses must be cleaned
loosing_responses = loosing_responses.str.strip("[]")
loosing_responses = loosing_responses.str.strip('"')

winning_responses = winning_responses.str.strip("[]")
winning_responses = winning_responses.str.strip('"')
```

---The following area is a Code cell (cell numver is 24)---
```python
df_no_ties["winning_response"] = winning_responses
df_no_ties["loosing_response"] = loosing_responses
```

---The following area is a Code cell (cell numver is 25)---
```python
texts = [
    "I do not feel comfortable",
    "I'm sorry, but",
    "I am sorry, but",
    "I apologize, but",
    "Unfortunately I",
    "I do not have enough",
    "I'm just an AI",
    "I'm an AI and",
    "I'm afraid",
    "I can't",
]

unprecise_responses = []
for _, row in df_no_ties.iterrows():
    if row["loosing_response"].startswith(tuple(texts)):
        if row["winning_response"].startswith(tuple(texts)):
            unprecise_responses.append("both")
        else:
            unprecise_responses.append("loosing")
    elif row["winning_response"].startswith(tuple(texts)):
        unprecise_responses.append("winning")
    else:
        continue

pd.Series(unprecise_responses).value_counts(normalize=True)
```

---The following area is a Code cell (cell numver is 26)---
```python
import os
os.environ["KERAS_BACKEND"] = "jax"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import plotly.express as px
```

---The following area is a Code cell (cell numver is 27)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en" # Name of pretrained models
    sequence_length = 512  # Input sequence length
    epochs = 3 # Training epochs
    batch_size = 16  # Batch size
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Code cell (cell numver is 28)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Code cell (cell numver is 29)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Code cell (cell numver is 30)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Code cell (cell numver is 31)---
```python
BASE_PATH
```

---The following area is a Code cell (cell numver is 32)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# Sample data
# df = df.sample(frac=0.10)

# Take the first prompt and its associated response
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Code cell (cell numver is 33)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Show Sample
test_df.head()
```

---The following area is a Code cell (cell numver is 34)---
```python
# Define a function to create options based on the prompt and choices
def make_pairs(row):
    row["encode_fail"] = False
    try:
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        prompt = ""
        row["encode_fail"] = True

    try:
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        response_a = ""
        row["encode_fail"] = True

    try:
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        response_b = ""
        row["encode_fail"] = True
        
    row['options'] = [f"Prompt: {prompt}\n\nResponse: {response_a}",  # Response from Model A
                      f"Prompt: {prompt}\n\nResponse: {response_b}"  # Response from Model B
                     ]
    return row
```

---The following area is a Code cell (cell numver is 35)---
```python
df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(df.head(2))  # Display the first 2 rows of df

test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Code cell (cell numver is 36)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Code cell (cell numver is 37)---
```python
model_df = pd.concat([df.model_a, df.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count', color_continuous_scale='viridis')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()
```

---The following area is a Code cell (cell numver is 38)---
```python
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

fig = px.bar(counts, x='Winner', y='Win Count',
             title='Winner distribution for Train Data',
             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},
             color='Winner', color_continuous_scale='viridis')

fig.update_layout(xaxis_title="Winner", yaxis_title="Win Count")

fig.show()
```

---The following area is a Code cell (cell numver is 39)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Code cell (cell numver is 40)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # Name of the model
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
)
```

---The following area is a Code cell (cell numver is 41)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Code cell (cell numver is 42)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Code cell (cell numver is 43)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Code cell (cell numver is 44)---
```python
# Train
train_texts = train_df.options.tolist()  # Extract training texts
train_labels = train_df.class_label.tolist()  # Extract training labels
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# Valid
valid_texts = valid_df.options.tolist()  # Extract validation texts
valid_labels = valid_df.class_label.tolist()  # Extract validation labels
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 45)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # Plot lr curve if plot is True
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('epoch'); plt.ylabel('lr')
        plt.title('LR Scheduler')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 46)---
```python
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Code cell (cell numver is 47)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')
```

---The following area is a Code cell (cell numver is 48)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Code cell (cell numver is 49)---
```python
# Define input layers
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# Create a DebertaV3Classifier backbone
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# Compute embeddings for first response: (P + R_A) using backbone
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# Compute embeddings for second response: (P + R_B), using the same backbone
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# Compute final output
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# Compile the model with optimizer, loss, and metrics
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Code cell (cell numver is 50)---
```python
model.summary()
```

---The following area is a Code cell (cell numver is 51)---
```python
# Start training the model
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Code cell (cell numver is 52)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Code cell (cell numver is 53)---
```python
# Build test dataset
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 54)---
```python
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Code cell (cell numver is 55)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

** @@@ Jupyter Notebook numver 37, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Step 1: Data Exploration
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
**Load the Data**
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd

# Load the datasets
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# Display the first few rows of each dataset
print("Train Data:")
print(train_data.head())

print("\nTest Data:")
print(test_data.head())

print("\nSample Submission:")
print(sample_submission.head())

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
**Inspect the Data**
```

---The following area is a Code cell (cell numver is 5)---
```python
# Display the summary of each dataset
print("Train Data Info:")
print(train_data.info())

print("\nTest Data Info:")
print(test_data.info())

print("\nSample Submission Info:")
print(sample_submission.info())

# Check for missing values
print("Missing Values in Train Data:")
print(train_data.isnull().sum())

print("\nMissing Values in Test Data:")
print(test_data.isnull().sum())

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
**Analyze the Data**
```

---The following area is a Code cell (cell numver is 7)---
```python
# Get basic statistics for numerical features
print("Train Data Description:")
print(train_data.describe())

# Explore the distribution of target variables
print("Distribution of Winner Model (Train Data):")
print(train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].sum())

```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Step 2: Data Preprocessing
```

---The following area is a Code cell (cell numver is 9)---
```python
import re

def clean_text(text):
    # Remove HTML tags if any
    text = re.sub(r'<.*?>', '', text)
    
    # Remove non-alphanumeric characters
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    # Convert to lowercase
    text = text.lower()
    
    # Remove extra whitespace
    text = text.strip()
    
    return text

```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
**Applying Cleaning Function:**
```

---The following area is a Code cell (cell numver is 11)---
```python
# Clean text data in train_data
train_data['prompt'] = train_data['prompt'].apply(clean_text)
train_data['response_a'] = train_data['response_a'].apply(clean_text)
train_data['response_b'] = train_data['response_b'].apply(clean_text)

# Clean text data in test_data
test_data['prompt'] = test_data['prompt'].apply(clean_text)
test_data['response_a'] = test_data['response_a'].apply(clean_text)
test_data['response_b'] = test_data['response_b'].apply(clean_text)

```

---The following area is a Code cell (cell numver is 12)---
```python
# Check column names in train_data and test_data
print("Train Data Columns:", train_data.columns)
print("Test Data Columns:", test_data.columns)

```

** @@@ Jupyter Notebook numver 38, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import pandas as pd
import ast
import json
import re
import torch
from tqdm import tqdm
```

---The following area is a Code cell (cell numver is 2)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 3)---
```python
def process_data(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    sentences = ' '.join(sentences)
    return sentences

def get_data(path, system_prompt=None):
    df = pd.read_csv(path)
    df['prompt'] = df['prompt'].apply(process_data)
    df['response_a'] = df['response_a'].apply(process_data)
    df['response_b'] = df['response_b'].apply(process_data)
    return df
```

---The following area is a Code cell (cell numver is 4)---
```python
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
test_df = get_data(test_path)
test_df.head()
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Load Gemma-2-9B model
> google/gemma-2-9b-it 

I have already downloaded and stored bfloat16 weights.
```

---The following area is a Code cell (cell numver is 6)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-tokenizer'
model_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-model'
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time
model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.bfloat16)
```

---The following area is a Code cell (cell numver is 10)---
```python
model
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## Let's do a simple inference
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time
prompt = 'Write a conversation between gemma and llama llm models'
input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')

outputs = model.generate(**input_ids, max_new_tokens=200)
response = tokenizer.decode(outputs[0])
print(response)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Let's do inference on test set with some prompt engineering
```

---The following area is a Code cell (cell numver is 14)---
```python
test_df
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_prompt(query, response_a, response_b):
    prompt = f"""
You are tasked with evaluating two responses generated by different models to determine which one is better. Given a query and two responses (RESPONSE_A from MODEL_A and RESPONSE_B from MODEL_B), you will assess the quality of each response based on relevance, accuracy, completeness, and overall coherence. If both responses are equally good or equally poor, you may declare a tie.

Instructions:

Query: {query}
RESPONSE_A (MODEL_A): {response_a}
RESPONSE_B (MODEL_B): {response_b}

Evaluation Criteria:

Relevance: How well does the response address the query?
Accuracy: Is the information provided correct and reliable?
Completeness: Does the response provide a comprehensive answer?
Coherence: Is the response logically structured and easy to understand?
Output:

If RESPONSE_A is better, output: RESPONSE_A
If RESPONSE_B is better, output: RESPONSE_B
If both responses are equally good or poor, output: TIE

You have to output a single line having either of these words - RESPONSE_A or RESPONSE_B or TIE \n
OUTPUT: 

    """
    return prompt
```

---The following area is a Code cell (cell numver is 16)---
```python
def predict(query, response_a, response_b, max_new_tokens=50, do_sample=False, temperature=1.0):
    prompt = get_prompt(query=query, response_a=response_a, response_b=response_b)

    input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')

    outputs = model.generate(**input_ids, max_new_tokens=50, do_sample=do_sample, temperature=temperature)
    response = tokenizer.decode(outputs[0])

    pattern = r"OUTPUT:\s*(RESPONSE_A|RESPONSE_B|TIE)"
    match = re.search(pattern, response)
    if match:
        pred = match.group(1)
    else:
        pred = None
    return pred
```

---The following area is a Code cell (cell numver is 17)---
```python
%%time
id_list = []
winner_model_a_list = []
winner_model_b_list = []
winner_tie_list = []
for idx in tqdm(range(0, len(test_df))):
    query_id = test_df.iloc[idx]['id']
    query = test_df.iloc[idx]['prompt']
    response_a = test_df.iloc[idx]['response_a']
    response_b = test_df.iloc[idx]['response_b']
    pred = predict(query, response_a, response_b, max_new_tokens=20, do_sample=True, temperature=0.7)
    id_list.append(query_id)
    if pred is not None:
        if 'A' in pred or 'a' in pred:
            winner_model_a_list.append(1)
            winner_model_b_list.append(0)                    
            winner_tie_list.append(0)
        if 'B' in pred or 'b' in pred:
            winner_model_a_list.append(0)
            winner_model_b_list.append(1)        
            winner_tie_list.append(0)            
    else:
        winner_model_a_list.append(0)
        winner_model_b_list.append(0)        
        winner_tie_list.append(1)
    torch.cuda.empty_cache()
```

---The following area is a Code cell (cell numver is 18)---
```python
submission_df = pd.DataFrame({'id': id_list, 'winner_model_a': winner_model_a_list, 'winner_model_b': winner_model_b_list, 'winner_tie': winner_tie_list})
```

---The following area is a Code cell (cell numver is 19)---
```python
submission_df
```

---The following area is a Code cell (cell numver is 20)---
```python
submission_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 39, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 30px; 
    border-radius: 15px; 
    text-align: center; 
    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.4);
    margin-bottom: 30px;
">
    <h1 style="font-size: 2.5em;">ðŸ§  LMSYS - AI Chat Evaluator: Enhancing Human-AI</h1>
</div>


<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-bottom: 20px;
">
    <h1>Welcome to my Notebook! ðŸ™</h1>
    <p> This notebook is dedicated to the <strong>Chatbot Arena Competition</strong>, where we explore and innovate in the realm of conversational AI.</p>
    <p><strong>Competition Overview:</strong> The goal is to develop a chatbot that excels in understanding and engaging users through natural and relevant conversations. You'll find data, techniques, and insights here to help you craft the best chatbot possible.</p>
    <p>Thank you for joining this journey. Dive in, explore, and letâ€™s make some magic with chatbots!</p>
</div>



# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
 1. Setup the Environment
</h1>
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install tensorflow pandas numpy scikit-learn

```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
2. Importing Libraries
</h1>
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
3. Loading.. Data
</h1>
```

---The following area is a Code cell (cell numver is 5)---
```python
# Load the dataset
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
4. Preprocessing Text Data
</h1>
```

---The following area is a Code cell (cell numver is 7)---
```python
# Concatenate responses and prompts for tokenization
texts = train_df['prompt'] + ' ' + train_df['response_a'] + ' ' + train_df['response_b']
tokenizer = Tokenizer()
tokenizer.fit_on_texts(texts)

# Convert text to sequences
def text_to_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)
    return pad_sequences(sequences, maxlen=500)  # Adjust maxlen based on your needs

# Process training data
X_prompt_a = text_to_sequences(train_df['prompt'])
X_response_a = text_to_sequences(train_df['response_a'])
X_response_b = text_to_sequences(train_df['response_b'])

# Labels
y_a = train_df['winner_model_a']
y_b = train_df['winner_model_b']
y_tie = train_df['winner_tie']

# Split data for training
X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_prompt_a, y_a, test_size=0.2, random_state=42)
X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_prompt_a, y_b, test_size=0.2, random_state=42)
X_train_tie, X_val_tie, y_train_tie, y_val_tie = train_test_split(X_prompt_a, y_tie, test_size=0.2, random_state=42)

```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
5. Building the Model
</h1>
```

---The following area is a Code cell (cell numver is 9)---
```python
# Define model parameters
vocab_size = len(tokenizer.word_index) + 1
embedding_dim = 100
max_len = 500

# Define inputs
input_prompt = Input(shape=(max_len,))
input_response_a = Input(shape=(max_len,))
input_response_b = Input(shape=(max_len,))

# Embedding layers
embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)

embedded_prompt = embedding_layer(input_prompt)
embedded_response_a = embedding_layer(input_response_a)
embedded_response_b = embedding_layer(input_response_b)

# LSTM layers
lstm_layer = LSTM(64)

encoded_prompt = lstm_layer(embedded_prompt)
encoded_response_a = lstm_layer(embedded_response_a)
encoded_response_b = lstm_layer(embedded_response_b)

# Concatenate encoded responses with prompt
concat_a = Concatenate()([encoded_prompt, encoded_response_a])
concat_b = Concatenate()([encoded_prompt, encoded_response_b])

# Dense layers for classification
dense_layer = Dense(64, activation='relu')

output_a = Dense(1, activation='sigmoid')(dense_layer(concat_a))
output_b = Dense(1, activation='sigmoid')(dense_layer(concat_b))
output_tie = Dense(1, activation='sigmoid')(dense_layer(concat_a))

# Model
model = Model(inputs=[input_prompt, input_response_a, input_response_b], 
              outputs=[output_a, output_b, output_tie])

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Model summary
model.summary()

```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
6. Training the Model
</h1>
```

---The following area is a Code cell (cell numver is 11)---
```python
# Model Compilation
model.compile(
    optimizer='adam', 
    loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'], 
    metrics=[['accuracy'], ['accuracy'], ['accuracy']]
)

# Model Training
history = model.fit(
    [X_train_a, X_train_b, X_train_b],
    [y_train_a, y_train_b, y_train_tie],
    validation_data=([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie]),
    epochs=5,  # Adjust epochs as needed
    batch_size=32
)

```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
7. Evaluateing the Model
</h1>
```

---The following area is a Code cell (cell numver is 13)---
```python
# Evaluate the model
eval_results = model.evaluate([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie])

# Unpack the results
val_loss = eval_results[0]
loss_a = eval_results[1]
loss_b = eval_results[2]
loss_tie = eval_results[3]
acc_a = eval_results[4]
acc_b = eval_results[5]
acc_tie = eval_results[6]

print(f'Validation Loss: {val_loss}')
print(f'Loss for model_a predictions: {loss_a}')
print(f'Loss for model_b predictions: {loss_b}')
print(f'Loss for tie predictions: {loss_tie}')
print(f'Accuracy for model_a predictions: {acc_a}')
print(f'Accuracy for model_b predictions: {acc_b}')
print(f'Accuracy for tie predictions: {acc_tie}')

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
8. Makeing Predictions
</h1>
```

---The following area is a Code cell (cell numver is 15)---
```python
# Preprocess test data
X_test_prompt = text_to_sequences(test_df['prompt'])
X_test_response_a = text_to_sequences(test_df['response_a'])
X_test_response_b = text_to_sequences(test_df['response_b'])

# Predict
predictions = model.predict([X_test_prompt, X_test_response_a, X_test_response_b])

# Unpack predictions
predicted_a = predictions[0].flatten()
predicted_b = predictions[1].flatten()
predicted_tie = predictions[2].flatten()

```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
9. Prepareing the Submission File
</h1>
```

---The following area is a Code cell (cell numver is 17)---
```python
# Format predictions for submission
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': predicted_a,
    'winner_model_b': predicted_b,
    'winner_tie': predicted_tie
})


# Save to CSV
submission.to_csv('/kaggle/working/submission.csv', index=False)

```

---The following area is a Code cell (cell numver is 18)---
```python
submission.head()

```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-top: 30px;
">
    <h2><strong>Thank You for Exploring My Notebook! ðŸ™<strong></h2>
    <p>I hope you found the content and insights valuable as we journeyed through the <strong>LMSYS - AI Chat Evaluator</strong> project together.</p>
    <p>Your feedback and support mean a lot to me. If you enjoyed this notebook and found it helpful, please consider giving it an <span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"> <strong>upvote â¬†ï¸</strong></span>. Your encouragement helps me continue improving and sharing useful content.</p>
    <p><span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"><strong> Thank you once again for your time and support!</strong></span></p>
</div>
```

** @@@ Jupyter Notebook numver 40, the number of votes :3 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd

# Veri setini yÃ¼kleme
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# Veri setinin ilk birkaÃ§ satÄ±rÄ±nÄ± inceleme
df.head()

```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from textblob import TextBlob
import string
```

---The following area is a Code cell (cell numver is 3)---
```python
from sklearn.preprocessing import OneHotEncoder

# Noktalama iÅŸaretleri sayÄ±sÄ±nÄ± Ã¶zellik olarak eklemek
df['response_a_punctuation_count'] = df['response_a'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))
df['response_b_punctuation_count'] = df['response_b'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))

# Kelime Ã§eÅŸitliliÄŸi Ã¶zelliÄŸi eklemek
df['response_a_unique_words'] = df['response_a'].apply(lambda x: len(set(x.split())))
df['response_b_unique_words'] = df['response_b'].apply(lambda x: len(set(x.split())))

# Stop words oranÄ± Ã¶zelliÄŸi eklemek (Ã¶rnek bir stop words listesi kullanÄ±yoruz)
stop_words = set(['the', 'and', 'is', 'in', 'at', 'of', 'it', 'to'])
df['response_a_stop_words_ratio'] = df['response_a'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))
df['response_b_stop_words_ratio'] = df['response_b'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))

# CÃ¼mlenin baÅŸ harf bÃ¼yÃ¼klÃ¼ÄŸÃ¼ Ã¶zelliÄŸi eklemek
df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: 1 if x[0].isupper() else 0)
df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: 1 if x[0].isupper() else 0)

# CÃ¼mlenin uzunluÄŸu ve farkÄ± Ã¶zelliÄŸi eklemek
df['response_a_length'] = df['response_a'].apply(len)
df['response_b_length'] = df['response_b'].apply(len)
df['response_length_difference'] = df['response_a_length'] - df['response_b_length']

# Kazanan model sÃ¼tunu eklemek
df['winner'] = df.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)

# Model adÄ± sÃ¼tunu eklemek
df['model_name'] = df.apply(lambda row: row['model_a'] if row['winner'] == 'model_a' else row['model_b'] if row['winner'] == 'model_b' else '', axis=1)

# One-hot encoding iÅŸlemi
encoder = OneHotEncoder()
winner_encoded = encoder.fit_transform(df[['winner']])

# Encode edilmiÅŸ sÃ¼tunlarÄ± dataframe'e ekleme
df_encoded = pd.concat([df, pd.DataFrame(winner_encoded.toarray(), columns=encoder.categories_[0])], axis=1)

# SonuÃ§larÄ± gÃ¶sterme


```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Veri setini hazÄ±rlama fonksiyonu
def prepare_data_for_ml(df):
    # Punctuation count fonksiyonu
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;:!?')

    # Yeni sÃ¼tunlar ekleme
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation)
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation)
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length'])

    # winner_encoded sÃ¼tunu oluÅŸturma
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner'])

    # model_name sÃ¼tunu oluÅŸturma
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b'])

    # Kategorik deÄŸiÅŸkenleri encode etme
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col])

    # Boolean deÄŸerleri 0 ve 1'e Ã§evirme
    df['model_a_win'] = df['winner'] == 'model_a'
    df['model_b_win'] = df['winner'] == 'model_b'
    df['winner_tie'] = df['winner'] == 'tie'
    df['tie'] = df['winner'] == 'tie'

    boolean_columns = ['response_b_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int)

    return df

# Veri setini hazÄ±rlama
prepared_df = prepare_data_for_ml(df)

# Gerekli sÃ¼tunlarÄ± seÃ§me ve eksik sÃ¼tunlarÄ± kontrol etme
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]
```

---The following area is a Code cell (cell numver is 5)---
```python
prepared_df.head()
```

---The following area is a Code cell (cell numver is 6)---
```python
# Veriyi hazÄ±rlama
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncoded kullanÄ±larak kodlanmÄ±ÅŸ hedef deÄŸiÅŸken

# Veriyi train ve test setlerine bÃ¶lelim
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Veriyi standardize edelim
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modeli eÄŸitme
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Model Ã¼zerinde tahmin yapma
y_pred = model.predict(X_test_scaled)

# SonuÃ§larÄ± deÄŸerlendirme
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie']))
```

---The following area is a Code cell (cell numver is 7)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# Veri setini hazÄ±rlama fonksiyonu
def prepare_data_for_ml(df):
    # Punctuation count fonksiyonu
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;:!?')

    # Yeni sÃ¼tunlar ekleme
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation)
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation)
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_length'] = df['response_a'].str.len()
    df['response_b_length'] = df['response_b'].str.len()
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length'])

    # winner_encoded sÃ¼tunu oluÅŸturma
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner'])

    # model_name sÃ¼tunu oluÅŸturma
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b'])

    # Kategorik deÄŸiÅŸkenleri encode etme
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col])

    # Boolean deÄŸerleri 0 ve 1'e Ã§evirme
    df['model_a_win'] = df['winner'] == 'model_a'
    df['model_b_win'] = df['winner'] == 'model_b'
    df['winner_tie'] = df['winner'] == 'tie'
    df['tie'] = df['winner'] == 'tie'

    boolean_columns = ['response_b_startswith_upper', 'response_a_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int)

    return df

# Veri setini hazÄ±rlama
prepared_df = prepare_data_for_ml(df)

# Gerekli sÃ¼tunlarÄ± seÃ§me ve eksik sÃ¼tunlarÄ± kontrol etme
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_startswith_upper',  # response_a_startswith_upper eklendi
    'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]

# Veriyi hazÄ±rlama
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncoded kullanÄ±larak kodlanmÄ±ÅŸ hedef deÄŸiÅŸken

# Veriyi train ve test setlerine bÃ¶lelim
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Veriyi standardize edelim
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Modeli eÄŸitme
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train)

# Model Ã¼zerinde tahmin yapma
y_pred = model.predict(X_test_scaled)

# SonuÃ§larÄ± deÄŸerlendirme
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie']))

```

---The following area is a Code cell (cell numver is 8)---
```python
df['winner']
```

---The following area is a Code cell (cell numver is 9)---
```python
import pandas as pd
import numpy as np
import string
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
```

---The following area is a Code cell (cell numver is 10)---
```python
# Kazanan modeli encode etmek
label_encoder = LabelEncoder()
df['winner_encoded'] = label_encoder.fit_transform(df['winner'])

# Numeric Ã¶zellikler ve metinsel Ã¶zellikler iÃ§in kolonlarÄ± ayÄ±rmak
numeric_features = [
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_a_unique_words', 'response_b_unique_words',
    'response_a_stop_words_ratio', 'response_b_stop_words_ratio',
    'response_a_length', 'response_b_length', 'response_length_difference'
]

text_features = ['prompt', 'response_a', 'response_b']

# ColumnTransformer oluÅŸturmak
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features),
        ('text', 'passthrough', text_features)  # Metinsel Ã¶zellikler passthrough olarak geÃ§irilecek
    ])

# Verileri X (Ã¶zellikler) ve y (hedef deÄŸiÅŸken) olarak ayÄ±rmak
X = df[numeric_features + text_features]
y = df['winner_encoded']

# Train ve test veri setlerini oluÅŸturmak
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Pipeline oluÅŸturmak
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Modeli eÄŸitmek
pipeline.fit(X_train, y_train)

# Test veri seti Ã¼zerinde modeli deÄŸerlendirmek
accuracy = pipeline.score(X_test, y_test)
print(f"Model accuracy: {accuracy}")

```

---The following area is a Code cell (cell numver is 11)---
```python
import pandas as pd

# CSV dosyalarÄ±nÄ± okuma
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# BirleÅŸtirme iÅŸlemi
df = pd.concat([train_df, test_df], ignore_index=True)

# Sonucu kontrol etme
df.head()

```

---The following area is a Code cell (cell numver is 12)---
```python
df.shape
```

---The following area is a Code cell (cell numver is 13)---
```python
len(df.id)
```

---The following area is a Code cell (cell numver is 14)---
```python
df.info()
```

---The following area is a Code cell (cell numver is 15)---
```python
winner_counts = {
    'model_a': df['winner_model_a'].value_counts(),
    'model_b': df['winner_model_b'].value_counts(),
    'tie': df['winner_tie'].sum()
}

print(winner_counts)
```

---The following area is a Code cell (cell numver is 16)---
```python
# winner_model_a ve winner_model_b sÃ¼tunlarÄ± sÄ±fÄ±r olan satÄ±rlarÄ± filtreleme
zero_winner_df = df[(df['winner_model_a'] == 1.0) & (df['winner_model_b'] == 1.0)].astype("float64")
```

---The following area is a Code cell (cell numver is 17)---
```python
# winner_model_a sÃ¼tunu 1 olan satÄ±rlarÄ± filtreleme
model_a_winner_df = combined_df[combined_df['winner_model_a'] == 1.0]

# winner_model_b sÃ¼tunu 1 olan satÄ±rlarÄ± filtreleme
model_b_winner_df = combined_df[combined_df['winner_model_b'] == 1.0]

# 1 olan satÄ±rlarda model_a ve model_b'nin en Ã§ok tekrar edileni bulma
model_a_counts = model_a_winner_df['model_a'].value_counts()
model_b_counts = model_b_winner_df['model_b'].value_counts()

print("model_a en Ã§ok tekrar edenler (combined):")
print(model_a_counts.head())

print("\nmodel_b en Ã§ok tekrar edenler (combined):")
print(model_b_counts.head())
```

---The following area is a Code cell (cell numver is 18)---
```python
# model_a sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma
response_a_counts = model_a_winner_df['response_a'].value_counts()
response_b_counts = model_b_winner_df['response_b'].value_counts()
```

---The following area is a Code cell (cell numver is 19)---
```python
# response_a sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma ve kazanan modele gÃ¶re gruplandÄ±rma
response_a_counts_df = model_a_winner_df.groupby(['model_a', 'prompt']).size().reset_index(name='count')

# response_b sÃ¼tununda en Ã§ok tekrar eden yanÄ±tlarÄ± bulma ve kazanan modele gÃ¶re gruplandÄ±rma
response_b_counts_df = model_b_winner_df.groupby(['model_b', 'prompt']).size().reset_index(name='count')

print("response_a en Ã§ok tekrar edenler (combined):")
print(response_a_counts_df.head())

print("\nresponse_b en Ã§ok tekrar edenler (combined):")
print(response_b_counts_df.head())
```

---The following area is a Code cell (cell numver is 20)---
```python
response_a_counts_df
```

---The following area is a Code cell (cell numver is 21)---
```python
top_models = ['gpt-4-1106-preview', 'gpt-4-0613', 'gpt-3.5-turbo-0613', 'gpt-4-0314']

```

---The following area is a Code cell (cell numver is 22)---
```python
# Her model iÃ§in en Ã§ok tekrar eden 10 yanÄ±tÄ± almak
for model in top_models:
    print(f"\nModel: {model} - En Ã§ok tekrar eden 10 yanÄ±t (response_a):")
    top_responses_a = response_a_counts_df[response_a_counts_df['model_a'] == model].sort_values(by='count', ascending=False).head(10)
    print(top_responses_a)

    print(f"\nModel: {model} - En Ã§ok tekrar eden 10 yanÄ±t (response_b):")
    top_responses_b = response_b_counts_df[response_b_counts_df['model_b'] == model].sort_values(by='count', ascending=False).head(10)
    print(top_responses_b)
```

---The following area is a Code cell (cell numver is 23)---
```python
# Kazanan modelin adÄ±nÄ± belirleyen ve yeni bir sÃ¼tun oluÅŸturan fonksiyon
def determine_winner(row):
    if row['winner_model_a'] == 1.0:
        return row['model_a']
    elif row['winner_model_b'] == 1.0:
        return row['model_b']
    else:
        return 'tie'

# Kazanan sÃ¼tunu oluÅŸturma
df['winner'] = df.apply(determine_winner, axis=1)

# Sadece prompt ve kazanan sÃ¼tununu alarak gruplama iÅŸlemi
grouped_df = df[['prompt', 'winner']].groupby('prompt')['winner'].apply(list).reset_index()

# CSV olarak Ã§Ä±ktÄ± alma
grouped_df.to_csv('prompt_winners.csv', index=False)

print("CSV dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu: 'prompt_winners.csv'")
```

---The following area is a Code cell (cell numver is 24)---
```python
prompt = pd.read_csv("/kaggle/working/prompt_winners.csv")
prompt.groupby("winner").count().reset_index().head(20)
```

---The following area is a Code cell (cell numver is 25)---
```python
# PromptlarÄ± kazanan modele gÃ¶re gruplayarak en Ã§ok tekrar edenleri bulma
winner_counts = prompt_df.explode('winner').groupby('winner').size().reset_index(name='count')
winner_counts = winner_counts.sort_values(by='count', ascending=False)

# CSV olarak Ã§Ä±ktÄ± alma
winner_counts.to_csv('prompt_top_winners.csv', index=False)

print("CSV dosyasÄ± baÅŸarÄ±yla oluÅŸturuldu: 'prompt_top_winners.csv'")
```

---The following area is a Code cell (cell numver is 26)---
```python
winner_counts.head(20)
```

---The following area is a Code cell (cell numver is 27)---
```python
# Prompt ve kazanan modeli gruplayarak Ã§Ä±ktÄ± alma
prompt_winners = df.groupby(['winner', 'prompt']).sum().reset_index()

prompt_winners
```

---The following area is a Code cell (cell numver is 28)---
```python
top_prompt = pd.read_csv("/kaggle/working/prompt_top_winners.csv")
top_prompt.head(20)
```

---The following area is a Code cell (cell numver is 29)---
```python
# 'tie' durumunda response_a ve response_b deÄŸerlerini gruplayarak Ã§Ä±ktÄ± alma
tie_responses = df[df['winner'] == 'tie'].groupby(['response_a', 'response_b']).sum().reset_index()[['response_a', 'response_b',"prompt"]]

tie_responses
```

---The following area is a Code cell (cell numver is 30)---
```python
def determine_winner_and_loser(row):
    if row['winner_model_a'] == 1.0:
        winner = row['model_a']
        loser = row['model_b']
        winner_response = row['response_a']
        loser_response = row['response_b']
    elif row['winner_model_b'] == 1.0:
        winner = row['model_b']
        loser = row['model_a']
        winner_response = row['response_b']
        loser_response = row['response_a']
    else:
        winner = 'tie'
        loser = 'tie'
        winner_response = ''
        loser_response = ''
    
    return pd.Series([winner, loser, len(winner_response), len(loser_response)], index=['winner', 'loser', 'winner_response_length', 'loser_response_length'])

# Kazanan ve kaybeden sÃ¼tunlarÄ±nÄ± oluÅŸturma
winner_loser_lengths = df.apply(determine_winner_and_loser, axis=1)

# Ã–nceki DataFrame'e kazanan ve kaybeden uzunluklarÄ±nÄ± ekleyerek yeni bir DataFrame oluÅŸturma
prompt_df = pd.concat([df[['prompt']], winner_loser_lengths], axis=1)

prompt_df.head(20)
```

---The following area is a Code cell (cell numver is 31)---
```python
# CSV dosyalarÄ±nÄ± okuma
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# BirleÅŸtirme iÅŸlemi
df = pd.concat([train_df, test_df], ignore_index=True)
```

---The following area is a Code cell (cell numver is 32)---
```python
train_df
```

---The following area is a Code cell (cell numver is 33)---
```python
df
```

---The following area is a Code cell (cell numver is 34)---
```python
# Polars DataFrame oluÅŸturma
import polars as pl
from textblob import TextBlob
import numpy as np
```

---The following area is a Code cell (cell numver is 35)---
```python
import polars as pl
import numpy as np
from textblob import TextBlob

# Polars DataFrame oluÅŸturma
df = pl.DataFrame(df)

# Sentiment analizi ve error sayÄ±mÄ± fonksiyonlarÄ±
def batch_analyze_sentiment(texts):
    return np.array([TextBlob(text).sentiment.polarity for text in texts])

def batch_count_errors(texts):
    return np.array([text.lower().count('error') for text in texts])

# Yeni sÃ¼tunlar oluÅŸturma
df = df.with_columns([
    pl.col('response_a').str.lengths().alias('response_a_length'),
    pl.col('response_b').str.lengths().alias('response_b_length'),
    (pl.col('response_a').str.lengths() - pl.col('response_b').str.lengths()).abs().alias('response_length_difference'),
])

# Sentiment analizi ve error sayÄ±mÄ± iÃ§in ilk 100 satÄ±rÄ± iÅŸleme al
if len(df) > 0:
    n_rows = min(100, len(df))
    
    sentiment_a = batch_analyze_sentiment(df['response_a'].head(n_rows).to_numpy())
    sentiment_b = batch_analyze_sentiment(df['response_b'].head(n_rows).to_numpy())
    errors_a = batch_count_errors(df['response_a'].head(n_rows).to_numpy())
    errors_b = batch_count_errors(df['response_b'].head(n_rows).to_numpy())

    # Yeni sÃ¼tunlarÄ± ekleme
    df = df.with_columns([
        pl.Series('sentiment_a', sentiment_a).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('sentiment_b', sentiment_b).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('errors_a', errors_a).extend(pl.Series([None] * (len(df) - n_rows))),
        pl.Series('errors_b', errors_b).extend(pl.Series([None] * (len(df) - n_rows))),
    ])

# Kazanan ve kaybeden sÃ¼tunlarÄ±nÄ± oluÅŸturma (eski sÃ¼rÃ¼mlerle uyumlu)
df = df.with_columns([
    pl.when(pl.col('sentiment_a') > pl.col('sentiment_b'))
      .then(pl.col('response_a'))
      .when(pl.col('sentiment_a') <= pl.col('sentiment_b'))
      .then(pl.col('response_b'))
      .alias('winner_sentiment'),
    pl.when(pl.col('sentiment_a') < pl.col('sentiment_b'))
      .then(pl.col('response_a'))
      .when(pl.col('sentiment_a') >= pl.col('sentiment_b'))
      .then(pl.col('response_b'))
      .alias('loser_sentiment')
])

df
```

** @@@ Jupyter Notebook numver 41, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction


In this notebook we demostrate how to finetune **Llama3** (8B) model from Meta using **QLoRA** and **SFTTrainer** from **tlr**.

## What is Llama3?

Llama3 is the latest release of open-source LLMs from Meta, with features pretrained and instruction-fine-tuned language models with 8B and 70B parameters.

## What is LoRA?

LoRA stands for Low-Rank Adaptation. It is a method used to fine-tune large language models (LLMs) by freezing the weights of the LLM and injecting trainable rank-decomposition matrices. The number of trainable parameters during fine-tunning will decrease therefore considerably. According to LoRA paper, this number decreases 10,000 times, and the computational resources size decreases 3 times.


## What is QLoRA?

QLoRA builds on LoRA by incorporating quantization techniques to further reduce memory usage while maintaining, or even enhancing, model performance.Â With QLoRA it is possible to finetune a 70B parameter model that requires 36 GPUs with only 2!


## What is PEFT?

Parameter-efficient Fine-tuning (PEFT) is a technique used in Natural Language Processing (NLP) to improve the performance of pre-trained language models on specific downstream tasks. It involves reusing the pre-trained modelâ€™s parameters and fine-tuning them on a smaller dataset, which saves computational resources and time compared to training the entire model from scratch. PEFT achieves this efficiency by freezing some of the layers of the pre-trained model and only fine-tuning the last few layers that are specific to the downstream task.

## What is SFTTrainer?

SFT in SFTTrainer stands for supervised fine-tuning. The **trl** (Transformer Reinforcement Learning) library from HuggingFace provides a simple API to fine-tune models using SFTTrainer.

## What is UltraChat200k?  

UltraChat-200k is an invaluable resource for natural language understanding, generation and dialog system research. With 1.4 million dialogues spanning a variety of topics, this parquet-formatted dataset offers researchers four distinct formats to aid in their studies: test_sft, train_sft, train_gen and test_gen. More details [here](https://www.kaggle.com/datasets/thedevastator/ultrachat-200k-nlp-dataset).

## Inspiration

For this notebook, I took inspiration from several sources:
* [Efficiently fine-tune Llama 3 with PyTorch FSDP and Q-Lora](https://www.philschmid.de/fsdp-qlora-llama3)  
* [Fine-tuning LLMs using LoRA](https://medium.com/@rajatsharma_33357/fine-tuning-llama-using-lora-fb3f48a557d5)  
* [Fine-tuning Llama-3â€“8B-Instruct QLORA using low cost resources](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)  
* [Llama2 Fine-Tuning with Low-Rank Adaptations (LoRA) on Gaudi 2 Processors](https://eduand-alvarez.medium.com/llama2-fine-tuning-with-low-rank-adaptations-lora-on-gaudi-2-processors-52cf1ee6ce11)
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Install and import libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes
!pip install -q -U transformers
!pip install -q -U peft
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
```

---The following area is a Code cell (cell numver is 4)---
```python
# from kaggle_secrets import UserSecretsClient
# user_secrets = UserSecretsClient()
# wandb_key = user_secrets.get_secret("wandb_api")
# import wandb
# ! wandb login $wandb_key
```

---The following area is a Code cell (cell numver is 5)---
```python
import os
import torch
from time import time
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    Trainer
)
from trl import SFTTrainer,setup_chat_format
import numpy as np
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 16
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Prepare Dataset
```

---The following area is a Code cell (cell numver is 8)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Initialize model


The model used is:

* **Model**: Llama3  
* **Framework**: Transformers   
* **Size**: 8B   
* **Type**: 8b-chat-hf (hf stands for HuggingFace). 
* **Version**: V1
```

---The following area is a Code cell (cell numver is 10)---
```python
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification

quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)

model_name = "/kaggle/input/llama-3/transformers/8b-chat-hf/1"

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    quantization_config=quantization_config,
    num_labels=4,
    device_map='auto'
)
```

---The following area is a Code cell (cell numver is 11)---
```python
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model

lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    task_type = 'SEQ_CLS'
)

model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
```

---The following area is a Code cell (cell numver is 12)---
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)

tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 13)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 14)---
```python
from datasets import DatasetDict, Dataset

def data_preprocesing(row):
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')

dataset = Dataset.from_pandas(train)
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])
tokenized_data.set_format("torch")
```

---The following area is a Code cell (cell numver is 15)---
```python
import gc

del train
gc.collect()
```

---The following area is a Code cell (cell numver is 16)---
```python
from transformers import DataCollatorWithPadding

collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 17)---
```python
def compute_metrics(evaluations):
    predictions, labels = evaluations
    predictions = np.argmax(predictions, axis=1)
    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),
    'accuracy':accuracy_score(predictions,labels)}
```

---The following area is a Code cell (cell numver is 18)---
```python
import torch.nn.functional as F

class CustomTrainer(Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)
        else:
            self.class_weights = None

    def compute_loss(self, model, inputs, return_outputs=False):
#         print(inputs)
        labels = inputs.pop("labels").long()

        outputs = model(**inputs)

        logits = outputs.get('logits')

        if self.class_weights is not None:
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)
        else:
            loss = F.cross_entropy(logits, labels)

        return (loss, outputs) if return_outputs else loss
```

---The following area is a Code cell (cell numver is 19)---
```python
training_args = TrainingArguments(
    output_dir = 'sentiment_classification',
    learning_rate = 1e-4,
    per_device_train_batch_size = 8,
    per_device_eval_batch_size = 8,
    num_train_epochs = 1,
    logging_steps=1,
    weight_decay = 0.01,
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    load_best_model_at_end = True,
    report_to="none"
)
```

---The following area is a Code cell (cell numver is 20)---
```python
trainer = CustomTrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_data,
    tokenizer = tokenizer,
    data_collator = collate_fn,
    compute_metrics = compute_metrics,
#     class_weights=class_weights,
)

train_result = trainer.train()
```

---The following area is a Code cell (cell numver is 21)---
```python

```

** @@@ Jupyter Notebook numver 42, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import numpy as np
from datasets import Dataset
from functools import partial
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, TFAutoModel
from transformers import DebertaV2Tokenizer
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input
from keras.preprocessing import sequence as sq
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train.head(5)
```

---The following area is a Code cell (cell numver is 4)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test.head(5)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Preprocessing
```

---The following area is a Code cell (cell numver is 6)---
```python
def combine_text(df):
    def process(input_str):
        stripped_str = input_str.strip('[]')
        sentences = [s.strip('"') for s in stripped_str.split('","')]
        return  ' '.join(sentences)

    # Flatten the arrays to single strings
    df['prompt'] = df['prompt'].apply(process)
    df['response_a'] = df['response_a'].apply(process)
    df['response_b'] = df['response_b'].apply(process)
    
    # Combine text data
#     df['combined_text'] = '[PROMPT] ' + df['prompt'] + ' [RESPONSE_A] ' + df['response_a'] + ' [RESPONSE_B] ' + df['response_b'] 
```

---The following area is a Code cell (cell numver is 7)---
```python
combine_text(train)
# print(train['combined_text'][69])
```

---The following area is a Code cell (cell numver is 8)---
```python
# Create labels
def create_label(df):
    def process(row):
        if row['winner_model_a'] == 1:
            return 0
        elif row['winner_model_b'] == 1:
            return 1
        elif row['winner_tie'] == 1:
            return 2
        
    df['label'] = df.apply(process, axis=1)
```

---The following area is a Code cell (cell numver is 9)---
```python
create_label(train)
print(train['label'][69])
```

---The following area is a Code cell (cell numver is 10)---
```python
print("train.shape", train.shape)
train.head()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## Tokenizer
```

---The following area is a Code cell (cell numver is 12)---
```python
max_length = 1024
```

---The following area is a Code cell (cell numver is 13)---
```python
# Tokenization using DebertaV2Tokenizer
model_name = "/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1"
# model_name = "/kaggle/input/deberta-v3/pytorch/large/1"
tokenizer = AutoTokenizer.from_pretrained(model_name)
# Limit the vocabulary size
# tokenizer.model_max_length = max_length
tokenizer.add_tokens(['[CLS]', '[SEP]', '[PAD]'], special_tokens=True)
```

---The following area is a Code cell (cell numver is 14)---
```python
def tokenize_df(df, tokenizer):
    # Check and set special tokens if they are not present
    if tokenizer.cls_token_id is None:
        tokenizer.cls_token_id = tokenizer.convert_tokens_to_ids('[CLS]')
    if tokenizer.sep_token_id is None:
        tokenizer.sep_token_id = tokenizer.convert_tokens_to_ids('[SEP]')
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')
        
    def process(row):
        max_len = max_length - 2 # 2 separator tokens
        # Tokenize prompt
        prompt_tokens = tokenizer(row['prompt'], truncation=True, max_length=max_len//4)['input_ids']
        remaining_length = max_len - len(prompt_tokens)

        # Tokenize response A
        response_a_tokens = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2)['input_ids']
        remaining_length -= len(response_a_tokens)

        # Tokenize response B
        response_b_tokens = tokenizer(row['response_b'], truncation=True, max_length=remaining_length//2)['input_ids']

        # Add responses
        input_ids = [tokenizer.cls_token_id] + prompt_tokens + [tokenizer.sep_token_id] + response_a_tokens + [tokenizer.sep_token_id] + response_b_tokens
        token_type_ids = [0] * (len(prompt_tokens) + 2) + [1] * (len(response_a_tokens) + 1) + [2] * len(response_b_tokens)
        attention_mask = [1] * len(input_ids)

        # Add padding
        padding_length = max_length - len(input_ids)
        if padding_length > 0:
            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length
            token_type_ids = token_type_ids + [0] * padding_length
            attention_mask = attention_mask + [0] * padding_length

        input_ids = input_ids[:max_length]
        token_type_ids = token_type_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        
        return input_ids, token_type_ids, attention_mask
    
    df[['input_ids', 'token_type_ids', 'attention_mask']] = df.apply(lambda row: pd.Series(process(row)), axis=1)
#     tokenized = df.apply(lambda row: pd.Series(process(row)), axis=1)
#     df.loc[:, ['input_ids', 'token_type_ids', 'attention_mask']] = tokenized
#     return df
```

---The following area is a Code cell (cell numver is 15)---
```python
# Convert labels to categorical
labels = tf.keras.utils.to_categorical(train['label'], num_classes=3)
```

---The following area is a Code cell (cell numver is 16)---
```python
tokenize_df(train, tokenizer)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Prepare data for training
input_ids = train['input_ids']
attention_mask = train['attention_mask']

X_train = sq.pad_sequences(input_ids, maxlen=max_length)
X_train_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)

y_train = labels
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Model
```

---The following area is a Code cell (cell numver is 19)---
```python
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D, Masking

# Define the CNN model
def create_cnn_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        Conv1D(filters=256, kernel_size=5, activation='relu'),
        GlobalMaxPooling1D(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Define the LSTM model
def create_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        LSTM(256, return_sequences=True),
        GlobalMaxPooling1D(),
        Dense(128, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model

# Define the CNN LSTM model
def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),
        Conv1D(filters=128, kernel_size=5, activation='relu'),
        LSTM(128, return_sequences=True),
        GlobalMaxPooling1D(),
        Dense(64, activation='relu'),
        Dropout(0.5),
        Dense(3, activation='softmax')
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    return model
```

---The following area is a Code cell (cell numver is 20)---
```python
# Parameters
vocab_size = tokenizer.vocab_size
# vocab_size = max_length
embedding_dim = 100
max_length = max_length
max_features = tokenizer.vocab_size
# max_features = max_length * 2
max_len = max_length
maxlen = max_len
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
# Create the model
# model = create_lstm_model(vocab_size, embedding_dim, max_length)
model = create_cnn_lstm_model(vocab_size, embedding_dim, max_length)
model.summary()
```

---The following area is a Code cell (cell numver is 21)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint
```

---The following area is a Code cell (cell numver is 22)---
```python
# from tensorflow.keras.layers import Layer
# from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D
# from keras.layers import  GlobalMaxPooling1D
# import tensorflow as tf

# class ApplyAttentionMask(Layer):
#     def call(self, inputs):
#         embeddings, attention_mask = inputs
#         return embeddings * tf.expand_dims(attention_mask, -1)

# input_layer = Input(shape=(max_length,),dtype='int32', name='main_input')
# attention_masks = Input(shape=(max_length,), dtype='float32', name="attention_masks")

# emb_layer = Embedding(max_features,
#                       embedding_dims,
#                       input_length=max_len
#                       )(input_layer)

# masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])

# # LSTM branch
# lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)
# lstm_out = LSTM(64, return_sequences=True)(lstm_out)
# lstm_out = LSTM(32)(lstm_out)
# lstm_out = BatchNormalization()(lstm_out)
# lstm_out = Dropout(0.5)(lstm_out)
# lstm_out = GlobalMaxPooling1D()(lstm_out)

# # CNN layer branch
# cnn_out = Conv1D(128, 5, activation='relu')(masked_embeddings)
# cnn_out = Conv1D(64, 5, activation='relu')(cnn_out)
# cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)
# cnn_out = BatchNormalization()(cnn_out)
# cnn_out = Dropout(0.5)(cnn_out)
# cnn_out = GlobalMaxPooling1D()(cnn_out)


# # Concatenate LSTM and CNN outputs
# merged = concatenate([lstm_out, cnn_out])
# merged = Dense(32, activation='sigmoid')(merged)
# merged = BatchNormalization()(merged)
# merged = Dropout(0.5)(merged)
# pred = Dense(3, activation='softmax')(merged)


# # Build model
# model = Model(inputs=[input_layer, attention_masks], outputs=[pred])
# adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
# adamax = Adamax(learning_rate=0.001)
# model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])
# model.summary()
```

---The following area is a Code cell (cell numver is 23)---
```python
# import tensorflow as tf
# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Masking
# from tensorflow.keras.models import Model
# from transformers import DebertaTokenizer, AutoModel

# def create_cnn_lstm_hybrid_model(base_model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes):
#     # Load pre-trained BERT model
#     model = AutoModel.from_pretrained(base_model_name)
    
#     # Define inputs
#     input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')
#     attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')

#     # Get BERT outputs
#     outputs = model(input_ids, attention_mask=attention_mask)
#     seq_output = outputs.last_hidden_state  # Shape: (batch_size, seq_length, hidden_size)

#     # Apply CNN
#     cnn_output = Conv1D(filters=cnn_output_channels, kernel_size=cnn_kernel_size, padding='same', activation='relu')(bert_seq_output)

#     # Apply Masking to handle padded sequences
#     masked_cnn_output = Masking()(cnn_output)
    
#     # Apply LSTM
#     rnn_output = LSTM(hidden_dim)(masked_cnn_output)

#     # Define the classifier layer
#     logits = Dense(num_classes, activation='softmax')(rnn_output)

#     # Create the model
#     model = Model(inputs=[input_ids, attention_mask], outputs=logits)

#     return model
```

---The following area is a Code cell (cell numver is 24)---
```python
# Define hyperparameters
# bert_model_name = '/kaggle/input/deberta_v3/keras/deberta_v3_large_en/2'
# bert_model_name = '/kaggle/input/deberta-v3/pytorch/large/1'
# bert_model_name = 'deberta_v3_large_en'
# model_name = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'
# cnn_output_channels = 128
# cnn_kernel_size = 5
# hidden_dim = 256
# num_classes = 3

# # Initialize the model
# model = create_cnn_lstm_hybrid_model(model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes)
# model.summary()
```

---The following area is a Code cell (cell numver is 25)---
```python
from keras.callbacks import EarlyStopping

# Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)

history = model.fit([X_train, X_train_attention_mask], y_train, epochs=20, batch_size=32, validation_split=0.2
                    , callbacks=[early_stopping])
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Test
```

---The following area is a Code cell (cell numver is 27)---
```python
# Encode test data
combine_text(test)
tokenize_df(test, tokenizer)

input_ids = test['input_ids']
attention_mask = test['attention_mask']

X_test = sq.pad_sequences(input_ids, maxlen=max_length)
X_test_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)
```

---The following area is a Code cell (cell numver is 28)---
```python
predictions = model.predict([X_test, X_test_attention_mask])
predictions
```

---The following area is a Code cell (cell numver is 29)---
```python
winner = pd.DataFrame(predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result = pd.concat([test['id'], winner], axis=1)
result.to_csv('submission.csv', index=False)
result
```

** @@@ Jupyter Notebook numver 43, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Setup
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## Model Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Load sample data
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv(TRAIN_CSV)
train = train.head(20)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenizer and Preprocess dataset
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 9)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Metrics
```

---The following area is a Code cell (cell numver is 11)---
```python
def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Model
```

---The following area is a Code cell (cell numver is 13)---
```python
class CustomLlama3(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 14)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 15)---
```python
model = CustomLlama3.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
### Training Arguments
```

---The following area is a Code cell (cell numver is 17)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=5,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2, ##GPU 16GB o
    per_device_eval_batch_size=2,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Training (Only Run one of Training or Inference)
```

---The following area is a Code cell (cell numver is 19)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

---The following area is a Code cell (cell numver is 20)---
```python
model.save_pretrained('pretrained_model')
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# Inference (Only Run one of Training or Inference)
```

---The following area is a Code cell (cell numver is 22)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 23)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  file Lora adapter
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
lora_adapter_path = "/kaggle/working/pretrained_model"

# Táº£i mÃ´ hÃ¬nh gá»‘c
model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# Táº£i tokenizer tÆ°Æ¡ng á»©ng
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Cáº¥u hÃ¬nh Lora
lora_config = peft_config

# Chuáº©n bá»‹ mÃ´ hÃ¬nh cho k-bit training náº¿u cáº§n
model_1 = prepare_model_for_kbit_training(model_1)

# Ãp dá»¥ng Lora Adapter vÃ o mÃ´ hÃ¬nh
model_1 = get_peft_model(model_1, lora_config)

# Táº£i cÃ¡c tham sá»‘ cá»§a Lora Adapter Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
model_1.load_adapter(lora_adapter_path, adapter_name="test")

# MÃ´ hÃ¬nh hoÃ n chá»‰nh Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng
model_1.eval()  # Äáº·t mÃ´ hÃ¬nh vÃ o cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ náº¿u cáº§n
```

---The following area is a Code cell (cell numver is 24)---
```python
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
import pandas as pd
import numpy as np

def softmax(row):
    e_row = np.exp(row - np.max(row))
    return e_row / e_row.sum()
```

---The following area is a Code cell (cell numver is 25)---
```python
data = ds.to_pandas()[0:10]
data["max_len"] = data["input_ids"].apply(len)
display(data[:3])
print()

print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 26)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=2, max_length=1024):
    a_win, b_win, tie = [], [], []

    model.eval()
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        labels = tmp["labels"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)
        pad_labels=[]
        for label in labels:
            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])
            pad_labels.append(label)
        labels = torch.tensor(pad_labels).to(device)
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    df['winner_model_a'] = a_win
    df['winner_model_b'] = b_win
    df['winner_tie'] = tie
    return df
```

---The following area is a Code cell (cell numver is 27)---
```python
result_df  = inference(data[0:4], model_1, device, batch_size=2, max_length=1024)

proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values
        
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]

result_df['winner_model_a'] = result_df['winner_model_a'].apply(lambda x: x[0])
result_df['winner_model_b'] = result_df['winner_model_b'].apply(lambda x: x[0])
result_df['winner_tie'] = result_df['winner_tie'].apply(lambda x: x[0])

result_df
```

** @@@ Jupyter Notebook numver 44, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import pandas as pd
import torch
from transformers import AutoTokenizer, AutoModel
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score

```

---The following area is a Code cell (cell numver is 1)---
```python
INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
train_df = pd.read_csv(f"{INPUT_DIR}/train.csv")

```

---The following area is a Code cell (cell numver is 2)---
```python
train_df
```

---The following area is a Code cell (cell numver is 3)---
```python
train_df.info()

```

---The following area is a Code cell (cell numver is 4)---
```python
MODEL_ID = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModel.from_pretrained(MODEL_ID)
```

---The following area is a Code cell (cell numver is 5)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

print(device)
```

---The following area is a Code cell (cell numver is 6)---
```python
# Move model to GPU
model.to(device)

```

---The following area is a Code cell (cell numver is 7)---
```python
if tokenizer.pad_token is None:
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    tokenizer.pad_token = '[PAD]'

```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_text(text):
    return tokenizer(text, padding=True, truncation=True, return_tensors="pt").to(device)
```

---The following area is a Code cell (cell numver is 9)---
```python
train_df['prompt_tokens'] = train_df['prompt'].apply(tokenize_text)
train_df['response_a_tokens'] = train_df['response_a'].apply(tokenize_text)
train_df['response_b_tokens'] = train_df['response_b'].apply(tokenize_text)

```

---The following area is a Code cell (cell numver is 10)---
```python
train_df
```

---The following area is a Code cell (cell numver is 11)---
```python
def get_embeddings(text_tokens):
    with torch.no_grad():
        outputs = model(**text_tokens)
    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

```

---The following area is a Code cell (cell numver is 12)---
```python
train_df['prompt_embeddings'] = train_df['prompt_tokens'].apply(lambda x: get_embeddings(x))
train_df['response_a_embeddings'] = train_df['response_a_tokens'].apply(lambda x: get_embeddings(x))
train_df['response_b_embeddings'] = train_df['response_b_tokens'].apply(lambda x: get_embeddings(x))

```

---The following area is a Code cell (cell numver is 13)---
```python
X = pd.concat([pd.DataFrame(train_df['prompt_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_a_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_b_embeddings'].tolist())], axis=1)
y = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)

```

---The following area is a Code cell (cell numver is 14)---
```python
#input 
X
```

---The following area is a Code cell (cell numver is 15)---
```python
#output
y
```

---The following area is a Code cell (cell numver is 16)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 17)---
```python
clf = RandomForestClassifier(n_estimators=100, random_state=42)
clf.fit(X_train, y_train)

```

---The following area is a Code cell (cell numver is 18)---
```python
y_pred = clf.predict(X_val)
accuracy = accuracy_score(y_val, y_pred)
print(f'Validation Accuracy: {accuracy:.4f}')

```

---The following area is a Code cell (cell numver is 19)---
```python
test_df = pd.read_csv(f"{INPUT_DIR}/test.csv")
```

---The following area is a Code cell (cell numver is 20)---
```python
# Process Test Data
test_df['prompt_tokens'] = test_df['prompt'].apply(tokenize_text)
test_df['response_a_tokens'] = test_df['response_a'].apply(tokenize_text)
test_df['response_b_tokens'] = test_df['response_b'].apply(tokenize_text)

test_df['prompt_embeddings'] = test_df['prompt_tokens'].apply(get_embeddings)
test_df['response_a_embeddings'] = test_df['response_a_tokens'].apply(get_embeddings)
test_df['response_b_embeddings'] = test_df['response_b_tokens'].apply(get_embeddings)

# Prepare Test Features
X_test = pd.concat([pd.DataFrame(test_df['prompt_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_a_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_b_embeddings'].tolist())], axis=1)
```

---The following area is a Code cell (cell numver is 21)---
```python
X_test
```

---The following area is a Code cell (cell numver is 22)---
```python
y_test_pred_prob = clf.predict_proba(X_test)

submission_df = pd.DataFrame(y_test_pred_prob, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission_df.insert(0, 'id', test_df['id'])

```

---The following area is a Code cell (cell numver is 23)---
```python
submission_df
```

---The following area is a Code cell (cell numver is 24)---
```python
submission_df.to_csv('/kaggle/working/submission.csv', index=False)

print("Submission file created.")
```

** @@@ Jupyter Notebook numver 45, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
```

---The following area is a Code cell (cell numver is 1)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Create a new DataFrame with the same 'id' column as 'test'
sample_submission = pd.DataFrame({'id': test['id']})

# Generate random winner columns
n_rows = len(test)
winners = np.random.choice(['a', 'b', 'tie'], size=n_rows)

# Create the winner columns
sample_submission['winner_model_a'] = (winners == 'a').astype(int)
sample_submission['winner_model_b'] = (winners == 'b').astype(int)
sample_submission['winner_tie'] = (winners == 'tie').astype(int)

# Verify that the sum of winner columns is 1 for each row
assert (sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1) == 1).all()

# Display the first few rows of the new DataFrame
print(sample_submission.head())
```

---The following area is a Code cell (cell numver is 3)---
```python
sample_submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 46, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction
 This notebook provides a comprehensive guide for loading, preprocessing, and training a text classification model using the Hugging Face Transformers library. We will use the Roberta tokenizer and a smaller Roberta model to manage computational resources efficiently. The process includes data loading, sampling, tokenization, and model preparation.

# Step 1: Load and Sample Data
 First, we load the dataset from a CSV file and take a random sample to reduce the dataset size for quicker processing.
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
from sklearn.model_selection import train_test_split

# Load data
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")

# Sample 10% of the data
df_sample = df.sample(frac=0.1, random_state=42)

# Check sample data
print(df_sample.head())
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Step 2: Split Data into Training and Validation Sets
We split the data into training and validation sets to evaluate the model's performance.
```

---The following area is a Code cell (cell numver is 3)---
```python
# Split data into training and validation sets
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_sample['prompt'].tolist(), 
    df_sample['winner_model_a'], 
    test_size=0.1, 
    random_state=42
)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Step 3: Load and Prepare the Tokenizer
We use a smaller Roberta model tokenizer for efficiency and tokenize the text data.
```

---The following area is a Code cell (cell numver is 5)---
```python
from transformers import RobertaTokenizer

# Load tokenizer
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

# Tokenize function
def tokenize_function(texts):
    return tokenizer(
        texts,
        padding="max_length",
        truncation=True,
        max_length=128,
        return_tensors='pt'
    )

# Tokenize texts
train_encodings = tokenize_function(train_texts)
val_encodings = tokenize_function(val_texts)

# Check tokenized data
print(train_encodings)
print(val_encodings)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Step 4: Prepare the Model Configuration
 Set up the configuration for the model, ensuring it uses the appropriate device (GPU if available).
```

---The following area is a Code cell (cell numver is 7)---
```python
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification
from torch.utils.data import Dataset
import torch
import numpy as np

# Config class
class Config:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.batch_size = 16

cfg = Config()

# Load model
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2).to(cfg.device)

# CustomDataset class
class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = np.array(labels, dtype=int)
    
    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)
        return item
    
    def __len__(self):
        return len(self.labels)

# Create datasets
train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

# TrainingArguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=cfg.batch_size,
    per_device_eval_batch_size=cfg.batch_size,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# Train model
trainer.train()
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Step 5: Inference Function
Define a function to perform inference on the tokenized data using the trained model.
```

---The following area is a Code cell (cell numver is 9)---
```python
def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):
    model.eval()
    results = []
    with torch.no_grad():
        for i in range(0, len(input_ids), batch_size):
            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)
            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)
            results.extend(outputs.logits.cpu().numpy())
    return np.array(results)
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# Step 6: Evaluate and Save Results
Tokenize the test data, perform inference, and save the results to a CSV file.
```

---The following area is a Code cell (cell numver is 11)---
```python
# Tokenize the test data
test_texts = df_sample['prompt'].tolist()
test_encodings = tokenize_function(test_texts)

# Perform inference
results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])

# Convert results to DataFrame and save as CSV
results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])
submission = pd.concat([df_sample[['id']], results_df], axis=1)
submission.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Conclusion
This notebook provided a structured approach to loading, preprocessing, and training a text classification model using the Transformers library. The steps included data sampling, tokenization, model configuration, and inference. The final results were saved for further analysis. This methodology ensures efficient use of computational resources while maintaining model performance.
```

** @@@ Jupyter Notebook numver 47, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS - Chatbot Arena Human Preference Predictions
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
**Due to the size of the train data, and I only using 0.5% of the train data!**

WIP: Compute embeddings using TPU in a differente notebook to use the full train data and then load the embeddings here!
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Install and load libraries
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install textstat SweetViz
```

---The following area is a Code cell (cell numver is 4)---
```python
import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np

import seaborn as sns
import matplotlib.pyplot as plt
import plotly.express as px
import sweetviz as sv

import re
import string
import nltk
from nltk.corpus import stopwords

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

from nltk.sentiment.vader import SentimentIntensityAnalyzer

import textstat
from sklearn.feature_extraction.text import CountVectorizer

from sklearn.model_selection import train_test_split

from sklearn.metrics.pairwise import cosine_similarity
from transformers import BertTokenizer, TFBertModel

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam

from sklearn.metrics import accuracy_score, \
                            log_loss, \
                            confusion_matrix, \
                            classification_report
```

---The following area is a Code cell (cell numver is 5)---
```python
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## Loading data
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Exploratory Data Analysis (EDA)
```

---The following area is a Code cell (cell numver is 9)---
```python
train_analysis = sv.analyze(train_data)
```

---The following area is a Code cell (cell numver is 10)---
```python
train_analysis.show_html('train_analysis.html')
```

---The following area is a Code cell (cell numver is 11)---
```python
train_data.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
print("Training Data -", train_data.shape)
print("Test Data -", test_data.shape)
```

---The following area is a Code cell (cell numver is 13)---
```python
train_data.describe(include=['O'])
```

---The following area is a Code cell (cell numver is 14)---
```python
print(train_data.info())
```

---The following area is a Code cell (cell numver is 15)---
```python
train_data.drop("id", axis=1).duplicated().sum()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
There exist 14 duplicated rows forming 7 groups, I will just keep one row per group.
```

---The following area is a Code cell (cell numver is 17)---
```python
train_data = train_data.drop_duplicates(keep="first", ignore_index=True)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
Checking the quality of the train data with respect the `id` column.
```

---The following area is a Code cell (cell numver is 19)---
```python
train_data.nunique()
```

---The following area is a Code cell (cell numver is 20)---
```python
assert train_data["id"].nunique() == len(train_data)
```

---The following area is a Code cell (cell numver is 21)---
```python
train_data.isna().sum()
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
### Distribution
```

---The following area is a Code cell (cell numver is 23)---
```python
model_df = pd.concat([train_data.model_a, train_data.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()
```

---The following area is a Code cell (cell numver is 24)---
```python
counts_a = train_data['winner_model_a'].value_counts().reset_index()
counts_b = train_data['winner_model_b'].value_counts().reset_index()
counts_tie = train_data['winner_tie'].value_counts().reset_index()

# Renaming columns for convinience
counts_a.columns = ['Winner', 'Count']
counts_b.columns = ['Winner', 'Count']
counts_tie.columns = ['Winner', 'Count']

# Adding column to identify the model
counts_a['Model'] = 'Model A'
counts_b['Model'] = 'Model B'
counts_tie['Model'] = 'Tie'

counts = pd.concat([counts_a, counts_b, counts_tie])

fig = px.bar(counts, x='Model', y='Count', 
             color='Model',
             title='Winner Distribution for Train Data',
             labels={'Model': 'Model', 'Count': 'Win Count', 'Winner': 'Winner'})

fig.update_layout(xaxis_title="Model", yaxis_title="Win Count")

fig.show()
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
Conclusions:

* There are 57477 training rows and 3 test rows.
    * Note: Test data will be replaced with the full test set (~25k rows, 70% for private LB) during scoring phase.
* The column `id` has no duplicated values.
* Model identities aren't revealed in the test set.
* Strings in columns prompt, `response_a`, and `response_a` are wrapped in a list. 
    * The reason is that each chat can contains more than one prompt/response pairs.
* After dropping `id` column, there exist 14 duplicated rows forming 7 groups, we just keep one row per group and shape of the training DataFrame becomes (57470, 8).
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Data preparation and Feature Engineering

* Cleaning data: clean text, such as removing special characters, normalizing to lowercase, removing stopwords and tokenizing.
* Tokenize Inputs: using the TensorFlow/Kerar tokenizer by training on training data and fitting on both training and test data.
* Padding sequences to `max_len`.
* Create BERT embeddings.
* Compute Similarity Features using BERT between the prompt and responses for each model. 
* Compute word count, character count, and lexical diversity for each response.
* Tokenize the text inputs for the BERT model.
```

---The following area is a Code cell (cell numver is 27)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv').sample(frac=0.001)
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
### Cleaning data

Clean text, such as removing special characters, normalizing to lowercase and removing stopwords.
```

---The following area is a Code cell (cell numver is 29)---
```python
def clean_text(text):
    text = text.lower()
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'https?://\S+|www\.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)
    text = re.sub(r'\n', '', text)
    text = re.sub(r'\w*\d\w*', '', text)
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text
```

---The following area is a Code cell (cell numver is 30)---
```python
# Cleaning texts
train_data['prompt_clean'] = train_data['prompt'].apply(clean_text)
train_data['response_a_clean'] = train_data['response_a'].apply(clean_text)
train_data['response_b_clean'] = train_data['response_b'].apply(clean_text)
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
### Tokenize Inputs

Using the TensorFlow/Kerar tokenizer on both training and test data. Padding sequences to `max_len`.
```

---The following area is a Code cell (cell numver is 32)---
```python
max_len = 512
```

---The following area is a Code cell (cell numver is 33)---
```python
tokenizer = Tokenizer(num_words=20000)
```

---The following area is a Code cell (cell numver is 34)---
```python
tokenizer.fit_on_texts(pd.concat([train_data['prompt_clean'], train_data['response_a_clean'], train_data['response_b_clean']]))
```

---The following area is a Code cell (cell numver is 35)---
```python
train_sequences = tokenizer.texts_to_sequences(train_data['prompt_clean'])
response_a_sequences = tokenizer.texts_to_sequences(train_data['response_a_clean'])
response_b_sequences = tokenizer.texts_to_sequences(train_data['response_b_clean'])
```

---The following area is a Code cell (cell numver is 36)---
```python
# Padding
train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')
response_a_sequences = pad_sequences(response_a_sequences, maxlen=max_len, padding='post')
response_b_sequences = pad_sequences(response_b_sequences, maxlen=max_len, padding='post')
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
### Sentiment Analysis

Sentiment analysis using `vaderSentiment`. VADER (Valence Aware Dictionary and sEntiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media
```

---The following area is a Code cell (cell numver is 38)---
```python
analyzer = SentimentIntensityAnalyzer()
```

---The following area is a Code cell (cell numver is 39)---
```python
def sentiment_analysis(text):
    return analyzer.polarity_scores(text)['compound']
```

---The following area is a Code cell (cell numver is 40)---
```python
train_data['sentiment_prompt'] = train_data['prompt_clean'].apply(sentiment_analysis)
train_data['sentiment_response_a'] = train_data['response_a_clean'].apply(sentiment_analysis)
train_data['sentiment_response_b'] = train_data['response_b_clean'].apply(sentiment_analysis)
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
### Text Features

Calculate text features such as word count, character count, 
lexical diversity, syllable count, sentence count and 
calculating the Flesch Reading Ease score (quantitative 
measurement of how readable a piece of text is) for each response. 

I am using `textstat` library that analyze text statistics.
```

---The following area is a Code cell (cell numver is 42)---
```python
def word_count(text):
    return len(text.split())

def char_count(text):
    return len(text)

def lexical_diversity(text):
    words = text.split()
    return len(set(words)) / len(words) if words else 0

def syllable_count(text):
    return textstat.syllable_count(text)

def sentence_count(text):
    return textstat.sentence_count(text)

def flesch_reading_ease(text):
    return textstat.flesch_reading_ease(text)
```

---The following area is a Code cell (cell numver is 43)---
```python
train_data['word_count_prompt'] = train_data['prompt_clean'].apply(word_count)
train_data['word_count_response_a'] = train_data['response_a_clean'].apply(word_count)
train_data['word_count_response_b'] = train_data['response_b_clean'].apply(word_count)
train_data['char_count_prompt'] = train_data['prompt_clean'].apply(char_count)
train_data['char_count_response_a'] = train_data['response_a_clean'].apply(char_count)
train_data['char_count_response_b'] = train_data['response_b_clean'].apply(char_count)
train_data['lexical_diversity_prompt'] = train_data['prompt_clean'].apply(lexical_diversity)
train_data['lexical_diversity_response_a'] = train_data['response_a_clean'].apply(lexical_diversity)
train_data['lexical_diversity_response_b'] = train_data['response_b_clean'].apply(lexical_diversity)
train_data['syllable_count_prompt'] = train_data['prompt_clean'].apply(syllable_count)
train_data['syllable_count_response_a'] = train_data['response_a_clean'].apply(syllable_count)
train_data['syllable_count_response_b'] = train_data['response_b_clean'].apply(syllable_count)
train_data['sentence_count_prompt'] = train_data['prompt_clean'].apply(sentence_count)
train_data['sentence_count_response_a'] = train_data['response_a_clean'].apply(sentence_count)
train_data['sentence_count_response_b'] = train_data['response_b_clean'].apply(sentence_count)
train_data['flesch_reading_ease_prompt'] = train_data['prompt_clean'].apply(flesch_reading_ease)
train_data['flesch_reading_ease_response_a'] = train_data['response_a_clean'].apply(flesch_reading_ease)
train_data['flesch_reading_ease_response_b'] = train_data['response_b_clean'].apply(flesch_reading_ease)
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
### Create BERT embeddings

Compute BERT embeddings for prompt and response for both train and test data.
Also compute Cosine Similarity features using BERT between the prompt and responses for each model. 

I will use `tf.data.Dataset` to create an efficient pipeline, and process the features in batches using GPU. Also I am going to save the intermediate embeddings using `joblib` library
```

---The following area is a Code cell (cell numver is 45)---
```python
# Load BERT
bert_model_name = 'bert-base-uncased'
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)
bert_model = TFBertModel.from_pretrained(bert_model_name)
```

---The following area is a Code cell (cell numver is 46)---
```python
@tf.function
def get_bert_embeddings(texts):
    inputs = bert_tokenizer(texts, 
                       return_tensors='tf', 
                       padding=True, 
                       truncation=True, 
                       max_length=512)
    outputs = bert_model(inputs)
    return outputs.last_hidden_state[:, 0, :]
```

---The following area is a Code cell (cell numver is 47)---
```python
def process_column(column_data):
    column_data = column_data.dropna().tolist()
    column_data = [str(text) for text in column_data]  
    dataset = tf.data.Dataset.from_tensor_slices(column_data)
    dataset = dataset.batch(8)  
    
    embeddings = []
    for batch in dataset:
        batch_list = [str(text) for text in batch.numpy().tolist()]  
        batch_embeddings = get_bert_embeddings(batch_list)
        embeddings.append(batch_embeddings)
    
    return np.concatenate(embeddings, axis=0)
```

---The following area is a Code cell (cell numver is 48)---
```python
def add_embeddings_to_dataframe(df, column_names):
    for column in column_names:
        print(f"Processing column: {column}")
        embeddings = process_column(df[column])
        df[f'{column}_embedding'] = list(embeddings)
    return df
```

---The following area is a Code cell (cell numver is 49)---
```python
columns_to_embed = ['prompt_clean', 'response_a_clean', 'response_b_clean']
```

---The following area is a Code cell (cell numver is 50)---
```python
train_data = add_embeddings_to_dataframe(train_data, columns_to_embed)
```

---The following area is a Code cell (cell numver is 51)---
```python
train_data['similarity_prompt_response_a'] = train_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)

train_data['similarity_prompt_response_b'] = train_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
### Prepare Data
```

---The following area is a Code cell (cell numver is 53)---
```python
X = train_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',
                'char_count_prompt', 'char_count_response_a', 'char_count_response_b',
                'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',
                'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',
                'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',
                'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',
                'similarity_prompt_response_a', 'similarity_prompt_response_b', 
                'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]

```

---The following area is a Code cell (cell numver is 54)---
```python
# Definindo a coluna alvo
train_data['winner'] = train_data.apply(lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2), axis=1)
```

---The following area is a Code cell (cell numver is 55)---
```python
y = train_data['winner']
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
### Splitting training and validation data
```

---The following area is a Code cell (cell numver is 57)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, 
                                                  random_state=42)
```

---The following area is a Markdown cell (cell numver is 58)---
```markdown
## Defining Models

* Random Forest
* Logistic Regression
* Support Vector Machin
* Gradient Boosting
* Neural Network
```

---The following area is a Code cell (cell numver is 59)---
```python
models = {
    'Random Forest': RandomForestClassifier(),
    'SVM': SVC(probability=True),
    'Gradient Boosting': GradientBoostingClassifier()
}
```

---The following area is a Code cell (cell numver is 60)---
```python
# Creating Neural Network
def create_nn_model(input_shape):
    model = Sequential()
    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(64, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(3, activation='softmax'))
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model
```

---The following area is a Code cell (cell numver is 61)---
```python
nn_model = create_nn_model(X_train.shape[1])
models['Neural Network'] = nn_model
```

---The following area is a Markdown cell (cell numver is 62)---
```markdown
Training models and evaluating:
```

---The following area is a Code cell (cell numver is 63)---
```python
results = {}

for name, model in models.items():
    print(f"Treinando e avaliando {name}...")
    
    if name == 'Neural Network':
        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=2)
        y_pred = np.argmax(model.predict(X_val), axis=1)
        y_pred_proba = model.predict(X_val)
    else:
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        y_pred_proba = model.predict_proba(X_val)

    accuracy = accuracy_score(y_val, y_pred)
    logloss = log_loss(y_val, y_pred_proba)

    results[name] = {
        'AcurÃ¡cia': accuracy,
        'Log Loss': logloss,
        'RelatÃ³rio de ClassificaÃ§Ã£o': classification_report(y_val, y_pred),
        'Matriz de ConfusÃ£o': confusion_matrix(y_val, y_pred)
    }

    print(f"AcurÃ¡cia de {name}: {accuracy}")
    print(f"Log Loss de {name}: {logloss}")
    print(f"RelatÃ³rio de ClassificaÃ§Ã£o de {name}:\n{classification_report(y_val, y_pred)}")
    print(f"Matriz de ConfusÃ£o de {name}:\n{confusion_matrix(y_val, y_pred)}\n")
```

---The following area is a Markdown cell (cell numver is 64)---
```markdown
Selecting the best model with respect the Log Loss
```

---The following area is a Code cell (cell numver is 65)---
```python
best_model_name = max(results, key=lambda name: results[name]['Log Loss'])
best_model = models[best_model_name]
```

---The following area is a Markdown cell (cell numver is 66)---
```markdown
## Prediction and Submission

Preparing test data, predicting on test data and creating submission data.
```

---The following area is a Code cell (cell numver is 67)---
```python
# Cleaning text from test data
test_data['prompt_clean'] = test_data['prompt'].apply(clean_text)
test_data['response_a_clean'] = test_data['response_a'].apply(clean_text)
test_data['response_b_clean'] = test_data['response_b'].apply(clean_text)
```

---The following area is a Code cell (cell numver is 68)---
```python
# Tokenize text from test data
test_sequences = tokenizer.texts_to_sequences(test_data['prompt_clean'])
response_a_test_sequences = tokenizer.texts_to_sequences(test_data['response_a_clean'])
response_b_test_sequences = tokenizer.texts_to_sequences(test_data['response_b_clean'])
```

---The following area is a Code cell (cell numver is 69)---
```python
# Padding sequences from test data
test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')
response_a_test_sequences = pad_sequences(response_a_test_sequences, maxlen=max_len, padding='post')
response_b_test_sequences = pad_sequences(response_b_test_sequences, maxlen=max_len, padding='post')
```

---The following area is a Code cell (cell numver is 70)---
```python
# Sentiment analysis for test data
test_data['sentiment_prompt'] = test_data['prompt_clean'].apply(sentiment_analysis)
test_data['sentiment_response_a'] = test_data['response_a_clean'].apply(sentiment_analysis)
test_data['sentiment_response_b'] = test_data['response_b_clean'].apply(sentiment_analysis)
```

---The following area is a Code cell (cell numver is 71)---
```python
# Creating features of text structure from test data
test_data['word_count_prompt'] = test_data['prompt_clean'].apply(word_count)
test_data['word_count_response_a'] = test_data['response_a_clean'].apply(word_count)
test_data['word_count_response_b'] = test_data['response_b_clean'].apply(word_count)
test_data['char_count_prompt'] = test_data['prompt_clean'].apply(char_count)
test_data['char_count_response_a'] = test_data['response_a_clean'].apply(char_count)
test_data['char_count_response_b'] = test_data['response_b_clean'].apply(char_count)
test_data['lexical_diversity_prompt'] = test_data['prompt_clean'].apply(lexical_diversity)
test_data['lexical_diversity_response_a'] = test_data['response_a_clean'].apply(lexical_diversity)
test_data['lexical_diversity_response_b'] = test_data['response_b_clean'].apply(lexical_diversity)
test_data['syllable_count_prompt'] = test_data['prompt_clean'].apply(syllable_count)
test_data['syllable_count_response_a'] = test_data['response_a_clean'].apply(syllable_count)
test_data['syllable_count_response_b'] = test_data['response_b_clean'].apply(syllable_count)
test_data['sentence_count_prompt'] = test_data['prompt_clean'].apply(sentence_count)
test_data['sentence_count_response_a'] = test_data['response_a_clean'].apply(sentence_count)
test_data['sentence_count_response_b'] = test_data['response_b_clean'].apply(sentence_count)
test_data['flesch_reading_ease_prompt'] = test_data['prompt_clean'].apply(flesch_reading_ease)
test_data['flesch_reading_ease_response_a'] = test_data['response_a_clean'].apply(flesch_reading_ease)
test_data['flesch_reading_ease_response_b'] = test_data['response_b_clean'].apply(flesch_reading_ease)
```

---The following area is a Code cell (cell numver is 72)---
```python
# Embedding from test data
test_data = add_embeddings_to_dataframe(test_data, columns_to_embed)
```

---The following area is a Code cell (cell numver is 73)---
```python
# Cosine similarity from test data
test_data['similarity_prompt_response_a'] = test_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)

test_data['similarity_prompt_response_b'] = test_data.apply(
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)
```

---The following area is a Code cell (cell numver is 74)---
```python
X_test = test_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',
                    'char_count_prompt', 'char_count_response_a', 'char_count_response_b',
                    'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',
                    'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',
                    'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',
                    'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',
                    'similarity_prompt_response_a', 'similarity_prompt_response_b', 
                    'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]
```

---The following area is a Code cell (cell numver is 75)---
```python
test_pred_proba = best_model.predict(X_test)
```

---The following area is a Code cell (cell numver is 76)---
```python
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_pred_proba[:, 0]
submission['winner_model_b'] = test_pred_proba[:, 1]
submission['winner_tie'] = test_pred_proba[:, 2]

submission.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 77)---
```python
submission
```

** @@@ Jupyter Notebook numver 48, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 3)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "/kaggle/input/checkpoint-5200/checkpoint-5200"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 2  # global batch size is 8 
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()

```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 5)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 7)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 10)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 11)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
#ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 14)---
```python
ds
```

---The following area is a Code cell (cell numver is 15)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 16)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 18)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 20)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 21)---
```python
train_idx, eval_idx = folds[config.fold_idx]

trainer = Trainer(
    args=training_args, 
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)
trainer.train(resume_from_checkpoint="/kaggle/input/checkpoint-5200/checkpoint-5200")
```

** @@@ Jupyter Notebook numver 49, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import sys
sys.path.append('/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 2)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import textstat
from textblob import TextBlob
import spacy
import concurrent.futures
import optuna
from sklearn.metrics import log_loss
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
# nltk.download('punkt')
```

---The following area is a Code cell (cell numver is 5)---
```python

def calculate_readability_scores(text):
    # This function calculates various readability scores
    return {
        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),
        "gunning_fog": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "ari": textstat.automated_readability_index(text),
        "coleman_liau_index": textstat.coleman_liau_index(text)
    }

def count_noun_phrases(text):
    blob = TextBlob(text)
    return len(blob.noun_phrases)

def analyze_sentiment(text):
    blob = TextBlob(text)
    return blob.sentiment.polarity

def count_passive_voice(text):
    doc = nlp(text)
    return sum(1 for token in doc if token.dep_ == 'auxpass')

def pos_tag_frequencies(text):
    words = word_tokenize(text)
    tags = nltk.pos_tag(words)
    freq_dist = nltk.FreqDist(tag for (word, tag) in tags)
    # Ensure all frequencies are stored in a consistent dictionary format
    return {tag: freq for tag, freq in freq_dist.items()}

def text_statistics(text):
    stats = calculate_readability_scores(text)
    stats.update({
        "word_count": len(word_tokenize(text)),
        "char_count": len(text),
        "sentence_count": len(sent_tokenize(text)),
        "avg_word_length": sum(len(word) for word in word_tokenize(text)) / len(word_tokenize(text)),
        "avg_sentence_length": sum(len(sent) for sent in sent_tokenize(text)) / len(sent_tokenize(text)),
        "lexical_diversity": len(set(word_tokenize(text))) / len(word_tokenize(text)),
        "noun_phrases_count": count_noun_phrases(text),
        "sentiment": analyze_sentiment(text),
        "passive_voice_count": count_passive_voice(text),
    })
    # Merge POS tag frequencies into the main stats dictionary
    pos_tags = pos_tag_frequencies(text)
    for tag, count in pos_tags.items():
        stats[f'pos_tag_{tag}'] = count
    return stats

def parallel_apply(df, column):
    # Drop NaN values to avoid errors during text processing
    texts = df[column].dropna()

    # Use ProcessPoolExecutor to apply the function in parallel
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = list(executor.map(text_statistics, texts))

    # Convert the list of dictionaries to a DataFrame
    results_df = pd.DataFrame(results)

    # Automatically handles missing POS tags by filling with 0 and converts data types appropriately
    # Filling missing POS tags handled by DataFrame initialization from dict
    results_df.fillna(0, inplace=True)
    for col in results_df.columns:
        if results_df[col].dtype == float:
            results_df[col] = results_df[col].astype(int)

    return results_df

```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
print('Data has been imported')
```

---The following area is a Code cell (cell numver is 7)---
```python
train.shape
```

---The following area is a Code cell (cell numver is 8)---
```python
# def parallel_apply(df, column):
#     with concurrent.futures.ProcessPoolExecutor() as executor:
#         results = list(executor.map(text_statistics, df[column].dropna()))  # Use dropna to handle NaNs gracefully
#     return pd.DataFrame(results)
```

---The following area is a Code cell (cell numver is 9)---
```python
# Applying parallel_apply to 'prompt' and 'response' columns
nlp = spacy.load('en_core_web_sm')
prompt_stats_df = parallel_apply(train, 'prompt')
response_a_stats_df = parallel_apply(train, 'response_a')
response_b_stats_df = parallel_apply(train, 'response_b')
```

---The following area is a Code cell (cell numver is 10)---
```python
train = train.join(prompt_stats_df.add_suffix('_prompt'))
train = train.join(response_a_stats_df.add_suffix('_response_a'))
train = train.join(response_b_stats_df.add_suffix('_response_b'))
```

---The following area is a Code cell (cell numver is 11)---
```python
train.shape
```

---The following area is a Code cell (cell numver is 12)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 13)---
```python
%%time

import time
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import log_loss
from scipy.stats import uniform, randint

# Convert the target into a single column with categorical labels
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# Define features and target
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]

X = train[features]
y = train['winner'] - 1
```

---The following area is a Code cell (cell numver is 14)---
```python
# Define the Optimization Function
def objective(trial):
    # Data splitting inside the trial
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # Define model and hyperparameters to optimize
    model_type = trial.suggest_categorical('model_type', ['XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier'])
    n_estimators = trial.suggest_int('n_estimators', 100, 500)
    max_depth = trial.suggest_int('max_depth', 2, 10)
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 0.1)

    if model_type == 'XGBClassifier':
        model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, use_label_encoder=False, eval_metric='logloss', random_state=42)
    elif model_type == 'LGBMClassifier':
        model = LGBMClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=42)
    elif model_type == 'CatBoostClassifier':
        model = CatBoostClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, verbose=0, random_state=42)

    # Training and evaluating the model
    model.fit(X_train, y_train)
    y_val_pred = model.predict_proba(X_val)
    return log_loss(y_val, y_val_pred)

# Run Optuna Optimization
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=5)

print('Best trial:', study.best_trial.params)

# Train the Best Model on Full Data
best_params = study.best_trial.params
model_type = best_params.pop('model_type')

if model_type == 'XGBClassifier':
    final_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)
elif model_type == 'LGBMClassifier':
    final_model = LGBMClassifier(**best_params, random_state=42)
elif model_type == 'CatBoostClassifier':
    final_model = CatBoostClassifier(**best_params, verbose=0, random_state=42)

final_model.fit(X, y)  # Training on the full dataset
```

---The following area is a Code cell (cell numver is 15)---
```python
final_model
```

---The following area is a Code cell (cell numver is 16)---
```python
# Applying parallel_apply to 'prompt' and 'response' columns
prompt_stats_df_test = parallel_apply(test, 'prompt')
response_a_stats_df_test = parallel_apply(test, 'response_a')
response_b_stats_df_test = parallel_apply(test, 'response_b')
```

---The following area is a Code cell (cell numver is 17)---
```python
test = test.join(prompt_stats_df_test.add_suffix('_prompt'))
test = test.join(response_a_stats_df_test.add_suffix('_response_a'))
test = test.join(response_b_stats_df_test.add_suffix('_response_b'))
```

---The following area is a Code cell (cell numver is 18)---
```python
test = test[features]
test
```

---The following area is a Code cell (cell numver is 19)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 20)---
```python

test_predictions = final_model.predict_proba(test)
```

---The following area is a Code cell (cell numver is 21)---
```python
test_predictions
```

---The following area is a Code cell (cell numver is 22)---
```python
test_raw = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv', usecols=['id'])

```

---The following area is a Code cell (cell numver is 23)---
```python
# Prepare the submission file
submission = pd.DataFrame({
    'id': test_raw['id'],
    'winner_model_a': test_predictions[:, 0],
    'winner_model_b': test_predictions[:, 1],
    'winner_tie': test_predictions[:, 2]
})

submission.head()
```

---The following area is a Code cell (cell numver is 24)---
```python
submission.to_csv('/kaggle/working/submission.csv', index= False)

```

** @@@ Jupyter Notebook numver 50, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Introduction ðŸ“œ
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
âœ”ï¸ What is the objective of this notebook?

The goal is to create a robust and efficient solution to predict users' preference of LLM responses using LightGBM and TF-IDF vectorization.

---

âœ”ï¸ What does this notebook cover?

- `Data Loading & EDA`

- `Theory behind TF-IDF`

- `Data Preprocessing`

- `Model Training`
       
- `Model Inference`
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Imports ðŸ“¦
```

---The following area is a Code cell (cell numver is 3)---
```python
# Handle warning messages
import warnings
warnings.filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Data preprocessing
import numpy as np
import pandas as pd
from pathlib import Path

# Data visualization
import plotly.graph_objects as go
from sklearn.metrics import confusion_matrix

# Model development
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold

# TF-IDF Vectorization
from sklearn.decomposition import TruncatedSVD
from sklearn.feature_extraction.text import TfidfVectorizer

# Similarity/distance features for TF-IDF vectors
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Configuration âš™ï¸
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    # Paths to competition data
    train_data = Path("/kaggle/input/lmsys-chatbot-arena/train.csv")
    test_data = Path("/kaggle/input/lmsys-chatbot-arena/test.csv")
    subm_data = Path("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
    
    # Colorscale for confusion matrix
    colorscale = "peach"
    
    # TF-IDF Vectorization parameters
    components = 32
    ngrams = (1, 7) 
    max_freq = 0.95 # Words that occur in more than 95% of the documents are omitted
    min_freq = 10   # Words that occur in less than 10 documents are omitted
    
    # Training arguments
    num_classes = 3
    early_stop = 50
    log_steps = 100
    
    # LightGBM parameters
    params = {
        "objective": "multiclass",
        "colsample_bytree": 0.8,
        "colsample_bynode": 0.8,
        "metric": "multiclass",
        "learning_rate": 0.02,
        "extra_trees": True,
        "num_rounds": 3000,
        "reg_lambda": 1.3,
        "num_classes": 3,
        "num_leaves": 64,
        "reg_alpha": 0.1,
        "device": "cpu",
        "max_depth": 6,
        "max_bin": 128,
        "verbose": -1,
        "seed": 42
    }
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# Exploratory Data Analysis (EDA) ðŸ—ƒï¸
```

---The following area is a Code cell (cell numver is 8)---
```python
class EDA:
    def read_data(self, path):
        # Read dataframe from path
        df = pd.read_csv(path)
        
        # Display the shape of the dataframe and the first 3 rows
        print(f"The shape of the dataframe is: {df.shape}")
        display(df.head(3))
        
        return df
    
    def pie_chart(self, data):
        # Calculate the counts for each winner column
        counts = {
            'winner_model_a': data['winner_model_a'].sum(),
            'winner_model_b': data['winner_model_b'].sum(),
            'winner_tie': data['winner_tie'].sum()
        }

        # Define the colors
        colors = ['#a89192', '#8083a8', '#a8c28c']  # creme, light blue, mint
        identifiers = ['Creme', 'Light Blue', 'Mint']
        
        # Create the pie chart
        fig = go.Figure(data=[go.Pie(labels=identifiers, 
                                     values=list(counts.values()), 
                                     textinfo='percent', 
                                     hole=0.1,
                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])
        
        # Update layout for a transparent background and move the pie to the left
        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', 
                          paper_bgcolor='rgba(0,0,0,0)', 
                          margin=dict(l=0, r=0, t=0, b=0))
        
        # Hide the legend
        fig.update_layout(showlegend=False)
        
        # Show the plot
        fig.show()

        # Display the counts as a table
        counts_df = pd.DataFrame(list(counts.items()), columns=['Class', 'Count'])
        counts_df['Identifier'] = identifiers
        display(counts_df)
        
    def response_length(self, data):
        # Create a copy of the dataframe to avoid modifying the original data
        data_copy = data.copy()
        
        # Calculate the number of words in each response
        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))
        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))
        
        # Calculate the average word count for each winner class
        word_counts = {
            'winner_model_a': int(
                data_copy[data_copy['winner_model_a'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            ),
            
            'winner_model_b': int(
                data_copy[data_copy['winner_model_b'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            ),
            
            'winner_tie': int(
                data_copy[data_copy['winner_tie'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()
            )
        }
        
        # Create custom hover text
        hover_texts = [f"Word Count: {value}<br>{key}" for key, value in word_counts.items()]
        
        # Create the bar chart
        fig = go.Figure(data=[go.Bar(
            x=list(word_counts.keys()),  # Winner class labels on x-axis
            y=list(word_counts.values()),
            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),
            hovertext=hover_texts,
            hoverinfo='text',
            orientation='v'  # Ensure bars are vertical
        )])
        
        # Update layout
        fig.update_layout(
            title='Average Response Word Count by Winner Class',
            xaxis_title='',
            yaxis_title='Average Response Word Count',
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis=dict(showticklabels=False)  # Hide x-axis labels
        )
        
        # Show the plot
        fig.show()
```

---The following area is a Code cell (cell numver is 9)---
```python
eda = EDA()
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data = eda.read_data(CFG.train_data)
```

---The following area is a Code cell (cell numver is 11)---
```python
test_data = eda.read_data(CFG.test_data)
```

---The following area is a Code cell (cell numver is 12)---
```python
subm_data = eda.read_data(CFG.subm_data)
```

---The following area is a Code cell (cell numver is 13)---
```python
print("Distribution of classes (winners):")
eda.pie_chart(train_data)
```

---The following area is a Code cell (cell numver is 14)---
```python
# Plot average response word count per winner model
eda.response_length(train_data)
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Theory ðŸ“’
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
âœ”ï¸ **Term Frequency - Inverse Document Frequency** or **TF-IDF** vectorization is used in text mining and information retrieval to assess the importance of words in a document relative to a corpus. This technique transforms text data into a numerical format suitable for machine learning algorithms.

---

âœ”ï¸ **Components of TF-IDF**

1. Term Frequency (TF):

   - *Definition:* Measures the frequency of a term in a document.
   
   - *Formula:* $ \text{TF}(t,d) = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t',d}} $ , where $ f_{t,d} $ is the frequency of term $ t $ in document $ d $.

2. Inverse Document Frequency (IDF):

   - *Definition:* Measures the importance of a term across the entire corpus.
   
   - *Formula:* $ \text{IDF}(t) = \log \left( \frac{N}{1 + n_t} \right) $ , where $ N $ is the total number of documents, and $ n_t $ is the number of documents containing term $ t $.

3. TF-IDF Score:

   - *Definition:* Product of TF and IDF scores.
   
   - *Formula:* $ \text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t) $
   
---

âœ”ï¸ ***N-grams* explained**

*N-grams* are contiguous sequences of $ n $ items (tokens) extracted from a text document. They provide a more comprehensive representation of the language structure and context compared to individual words.

*Formula:* $ N\text{-grams} = [t_1, t_2, ..., t_n] $

*Example:* For `ngrams = (1, 3)`, it means we are considering all possible combinations of tokens within a sliding window of length 3 in the text document. Each combination of 3 tokens represents a trigram. 

For instance, consider the sentence: "I love coding."

With `ngrams = (1, 3)`, the n-grams extracted from this sentence would include:

   * Unigrams (1-grams): ["I"], ["love"], ["coding"]
    
   * Bigrams (2-grams): ["I love"], ["love coding"]
    
   * Trigrams (3-grams): ["I love coding"]

This way, $ N-grams $ capture not only individual words but also phrases and contextual information within the text.
  
---
   
âœ”ï¸ **Steps of TF-IDF**

1. Tokenization:

   - *Definition:* Breaks text into tokens.
   
   - *Example:* "I love coding" -> ["I", "love", "coding"]

2. Document Frequency Calculation:

   - *Definition:* Counts the number of documents containing each term.
   
   - *Example:* "love" appears in 1 document out of 1.

3. TF-IDF Calculation:

   - *Definition:* Computes the TF-IDF score for each term in each document.
   
   - *Example:* For ngrams = (1, 3), "love" appears in Document 1, the TF-IDF score for "love" would be calculated based on its TF and IDF.

4. Vectorization:

   - *Definition:* Represents each document as a vector of TF-IDF scores.
   
   - *Example:* Each document becomes a high-dimensional vector where each dimension corresponds to a unique term or n-gram.
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Data Preprocessing ðŸ› ï¸
```

---The following area is a Code cell (cell numver is 18)---
```python
class DataPreprocessing:
    # Check if any value in the input list is None
    @staticmethod
    def retrieve_none(vals):
        return int(any(val is None for val in vals))

    # Calculate the total length of strings in the input list
    @staticmethod
    def retrieve_length(vals):
        length = 0
        for val in vals:
            if isinstance(val, str):
                length += len(val)
        return length
    
    # Calculate the count of unique works in the input list
    @staticmethod
    def retrieve_nuniques(vals):
        if isinstance(vals, str):
            return len(set(vals.split()))
        return 0
    
    # Replace 'None' in the list with the string 'NONE', and join elements with a space
    @staticmethod
    def clean_response(text):
        if isinstance(text, list):
            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])
            return cleaned_text

        return text

    def add_features(self, data):
        # Add features related to the length and presence of None values in response columns.
        data[f"response_a_len"] = data[f"response_a"].apply(self.retrieve_length)
        data[f"response_b_len"] = data[f"response_b"].apply(self.retrieve_length)

        # Calculate unique word count for responses
        data[f"response_a_unique"] = data[f"response_a"].apply(self.retrieve_nuniques)
        data[f"response_b_unique"] = data[f"response_b"].apply(self.retrieve_nuniques)

        # Calculate length difference, mean length, and length difference ratio.
        data["response_len_diff"] = data["response_a_len"] - data["response_b_len"]
        data["response_len_mean"] = (data["response_a_len"] + data["response_b_len"]) / 2
        data["response_diff_ratio"] = data["response_len_diff"] / data["response_len_mean"]

        # Calculate unique word count difference, mean, and ratio.
        data["response_unique_diff"] = data["response_a_unique"] - data["response_b_unique"]
        data["response_unique_mean"] = (data["response_a_unique"] + 
                                        data["response_b_unique"]) / 2
        data["response_unique_ratio"] = (data["response_unique_diff"] / 
                                         data["response_unique_mean"])

        # Check if any value in response columns is None.
        data["a_has_none"] = data["response_a"].apply(self.retrieve_none)
        data["b_has_none"] = data["response_b"].apply(self.retrieve_none)
        data["has_none_diff"] = data["a_has_none"] - data["b_has_none"]

        return data
    
    # Calculate cosine similarity between prompt and responses
    @staticmethod
    def calculate_cosine_similarity(tfidf_matrix, 
                                    prompt_idx, 
                                    response_a_idx, 
                                    response_b_idx):
        
        # Cosine similarity between prompt (p) and response_a (a)
        similarity_pa = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]

        # Cosine similarity between prompt (p) and response_b (b)
        similarity_pb = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]

        return similarity_pa, similarity_pb

    # Calculate distances (Euclidean/Laplacian) between prompt and responses
    @staticmethod
    def calculate_distances(tfidf_matrix, 
                            prompt_idx, 
                            response_a_idx, 
                            response_b_idx, 
                            distance_metric):
        
        # Distance between prompt (p) and response_a (a)
        distance_pa = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]
        
        # Distance between prompt (p) and response_b (b)
        distance_pb = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1),
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]
        
        return distance_pa, distance_pb

    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):
        # Initialize TF-IDF Vectorizer
        tfidf_vectorizer = TfidfVectorizer(analyzer='char', 
                                           ngram_range=ngrams, 
                                           min_df=min_freq, 
                                           max_df=max_freq,
                                           lowercase=False,
                                           sublinear_tf=True)

        # Combine train and test data into a single DataFrame
        full_data = pd.concat([train, test], ignore_index=True)

        # Clean and prepare the text columns
        for col in ['prompt', 'response_a', 'response_b']:
            full_data[col] = full_data[col].apply(self.clean_response)

        # Combine all text columns into a single corpus for TF-IDF vectorization
        full_corpus = pd.concat([full_data['prompt'], 
                                 full_data['response_a'], 
                                 full_data['response_b']], 
                                 ignore_index=True)

        # Compute the TF-IDF matrix
        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)

        # Perform dimensionality reduction with TruncatedSVD
        svd = TruncatedSVD(n_components=components, random_state=42)
        reduced_matrix = svd.fit_transform(tfidf_matrix)

        # Calculate split indices for separating different parts of the corpus
        len_full = len(full_data)
        split_index_01 = len_full
        split_index_02 = len_full * 2

        # Split the reduced matrix into prompts, response_a, and response_b parts
        full_tfidf_prompts = reduced_matrix[:split_index_01]
        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]
        full_tfidf_response_b = reduced_matrix[split_index_02:]

        # Separate the reduced matrix into training and testing sets
        len_train = len(train)
        train_tfidf_prompts = full_tfidf_prompts[:len_train]
        train_tfidf_response_a = full_tfidf_response_a[:len_train]
        train_tfidf_response_b = full_tfidf_response_b[:len_train]
        test_tfidf_prompts = full_tfidf_prompts[len_train:]
        test_tfidf_response_a = full_tfidf_response_a[len_train:]
        test_tfidf_response_b = full_tfidf_response_b[len_train:]

        # Create DataFrames to hold the SVD features for train and test sets
        feature_names = [f'svd_feature_{i}' for i in range(components)]
        train_features = pd.DataFrame(index=train.index)
        test_features = pd.DataFrame(index=test.index)

        # Assign SVD features to the respective columns in the feature DataFrames
        for i in range(components):
            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]
            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]
            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]
            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]
            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]
            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]

        # Concatenate the new features with the original train and test DataFrames
        train = pd.concat([train, train_features], axis=1)
        test = pd.concat([test, test_features], axis=1)

        # Calculate similarity and distance features
        for df, len_df in zip([train, test], [len(train), len(test)]):
            prompt_indices = df.index

            # Calculate cosine similarity features
            df['similarity_pa'], df['similarity_pb'] = zip(*[
                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)
                for i in prompt_indices
            ])

            # Calculate Euclidean distance features
            df['euclidean_pa'], df['euclidean_pb'] = zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         euclidean_distances)
                for i in prompt_indices
            ])

            # Calculate Laplacian kernel distance features
            df['laplacian_pa'], df['laplacian_pb']= zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         laplacian_kernel)
                for i in prompt_indices
            ])

        return train, test
    
    # Merges multiple labels into a single label
    def merge_label(self, row):
        if row["winner_model_a"] == 1:
            return 0
        if row["winner_model_b"] == 1:
            return 1
        if row["winner_tie"] == 1:
            return 2
        raise ValueError("The value is invalid.")
```

---The following area is a Code cell (cell numver is 19)---
```python
dp = DataPreprocessing()
```

---The following area is a Code cell (cell numver is 20)---
```python
# Add length, similarity and distance features
train_data = dp.add_features(train_data)
test_data = dp.add_features(test_data)
```

---The following area is a Code cell (cell numver is 21)---
```python
# Extract TF-IDF features and perform dimensionality reduction
train_data, test_data = dp.create_tfidf_features(train_data, 
                                                 test_data, 
                                                 CFG.ngrams,
                                                 CFG.min_freq, 
                                                 CFG.max_freq, 
                                                 CFG.components)
```

---The following area is a Code cell (cell numver is 22)---
```python
# Merge multiple labels into a single label
train_data["target"] = train_data[
    ["winner_model_a", "winner_model_b", "winner_tie"]
                                 ].apply(lambda x: dp.merge_label(x), axis=1)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Model Development ðŸ§ 
```

---The following area is a Code cell (cell numver is 24)---
```python
class ModelDevelopment:
    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):
        # Extract feature values and target labels from the training and testing data
        X_train = train_data[feature_cols].values
        X_test = test_data[feature_cols].values
        Y_train = train_data["target"]

        # List to store predictions
        train_preds_list = []
        test_preds_list = []

        # Initialize StratifiedKFold
        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):
            # Split the training data into training and validation sets for the current fold
            x_train, x_valid = X_train[train_index], X_train[valid_index]
            y_train, y_valid = Y_train[train_index], Y_train[valid_index]

            # Create LightGBM dataset objects for training and validation
            train = lgb.Dataset(x_train, y_train)
            valid = lgb.Dataset(x_valid, y_valid, reference=train)

            # Train the model on the current fold
            model = lgb.train(
                params,
                train,
                valid_sets=[train, valid],
                feature_name=feature_cols,
                callbacks=[lgb.early_stopping(early_stop),
                           lgb.log_evaluation(log_steps)])

            # Make predictions on the train and test sets
            train_preds = model.predict(X_train)
            test_preds = model.predict(X_test)

            train_preds_list.append(train_preds)
            test_preds_list.append(test_preds)

        # Average predictions
        train_preds = np.mean(train_preds_list, axis=0)
        test_preds = np.mean(test_preds_list, axis=0)

        return train_preds, test_preds
    
    # Confusion matrix for train data predictions
    def plot_cm(self, y_true, y_pred, labels, colorscale):
        cm = confusion_matrix(y_true, y_pred, labels=labels)

        # Create a custom hover text formatter
        def format_hover_text(value):
            if value >= 10000:
                return str(int(value))  # Convert to integer without commas or "k"
            else:
                return str(value)

        # Create the heatmap
        fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=labels,
            y=labels,
            colorscale=colorscale,
            zmin=0,
            zmax=20000,
            text=cm,
            texttemplate="%{text:.0f}",
            hovertemplate="True: %{y}<br>Predicted: %{x}<br>Count: %{z:,.0f}<extra></extra>",
            customdata=[format_hover_text(value) for value in cm.flatten()]
        ))

        # Update layout for a transparent background and square aspect ratio
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis_title="Predicted Labels",
            yaxis_title="True Labels",
            xaxis=dict(constrain='domain'),
            yaxis=dict(constrain='domain', scaleanchor='x'),
            width=650,  
            height=650,  
            margin=dict(t=65, b=65, l=65, r=65) 
        )

        # Show the plot
        fig.show()
```

---The following area is a Code cell (cell numver is 25)---
```python
md = ModelDevelopment()
```

---The following area is a Code cell (cell numver is 26)---
```python
# Define label columns
label_cols = ["winner_model_a", "winner_model_b", "winner_tie"]

# Define the list of features to exclude from the training data
excluded_features = ['id', 
                     'model_a', 
                     'model_b', 
                     'prompt', 
                     'response_a', 
                     'response_b',
                     'winner_model_a', 
                     'winner_model_b', 
                     'winner_tie', 
                     'target', 
                     'fold_id']

features = [col for col in train_data.columns if col not in excluded_features]
```

---The following area is a Code cell (cell numver is 27)---
```python
# Train LightGBM
train_preds, test_preds = md.train_lgb(train_data, 
                                       test_data, 
                                       features,
                                       CFG.params, 
                                       CFG.early_stop, 
                                       CFG.log_steps)
```

---The following area is a Code cell (cell numver is 28)---
```python
# Confusion matrix for (mean) predictions on train data
md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# Submit Predictions ðŸ’¡
```

---The following area is a Code cell (cell numver is 30)---
```python
# Assign the predicted test labels to the submission dataframe
subm_data[label_cols] = test_preds

# Save the submission dataframe and display the first 3 rows
subm_data.to_csv("submission.csv", index=False)
display(subm_data.head(3))
```

** @@@ Jupyter Notebook numver 51, the number of votes :2 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from scipy.sparse import hstack, csr_matrix

# Load data
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# Initialize vectorizers and label encoders
n_features = 2**10
vectorizers = {
    'prompt': HashingVectorizer(n_features=n_features),
    'response_a': HashingVectorizer(n_features=n_features),
    'response_b': HashingVectorizer(n_features=n_features)
}
model_encoder = LabelEncoder()

# Encode model identifiers
train['model_a_encoded'] = model_encoder.fit_transform(train['model_a'])
train['model_b_encoded'] = model_encoder.transform(train['model_b'])

# Process text data into vectors
def process_and_concat_features(data, vectorizers):
    features_list = []
    for column, vectorizer in vectorizers.items():
        print(f"Vectorizing '{column}'...")
        transformed_data = vectorizer.transform(data[column])
        features_list.append(transformed_data)
    final_features = hstack(features_list)  # Keep as sparse matrix
    return final_features

train_features = process_and_concat_features(train, vectorizers)

# Combine model identifiers with text features
model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']])
X_combined = hstack([model_features, train_features])

# Encode target variable
train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(train['winner'])

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)

# Convert the data to DMatrix
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test, label=y_test)

# Set up parameters
params = {
    'objective': 'multi:softprob',
    'num_class': len(np.unique(y_encoded)),
    'eval_metric': 'mlogloss',
    'tree_method':'hist',
    'device':'cuda'
}

# Train the model
num_boost_round = 100
bst = xgb.train(params, dtrain, num_boost_round)

# Predict the probabilities
pred_probs = bst.predict(dtest)
predictions = np.argmax(pred_probs, axis=1)

# Evaluate the model
from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(y_test, predictions)
print(f"Test Accuracy: {test_accuracy:.2f}")

```

---The following area is a Code cell (cell numver is 2)---
```python
# Load data
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test_features = process_and_concat_features(test, vectorizers)

dtest = xgb.DMatrix(test_features)

# Predict the probabilities
pred_probs = bst.predict(dtest)

# Create DataFrame with the prediction probabilities for each class
df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_)
df_submission.insert(0, 'id', test['id'])
df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']

# Save predictions to CSV
df_submission.to_csv('submission.csv', index=False)
print(df_submission.head())

```

** @@@ Jupyter Notebook numver 52, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "tensorflow"  # or "jax" or "torch"
import re

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Num GPUs Available
```

---The following area is a Code cell (cell numver is 3)---
```python
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# TPU
```

---The following area is a Code cell (cell numver is 5)---
```python
# # Detect hardware, return appropriate distribution strategy
# try:
#     tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.
#     print('Running on TPU ', tpu.master())
# except ValueError:
#     tpu = None

# if tpu:
#     tf.config.experimental_connect_to_cluster(tpu)
#     tf.tpu.experimental.initialize_tpu_system(tpu)
#     strategy = tf.distribute.experimental.TPUStrategy(tpu)
# else:
#     strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.

# print("REPLICAS: ", strategy.num_replicas_in_sync)

```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Configuration
```

---The following area is a Code cell (cell numver is 7)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en"
    sequence_length = 512
    epochs = 6
    batch_size = 16
#     batch_size = 16 * strategy.num_replicas_in_sync
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Reproducibility 
è®¾ç½®éšæœºç§å­çš„å€¼ä»¥åœ¨æ¯æ¬¡è¿è¡Œä¸­äº§ç”Ÿç±»ä¼¼çš„ç»“æžœã€‚
```

---The following area is a Code cell (cell numver is 9)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
#  Mixed Precision

åœ¨æœ¬ç¬”è®°ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ··åˆç²¾åº¦è€Œä¸æ˜¯ float32 ç²¾åº¦è¿›è¡Œè®­ç»ƒå’ŒæŽ¨ç†ï¼Œä»¥å‡å°‘ GPU å†…å­˜ä½¿ç”¨é‡ã€‚è¿™æœ€ç»ˆå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿä½¿ç”¨æ›´å¤§çš„æ‰¹é‡å¤§å°ï¼Œä»Žè€Œå‡å°‘æˆ‘ä»¬çš„è®­ç»ƒå’ŒæŽ¨ç†æ—¶é—´ã€‚
```

---The following area is a Code cell (cell numver is 11)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
#åœ¨mixed_float16ç­–ç•¥ä¸‹ï¼Œæ¨¡åž‹çš„æŸäº›éƒ¨åˆ†ä¼šè‡ªåŠ¨ä½¿ç”¨float16è¿›è¡Œè®¡ç®—ï¼Œè€Œå…¶ä»–éƒ¨åˆ†ï¼ˆå¦‚æŸå¤±å‡½æ•°çš„è®¡ç®—ï¼‰åˆ™å¯èƒ½ä»ç„¶ä½¿ç”¨float32ä»¥ä¿æŒç¨³å®šæ€§ã€‚
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Dataset Path
```

---The following area is a Code cell (cell numver is 13)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Meta Data 
## Files

### `train.csv`
- `id`: Unique identifier for each row.
- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).

### `test.csv`
- `id`: Unique identifier for each row.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 
ultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')
df = pd.concat([df, ultrachat_df], axis=0)
lmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')
df = pd.concat([df, lmsys_33k_deduplicated], axis=0)
# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')
# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'["{x}"]')
# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)

# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# display(ultrafeedback_lmsysformat.head())
display(df.head())
```

---The following area is a Code cell (cell numver is 16)---
```python
df = df.drop("id", axis=1)
df = df.drop_duplicates(keep="first", ignore_index=True)

for col in ["prompt"]:
    df[col] = df[col].apply(lambda x: eval(x))
    test_df[col] = test_df[col].apply(lambda x: eval(x))
for col in ["response_a", "response_b"]:
    df[col] = df[col].apply(lambda x: eval(x.replace("null", "None")))
    test_df[col] = test_df[col].apply(lambda x: eval(x.replace("null", "None")))
    
# Sample data
# df = df.sample(frac=0.01)

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
display(df.head())
# Show Sample
display(test_df.head())
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## Contextualize Response with Prompt

åœ¨æˆ‘ä»¬çš„æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°†æ ¹æ®æç¤ºå¯¹æ¯ä¸ªå›žç­”è¿›è¡Œæƒ…å¢ƒåŒ–ï¼Œè€Œä¸æ˜¯å¯¹æ‰€æœ‰å›žç­”ä½¿ç”¨å•ä¸€æç¤ºã€‚è¿™æ„å‘³ç€ï¼Œå¯¹äºŽæ¯ä¸ªå›žç­”ï¼Œæˆ‘ä»¬å°†ä¸ºæ¨¡åž‹æä¾›åŒä¸€ç»„æç¤ºåŠå…¶å„è‡ªçš„å›žç­”ï¼ˆä¾‹å¦‚ï¼Œâ€œ(P + R_A)â€ï¼Œâ€œ(P + R_B)â€ç­‰ï¼‰ã€‚

> æŸäº›æç¤ºå’Œå“åº”å¯èƒ½æœªä½¿ç”¨ `utf-8` ç¼–ç ï¼Œå¯¼è‡´åˆ›å»ºæ•°æ®åŠ è½½å™¨æ—¶å‡ºé”™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ç”¨ç©ºå­—ç¬¦ä¸²æ›¿æ¢å®ƒä»¬ã€‚
```

---The following area is a Code cell (cell numver is 18)---
```python
def make_pairs(row):
    row['options'] = []
    row["encode_fail"] = False

    try:
        # ç¡®ä¿æ‰€æœ‰éœ€è¦çš„é”®éƒ½å­˜åœ¨äºŽrowå­—å…¸ä¸­
        prompts = row['prompt']
        responses_a = row['response_a']
        responses_b = row['response_b']
        
        # æ£€æŸ¥åˆ—è¡¨é•¿åº¦æ˜¯å¦åŒ¹é…
        if not (len(prompts) == len(responses_a) == len(responses_b)):
            raise ValueError("The lists 'prompt', 'response_a', and 'response_b' must be of the same length.")
            
        response_a_str = ''
        response_b_str = ''
        
        for idx in range(len(prompts)):
            response_a_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_a[idx]}"
            response_b_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_b[idx]}"
        
        # æ–‡æœ¬æ¸…æ´—ï¼Œä¾‹å¦‚åŽ»é™¤æ— æ³•è¯†åˆ«çš„Unicodeå­—ç¬¦æˆ–æ›¿æ¢å®ƒä»¬
        clean_response_a_str = "".join(filter(lambda x: ord(x) < 128, response_a_str))
        clean_response_b_str = "".join(filter(lambda x: ord(x) < 128, response_b_str))
        
        row['options'].append(clean_response_a_str)
        row['options'].append(clean_response_b_str)
        
    except KeyError as e:
        print(f"Missing key in row: {e}")
        row["encode_fail"] = True
    except ValueError as e:
        print(e)
        row["encode_fail"] = True
    except Exception as e:
        # æ•èŽ·å…¶ä»–æ‰€æœ‰å¼‚å¸¸
        print(f"An unexpected error occurred: {e}")
        row["encode_fail"] = True

    return row
```

---The following area is a Code cell (cell numver is 19)---
```python
df = df.apply(make_pairs, axis=1)
display(df.head(2))

test_df = test_df.apply(make_pairs, axis=1)
display(test_df.head(2))
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Encoding Fail Statistics
```

---The following area is a Code cell (cell numver is 21)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# EDA
```

---The following area is a Code cell (cell numver is 23)---
```python
class DataFrameStatsProcessor:
    def __init__(self, df):
        self.df = df

    def _is_empty(self, string: str) -> bool:
        return bool(re.match("^\s*$", string))

    def _len(self, string: str) -> int:
        if string is None:
            return 0
        return len(string)

    def _add_len_stats(self, col: str) -> pd.DataFrame:
        if col == "prompt":
            col_prefix = "p_len"
        elif col == "response_a":
            col_prefix = "res_a_len"
        elif col == "response_b":
            col_prefix = "res_b_len"
        
        self.df[f"{col_prefix}_sum"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))
        self.df[f"{col_prefix}_mean"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))
        self.df[f"{col_prefix}_max"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))
        self.df[f"{col_prefix}_sum_log"] = np.log1p(self.df[f"{col_prefix}_sum"])
        self.df[f"{col_prefix}_mean_log"] =  np.log1p(self.df[f"{col_prefix}_mean"])
        self.df[f"{col_prefix}_max_log"] = np.log1p(self.df[f"{col_prefix}_max"])
        
        return self.df
    
    def z_score_normalize(self, columns):
        """
        å¯¹æŒ‡å®šçš„åˆ—è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–ã€‚
        å‚æ•°:
            columns (list): éœ€è¦è¿›è¡ŒZå¾—åˆ†å½’ä¸€åŒ–çš„åˆ—ååˆ—è¡¨ã€‚
        """
        for col in columns:
            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()
    
    def process_dataframe(self):
        self.df["n_prompts"] = self.df["prompt"].apply(lambda x: len(x))
        self.df["n_res_a"] = self.df["response_a"].apply(lambda x: len(x))
        self.df["n_res_b"] = self.df["response_b"].apply(lambda x: len(x))
        assert ((self.df["n_prompts"] == self.df["n_res_a"]) & (self.df["n_prompts"] == self.df["n_res_b"])).all()

        self.df["n_na_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))

        self.df["n_miss_res_a"] = self.df["n_na_res_a"] + self.df["n_empty_res_a"]
        self.df["n_miss_res_b"] = self.df["n_na_res_b"] + self.df["n_empty_res_b"]

        self.df["n_eff_res_a"] = self.df["n_res_a"] - self.df["n_miss_res_a"]
        self.df["n_eff_res_b"] = self.df["n_res_b"] - self.df["n_miss_res_b"]

        self._add_len_stats("prompt")
        self._add_len_stats("response_a")
        self._add_len_stats("response_b")

        self.df["res_len_mean_diff"] = self.df["res_a_len_mean"] - self.df["res_b_len_mean"]
        self.df["res_len_mean_diff_clip"] = self.df["res_len_mean_diff"].clip(-6000, 6000)

        self.df["n_miss_prompts"] = self.df["n_na_prompts"] + self.df["n_empty_prompts"]
        self.df["n_eff_prompts"] = self.df["n_prompts"] - self.df["n_miss_prompts"]

        self.df["na_prompt_ratio"] = self.df["n_na_prompts"] / self.df["n_prompts"]
        self.df["empty_prompt_ratio"] = self.df["n_empty_prompts"] / self.df["n_prompts"]
        self.df["miss_prompt_ratio"] = self.df["n_miss_prompts"] / self.df["n_prompts"]

        self.df["na_res_a_ratio"] = self.df["n_na_res_a"] / self.df["n_res_a"]
        self.df["empty_res_a_ratio"] = self.df["n_empty_res_a"] / self.df["n_res_a"]
        self.df["miss_res_a_ratio"] = self.df["n_miss_res_a"] / self.df["n_res_a"]
        self.df["na_res_b_ratio"] = self.df["n_na_res_b"] / self.df["n_res_b"]
        self.df["empty_res_b_ratio"] = self.df["n_empty_res_b"] / self.df["n_res_b"]
        self.df["miss_res_b_ratio"] = self.df["n_miss_res_b"] / self.df["n_res_b"]

        for col, col_prefix in zip(["prompt", "response_a", "response_b"], ["p_len", "res_a_len", "res_b_len"]):
            self.df[f"{col_prefix}_med"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))
            self.df[f"{col_prefix}_std"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))

        self.df["p_len_eff_mean"] = self.df["p_len_sum"] / self.df["n_eff_prompts"]
        self.df["res_a_len_eff_mean"] = self.df["res_a_len_sum"] / self.df["n_eff_res_a"]
        self.df["res_b_len_eff_mean"] = self.df["res_b_len_sum"] / self.df["n_eff_res_b"]

        for stats in ["sum", "mean", "max", "med", "eff_mean"]:
            self.df[f"p_a_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_a_len_{stats}"]
            self.df[f"p_b_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            self.df[f"a_b_{stats}_diff"] = self.df[f"res_a_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            
        len_feature_a_col = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                     "p_a_eff_mean_diff"]
        
        len_feature_b_col = ["res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                             "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                             "p_b_eff_mean_diff"]
        
        numerical_feature_columns = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                                     "p_a_eff_mean_diff", "res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                                     "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                                     "p_b_eff_mean_diff"]
        # ç¡®ä¿ä¸é™¤ä»¥é›¶è¿›è¡Œå½’ä¸€åŒ–
        for col in numerical_feature_columns:
            if self.df[col].std() == 0:
                print(f"Warning: Standard deviation is zero for column {col}. Skipping normalization.")
            else:
                self.z_score_normalize([col])
                
        self.df = self.df.fillna(0)
        
        # é€‰æ‹©è¿™äº›åˆ—å¹¶å°†å®ƒä»¬è½¬æ¢ä¸ºåˆ—è¡¨
        len_features_a = self.df[len_feature_a_col].values.tolist()
        len_features_b = self.df[len_feature_b_col].values.tolist()

        return len_features_a, len_features_b
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# Data Split

åœ¨ä¸‹é¢æä¾›çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨class_labelåˆ—çš„åˆ†å±‚å°†çŽ°æœ‰æ•°æ®åˆ†ä¸ºè®­ç»ƒå’ŒéªŒè¯ã€‚
```

---The following area is a Code cell (cell numver is 25)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# Preprocessing
```

---The following area is a Code cell (cell numver is 27)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, 
    sequence_length=CFG.sequence_length, 
)
```

---The following area is a Code cell (cell numver is 28)---
```python
def preprocess_fn(text, label=None, features_a=None, features_b=None):
    text = preprocessor(text)
    if features_a is not None:
        text['features_a'] = features_a
    if features_b is not None:
         text['features_b'] = features_b
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# FGM
```

---The following area is a Code cell (cell numver is 30)---
```python
# # æ·»åŠ  FGM æ‰°åŠ¨å‡½æ•°
# def fgm_perturb(features, epsilon=1.0):
#     # è®¡ç®—æ‰°åŠ¨é‡ï¼Œepsilon ä¸ºæ‰°åŠ¨æ¯”ä¾‹
#     perturbation = np.random.uniform(-1, 1, features.shape) * epsilon
#     # åº”ç”¨æ‰°åŠ¨
#     return features + perturbation
```

---The following area is a Code cell (cell numver is 31)---
```python
# # ä¿®æ”¹æ•°æ®é¢„å¤„ç†å‡½æ•°ä»¥åŒ…å« FGM æ‰°åŠ¨
# def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):
#     # é¢„å¤„ç†æ–‡æœ¬
#     text = preprocessor(text)
#     if features_a is not None:
#         if is_fgm:
#             # å¦‚æžœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨
#             features_a = fgm_perturb(features_a, epsilon)
#         text['features_a'] = features_a
#     if features_b is not None:
#         if is_fgm:
#             # å¦‚æžœæ˜¯ FGMï¼Œåº”ç”¨æ‰°åŠ¨
#             features_b = fgm_perturb(features_b, epsilon)
#         text['features_b'] = features_b
#     return (text, label) if label is not None else text
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
# AWP
```

---The following area is a Code cell (cell numver is 33)---
```python
#å®šä¹‰ AWP æ‰°åŠ¨å‡½æ•°
def awp_perturb(model, epsilon=1e-4):
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            # èŽ·å–æƒé‡
            weights = layer.kernel
            # è®¡ç®—æ‰°åŠ¨
            perturbation = tf.random.normal(weights.shape, stddev=epsilon)
            # åº”ç”¨æ‰°åŠ¨
            layer.kernel.assign_add(perturbation)

#åˆ›å»º AWP å›žè°ƒå‡½æ•°
class AWPCallback(keras.callbacks.Callback):
    def __init__(self, epsilon):
        super(AWPCallback, self).__init__()
        self.epsilon = epsilon

    def on_batch_begin(self, batch, logs=None):
        # åœ¨æ¯ä¸ªæ‰¹æ¬¡å¼€å§‹æ—¶åº”ç”¨ AWP æ‰°åŠ¨
        awp_perturb(self.model, self.epsilon)
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# DataLoader

ä¸‹é¢çš„ä»£ç ä½¿ç”¨tf.data.Datasetä¸ºæ•°æ®å¤„ç†è®¾ç½®äº†ä¸€ä¸ªå¥å£®çš„æ•°æ®æµç®¡é“ã€‚
```

---The following area is a Code cell (cell numver is 35)---
```python
def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,
                                cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE
    if (features_a is not None) and (features_b is not None):
        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # Create slices
    else:
        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)
    ds = ds.cache() if cache else ds
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)
#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),
#                 num_parallel_calls=tf.data.AUTOTUNE)
    opt = tf.data.Options()
    if shuffle:
        ds = ds.shuffle(shuffle, seed=CFG.seed)
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)
    ds = ds.batch(batch_size, drop_remainder=False)
    ds = ds.prefetch(AUTO)
    
    return ds
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
## Build Train/Valid Dataloader
```

---The following area is a Code cell (cell numver is 37)---
```python
train_features_processor = DataFrameStatsProcessor(train_df.copy())
train_features_a, train_features_b = train_features_processor.process_dataframe()
valid_features_processor = DataFrameStatsProcessor(valid_df.copy())
valid_features_a, valid_features_b = valid_features_processor.process_dataframe()
```

---The following area is a Code cell (cell numver is 38)---
```python
# # Train
train_texts = train_df.options.tolist()  
train_labels = train_df.class_label.tolist() 
train_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, 
                         batch_size=CFG.batch_size,
                         shuffle=True)
# # Valid
valid_texts = valid_df.options.tolist()  
valid_labels = valid_df.class_label.tolist() 
valid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, 
                         batch_size=CFG.batch_size,
                         shuffle=False)
print(train_ds)
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
# LR Schedule

å®žæ–½å­¦ä¹ çŽ‡è°ƒåº¦ç¨‹åºå¯¹äºŽè¿ç§»å­¦ä¹ è‡³å…³é‡è¦ã€‚

å­¦ä¹ çŽ‡ä»Ž lr_start å¼€å§‹ï¼Œç„¶åŽä½¿ç”¨å„ç§æŠ€æœ¯é€æ¸å‡å°åˆ° lr_minï¼ŒåŒ…æ‹¬ï¼š

- stepï¼šä»¥ç±»ä¼¼æ¥¼æ¢¯çš„æ–¹å¼é€æ­¥é™ä½Žå­¦ä¹ çŽ‡ã€‚
- cosï¼šåˆ©ç”¨ä½™å¼¦æ›²çº¿é€æ¸é™ä½Žå­¦ä¹ çŽ‡ã€‚
- expï¼šä»¥æŒ‡æ•°æ–¹å¼é™ä½Žå­¦ä¹ çŽ‡ã€‚

**é‡è¦æ€§**ï¼šç»“æž„è‰¯å¥½çš„å­¦ä¹ çŽ‡è°ƒåº¦å¯¹äºŽæœ‰æ•ˆçš„æ¨¡åž‹è®­ç»ƒè‡³å…³é‡è¦ï¼Œå¯ç¡®ä¿æœ€ä½³æ”¶æ•›å¹¶é¿å…è¯¸å¦‚è¿‡å†²æˆ–åœæ»žç­‰é—®é¢˜ã€‚
```

---The following area is a Code cell (cell numver is 40)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr
    
    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 41)---
```python
lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs)
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
# Model Checkpointing

ä¸‹é¢çš„ä»£ç å°†åˆ›å»ºä¸€ä¸ªå›žè°ƒï¼Œåœ¨è®­ç»ƒæœŸé—´ä¿å­˜æ¨¡åž‹çš„æœ€ä½³æ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨æäº¤æ—¶ä½¿ç”¨å®ƒè¿›è¡ŒæŽ¨ç†ã€‚
```

---The following area is a Code cell (cell numver is 43)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')  # Get Model checkpoint callback
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# Metric

è¿™æ¬¡æ¯”èµ›çš„æŒ‡æ ‡æ˜¯å¯¹æ•°æŸå¤±ã€‚è¿™ä¸ªåº¦é‡å¯ä»¥ç”¨æ•°å­¦è¡¨ç¤ºä¸ºï¼š

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$
```

---The following area is a Code cell (cell numver is 45)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss", label_smoothing=0.1, from_logits=False)
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
# Modeling
```

---The following area is a Code cell (cell numver is 47)---
```python
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dropout

with strategy.scope():

    # å°†æ‰€æœ‰è¾“å…¥å±‚æ•´åˆåˆ°ä¸€ä¸ªå­—å…¸ä¸­
    inputs = {
        "token_ids": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
        "padding_mask": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
        "features_a": keras.layers.Input(shape=(14,), name="features_a", dtype=tf.float32),
        "features_b": keras.layers.Input(shape=(14,), name="features_b", dtype=tf.float32),
    }
    
    # Create a DebertaV3Classifier backbone
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        CFG.preset,
    )

   # ä¿®æ”¹ response_a å’Œ response_b çš„åˆ›å»ºæ–¹å¼ï¼ŒåŒ…å« padding_mask
    response_a = {
        "token_ids": inputs["token_ids"][:, 0, :],
        "padding_mask": inputs["padding_mask"][:, 0, :]
    }
    embed_a = backbone(response_a)

    response_b = {
        "token_ids": inputs["token_ids"][:, 1, :],
        "padding_mask": inputs["padding_mask"][:, 1, :]
    }
    embed_b = backbone(response_b)
    
    # å°†æ•°å€¼ç‰¹å¾åµŒå…¥
    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_a"])
    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_b"])
    
    # ä½¿ç”¨ Flatten å±‚å°†æ•°å€¼ç‰¹å¾åµŒå…¥å±•å¹³ä¸ºäºŒç»´å¼ é‡
    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)
    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)
    
    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)
    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)
    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])
    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])
    
    # åˆå¹¶æ–‡æœ¬åµŒå…¥å’Œæ•°å€¼ç‰¹å¾åµŒå…¥
    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_a])
    
    # æ·»åŠ L2æ­£åˆ™åŒ–å’ŒDropoutåˆ°æ¨¡åž‹ä¸­
    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2æ­£åˆ™åŒ–
    combined_embeds = Dropout(0.05)(combined_embeds)  # Dropoutå±‚ï¼Œä¸¢å¼ƒ5%çš„ç¥žç»å…ƒ
    
    # å®šä¹‰ temperature_scale å‡½æ•°
    def temperature_scale(logits, T=1.0):
        return logits / T
    
    # å®šä¹‰æ¸©åº¦å‚æ•° T
    T = 0.85
    # åº”ç”¨æ¸©åº¦ç¼©æ”¾
    scaled_logits = temperature_scale(combined_embeds, T)
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(scaled_logits)
    
    model = keras.Model(inputs,  outputs)
    
    # Compile the model with optimizer, loss, and metrics
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),
        metrics=[
            log_loss,
            keras.metrics.CategoricalAccuracy(name="accuracy"),
        ],
    )
    
    # æ·»åŠ  AWP å›žè°ƒåˆ°æ¨¡åž‹è®­ç»ƒä¸­
    awp_cb = AWPCallback(epsilon=1e-4)  # æ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´ epsilon çš„å€¼
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
### Model Summary
```

---The following area is a Code cell (cell numver is 49)---
```python
model.summary()
```

---The following area is a Markdown cell (cell numver is 50)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 51)---
```python
# try:
#     history = model.fit(
#         train_ds,
#         epochs=CFG.epochs,
#         validation_data=valid_ds,
#         callbacks=[lr_cb, ckpt_cb]
#     )
# except tf.errors.InvalidArgumentError as e:
#     print(f"å‡ºçŽ°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}")
try:
    history = model.fit(
        train_ds,
        epochs=CFG.epochs,
        validation_data=valid_ds,
        callbacks=[lr_cb, ckpt_cb, awp_cb]  # å°† AWP å›žè°ƒæ·»åŠ åˆ°è®­ç»ƒå›žè°ƒåˆ—è¡¨ä¸­
    )
except tf.errors.InvalidArgumentError as e:
    print(f"å‡ºçŽ°æ— æ•ˆå‚æ•°é”™è¯¯ï¼š{e}")
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
## Load Best Model
```

---The following area is a Code cell (cell numver is 53)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Markdown cell (cell numver is 54)---
```markdown
# Prediction
```

---The following area is a Code cell (cell numver is 55)---
```python
# # ä½¿ç”¨ FGM æ‰°åŠ¨çš„æ•°æ®é›†è¯„ä¼°æ¨¡åž‹
# fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,
#                                      is_fgm=True, epsilon=1.0)
# evaluation_results = model.evaluate(fgm_ds)

# print(f"Evaluation results on FGM perturbed dataset: {evaluation_results}")
```

---The following area is a Code cell (cell numver is 56)---
```python
test_df_features_processor = DataFrameStatsProcessor(test_df)
test_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()
```

---The following area is a Code cell (cell numver is 57)---
```python
test_texts = test_df.options.tolist()
test_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
print(test_ds)
```

---The following area is a Code cell (cell numver is 58)---
```python
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Markdown cell (cell numver is 59)---
```markdown
# Submission
```

---The following area is a Code cell (cell numver is 60)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

** @@@ Jupyter Notebook numver 53, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Configuration
```

---The following area is a Code cell (cell numver is 1)---
```python
class CFG:
    OFFLINE = True#False #False # requirement for submision competition, during development recommend online model support LLM Evaluation Tools
    USE_LLAMA3 = False # for GPU version
    USE_GEMMA2 = False # for GPU version only 
    TASK_GEN = False # for generative Text output task (seem not suitble for this competition)
    TASK_CLASSIFICATION = True  # for text classiction (suitable for this competition)
#     model1 = "/kaggle/input/llama-3/transformers/8b-hf/1"  # llama3 8B |
    model2 = "/kaggle/input/gemma/transformers/2b-it/3" #  gemma 2B
    model3 = "/kaggle/input/gemma/transformers/7b-it/3" # gemma 7B
#     model4 = "/kaggle/input/gemma-2/pytorch/gemma-2-9b-it/1" # gemma 2 9B
    trainFile = "/kaggle/input/lmsys-chatbot-arena/train.csv"
    testFile = "/kaggle/input/lmsys-chatbot-arena/test.csv"
    submitSample = "/kaggle/input/lmsys-chatbot-arena/sample_submission.csv"
    FEW_SHOT_TEST= False#True
    USE_RAG = False#False#False #True#True , in this project, prefer use fine tuning for p
    USE_WANDB = False#True # for  LLM evalution and debug , track fine tuning performance
    USE_TRULENS = False # for LLM evalution For RAG prefer 
    USE_DEEPEVAL = False # for LLM evalution   (require openAI API key)
    USE_TRAIN =  True #True #False#True Much be use GPU for Training
    USE_INFER =  False # for submision prediction only , no test model
    loggingSteps= 10#100 #100, #20, #5,#10,
    maxTrainData = 1500#3500#5000 #10000#5000 #10000
    maxEvalData = 20#100 # 20 
    maxToken=  650 #512#768#512#768 # 512 for test only
    
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Install Library
```

---The following area is a Code cell (cell numver is 3)---
```python
# installDir = #"/kaggle/input/ai-math-llm-install-package/Universal-LLM-install-page/Universal-LLM-install-page"
installDir = "/kaggle/input/universal-llm-install-package2/Universal-LLM-install-page"
# install Libary for offline
if CFG.OFFLINE:
    !pip install transformers --no-index --no-deps --find-links=file://{installDir}/tranforemers
    !pip install -U datasets     --no-index --no-deps --find-links=file://{installDir}/datasets
    !pip install -U accelerate   --no-index --no-deps --find-links=file://{installDir}/accelerate
    !pip install build        --no-index  --no-deps --find-links=file://{installDir}/build-1.2.1-py3-none-any.whl
    !pip install -U bitsandbytes --no-index --no-deps --find-links=file://{installDir}/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl
    !pip install langchain --no-index --no-deps --find-links=file://{installDir}/langchain-0.2.6-py3-none-any.whl
    !pip install langchain-core  --no-index --no-deps --find-links=file://{installDir}/langchain_core-0.2.10-py3-none-any.whl
    !pip install langsmith   --no-index --no-deps --find-links=file://{installDir}/langsmith-0.1.82-py3-none-any.whl
    !pip install langchain-community --no-index --no-deps --find-links=file://{installDir}/langchain_community-0.2.5-py3-none-any.whl
    !pip install sentence-transformers --no-index --no-deps --find-links=file://{installDir}/sentence_transformers-3.0.1-py3-none-any.whl
    !pip install chromadb --no-index --no-deps --find-links=file://{installDir}/chromadb-0.5.3-py3-none-any.whl
    !pip install faiss-cpu --no-index --no-deps --find-links=file://{installDir}/faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl
    !pip install -U huggingface_hub --no-index --no-deps --find-links=file://{installDir}/huggingface_hub
    !pip install -qU langchain-text-splitters --no-index --no-deps --find-links=file://{installDir}/langchain_text_splitters-0.2.2-py3-none-any.whl
    !pip install -U peft  --no-index --no-deps --find-links=file://{installDir}/peft-0.11.1-py3-none-any.whl
    !pip install -U trl  --no-index --no-deps --find-links=file://{installDir}/trl-0.9.4-py3-none-any.whl 
    !pip install umap-learn  --no-index --no-deps --find-links=file://{installDir}/umap_learn
    !pip install evaluate  --no-index --no-deps --find-links=file://{installDir}/evaluate-0.4.2-py3-none-any.whl 
    !pip install deepeval  --no-index --no-deps --find-links=file://{installDir}/deepeval-0.21.62-py3-none-any.whl
    !pip install weave  --no-index --no-deps --find-links=file://{installDir}/weave-0.50.2-py3-none-any.whl
    !pip install openai --no-index --no-deps --find-links=file://{installDir}/openai-1.35.7-py3-none-any.whl
    !pip install langchain_openai --no-index --no-deps --find-links=file://{installDir}/langchain_openai-0.1.13-py3-none-any.whl
    !pip install trulens --no-index --no-deps --find-links=file://{installDir}/trulens-0.13.4-py3-none-any.whl
    !pip install trulens-eval --no-index --no-deps --find-links=file://{installDir}/trulens_eval-0.32.0-py3-none-any.whl
    
else: # install libary for online
    !pip install git+https://github.com/huggingface/transformers.git  # install transformer from source
    # !pip install --upgrade torch datasets accelerate peft bitsandbytes trl
    # !pip install --upgrade accelerate peft bitsandbytes trl
    !pip install --upgrade datasets accelerate bitsandbytes  # add datasets, accelerate , bitsndbytes
    !pip install langchain  langchain-community sentence-transformers chromadb  faiss-cpu #pypdf
    !pip install --upgrade huggingface_hub
    !pip install -qU langchain-text-splitters
    # for loRA fine tuning 
    !pip install --upgrade peft trl
    # for advance RAG & LLM evalution 
    !pip install portalocker openai langchain_openai # for deepeval dependance library (require openai api key)
    !pip install --upgrade umap-learn evaluate deepeval weave trulens trulens-eval
    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Import Library
```

---The following area is a Code cell (cell numver is 5)---
```python
import os, json, time
import gc
from IPython.display import display, Markdown
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import transformers
import torch
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments
from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding
from langchain_community.document_loaders import TextLoader # new version
from langchain.prompts.prompt import PromptTemplate
from langchain_core.runnables import ConfigurableField
from langchain_community.vectorstores import FAISS, Chroma




# Text chunk spliter
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter , SentenceTransformersTokenTextSplitter# Text Splitter
from langchain.embeddings import HuggingFaceEmbeddings
from datasets import Dataset, DatasetDict, load_dataset

# for evaluate  LLM 
import evaluate
import pytest
import trulens
from sklearn.metrics import (classification_report, ConfusionMatrixDisplay  , 
                             f1_score , accuracy_score, precision_score, recall_score)

import warnings
# warnings.filterwarnings("error") # for handle warning as error
# warnings.filterwarnings("ignore", category=DeprecationWarning) 
```

---The following area is a Code cell (cell numver is 6)---
```python
device =  torch.device('cuda'  if  torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 7)---
```python
def clearMemory():
    for _ in range(5):
        torch.cuda.empty_cache()
        gc.collect()
        time.sleep(0.3)
```

---The following area is a Code cell (cell numver is 8)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 9)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory
if False:
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# W&B inital For Online Only (LLM evalution Tool)
```

---The following area is a Code cell (cell numver is 11)---
```python
if CFG.USE_WANDB:
    import wandb
    from kaggle_secrets import UserSecretsClient
    user_secrets = UserSecretsClient()
    my_secret = user_secrets.get_secret("wandb_api_key")
    wandb.login(key=my_secret) # login
    reportTo= "wandb" # use for fine tuning training logging 
else:
    reportTo = "none"# None
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Load Dataset
```

---The following area is a Code cell (cell numver is 13)---
```python
trainDF =  pd.read_csv(CFG.trainFile)
trainDF
```

---The following area is a Code cell (cell numver is 14)---
```python
trainDF[trainDF["winner_tie"] ==1] # 
```

---The following area is a Code cell (cell numver is 15)---
```python
nullTranDF=trainDF[trainDF.response_a == 'null']
```

---The following area is a Code cell (cell numver is 16)---
```python
len(nullTranDF)
```

---The following area is a Code cell (cell numver is 17)---
```python
trainDF.isnull().sum()
```

---The following area is a Code cell (cell numver is 18)---
```python
testDF =  pd.read_csv(CFG.testFile)
testDF.head()
```

---The following area is a Code cell (cell numver is 19)---
```python
submitDF = pd.read_csv(CFG.submitSample)
submitDF.head()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# EDA
```

---The following area is a Code cell (cell numver is 21)---
```python
def printUniqueValue(df, showAll= True):
    for col in df.columns:
        if showAll ==True:
            print(f"""{col} :  {df[col].unique()}""")
        else:
            if df[col].dtype == "object": # only show object type columns unique values
                print(f"{col} : {df[col].unique()}")
        
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
## print  unqiue Value
```

---The following area is a Code cell (cell numver is 23)---
```python
printUniqueValue(trainDF, showAll=True)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## Model distribution
```

---The following area is a Code cell (cell numver is 25)---
```python
trainDF.columns
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# Load LLM Model
```

---The following area is a Code cell (cell numver is 27)---
```python
if CFG.USE_TRAIN == True:
    # for LoRA fine tuning
    from trl import SFTTrainer
    from peft import LoraConfig, PeftModel, get_peft_model , prepare_model_for_kbit_training #prepare_model_for_int8_training deprecated 

```

---The following area is a Code cell (cell numver is 28)---
```python
do_sample= True 
top_p=0.95 
top_k= 2
temperature=0.2#0.7 
num_beams = 3
max_length= 512

# Quantized Config for GPU support only
bnb_config = BitsAndBytesConfig(
        load_in_4bit = True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype= torch.bfloat16,
        bnb_4bit_use_double_quant=True # Activate nested quantization for 4-bit base models (double quantization)

)

```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# define Label for Text classification
```

---The following area is a Code cell (cell numver is 30)---
```python
id2label = {0: "winner_model_a", 1: "winner_model_b", 2: "winner_tie"}
label2id = {"winner_model_a": 0, "winner_model_b": 1, "winner_tie": 2}
```

---The following area is a Code cell (cell numver is 31)---
```python
if device.type == "cuda": # use 7b/8b/9b model gain performance
    if CFG.USE_LLAMA3:
        modelSel = CFG.model1
        llmModel = "llama3_8b"
        
    elif CFG.USE_GEMMA2:
        modelSel = CFG.model4
        llmModel = "gemma2_9b"
    
    else:
        modelSel = CFG.model3
        llmModel = "gemma_7b"
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config= bnb_config)
#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels =3 ,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type= "multi_label_classification", #problem unmatch issues
                                                 quantization_config= bnb_config) #remove quantization
#          model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  num_labels =3)
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config= bnb_config)
#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
    
    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer
    tokenizer.add_eos_token = True  # We'll add <eos> at the end
    tokenizer.padding_side = "right"
    

else: # for cpu select smaller model
    modelSel = CFG.model2
    llmModel = 'gemma_2b'
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
        
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels =3,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type= "multi_label_classification",
                                                                  )
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")

    tokenizer = AutoTokenizer.from_pretrained(modelSel) # inital tokenizer
    tokenizer.add_eos_token = True  # We'll add <eos> at the end
    tokenizer.padding_side = "right"
    
        
        
```

---The following area is a Code cell (cell numver is 32)---
```python
model
```

---The following area is a Code cell (cell numver is 33)---
```python
llmModel
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# Prompt Engineering
```

---The following area is a Code cell (cell numver is 35)---
```python
if CFG.TASK_GEN:
    templatePrompt1 = """Question: {question}.\nOnly require given final result in JSON format with key 'answer'
            """
    templatePrompt2 = "Answer the user Question.\n###\n{format_instructions}\n###\nQuestion: {query}\n"
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
### Generate Response from LLM
```

---The following area is a Code cell (cell numver is 37)---
```python
if CFG.TASK_GEN:
    def generateResponse(query, maxNewToken =256):
        """
        Direct send message to LLM model, get resposne
        """
    
        inputIds = tokenizer(query, return_tensors="pt").to(device)
        response = model.generate(**inputIds,
                              do_sample= True,
                              top_p =0.95,
                              top_k= 3,
                              temperature= 0.5,
#                               max_lenght = 1024, 
                              max_new_tokens= maxNewToken,
                             )
    
        return tokenizer.decode(response[0][len(inputIds["input_ids"]):], skip_special_tokens = True)
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# Simple parser to extract data
```

---The following area is a Code cell (cell numver is 39)---
```python
import re
from  json.decoder import JSONDecodeError
if CFG.TASK_GEN:

    def isInteger(text):
        try:
            if int(text) >= 0:
                return True
            else:
                return False
        except ValueError:
            return False

    def llmJSONparser(txt, key="answer:", integerOut= False):
        """
        try to get answer from LLM response , expect in JSON format, 
        """
        try:
            subText = txt.split("{") # split several {} in list 
            for txtSeg in subText: # loop in list to find answer
                end = txtSeg.find("}") # find end position in text segment
                sub = txtSeg[:end] #subsring with {} context
                temp = sub.replace("*", "") # remove * symbol
                temp = temp.replace("\"", "") # reomve \" symbol
                temp = temp.lower() # convert to lower case
                answerloc = temp.find(key) # find key word "answer" position
                if answerloc != -1:
                    print(f"find answer location : {answerloc}")
                    newTxt = temp[answerloc:] # substring start answer
#                   print("Temp: ", temp)
                    subTxt = newTxt.split("\n")
                    #       print(subTxt)
                    rel =subTxt[0][len(key):].strip() # get answer value with remove space
                    rel= rel.replace(',', '') # remove , symbol
                    print(rel)
                    if integerOut: # expect integer output 
                        if isInteger(rel):
                            return rel
                        else:
                            continue # not find the value
                    else:
                        return rel
                
            return None # can't find answer
        except :
            print(f"""Error LLM JSON parser input txt {txt}""" )
            return None
        return None


    def getLLMAnswerParser(txt, key="answer:"):
        """
        when json parser failure, seem answer not JSON format, 
        use "answer" for key word search final answer 
        """
         # find answer  
        temp = txt.replace("*", "") # remove * symbol
        temp = temp.replace("\"", "") # reomve "" symbol
        temp = temp.lower() # convert to lower case
        # find answer key word
        start = temp.find(key)
        print(f"Start loc: {start}")
        subStr = temp[start:]
        if start != -1:
            subTxt = subStr.split("\n")
           #print(subTxt)
            rel =subTxt[0][len(key):].strip() # get answer value with remove space
            rel= rel.replace(',', '') # remove , symbol
            print(rel)
            return rel
    
        print(subStr)
        return None
    

```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# Add parser to control extract data from LLM Structure Output
```

---The following area is a Code cell (cell numver is 41)---
```python
from langchain_core.output_parsers import (StrOutputParser, 
                                           JsonOutputParser,
                                           PydanticOutputParser,
                                          )
# for LLM structure output
from langchain_core.pydantic_v1 import BaseModel, Field, validator
# from pydantic import BaseModel, Field
```

---The following area is a Code cell (cell numver is 42)---
```python
if CFG.TASK_GEN:
# define data strauctrue for LLM output structure 
    class Answer(BaseModel):
        answer: str = Field(description="the answer of question response from LLM")
        explanation: str = Field(description="explain how answer come from and reasoning")
    
    
```

---The following area is a Code cell (cell numver is 43)---
```python
# test Parser
if CFG.TASK_GEN:
    jsonParser = JsonOutputParser(pydantic_object=Answer)  # json parser
    pydanticParser =  PydanticOutputParser(pydantic_object=Answer) # pydantic base parse
             
```

---The following area is a Code cell (cell numver is 44)---
```python
if CFG.TASK_GEN:
    print(jsonParser.get_format_instructions())
```

---The following area is a Code cell (cell numver is 45)---
```python
if CFG.TASK_GEN:
    print(pydanticParser.get_format_instructions())
```

---The following area is a Code cell (cell numver is 46)---
```python
%%time
if CFG.TASK_GEN:
    ret = generateResponse("What is Machine Learning?", maxNewToken=256) # test Model 
```

---The following area is a Code cell (cell numver is 47)---
```python
if CFG.TASK_GEN:
    print(ret) # seem LLM default output in Markdown format
```

---The following area is a Code cell (cell numver is 48)---
```python
if CFG.TASK_GEN:
    display(Markdown(ret))
```

---The following area is a Code cell (cell numver is 49)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 50)---
```python
%%time 
if CFG.TASK_GEN:
    # test prompt template with structure format and test parser
    query = "What is Machine Learning?"
    newPrompt =PromptTemplate(input_variables=["question"], template=templatePrompt1)
    finalPrompt = newPrompt.format(
        question= query
    )
    rel = generateResponse(finalPrompt,  maxNewToken=1024)
    jsonTxt = llmJSONparser(rel, key="answer:", integerOut= False)
    print(f"Question : {query}\nResponse Answer: {jsonTxt}") # convert output structure format
```

---The following area is a Code cell (cell numver is 51)---
```python
if CFG.TASK_GEN:
    print(rel)
```

---The following area is a Code cell (cell numver is 52)---
```python
if CFG.TASK_GEN:
    templatePrompt2
```

---The following area is a Code cell (cell numver is 53)---
```python
%%time 
if CFG.TASK_GEN:
    # test Structure output control by PydanticOutputParser
    query = "What is Machine Learning?"
    newPrompt = PromptTemplate(template=templatePrompt2,
                          input_variables=["query"],
                          partial_variables={"format_instructions": pydanticParser.get_format_instructions()},  
                          )
    finalPrompt = newPrompt.format(
            query=query,
        )
    print(f"Final Prompt: {finalPrompt}")
    print("Response:\n")
    rel = generateResponse(finalPrompt,  maxNewToken=2048)
    print(rel)
```

---The following area is a Code cell (cell numver is 54)---
```python
clearMemory()
```

---The following area is a Code cell (cell numver is 55)---
```python
# pydanticParser.parse(rel)
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
# Prepare Dataset for Fine Tuning
```

---The following area is a Code cell (cell numver is 57)---
```python
tempTrainData= (trainDF["prompt"])
```

---The following area is a Code cell (cell numver is 58)---
```python
tempTrainData[0]
```

---The following area is a Code cell (cell numver is 59)---
```python
def dataPreprocess(inputStr):
    # concatenate strings in list
    stripStr = inputStr.strip("[]") # remove list symbol
#     print(stripStr)
    sentence = [s.strip("\"") for s in stripStr.split('","')] # list spliter and remove start 
#     print(sentence)
    finalStr = " ".join(sentence) #concatenate snetence into single string 
    return finalStr
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
# Preprocessing Training / Testing dataset
```

---The following area is a Code cell (cell numver is 61)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 62)---
```python
tempTrainData[0]
```

---The following area is a Code cell (cell numver is 63)---
```python
dataPreprocess(tempTrainData[0])
```

---The following area is a Code cell (cell numver is 64)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 65)---
```python
testDF
```

---The following area is a Code cell (cell numver is 66)---
```python
# Cleaning Training Dataset
trainDF.loc[:, "prompt"] = trainDF["prompt"].apply(dataPreprocess) # convert list of string into single sentence string
trainDF.loc[:, "response_a"] = trainDF["response_a"].apply(dataPreprocess) # convert list of string into single sentence string
trainDF.loc[:, "response_b"] = trainDF["response_b"].apply(dataPreprocess) # convert list of string into single sentence string 

# Clearning Testing Dataset 
testDF.loc[:, "prompt"] = testDF["prompt"].apply(dataPreprocess) # convert list of string into single sentence string
testDF.loc[:, "response_a"] =  testDF["response_a"].apply(dataPreprocess) # convert list of string into single sentence string
testDF.loc[:, "response_b"] = testDF["response_b"].apply(dataPreprocess) # convert list of string into single sentence string 

```

---The following area is a Code cell (cell numver is 67)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 68)---
```python
testDF
```

---The following area is a Markdown cell (cell numver is 69)---
```markdown
## Create Training datase
```

---The following area is a Code cell (cell numver is 70)---
```python
if CFG.USE_TRAIN:
    def getTokenLength(texts):
        ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
        # return length of inputs_ids for each text
        return [len(t) for t in ids]
```

---The following area is a Code cell (cell numver is 71)---
```python
# convert targat from one hot encoding into caterogy (numberic)
targetCol = ["winner_model_a", "winner_model_b", "winner_tie"]
trainDF["label"] = np.argmax(trainDF[targetCol].values, axis =1 )  # 0: for winner model a, 1 for winner model b , 2 for winner tie
```

---The following area is a Code cell (cell numver is 72)---
```python
trainDF["label"].value_counts()
```

---The following area is a Code cell (cell numver is 73)---
```python
trainDF["label"]
```

---The following area is a Code cell (cell numver is 74)---
```python
trainDF["label"].value_counts().plot(kind="bar");
```

---The following area is a Code cell (cell numver is 75)---
```python
# trainDF["labelStr"]= trainDF["label"].apply(str)
```

---The following area is a Code cell (cell numver is 76)---
```python
# trainDF["labelStr"]
```

---The following area is a Code cell (cell numver is 77)---
```python
# create training data set for training feature
# trainDF["text"]=  ("User Prompt: " + trainDF["prompt"] + 
#                      "\n\n---\n\nModel A: " + trainDF["response_a"] +
#                      "\n\n---\n\nModel B: " + trainDF["response_b"] 
#                    )
```

---The following area is a Code cell (cell numver is 78)---
```python
trainDF["text"]=  ("<prompt>: " + trainDF["prompt"] + 
                     "\n\n<response_a>: " + trainDF["response_a"] +
                     "\n\n<response_b>: " + trainDF["response_b"] 
                   )
```

---The following area is a Code cell (cell numver is 79)---
```python
testDF
```

---The following area is a Markdown cell (cell numver is 80)---
```markdown
## Prepare Test Data set for submission
```

---The following area is a Code cell (cell numver is 81)---
```python
# testDF["text"] = ("User Prompt: " + testDF["prompt"] + 
#                      "\n\n---\n\nModel A: " + testDF["response_a"] +
#                      "\n\n---\n\nModel B: " + testDF["response_b"]
#                  )
```

---The following area is a Code cell (cell numver is 82)---
```python
testDF["text"]=  ("<prompt>: " + testDF["prompt"] + 
                     "\n\n<response_a>: " + testDF["response_a"] +
                     "\n\n<response_b>: " + testDF["response_b"] 
                   )
```

---The following area is a Code cell (cell numver is 83)---
```python
print(trainDF["text"][1]) #print the train data sample
```

---The following area is a Code cell (cell numver is 84)---
```python
if False:  # only for statistic count the length of token
    trainDF.loc[:, 'token_count'] = getTokenLength(trainDF['text']) #calucate each 
```

---The following area is a Markdown cell (cell numver is 85)---
```markdown
## Fine Tuning
```

---The following area is a Code cell (cell numver is 86)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 87)---
```python
if False: # only for statistic count the length of token
    print(trainDF['token_count'].describe().to_frame().astype(int)) # check token length in statistic
```

---The following area is a Code cell (cell numver is 88)---
```python
if False:  # only for statistic count the length of token
    # get length of tokens which covers 80% of data, we'll still take 1024 length!
    print(np.percentile(trainDF["token_count"],  q=65))
```

---The following area is a Code cell (cell numver is 89)---
```python
if False:
    trainDF.drop("token_count", axis=1, inplace=True)
```

---The following area is a Code cell (cell numver is 90)---
```python
trainDF
```

---The following area is a Code cell (cell numver is 91)---
```python
print(testDF["text"][0])
```

---The following area is a Code cell (cell numver is 92)---
```python
if CFG.USE_WANDB and CFG.USE_TRAIN:
     # Start a new wandb run
    wandbFineTuningProject = "lmsys-chatbot-araena-fine-tuning"
    runTask1 = wandb.init(project=wandbFineTuningProject, job_type="generation", anonymous="allow")
    # define W&B Table
    wandbCol1 =  ["model", "user query", "modela_ans", "modelb_ans", "label"] #define column for record
    wandbFineTuneTable =wandb.Table(columns=wandbCol1)
```

---The following area is a Code cell (cell numver is 93)---
```python
if CFG.USE_TRAIN == True: #requred GPU support
    # for LoRA fine tuning
    from trl import SFTTrainer
    from peft import LoraConfig, PeftModel, get_peft_model #, prepare_model_for_kbit_training #prepare_model_for_int8_training
```

---The following area is a Markdown cell (cell numver is 94)---
```markdown
# Setup LoRA For Fine tuning
```

---The following area is a Code cell (cell numver is 95)---
```python
if CFG.USE_TRAIN:
    #LoRA config 
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        task_type="SEQ_CLS",  # for Sequence Classification 
        target_modules = ["q_proj", "o_proj", "k_proj", "v_proj",
                      "gate_proj", "up_proj", "down_proj"],
        lora_dropout= 0.05,
    )
    
```

---The following area is a Code cell (cell numver is 96)---
```python
# if False:#CFG.USE_TRAIN:
#     model = prepare_model_for_kbit_training(model) # convert float32 to int8 # take care will out of memory for T4 GPU
#     model= get_peft_model(model, lora_config) #get Model With LORD  # take care int8 train will out of memory for T4 GPU
#     model.print_trainable_parameters()
    
if False:#True:
    model.config.use_cache = False
    model = prepare_model_for_kbit_training(model)
    model= get_peft_model(model, lora_config)
    model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 97)---
```python
model
```

---The following area is a Code cell (cell numver is 98)---
```python
if CFG.USE_TRAIN:
    # Create a preprocessing function to tokenize train data and truncate sequences 
    def tokenizeProcess(sample):
        return tokenizer(sample["text"],  max_length=CFG.maxToken, padding=True, truncation=True)
    
 
```

---The following area is a Markdown cell (cell numver is 99)---
```markdown
# Create Training/Validation Dataset
```

---The following area is a Code cell (cell numver is 100)---
```python
trainDF.describe()
```

---The following area is a Code cell (cell numver is 101)---
```python
if CFG.USE_TRAIN:
#     maxTrainData = 300#5000#10000#5000 #10000
#     maxEvalData = 20#100 
    tempTrainDF = trainDF[:CFG.maxTrainData]
    tempEvalDF =  trainDF[CFG.maxTrainData: CFG.maxTrainData+ CFG.maxEvalData]
    # convert HuggingFace dataset
    trainDataset = Dataset.from_pandas(tempTrainDF, split="train")
    evalDataset = Dataset.from_pandas(tempEvalDF, split="test")
    
```

---The following area is a Code cell (cell numver is 102)---
```python
if CFG.USE_TRAIN:
    print(tempTrainDF["label"].value_counts())
    print(tempEvalDF["label"].value_counts())
```

---The following area is a Markdown cell (cell numver is 103)---
```markdown
### Create Submit Dataset
```

---The following area is a Code cell (cell numver is 104)---
```python
submitDataset = Dataset.from_pandas(testDF, split="test") 
```

---The following area is a Code cell (cell numver is 105)---
```python
submitDataset
```

---The following area is a Code cell (cell numver is 106)---
```python
if CFG.USE_TRAIN:
    del tempTrainDF
    del tempEvalDF
    print(len(trainDataset), len(evalDataset))
```

---The following area is a Code cell (cell numver is 107)---
```python
if CFG.USE_TRAIN:
    # convert dataset to  Datasetdict 
    datasetDict= DatasetDict({
        "train": trainDataset,
        'test': evalDataset
    })
```

---The following area is a Code cell (cell numver is 108)---
```python
submitDataDict = DatasetDict({
    "test": submitDataset
})
```

---The following area is a Code cell (cell numver is 109)---
```python
submitDataDict
```

---The following area is a Code cell (cell numver is 110)---
```python
if CFG.USE_TRAIN:
    print(datasetDict)
```

---The following area is a Code cell (cell numver is 111)---
```python
if CFG.USE_TRAIN:
    # drop unused column 
#     datasetDict =datasetDict.remove_columns(['id', 'model_a', 'model_b', 'response_a', 'response_b'])
    datasetDict = datasetDict.remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
                                 'winner_model_a', 'winner_model_b', 'winner_tie'])
```

---The following area is a Code cell (cell numver is 112)---
```python
# convert Train dataset into tokenize 
if CFG.USE_TRAIN:
    datasetDict = datasetDict.map(tokenizeProcess, batched=True)
```

---The following area is a Code cell (cell numver is 113)---
```python
# datasetDict["train"].remove_columns(['id', 'model_a', 'model_b','winner_model_a', 'winner_model_b', 'winner_tie'])
```

---The following area is a Code cell (cell numver is 114)---
```python
datasetDict
```

---The following area is a Markdown cell (cell numver is 115)---
```markdown
## Convert Test submisioin datasetDict
```

---The following area is a Code cell (cell numver is 116)---
```python
submitDataDict.map(tokenizeProcess, batched=True)
```

---The following area is a Code cell (cell numver is 117)---
```python
submitDataDict["test"][0]
```

---The following area is a Code cell (cell numver is 118)---
```python
datasetDict["train"][0]
```

---The following area is a Code cell (cell numver is 119)---
```python
if True:
    datasetDict = datasetDict.rename_column("label", "labels")
```

---The following area is a Code cell (cell numver is 120)---
```python
datasetDict
```

---The following area is a Code cell (cell numver is 121)---
```python
if CFG.USE_TRAIN:
    accList = []
    f1List = []
    recallList = []
    preciseList = []
    def compute_metrics1(pred):
        logits, labels= pred
        predictions = np.argmax(logits, axis=1) # 
        return {"accuracy": (predictions == labels).mean()}
    
    def compute_metrics2(pred):
        logits, labels= pred
        predictions = np.argmax(logits, axis=-1) # 
        accuracy = (predictions == labels).mean()
        f1score = f1_score(labels, predictions, average='weighted')
        recallScore = recall_score(labels, predictions,  average='weighted')
        precision = precision_score(labels, predictions,  average='weighted')
        accList.append(accuracy)
        f1List.append(f1score)
        recallList.append(recallScore)
        preciseList.append(precision)
        return {"accuracy": accuracy , "recall":  recallScore,  "precision":precision, 'f1-score': f1score }
    
    def compute_metrics3(eval_preds) -> dict:
        preds = eval_preds.predictions
        labels = eval_preds.label_ids
        probs = torch.from_numpy(preds).float().softmax(-1).numpy()
        loss = log_loss(y_true=labels, y_pred=probs)
        acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
        return {"acc": acc, "log_loss": loss}
    
    def formatFuc(sample):
        text = f"{sample['text']}"
        retrun [text]
    
```

---The following area is a Code cell (cell numver is 122)---
```python
datasetDict["test"]
```

---The following area is a Code cell (cell numver is 123)---
```python
if CFG.USE_TRAIN:
    from transformers import DataCollatorWithPadding

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    trainArg = TrainingArguments(
        per_device_train_batch_size=1,
        per_device_eval_batch_size=1,
        gradient_accumulation_steps=4,
        eval_accumulation_steps=1,
        warmup_steps=2,
#         max_steps=300, # , the total number of training steps to perform, overide num_train epochs
        num_train_epochs=1, # set train epochs
        learning_rate=2e-5,
#         evaluation_strategy= 'steps', # older version 
        eval_strategy="steps",
        save_strategy="steps",
        fp16=True,
        logging_steps=CFG.loggingSteps, # 10,#100, #20, #5,#10,
#         save_steps= 100,
        output_dir= '/kaggle/working/lora_model',#"./results",#"outputs",
        optim="paged_adamw_8bit",
        weight_decay=0.01,
        load_best_model_at_end=True,
#         overwrite_output_dir=True,
#         label_names=["label"],
        report_to= reportTo # avoid wnb access token request during training
    )
    
    trainer = SFTTrainer(
        model = model,
        train_dataset=datasetDict["train"],
        eval_dataset= datasetDict["test"],
        args = trainArg,
        max_seq_length=2048,
        peft_config = lora_config,
#         formatting_func = formatFuc, #formatFuc1, #dataset aleader input_ids  
        data_collator=data_collator,
        compute_metrics = compute_metrics2 #compute_metrics1        
    
    )
```

---The following area is a Code cell (cell numver is 124)---
```python
clearMemory()

```

---The following area is a Code cell (cell numver is 125)---
```python
device
```

---The following area is a Code cell (cell numver is 126)---
```python
%%time
if CFG.USE_TRAIN:
    print("Training the Model")
    trainer.train()
#     print("Saving the model!")
    # only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.
#     trainer.model.save_pretrained('fine-tuned-model')
```

---The following area is a Code cell (cell numver is 127)---
```python
# # only saves the incremental ðŸ¤— PEFT weights (adapter_model.bin) that were trained, meaning it is super efficient to store, transfer, and load.
if CFG.USE_TRAIN:
    print("Saving the model!")
    trainer.model.save_pretrained('lora_model')
```

---The following area is a Code cell (cell numver is 128)---
```python
clearMemory()
```

---The following area is a Markdown cell (cell numver is 129)---
```markdown
# Inference Test
```

---The following area is a Code cell (cell numver is 130)---
```python
def getClassifierOutput(text):
        """
        Direct sendtext LLM model, get classification output ,
        
        """
        with torch.no_grad():
            inputIds = tokenizer(text, return_tensors="pt").to(device)
            logits = model(**inputIds).logits
            probabilities = nn.functional.softmax(logits, dim=-1)
            classID = logits.argmax().item()
            return probabilities , classID 
```

---The following area is a Code cell (cell numver is 131)---
```python
print(submitDataDict["test"]["text"][0])
```

---The following area is a Code cell (cell numver is 132)---
```python
# accList
# f1List
# recallList
# preciseList
```

---The following area is a Code cell (cell numver is 133)---
```python
list(range(1, len(accList)+1))
```

---The following area is a Code cell (cell numver is 134)---
```python
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 135)---
```python
plt.plot(list(range(1, len(accList)+1)), accList)
plt.xlabel("Step")
plt.ylabel("Accuary")
plt.title("Accuracy")
plt.show();
```

---The following area is a Code cell (cell numver is 136)---
```python
plt.plot(list(range(1, len(f1List)+1)), f1List)
plt.xlabel("Step")
plt.ylabel("F1-Score")
plt.title("F1 Score")
plt.show();
```

---The following area is a Code cell (cell numver is 137)---
```python
plt.plot(list(range(1, len(recallList)+1)), recallList)
plt.xlabel("Step")
plt.ylabel("Recall")
plt.title("Recall")
plt.show();
```

---The following area is a Code cell (cell numver is 138)---
```python
plt.plot(list(range(1, len(preciseList)+1)), preciseList)
plt.xlabel("Step")
plt.ylabel("Precision")
plt.title("Precision")
plt.show();
```

---The following area is a Code cell (cell numver is 139)---
```python
submitDF
```

---The following area is a Code cell (cell numver is 140)---
```python
finalDF = submitDF.copy()
finalDF
```

---The following area is a Code cell (cell numver is 141)---
```python
from torch import nn
```

---The following area is a Code cell (cell numver is 142)---
```python
wina , winb , tie =[], [], [] 
for i, text in enumerate(submitDataDict["test"]["text"]):
    probs , classID = getClassifierOutput(text)
#     print("{}: ", probs.tolist()[0])
    rounded_probs = [round(val, 6) for val in probs.tolist()[0]]
    print(rounded_probs)
    print("Classified ID : ", classID)
    wina.append(rounded_probs[0])
    winb.append(rounded_probs[1])
    tie.append(rounded_probs[2])
    #calculate probability
#     print("Probabilities: ", [round(val, 6) for val in probabilities.tolist()[0]])
    #write to submit DF
#     finalDF.iloc[i, 1:] = [round(val, 6) for val in probabilities.tolist()[0]] #logits.tolist()[0] #update column 
finalDF["winner_model_a"] = wina
finalDF["winner_model_b"] = winb
finalDF["winner_tie"] = tie
```

---The following area is a Code cell (cell numver is 143)---
```python
finalDF.head()
```

---The following area is a Code cell (cell numver is 144)---
```python
finalDF.to_csv('submission.csv' , index=False)
```

---The following area is a Code cell (cell numver is 145)---
```python
# clearMemory()
```

** @@@ Jupyter Notebook numver 54, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 55, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keywords torch RoBERTa for Submission
with internet-off condition


- https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert <br/>
train data processing

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta <br/>
model training using processed train data

- https://www.kaggle.com/code/stpeteishii/download-keybert <br/>
download keybert

- https://www.kaggle.com/code/stpeteishii/save-distilbert-base-nli-mean-tokens <br/>
download distilbert-base-nli-mean-tokens

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta-for-submission <br/>
test data processing, inferance (this notebook)
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install keybert --no-index --find-links=file:///kaggle/input/download-keybert
```

---The following area is a Code cell (cell numver is 2)---
```python
from keybert import KeyBERT
```

---The following area is a Code cell (cell numver is 3)---
```python
import numpy as np 
import pandas as pd 
import os
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import matplotlib.pyplot as plt 
import transformers
import random
import warnings
warnings.simplefilter('ignore')
scaler = torch.cuda.amp.GradScaler() 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 4)---
```python
def random_seed(SEED):
    
    random.seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    
SEED = 508
random_seed(SEED)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Process Test Data
```

---The following area is a Code cell (cell numver is 6)---
```python
from sentence_transformers import SentenceTransformer

local_model = SentenceTransformer('/kaggle/input/save-distilbert-base-nli-mean-tokens')
modelky = KeyBERT(model=local_model)
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')#, encoding='iso-8859-1')
test['prompt_kw']='-'
test['res_a_kw']='-'
test['res_b_kw']='-'

tkw0 = modelky.extract_keywords(test['prompt'],top_n=5)
tkw1 = modelky.extract_keywords(test['response_a'],top_n=10)
tkw2 = modelky.extract_keywords(test['response_b'],top_n=10)
```

---The following area is a Code cell (cell numver is 8)---
```python
for i,w in enumerate(tkw0): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'prompt_kw']=' '.join(ws)
    
for i,w in enumerate(tkw1): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_a_kw']=' '.join(ws)  
    
for i,w in enumerate(tkw2): 
    ws=[]
    for wi in w:
        if '_' not in wi[0]:
            ws+=[wi[0]]
    test.loc[i,'res_b_kw']=' '.join(ws)   

test['res_a_kw']=test['res_a_kw']+' // '+test['prompt_kw']
test['res_b_kw']=test['res_b_kw']+' // '+test['prompt_kw']
test=test.iloc[:,4:]
display(test)

#test.to_csv('test_key.csv',index=False)
```

---The following area is a Code cell (cell numver is 9)---
```python
testA=test[['res_a_kw']]
testA['label']=0
testA.columns=['text','label']
testB=test[['res_b_kw']]
testB['label']=0
testB.columns=['text','label']
TEST=pd.concat([testA,testB],axis=0)
```

---The following area is a Code cell (cell numver is 10)---
```python
max_sens = 8
p_test=TEST.reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 11)---
```python
class BERTDataSet(Dataset):
    
    def __init__(self,sentences,targets):        
        self.sentences = sentences
        self.targets = targets
        
    def __len__(self):        
        return len(self.sentences)
    
    def __getitem__(self,idx):        
        sentence = self.sentences[idx]    
        bert_sens = tokenizer.encode_plus(
                                sentence,
                                add_special_tokens = True, 
                                max_length = max_sens, 
                                pad_to_max_length = True, 
                                return_attention_mask = True)

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)

        target = torch.tensor(self.targets[idx],dtype=torch.float)
        
        return {
                'ids': ids,
                'mask': mask,

                'targets': target
            }
```

---The following area is a Code cell (cell numver is 12)---
```python
test_dataset = BERTDataSet(p_test["text"],p_test["label"])
test_batch = 32
test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=8,pin_memory=True)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# def predicting
use saved models
```

---The following area is a Code cell (cell numver is 14)---
```python
#model initialized
tokenizer = transformers.RobertaTokenizer.from_pretrained("/kaggle/input/roberta-base")
model = transformers.RobertaForSequenceClassification.from_pretrained("/kaggle/input/roberta-base",num_labels=1)
pths = [os.path.join("/kaggle/input/lmsys-keywords-torch-roberta",s) for s in os.listdir("/kaggle/input/lmsys-keywords-torch-roberta") if ".pth" in s]
print(pths)
```

---The following area is a Code cell (cell numver is 15)---
```python
def predicting(
    test_dataloader,
    model,
    pths 
):
    allpreds = []    
    for pth in pths:  
        state = torch.load(pth, map_location=torch.device('cpu'))      
        model.load_state_dict(state["state_dict"])
        model.to(device)
        model.eval()      
        preds = []
        allvalloss=0

        with torch.no_grad():
            for a in test_dataloader:
                ids = a["ids"].to(device)
                mask = a["mask"].to(device)
                output = model(ids,mask)
                output = output["logits"].squeeze(-1)
                preds.append(output.cpu().numpy())

            preds = np.concatenate(preds)           
            allpreds.append(preds)

    return allpreds
```

---The following area is a Code cell (cell numver is 16)---
```python
tpreds = predicting(test_dataloader,model,pths)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Prediction Result
```

---The following area is a Code cell (cell numver is 18)---
```python
test_pred = []
for p in tpreds[0]:
    test_pred+=[p]
```

---The following area is a Code cell (cell numver is 19)---
```python
submit=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
submit['winner_model_a']=test_pred[0:len(test)]
submit['winner_model_b']=test_pred[len(test):]
pa=submit['winner_model_a']
pb=submit['winner_model_b']
submit['winner_tie']=np.clip((pa+pb),0,1)
display(submit)
submit.to_csv('submission.csv',index=False)
```

** @@@ Jupyter Notebook numver 56, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keywords torch RoBERTa
under internet on condition

https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install chardet
```

---The following area is a Code cell (cell numver is 2)---
```python
#debug = False
#debug2 = False
```

---The following area is a Code cell (cell numver is 3)---
```python
import numpy as np 
import pandas as pd 
import os
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import matplotlib.pyplot as plt 
import transformers
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import random
import chardet
import warnings
warnings.simplefilter('ignore')
scaler = torch.cuda.amp.GradScaler() 
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
device
```

---The following area is a Code cell (cell numver is 4)---
```python
def random_seed(SEED):
    
    random.seed(SEED)
    os.environ['PYTHONHASHSEED'] = str(SEED)
    np.random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    
SEED = 508
random_seed(SEED)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# label is ranking
```

---The following area is a Code cell (cell numver is 6)---
```python
train0=pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/train_key.csv')
train0=train0[0:20000]
display(train0)
print(train0.columns.tolist())

test0=pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/test_key.csv')
display(test0)
print(test0.columns.tolist())
```

---The following area is a Code cell (cell numver is 7)---
```python
trainA=train0[['res_a_kw','winner_model_a']]
trainA.columns=['text','label']
trainB=train0[['res_b_kw','winner_model_b']]
trainB.columns=['text','label']
data=pd.concat([trainA,trainB],axis=0)

testA=test0[['res_a_kw']]
testA['label']=0
testA.columns=['text','label']
testB=test0[['res_b_kw']]
testB['label']=0
testB.columns=['text','label']
TEST=pd.concat([testA,testB],axis=0)
```

---The following area is a Code cell (cell numver is 8)---
```python
from sklearn.model_selection import train_test_split
train, test = train_test_split(data, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 9)---
```python
#tokenizer = transformers.BertTokenizer.from_pretrained("../input/bert-base-uncased")
tokenizer = transformers.AutoTokenizer.from_pretrained("roberta-base")
```

---The following area is a Code cell (cell numver is 10)---
```python
test_s = train['text'].iloc[0]
result1 = tokenizer.encode_plus(test_s)
tokenizer.decode(result1["input_ids"])
```

---The following area is a Code cell (cell numver is 11)---
```python
len(test_s.split(" "))
```

---The following area is a Code cell (cell numver is 12)---
```python
result2 = tokenizer.encode_plus(
    test_s,
    add_special_tokens = True, 
    max_length = 20, 
    pad_to_max_length = True, 
    truncation = True 
)
```

---The following area is a Code cell (cell numver is 13)---
```python
tokenizer.decode(result2["input_ids"])
```

---The following area is a Code cell (cell numver is 14)---
```python
max_sens = 20

train = train.sort_values("label").reset_index(drop=True)

train["kfold"] = train.index % 5

p_train = train[train["kfold"]!=0].reset_index(drop=True)
p_valid = train[train["kfold"]==0].reset_index(drop=True)

p_test=TEST.reset_index(drop=True)

```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
'token_type_ids' no need in RoBERTa/DeBERTa
```

---The following area is a Code cell (cell numver is 16)---
```python
class BERTDataSet(Dataset):
    
    def __init__(self,sentences,targets):        
        self.sentences = sentences
        self.targets = targets
        
    def __len__(self):        
        return len(self.sentences)
    
    def __getitem__(self,idx):        
        sentence = self.sentences[idx]    
        bert_sens = tokenizer.encode_plus(
                                sentence,
                                add_special_tokens = True, 
                                max_length = max_sens, 
                                pad_to_max_length = True, 
                                return_attention_mask = True)

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)

        target = torch.tensor(self.targets[idx],dtype=torch.float)
        
        return {
                'ids': ids,
                'mask': mask,

                'targets': target
            }
```

---The following area is a Code cell (cell numver is 17)---
```python
train_dataset = BERTDataSet(p_train["text"],p_train["label"])
valid_dataset = BERTDataSet(p_valid["text"],p_valid["label"])
test_dataset = BERTDataSet(p_test["text"],p_test["label"])


train_batch = 16
valid_batch = 32
test_batch = 32

train_dataloader = DataLoader(train_dataset,batch_size=train_batch,shuffle = True,num_workers=8,pin_memory=True)
valid_dataloader = DataLoader(valid_dataset,batch_size=valid_batch,shuffle = False,num_workers=8,pin_memory=True)
test_dataloader = DataLoader(test_dataset,batch_size=test_batch,shuffle = False,num_workers=8,pin_memory=True)
```

---The following area is a Code cell (cell numver is 18)---
```python
model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)
#model = transformers.BertForSequenceClassification.from_pretrained("../input/bert-base-uncased",num_labels=1)
```

---The following area is a Code cell (cell numver is 19)---
```python
model.to(device)
model.train()
```

---The following area is a Code cell (cell numver is 20)---
```python
for a in train_dataloader:
    ids = a["ids"].to(device)
    mask = a["mask"].to(device)
    output = model(ids,mask)
    break
```

---The following area is a Code cell (cell numver is 21)---
```python
output = output["logits"].squeeze(-1).shape
```

---The following area is a Code cell (cell numver is 22)---
```python
from transformers import AdamW
LR=2e-5
optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) 
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# set epochs
```

---The following area is a Code cell (cell numver is 24)---
```python
from transformers import get_linear_schedule_with_warmup
epochs = 30
#if debug:
#    epochs = 1
train_steps = int(len(p_train)/train_batch*epochs)
print(train_steps)
num_steps = int(train_steps*0.1)
scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)
```

---The following area is a Code cell (cell numver is 25)---
```python
def loss_fn(output,target):
    return torch.sqrt(nn.MSELoss()(output,target))
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# def training
```

---The following area is a Code cell (cell numver is 27)---
```python
def training(
    train_dataloader,
    model,
    optimizer,
    scheduler
):
    
    model.train()
    torch.backends.cudnn.benchmark = True
    allpreds = []
    alltargets = []

    for a in train_dataloader:

        losses = []
        optimizer.zero_grad()

        with torch.cuda.amp.autocast():

            ids = a["ids"].to(device,non_blocking=True)
            mask = a["mask"].to(device,non_blocking=True)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            target = a["targets"].to(device,non_blocking=True)
            loss = loss_fn(output,target)

            losses.append(loss.item())
            allpreds.append(output.detach().cpu().numpy())
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())

        scaler.scale(loss).backward() 
        scaler.step(optimizer) 
        scaler.update() 
        
        del loss 

        scheduler.step() 

    allpreds = np.concatenate(allpreds)
    alltargets = np.concatenate(alltargets)
    losses = np.mean(losses)
    train_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))

    return losses,train_rme_loss
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# def validating
```

---The following area is a Code cell (cell numver is 29)---
```python
def validating(valid_dataloader,model):
    
    model.eval()
    allpreds = []
    alltargets = []

    for a in valid_dataloader:
        losses = []
        with torch.no_grad():

            ids = a["ids"].to(device)
            mask = a["mask"].to(device)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            target = a["targets"].to(device)
            loss = loss_fn(output,target)
            losses.append(loss.item())
            allpreds.append(output.detach().cpu().numpy())
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())
            
            del loss

    allpreds = np.concatenate(allpreds)
    alltargets = np.concatenate(alltargets)
    losses = np.mean(losses)
    valid_rme_loss = np.sqrt(mean_squared_error(alltargets,allpreds))

    return allpreds,losses,valid_rme_loss
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
if debug2 == False:
        for a in range(epochs):
            for b in train_dataloader:
                break

        losses,train_rme_loss = training(train_dataloader,model,optimizer,scheduler)

        for a in valid_dataloader:
            break

# Train and Validate
```

---The following area is a Code cell (cell numver is 31)---
```python
trainlosses = []
vallosses = []
bestscore = None
trainscores = []
validscores = []

for epoch in tqdm(range(epochs)):
    
    print("---------------" + str(epoch) + "start-------------")
    
    trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)    
    trainlosses.append(trainloss)
    trainscores.append(trainscore)
    
    print("trainscore is " + str(trainscore))
    
    preds,validloss,valscore=validating(valid_dataloader,model)    
    vallosses.append(validloss)
    validscores.append(valscore)
    
    print("valscore is " + str(valscore))
    
    if bestscore is None:
        bestscore = valscore
        
        print("Save first model")
        
        state = {
                        'state_dict': model.state_dict(),
                        'optimizer_dict': optimizer.state_dict(),
                        "bestscore":bestscore
                    }
            
        torch.save(state, "model0.pth")
        
    elif bestscore > valscore:
        
        bestscore = valscore        
        print("found better point")        
        state = {
                        'state_dict': model.state_dict(),
                        'optimizer_dict': optimizer.state_dict(),
                        "bestscore":bestscore
                    }
            
        torch.save(state, "model0.pth")
        
    else:
        pass
    
```

---The following area is a Code cell (cell numver is 32)---
```python
plt.scatter(p_valid['label'],preds, alpha=0.2)
plt.title('Validation Prediction Result')
plt.xlabel('Actual')
plt.ylabel('Prediction')
plt.show()

x = np.arange(epochs)
plt.title('Validation Losses')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(x,trainlosses)
plt.plot(x,vallosses)
plt.show()

x = np.arange(epochs)
plt.title('Validation Scores')
plt.xlabel('Epoch')
plt.ylabel('Score')
plt.plot(x,trainscores)
plt.plot(x,validscores)
plt.show()
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# save model
```

---The following area is a Code cell (cell numver is 34)---
```python
bestscores = []
bestscores.append(bestscore)

for fold in range(1,5):
    
    # initializing the data
    p_train = train[train["kfold"]!=fold].reset_index(drop=True)
    p_valid = train[train["kfold"]==fold].reset_index(drop=True)

    train_dataset = BERTDataSet(p_train["text"],p_train["label"])
    valid_dataset = BERTDataSet(p_valid["text"],p_valid["label"])
    
    model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)
    
    model.to(device)
    LR=2e-5
    optimizer = AdamW(model.parameters(), LR,betas=(0.9, 0.999), weight_decay=1e-2) # AdamW optimizer
    train_steps = int(len(p_train)/train_batch*epochs)
    num_steps = int(train_steps*0.1)
    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)

    trainlosses = []
    vallosses = []
    bestscore = None
    trainscores = []
    validscores = []

    for epoch in tqdm(range(epochs)):

        print("---------------" + str(epoch) + "start-------------")

        trainloss,trainscore = training(train_dataloader,model,optimizer,scheduler)
        trainlosses.append(trainloss)
        trainscores.append(trainscore)

        print("trainscore is " + str(trainscore))

        preds,validloss,valscore=validating(valid_dataloader,model)
        vallosses.append(validloss)
        validscores.append(valscore)

        print("valscore is " + str(valscore))

        if bestscore is None:
            bestscore = valscore

            print("Save first model")

            state = {
                            'state_dict': model.state_dict(),
                            'optimizer_dict': optimizer.state_dict(),
                            "bestscore":bestscore
                        }

            torch.save(state, "model" + str(fold) + ".pth") 

        elif bestscore > valscore:
            bestscore = valscore
            print("found better point")

            state = {
                            'state_dict': model.state_dict(),
                            'optimizer_dict': optimizer.state_dict(),
                            "bestscore":bestscore
                        }
            torch.save(state, "model"+ str(fold) + ".pth")

        else:
            pass


    bestscores.append(bestscore)
```

---The following area is a Code cell (cell numver is 35)---
```python
bestscores
```

---The following area is a Code cell (cell numver is 36)---
```python
np.mean(bestscores)
print("My CV is " + str(np.mean(bestscores))+ ".")
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# def predicting
not use saved models
```

---The following area is a Code cell (cell numver is 38)---
```python
def predicting(test_dataloader,model):
    
    model.to(device)
    model.eval()   
    allpreds = []
    preds = []
    allvalloss=0

    with torch.no_grad():
        for a in test_dataloader:

            ids = a["ids"].to(device)
            mask = a["mask"].to(device)

            output = model(ids,mask)
            output = output["logits"].squeeze(-1)
            preds.append(output.cpu().numpy())

        preds = np.concatenate(preds)
        allpreds.append(preds)

    return allpreds
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
# Predict
```

---The following area is a Code cell (cell numver is 40)---
```python
tpreds = predicting(test_dataloader,model)
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# Prediction Result
```

---The following area is a Code cell (cell numver is 42)---
```python
test_pred = []
for p in tpreds[0]:
    test_pred+=[p]
```

---The following area is a Code cell (cell numver is 43)---
```python
submit=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
pa=test_pred[0:len(test0)]
pb=test_pred[len(test0):]
pc=[]
for i in range(len(test0)):
    pc+=[np.clip(1-(pa[i]+pb[i]),0,1)]
submit['winner_model_a']=pa
submit['winner_model_b']=pb
submit['winner_tie']=pc
display(submit)
submit.to_csv('submission.csv',index=False)
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
[Caution] The 'submission.csv' is not available for submission, because this noteboook runs under internet-on condition.
```

** @@@ Jupyter Notebook numver 57, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Disable GPU

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model

# Load the dataset
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Combine prompts and responses for feature extraction
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']

# Vectorize the text data using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_a = tfidf.fit_transform(train_data['text_a']).toarray()
X_b = tfidf.transform(train_data['text_b']).toarray()

# Combine the vectorized responses
X = np.hstack([X_a, X_b])

# Target variable
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Define the model using Functional API
input_layer = Input(shape=(X_train.shape[1],))
x = Dense(512, activation='relu')(input_layer)
x = Dropout(0.5)(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.5)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.5)(x)
output_layer = Dense(3, activation='softmax')(x)

model = Model(inputs=input_layer, outputs=output_layer)
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
val_predictions = model.predict(X_val)
loss = log_loss(y_val, val_predictions)
print(f'Validation Log Loss: {loss}')

# Prepare the test data
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b']
X_test_a = tfidf.transform(test_data['text_a']).toarray()
X_test_b = tfidf.transform(test_data['text_b']).toarray()
X_test = np.hstack([X_test_a, X_test_b])
X_test = scaler.transform(X_test)

# Predict on the test set
test_predictions = model.predict(X_test)

# Create the submission file
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_predictions[:, 0]
submission['winner_model_b'] = test_predictions[:, 1]
submission['winner_tie'] = test_predictions[:, 2]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 58, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install peft bitsandbytes
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from datasets import Dataset, load_metric, load_dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback,
    AutoConfig
)

from pathlib import Path
from sklearn.metrics import log_loss
import numpy as np
import json

from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType, LoftQConfig
import bitsandbytes as bnb

from tqdm.auto import tqdm
import random
```

---The following area is a Code cell (cell numver is 3)---
```python
from kaggle_secrets import UserSecretsClient
import wandb
user_secrets = UserSecretsClient()
my_secret = user_secrets.get_secret("wandb_api_key") 
wandb.login(key=my_secret)
```

---The following area is a Code cell (cell numver is 4)---
```python
%env WANDB_LOG_MODEL="checkpoint"
%env WANDB_PROJECT=LMSYS
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Data & Configs
```

---The following area is a Code cell (cell numver is 6)---
```python
def seed_everything(seed):
    import random
    import os
    import numpy as np
    import torch
    
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(2024)
```

---The following area is a Code cell (cell numver is 7)---
```python
max_len = 1024
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Generate label & remove columns
```

---The following area is a Code cell (cell numver is 9)---
```python
folder_path = Path('/kaggle/input/lmsys-chatbot-arena')
```

---The following area is a Code cell (cell numver is 10)---
```python
train_df = load_dataset('csv', data_files=str(folder_path / 'train.csv'))
# train_df = load_dataset('csv', data_files='external_train.csv')
# train_df = load_dataset('csv', data_files='/kaggle/input/lmsys-additional-33k-labelled-conversations/lmsys-33k-deduplicated.csv')
test_df = load_dataset('csv', data_files=str(folder_path / 'test.csv'))
sample_df = load_dataset('csv', data_files=str(folder_path / 'sample_submission.csv'))
```

---The following area is a Code cell (cell numver is 11)---
```python
def get_label(sample):
    sample['label'] = np.argmax([sample['winner_model_a'], sample['winner_model_b'], sample['winner_tie']])
    return sample
train_df = train_df.map(get_label)
```

---The following area is a Code cell (cell numver is 12)---
```python
train_df = train_df.select_columns(['prompt', 'response_a', 'response_b', 'label'])
train_df
```

---The following area is a Code cell (cell numver is 13)---
```python
dataset = train_df['train'].train_test_split(0.05)
dataset
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Models
```

---The following area is a Code cell (cell numver is 15)---
```python
# model_name = "microsoft/deberta-v3-base"
adapther_tokenizer_path = "/kaggle/input/deberta-finetuned/DeBerta-base-5"
model_name = "/kaggle/input/deberta-finetuned/LMSYS/checkpoint-2980"
```

---The following area is a Code cell (cell numver is 16)---
```python
tokenizer = AutoTokenizer.from_pretrained(adapther_tokenizer_path, model_max_length=max_len)

model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
)
```

---The following area is a Code cell (cell numver is 17)---
```python
loftq_config = LoftQConfig(loftq_bits=4)
peft_config = LoraConfig(
#     target_modules=['query_proj', 'value_proj', 'key_proj'],
    use_rslora=True,
    r=16,
    lora_alpha=8,
    lora_dropout=0.1,
    task_type=TaskType.SEQ_CLS,
    init_lora_weights='loftq',
)

model = get_peft_model(model, peft_config)
```

---The following area is a Code cell (cell numver is 18)---
```python
def print_trainable_parameters(model):
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )
    
print_trainable_parameters(model)
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 20)---
```python
sep_token = tokenizer.sep_token_id
cls_token = tokenizer.cls_token_id
tokenizer.add_special_tokens({'pad_token': '<pad>'})
# model.resize_token_embeddings(len(tokenizer))
len_to_each = max_len // 3 - 3
```

---The following area is a Code cell (cell numver is 21)---
```python
def tokenize_function(sample):
    prompt =  tokenizer(sample['prompt'], max_length=len_to_each, truncation=True, padding=True).input_ids
    response_a = tokenizer(sample['response_a'], max_length=len_to_each, truncation=True, padding=True).input_ids
    response_b = tokenizer(sample['response_b'], max_length=len_to_each, truncation=True, padding=True).input_ids
    
    sample['input_ids'] = [cls_token] + prompt + [sep_token] + response_a + [sep_token] + response_b
    return sample
```

---The following area is a Code cell (cell numver is 22)---
```python
dataset = dataset.map(tokenize_function)
```

---The following area is a Code cell (cell numver is 23)---
```python
dataset
```

---The following area is a Code cell (cell numver is 24)---
```python
dataset = dataset.select_columns(['input_ids', 'label'])
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## Train
```

---The following area is a Code cell (cell numver is 26)---
```python
args = TrainingArguments(
    "LMSYS",
    eval_strategy = "steps",
    save_strategy = "steps",
    save_steps=500,
    learning_rate=3e-4,
    fp16=True,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,
    optim='adamw_hf',
    num_train_epochs=3,
    weight_decay=0.01,
    load_best_model_at_end=True,
    save_total_limit = 2,
    metric_for_best_model='accuracy',
    report_to="wandb",
    run_name="DeBerta-base-train-data",
    eval_steps=500,
    logging_strategy="steps", 
    logging_steps=100,
)
```

---The following area is a Code cell (cell numver is 27)---
```python
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    return {
        'eval_log_loss': log_loss(labels, probabilities),
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()
    }
```

---The following area is a Code cell (cell numver is 28)---
```python
data_collator = DataCollatorWithPadding(tokenizer)
```

---The following area is a Code cell (cell numver is 29)---
```python
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=dataset['train'],
    eval_dataset=dataset['test'],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],
)
```

---The following area is a Code cell (cell numver is 30)---
```python
trainer.train()
wandb.finish()
```

---The following area is a Code cell (cell numver is 31)---
```python
save_folder = 'DeBerta-base-trained'
model = model.merge_and_unload()
model.save_pretrained(save_folder)
tokenizer.save_pretrained(save_folder)
```

---The following area is a Code cell (cell numver is 32)---
```python
trainer.save_model(f"{save_folder}-trainer")
```

---The following area is a Code cell (cell numver is 33)---
```python

```

** @@@ Jupyter Notebook numver 59, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install git+https://github.com/huggingface/transformers
!pip install -U bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    AutoModel
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = True )

```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 8)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 10)---
```python
from transformers import AutoModel, MistralPreTrainedModel, MistralModel
class Llama3ForSFT(MistralPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = MistralModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
#         if self.config.pretraining_tp > 1:
#             lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
#             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
#             logits = torch.cat(logits, dim=-1)
#         else:
        logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 11)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)

model = Llama3ForSFT.from_pretrained(
    model_path, 
    torch_dtype=torch.float16, 
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 13)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=3,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 15)---
```python
import transformers 
transformers.__version__
```

---The following area is a Code cell (cell numver is 16)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

** @@@ Jupyter Notebook numver 60, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install  /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
!pip install  /kaggle/input/textstat-dependencies/textstat-0.7.4-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import textstat as ts
import json
from ftfy import fix_encoding
from catboost import CatBoostClassifier,Pool
from lightgbm import LGBMClassifier
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

from tqdm.notebook import tqdm
tqdm.pandas()
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Data
```

---The following area is a Code cell (cell numver is 4)---
```python
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 5)---
```python
response_a = pd.read_csv("/kaggle/input/responses-textstat/response_a.csv")
response_b = pd.read_csv("/kaggle/input/responses-textstat/response_b.csv")
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## Get true data
```

---The following area is a Code cell (cell numver is 7)---
```python
unused_columns = ['model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'text_standard']
```

---The following area is a Code cell (cell numver is 8)---
```python
textstat_train = response_a.drop(columns=unused_columns).merge(response_b.drop(columns=unused_columns), on='id')
```

---The following area is a Code cell (cell numver is 9)---
```python
textstat_train = textstat_train.groupby('id').mean()
textstat_train.to_csv('textstat.csv', index=False)
```

---The following area is a Code cell (cell numver is 10)---
```python
X = textstat_train.copy()
```

---The following area is a Code cell (cell numver is 11)---
```python
y = response_a[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].groupby('id').agg('max').apply(pd.Series.argmax, axis=1)
y.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)
clfs = []
scores = []
for train_index, test_index in tqdm(skf.split(X, y), total=5):

    X_train, X_test = X.iloc[train_index], X.iloc[test_index]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]

    # Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÐºÐ»Ð°ÑÑ Ð´Ð»Ñ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ Ð¾Ð±ÑƒÑ‡ÐµÐ½Ð¸Ñ 
    train_dataset = Pool(data=X_train, label=y_train)
    eval_dataset = Pool(data=X_test, label=y_test)

    clf = CatBoostClassifier(
        depth=6,
        iterations=10000,
        learning_rate=0.06,
#         loss_function="MultiLogloss",  # MultiLogloss
        eval_metric = 'AUC', 
#         custom_metric=["Logloss"],  # 'AUC / Accuracy,
        
        # Ð“Ð»Ð°Ð²Ð½Ð°Ñ Ñ„Ð¸ÑˆÐºÐ° ÐºÐ°Ñ‚Ð±ÑƒÑÑ‚Ð° - Ñ€Ð°Ð±Ð¾Ñ‚Ð° Ñ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ð¼Ð¸ Ð¿Ñ€Ð¸Ð·Ð½Ð°ÐºÐ°Ð¼Ð¸
#         cat_features=cat_features,
        # ignored_features = ignored_features,
        
        # Ð ÐµÐ³ÑƒÐ»ÑÑ€Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸ ÑƒÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ðµ
#         colsample_bylevel=0.4,
#         subsample=0.95,
        l2_leaf_reg=10,
        min_data_in_leaf=50,
        max_bin=70,
        random_strength=1,
        
        # ÐŸÐ°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ñ‹ ÑÐºÐ¾Ñ€ÐµÐ½Ð¸Ñ
        task_type="CPU",    
        thread_count=-1,
        bootstrap_type="Bernoulli", 
        
        # Ð’Ð°Ð¶Ð½Ð¾Ðµ!
        random_seed=2024,
#         auto_class_weights="SqrtBalanced",
        early_stopping_rounds=200)

    clfs.append(clf)

    clf.fit(
        train_dataset,
        eval_set=eval_dataset,
        verbose=200,
        use_best_model=True,
        plot=False)

    scores.append(np.mean([v for k, v in clf.best_score_["validation"].items() if "Recall" in k], dtype="float16"))
    # scores.append(clf.best_score_['validation']['MultiClass'])
    # clf.save_model("../tmp_data/cool_catboost_model_{}_deep".format(n))

assert len(clfs) == 5
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Generate test
```

---The following area is a Code cell (cell numver is 14)---
```python
df_test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))

    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])
    return tmp
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_features(df, column):
    df = df.copy()
    df['flesch_reading_ease'] = df[column].progress_apply(lambda x: ts.flesch_reading_ease(str(x)))
    df['flesch_kincaid_grade'] = df[column].progress_apply(lambda x: ts.flesch_kincaid_grade(str(x)))
    df['smog_index'] = df[column].progress_apply(lambda x: ts.smog_index(str(x)))
    df['automated_readability_index'] = df[column].progress_apply(lambda x: ts.automated_readability_index(str(x)))
    df['dale_chall_readability_score'] = df[column].progress_apply(lambda x: ts.dale_chall_readability_score(str(x)))
    df['difficult_words'] = df[column].progress_apply(lambda x: ts.difficult_words(str(x)))
    df['linsear_write_formula'] = df[column].progress_apply(lambda x: ts.linsear_write_formula(str(x)))
    df['gunning_fog'] = df[column].progress_apply(lambda x: ts.gunning_fog(str(x)))
    df['text_standard'] = df[column].progress_apply(lambda x: ts.text_standard(str(x)))
    df['fernandez_huerta'] = df[column].progress_apply(lambda x: ts.fernandez_huerta(str(x)))
    df['szigriszt_pazos'] = df[column].progress_apply(lambda x: ts.szigriszt_pazos(str(x)))
    df['gutierrez_polini'] = df[column].progress_apply(lambda x: ts.gutierrez_polini(str(x)))
    df['crawford'] = df[column].progress_apply(lambda x: ts.crawford(str(x)))
#     df['gulpease_index'] = df[column].progress_apply(lambda x: ts.gulpease_index(str(x)))
#     df['osman'] = df[column].progress_apply(lambda x: ts.osman(str(x)))
    return df
```

---The following area is a Code cell (cell numver is 17)---
```python
test_expl = get_exploded(df_test)
test_a = get_features(test_expl, 'response_a')
test_b = get_features(test_expl, 'response_b')
```

---The following area is a Code cell (cell numver is 18)---
```python
unused_columns = ['prompt', 'response_a', 'response_b', 'text_standard']
textstat_test = test_a.drop(columns=unused_columns).merge(test_b.drop(columns=unused_columns), on='id')
test = textstat_test.groupby('id').mean()
test
```

---The following area is a Code cell (cell numver is 19)---
```python
y_pred = []
for clf in tqdm(clfs, total=len(clfs)):
    y_predict = clf.predict_proba(test)
    y_pred.append(y_predict)
```

---The following area is a Code cell (cell numver is 20)---
```python
y_pred = sum(y_pred) / len(y_pred)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# Submition
```

---The following area is a Code cell (cell numver is 22)---
```python
sample_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 23)---
```python
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred
```

---The following area is a Code cell (cell numver is 24)---
```python
sample_df.head()
```

---The following area is a Code cell (cell numver is 25)---
```python
sample_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 61, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
# from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from ftfy import fix_encoding
import re
import nltk
from nltk.corpus import stopwords
# nltk.download('stopwords')
from torch.utils.data import Dataset
import torch
import json
import optuna
import numpy as np

from tqdm.auto import tqdm
tqdm.pandas()
```

---The following area is a Code cell (cell numver is 3)---
```python
stop_words = pd.read_csv("/kaggle/input/nltk-english-stopwords/nltk_eng_stopwords.csv")["list_of_stopwords"].tolist()
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Load Data
```

---The following area is a Code cell (cell numver is 5)---
```python
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 6)---
```python
# if test_df.shape[0] < 10:
#     train_df = train_df[:100]
```

---The following area is a Code cell (cell numver is 7)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))

    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])
    return tmp
```

---The following area is a Code cell (cell numver is 8)---
```python
tmp_train = get_exploded(train_df)
tmp_test = get_exploded(test_df)
```

---The following area is a Code cell (cell numver is 9)---
```python
class MyDataset(Dataset):
    def __init__(self, df, col):
        self.col = col
        self.df = df.copy()
        
        self.df["prompt"] = self.df["prompt"].progress_apply(self.fix_encode)
        self.df[col] = self.df[col].progress_apply(self.fix_encode)
        
        self.df = self.df.explode(['prompt', col])
    
    def fix_encode(self, x):
        return json.loads(fix_encoding(x))

    def __len__(self):
        return len(self.df)

    def __getitem__(self, i):
        QA_input = {}
        QA_input['question'] = str(self.df.iloc[i]["prompt"])
        QA_input['context'] = str(self.df.iloc[i][self.col])
        
        if not QA_input['question']:
#             print(True)
            QA_input['question'] = 'empty_text' * 10
        if not QA_input['context']:
#             print(True)
            QA_input['context'] = 'empty_text' * 10
        
        QA_input['question'] = QA_input['question'][:510]
        QA_input['context'] = QA_input['context'][:510]

        return QA_input


dataset_a = MyDataset(train_df, col='response_a')
dataset_b = MyDataset(train_df, col='response_b')

dataset_a_test = MyDataset(test_df, col='response_a')
dataset_b_test = MyDataset(test_df, col='response_b')
```

---The following area is a Code cell (cell numver is 10)---
```python
len(dataset_a)
```

---The following area is a Code cell (cell numver is 11)---
```python
outs_dict = {'dataset_a': [], 'dataset_b': []}
outs_dict_test = {'dataset_a': [], 'dataset_b': []}
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Models

## Generate Score
```

---The following area is a Code cell (cell numver is 13)---
```python
model_list = [
    "deepset/roberta-base-squad2",
    "deepset/deberta-v3-base-squad2",
    "distilbert/distilbert-base-cased-distilled-squad"
#     "Palak/microsoft_deberta-large_squad"
#     'distilbert/distilbert-base-cased-distilled-squad',
#     'deepset/bert-large-uncased-whole-word-masking-squad2'
]

model_list_kaggle = model_list.copy()
for i, model_name in enumerate(model_list_kaggle):
    model_list_kaggle[i] = '/kaggle/input/deberta-v3-base/' + model_name
model_list_kaggle
```

---The following area is a Code cell (cell numver is 14)---
```python
nlp_list = []

for model_name in model_list_kaggle:
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
    
    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer, device='cuda',torch_dtype=torch.float16)
    
    nlp_list.append(nlp)
```

---The following area is a Code cell (cell numver is 15)---
```python
for model_name, pipeline in tqdm(zip(model_list, nlp_list), total=len(nlp_list)):
    pipeline.save_pretrained(model_name)
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_outs(model_list):
    outs_dict = {}
    for model_name in model_list:
        outs_dict[f'{model_name}-a'] = []
        outs_dict[f'{model_name}-b'] = []
    return outs_dict
```

---The following area is a Code cell (cell numver is 17)---
```python
outs_train = get_outs(model_list)
outs_test = get_outs(model_list)
```

---The following area is a Code cell (cell numver is 18)---
```python
# tokenizer_kwargs = {"truncation": True, 'max_length': 512, 'padding': True}
```

---The following area is a Code cell (cell numver is 19)---
```python
def get_score(nlp, dataset) -> list:
    scores = []
    for sample in tqdm(dataset, total=len(dataset)):
        try:
            out = nlp(sample, doc_stride=47)
        except:
            print('omom')
            out = {}
            out['score'] = 0
        scores.append(out['score'])
#     for out in tqdm(nlp(dataset, 
#         handle_impossible_answer=True,
#         max_seq_len=384,
#         ), total=len(dataset)):
#         scores.append(out['score'])
    return scores
```

---The following area is a Code cell (cell numver is 20)---
```python
for model_name, nlp in tqdm(zip(model_list, nlp_list), total=len(model_list)):
    outs_train[f'{model_name}-a'] = get_score(nlp, dataset_a)
    outs_train[f'{model_name}-b'] = get_score(nlp, dataset_b)
    
    outs_test[f'{model_name}-a'] = get_score(nlp, dataset_a_test)
    outs_test[f'{model_name}-b'] = get_score(nlp, dataset_b_test)
    
    del nlp
```

---The following area is a Code cell (cell numver is 21)---
```python
outs_train = pd.DataFrame(outs_train)
outs_test = pd.DataFrame(outs_test)
```

---The following area is a Code cell (cell numver is 22)---
```python
outs_train.head()
```

---The following area is a Code cell (cell numver is 23)---
```python
df_bert = pd.concat([outs_train, tmp_train[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].reset_index()], axis=1).drop('index', axis=1)
df_bert_test = pd.concat([outs_test, tmp_test['id'].reset_index()], axis=1).drop('index', axis=1)
df_bert.head()
```

---The following area is a Code cell (cell numver is 24)---
```python
model_dataset = outs_train.columns
model_dataset
```

---The following area is a Code cell (cell numver is 25)---
```python
df_bert_train = df_bert.groupby('id').mean()

df_bert_test = df_bert_test.groupby('id').mean()
```

---The following area is a Code cell (cell numver is 26)---
```python
df_bert_train.to_csv('df_bert_train.csv', index=False)
df_bert_test.to_csv('df_bert_test.csv', index=False)
```

---The following area is a Code cell (cell numver is 27)---
```python
df_bert_train.head()
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
## Collect in one column
```

---The following area is a Code cell (cell numver is 29)---
```python
df_bert_train['winner'] = df_bert_train[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(np.argmax, axis=1)
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 30)---
```python
df_bert_train.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)
```

---The following area is a Code cell (cell numver is 31)---
```python
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 32)---
```python
df_bert_train.to_csv('deberts.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
## Train model
```

---The following area is a Code cell (cell numver is 34)---
```python
model = CatBoostClassifier(verbose=False, random_state=2024)
```

---The following area is a Code cell (cell numver is 35)---
```python
target = 'winner'
```

---The following area is a Code cell (cell numver is 36)---
```python
df_bert_train.head()
```

---The following area is a Code cell (cell numver is 37)---
```python
X_train = df_bert_train.drop(columns=target)
y_train = df_bert_train[target]
```

---The following area is a Code cell (cell numver is 38)---
```python
model.fit(X_train, y_train)
```

---The following area is a Code cell (cell numver is 39)---
```python
y_pred = model.predict_proba(df_bert_test)
y_pred
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# Submit
```

---The following area is a Code cell (cell numver is 41)---
```python
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred
```

---The following area is a Code cell (cell numver is 42)---
```python
sample_df
```

---The following area is a Code cell (cell numver is 43)---
```python
sample_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 62, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
# Import necessary libraries
import numpy as np
import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, log_loss, confusion_matrix, roc_curve, auc
from imblearn.over_sampling import SMOTE
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import matplotlib.pyplot as plt
import seaborn as sns

print('All libraries has been imported')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Data Collection
train_file = '/kaggle/input/lmsys-chatbot-arena/train.csv'
test_file = '/kaggle/input/lmsys-chatbot-arena/test.csv'
train_data = pd.read_csv(train_file)
test_data = pd.read_csv(test_file)

```

---The following area is a Code cell (cell numver is 3)---
```python
# Data Understanding
print("Training Dataset Info:")
print(train_data.info())
print("\nFirst few rows:")
print(train_data.head())

print("\nTest Dataset Info:")
print(test_data.info())
print("\nFirst few rows:")
print(test_data.head())

# Visualize class distribution before resampling
plt.figure(figsize=(12, 6))
sns.countplot(x=train_data['winner_model_a'])
plt.title("Class Distribution Before Resampling")
plt.show()
```

---The following area is a Code cell (cell numver is 4)---
```python
# Data Preparation
def clean_text(text, stop_words):
    text = re.sub(r'\[.*?\]', '', text)
    text = re.sub(r'http\S+|www.\S+', '', text)
    text = re.sub(r'<.*?>+', '', text)
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    text = text.lower()
    text = ' '.join(word for word in text.split() if word not in stop_words)
    return text

stop_words = set()

# Clean text data
train_data['prompt'] = train_data['prompt'].apply(lambda x: clean_text(x, stop_words))
train_data['response_a'] = train_data['response_a'].apply(lambda x: clean_text(x, stop_words))
train_data['response_b'] = train_data['response_b'].apply(lambda x: clean_text(x, stop_words))

test_data['prompt'] = test_data['prompt'].apply(lambda x: clean_text(x, stop_words))
test_data['response_a'] = test_data['response_a'].apply(lambda x: clean_text(x, stop_words))
test_data['response_b'] = test_data['response_b'].apply(lambda x: clean_text(x, stop_words))

```

---The following area is a Code cell (cell numver is 5)---
```python
# Vectorize text data
vectorizer = TfidfVectorizer(max_features=1000)
train_text = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']
test_text = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']

X_train_text = vectorizer.fit_transform(train_text)
X_test_text = vectorizer.transform(test_text)

# Verbosity Bias - Add response lengths and length differences
train_data['response_a_length'] = train_data['response_a'].apply(len)
train_data['response_b_length'] = train_data['response_b'].apply(len)
test_data['response_a_length'] = test_data['response_a'].apply(len)
test_data['response_b_length'] = test_data['response_b'].apply(len)
train_data['length_diff'] = train_data['response_a_length'] - train_data['response_b_length']
test_data['length_diff'] = test_data['response_a_length'] - test_data['response_b_length']

# Position Bias - Add position bias feature
train_data['position_bias_a'] = 0  # Assuming response_a is always the first
train_data['position_bias_b'] = 1  # Assuming response_b is always the second
test_data['position_bias_a'] = 0
test_data['position_bias_b'] = 1

# Self-Enhancement Bias - Add self-enhancement detection feature
def detect_self_enhancement(text):
    keywords = ['best', 'better', 'excellent', 'superior', 'number one']
    for keyword in keywords:
        if keyword in text:
            return 1
    return 0

train_data['self_enhancement_a'] = train_data['response_a'].apply(detect_self_enhancement)
train_data['self_enhancement_b'] = train_data['response_b'].apply(detect_self_enhancement)
test_data['self_enhancement_a'] = test_data['response_a'].apply(detect_self_enhancement)
test_data['self_enhancement_b'] = test_data['response_b'].apply(detect_self_enhancement)

```

---The following area is a Code cell (cell numver is 6)---
```python
# Encoding Categorical Features
categorical_columns = ['model_a', 'model_b']
for column in categorical_columns:
    if column not in test_data.columns:
        test_data[column] = 'missing'
train_data_encoded = pd.get_dummies(train_data, columns=categorical_columns)
test_data_encoded = pd.get_dummies(test_data, columns=categorical_columns)
train_data_encoded, test_data_encoded = train_data_encoded.align(test_data_encoded, join='left', axis=1, fill_value=0)

# Handle Missing Columns
test_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], errors='ignore', inplace=True)

# Remove non-numeric columns
non_numeric_columns = train_data_encoded.select_dtypes(exclude=[np.number]).columns
train_data_encoded.drop(columns=non_numeric_columns, inplace=True)
test_data_encoded.drop(columns=non_numeric_columns, inplace=True)

# Combine all features into training and testing sets
X_train_combined = np.hstack((X_train_text.toarray(), train_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie']).values))
X_test_combined = np.hstack((X_test_text.toarray(), test_data_encoded.values))
X = X_train_combined
y = train_data_encoded['winner_model_a']

```

---The following area is a Code cell (cell numver is 7)---
```python
# Modeling
# Resample Data
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Visualize class distribution after resampling
plt.figure(figsize=(12, 6))
sns.countplot(x=y_resampled)
plt.title("Class Distribution After Resampling")
plt.show()

```

---The following area is a Code cell (cell numver is 8)---
```python
# Split the resampled data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

```

---The following area is a Code cell (cell numver is 9)---
```python
# Hyperparameter tuning and model selection
models = {
    'Logistic Regression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),
        'params': {'C': [0.01, 0.1, 1, 10, 100]}
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),
        'params': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30]}
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),
        'params': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]}
    }
}

best_models = {}

for model_name, config in models.items():
    grid_search = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=5, scoring='neg_log_loss', verbose=2, n_jobs=-1)
    grid_search.fit(X_train, y_train)
    best_models[model_name] = grid_search.best_estimator_
    print(f"Best {model_name} Model: {grid_search.best_params_}")

```

---The following area is a Code cell (cell numver is 10)---
```python
# Evaluation
for model_name, model in best_models.items():
    y_val_pred = model.predict(X_val)
    y_val_pred_proba = model.predict_proba(X_val)
    print(f"Classification Report on Validation Set ({model_name}):")
    print(classification_report(y_val, y_val_pred, zero_division=1))
    print(f"Log Loss ({model_name}): {log_loss(y_val, y_val_pred_proba)}")

    # Confusion Matrix
    cm = confusion_matrix(y_val, y_val_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
    plt.title(f'Confusion Matrix ({model_name})')
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba[:, 1])
    roc_auc = auc(fpr, tpr)
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title(f'Receiver Operating Characteristic ({model_name})')
    plt.legend(loc="lower right")
    plt.show()

```

---The following area is a Code cell (cell numver is 11)---
```python
# Deployment with the best model (example: Logistic Regression)
best_lr_model = best_models['Logistic Regression']
test_predictions_proba = best_lr_model.predict_proba(X_test_combined)
submission_df = pd.DataFrame({
    'id': test_data['id'],
    'winner_model_a': test_predictions_proba[:, 0],
    'winner_model_b': test_predictions_proba[:, 1],
    'winner_tie': 0.0  # Assuming binary classification
})
submission_df.to_csv('submission.csv', index=False)
print(submission_df.head())
print("Submission file saved successfully.")

```

** @@@ Jupyter Notebook numver 63, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.multioutput import MultiOutputClassifier
import numpy as np
# Load the dataset
train_df = pd.read_csv('/kaggle/input/my-files/train.csv')

# Check the column names
print(train_df.columns)

# Check if the 'winner' column is created properly
def get_winner(row):
    if row['winner_model_a'] == 1:
        return 'a'
    elif row['winner_model_b'] == 1:
        return 'b'
    else:
        return 'tie'

train_df['winner'] = train_df.apply(get_winner, axis=1)

# Display the first few rows to ensure 'winner' column is created
print(train_df.head())

# Check if 'winner' column exists in the DataFrame
print('winner' in train_df.columns)

# Visualize the distribution of the 'winner' variable
sns.countplot(data=train_df, x='winner')
plt.title("Distribution of Winners")
plt.xlabel("Winner")
plt.ylabel("Count")
plt.show()

# Feature engineering: Adding length of responses as features
train_df['response_a_length'] = train_df['response_a'].apply(len)
train_df['response_b_length'] = train_df['response_b'].apply(len)

# Display the correlation matrix
correlation_matrix = train_df[['response_a_length', 'response_b_length']].corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title("Correlation Matrix")
plt.show()

# Prepare the data for training
X = train_df[['response_a_length', 'response_b_length']]
y = train_df['winner']

# One-hot encode the target variable
y = pd.get_dummies(y)

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the model
model = MultiOutputClassifier(RandomForestClassifier())
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict_proba(X_val)

# Convert list of arrays to a single array
y_pred = np.array([pred[:, 1] for pred in y_pred]).T

# Evaluate the model
loss = log_loss(y_val, y_pred)
print(f'Log Loss: {loss}')

# Prepare the submission file
# Load the test set
test_df = pd.read_csv('/kaggle/input/my-files/test.csv')

# Add feature engineering steps for the test set
test_df['response_a_length'] = test_df['response_a'].apply(len)
test_df['response_b_length'] = test_df['response_b'].apply(len)

# Make predictions on the test set
test_pred = model.predict_proba(test_df[['response_a_length', 'response_b_length']])

# Convert list of arrays to a single array
test_pred = np.array([pred[:, 1] for pred in test_pred]).T

# Format the predictions for submission
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': test_pred[:, 0],
    'winner_model_b': test_pred[:, 1],
    'winner_tie': test_pred[:, 2]
})

# Save the submission file
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 64, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes
!pip install -q -U transformers
!pip install -q -U peft
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import os
import torch
from time import time
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    Trainer,
    EarlyStoppingCallback
)
from trl import SFTTrainer,setup_chat_format
import numpy as np
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoTokenizer
from datasets import DatasetDict, Dataset
```

---The following area is a Code cell (cell numver is 2)---
```python
class CFG:
    VER = 1
    NUM_LABELS = 3
    BATCH_SIZE = 4
    EPOCHS = 1
    
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'

    
    SEED = 2024 
    MAX_LENGTH = 1024 
    
    OUTPUT_DIR = 'Llama 3 8b fine-tuned model'
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Prepare Dataset
```

---The following area is a Code cell (cell numver is 4)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train = train[:int(len(train)*0.01)]
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 5)---
```python
from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)

dataset_train = Dataset.from_pandas(train_df)
dataset_val = Dataset.from_pandas(val_df)

# Combine them into a single DatasetDict
dataset = DatasetDict({
    'train': dataset_train,
    'val': dataset_val,
})
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Model Loading -Quantization
```

---The following area is a Code cell (cell numver is 7)---
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)


model = AutoModelForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    quantization_config=quantization_config,
    num_labels=CFG.NUM_LABELS,
    device_map='auto'
)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# LoRA Configuration
```

---The following area is a Code cell (cell numver is 9)---
```python
lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    task_type = 'SEQ_CLS'
)
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
```

---The following area is a Code cell (cell numver is 10)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, add_prefix_space=True)

tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True
```

---The following area is a Code cell (cell numver is 11)---
```python
model.config.pad_token_id = tokenizer.pad_token_id
# model.config.use_cache = False
model.config.pretraining_tp = 1
```

---The following area is a Code cell (cell numver is 12)---
```python
def data_preprocesing(row):
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')

# dataset = Dataset.from_pandas(train)
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])
tokenized_data.set_format("torch")
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Finetune Llama 3: Model Training
```

---The following area is a Code cell (cell numver is 14)---
```python
training_args = TrainingArguments(
    per_device_train_batch_size=CFG.BATCH_SIZE,
    num_train_epochs=CFG.EPOCHS,
    logging_dir = f'./logs_v{CFG.VER}',
    output_dir = f'./output_v{CFG.VER}',
    logging_steps=10,
    save_steps=10,
    logging_first_step=True,
    overwrite_output_dir=True,
    warmup_ratio=0.0,
    learning_rate=5e-5,
    lr_scheduler_type='constant',
    weight_decay=0.01,
    eval_steps=10,
    evaluation_strategy='steps',
    save_total_limit=2,
    report_to='none',
    load_best_model_at_end = True
)
```

---The following area is a Code cell (cell numver is 15)---
```python
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_data['train'],
    eval_dataset=tokenized_data['val'],
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]
)

train_result = trainer.train()
```

** @@@ Jupyter Notebook numver 65, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/accelerate-0.29.3-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/fsspec-2024.3.1-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/unsloth-2024.4-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/peft-0.10.0-py3-none-any.whl'

```

---The following area is a Code cell (cell numver is 1)---
```python

!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl'
##!pip install '/kaggle/input/llm-detect-pip/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
!pip install '/kaggle/input/nvidia-nccl-2-19-3/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl'
#!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'


```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/shtab-1.7.1-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/tyro-0.8.3-py3-none-any.whl'
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/trl-0.8.5-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
```

---The following area is a Code cell (cell numver is 5)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 6)---
```python
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl'
```

---The following area is a Code cell (cell numver is 7)---
```python
import torch
print(f"pytorch version {torch.__version__}")
```

---The following area is a Code cell (cell numver is 8)---
```python

```

---The following area is a Code cell (cell numver is 9)---
```python

```

---The following area is a Code cell (cell numver is 10)---
```python
from unsloth import FastLanguageModel
```

---The following area is a Code cell (cell numver is 11)---
```python

```

---The following area is a Code cell (cell numver is 12)---
```python

```

---The following area is a Code cell (cell numver is 13)---
```python

```

---The following area is a Code cell (cell numver is 14)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 15)---
```python
import pandas as pd
import numpy as np
```

---The following area is a Code cell (cell numver is 16)---
```python
import os
from tqdm import tqdm
import bitsandbytes as bnb
import torch
import torch.nn as nn
import transformers
from datasets import Dataset
from peft import LoraConfig, PeftConfig
from peft import PeftModel
from trl import SFTTrainer
from trl import setup_chat_format
from transformers import (AutoModelForCausalLM, 
                          AutoTokenizer, 
                          BitsAndBytesConfig, 
                          TrainingArguments, 
                          pipeline, 
                          logging)
from sklearn.metrics import (accuracy_score, 
                             classification_report, 
                             confusion_matrix)
from sklearn.model_selection import train_test_split
```

---The following area is a Code cell (cell numver is 17)---
```python
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))
```

---The following area is a Code cell (cell numver is 18)---
```python
def generate_test_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred 
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
"""
```

---The following area is a Code cell (cell numver is 19)---
```python

```

---The following area is a Code cell (cell numver is 20)---
```python
'''
MODEL_PATH = "/kaggle/input/llama-3/transformers/8b-hf/1"

quantization_config = BitsAndBytesConfig(
    load_in_4bit = True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=True,
)

model1 = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map = "auto",
    trust_remote_code = True,
    quantization_config=quantization_config,
)

tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=torch.float16)
'''
```

---The following area is a Code cell (cell numver is 21)---
```python
max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!
dtype = torch.float16 # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+
load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",

    "unsloth/llama-2-7b-bnb-4bit",
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit"
] # More models at https://huggingface.co/unsloth

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name = "/kaggle/input/unslothmistral-7b-instruct-v0-2-bnb-4bit/unsloth-mistral-7b-instruct-v0.2-bnb-4bit", # Choose ANY! eg teknium/OpenHermes-2.5-Mistral-7B
    max_seq_length = max_seq_length,
    dtype = dtype,
    load_in_4bit = load_in_4bit,
    # token = "hf_...", # use one if using gated models like meta-llama/Llama-2-7b-hf
)
```

---The following area is a Code cell (cell numver is 22)---
```python
model = FastLanguageModel.get_peft_model(
    model,
    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
    target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                      "gate_proj", "up_proj", "down_proj",],
    lora_alpha = 16,
    lora_dropout = 0, # Supports any, but = 0 is optimized
    bias = "none",    # Supports any, but = "none" is optimized
    # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
    use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
    random_state = 3407,
    use_rslora = False,  # We support rank stabilized LoRA
    loftq_config = None, # And LoftQ
)
```

---The following area is a Code cell (cell numver is 23)---
```python
submission = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'))
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
```

---The following area is a Code cell (cell numver is 24)---
```python
result = []
for index, row in train.iterrows():
    if(row['winner_model_a']==1): result.append('winner_model_a')
    elif(row['winner_model_b']==1): result.append('winner_model_b')
    elif(row['winner_tie']==1): result.append('winner_tie')
train.insert(6, "Preference", result, True)
train
```

---The following area is a Code cell (cell numver is 25)---
```python
train = train[:1000]
train.shape
```

---The following area is a Code cell (cell numver is 26)---
```python
train_size = int(train.shape[0]*0.6)
test_val_size = train.shape[0]-train_size
train_, test_val  = train_test_split(train, train_size=train_size,
                                    test_size=test_val_size,
                                    random_state=42)

test_, val_ = train_test_split(test_val, train_size=int(test_val_size/2),
                                    test_size=int(test_val_size/2),
                                    random_state=42)
```

---The following area is a Code cell (cell numver is 27)---
```python
def past_generate_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}
            ----------------------------------------------------------------------------------------------------------
            model_a: {data_point["model_a"]}
            response_a: {data_point["response_a"]}
            ----------------------------------------------------------------------------------------------------------
            model_b: {data_point["model_b"]}
            response_b: {data_point["response_b"]}
            ----------------------------------------------------------------------------------------------------------
            Preference= {data_point["Preference"]} """.strip()

def generate_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
Human Preference Label: {data_point["Preference"]}
"""

def past_generate_test_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}
            ----------------------------------------------------------------------------------------------------------

            response_a: {data_point["response_a"]}
            ----------------------------------------------------------------------------------------------------------

            response_b: {data_point["response_b"]}
            ----------------------------------------------------------------------------------------------------------
            Preference: """.strip()

def generate_test_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}
Model A Response: {data_point["response_a"]}
Model B Response: {data_point["response_b"]}
"""

```

---The following area is a Code cell (cell numver is 28)---
```python
X_train = pd.DataFrame(train_.apply(generate_prompt, axis=1),
                       columns=["text"])
X_eval = pd.DataFrame(val_.apply(generate_prompt, axis=1),
                      columns=["text"])
X_test = pd.DataFrame(test_.apply(generate_test_prompt, axis=1), columns=["text"])

y_true = test_.Preference

train_data = Dataset.from_pandas(X_train)
eval_data = Dataset.from_pandas(X_eval)
```

---The following area is a Code cell (cell numver is 29)---
```python
def evaluate(y_true, y_pred):
    labels = ['winner_model_a', 'winner_model_b', 'winner_tie']
    mapping = {'none':0, 'winner_model_a': 1, 'winner_model_b': 2, 'winner_tie': 3}
    def map_func(x):
        return mapping.get(x, 1)

    y_true = np.vectorize(map_func)(y_true)
    y_pred = np.vectorize(map_func)(y_pred)

    # Calculate accuracy
    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)
    print(f'Accuracy: {accuracy:.3f}')

    # Generate accuracy report
    unique_labels = set(y_true)  # Get unique labels

    for label in unique_labels:
        label_indices = [i for i in range(len(y_true))
                         if y_true[i] == label]
        label_y_true = [y_true[i] for i in label_indices]
        label_y_pred = [y_pred[i] for i in label_indices]
        accuracy = accuracy_score(label_y_true, label_y_pred)
        print(f'Accuracy for label {label}: {accuracy:.3f}')

    # Generate classification report
    class_report = classification_report(y_true=y_true, y_pred=y_pred)
    print('\nClassification Report:')
    print(class_report)

    # Generate confusion matrix
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])
    print('\nConfusion Matrix:')
    print(conf_matrix)
```

---The following area is a Code cell (cell numver is 30)---
```python
from trl import SFTTrainer
from transformers import TrainingArguments


trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = train_data,
    eval_dataset = eval_data,
    dataset_text_field = "text",
    max_seq_length = max_seq_length,
    dataset_num_proc = 2,
    packing = False, # Can make training 5x faster for short sequences.
    args = TrainingArguments(
        per_device_train_batch_size = 1,
        gradient_accumulation_steps = 2,
        warmup_steps = 5,
        #evaluation_strategy="steps",         # evaluate at the end of each epoch
        #save_strategy="steps",
        #save_steps=10,                      # save checkpoint every 500 steps
        #eval_steps=10,
        max_steps = 100,
        learning_rate = 2e-4,
        fp16 = True,
        bf16 = False,
        logging_steps = 1,
        optim = "adamw_8bit",
        weight_decay = 0.01,
        #load_best_model_at_end=True,
        lr_scheduler_type = "linear",
        seed = 3407,
        output_dir = "outputs",
    ),
)
```

---The following area is a Code cell (cell numver is 31)---
```python
import wandb
wandb.init(mode='disabled')
```

---The following area is a Code cell (cell numver is 32)---
```python
import gc
gc.collect()
```

---The following area is a Code cell (cell numver is 33)---
```python
trainer.train()
```

---The following area is a Code cell (cell numver is 34)---
```python

```

---The following area is a Code cell (cell numver is 35)---
```python

```

---The following area is a Code cell (cell numver is 36)---
```python
test['text'] = test.apply(lambda x: generate_test_prompt(x), axis=1)
```

---The following area is a Code cell (cell numver is 37)---
```python
'''
prompt = test.loc[2]["text"]

#print(prompt)

#FastLanguageModel.for_inference(model) # Enable native 2x faster inference

inputs = tokenizer(prompt, return_tensors='pt').to(device)

outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
out1 = tokenizer.batch_decode(outputs)
print("*************")
print(out1)
'''
```

---The following area is a Code cell (cell numver is 38)---
```python
def extract_logits(text):
    try:
        d = {'winner_a': [], 'winner_b': [], 'winner_tie': []}
        logits = re.findall(r"Logits: \[(.*?)\]", text)[0]
        logits_dict = {k.strip(): float(v) for k, v in [item.split(": ") for item in logits[1:-1].split(", ")]}

        for k, v in logits_dict.items():
            if '_a' in k:
                d['winner_a'].append(v)
            elif '_b' in k:
                d['winner_b'].append(v)
            elif '_tie' in k:
                d['winner_tie'].append(v)
            
        df = pd.DataFrame.from_dict(d)
    
    except:
        d = {'winner_a': [0.33], 'winner_b': [0.33], 'winner_tie': [0.33]}
        df = pd.DataFrame.from_dict(d)


    
    

    return df
```

---The following area is a Code cell (cell numver is 39)---
```python
prompt = X_test.iloc[2]["text"]

#print(prompt)



FastLanguageModel.for_inference(model) # Enable native 2x faster inference

inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#print(inputs)

print("*************")
outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
out1 = tokenizer.batch_decode(outputs)

print(out1)


```

---The following area is a Code cell (cell numver is 40)---
```python

```

---The following area is a Code cell (cell numver is 41)---
```python
# import re
# label_pattern = re.compile(r'Human Preference Label:\s*(\w+)')
```

---The following area is a Code cell (cell numver is 42)---
```python
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')
#     logits_match = logits_pattern.search(text[0])

#     if logits_match:
#         logits_str = logits_match.group(1)
#         logits_list = [(x.strip()) for x in logits_str.split(',')]

#         # Output results
#         #print("Logits:", logits_list)
#     else:
#         # if it is not able to find string, give deafult values
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
    
    
#     return logits_list
```

---The following area is a Code cell (cell numver is 43)---
```python
# y_pred = []
# logits = pd.DataFrame()
# for i in tqdm(range(len(X_test))):
#     prompt = X_test.iloc[i]["text"]
#     FastLanguageModel.for_inference(model) # Enable native 2x faster inference

#     inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#     outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
#     out1 = tokenizer.batch_decode(outputs)
#     try:
#         preferences = label_pattern.search(out1[0]).group(1)
#         lgts = extract_logits(out1[0])
#     except Exception as e:
#         print(out1)
#         preferences = 'winner_tie'
#     print(preferences)
#     print(lgts)
#     y_pred.append(preferences)
    
#     logits = pd.concat([logits, lgts],axis=0)
```

---The following area is a Code cell (cell numver is 44)---
```python
##evaluate(y_true, y_pred)
```

---The following area is a Code cell (cell numver is 45)---
```python
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')
#     logits_match = logits_pattern.search(text[0])

#     if logits_match:
#         logits_str = logits_match.group(1)
#         logits_list = [(x.strip()) for x in logits_str.split(',')]

#         # Output results
#         #print("Logits:", logits_list)
#     else:
#         # if it is not able to find string, give deafult values
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
    
    
#     return logits_list
```

---The following area is a Code cell (cell numver is 46)---
```python
logits_list = pd.DataFrame()
for i in tqdm(range(len(test))):
    prompt = test.iloc[i]["text"]
    FastLanguageModel.for_inference(model) # Enable native 2x faster inference

    inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

    outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)
    out1 = tokenizer.batch_decode(outputs)
    preferences = extract_logits(out1[0])
    #print(preferences)
    logits_list = pd.concat([logits_list, preferences],axis=0)
```

---The following area is a Code cell (cell numver is 47)---
```python
# import pandas as pd



# # Function to convert string list to dictionary
# def logits_to_dict(logits):
#     return {item.split(':')[0].strip("' "): float(item.split(':')[1].strip()) for item in logits}

# # Convert each list to a dictionary
# logits_dicts = [logits_to_dict(logits) for logits in logits_list]

# # Create DataFrame
# df = pd.DataFrame(logits_dicts)

# df

```

---The following area is a Code cell (cell numver is 48)---
```python
df1 = logits_list.reset_index(drop=True)
```

---The following area is a Code cell (cell numver is 49)---
```python
df = pd.concat([test['id'], df1],axis=1)
```

---The following area is a Code cell (cell numver is 50)---
```python

```

---The following area is a Code cell (cell numver is 51)---
```python
df
```

---The following area is a Code cell (cell numver is 52)---
```python
df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 53)---
```python

```

** @@@ Jupyter Notebook numver 66, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Importing Libraries
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification,Qwen2ForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers


```

---The following area is a Code cell (cell numver is 4)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
train = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytes configuration
# bnb_config =  BitsAndBytesConfig(
#     load_in_8bit=True,
#     bnb_8bit_compute_dtype=torch.float16,
#     bnb_8bit_use_double_quant=False)

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)
# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')

base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 12)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 15)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 16)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 18)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 20)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 21)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Code cell (cell numver is 22)---
```python

out = pd.DataFrame(llama_preds, 
                index = test.id, 
                    columns = train.columns[-3:])
display(out.head())

```

---The following area is a Code cell (cell numver is 23)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 67, the number of votes :1 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
The work in this notebook is inspired by these notebooks:
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## Importing Libraries
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

import gc
import os
import io
import time
import json
import random
import pickle
import zipfile
import datetime
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers


```

---The following area is a Code cell (cell numver is 3)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1284
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Prepare Data
```

---The following area is a Code cell (cell numver is 5)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 6)---
```python
# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## Load model 
> We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 10)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 11)---
```python
# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Load weights
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
#Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

#Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 15)---
```python
gc.collect()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Inference
```

---The following area is a Code cell (cell numver is 17)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}
```

---The following area is a Code cell (cell numver is 19)---
```python
# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 20)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 22)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 23)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 24)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 25)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 26)---
```python
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 27)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 28)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 29)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)

```

---The following area is a Code cell (cell numver is 30)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 31)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 32)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 33)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 34)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 35)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 36)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 37)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 38)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 39)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 40)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 41)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 42)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.2 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 44)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 45)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 46)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 68, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Refer: [IFD](https://github.com/tianyi-lab/Superfiltering)
$$\mathrm{IFD}_\theta(Q,A)=\frac{\mathrm{PPL}_\theta(A|Q)}{\mathrm{PPL}_\theta(A)}$$
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import numpy as np 
import pandas as pd 
import regex as re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix, save_npz, load_npz, hstack
import lightgbm as lgb
from tqdm import tqdm
import gensim
import itertools
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
from transformers import DebertaV2Tokenizer, DebertaV2Model
from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer
import torch
import joblib
import unicodedata
import re
import time
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 2)---
```python
MAX_LENGTH = 1024
deberta_path = "/kaggle/input/debertav3base"
gpt_path = "/kaggle/input/qwen2-1-5b-instruct"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
SEED = 42
set_seed(SEED)
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
vectorize_on_train_and_test = True
#quick_test for training on small part of train data (and not using bunch of GPU on submit)
#(if this is on - saved models won't be fully trained)
quick_test = True
quick_test_items = 1000
#automatically disable quick_test if we detect actual test data... (assures full training when scoring)
if (len(test)) > 3:quick_test = False
if quick_test: train = train.head(quick_test_items)

def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)
print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))

target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
train.head(3)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### TF-IDF
```

---The following area is a Code cell (cell numver is 5)---
```python
train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)
test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)

if vectorize_on_train_and_test:
    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)
else:
    vector_fit_text = train_text
```

---The following area is a Code cell (cell numver is 6)---
```python
def custom_tokenizer(text):
    return re.findall(r'[^\W]+', text)

#word-level vectorizer
tfidf_word_vectorizer = TfidfVectorizer(
    ngram_range=(1, 5),
    tokenizer=custom_tokenizer,
    token_pattern=None,
    strip_accents='unicode',
    min_df=4,
    max_features=300
)

#char-level vectorizer
tfidf_char_vectorizer = TfidfVectorizer(
    analyzer='char',
    ngram_range=(1, 5), 
    max_features=1000, 
    min_df=4
)

def batch_process(texts, batch_size):
    for i in range(0, len(texts), batch_size):
        yield texts[i:i + batch_size]

#doing in batches so we can see progress
batch_size = 1000
for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):
    if len(batch) >= tfidf_word_vectorizer.min_df:
        tfidf_word_vectorizer.fit(batch)
    if len(batch) >= tfidf_char_vectorizer.min_df:
        tfidf_char_vectorizer.fit(batch)
        
def get_tfidf_vectors(df):
    vectorized_columns = []
    for column in columns_to_vectorize:
        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))
        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))
    return hstack(vectorized_columns)

tfidf_train_vectors = get_tfidf_vectors(train)
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
### LEN
```

---The following area is a Code cell (cell numver is 8)---
```python
def has_none(vals) -> int:
    # some responses contains null and probably they are useful for prediction
    return int(any(val is None for val in vals))

def str_length(vals) -> int:
    length = 0
    for val in vals:
        if isinstance(val, str):
            length += len(val)
    return length

def get_length_features(data: pd.DataFrame):
    length_feature_array = []
    length_feature_array.append(data["response_a"].apply(str_length))
    length_feature_array.append(data["response_b"].apply(str_length))
    length_feature_array.append(length_feature_array[0] - length_feature_array[1])
    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)
    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))
    length_feature_array.append(data["response_a"].apply(has_none))
    length_feature_array.append(data["response_b"].apply(has_none))
    length_feature_array.append(data["response_a"].apply(has_none) - data["response_b"].apply(has_none))
    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)
    length_feature_array = np.transpose(length_feature_array, (1, 0))
    return length_feature_array
train_length_features = get_length_features(train)
print(train_length_features.shape)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### IFD
```

---The following area is a Code cell (cell numver is 10)---
```python
def get_ifd_features(data: pd.DataFrame):
    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)
    tokenizer = AutoTokenizer.from_pretrained(gpt_path)
    model.eval()
    def get_ppl_features(output, instruct=''):
        try:
            answer_start_index = 0
            if instruct != '':
                answer_start_index = tokenizer.encode(instruct, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).shape[1]
            input_ids = tokenizer.encode(instruct + output, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).to(device)
            labels = input_ids.clone()
            labels[:, :answer_start_index] = -100
            with torch.no_grad():
                outputs = model(input_ids, labels=labels)
            perplexity = torch.exp(outputs.loss)
            return perplexity.to('cpu').item()
        except:
            return 0
        
    ppl_ifd_feature_array = []
    for i in tqdm(range(len(data)), desc="Scoring ifd"):
        instruct = data['prompt'][i]
        output_a = data['response_a'][i]
        output_b = data['response_b'][i]
        
        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)
        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)
        try:
            ifd_a = ppl_ca_a / ppl_da_a
        except ZeroDivisionError:
            ifd_a = 0
        try:
            ifd_b = ppl_ca_b / ppl_da_b
        except ZeroDivisionError:
            ifd_b = 0
        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])
    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)
    return ppl_ifd_feature_array
ppl_ifd_features = get_ifd_features(train)
print(ppl_ifd_features.shape)
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
### Combine
```

---The following area is a Code cell (cell numver is 12)---
```python
tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2
ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7
train_length_features_csr = csr_matrix(train_length_features)*0.1
combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])
print(combined_train_vectors.shape)
print("Vectorizing test text...")
tfidf_test_vectors = get_tfidf_vectors(test)
tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2
test_ppl_ifd_features = get_ifd_features(test)
test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7
test_length_features = get_length_features(test)
test_length_features_csr = csr_matrix(test_length_features)*0.1
combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr]) 
print("Done!")
```

---The following area is a Code cell (cell numver is 13)---
```python
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss, accuracy_score
from sklearn.model_selection import train_test_split
import joblib

max_estimators = 1000
early_stopping_limit = 100

# Data preparation
X = combined_train_vectors
y_encoded = train['label'].values

# LightGBM parameters
params = {
    'n_estimators': max_estimators,
    'max_depth': 4,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'multiclass',
    'num_class': 3,
    'metric': 'multi_logloss',
    'random_state': 42,
    'learning_rate': 0.03,
    'verbose': -1  # keep logs quiet
}

# Create the model
model = lgb.LGBMClassifier(**params)

# 5-fold cross-validation
stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
logloss_scores = []
accuracy_scores = []
test_pred_list = []

for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):
    print(f"\nFold {fold + 1}")
    X_train_fold, X_val_fold = X[train_indices], X[val_indices]
    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]

    def callback(env):
        if env.iteration % 10 == 0: print ("Iteration:", env.iteration, "\tLog Loss:", env.evaluation_result_list[0][2])

    model.fit(
        X_train_fold, y_train_fold,
        eval_set=[(X_val_fold, y_val_fold)],
        eval_metric='multi_logloss',
        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]
    )

    y_pred_proba_fold = model.predict_proba(X_val_fold)
    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)
    logloss_scores.append(logloss_fold)
    print(f"Log Loss: {logloss_fold}")
    
    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)
    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)
    accuracy_scores.append(accuracy_fold)
    print(f"Accuracy: {accuracy_fold}")

    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))

# Calculate and print average scores
average_logloss = np.mean(logloss_scores)
average_accuracy = np.mean(accuracy_scores)
print(f"\nAverage Log Loss: {average_logloss}")
print(f"Average Accuracy: {average_accuracy}")
```

---The following area is a Code cell (cell numver is 14)---
```python
preds_test = np.mean(test_pred_list, axis=0)
submission = pd.DataFrame({
    'id': test["id"],
    'winner_model_a': preds_test[:, 0],
    'winner_model_b': preds_test[:, 1], 
    'winner_tie': preds_test[:, 2]
})
submission.to_csv('submission.csv', index=False)
display(submission)
```

** @@@ Jupyter Notebook numver 69, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# No Installation Required

microsoft/Phi-3-mini-4k-instruct + LoRA > GPU Parallel Training

The max sequence length has a significant impact on model performance, 
but due to insufficient memory, it was set to a maximum length of 768.


## Load Library
```

---The following area is a Code cell (cell numver is 1)---
```python
import multiprocessing as mp
mp.set_start_method('spawn')
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional,Tuple
```

---The following area is a Code cell (cell numver is 3)---
```python
# params
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"
model_path = "model_checkpoint.pth"
seed = 42
lora_r = 2
quantize_bit = 16
learning_rate = 5e-4
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.999
eps = 1e-9
l1_rate = 1e-10
batch_size = 1
max_len = 256
n_sample = 0.10
n_epoch = 2
device = "cuda"
file_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Preprocess
```

---The following area is a Code cell (cell numver is 5)---
```python
def cl(x):
  if x == [1,0,0]:
    return 0
  elif x == [0,1,0]:
    return 1
  else :
    return 2
```

---The following area is a Code cell (cell numver is 6)---
```python
def preprocess_data(file_path, sample = False):
    train = pd.read_csv(file_path)
    clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]

    clf_train.loc[:, "prompt"] = clf_train["prompt"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_a"] = clf_train["response_a"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_b"] = clf_train["response_b"].apply(lambda x: json.loads(x)[0])

    clf_train = clf_train.dropna()
    clf_train = clf_train.reset_index(drop = True)

    clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train)) ]

    clf_train = clf_train[['prompt','response_a','response_b','target']]

    clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))

    clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))
    clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))
    clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))

    clf_train['len'] = clf_train['p_len'] + clf_train['a_len']+ clf_train['b_len']
    
    if sample:
        clf_train = clf_train.sample(int(len(clf_train)*n_sample), weights = "len", random_state=seed).reset_index(drop=True)

    t_dat, v_dat = train_test_split(clf_train, test_size=0.2, random_state=42, stratify = clf_train['labels'])

    t_dat = t_dat.reset_index(drop=True)
    v_dat = v_dat.reset_index(drop=True)

    t_dat = t_dat.drop( labels= 'target' , axis = 1)
    v_dat = v_dat.drop( labels= 'target' , axis = 1)
    return t_dat, v_dat
```

---The following area is a Code cell (cell numver is 7)---
```python
class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.prompt = df['prompt']
        self.response_a = df['response_a']
        self.response_b = df['response_b']
        self.max_len = max_len
        self.targets = df.get('labels', None)

    def __len__(self):
        return len(self.prompt)

    def __getitem__(self, index):
        prompt = str(self.prompt[index])
        response_a = str(self.response_a[index])
        response_b = str(self.response_b[index])

        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])

        final_prompt_len = min(self.max_len, prompt_len)
        final_a_len = min(self.max_len, response_a_len)
        final_b_len = min(self.max_len, response_b_len)

        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True,padding='max_length', return_attention_mask=True, return_tensors='pt')

        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}
```

---The following area is a Code cell (cell numver is 8)---
```python
def custom_collate_fn(batch, tokenizer):

    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None

    # Find the maximum length of the sequences in the batch
    max_len = max([input_id.size(0) for input_id in input_ids])

    # Re-tokenize with the new max length
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        input_ids = item['input_ids'][:max_len]
        attention_mask = item['attention_mask'][:max_len]

        new_input_ids.append(input_ids)
        new_attention_masks.append(attention_mask)

    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)

    output = {
    'input_ids': new_input_ids,
    'attention_mask': new_attention_masks}

    if labels is not None:
        output['labels'] = labels

    return output
```

---The following area is a Code cell (cell numver is 9)---
```python
def create_dataloaders(df,tokenizer,max_len, batch_size, shuffle = True):
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)
    )
    return dataloader
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## Load Model
```

---The following area is a Code cell (cell numver is 11)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    qmin = 0.
    qmax = 2.**num_bits - 1.

    min_val, max_val = tensor.min(), tensor.max()
    scale = (max_val - min_val) / (qmax - qmin)
    zero_point = qmin - min_val / scale

    quantized_tensor = torch.round(tensor / scale + zero_point)
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)
    quantized_tensor = (quantized_tensor - zero_point) * scale

    return quantized_tensor

def quantize_model(model, num_bits=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)
        elif isinstance(module, nn.Conv2d):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)

    return model


# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## Add Adepter Layers
Copied from transformers.models.phi3.modeling_phi3.Phi3Attention

[github url](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py)
```

---The following area is a Code cell (cell numver is 13)---
```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout = 0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha
        self.rank = rank
        self.lora_a = nn.Linear(in_features, rank, bias=False)
        self.lora_b = nn.Linear(rank, out_features, bias=False)
        self.dropout = nn.Dropout(lora_dropout)

    def forward(self, x):
        lora_out =  self.alpha * self.lora_b(self.lora_a(x))
        lora_out = self.dropout(lora_out)
        return lora_out
```

---The following area is a Code cell (cell numver is 14)---
```python
from transformers.models.phi3.modeling_phi3 import (
Phi3RotaryEmbedding, 
# Phi3LongRoPEScaledRotaryEmbedding,
apply_rotary_pos_emb,
repeat_kv
)
```

---The following area is a Code cell (cell numver is 15)---
```python
class Phi3Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will "
                "lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )

        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta
        self.rope_scaling = config.rope_scaling
        self.is_causal = True

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )

        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)
        self._init_rope()
        
        ########################## LoRA adapters ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)
        ########################## LoRA adapters ##########################
        
    def _init_rope(self):
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"Unknown RoPE scaling type {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
#         logger.warning_once("You are not running the flash-attention implementation, expect numerical differences.")

        bsz, q_len, _ = hidden_states.size()
        ########################## LoRA adapters ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)
        ########################## LoRA adapters ##########################
        query_pos = self.num_heads * self.head_dim
        query_states = qkv[..., :query_pos]
        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]

        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights += causal_mask

        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        ########################## LoRA adapters ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)
        ########################## LoRA adapters ##########################
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value

```

---The following area is a Code cell (cell numver is 16)---
```python
def replace_attention_module(config,layer,layer_idx):
    if hasattr(layer, 'self_attn') and layer_idx > 12:

        new_attention = Phi3Attention(config,layer_idx)

        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention
```

---The following area is a Code cell (cell numver is 17)---
```python
loss_fn = nn.CrossEntropyLoss()

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # config ì¶”ê°€
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # config ì„¤ì •
        self.peft_model = lora_model
        self.dropout = nn.Dropout(0.1)
        self.classifier = nn.Linear(self.config.hidden_size, 3)
#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)
#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)
        pooled_output = outputs.last_hidden_state.mean(dim =1)
        output_dropout = self.dropout(pooled_output)
        logits = self.classifier(output_dropout)
        loss = None
        if labels is not None:
          labels = labels
          loss = loss_fn(logits, labels)
        return loss, logits
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Parallel Train
```

---The following area is a Code cell (cell numver is 19)---
```python
def parallel_function(model_name,attention_name,file_path):
    mp.set_start_method('spawn', force=True)

    accelerator = Accelerator(mixed_precision="fp16")
    if accelerator.is_main_process:
        datasets.utils.logging.set_verbosity_warning()
        transformers.utils.logging.set_verbosity_info()
    else:
        datasets.utils.logging.set_verbosity_error()
        transformers.utils.logging.set_verbosity_error()

    set_seed(seed)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    tokenizer.padding_side = "right"  # Fix weird overflow issue with fp16 training
    
    model = AutoModel.from_pretrained(model_name,torch_dtype=torch.float16)
    model = quantize_model(model)
    for idx, layer in enumerate(model.layers):
        replace_attention_module(model.config,layer,idx)
    model = LoraModelForClassification(model)

    for param in model.peft_model.parameters():
        param.requires_grad = False
    for param in model.classifier.parameters():
        param.requires_grad = True

    for name, module in model.named_modules():
        if isinstance(module, attention_name):
            module.qkv_lora.lora_a.weight.requires_grad = True
            module.qkv_lora.lora_b.weight.requires_grad = True
            module.o_lora.lora_a.weight.requires_grad = True
            module.o_lora.lora_b.weight.requires_grad = True

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad == True)
    print(f"Total trainable parameters: {total_params}")

    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    
    t_dat, v_dat = preprocess_data(file_path,sample = True)
    train_dataloader = create_dataloaders(t_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)
    eval_dataloader = create_dataloaders(v_dat,tokenizer,max_len,batch_size=batch_size, shuffle = True)

    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader)

    lr_scheduler = get_linear_schedule_with_warmup(
        optimizer=optimizer,
        num_warmup_steps=100,
        num_training_steps=len(train_dataloader) * n_epoch,
    )

    progress_bar = tqdm(range(n_epoch * len(train_dataloader)), disable=not accelerator.is_main_process)

    train_loss = 0
    valid_loss = 0
    
    for epoch in range(n_epoch):
        model.train()
        for step, batch in enumerate(train_dataloader):
            loss, _ = model(**batch)
            accelerator.backward(loss)

            optimizer.step()
            lr_scheduler.step()
            optimizer.zero_grad()
            progress_bar.update(1)

            train_loss += loss.item()
        
        all_predictions = []
        all_labels = []
        model.eval()
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                loss, logits = model(**batch)
            predictions = logits.argmax(dim=-1)
            all_predictions.append(accelerator.gather(predictions))
            all_labels.append(accelerator.gather(batch["labels"]))
            
            valid_loss += loss.item()

        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()
        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()

        acc_metric = accuracy_score(all_labels, all_predictions)
        eval_metric = f1_score(all_labels, all_predictions, average="macro")
        train_loss_avg = train_loss / len(train_dataloader)
        valid_loss_avg = valid_loss / len(eval_dataloader)
        
        accelerator.print(f"epoch: {epoch} \n accuracy: {acc_metric:.3f} \n f1 score: {eval_metric:.3f} \n train loss: {train_loss_avg:.3f} \n valid loss: {valid_loss_avg:.3f}")

    model = accelerator.unwrap_model(model)
    accelerator.wait_for_everyone()

    # ëª¨ë¸ ìƒíƒœ ì‚¬ì „ ì €ìž¥
    if accelerator.is_main_process:
        torch.save(model.state_dict(), model_path)

    # ë™ê¸°í™” ì™„ë£Œ ë©”ì‹œì§€
    accelerator.wait_for_everyone()
```

---The following area is a Code cell (cell numver is 20)---
```python
notebook_launcher(parallel_function, args=(model_name,Phi3Attention,file_path,), num_processes=2)
```

** @@@ Jupyter Notebook numver 70, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Create a new DataFrame with the same 'id' column as 'test'
sample_submission = pd.DataFrame({'id': test['id']})

# Generate random floats for each column
n_rows = len(test)
random_values = np.random.rand(n_rows, 3)

# Normalize the random values so they sum to 1 for each row
normalized_values = random_values / random_values.sum(axis=1)[:, np.newaxis]

# Add the normalized random values to the DataFrame
sample_submission['winner_model_a'] = normalized_values[:, 0]
sample_submission['winner_model_b'] = normalized_values[:, 1]
sample_submission['winner_tie'] = normalized_values[:, 2]

# Verify that the sum of winner columns is very close to 1 for each row
# (using np.isclose due to potential floating-point precision issues)
assert np.allclose(sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1), 1)

# Display the first few rows of the new DataFrame
print(sample_submission.head())
```

---The following area is a Code cell (cell numver is 1)---
```python
sample_submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 71, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Note
- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

# Import
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers -U --no-index --find-links /kaggle/input/lmsys-transformers/lmsys_transformers
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
import json
import time
import sklearn
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
import warnings
warnings.filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 4)---
```python
# /kaggle/input/llama-3-8b-instruct-bnb-4bit/llama-3-8b-Instruct-bnb-4bit/config.json
MODEL_NAME = '/kaggle/input/meta-llama-3-8b/LLM-Research/Meta-Llama-3-8B'
WEIGHTS_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/checkpoint-550'
TOKENIZER_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/tokenizer'
MAX_LENGTH = 2400
BATCH_SIZE = 4
DEVICE = torch.device("cuda")    
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 6)---
```python
def process_text(text: str) -> list:
    x = json.loads(text)
    x = ['none' if pd.isna(i) else i for i in x]
    return x


def merge_text(x):
    prompt = x['prompt']
    response_a = x['response_a']
    response_b = x['response_b']
    res = ''
    for i in range(len(prompt)):
        if i == len(prompt) - 1:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}'
        else:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}' + '\n\n'
    return res
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

if test.shape[0] == 3:
    test = train.head(100)


test['prompt'] = test['prompt'].map(lambda x: process_text(x))
test['response_a'] = test['response_a'].map(lambda x: process_text(x))
test['response_b'] = test['response_b'].map(lambda x: process_text(x))

test['text'] = test.apply(lambda x: merge_text(x), axis=1)
test.head()
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(tokenizer, x):
    tokenized = tokenizer(x, max_length=MAX_LENGTH, truncation=True)
    return tokenized['input_ids'], tokenized['attention_mask']
```

---The following area is a Code cell (cell numver is 10)---
```python
llama31_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

llama31_data = pd.DataFrame()
llama31_data["id"] = test["id"]
llama31_data["input_ids"], llama31_data["attention_mask"] = tokenize(llama31_tokenizer, list(test['text']))
llama31_data["length"] = llama31_data["input_ids"].apply(len)
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 12)---
```python
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

device_0 = torch.device('cuda:0')
llama31_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    use_cache=False,
    device_map=device_0
)
llama31_model_0.config.pad_token_id = llama31_tokenizer.pad_token_id

device_1 = torch.device('cuda:1')
llama31_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    use_cache=False,
    device_map=device_1
)
llama31_model_1.config.pad_token_id = llama31_tokenizer.pad_token_id


lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    # only target self-attention
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
    # layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=0,
    bias='none',
    task_type=TaskType.SEQ_CLS,
)

# llama31_model_0 = prepare_model_for_kbit_training(llama31_model_0)
# llama31_model_0 = get_peft_model(llama31_model_0, lora_config)
# llama31_model_0.print_trainable_parameters()

# llama31_model_1 = prepare_model_for_kbit_training(llama31_model_1)
# llama31_model_1 = get_peft_model(llama31_model_1, lora_config)
# llama31_model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 14)---
```python
# Get peft
llama31_model_0 = PeftModel.from_pretrained(llama31_model_0, model_id=WEIGHTS_PATH).to(device_0) 
llama31_model_0.eval()

llama31_model_1 = PeftModel.from_pretrained(llama31_model_1, model_id=WEIGHTS_PATH).to(device_1)
llama31_model_1.eval();
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 16)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, tokenizer, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 17)---
```python
st = time.time()

llama31_data = llama31_data.sort_values("length", ascending=False).reset_index(drop=True)
sub_1 = llama31_data.iloc[0::2].copy()
sub_2 = llama31_data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (llama31_model_0, llama31_model_1), (llama31_tokenizer, llama31_tokenizer), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 18)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 72, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.

## ðŸ†• Update in version 4
The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.
In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.
This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.
We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:
1. Flash Attention
2. Memory Efficient Attention
3. A PyTorch (naive) implementation

By default, all of those are enabled but we can also manually enable/disable certain backends.
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 73, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
from tqdm import tqdm
import pandas as pd
import json
import torch
from transformers import AutoModel
from numpy.linalg import norm
import torch.nn as nn
import numpy as np
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
import re
```

---The following area is a Code cell (cell numver is 1)---
```python
device = "cuda" if torch.cuda.is_available else "cpu"
device
```

---The following area is a Code cell (cell numver is 2)---
```python
tokenizer = AutoModel.from_pretrained('/kaggle/input/jinaai/pytorch/default/4')
```

---The following area is a Code cell (cell numver is 3)---
```python
class EmbeddingModel(nn.Module):
    def __init__(self, embedding_model, max_sequences):
        super(EmbeddingModel, self).__init__()
        self.embedding = embedding_model
        self.max_seq_length = max_sequences
        self.device = device

    def forward(self, prompts, responses_a, responses_b):
        batch_features_a = []
        batch_features_b = []

        for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):
            prompt = json.loads(prompt)
            response_a = json.loads(response_a)
            response_b = json.loads(response_b)
            
            prompt = ["" if p is None else p for p in prompt]
            response_a = ["" if r is None else r for r in response_a]
            response_b = ["" if r is None else r for r in response_b]
            
            
            embedded_prompt = torch.from_numpy(self.embedding.encode(prompt)).to(self.device)
           
            embedded_response_a = torch.from_numpy(self.embedding.encode(response_a)).to(self.device)
            embedded_response_b = torch.from_numpy(self.embedding.encode(response_b)).to(self.device)

            features_a = []
            features_b = []
            for i in range(len(embedded_prompt)):
                combined_a = torch.cat((embedded_prompt[i], embedded_response_a[i]), dim=0)
                combined_b = torch.cat((embedded_prompt[i], embedded_response_b[i]), dim=0)

                features_a.append(combined_a)
                features_b.append(combined_b)

            features_a = torch.stack(features_a) if features_a else torch.tensor([]).to(self.device)
            features_b = torch.stack(features_b) if features_b else torch.tensor([]).to(self.device)

            features_a = self.pad_to_shape(features_a, (self.max_seq_length, 768 * 2))
            features_b = self.pad_to_shape(features_b, (self.max_seq_length, 768 * 2))

            batch_features_a.append(features_a)
            batch_features_b.append(features_b)

        return torch.stack(batch_features_a).to(self.device), torch.stack(batch_features_b).to(self.device)

    def pad_to_shape(self, tensor, shape):
        current_shape = tensor.shape
        padding = [(0, max(s - cs, 0)) for cs, s in zip(current_shape, shape)]
        padded_tensor = F.pad(tensor, pad=[p for pair in reversed(padding) for p in pair], mode='constant', value=0)
        return padded_tensor[:shape[0], :shape[1]]

class Model(nn.Module):
    def __init__(self, embedding_model, max_sequences=64, hidden_dim=512, dropout=0.3):
        super(Model, self).__init__()
        self.device = device
        self.embedding = EmbeddingModel(embedding_model, max_sequences)
        self.lstm_input_a = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)
        self.lstm_input_b = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)

        self.conv_input_a = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)
        self.conv_input_b = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)

        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2 + hidden_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3),
            nn.Softmax()
        ).to(self.device)

    def forward(self, prompts, responses_a, responses_b):
        batch_a, batch_b = self.embedding(prompts, responses_a, responses_b)

        batch_a_lstm, _ = self.lstm_input_a(batch_a)  # (batch, 64, hidden_dim)
        batch_b_lstm, _ = self.lstm_input_b(batch_b)  # (batch, 64, hidden_dim)

        batch_a_cnn = self.conv_input_a(batch_a.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)
        batch_b_cnn = self.conv_input_b(batch_b.permute(0, 2, 1)).permute(0, 2, 1)  # (batch, 64, hidden_dim)

        batch_a_lstm = batch_a_lstm[:, -1, :] 
        batch_b_lstm = batch_b_lstm[:, -1, :]  
        batch_a_cnn = batch_a_cnn[:, -1, :]    
        batch_b_cnn = batch_b_cnn[:, -1, :]
        
        combined = torch.cat([batch_a_lstm, batch_a_cnn, batch_b_lstm, batch_b_cnn], dim=1)
        flattened = combined.view(combined.size(0), -1)

        output = self.fc(flattened)
        return output
    
class DatasetLMSYS(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]
        prompt = sample['prompt']
        response_a = sample['response_a']
        response_b = sample['response_b']
        label = sample['model_result']
        return prompt, response_a, response_b, label
    
class DatasetLMSYSTest(Dataset):
    def __init__(self, data):
        self.data = data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]
        _id = sample['id']
        _prompt = sample['prompt']
        _response_a = sample['response_a']
        _response_b = sample['response_b']
        return _id, _prompt, _response_a, _response_b
```

---The following area is a Code cell (cell numver is 4)---
```python
model = Model(tokenizer).to(device)
```

---The following area is a Code cell (cell numver is 5)---
```python
batch_size = 128
learning_rate = 0.001
epochs = 5
```

---The following area is a Code cell (cell numver is 6)---
```python
file_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
file_data['model_result'] = file_data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)
file_data = file_data[['prompt', 'response_a', 'response_b', 'model_result']]
train_loader = DataLoader(
    dataset=DatasetLMSYS(file_data),
    batch_size=batch_size,
    shuffle=True
)

file_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
test_loader = DataLoader(
    dataset=DatasetLMSYSTest(file_test),
    batch_size=batch_size,
    shuffle=False
)
```

---The following area is a Code cell (cell numver is 7)---
```python
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

---The following area is a Code cell (cell numver is 8)---
```python
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")
    running_loss = 0
    total_train = 0
    correct_train = 0
    # Train
    model.train()
    for batch in tqdm(train_loader):
        prompts, responses_a, responses_b, labels = batch
        labels = labels.to(device)

        outputs = model(prompts, responses_a, responses_b)
        _, predicted_idx = torch.max(outputs.data, 1)

        loss = criterion(outputs, labels)
        optimizer.zero_grad()

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        total_train += labels.size(0)
        correct_train += (predicted_idx == labels).sum().item()

        del labels, outputs
        
    train_accuracy = 100 * correct_train / total_train
    print(f"\nTraining Loss: {running_loss/len(train_loader):.4f} | Training Accuracy: {train_accuracy:.2f}%")
print("\n==> Training finished!")
```

---The following area is a Code cell (cell numver is 9)---
```python
def test(model, test_loader, device):
    model.eval() 
    results = [] 
    with torch.no_grad(): 
        for batch in tqdm(test_loader):
            ids, prompts, responses_a, responses_b = batch
            outputs = model(prompts, responses_a, responses_b)
            _, predicted_idx = torch.max(outputs.data, 1)
            
            for idx, output, prediction in zip(ids, outputs, predicted_idx):
                results.append({
                    'id': idx.item(),
                    'winner_model_a': output[0].item(),
                    'winner_model_b': output[1].item(),
                    'winner_tie': output[2].item()
                })
    df_results = pd.DataFrame(results)
    return df_results

df_results = test(model, test_loader, device)
df_results
```

** @@@ Jupyter Notebook numver 74, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# LMSYS Keras Gemma 2B
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U keras-nlp tensorflow-text
# Install tensorflow-cpu so tensorflow does not attempt to access the TPU.
!pip install -q -U tensorflow-cpu
```

---The following area is a Code cell (cell numver is 2)---
```python
import jax

jax.devices()
```

---The following area is a Code cell (cell numver is 3)---
```python
import os

# The Keras 3 distribution API is only implemented for the JAX backend for now
os.environ["KERAS_BACKEND"] = "jax"
# Pre-allocate all TPU memory to minimize memory fragmentation and allocation overhead.
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
```

---The following area is a Code cell (cell numver is 4)---
```python
import keras
import keras_nlp
```

---The following area is a Code cell (cell numver is 5)---
```python
# Create a device mesh with (1, 8) shape so that the weights are sharded across
# all 8 TPUs.
device_mesh = keras.distribution.DeviceMesh(
    (8, 1),
    ["batch", "model"],
    devices=keras.distribution.list_devices(),
)
```

---The following area is a Code cell (cell numver is 6)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# Weights that match 'token_embedding/embeddings' will be sharded on 8 TPUs
layout_map["token_embedding/embeddings"] = (model_dim, None)
# Regex to match against the query, key and value matrices in attention layers
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
```

---The following area is a Code cell (cell numver is 7)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))

```

---The following area is a Code cell (cell numver is 8)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0] if 'null' not in x else None)

raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)


train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].apply(lambda x: '<start_of_turn>user\nFind which one is the best answer for the question:\n'+x['prompt']+'\n\nA:\n'+x['response_a']+'\n\nB\n:'+x['response_b']+'\n\nC:\n both right (or) both wrong<end_of_turn>\n<start_of_turn>model\n', axis=1).apply(lambda x: remove_surrogates(x)),
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
#         'label' : raw_train_dataset[label_columns].apply(lambda x: 'A' if x.values.tolist()[0] == 1 else 'B' if x.values.tolist()[1] == 1 else 'C', axis=1)
})

train_dataset = train_dataset[:4000]
raw_train_dataset = raw_train_dataset[:4000]
```

---The following area is a Code cell (cell numver is 9)---
```python
len(train_dataset)
```

---The following area is a Code cell (cell numver is 10)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

keras.distribution.set_distribution(model_parallel)
```

---The following area is a Code cell (cell numver is 11)---
```python
keras.config.set_floatx("float16")

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma/keras/gemma_instruct_2b_en/2")
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 12)---
```python
gemma_lm.backbone.enable_lora(rank=8)

```

---The following area is a Code cell (cell numver is 13)---
```python
# for layer in gemma_lm.backbone.layers[:16]:
#     layer.trainable = False
```

---The following area is a Code cell (cell numver is 14)---
```python
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 15)---
```python
def preprocess_fn(text, label=None):
    preprocessed = gemma_lm._preprocessor(text, sequence_length=3072)[0]
    # Ensure the preprocess function returns only the necessary inputs
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}, label if label is not None else {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}
```

---The following area is a Code cell (cell numver is 16)---
```python
gemma_lm.layers[-1]
```

---The following area is a Code cell (cell numver is 17)---
```python
import gc
del gemma_lm.layers[-1]

gc.collect()
```

---The following area is a Code cell (cell numver is 18)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

inputs = {
    "token_ids": keras.Input(shape=(3072,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(3072,), dtype=tf.int32, name="padding_mask"),
}
x = gemma_lm.backbone(inputs)
print(x.shape)
x = GlobalAveragePooling1D()(x)
print(x.shape)

outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 19)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,
                weight_decay=0.01,)
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])

```

---The following area is a Code cell (cell numver is 20)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 21)---
```python
import tensorflow as tf


ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values)).batch(8).map(preprocess_fn)
ds = ds.shuffle(ds.cardinality())

```

---The following area is a Code cell (cell numver is 22)---
```python
train_split = ds.take(int(len(ds)*0.9))
val_split = ds.skip(int(len(ds)*0.9)).take(int(len(ds)*0.1))
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=8)
```

---The following area is a Code cell (cell numver is 23)---
```python
model.get_layer("gemma_backbone").save_lora_weights('/kaggle/working/lora19.lora.h5')
```

** @@@ Jupyter Notebook numver 75, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
#pip install torch torchvision torchaudio
```

---The following area is a Code cell (cell numver is 2)---
```python
#pip install pandas numpy scikit-learn tensorflow
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# Load the dataset
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# Combine prompts and responses for feature extraction
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']

# Vectorize the text data using TF-IDF
tfidf = TfidfVectorizer(max_features=5000)
X_a = tfidf.fit_transform(train_data['text_a']).toarray()
X_b = tfidf.transform(train_data['text_b']).toarray()

# Combine the vectorized responses
X = np.hstack([X_a, X_b])

# Target variable
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values

# Split the data into training and validation sets
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_val = scaler.transform(X_val)

# Build a simple neural network model
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)),
    Dropout(0.5),
    Dense(256, activation='relu'),
    Dropout(0.5),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(3, activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))

# Evaluate the model
val_predictions = model.predict(X_val)
loss = log_loss(y_val, val_predictions)
print(f'Validation Log Loss: {loss}')

# Prepare the test data
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b']
X_test_a = tfidf.transform(test_data['text_a']).toarray()
X_test_b = tfidf.transform(test_data['text_b']).toarray()
X_test = np.hstack([X_test_a, X_test_b])
X_test = scaler.transform(X_test)

# Predict on the test set
test_predictions = model.predict(X_test)

# Create the submission file
submission = pd.DataFrame(test_data['id'])
submission['winner_model_a'] = test_predictions[:, 0]
submission['winner_model_b'] = test_predictions[:, 1]
submission['winner_tie'] = test_predictions[:, 2]
submission.to_csv('submission.csv', index=False)

```

** @@@ Jupyter Notebook numver 76, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel
from transformers import LlamaForCausalLM, LlamaTokenizer
import torch.nn.functional as F
np.random.seed(1337)

```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
model = "/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf"

tokenizer = AutoTokenizer.from_pretrained(model)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## Prepare train set
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train.head(5)
```

---The following area is a Code cell (cell numver is 7)---
```python
def put_text(train):
    train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A:\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
    return train

train = put_text(train)
print(train['text'][0])
```

---The following area is a Code cell (cell numver is 8)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)


train = put_text(train)
print(train['text'][0])
```

---The following area is a Code cell (cell numver is 9)---
```python
train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))

```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 14300
max_len = 1024
maxlen = max_len
batch_size = 16
embedding_dims = 100
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 100
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
num_samples = INPUT_IDS.shape[0]

# Sá»‘ lÆ°á»£ng máº«u cho X_valid (20% cá»§a X_train)
num_valid_samples = int(num_samples * 0.2)

# XÃ¡o trá»™n cÃ¡c chá»‰ sá»‘ cá»§a X_train
indices = np.random.permutation(num_samples)

# Chá»n 20% chá»‰ sá»‘ Ä‘áº§u tiÃªn lÃ m chá»‰ sá»‘ cho X_valid
valid_indices = indices[:num_valid_samples]

# CÃ¡c chá»‰ sá»‘ cÃ²n láº¡i lÃ m chá»‰ sá»‘ cho X_train
train_indices = indices[num_valid_samples:]

# Táº¡o X_valid vÃ  X_train má»›i tá»« cÃ¡c chá»‰ sá»‘ Ä‘Ã£ chá»n
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=max_len)
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=max_len)
y_train = LABELS[train_indices]

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=max_len)
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=max_len)
y_valid = LABELS[valid_indices]
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
X_valid = np.array(X_valid)
y_valid = np.array(y_valid)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
from tensorflow.keras.layers import Layer
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D
from keras.layers import  GlobalMaxPooling1D
import tensorflow as tf

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        embeddings, attention_mask = inputs
        return embeddings * tf.expand_dims(attention_mask, -1)

input_layer = Input(shape=(max_len,),dtype='int32', name='main_input')
attention_masks = Input(shape=(max_len,), dtype='float32', name="attention_masks")

emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=max_len
                      )(input_layer)

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])

# LSTM branch (with Batch Normalization and Dropout)
lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)
lstm_out = BatchNormalization()(lstm_out) # Batch Normalization helps to normalize activations and speed up convergence
lstm_out = Dropout(0.5)(lstm_out) # Dropout = 0.5 helps to prevent overfitting
lstm_out = LSTM(64, return_sequences=True)(lstm_out)
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)
lstm_out = LSTM(32)(lstm_out)
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)

# CNN layer branch (with Batch Normalization and Dropout)
cnn_out = Conv1D(64, 5, activation='relu')(masked_embeddings)
cnn_out = BatchNormalization()(cnn_out)
cnn_out = Dropout(0.5)(cnn_out)
cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)
cnn_out = BatchNormalization()(cnn_out)
cnn_out = Dropout(0.5)(cnn_out)
cnn_out = GlobalMaxPooling1D()(cnn_out)


# Concatenate LSTM and CNN outputs
merged = concatenate([lstm_out, cnn_out])
merged = Dense(32, activation='sigmoid')(merged)
merged = BatchNormalization()(merged)
merged = Dropout(0.5)(merged)
pred = Dense(3, activation='softmax')(merged)


# Build model
model = Model(inputs=[input_layer, attention_masks], outputs=[pred])
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)
adamax = Adamax(learning_rate=0.001)
model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## Training
```

---The following area is a Code cell (cell numver is 21)---
```python
def clip_indices(data, max_index):
    return np.where(data >= max_index, max_index - 1, data)
X_train = clip_indices(X_train, 14300)
X_train_attention = clip_indices(X_train_attention, 14300)
X_valid = clip_indices(X_valid, 14300)
X_valid_attention = clip_indices(X_valid_attention, 14300)
```

---The following area is a Code cell (cell numver is 22)---
```python
from keras.callbacks import EarlyStopping
checkpoint = ModelCheckpoint('/kaggle/working/model.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)

model.fit([X_train,X_train_attention], y_train,
          batch_size=16,
          epochs=nb_epoch,
#           callbacks=[checkpoint, early_stopping],
          callbacks=[early_stopping],
          validation_data=([X_valid,X_valid_attention], y_valid))
```

---The following area is a Code cell (cell numver is 23)---
```python
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])

model.save('model.keras', overwrite=True)
model.save_weights("model.weights.h5", overwrite=True)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## Test Model
```

---The following area is a Code cell (cell numver is 25)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 26)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 27)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A:\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 28)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 29)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=max_len)
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=max_len)

```

---The following area is a Code cell (cell numver is 30)---
```python
test
```

---The following area is a Code cell (cell numver is 31)---
```python
y_predict = model.predict([X_test,X_test_attention])
y_predict
```

---The following area is a Code cell (cell numver is 32)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)

```

---The following area is a Code cell (cell numver is 33)---
```python
result_df.to_csv('submission.csv', index=False)
result_df
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
____________________
```

---The following area is a Code cell (cell numver is 35)---
```python
# # Import necessary libraries
# import pandas as pd
# import numpy as np
# from sklearn.model_selection import train_test_split
# from sklearn.feature_extraction.text import TfidfVectorizer
# from sklearn.ensemble import RandomForestClassifier
# from sklearn.metrics import log_loss
# from sklearn.preprocessing import LabelEncoder

# # Load the data
# train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
# test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# # Inspect the data
# print(train.head())
# print(test.head())

# # Data Preprocessing
# # Combine responses into one text feature
# train['response_combined'] = train['response_a'] + ' ' + train['response_b']
# test['response_combined'] = test['response_a'] + ' ' + test['response_b']

# # Encode the target labels
# label_encoder = LabelEncoder()
# train['winner'] = label_encoder.fit_transform(train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1))

# # Feature Engineering
# # Vectorize the combined responses using TF-IDF
# tfidf = TfidfVectorizer(max_features=1000)
# X_train_tfidf = tfidf.fit_transform(train['response_combined'])
# X_test_tfidf = tfidf.transform(test['response_combined'])

# # Prepare the data for model training
# X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, train['winner'], test_size=0.2, random_state=42)

# # Train the model
# model = RandomForestClassifier(n_estimators=100, random_state=42)
# model.fit(X_train, y_train)

# # Validate the model
# y_val_pred_proba = model.predict_proba(X_val)
# val_log_loss = log_loss(y_val, y_val_pred_proba)
# print(f'Validation Log Loss: {val_log_loss}')

# # Predict on the test set
# test_pred_proba = model.predict_proba(X_test_tfidf)

# # Prepare the submission file
# submission = pd.DataFrame(test['id'], columns=['id'])
# submission['winner_model_a'] = test_pred_proba[:, label_encoder.transform(['winner_model_a'])]
# submission['winner_model_b'] = test_pred_proba[:, label_encoder.transform(['winner_model_b'])]
# submission['winner_tie'] = test_pred_proba[:, label_encoder.transform(['winner_tie'])]
# submission.to_csv('submission.csv', index=False)

# # Inspect the submission file
# print(submission.head())
```

** @@@ Jupyter Notebook numver 77, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Result
- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| subset | log loss |
| - | - |
| Eval | 0.9231|
| LB | 0.936 |

## Note
If you want to reproduce the code, please note the following:
- use all data
- set per_device_train_batch_size=4
- 1 epoch using A10 took ~15h
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 4)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### Tokenizer and prepare dataset, metrics
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }

```

---The following area is a Code cell (cell numver is 7)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred
    preds = logits.argmax(axis=-1)
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])
    acc = accuracy_score(labels, preds)
    probs = softmax(logits, axis=-1)
    log_loss_ = log_loss(labels, probs)
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### Model
```

---The following area is a Code cell (cell numver is 9)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)
        self.vocab_size = config.vocab_size
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)
        logits = logits.float()

        loss = None
        if labels is not None:
            # Shift so that tokens < n predict n
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # Flatten the tokens
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # Enable model parallelism
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 10)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 11)---
```python
model = Llama3ForSFT.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, peft_config)
print(model)
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 13)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,
    evaluation_strategy = "epoch",
    save_strategy = "steps",
    save_steps=200,
    save_total_limit=1,
    logging_strategy="steps",
    logging_steps=10,
    warmup_steps=20,
    optim="adamw_8bit",
    learning_rate=2e-4,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=4,
    gradient_accumulation_steps=2,
    num_train_epochs=1,
    fp16=True,
    metric_for_best_model="log_loss",
    greater_is_better = False,
    report_to="none",
)

```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Training !
```

---The following area is a Code cell (cell numver is 15)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)
trainer.train()
```

---The following area is a Code cell (cell numver is 16)---
```python
model.save_pretrained('pretrained_model')
```

---The following area is a Code cell (cell numver is 17)---
```python
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from peft import LoraConfig, get_peft_model, TaskType

# # ÄÆ°á»ng dáº«n tá»›i mÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c vÃ  file Lora adapter
# model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
# lora_adapter_path = "/kaggle/input/model-1"

# # Táº£i mÃ´ hÃ¬nh gá»‘c
# model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# # Táº£i tokenizer tÆ°Æ¡ng á»©ng
# tokenizer = AutoTokenizer.from_pretrained(model_path)

# # Cáº¥u hÃ¬nh Lora
# lora_config = LoraConfig(
#     r=8,            # Rank cá»§a Lora
#     lora_alpha=16,  # Há»‡ sá»‘ khuáº¿ch Ä‘áº¡i cá»§a Lora
#     task_type=TaskType.CAUSAL_LM  # Loáº¡i nhiá»‡m vá»¥ cá»§a mÃ´ hÃ¬nh
# )

# # Chuáº©n bá»‹ mÃ´ hÃ¬nh cho k-bit training náº¿u cáº§n
# model_1 = prepare_model_for_kbit_training(model_1)

# # Ãp dá»¥ng Lora Adapter vÃ o mÃ´ hÃ¬nh
# model_1 = get_peft_model(model_1, lora_config)

# # Táº£i cÃ¡c tham sá»‘ cá»§a Lora Adapter Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
# model_1.load_adapter(lora_adapter_path, adapter_name="test")

# # MÃ´ hÃ¬nh hoÃ n chá»‰nh Ä‘Ã£ sáºµn sÃ ng sá»­ dá»¥ng
# model_1.eval()  # Äáº·t mÃ´ hÃ¬nh vÃ o cháº¿ Ä‘á»™ Ä‘Ã¡nh giÃ¡ náº¿u cáº§n

# # Tokenize má»™t cÃ¢u vÃ­ dá»¥ vÃ  sá»­ dá»¥ng mÃ´ hÃ¬nh Ä‘á»ƒ táº¡o vÄƒn báº£n
# sentence = "Hello, how are you?"
# inputs = tokenizer(sentence, return_tensors="pt")
# outputs = model_1.generate(**inputs)

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---The following area is a Code cell (cell numver is 18)---
```python
# !zip -r model_2.zip /kaggle/working/saved_model_2
```

** @@@ Jupyter Notebook numver 78, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, classification_report
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.utils import to_categorical
```

---The following area is a Code cell (cell numver is 1)---
```python
# Load the data
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
# Check the column names
print("Train Data Columns:", train_df.columns)
print("\nTest Data Columns:", test_df.columns)
```

---The following area is a Code cell (cell numver is 3)---
```python
# Use a smaller subset of data
train_df = train_df.head(2000)
test_df = test_df.head(2000)
```

---The following area is a Code cell (cell numver is 4)---
```python
# Combine the winner columns into a single target column
train_df['winner'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 5)---
```python
# Combine all text data for tokenization
all_text = pd.concat([train_df['prompt'], train_df['response_a'], train_df['response_b'],
                      test_df['prompt'], test_df['response_a'], test_df['response_b']])
```

---The following area is a Code cell (cell numver is 6)---
```python
# Tokenize the text data
tokenizer = Tokenizer()
tokenizer.fit_on_texts(all_text)
vocab_size = len(tokenizer.word_index) + 1
```

---The following area is a Code cell (cell numver is 7)---
```python
def tokenize_and_pad(text_series, tokenizer, max_len):
    sequences = tokenizer.texts_to_sequences(text_series)
    padded_sequences = pad_sequences(sequences,maxlen=max_len)
    return padded_sequences
```

---The following area is a Code cell (cell numver is 8)---
```python
# Define maximum sequence length
max_len = 100
```

---The following area is a Code cell (cell numver is 9)---
```python
# Tokenize and pad the training data
x_prompt = tokenize_and_pad(train_df['prompt'],tokenizer, max_len)
x_response_a = tokenize_and_pad(train_df['response_a'],tokenizer, max_len)
x_response_b = tokenize_and_pad(train_df['response_b'],tokenizer, max_len)
x_train = np.concatenate([x_prompt,x_response_a,x_response_b],axis=1)
```

---The following area is a Code cell (cell numver is 10)---
```python
# Encode the target variable
y_train = to_categorical(train_df['winner'])
```

---The following area is a Code cell (cell numver is 11)---
```python
# Split the data into training and validation sets
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)
```

---The following area is a Code cell (cell numver is 12)---
```python
# Build the Keras model
model = Sequential()
model.add(Embedding(vocab_size,128))
model.add(LSTM(64,return_sequences=True))
model.add(Dropout(0.5))
model.add(LSTM(64))
model.add(Dense(3,activation='softmax'))
```

---The following area is a Code cell (cell numver is 13)---
```python
# Compile the model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
```

---The following area is a Code cell (cell numver is 14)---
```python
# Train the model
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))
```

---The following area is a Code cell (cell numver is 15)---
```python
# Create a figure and a set of subplots
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))

# Plot training & validation loss values
ax1.plot(history.history['loss'], label='Train')
ax1.plot(history.history['val_loss'], label='Validation')
ax1.set_title('Model Loss')
ax1.set_xlabel('Epoch')
ax1.set_ylabel('Loss')
ax1.legend(loc='upper right')

# Plot training & validation accuracy values
ax2.plot(history.history['accuracy'], label='Train')
ax2.plot(history.history['val_accuracy'], label='Validation')
ax2.set_title('Model Accuracy')
ax2.set_xlabel('Epoch')
ax2.set_ylabel('Accuracy')
ax2.legend(loc='upper left')

# Adjust layout to prevent overlap
plt.tight_layout()
plt.show()
```

---The following area is a Code cell (cell numver is 16)---
```python
# Prepare the test data
x_test_prompt = tokenize_and_pad(test_df['prompt'],tokenizer, max_len)
x_test_response_a = tokenize_and_pad(test_df['response_a'],tokenizer, max_len)
x_test_response_b = tokenize_and_pad(test_df['response_b'],tokenizer, max_len)
x_test = np.concatenate([x_test_prompt,x_test_response_a,x_test_response_b],axis=1)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Predict on the test data
y_test_pred = model.predict(x_test)
y_test_pred_labels = np.argmax(y_test_pred,axis=1)
```

---The following area is a Code cell (cell numver is 18)---
```python
# Convert predictions to binary columns
test_df['winner_model_a'] = (y_test_pred_labels == 0).astype(float)
test_df['winner_model_b'] = (y_test_pred_labels == 1).astype(float)
test_df['winner_tie'] = (y_test_pred_labels == 2).astype(float)
```

---The following area is a Code cell (cell numver is 19)---
```python
# Save the predictions to submission.csv
submission_df = test_df[['id','winner_model_a','winner_model_b','winner_tie']]
submission_df.to_csv('submission.csv',index=False)
```

** @@@ Jupyter Notebook numver 79, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
data=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# Load data

# Preprocess data
# Create labels
data['winner'] = data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)

# Encode categorical features
label_encoder = LabelEncoder()
data['model_a'] = label_encoder.fit_transform(data['model_a'])
data['model_b'] = label_encoder.fit_transform(data['model_b'])

# Feature and target split
X = data[['model_a', 'model_b', 'prompt', 'response_a', 'response_b']]
y = data['winner']

# Text vectorization (simple approach: count the number of words)
X['prompt_len'] = X['prompt'].apply(lambda x: len(str(x).split()))
X['response_a_len'] = X['response_a'].apply(lambda x: len(str(x).split()))
X['response_b_len'] = X['response_b'].apply(lambda x: len(str(x).split()))
X = X.drop(['prompt', 'response_a', 'response_b'], axis=1)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# One-hot encode labels
y = to_categorical(y, num_classes=3)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Build the model
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.1))
model.add(Dense(16, activation='relu'))
model.add(Dense(3, activation='softmax'))

# Compile the model
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# Define early stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the model
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])

# Evaluate the model
loss, accuracy = model.evaluate(X_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# Save the model
model.save('chatbot_preference_model.h5')

# Print validation loss during training
val_loss = history.history['val_loss']
print(f"Validation Loss: {val_loss[-1]:.4f}")

```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder, StandardScaler

# Load the test data
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

# Add default values for 'model_a' and 'model_b' as placeholders
test['model_a'] = 'gpt-3.5-turbo-0613'  # Example default value
test['model_b'] = 'gpt-3.5-turbo-0613'  # Example default value

# Encode categorical features using the same label encoder used for training data
label_encoder = LabelEncoder()
test['model_a'] = label_encoder.fit_transform(test['model_a'])
test['model_b'] = label_encoder.fit_transform(test['model_b'])

# Text vectorization (simple approach: count the number of words)
test['prompt_len'] = test['prompt'].apply(lambda x: len(str(x).split()))
test['response_a_len'] = test['response_a'].apply(lambda x: len(str(x).split()))
test['response_b_len'] = test['response_b'].apply(lambda x: len(str(x).split()))

# Drop original text columns
X_test = test.drop(['prompt', 'response_a', 'response_b'], axis=1)

# Normalize features using the same scaler used for training data
# Assuming the scaler was fit during training and is now loaded
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test[['model_a', 'model_b', 'prompt_len', 'response_a_len', 'response_b_len']])

# Load the trained model
model = load_model('chatbot_preference_model.h5')

# Make predictions
probs = model.predict(X_test_scaled)

# Prepare the submission file
submission = pd.DataFrame(probs, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission['id'] = test['id']

# Reorder columns to have 'id' first
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

submission.to_csv('submission.csv', index=False)
print("Submission file generated successfully!")

```

---The following area is a Code cell (cell numver is 4)---
```python
print(submission)
```

** @@@ Jupyter Notebook numver 80, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/libs-install
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from catboost import CatBoostClassifier, Pool
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, log_loss

from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Code cell (cell numver is 2)---
```python
train_df = pd.read_csv('/kaggle/input/embedding/train_embed.csv')
train_embed = np.load('/kaggle/input/embedding/gemma2_train_embed.npy')

train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)
```

---The following area is a Code cell (cell numver is 3)---
```python
# splits
Targets = ['winner_model_a','winner_model_b','winner_tie']

y = train_df['label'].values
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values

print(X_train.shape, y_train.shape)
print(X_test.shape, y_test.shape)
```

---The following area is a Code cell (cell numver is 4)---
```python
model_cb = CatBoostClassifier()
model_cb.load_model('/kaggle/input/catboost-mike/catboost.cbm')
```

---The following area is a Code cell (cell numver is 5)---
```python
model_cb
```

---The following area is a Code cell (cell numver is 6)---
```python
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
MAX_LENGTH = 1024
BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')

tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)
```

---The following area is a Code cell (cell numver is 7)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'
print(test['text'][0])
```

---The following area is a Code cell (cell numver is 8)---
```python
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',
                   max_length=MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')


data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in  tokens['attention_mask']]
data[:2]
```

---The following area is a Code cell (cell numver is 9)---
```python
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 10)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 11)---
```python
test_embeddings = torch.cat([results[0], results[1]], dim=0)
test_embeddings.shape
```

---The following area is a Code cell (cell numver is 12)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Code cell (cell numver is 13)---
```python
preds = model_cb.predict_proba(test_embeddings.numpy())
preds
```

---The following area is a Code cell (cell numver is 14)---
```python
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
sample_sub[Targets] =  preds

display(sample_sub)
```

---The following area is a Code cell (cell numver is 15)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 16)---
```python

```

** @@@ Jupyter Notebook numver 81, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python

```

---The following area is a Code cell (cell numver is 1)---
```python
pip install transformers datasets
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
import torch.nn as nn
from tqdm import tqdm
from sklearn.metrics import log_loss

# Define the SiameseLSTM class
class SiameseLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(SiameseLSTM, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_size, 3)  # Output size 3 for 3 classes: model A wins, model B wins, tie

    def forward_one(self, x):
        x = self.embedding(x)
        _, (h, _) = self.lstm(x)
        return h[-1]

    def forward(self, x1, x2):
        h1 = self.forward_one(x1)
        h2 = self.forward_one(x2)
        return self.fc(torch.abs(h1 - h2))

# Step 1: Load your training dataset
df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')

# Filter out invalid cases and prepare data
data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values

def determine_label(row):
    if row[2] == 1:
        return 0  # model A wins
    elif row[3] == 1:
        return 1  # model B wins
    elif row[4] == 1:
        return 2  # tie
    else:
        return -1  # Invalid or unclear case

labels = [determine_label(row) for row in data if determine_label(row) != -1]
data = [row[:2] for row in data if determine_label(row) != -1]

# Step 2: Define a custom Dataset class
class SiameseDataset(Dataset):
    def __init__(self, data, labels, tokenizer, max_length):
        self.data = data
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]
        label = self.labels[idx]

        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),
            'input_ids_b': torch.tensor(tokens_b['input_ids']),
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Step 3: Initialize BERT tokenizer
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')
max_length = 128  # Adjust according to your dataset

# Step 4: Create instances of Dataset and DataLoader for training
train_dataset = SiameseDataset(data, labels, tokenizer, max_length)
batch_size = 32
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

# Step 5: Define the Siamese network model
input_size = len(tokenizer)
hidden_size = 300
num_layers = 1
model = SiameseLSTM(input_size, hidden_size, num_layers)

# Step 6: Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

# Step 7: Training loop
num_epochs = 5  # Adjust as needed
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids_a, input_ids_b)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')

# Load your test dataset
df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')

# Define a custom Dataset class for testing
class SiameseTestDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]

        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),
            'input_ids_b': torch.tensor(tokens_b['input_ids']),
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),
        }

# Prepare test data
test_data = df_test[['response_a', 'response_b']].values.tolist()

# Create instance of SiameseTestDataset
test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)

# Create DataLoader for the test dataset
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the saved model
model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))
model.eval()  # Set model to evaluation mode

# Perform inference on the test data and generate predictions
all_preds = []
with torch.no_grad():
    for batch in tqdm(test_loader, desc='Testing'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)

        outputs = model(input_ids_a, input_ids_b)
        probabilities = nn.Softmax(dim=1)(outputs)
        all_preds.extend(probabilities.cpu().numpy().tolist())

# Create a DataFrame for predictions
pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
pred_df['id'] = df_test['id']

# Reorder columns to match the required format
pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

# Save predictions to CSV for submission
pred_df.to_csv('submission.csv', index=False)
print(pred_df.head())

```

---The following area is a Code cell (cell numver is 3)---
```python

```

---The following area is a Code cell (cell numver is 4)---
```python

```

** @@@ Jupyter Notebook numver 82, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is

This is a inference notebook using 4-bit quantized [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) and a LoRA adapter trained using the script I uploaded [here](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune).
Although we can choose to merge the LoRA adapter to the base model for faster inference, naively doing so could introduce non-negligible quantization error. Therefore, I opted to keep the LoRA adapter unmerged. 

## Result

| subset | log loss |
| - | - |
| eval set | 0.9371 |
| public LB | 0.941 |

The submission takes around 4 hours with `max_length=2048` without TTA.
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 3)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## Configurations
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load & pre-process Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True
tokenizer.padding_side = "right"

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load base model on GPU 0
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
#### Load LoRA adapter
```

---The following area is a Code cell (cell numver is 17)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 19)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 20)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

** @@@ Jupyter Notebook numver 83, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
#  Llama-3 8b [TPU Train]

Learning to train llms on tpu, Hope this will help you too!

Notebook inspired from:

* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)


Prerequisite: Access to using llama-3

Note: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)

Please upvote if you learn or find this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
# Install libs
!pip install -qq peft==0.6.0
!pip install -qq bitsandbytes==0.41.1
!pip install -qq accelerate==0.24.1
!pip install -qq transformers==4.35.0
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q 
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q
!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT
!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import torch.nn.functional as F

import torch_xla.debug.profiler as xp
import torch_xla.core.xla_model as xm
import torch_xla.experimental.xla_sharding as xs
import torch_xla.runtime as xr

xr.use_spmd()

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh
from spmd_util import partition_module

tqdm.pandas()

print(f'Torch Version: {torch.__version__}')
```

---The following area is a Code cell (cell numver is 4)---
```python
# import os
# import gc
# import re
# from time import time
# import random
# import warnings
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from tqdm.auto import tqdm

# import torch
# import transformers
# from sklearn.metrics import accuracy_score
# from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
# from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# import torch.nn.functional as F

# import torch_xla.debug.profiler as xp
# import torch_xla.core.xla_model as xm
# import torch_xla.experimental.xla_sharding as xs
# import torch_xla.runtime as xr

# xr.use_spmd()

# from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
# from torch_xla.experimental.xla_sharding import Mesh
# from spmd_util import partition_module

# tqdm.pandas()

# print(f'Torch Version: {torch.__version__}')
# /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
#   from .autonotebook import tqdm as notebook_tqdm
# /usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
#   warn("The installed version of bitsandbytes was compiled without GPU support. "
# /usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
# Torch Version: 2.1.2+cpu

# # Configs
# class CFG:
#     NUM_EPOCHS = 1
#     BATCH_SIZE = 1  # It was 16 before
#     DROPOUT = 0.05 
#     MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
#     SEED = 2024 
#     MAX_LENGTH = 1024 
#     NUM_WARMUP_STEPS = 128
#     LR_MAX = 5e-5 
#     NUM_LABELS = 3 
#     LORA_RANK = 4
#     LORA_ALPHA = 8
#     LORA_MODULES = ['o_proj', 'v_proj']
    
# DEVICE = xm.xla_device()  # Initialize TPU Device
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 1 # It was 16 before
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
    
DEVICE = xm.xla_device() # Initialize TPU Device
```

---The following area is a Code cell (cell numver is 7)---
```python
def set_seeds(seed):
    """Set seeds for reproducibility """
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
    # Set seed for all TPU cores
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 10)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 12)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 13)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 14)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 15)---
```python
# Train with only take 50% train dataset
# train = train[:int(len(train) * 0.5)]
train = train[int(len(train) * 0.5):]

# Tsplit to two halves
first_half = train[:int(len(train) * 0.5)]

first_half.loc[:, 'token_count'] = get_token_lengths(first_half['text'])
first_half.loc[:, 'label'] = np.argmax(first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)


train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model and get the max index
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 16)---
```python

```

---The following area is a Code cell (cell numver is 17)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 18)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 19)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 21)---
```python
# # Tokenize Data
# tokens = tokenizer(
#     train['text'].tolist(), 
#     padding='max_length', 
#     max_length=CFG.MAX_LENGTH, 
#     truncation=True, 
#     return_tensors='np')

# # Input IDs are the token IDs
# INPUT_IDS = tokens['input_ids']
# # Attention Masks to Ignore Padding Tokens
# ATTENTION_MASKS = tokens['attention_mask']
# # Label of Texts
# LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

# print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
# print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 22)---
```python
tokens = tokenizer(
    first_half['text'].tolist(), 
    padding='max_length', 
    max_length=CFG.MAX_LENGTH, 
    truncation=True, 
    return_tensors='np'
)

INPUT_IDS = tokens['input_ids']
ATTENTION_MASKS = tokens['attention_mask']
LABELS = first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values
print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 23)---
```python
def train_dataset(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        # Shuffle Indices
        np.random.shuffle(IDXS)
        # Iterate Over All Indices Once
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output
            
            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)
```

---The following area is a Code cell (cell numver is 24)---
```python
# Define dataset generator for the first half
def train_dataset_first_half(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        np.random.shuffle(IDXS)
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset_first_half(CFG.BATCH_SIZE)
# Calculate STEPS_PER_EPOCH for the first half
STEPS_PER_EPOCH = len(first_half) // CFG.BATCH_SIZE


```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# Load Model
```

---The following area is a Code cell (cell numver is 26)---
```python
# Load model for classification with 3 target label
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    num_labels=CFG.NUM_LABELS,
    torch_dtype=torch.bfloat16)

base_model.config.pretraining_tp = 1 

# Assign Padding TOKEN
base_model.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# Low-Rank Adaptation [LORA]
```

---The following area is a Code cell (cell numver is 29)---
```python
lora_config = LoraConfig(
    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices
    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations
    lora_dropout= CFG.DROPOUT, 
    bias='none',
    inference_mode=False,
    task_type=TaskType.SEQ_CLS,
    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection
```

---The following area is a Code cell (cell numver is 30)---
```python
# Create LoRa Model
model = get_peft_model(base_model, lora_config)
# Trainable Parameters
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 31)---
```python
# Number of TPU Nodes
num_devices = xr.global_runtime_device_count()
mesh_shape = (1, num_devices, 1)
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))
# distribute model
partition_module(model, mesh)

print(f'num_devices: {num_devices}')
```

---The following area is a Code cell (cell numver is 32)---
```python
# Verfy The Trainable Layers
MODEL_LAYERS_ROWS = []
TRAINABLE_PARAMS = []
N_TRAINABLE_PARAMS = 0

for name, param in model.named_parameters():
    # Layer Parameter Count
    n_parameters = int(torch.prod(torch.tensor(param.shape)))
    # Only Trainable Layers
    if param.requires_grad:
        # Add Layer Information
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,
            'name': name,
            'dtype': param.data.dtype,
        })
        # Append Trainable Parameter
        TRAINABLE_PARAMS.append({ 'params': param })
        # Add Number Of Trainable Parameters"
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))

print(f"""
===============================
N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}
N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}
===============================
""")
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 34)---
```python
# LR & Optimizer
N_SAMPLES = len(train)
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)

# Cosine Learning Rate With Warmup
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)

print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')
```

---The following area is a Code cell (cell numver is 35)---
```python
# Set the data type for the optimizer's state (e.g., momentum buffers)
for state in OPTIMIZER.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:
            state[v] = v.to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 36)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)

print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')
print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')
print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')
```

---The following area is a Code cell (cell numver is 37)---
```python
%%time
# Dummy Prediction
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    
print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')
```

---The following area is a Code cell (cell numver is 38)---
```python
# Put Model In Train Mode
model.train()

# Loss Function, Cross Entropy
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 39)---
```python
st = time()
warnings.filterwarnings("error")
METRICS = {
    'loss': [],
    'accuracy': {'y_true': [], 'y_pred': [] }}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):
    ste = time()
    for step in range(STEPS_PER_EPOCH):
        # Zero Out Gradients
        OPTIMIZER.zero_grad()
        
        # Get Batch
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # Forward Pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # Logits Float32
        logits = outputs.logits.to(dtype=torch.float32)
        
        # Backward Pass
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))
        loss.backward()
        
        # optimizer step
        OPTIMIZER.step()
        xm.mark_step()
        
        # Update Learning Rate Scheduler
        lr_scheduler.step()
        
        # Update Metrics And Progress Bar
        METRICS['loss'].append(float(loss))
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()
        
        if (step + 1) % 200 == 0:  
            metrics = 'Âµ_loss: {:.3f}'.format(np.mean(METRICS['loss']))
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])
            metrics += ', Âµ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))
            lr = OPTIMIZER.param_groups[0]['lr']
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')
    
    print(f'\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )

    # If stopped, and to continue training in future on tpu we save model and optimizer
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    
    
    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')
```

---The following area is a Code cell (cell numver is 40)---
```python
plt.figure(figsize=(15, 6))
plt.plot(METRICS['loss'])    
plt.xlabel('Step per epoch')
plt.ylabel('Loss')
plt.title('Loss Plot step per epoch')    
plt.show()
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# Save Model
```

---The following area is a Code cell (cell numver is 42)---
```python
model = model.cpu()
torch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 84, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
from itertools import zip_longest
from  tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from transformers import BertTokenizer, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
import  pandas as pd
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


def process(example):
    sentences = [s.strip('"').replace('[MASK]','') for s in example['prompt'].strip('[]').split('","')]
    sentences_a = [s.strip('"').replace('[MASK]','') for s in example['response_a'].strip('[]').split('","')]
    sentences_b = [s.strip('"').replace('[MASK]','') for s in example['response_b'].strip('[]').split('","')]
    texts_a = [p for pair in zip_longest(sentences, sentences_a, fillvalue='') for p in pair if p]
    texts_b = [p for pair in zip_longest(sentences, sentences_b, fillvalue='') for p in pair if p]
    return pd.Series([' '.join(sentences), ' '.join(sentences_a), ' '.join(sentences_b), '\n'.join(texts_a), '\n'.join(texts_b)], index=['prompt', 'response_a', 'response_b', 'text_a','text_b'])

test[['prompt', 'response_a', 'response_b', 'text_a','text_b']] = test.apply(process, axis=1)
test = test.dropna()


# pet
class Senmamtic_news(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
        self.sep_token = tokenizer.sep_token
        self.mask_token = tokenizer.mask_token

        prompt_new = []
        tk0 = tqdm(data['prompt'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            prompt_new.append(text)
        # response_a
        print(f'== response_a ==')
        response_a_new = []
        tk0 = tqdm(data['response_a'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_a_new.append(text)

        # response_b
        print(f'== response_b ==')
        response_b_new = []
        tk0 = tqdm(data['response_b'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_b_new.append(text)
        self.data['prompt'] = prompt_new
        self.data['response_a'] = response_a_new
        self.data['response_b'] = response_b_new


    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        # text, label = self.data[idx]
#         semantic_label = self.data['semantic_labels'][idx]
        prompt = self.data['prompt'][idx]
        response_a = self.data['response_a'][idx]
        response_b = self.data['response_b'][idx]
        system_prompt = f"""{prompt}:
                        Response A: {response_a}
                        Response B: {response_b}
                        Which is better? Choose 'A', 'B', or 'both'.
""" +self.mask_token


        inputs = self.tokenizer(system_prompt, truncation=True, max_length=1600)
#         semantic_label = self.tokenizer(semantic_label, truncation=True, max_length=2,add_special_tokens=False)['input_ids']
        input_ids  = torch.tensor(inputs['input_ids'], dtype=torch.long)
        mask_index = torch.where(input_ids == self.tokenizer.mask_token_id)[0]

        return {
            'input_ids': input_ids,
            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
#             'semantic_label': torch.tensor(semantic_label, dtype=torch.long),
            # 'semantic_label': torch.tensor([-100]*(len(inputs['input_ids'])-3)+semantic_label+[-100], dtype=torch.long),
            'mask_index': mask_index
        }
def collate_fn_semantic(batch):
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
#     semantic_label = [item['semantic_label'] for item in batch]
    mask_index = [item['mask_index'] for item in batch]

    input_ids = pad_sequence(input_ids, batch_first=True)
    attention_mask = pad_sequence(attention_mask, batch_first=True,)
#     semantic_label = pad_sequence(semantic_label, batch_first=True,padding_value=-100)
    mask_index = torch.stack(mask_index)
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
#         'semantic_label': semantic_label,
        'mask_index': mask_index
    }
def get_semantic_data_loader(batch_size=32, mode='train',shuffle=True):
    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/deberta-small/deberta')
    if mode == 'train':
        dataset = Senmamtic_news(train, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle,collate_fn=collate_fn_semantic)
        return dataloader
    else:
        dataset = Senmamtic_news(test, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False,collate_fn=collate_fn_semantic,drop_last = False)
        return dataloader
import torch
from torch import nn
from transformers import AutoModelForMaskedLM, AutoConfig
from tqdm import tqdm

# å®šä¹‰æ•°æ®åŠ è½½å™¨å‡½æ•°å’Œç´¢å¼•å­—å…¸
# from get_dataloader import get_semantic_data_loader


# å®šä¹‰æ¨¡åž‹ç±»
class BertPET(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        self.bert = AutoModelForMaskedLM.from_pretrained(model_path)
        self.config = AutoConfig.from_pretrained(model_path)
    # {"336": 0,
    #             "736": 1,
    #             "462": 2 }
    def forward(self, input_ids, attention_mask, mask_index, labels=None, return_logits=False):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits
        mask_index = mask_index.unsqueeze(-1).expand(-1, -1, output.size(-1))
        output = torch.gather(output, 1, mask_index).squeeze(1)
        if return_logits:
            # è¿”å›žç‰¹å®šç´¢å¼•çš„ logits
            logits = output[:,[336, 732, 462]]
            return logits

# æŽ¨ç†å‡½æ•°
def inference(model, dataloader, device):
    model.eval()
    softmax = nn.Softmax(dim=-1)
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Inference", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            mask_index = batch['mask_index'].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask, mask_index=mask_index, return_logits=True)
#             logits = logits.squeeze(1)

            probs = softmax(logits)
            probs_list = probs.cpu().numpy().tolist()
            for i in probs_list:
                all_probs.append(i)
        

    return all_probs

# è®¾ç½®è®¾å¤‡
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# åŠ è½½æ¨¡åž‹å’Œæƒé‡
model_path = '/kaggle/input/deberta-small/deberta'  # é¢„è®­ç»ƒæ¨¡åž‹è·¯å¾„
checkpoint_path = '/kaggle/input/deberta-v3/model_checkpoint_epoch_2.pt'  # æœ€ä½³æƒé‡æ–‡ä»¶è·¯å¾„
model = BertPET(model_path)
model.load_state_dict(torch.load(checkpoint_path, map_location=device))
model.to(device)

# èŽ·å–æµ‹è¯•æ•°æ®
test_loader = get_semantic_data_loader(2, mode='test')

# è¿›è¡ŒæŽ¨ç†
test_probs = inference(model, test_loader, device)


sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

test[['winner_model_a', 'winner_model_b', 'winner_tie']] = test_probs
sample_sub = sample_sub[['id']].merge(test[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']], on='id', how='left')
sample_sub.to_csv('submission.csv', index=False)
display(sample_sub.head())
```

** @@@ Jupyter Notebook numver 85, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
import torch.nn.functional as F
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 3)---
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2')

tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True
# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 4)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 7)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 8)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 9)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 1)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 10)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 11)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 12)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 14)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 15)---
```python
max_features = 21540#14300
maxlen = 1024
batch_size = 16
embedding_dims = 200
nb_filter = 150
filter_length = 3
hidden_dims = 100
nb_epoch = 14
```

---The following area is a Code cell (cell numver is 16)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
X_train = sq.pad_sequences(INPUT_IDS, maxlen=maxlen)
y_train = LABELS
```

---The following area is a Code cell (cell numver is 17)---
```python
X_train = np.array(X_train)
y_train = np.array(y_train)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# Define Model
```

---The following area is a Code cell (cell numver is 19)---
```python
'''This example demonstrates the use of Convolution1D for text classification.
Gets to 0.88 test accuracy after 2 epochs.
90s/epoch on Intel i5 2.4Ghz CPU.
10s/epoch on Tesla K40 GPU.
'''
from keras.layers import Concatenate
from keras.layers import  GlobalMaxPooling1D

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})


# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)



model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int64', name='main_input')
emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)
def max_1d(X):
    return K.max(X, axis=1)

# we add a Convolution1D, which will learn nb_filter
# word group filters of size 3:

con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size =3,
                    strides=1)(emb_layer)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 4:

con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)


# we add a Convolution1D, which will learn nb_filter
# word group filters of size 5:

con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)


cnn_layer =Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])


#LSTM


x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)
output_layer= Dropout(0.2)(dense_layer)
output_layer = Dense(3, trainable=True,activation='softmax')(output_layer)




model = Model(inputs=[input_layer], outputs=[output_layer])
adadelta = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-06)

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])
model.summary()

```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 21)---
```python
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')
model.fit(X_train, y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint])

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])
```

---The following area is a Code cell (cell numver is 22)---
```python
model.save('model_LSTM_mix_CNN.keras')  # LÆ°u toÃ n bá»™ model
```

---The following area is a Code cell (cell numver is 23)---
```python
model.predict(X_train)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# Test Model
```

---The following area is a Code cell (cell numver is 25)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# Drop 'Null' for training
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)
test.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(test))
```

---The following area is a Code cell (cell numver is 26)---
```python
test.head()
```

---The following area is a Code cell (cell numver is 27)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])
```

---The following area is a Code cell (cell numver is 28)---
```python
# Tokenize Data
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_test = tokens_test['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS2 = tokens_test['attention_mask']


print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')
```

---The following area is a Code cell (cell numver is 29)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)
```

---The following area is a Code cell (cell numver is 30)---
```python
test
```

---The following area is a Code cell (cell numver is 31)---
```python
y_predict = model.predict(X_test)
y_predict
```

---The following area is a Code cell (cell numver is 32)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
result_df = pd.concat([test['id'], winner_df], axis=1)
```

---The following area is a Code cell (cell numver is 33)---
```python
result_df.to_csv('submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 34)---
```python
result_df
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 86, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## What this notebook is
This notebook demonstrates how I trained Gemma-2 9b to obtain LB: 0.941. The inference code can be found [here](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora).
I used 4-bit quantized [Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit) uploaded by unsloth team as a base-model and added LoRA adapters and trained for 1 epoch.

## Result

I used `id % 5 == 0` as an evaluation set and used all the rest for training.

| subset | log loss |
| - | - |
| eval | 0.9371|
| LB | 0.941 |

## What is QLoRA fine-tuning?

In the conventional fine-tuning, weight ($\mathbf{W}$) is updated as follows:

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

where $L$ is a loss at this step and $\eta$ is a learning rate.

[LoRA](https://arxiv.org/abs/2106.09685) tries to approximate the $\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$ by factorizing $\Delta \mathbf{W}$ into two (much) smaller matrices, $\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$ and $\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$ with $r \ll \text{min}(\text{d}, \text{k})$.

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

During training, only $\mathbf{A}$ and $\mathbf{B}$ are updated while freezing the original weights, meaning that only a fraction (e.g. <1%) of the original weights need to be updated during training. This way, we can reduce the GPU memory usage significantly during training while achieving equivalent performance to the usual (full) fine-tuning.

[QLoRA](https://arxiv.org/abs/2305.14314) pushes the efficiency further by quantizing LLM. For example, a 8B parameter model alone would take up 32GB of VRAM in 32-bit, whereas quantized 8-bit/4-bit 8B model only need 8GB/4GB respectively. 
Note that QLoRA only quantize LLM's weights in low precision (e.g. 8-bit) while the computation of forward/backward are done in higher precision (e.g. 16-bit) and LoRA adapter's weights are also kept in higher precision.

1 epoch using A6000 took ~15h in 4-bit while 8-bit took ~24h and the difference in log loss was not significant.

## Note
It takes prohivitively long time to run full training on kaggle kernel. I recommend to use external compute resource to run the full training.
This notebook uses only 100 samples for demo purpose, but everything else is same as my setup.
```

---The following area is a Code cell (cell numver is 1)---
```python
# gemma-2 is available from transformers>=4.42.3
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass
# å¯¼å…¥æ‰€éœ€åº“
from torch.utils.tensorboard import SummaryWriter
import psutil  # ç”¨äºŽèŽ·å–ç³»ç»Ÿå’Œè¿›ç¨‹çš„èµ„æºä½¿ç”¨ä¿¡æ¯
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 3)---
```python
# !pip install --upgrade kaggle
# #è¦å…ˆè¿™ä¹ˆå¯åŠ¨ä¸‹æ‰èƒ½åˆ›å»º
# !kaggle datasets list
```

---The following area is a Code cell (cell numver is 4)---
```python
# import json

# token = {
#     "username": "qinhaoyang",
#     "key": "02c6cdf132dabb5ddd9de0d37d8a7777"
# }

# with open('/root/.kaggle/kaggle.json', 'w') as file:
#     json.dump(token, file)
# with open('/kaggle/working/kaggle.json', 'w') as file:
#     json.dump(token, file)
```

---The following area is a Code cell (cell numver is 5)---
```python
# !chmod 600 /root/.kaggle/kaggle.json

# # å®šä¹‰ JSON æ•°æ®
# data ={
#   "title": "LMSYS-model", #//æ•°æ®é›†æ ‡é¢˜
#   "subtitle": "",
#   "description": "",
#   "id": "qinhaoyang/LMSYS-model",
#   "licenses": [
#         {
#             "name": "unknown"
#         }
#     ],
#     "keywords": [],
#     "collaborators": [],
#     "data": []
# }


# with open('/kaggle/working/dataset-metadata.json', 'w') as file:
#     json.dump(data, file)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
### Configurations
```

---The following area is a Code cell (cell numver is 7)---
```python
#!kaggle kernels output qinhaoyang/training-gemma-2-9b-4-bit-qlora-fine-tuning -p /kaggle/working
```

---The following area is a Code cell (cell numver is 8)---
```python
#!kaggle datasets version -p /kaggle/working/output/ -m "Description of the dataset changes" --dir-mode tar
```

---The following area is a Code cell (cell numver is 9)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4-bit quantized gemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 1  # global batch size is 8 
    per_device_eval_batch_size: int = 4
    n_epochs: int = 1
    freeze_layers: int = 16  # there're 42 layers in total, we don't add adapters to the first 16 layers
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
#### Training Arguments
```

---The following area is a Code cell (cell numver is 11)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### LoRA config
```

---The following area is a Code cell (cell numver is 13)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # only target self-attention
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### Instantiate the tokenizer & model
```

---The following area is a Code cell (cell numver is 15)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # We'll add <eos> at the end
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 16)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model
```

---The following area is a Code cell (cell numver is 17)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
### Instantiate the dataset
```

---The following area is a Code cell (cell numver is 19)---
```python
#ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
ds = Dataset.from_csv("/kaggle/input/lmsys-72k-dataset/lmsys-7.2k.csv")
#ds = ds.select(torch.arange(100))  # We only use the first 100 data for demo purpose
```

---The following area is a Code cell (cell numver is 20)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))
```

---The following area is a Code cell (cell numver is 21)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
### Compute metrics

We'll compute the log-loss used in LB and accuracy as a auxiliary metric.
```

---The following area is a Code cell (cell numver is 23)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
### Split

Here, train and eval is splitted according to their `id % 5`
```

---The following area is a Code cell (cell numver is 25)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 26)---
```python
# åŠ è½½ checkpoint
checkpoint = "/kaggle/input/lmsys-gemma-checkpoint/output/checkpoint-10200"
```

---The following area is a Code cell (cell numver is 27)---
```python
#%tensorboard --logdir=/kaggle/working
```

---The following area is a Code cell (cell numver is 28)---
```python
# å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®°å½•å½“å‰è¿›ç¨‹çš„å†…å­˜ä½¿ç”¨æƒ…å†µï¼Œå¹¶å†™å…¥TensorBoard
def log_memory_usage(step, writer):  # step æ˜¯è®°å½•çš„æ­¥éª¤æ ‡è¯†ï¼Œwriter æ˜¯SummaryWriterå®žä¾‹
    # èŽ·å–å½“å‰è¿›ç¨‹
    process = psutil.Process(os.getpid())
    # èŽ·å–è¿›ç¨‹çš„å†…å­˜ä¿¡æ¯
    mem_info = process.memory_info()
    # å°†RSSï¼ˆå¸¸é©»é›†å¤§å°ï¼‰å’ŒVMSï¼ˆè™šæ‹Ÿå†…å­˜å¤§å°ï¼‰è½¬æ¢ä¸ºMBï¼Œå¹¶è®°å½•åˆ°TensorBoard
    writer.add_scalar('Memory Usage/RSS (MB)', mem_info.rss / (1024 * 1024), step)  # ç‰©ç†å†…å­˜ä½¿ç”¨
    writer.add_scalar('Memory Usage/VMS (MB)', mem_info.vms / (1024 * 1024), step)  # è™šæ‹Ÿå†…å­˜ä½¿ç”¨
    print(f"Memory usage logged at step {step}")
# åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰çš„TrainerCallbackå­ç±»æ¥åœ¨æ¯ä¸ªepochç»“æŸæ—¶è®°å½•å†…å­˜ä½¿ç”¨
class MemoryUsageLoggingCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):  # è¿™æ˜¯åœ¨æ¯ä¸ªepochç»“æŸæ—¶è¢«è°ƒç”¨çš„å›žè°ƒæ–¹æ³•
        # ä½¿ç”¨å½“å‰çš„epochæ•°ä½œä¸ºè®°å½•çš„æ­¥æ•°
        current_epoch = state.epoch
        # è°ƒç”¨ä¹‹å‰å®šä¹‰çš„å‡½æ•°ï¼Œè®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ
        log_memory_usage(current_epoch, tb_writer)
    
    def on_log(self, args, state, control, **kwargs):
        logs = kwargs.get("logs", {})
        for key, value in logs.items():
            if isinstance(value, (int, float)):
                tb_writer.add_scalar(f"{key.capitalize()}", value, state.global_step)
                print(f"{key}: {value}")

        # é¢å¤–è®°å½•è®­ç»ƒå’Œè¯„ä¼°çš„æŸå¤±ä¸Žå‡†ç¡®çŽ‡
        if "loss" in logs:
            tb_writer.add_scalar("Loss/train", logs["loss"], state.global_step)
        if "eval_loss" in logs:
            tb_writer.add_scalar("Loss/eval", logs["eval_loss"], state.global_step)
        if "accuracy" in logs:
            tb_writer.add_scalar("Accuracy/train", logs["accuracy"], state.global_step)
        if "eval_accuracy" in logs:
            tb_writer.add_scalar("Accuracy/eval", logs["eval_accuracy"], state.global_step)

# åˆå§‹åŒ–SummaryWriter
tb_writer = SummaryWriter(log_dir="/kaggle/working/Gemma/tensorboard_logs")

# å®šä¹‰è®­ç»ƒå’Œè¯„ä¼°æ•°æ®é›†
train_idx, eval_idx = folds[config.fold_idx]

# åˆå§‹åŒ–Traineræ—¶ï¼ŒåŒ…å«æ¨¡åž‹ã€æ•°æ®é›†ç­‰é…ç½®ï¼ŒåŒæ—¶åŠ å…¥è‡ªå®šä¹‰çš„å†…å­˜ä½¿ç”¨è®°å½•Callback
trainer = Trainer(
    args=training_args,  # è®­ç»ƒå‚æ•°
    model=model,  # è®­ç»ƒçš„æ¨¡åž‹
    tokenizer=tokenizer,  # åˆ†è¯å™¨
    train_dataset=ds.select(train_idx),  # è®­ç»ƒæ•°æ®é›†
    eval_dataset=ds.select(eval_idx),  # è¯„ä¼°æ•°æ®é›†
    compute_metrics=compute_metrics,  # è¯„ä¼°æŒ‡æ ‡è®¡ç®—æ–¹æ³•
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # æ•°æ®å¤„ç†
    callbacks=[MemoryUsageLoggingCallback()],  # æ·»åŠ è‡ªå®šä¹‰å›žè°ƒåˆ°Trainer
)

# æ£€æŸ¥æ˜¯å¦æœ‰ checkpoint æ¥ç»§ç»­è®­ç»ƒ
if checkpoint:
    trainer.train(resume_from_checkpoint=checkpoint)
else:
    trainer.train()

# è®­ç»ƒå®ŒæˆåŽï¼Œè®°å¾—å…³é—­SummaryWriterä»¥é‡Šæ”¾èµ„æº
tb_writer.close()
```

---The following area is a Code cell (cell numver is 29)---
```python
model.save_pretrained("/kaggle/working/Gemma") # æŒ‡å®šä¿å­˜æ¨¡åž‹çš„æœ¬åœ°è·¯å¾„
tokenizer.save_pretrained("/kaggle/working/Gemma") # å¦‚æžœéœ€è¦ï¼ŒåŒæ—¶ä¿å­˜tokenizer
```

---The following area is a Code cell (cell numver is 30)---
```python
# !kaggle datasets version -p /kaggle/working -m "Description of the dataset changes" --dir-mode tar
```

** @@@ Jupyter Notebook numver 87, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®èª­ã¿è¾¼ã¿
train_file_path = '/kaggle/input/lmsysdataset/train.csv'
test_file_path = '/kaggle/input/lmsysdataset/test.csv'

train_df = pd.read_csv(train_file_path)
test_df = pd.read_csv(test_file_path)

# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®ä½œæˆ
train_df['target'] = train_df.apply(lambda row: 1 if row['winner_model_a'] == 1 else 0, axis=1)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®çµåˆ
train_df['text_a'] = train_df['prompt'] + ' ' + train_df['response_a']
train_df['text_b'] = train_df['prompt'] + ' ' + train_df['response_b']
test_df['text_a'] = test_df['prompt'] + ' ' + test_df['response_a']
test_df['text_b'] = test_df['prompt'] + ' ' + test_df['response_b']

# TF-IDFãƒ™ã‚¯ãƒˆãƒ©ã‚¤ã‚¶ã®è¨­å®š
vectorizer = TfidfVectorizer(max_features=5000)

# ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–
X_a = vectorizer.fit_transform(train_df['text_a'])
X_b = vectorizer.fit_transform(train_df['text_b'])
X_test_a = vectorizer.transform(test_df['text_a'])
X_test_b = vectorizer.transform(test_df['text_b'])

# è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®åˆ†å‰²
X_train_a, X_valid_a, y_train, y_valid = train_test_split(X_a, train_df['target'], test_size=0.2, random_state=42)
X_train_b, X_valid_b, _, _ = train_test_split(X_b, train_df['target'], test_size=0.2, random_state=42)

# ãƒ­ã‚¸ã‚¹ãƒ†ã‚£ãƒƒã‚¯å›žå¸°ãƒ¢ãƒ‡ãƒ«ã®è¨­å®š
model_a = LogisticRegression(max_iter=1000)
model_b = LogisticRegression(max_iter=1000)

# ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´
model_a.fit(X_train_a, y_train)
model_b.fit(X_train_b, y_train)

# æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
valid_preds_a = model_a.predict_proba(X_valid_a)[:, 1]
valid_preds_b = model_b.predict_proba(X_valid_b)[:, 1]

# ãƒ­ã‚°æå¤±ã®è¨ˆç®—
loss_a = log_loss(y_valid, valid_preds_a)
loss_b = log_loss(y_valid, valid_preds_b)

print(f'Log Loss for model_a: {loss_a}')
print(f'Log Loss for model_b: {loss_b}')

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬
test_preds_a = model_a.predict_proba(X_test_a)[:, 1]
test_preds_b = model_b.predict_proba(X_test_b)[:, 1]

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': test_preds_a,
    'winner_model_b': test_preds_b,
    'winner_tie': 0.0  # åŒç‚¹ã®å ´åˆã¯0ã¨ã—ã¾ã™ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰
})

# æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä¿å­˜
submission.to_csv('/kaggle/working/submission.csv', index=False)
print("Submission file created successfully!")

```

---The following area is a Code cell (cell numver is 1)---
```python

```

** @@@ Jupyter Notebook numver 88, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
train=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 2)---
```python
import tensorflow as tf
# detect and init the TPU


```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## Import libraries
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import tensorflow as tf
import numpy as np
import transformers 
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer

```

---The following area is a Code cell (cell numver is 5)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased')
```

---The following area is a Code cell (cell numver is 6)---
```python
from transformers import TFBertModel

# Load BERT model
bert_model = TFBertModel.from_pretrained('/kaggle/input/bert-base-uncased')

```

---The following area is a Code cell (cell numver is 7)---
```python
for i in train.index:
    if train.loc[i,'winner_model_a']==1:
        train.loc[i,'winner']=0
    elif train.loc[i,'winner_model_b']==1:
        train.loc[i,'winner']=1
    else :
        train.loc[i,'winner']=2
```

---The following area is a Code cell (cell numver is 8)---
```python
features=['prompt','response_a','response_b','winner']
```

---The following area is a Code cell (cell numver is 9)---
```python
train_data=train[features]
```

---The following area is a Code cell (cell numver is 10)---
```python
X_train,X_val=train_test_split(train_data,test_size=0.2,random_state=42)

```

---The following area is a Code cell (cell numver is 11)---
```python
# Tokenize function
def tokenize_function(df):
    prompt_encodings = tokenizer(
        df['prompt'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    response_a_encodings = tokenizer(
        df['response_a'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    response_b_encodings = tokenizer(
        df['response_b'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    return prompt_encodings, response_a_encodings, response_b_encodings 


train_prompt_encodings, train_response_a_encodings, train_response_b_encodings = tokenize_function(X_train)
val_prompt_encodings, val_response_a_encodings, val_response_b_encodings = tokenize_function(X_val)

```

---The following area is a Code cell (cell numver is 12)---
```python
import tensorflow as tf
from tensorflow.keras.utils import to_categorical 

# Prepare input features and labels
train_labels = to_categorical(X_train['winner'].tolist(), num_classes=3)
val_labels = to_categorical(X_val['winner'].tolist(), num_classes=3)

# Create TensorFlow datasets
train_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids_prompt': train_prompt_encodings['input_ids'],
        'attention_mask_prompt': train_prompt_encodings['attention_mask'],
        'input_ids_response_a': train_response_a_encodings['input_ids'],
        'attention_mask_response_a': train_response_a_encodings['attention_mask'],
        'input_ids_response_b': train_response_b_encodings['input_ids'],
        'attention_mask_response_b': train_response_b_encodings['attention_mask'],
    },
    train_labels
)).shuffle(1000).batch(1)

val_dataset = tf.data.Dataset.from_tensor_slices((
    {
        'input_ids_prompt': val_prompt_encodings['input_ids'],
        'attention_mask_prompt': val_prompt_encodings['attention_mask'],
        'input_ids_response_a': val_response_a_encodings['input_ids'],
        'attention_mask_response_a': val_response_a_encodings['attention_mask'],
        'input_ids_response_b': val_response_b_encodings['input_ids'],
        'attention_mask_response_b': val_response_b_encodings['attention_mask'],
    },
    val_labels
)).batch(1)

```

---The following area is a Code cell (cell numver is 13)---
```python

# Define inputs
input_ids_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_prompt")
attention_mask_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_prompt")

input_ids_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_a")
attention_mask_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_a")

input_ids_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_b")
attention_mask_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_b")

```

---The following area is a Code cell (cell numver is 14)---
```python
import tensorflow as tf
from transformers import TFBertModel

class BertEmbeddingLayer(tf.keras.layers.Layer):
    def __init__(self, bert_model_name='bert-base-uncased', **kwargs):
        super(BertEmbeddingLayer, self).__init__(**kwargs)
        self.bert = bert_model
        
    def call(self, inputs):
        input_ids, attention_mask = inputs
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state[:, 0, :]  # CLS token embedding

# Initialize the custom BERT layer
bert_layer = BertEmbeddingLayer()

```

---The following area is a Code cell (cell numver is 15)---
```python

prompt_embeddings = bert_layer([input_ids_prompt, attention_mask_prompt])
response_a_embeddings = bert_layer([input_ids_response_a, attention_mask_response_a])
response_b_embeddings = bert_layer([input_ids_response_b, attention_mask_response_b])

# Concatenate embeddings
combined_embeddings = tf.keras.layers.Concatenate()([prompt_embeddings, response_a_embeddings, response_b_embeddings])

```

---The following area is a Code cell (cell numver is 16)---
```python

dense_layer = tf.keras.layers.Dense(256, activation='relu')(combined_embeddings)
dropout_layer = tf.keras.layers.Dropout(0.2)(dense_layer)
output_layer = tf.keras.layers.Dense(3, activation='softmax')(dropout_layer)

# Build and compile the model
model = tf.keras.Model(inputs=[
    input_ids_prompt, attention_mask_prompt,
    input_ids_response_a, attention_mask_response_a,
    input_ids_response_b, attention_mask_response_b
], outputs=output_layer)

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy'])

```

---The following area is a Code cell (cell numver is 17)---
```python

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=1
)

```

---The following area is a Code cell (cell numver is 18)---
```python
def encode_text(texts, max_length):
    return tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='tf'
    )
```

---The following area is a Code cell (cell numver is 19)---
```python
test
```

---The following area is a Code cell (cell numver is 20)---
```python
max_length = 128  # Adjust according to your model's max sequence length
input_ids_prompt = encode_text(test['prompt'].tolist(), max_length)
input_ids_response_a = encode_text(test['response_a'].tolist(), max_length)
input_ids_response_b = encode_text(test['response_b'].tolist(), max_length)
```

---The following area is a Code cell (cell numver is 21)---
```python
predictions = model.predict({
    'input_ids_prompt': input_ids_prompt['input_ids'],
    'attention_mask_prompt': input_ids_prompt['attention_mask'],
    'input_ids_response_a': input_ids_response_a['input_ids'],
    'attention_mask_response_a': input_ids_response_a['attention_mask'],
    'input_ids_response_b': input_ids_response_b['input_ids'],
    'attention_mask_response_b': input_ids_response_b['attention_mask']
})
```

---The following area is a Code cell (cell numver is 22)---
```python
print("done till here")
```

---The following area is a Code cell (cell numver is 23)---
```python
results=pd.DataFrame({
    'ID':test['id'],
    'winner_model_a':predictions[0],
    'winner_model_b':predictions[1],
    'winner_tie':predictions[2],
    
})
```

---The following area is a Code cell (cell numver is 24)---
```python
results.to_csv('/kaggle/working/submission.csv', index=False)
```

---The following area is a Code cell (cell numver is 25)---
```python

```

---The following area is a Code cell (cell numver is 26)---
```python

```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Code cell (cell numver is 28)---
```python

```

** @@@ Jupyter Notebook numver 89, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## Gemma 2 - 9b 

We use Gemma 2 9b model to get embeddings and train a classifier on it. This is first part and in this we only compute embed. You can also use other models. Let's get started!

Upvote if you found this helpful!
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes 
!pip install -q git+https://github.com/huggingface/transformers
!pip install sentencepiece
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import gc
import re
from time import time

import torch
import transformers
import sklearn
import random
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:
    MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
    MAX_LENGTH = 1024
    BATCH_SIZE = 2
    
device0 = torch.device('cuda:0')
device1 = torch.device('cuda:1')
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Load model
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = GemmaTokenizer.from_pretrained(CFG.MODEL_PATH)

bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_use_double_quant=False)

model_0 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:0',
                                        quantization_config=bnb_config_4bit)        

model_1 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",
                                        device_map='cuda:1',
                                        quantization_config=bnb_config_4bit)     
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 9)---
```python
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences[-1] if sentences else ''
  
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena-additional-data-90k-columns/Merged_data.csv')

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)


train['text'] = '<start_of_turn>User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n----\n\nModel B:\n'  + train['response_b'] + '<end_of_turn><eos>'
```

---The following area is a Code cell (cell numver is 10)---
```python
# taking only 40k sample
train = train[:40000]
train.head(1)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(train['text'][10])
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 13)---
```python
tokens = tokenizer(train['text'].tolist(),
                   padding='max_length',
                   max_length=CFG.MAX_LENGTH,
                   truncation=True,
                   return_tensors='pt')

INPUT_IDS = tokens['input_ids']
ATTENTION_MASKS = tokens['attention_mask']

data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in INPUT_IDS]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in ATTENTION_MASKS]
data[:2]
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Compute embedding
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_embeddings(df, model, device, batch_size=CFG.BATCH_SIZE):  
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []

    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        gc.collect()
        torch.cuda.empty_cache()
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]
            embed_mean = torch.mean(embed, dim=1).cpu() #mean pool
            embed_list.append(embed_mean) 
            
            torch.cuda.empty_cache()
        
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings

def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)
```

---The following area is a Code cell (cell numver is 16)---
```python
st = time.time()

N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

results = {}

t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

t0.join()
t1.join()

print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 17)---
```python
embeddings = torch.cat([results[0], results[1]], dim=0)
embeddings.shape
```

---The following area is a Code cell (cell numver is 18)---
```python
gc.collect()
del model_1
del  model_0
torch.cuda.empty_cache()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Save embed
```

---The following area is a Code cell (cell numver is 20)---
```python
save_path = 'gemma2_train_embed.npy'

# Save the embeddings as .npy file
np.save(save_path, embeddings.numpy())
# we also save train just for completeness
train.to_csv('train_embed.csv', index=False)

print(f"Concatenated embeddings saved to {save_path}")
```

** @@@ Jupyter Notebook numver 90, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# About this notebook
- Deberta-v3 xsmall starter code
- Original notebook is [here](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)
- Inference notebook will be updated a little later.
- In this notebook, I just use only prompt so you can use response_a, response_a, etc.



If this notebook is helpful, feel free to upvote.

And please upvote the original notebook :)

`V1` - Run debug mode for test
- only used prompt, you can add more texts

`V2` - Remove existing preprocessing (this part needs more ideas)
- please see [this](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/497832)
- changed `max_length = 1024` and `lr = 1e-5`

`V3` - Will be updated for all texts or HuggingFace, etc.
```

---The following area is a Code cell (cell numver is 1)---
```python
# ====================================================
# Directory settings
# ====================================================
import os

OUTPUT_DIR = './'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
```

---The following area is a Code cell (cell numver is 2)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 3)---
```python
# ====================================================
# CFG
# ====================================================
class CFG:
    wandb=False
    competition='LMSYS'
    _wandb_kernel='test'
    debug=True
    apex=True
    print_freq=20
    num_workers=4
    model="microsoft/deberta-v3-xsmall" # ["microsoft/deberta-v3-small, microsoft/deberta-v3-base"]
    gradient_checkpointing=False
    scheduler='cosine' # ['linear', 'cosine']
    batch_scheduler=True
    num_cycles=0.5
    num_warmup_steps=0
    epochs=4
    encoder_lr=1e-5
    decoder_lr=1e-5
    min_lr=1e-5
    eps=1e-6
    betas=(0.9, 0.999)
    batch_size=4
    max_len=2048
    weight_decay=0.01
    gradient_accumulation_steps=1
    max_grad_norm=1000
    target_label=['target']
    target_cols=['winner_model_a', 'winner_model_b', 'winner_tie']
    seed=42
    train=True
    
if CFG.debug:
    CFG.epochs = 2
    CFG.trn_fold = [0, 1]
```

---The following area is a Code cell (cell numver is 4)---
```python
# ====================================================
# Library
# ====================================================
import os
import gc
import re
import ast
import sys
import copy
import json
import time
import math
import string
import pickle
import random
import joblib
import itertools
import warnings
warnings.filterwarnings("ignore")

import scipy as sp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
from tqdm.auto import tqdm
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold

import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.nn.functional as F
from torch.optim import Adam, SGD, AdamW
from torch.utils.data import DataLoader, Dataset

os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels transformers')
os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels tokenizers')

import tokenizers
import transformers
print(f"tokenizers.__version__: {tokenizers.__version__}")
print(f"transformers.__version__: {transformers.__version__}")
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
%env TOKENIZERS_PARALLELISM=true

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---The following area is a Code cell (cell numver is 5)---
```python
# ====================================================
# Utils
# ====================================================
def get_score(y_trues, y_preds):
    score = log_loss(y_trues, y_preds, labels=[0, 1, 2])
    return score


def get_logger(filename=OUTPUT_DIR+'train'):
    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter
    logger = getLogger(__name__)
    logger.setLevel(INFO)
    handler1 = StreamHandler()
    handler1.setFormatter(Formatter("%(message)s"))
    handler2 = FileHandler(filename=f"{filename}.log")
    handler2.setFormatter(Formatter("%(message)s"))
    logger.addHandler(handler1)
    logger.addHandler(handler2)
    return logger

LOGGER = get_logger()


def seed_everything(seed=42):
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=CFG.seed)
```

---The following area is a Code cell (cell numver is 6)---
```python
# =====================================
# We will add targets in dataframs
# =====================================
def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)
    labels[df['winner_model_a'] == 1] = 0
    labels[df['winner_model_b'] == 1] = 1
    labels[df['winner_tie'] == 1] = 2
    df['target'] = labels
    return df


```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return sentences

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 8)---
```python
add_label(train).head(5)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 10)---
```python
# ====================================================
# tokenizer
# ====================================================
# Define special tokens
special_tokens = ['[R_STRAT]', '[R_END]', '<PROMPT>', '<RESPONSE>', '[NL]', '[NLNL]']

def preprocess_text(text):
    text = text.replace('\n\n', ' [NLNL] ')
    text = text.replace('\n', ' [NL] ')
    return text

def format_conversation(row):
    conversations = []
    num_turns = min(len(row['prompt']), len(row['response_a']), len(row['response_b']))
    
    for i in range(num_turns):
        prompt = f"<PROMPT> {row['prompt'][i]}"
        response_a = f"<RESPONSE> [R_STRAT] {preprocess_text(row['response_a'][i])} [R_END]"
        response_b = f"[R_STRAT] {preprocess_text(row['response_b'][i])} [R_END]"
        conversations.append(f"{prompt} {response_a} {response_b}")
        
    return ' [NLNL] '.join(conversations)

train['text'] = train.apply(format_conversation, axis=1)

# Add special tokens to tokenizer
tokenizer = AutoTokenizer.from_pretrained(CFG.model)
special_tokens_dict = {'additional_special_tokens': special_tokens}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)
CFG.tokenizer = tokenizer
```

---The following area is a Code cell (cell numver is 11)---
```python
tokenizer.decode(
    tokenizer(
        train['text'][1]
    ).input_ids
)
```

---The following area is a Code cell (cell numver is 12)---
```python
# ====================================================
# Dataset
# ====================================================
def prepare_input(cfg, text):
    inputs = cfg.tokenizer.encode_plus(
        text, 
        return_tensors=None, 
        add_special_tokens=True, 
        max_length=CFG.max_len,
        pad_to_max_length=True,
        truncation=True
    )
    for k, v in inputs.items():
        inputs[k] = torch.tensor(v, dtype=torch.long)
    return inputs


class TrainDataset(Dataset):
    def __init__(self, cfg, df):
        self.cfg = cfg
        self.texts = df['text'].values # only use prompt, please feel free add other texts
        self.labels = df[cfg.target_label].values.squeeze().tolist()

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, item):
        inputs = prepare_input(self.cfg, self.texts[item])
        label = torch.tensor(self.labels[item], dtype=torch.long)
        return inputs, label
    
    
def collate(inputs):
    mask_len = int(inputs["attention_mask"].sum(axis=1).max())
    for k, v in inputs.items():
        inputs[k] = inputs[k][:,:mask_len]
    return inputs
```

---The following area is a Code cell (cell numver is 13)---
```python
# ====================================================
# Model
# ====================================================
class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()
        
    def forward(self, last_hidden_state, attention_mask):
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min=1e-9)
        mean_embeddings = sum_embeddings / sum_mask
        return mean_embeddings
    

class CustomModel(nn.Module):
    def __init__(self, cfg, config_path=None, pretrained=False):
        super().__init__()
        self.cfg = cfg
        if config_path is None:
            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)
            self.config.hidden_dropout = 0.
            self.config.hidden_dropout_prob = 0.
            self.config.attention_dropout = 0.
            self.config.attention_probs_dropout_prob = 0.
            LOGGER.info(self.config)
        else:
            self.config = torch.load(config_path)
        if pretrained:
            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)
        else:
            self.model = AutoModel(self.config)
        if self.cfg.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()
        self.pool = MeanPooling()
        self.fc = nn.Linear(self.config.hidden_size, 3)
        self._init_weights(self.fc)
        
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        
    def feature(self, inputs):
        outputs = self.model(**inputs)
        last_hidden_states = outputs[0]
        feature = self.pool(last_hidden_states, inputs['attention_mask'])
        return feature

    def forward(self, inputs):
        feature = self.feature(inputs)
        output = self.fc(feature)
        return output
```

---The following area is a Code cell (cell numver is 14)---
```python
# ====================================================
# Helper functions
# ====================================================
class AverageMeter(object):
    """Computes and stores the average and current value"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def asMinutes(s):
    m = math.floor(s / 60)
    s -= m * 60
    return '%dm %ds' % (m, s)


def timeSince(since, percent):
    now = time.time()
    s = now - since
    es = s / (percent)
    rs = es - s
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))


def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):
    model.train()
    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)
    losses = AverageMeter()
    start = end = time.time()
    global_step = 0
    for step, (inputs, labels) in enumerate(train_loader):
        inputs = collate(inputs)
        for k, v in inputs.items():
            inputs[k] = v.to(device)
        labels = labels.to(device)
        batch_size = labels.size(0)
        with torch.cuda.amp.autocast(enabled=CFG.apex):
            y_preds = model(inputs)
            loss = criterion(y_preds, labels)
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps
        losses.update(loss.item(), batch_size)
        scaler.scale(loss).backward()
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)
        if (step + 1) % CFG.gradient_accumulation_steps == 0:
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()
            global_step += 1
            if CFG.batch_scheduler:
                scheduler.step()
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):
            print('Epoch: [{0}][{1}/{2}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  'Grad: {grad_norm:.4f}  '
                  'LR: {lr:.8f}  '
                  .format(epoch+1, step, len(train_loader), 
                          remain=timeSince(start, float(step+1)/len(train_loader)),
                          loss=losses,
                          grad_norm=grad_norm,
                          lr=scheduler.get_lr()[0]))
    return losses.avg


def valid_fn(valid_loader, model, criterion, device):
    losses = AverageMeter()
    model.eval()
    preds = []
    start = end = time.time()
    for step, (inputs, labels) in enumerate(valid_loader):
        inputs = collate(inputs)
        for k, v in inputs.items():
            inputs[k] = v.to(device)
        labels = labels.to(device)
        batch_size = labels.size(0)
        with torch.no_grad():
            y_preds = model(inputs)
            loss = criterion(y_preds, labels)
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps
        losses.update(loss.item(), batch_size)
        preds.append(y_preds.softmax(1).to('cpu').numpy())
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):
            print('EVAL: [{0}/{1}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  .format(step, len(valid_loader),
                          loss=losses,
                          remain=timeSince(start, float(step+1)/len(valid_loader))))
    predictions = np.concatenate(preds)
    return losses.avg, predictions
```

---The following area is a Code cell (cell numver is 15)---
```python
# ====================================================
# train loop
# ====================================================
def train_loop(train_df, valid_df):
    
    LOGGER.info(f"========== training ==========")

    # ====================================================
    # loader
    # ====================================================
    valid_labels = valid_df[CFG.target_label].values
    
    train_dataset = TrainDataset(CFG, train_df)
    valid_dataset = TrainDataset(CFG, valid_df)

    train_loader = DataLoader(train_dataset,
                              batch_size=CFG.batch_size,
                              shuffle=True,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)
    valid_loader = DataLoader(valid_dataset,
                              batch_size=CFG.batch_size * 2,
                              shuffle=False,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)

    # ====================================================
    # model & optimizer
    # ====================================================
    model = CustomModel(CFG, config_path=None, pretrained=True)
    torch.save(model.config, OUTPUT_DIR+'config.pth')
    model.to(device)
    
    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):
        param_optimizer = list(model.named_parameters())
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': weight_decay},
            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': 0.0},
            {'params': [p for n, p in model.named_parameters() if "model" not in n],
             'lr': decoder_lr, 'weight_decay': 0.0}
        ]
        return optimizer_parameters

    optimizer_parameters = get_optimizer_params(model,
                                                encoder_lr=CFG.encoder_lr, 
                                                decoder_lr=CFG.decoder_lr,
                                                weight_decay=CFG.weight_decay)
    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)
    
    # ====================================================
    # scheduler
    # ====================================================
    def get_scheduler(cfg, optimizer, num_train_steps):
        if cfg.scheduler == 'linear':
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps
            )
        elif cfg.scheduler == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles
            )
        return scheduler
    
    num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)
    scheduler = get_scheduler(CFG, optimizer, num_train_steps)

    # ====================================================
    # loop
    # ====================================================
    criterion = nn.CrossEntropyLoss()
    
    best_score = np.inf

    for epoch in range(CFG.epochs):

        start_time = time.time()

        # train
        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)

        # eval
        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)
        
        # scoring
        score = get_score(valid_labels, predictions)

        elapsed = time.time() - start_time

        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')
        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')
        if CFG.wandb:
            wandb.log({"epoch": epoch+1, 
                       "avg_train_loss": avg_loss, 
                       "avg_val_loss": avg_val_loss,
                       "score": score})
        
        if best_score > score:
            best_score = score
            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')
            torch.save({'model': model.state_dict(),
                        'predictions': predictions},
                        OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth")

    predictions = torch.load(OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth", 
                             map_location=torch.device('cpu'))['predictions']
    valid_df[[f"pred_{c}" for c in CFG.target_cols]] = predictions

    torch.cuda.empty_cache()
    gc.collect()
    
    return valid_df
```

---The following area is a Code cell (cell numver is 16)---
```python
if __name__ == '__main__':
    
    def get_result(oof_df):
        labels = oof_df[CFG.target_cols].values
        labels = np.argmax(labels, axis=1)
        preds = oof_df[[f"pred_{c}" for c in CFG.target_cols]].values
        score = get_score(labels, preds)
        LOGGER.info(f'Score: {score:<.4f}')
    
    if CFG.train:
        # Split data into train and validation (80% train, 20% validation)
        train_df = train.sample(frac=0.8, random_state=CFG.seed).reset_index(drop=True)
        valid_df = train.drop(train_df.index).reset_index(drop=True)

        _oof_df = train_loop(train_df, valid_df)
        LOGGER.info(f"========== result ==========")
        get_result(_oof_df)
        _oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')
        
    if CFG.wandb:
        wandb.finish()
```

** @@@ Jupyter Notebook numver 91, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## ðŸ¦™ðŸ¦™ðŸ¦™ What this notebook is
This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.
I made a few improvements upon @kishanvavdara's work:

### 38% faster inference
Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:

#### 1. Dynamic padding
Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.

#### 2. Sort the test data by input length
To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.

### Longer input sequence
Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.
Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.

## Things I have tried but didn't work

### Test Time Augmentation (TTA)
I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.
We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.
TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.

### Truncate each input
The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.
I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Import libs
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2, "Sorry - multi-GPU required!"
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    weights_path = '/kaggle/input/lmsys-model/model'
    max_length = 1280
    batch_size = 8
    device = torch.device("cuda")    
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Prepare Data
```

---The following area is a Code cell (cell numver is 7)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]
# swap response_a & response_b
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# Load model 
We load 1 model on each gpu.
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

# Load base model on GPU 0
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# Load weights
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRA configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj']
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device_0) 
# Load weights
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# Trainable Parameters
model_0.print_trainable_parameters()
model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# Inference
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]
        input_ids = tmp["input_ids"].to_list()
        attention_mask = tmp["attention_mask"].to_list()
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding=True,
            max_length=max_length,
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()
        
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()

# sort by input length to fully leverage dynaminc padding
data = data.sort_values("length", ascending=False)
# the total #tokens in sub_1 and sub_2 should be more or less the same
sub_1 = data.iloc[0::2].copy()
sub_2 = data.iloc[1::2].copy()

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # sort by input length to boost speed
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    tta_result_df = pd.concat(list(results), axis=0)
    # recall TTA's order is flipped
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # average original result and TTA result.
    proba = (proba + tta_proba) / 2

print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]
result_df.loc[:, "winner_model_b"] = proba[:, 1]
result_df.loc[:, "winner_tie"] = proba[:, 2]
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
submission_df.to_csv('submission.csv', index=False)
display(submission_df)
```

---The following area is a Code cell (cell numver is 24)---
```python

```

** @@@ Jupyter Notebook numver 92, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# Imports
```

---The following area is a Code cell (cell numver is 1)---
```python
from IPython.display import clear_output
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q bertopic
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install -U numpy==1.23.5
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
from ast import literal_eval
import numpy as np
import os
import random
import re

from umap import UMAP
from hdbscan import HDBSCAN
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sentence_transformers import SentenceTransformer
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance
from bertopic.vectorizers import ClassTfidfTransformer



```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
**Before we start, here is a summary derived from the excellent EDA done in this notebook :**https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda

**General findings**

- There are 64 different models in the training set.
- For each prompt, there are 3 responses, each from different model and ranked according to the human preference.
- The most common (frequent models) in the training data are as follows :

    - gpt-4-1106-preview 
    - gpt-3.5-turbo-0613
    - gpt-4-0613 
    - claude-2.1 
    - (gpt-4-0314,claude-instant-1)
    
    
- Number of Turns : The number of prompt/response pairs in the training data.

    - Around 86.88% of conversations are single-turn.
    - Over 99.19% of conversations are less than 6 turns.
    - The maximum number of turns is 36.
    
    

**Model response preferences**

- Three LLMs have win rate over 50%, including gpt-3.5-turbo-0314, gpt-4-0125-preview and gpt-4-1106-preview.
- A lower tie rate means that a winner can be judged more deterministically


**Biases and correlations in response/prompts**


- No position bias for human judges. i.e , the positions A,B,C in the training data have no such positional bias in them in context of preference of annotators.
 
- Correlation between response lenghts of models for the same prompt: There exist a strong correlation between response length for the same prompt.
 
- Correlation between prompt length and response length : the linear relationship between prompt length and response length, the correlation seems much weaker.

- Verbosity Bias (how the verbosity of the answer affects human preference) : 
    - There is a clear verbosity bias with the data.
    - Correlation of "Mean response length" vs "win rate"  =  0.488
    - gpt-4-0125-preview and gpt-4-1106-preview are models with the top-2 longest average response length.
    

**null/empty responses or prompts**

- There exists 5 samples with empty prompts.
- All of the empty prompts are a single space " " and appear at the last prompt during conversation.
- Models can still continue to respond even if an empty prompt is sent.
- Missing responses can be empty or None.

- efffect on judges
    - We can see that the tie rate drops to around 0.15, which is quite reasonble.
    - If only one model has missing responses, judges might tend to vote the other responding normally or tie.
    

**checking train and test data**
```

---The following area is a Code cell (cell numver is 6)---
```python
def parse_response(response_object:str)->[str,]:
    try:
        resp = literal_eval(response_object)
        
    except Exception as e:
        try:
            response_object = response_object.replace('null',None)
            resp = literal_eval(response_object)
        except Exception as e:
#             print(type(response_object),response_object)
            resp = []

    return resp
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
train['prompt'] = train['prompt'].apply(literal_eval)
train['response_a'] = train['response_a'].apply(parse_response)
train['response_b'] = train['response_b'].apply(parse_response)

test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

_ = print(train.shape),print(test.shape)
```

---The following area is a Code cell (cell numver is 8)---
```python
train.head()
```

---The following area is a Code cell (cell numver is 9)---
```python
#dropping empty responses
train_ss = train[(train.response_a.apply(len)>0) & (train.response_b.apply(len)>0)]

train_ss.shape
```

---The following area is a Code cell (cell numver is 10)---
```python
test.head()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# Topic Modelling 

**How it works (from: https://maartengr.github.io/BERTopic/algorithm/algorithm.html)**

![image.png](attachment:60ac0df8-5866-4948-9240-7e6453976a10.png)

1. Embed documents : We start by converting our documents to numerical representations using a tranformer based model embedding.

2. Dimensionality reduction : After having created our numerical representations of the documents we have to reduce the dimensionality of these representations. Cluster models typically have difficulty handling high dimensional data due to the curse of dimensionality.

3. Cluster Documents : After having reduced our embeddings, we can start clustering our data. For that, we leverage a density-based clustering technique, HDBSCAN. It can find clusters of different shapes and has the nice feature of identifying outliers where possible.

4. Bag-of-words : To create topic representations in BERTopic's algorithm while allowing for modularity, HDBSCAN is used as a clustering model because it accommodates clusters with varying densities and shapes. Instead of using a centroid-based method, all documents in a cluster are combined into a single document, and the frequency of each word is counted to form a bag-of-words representation. This representation, normalized for cluster size differences, focuses on words at the cluster level without assuming a specific cluster structure.

5. Topic representation : From the generated bag-of-words representation, we want to know what makes one cluster different from another. Which words are typical for cluster 1 and not so much for all other clusters? To solve this, we need to modify TF-IDF such that it considers topics (i.e., clusters) instead of documents.

6. Fine-tune Topic representation : we can consider the c-TF-IDF generated topics to be candidate topics. They each contain a set of keywords and representative documents that we can use to further fine-tune the topic representations. Having a set of representative documents for each topic is huge advantage as it allows for fine-tuning on a reduced number of documents. This reduces computation for large models as they only need to operate on that small set of representative documents for each topic. 



    Let us try to find the underlying patterns in the prompt, using a unsupervised topic modelling approach. This will give an high level idea of what the constituent topics in the prompts are.
```

---The following area is a Code cell (cell numver is 12)---
```python
# Setup modules


# Step 1 - Extract embeddings
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# Step 2 - Reduce dimensionality
umap_model = UMAP(
    n_neighbors=20, 
    n_components=5,
    min_dist=0.0, 
    metric="cosine",
    random_state=7,
)

# Step 3 - Cluster reduced embeddings
hdbscan_model = HDBSCAN(
    min_cluster_size=32, 
    min_samples=1,
    metric="euclidean", 
    cluster_selection_method="eom",
    prediction_data=True
)

# Step 4 - Tokenize topics
vectorizer_model = CountVectorizer(stop_words="english")

# Step 5 - Create topic representation
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True,)

# Fine-tune topic representations with  a `bertopic.representation` model
representation_model = MaximalMarginalRelevance(diversity=0.4,
                                                top_n_words=15
                                               )
```

---The following area is a Code cell (cell numver is 13)---
```python
# Build topic modeling pipeline
topic_model = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    representation_model=representation_model,
    n_gram_range=(1,5),
    language="english"
)
```

---The following area is a Code cell (cell numver is 14)---
```python
train_prompt_concatenated = train.prompt.apply(lambda x: "\n\n".join(x)).to_list()
len(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 15)---
```python
%%time
topics,topic_proba = topic_model.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 16)---
```python
topic_model.save("topic_model_unguided", serialization="safetensors")

```

---The following area is a Code cell (cell numver is 17)---
```python
print(f" number of unique topics: {len(np.unique(topics))}")

```

---The following area is a Code cell (cell numver is 18)---
```python
topic_info = topic_model.get_topic_info()
topic_info.head(10)
```

---The following area is a Code cell (cell numver is 19)---
```python
#get a specific topic repr 
topic_model.get_topic(0)
```

---The following area is a Code cell (cell numver is 20)---
```python
#visualize topic repr
topic_model.visualize_barchart(top_n_topics=30,n_words=10)
```

---The following area is a Code cell (cell numver is 21)---
```python
topic_model.visualize_term_rank()

```

---The following area is a Code cell (cell numver is 22)---
```python
topic_model.visualize_heatmap(top_n_topics=20)
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# Guided topic modelling 

    The following illustration gives a idea behind guided topic modelling (from: https://maartengr.github.io/BERTopic/getting_started/guided)

![image.png](attachment:e350c127-4025-4b0a-9942-3768abb34873.png)
```

---The following area is a Code cell (cell numver is 24)---
```python
task_topics = "Code to text,,text to code,Named entity recognition,Sentiment Analysis,Translation,Question Answering,Program Execution, Miscallenous tasks,Text Categorization,Language Identification, Information Extraction,Text Quality,Summarization,text completion,essay writing,poem writing,creative writing,fact verification,reasoning,mathematical,grammer task,rephrasing,style transfer,paraphrasing,natural language inference,question generation,text matching,dialogue generation,harmfullness detection,toxic language detection,fact verification,keyword tagging".split(",")

print(task_topics)
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
**Modified task keywords using CHATGPT**
```

---The following area is a Code cell (cell numver is 26)---
```python
#define seed topics 
task_topics_modified = [
    ['Code to text', 'source code', 'comments', 'explanation', 'description', 'documentation'],
    ['Text to code', 'programming', 'syntax', 'function', 'script', 'automation'],
    ['Named entity recognition', 'NER', 'entities', 'classification', 'annotation', 'identification'],
    ['Sentiment Analysis', 'emotion', 'opinion', 'polarity', 'attitude', 'mood'],
    ['Translation', 'bilingual', 'language pair', 'conversion', 'interpretation', 'localization'],
    ['Question Answering', 'QA', 'response', 'inquiry', 'knowledge', 'retrieval'],
    ['Program Execution', 'run', 'execute', 'compile', 'script', 'process'],
    ['Miscellaneous tasks', 'varied', 'general', 'diverse', 'assorted', 'multiple'],
    ['Text Categorization', 'classification', 'labeling', 'sorting', 'grouping', 'organization'],
    ['Language Identification', 'detection', 'recognition', 'classification', 'language', 'dialect'],
    ['Information Extraction', 'data mining', 'retrieval', 'extraction', 'parsing', 'harvesting'],
    ['Text Quality', 'clarity', 'readability', 'coherence', 'accuracy', 'precision'],
    ['Summarization', 'abstract', 'condense', 'overview', 'digest', 'outline'],
    ['Text completion', 'autocomplete', 'fill-in', 'predictive', 'continuation', 'suggestion'],
    ['Essay writing', 'composition', 'argument', 'thesis', 'structure', 'drafting'],
    ['Poem writing', 'verse', 'rhyme', 'stanza', 'meter', 'lyric'],
    ['Creative writing', 'story', 'imagination', 'narrative', 'fiction', 'expression'],
    ['Fact verification', 'truth', 'validation', 'accuracy', 'confirmation', 'authenticity'],
    ['Reasoning', 'logic', 'deduction', 'inference', 'rationale', 'analysis'],
    ['Mathematical', 'calculation', 'formula', 'equation', 'computation', 'arithmetic'],
    ['Grammar task', 'syntax', 'rules', 'correction', 'structure', 'editing'],
    ['Rephrasing', 'paraphrase', 'reword', 'rewrite', 'restatement', 'alteration'],
    ['Style transfer', 'transformation', 'conversion', 'adaptation', 'modification', 'recasting'],
    ['Paraphrasing', 'rewording', 'restating', 'rephrasing', 'altering', 'modifying'],
    ['Natural language inference', 'NLI', 'hypothesis', 'entailment', 'contradiction', 'inference'],
    ['Question generation', 'inquiry', 'query', 'interrogative', 'ask', 'question'],
    ['Text matching', 'similarity', 'comparison', 'alignment', 'correlation', 'matching'],
    ['Dialogue generation', 'conversation', 'interaction', 'exchange', 'communication', 'chatbot'],
    ['Harmfulness detection', 'toxicity', 'abuse', 'malice', 'danger', 'risk'],
    ['Toxic language detection', 'abusive', 'offensive', 'harmful', 'inappropriate', 'insulting'],
    ['Fact verification', 'validation', 'authenticity', 'accuracy', 'truth', 'confirmation'],
    ['Keyword tagging', 'labeling', 'annotation', 'classification', 'indexing', 'tagging'],
    ['Topic modeling', 'themes', 'topics', 'clustering', 'segmentation', 'grouping'],
    ['Contextual embedding', 'context', 'representation', 'vectors', 'embeddings', 'contextualization'],
    ['Coreference resolution', 'pronouns', 'anaphora', 'antecedents', 'referents', 'binding'],
    ['Semantic similarity', 'meaning', 'relation', 'comparison', 'equivalence', 'likeness'],
    ['Document summarization', 'overview', 'digest', 'abstract', 'compendium', 'condensation'],
    ['Speech recognition', 'transcription', 'audio', 'voice', 'ASR', 'spoken'],
    ['Optical character recognition', 'OCR', 'text', 'image', 'scanning', 'extraction'],
    ['Text generation', 'creation', 'synthesis', 'generation', 'writing', 'production'],
    ['Dialogue summarization', 'conversation', 'overview', 'recap', 'condensation', 'summary'],
    ['Data anonymization', 'privacy', 'masking', 'obfuscation', 'anonymity', 'de-identification']
]



len(task_topics_modified)
```

---The following area is a Code cell (cell numver is 27)---
```python
topic_model_guided = BERTopic(
    embedding_model=embedding_model,
    umap_model=umap_model,
    hdbscan_model=hdbscan_model,
    vectorizer_model=vectorizer_model,
    ctfidf_model=ctfidf_model,
    representation_model=representation_model,
    n_gram_range=(1,5),
    language="english",
    seed_topic_list=task_topics_modified)
```

---The following area is a Code cell (cell numver is 28)---
```python
%%time
topics,topic_proba = topic_model_guided.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 29)---
```python
topic_model_guided.visualize_barchart(top_n_topics=30,n_words=10)
```

---The following area is a Code cell (cell numver is 30)---
```python
topic_model_guided.save("topic_model_guided", serialization="safetensors")

```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
# Resources 



* https://research.google/pubs/large-language-models-are-effective-text-rankers-with-pairwise-ranking-prompting/
* https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms
* https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives
* https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda
* https://www.kaggle.com/code/robikscube/lmsys-chatbot-arena-data-anaylsis#Response-Length-Baseline
* https://medium.com/data-reply-it-datatech/bertopic-topic-modeling-as-you-have-never-seen-it-before-abb48bbab2b2
```

** @@@ Jupyter Notebook numver 93, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
<center><img src="https://keras.io/img/logo-small.png" alt="Keras logo" width="100"><br/>
This starter notebook is provided by the Keras team.</center>

# LMSYS - Chatbot Arena Human Preference Predictions with [KerasNLP](https://github.com/keras-team/keras-nlp) and [Keras](https://github.com/keras-team/keras)

<div align="center">
    <img src="https://i.ibb.co/wJMF5HL/lmsys.png">
</div>

In this competition, our aim is to predict which LLM responses users will prefer in a head-to-head battle between chatbots powered by large language models (LLMs). In other words, the goal of the competition is to predict the preferences of the judges and determine the likelihood that a given prompt/response pair is selected as the winner. This notebook will guide you through the process of fine-tuning the **DebertaV3** model for this competition using the **Shared Weight** strategy with KerasNLP. This strategy is similar to how Multiple Choice Question (MCQ) models are trained. Additionally, we will use mixed precision for faster training and inference.

**Did you know**: This notebook is backend-agnostic, which means it supports TensorFlow, PyTorch, and JAX backends. However, the best performance can be achieved with `JAX`. KerasNLP and Keras enable the choice of the preferred backend. Explore further details on [Keras](https://keras.io/keras_3/).

**Note**: For a deeper understanding of KerasNLP, refer to the [KerasNLP guides](https://keras.io/keras_nlp/).


# ðŸ“š | Import Libraries
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
os.environ["KERAS_BACKEND"] = "jax"  # or "tensorflow" or "torch"

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import plotly.express as px
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## Library Version
```

---The following area is a Code cell (cell numver is 3)---
```python
print("TensorFlow:", tf.__version__)
print("Keras:", keras.__version__)
print("KerasNLP:", keras_nlp.__version__)
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# âš™ï¸ | Configuration
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:
    seed = 42  # Random seed
    preset = "deberta_v3_extra_small_en" # Name of pretrained models
    sequence_length = 512  # Input sequence length
    epochs = 3 # Training epochs
    batch_size = 16  # Batch size
    scheduler = 'cosine'  # Learning rate scheduler
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# â™»ï¸ | Reproducibility 
Sets value for random seed to produce similar result in each run.
```

---The following area is a Code cell (cell numver is 7)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# ðŸ§® | Mixed Precision

In this notebook, we will use mixed precision instead of float32 precision for training and inference to reduce GPU memory usage. This will ultimately allow us to use larger batch sizes, thus reducing our training and inference time.
```

---The following area is a Code cell (cell numver is 9)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# ðŸ“ | Dataset Path
```

---The following area is a Code cell (cell numver is 11)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# ðŸ“– | Meta Data 

The competition dataset comprises user interactions from the ChatBot Arena. In each interaction, a judge presents one or more prompts to two different large language models and then indicates which model provided the more satisfactory response. The training data contains `55,000` rows, with an expected `25,000` rows in the test set.

## Files

### `train.csv`
- `id`: Unique identifier for each row.
- `model_[a/b]`: Model identity, present in train.csv but not in test.csv.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.
- `winner_model_[a/b/tie]`: Binary columns indicating the judge's selection (ground truth target).

### `test.csv`
- `id`: Unique identifier for each row.
- `prompt`: Input prompt given to both models.
- `response_[a/b]`: Model_[a/b]'s response to the prompt.

> Note that each interaction may have multiple prompts and responses, but this notebook will use only **one prompt per interaction**. You can choose to use all prompts and responses. Additionally, prompts and responses in the dataframe are provided as string-formatted lists, so they need to be converted to literal lists using `eval()`.


## Train Data
```

---The following area is a Code cell (cell numver is 13)---
```python
# Load Train Data
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# Sample data
# df = df.sample(frac=0.10)

# Take the first prompt and its associated response
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Label conversion
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# Show Sample
df.head()
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## Test Data
```

---The following area is a Code cell (cell numver is 15)---
```python
# Load Test Data
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# Take the first prompt and response
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# Show Sample
test_df.head()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
## Contextualize Response with Prompt

In our approach, we will contextualize each response with the prompt instead of using a single prompt for all responses. This means that for each response, we will provide the model with the same set of prompts combined with their respective response (e.g., `(P + R_A)`, `(P + R_B)`, etc.). This approach is similar to the multiple-choice question task in NLP.

> Note that some prompts and responses may not be encoded with `utf-8`, resulting in errors when creating the dataloader. In such cases, we will replace them with an empty string.
```

---The following area is a Code cell (cell numver is 17)---
```python
# Define a function to create options based on the prompt and choices
def make_pairs(row):
    row["encode_fail"] = False
    try:
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        prompt = ""
        row["encode_fail"] = True

    try:
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        response_a = ""
        row["encode_fail"] = True

    try:
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        response_b = ""
        row["encode_fail"] = True
        
    row['options'] = [f"Prompt: {prompt}\n\nResponse: {response_a}",  # Response from Model A
                      f"Prompt: {prompt}\n\nResponse: {response_b}"  # Response from Model B
                     ]
    return row

```

---The following area is a Code cell (cell numver is 18)---
```python
df = df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(df.head(2))  # Display the first 2 rows of df

test_df = test_df.apply(make_pairs, axis=1)  # Apply the make_pairs function to each row in df
display(test_df.head(2))  # Display the first 2 rows of df
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## Encoding Fail Statistics

Let's examine how many samples have encoding issues. From the code below, we can see that only $1\%$ of the samples failed to be encoded, while $99\%$ of the samples don't have any issues. A similar pattern can be expected for the test data as well. Thus, considering empty strings for this small portion of the data will not have much impact on our training and inference.
```

---The following area is a Code cell (cell numver is 20)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# ðŸŽ¨ | Exploratory Data Analysis (EDA)

## LLM Distribution
```

---The following area is a Code cell (cell numver is 22)---
```python
model_df = pd.concat([df.model_a, df.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Create a bar plot with custom styling using Plotly
fig = px.bar(counts, x='LLM', y='Count',
             title='Distribution of LLMs',
             color='Count', color_continuous_scale='viridis')

fig.update_layout(xaxis_tickangle=-45)  # Rotate x-axis labels for better readability

fig.show()

```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
## Winning Distribution
```

---The following area is a Code cell (cell numver is 24)---
```python
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

fig = px.bar(counts, x='Winner', y='Win Count',
             title='Winner distribution for Train Data',
             labels={'Winner': 'Winner', 'Win Count': 'Win Count'},
             color='Winner', color_continuous_scale='viridis')

fig.update_layout(xaxis_title="Winner", yaxis_title="Win Count")

fig.show()

```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# ðŸ”ª | Data Split

In the code snippet provided below, we will divide the existing data into training and validation using a stratification of `class_label` column.
```

---The following area is a Code cell (cell numver is 26)---
```python
from sklearn.model_selection import train_test_split  # Import package

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# ðŸ½ï¸ | Preprocessing

**What it does:** The preprocessor takes input strings and transforms them into a dictionary (`token_ids`, `padding_mask`) containing preprocessed tensors. This process starts with tokenization, where input strings are converted into sequences of token IDs.

**Why it's important:** Initially, raw text data is complex and challenging for modeling due to its high dimensionality. By converting text into a compact set of tokens, such as transforming `"The quick brown fox"` into `["the", "qu", "##ick", "br", "##own", "fox"]`, we simplify the data. Many models rely on special tokens and additional tensors to understand input. These tokens help divide input and identify padding, among other tasks. Making all sequences the same length through padding boosts computational efficiency, making subsequent steps smoother.

Explore the following pages to access the available preprocessing and tokenizer layers in **KerasNLP**:
- [Preprocessing](https://keras.io/api/keras_nlp/preprocessing_layers/)
- [Tokenizers](https://keras.io/api/keras_nlp/tokenizers/)
```

---The following area is a Code cell (cell numver is 28)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # Name of the model
    sequence_length=CFG.sequence_length, # Max sequence length, will be padded if shorter
)
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
Now, let's examine what the output shape of the preprocessing layer looks like. The output shape of the layer can be represented as $(num\_responses, sequence\_length)$.
```

---The following area is a Code cell (cell numver is 30)---
```python
outs = preprocessor(df.options.iloc[0])  # Process options for the first row

# Display the shape of each processed output
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
We'll use the `preprocessing_fn` function to transform each text option using the `dataset.map(preprocessing_fn)` method.
```

---The following area is a Code cell (cell numver is 32)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # Preprocess text
    return (text, label) if label is not None else text  # Return processed text and label if available
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# ðŸš | DataLoader

The code below sets up a robust data flow pipeline using `tf.data.Dataset` for data processing. Notable aspects of `tf.data` include its ability to simplify pipeline construction and represent components in sequences.

To learn more about `tf.data`, refer to this [documentation](https://www.tensorflow.org/guide/data).
```

---The following area is a Code cell (cell numver is 34)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNE option
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # Create slices
    ds = tf.data.Dataset.from_tensor_slices(slices)  # Create dataset from slices
    ds = ds.cache() if cache else ds  # Cache dataset if enabled
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # Map preprocessing function
    opt = tf.data.Options()  # Create dataset options
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # Shuffle dataset if enabled
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # Set dataset options
    ds = ds.batch(batch_size, drop_remainder=False)  # Batch dataset
    ds = ds.prefetch(AUTO)  # Prefetch next batch
    return ds  # Return the built dataset
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
## Build Train/Valid Dataloader
```

---The following area is a Code cell (cell numver is 36)---
```python
# Train
train_texts = train_df.options.tolist()  # Extract training texts
train_labels = train_df.class_label.tolist()  # Extract training labels
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# Valid
valid_texts = valid_df.options.tolist()  # Extract validation texts
valid_labels = valid_df.class_label.tolist()  # Extract validation labels
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# âš“ | LR Schedule

Implementing a learning rate scheduler is crucial for transfer learning. The learning rate initiates at `lr_start` and gradually tapers down to `lr_min` using various techniques, including:
- `step`: Lowering the learning rate in step-wise manner resembling stairs.
- `cos`: Utilizing a cosine curve to gradually reduce the learning rate.
- `exp`: Exponentially decreasing the learning rate.

**Importance:** A well-structured learning rate schedule is essential for efficient model training, ensuring optimal convergence and avoiding issues such as overshooting or stagnation.
```

---The following area is a Code cell (cell numver is 38)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # Learning rate update function
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # Plot lr curve if plot is True
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('epoch'); plt.ylabel('lr')
        plt.title('LR Scheduler')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # Create lr callback
```

---The following area is a Code cell (cell numver is 39)---
```python
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# ðŸ’¾ | Model Checkpointing

The following code will create a callback that will save the best checkpoint of the model during training, which we will use for inference in the submission.
```

---The following area is a Code cell (cell numver is 41)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')  # Get Model checkpoint callback
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
# ðŸ“ | Metric

The metric for this competition is **Log Loss**. This metric can be expressed mathematically as,

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$

where $ N $ is the number of samples, $ y_i $ is the true label, and $ p_i $ is the predicted probability of the sample belonging to the positive class.

Note that this metric is similar to categorical cross entropy widely used in classification tasks. Thus, we don't need to implement the loss from scratch. As the Keras library already has an implementation of this metric, we will simply use the metric to monitor performance of our model.
```

---The following area is a Code cell (cell numver is 43)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
# ðŸ¤– | Modeling

The `KerasNLP` library provides various NLP model architectures such as `Bert`, `Roberta`, `DebertaV3`, and more. While this notebook focuses on `DebertaV3`, you can explore others in the [KerasNLP documentation](https://keras.io/api/keras_nlp/models/). For a deeper understanding, refer to the [getting started guide](https://keras.io/guides/keras_nlp/getting_started/).

Our approach utilizes `keras_nlp.models.DebertaV3Classifier` to process each prompt and response pair, generating output embeddings. We then concatenate these embeddings and pass them through a Pooling layer and a classifier to obtain logits, followed by a `softmax` function for the final output.

When dealing with multiple responses, we use a weight-sharing strategy. This means we provide the model with one response at a time along with the prompt `(P + R_A)`, `(P + R_B)`, etc., using the same model weights for all responses. After obtaining embeddings for all responses, we concatenate them and apply average pooling. Next, we use a `Linear/Dense` layer along with the `Softmax` function as the classifier for the final result. Providing all responses at once would increase text length and complicate model handling. Note that, in the classifier, we use 3 classes for `winner_model_a`, `winner_model_b`, and `draw` cases.

The diagram below illustrates this approach:

<div align="center">
    <img src="https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png">
</div>

From a coding perspective, note that we use the same model for all responses with shared weights, contrary to the separate models implied in the diagram.
```

---The following area is a Code cell (cell numver is 45)---
```python
# Define input layers
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# Create a DebertaV3Classifier backbone
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# Compute embeddings for first response: (P + R_A) using backbone
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# Compute embeddings for second response: (P + R_B), using the same backbone
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# Compute final output
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# Compile the model with optimizer, loss, and metrics
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Markdown cell (cell numver is 46)---
```markdown
### Model Summary
```

---The following area is a Code cell (cell numver is 47)---
```python
model.summary()
```

---The following area is a Markdown cell (cell numver is 48)---
```markdown
### Model Plot

In the model graph below, it may seem there are **four** inputs, but actually, there are **two** as discussed before. Our input consists of two parts, one for each response. However, for each input, we have `token_ids` and `padding_mask`, which makes it look like we have four inputs, but in reality, we have two inputs.
```

---The following area is a Code cell (cell numver is 49)---
```python
# Currently throwing error !! [probably library or env issue, so hopefully will be fixed soon]

# keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)
```

---The following area is a Markdown cell (cell numver is 50)---
```markdown
# ðŸš‚ | Training
```

---The following area is a Code cell (cell numver is 51)---
```python
# Start training the model
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
## Load Best Model

After training, let's load the weight with best result to get the best performance.
```

---The following area is a Code cell (cell numver is 53)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Markdown cell (cell numver is 54)---
```markdown
# ðŸ§ª | Prediction
```

---The following area is a Code cell (cell numver is 55)---
```python
# Build test dataset
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 56)---
```python
# Make predictions using the trained model on test data
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Markdown cell (cell numver is 57)---
```markdown
# ðŸ“¬ | Submission

Following code will prepare the submission file.
```

---The following area is a Code cell (cell numver is 58)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

---The following area is a Markdown cell (cell numver is 59)---
```markdown
# ðŸ”­ | Future Directions

In this notebook, we've achieved a good score with a small model and modest token length. But there's plenty of room to improve. Here's how:

1. Try bigger models like `Deberta-Base` or `Deberta-Small`, or even LLMs like `Gemma`.
2. Increase max token length to reduce loss of data.
3. Use a five-fold cross-validation and ensemble to make the model robust and get better scores.
4. Add augmentation like shuffling response orders for more robust performance.
5. Train for more epochs.
6. Tune the learning rate scheduler.

# ðŸ“Œ | Reference

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 94, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
## llama3-8b

shout out to:
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python

import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("Sorry - GPU required!")

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
MAX_LENGTH = 1024
BATCH_SIZE = 8
DEVICE = torch.device("cuda")    

# # Prepare Data 

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# concatenate strings in list
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))

# Prepare text for model
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
print(test['text'][0])

# # Tokenize


tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# Move tensors to CPU and convert them to lists
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]

# # Load model 
# We load 1 model on each gpu.  

# BitsAndBytes configuration
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False)

# Load base model on GPU 0
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:0')
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# Load base model on GPU 1
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')
base_model_1.config.pad_token_id = tokenizer.pad_token_id

# Now, we have sucessfully loaded one model on each GPU!

# # Load weights 

# LoRa configuration
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.10,
    bias='none',
    inference_mode=True,
    task_type=TaskType.SEQ_CLS,
    target_modules=['o_proj', 'v_proj'])

# Get peft
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
# Load weights
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()

model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()

# Trainable Parameters
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()

# # Inference
# 

import gc
gc.collect()

def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df

st = time.time()

N_SAMPLES = len(data)

# Split the data into two subsets
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# Function to run inference in a thread
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# Dictionary to store results from threads
results = {}

# start threads
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# Wait for all threads to finish
t0.join()
t1.join()

# Combine results back into the original DataFrame
data = pd.concat([results[0], results[1]], axis=0)

print(f"Processing complete. Total time: {time.time() - st}")

# Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) 

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 3)---
```python
llama_preds = data[TARGETS].values
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 5)---
```python
TAG = 'lmsys-chatbot-arena'

import os
RUNPOD = os.path.exists('/workspace/')
KAGGLE = not RUNPOD
if KAGGLE: print('kaggle')
```

---The following area is a Code cell (cell numver is 6)---
```python
try:
    import pandas as pd
except:
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
    
```

---The following area is a Code cell (cell numver is 7)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json
        !chmod 600 /root/.kaggle/kaggle.json

    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    
```

---The following area is a Code cell (cell numver is 8)---
```python
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 9)---
```python
import os
import io
import gc
import time
import json
import random
import pickle
import zipfile
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers

os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 10)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')

print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 11)---
```python
params = {}
if False:#len(test) < 10: 
    pass;
    params['subsample'] = 30
else:
    # params['subsample'] = 2
    params['fold'] = -1


params['n_epochs'] = 1
params['n_lgb'] = 1
params['model'] = 'microsoft/deberta-v3-small'
```

---The following area is a Code cell (cell numver is 12)---
```python
# params = {}
FULL = params.get('fold', 0) < 0
N_FOLDS = int(params.get('n_folds', 3)); 
FOLD = int(params.get('fold', 0))
SEED = int(params.get('seed', 3))
SS = int(params.get('subsample', 1))

print(N_FOLDS, FOLD, SEED, SS)

```

---The following area is a Code cell (cell numver is 13)---
```python
from sklearn.model_selection import StratifiedKFold

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() 
```

---The following area is a Code cell (cell numver is 14)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x
```

---The following area is a Code cell (cell numver is 15)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())
```

---The following area is a Code cell (cell numver is 16)---
```python
def len_join_strings_j(x):
    x = json.loads(x)
    return len_join_strings(x)
```

---The following area is a Code cell (cell numver is 17)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 18)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False
INFER = True # or KAGGLE 
SAVE = False
```

---The following area is a Code cell (cell numver is 19)---
```python
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 20)---
```python
LGB = True
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 21)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 22)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())
    x = symlog(x)
    return x

def get_features(df):
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except: pass

    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)
            
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])
    return np.hstack([v, extras])
    # return v

```

---The following area is a Code cell (cell numver is 23)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 24)---
```python
if INFER and params.get('n_lgb', 1) > 0:
    df = test
    yps = []; b = 1000
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])
        ypms = []
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))
        yps.append(np.stack(ypms).mean(0))
        # break;
        print('.', end = '')
        
        if len(yps) % 2 == 0:
            gc.collect()
    print()

    yp = np.concatenate(yps)
```

---The following area is a Code cell (cell numver is 25)---
```python
lgb_preds = yp
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## Blend predictions

$\operatorname{preds} = 0.2 \cdot \operatorname{lgbm boosting preds} + 0.8 \cdot \operatorname{llama preds}$
```

---The following area is a Code cell (cell numver is 27)---
```python
lgb_wt = 0.2 
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds
```

---The following area is a Code cell (cell numver is 28)---
```python
out = pd.DataFrame(preds, 
                index = df.id, 
                    columns = train.columns[-3:])
display(out.head())
```

---The following area is a Code cell (cell numver is 29)---
```python
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 95, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
#  Llama-3 8b [TPU Train]

Learning to train llms on tpu, Hope this will help you too!

Notebook inspired from:

* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)


Prerequisite: Access to using llama-3

Note: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)

Please upvote if you learn or find this helpful!

# Import libs
```

---The following area is a Code cell (cell numver is 1)---
```python
# Install libs
!pip install -qq peft==0.6.0
!pip install -qq bitsandbytes==0.41.1
!pip install -qq accelerate==0.24.1
!pip install -qq transformers==4.35.0
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q 
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q
!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT
!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
import torch.nn.functional as F

import torch_xla.debug.profiler as xp
import torch_xla.core.xla_model as xm
import torch_xla.experimental.xla_sharding as xs
import torch_xla.runtime as xr

xr.use_spmd()

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor
from torch_xla.experimental.xla_sharding import Mesh
from spmd_util import partition_module

tqdm.pandas()

print(f'Torch Version: {torch.__version__}')
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# Configs
```

---The following area is a Code cell (cell numver is 4)---
```python
class CFG:
    NUM_EPOCHS = 1
    BATCH_SIZE = 16
    DROPOUT = 0.05 
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    SEED = 2024 
    MAX_LENGTH = 1024 
    NUM_WARMUP_STEPS = 128
    LR_MAX = 5e-5 
    NUM_LABELS = 3 
    LORA_RANK = 4
    LORA_ALPHA = 8
    LORA_MODULES = ['o_proj', 'v_proj']
    
DEVICE = xm.xla_device() # Initialize TPU Device
```

---The following area is a Code cell (cell numver is 5)---
```python
def set_seeds(seed):
    """Set seeds for reproducibility """
    os.environ['PYTHONHASHSEED'] = str(seed)
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        
    # Set seed for all TPU cores
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# Tokenizer
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# save tokenizer to load offline during inference
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 8)---
```python
# Utility function giving token length
def get_token_lengths(texts):
    # tokenize and receive input_ids for reach text
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # return length of inputs_ids for each text
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# Prepare train
```

---The following area is a Code cell (cell numver is 10)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# Drop 'Null' for training
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")
print('Total train samples: ', len(train))
```

---The following area is a Code cell (cell numver is 11)---
```python
train.head(5)
```

---The following area is a Code cell (cell numver is 12)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])
```

---The following area is a Code cell (cell numver is 13)---
```python
# Train with only take 50% train dataset
train = train[:int(len(train) * 0.5)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])

# prepare label for model
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# Display data
display(train.head())
```

---The following area is a Code cell (cell numver is 14)---
```python
train.label.value_counts()
```

---The following area is a Code cell (cell numver is 15)---
```python
# token Count
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 16)---
```python
# get length of tokens which covers 90% of data, we'll still take 1024 length!
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# Tokenize
```

---The following area is a Code cell (cell numver is 18)---
```python
# Tokenize Data
tokens = tokenizer(
    train['text'].tolist(), 
    padding='max_length', 
    max_length=CFG.MAX_LENGTH, 
    truncation=True, 
    return_tensors='np')

# Input IDs are the token IDs
INPUT_IDS = tokens['input_ids']
# Attention Masks to Ignore Padding Tokens
ATTENTION_MASKS = tokens['attention_mask']
# Label of Texts
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')
print(f'LABELS shape: {LABELS.shape}')
```

---The following area is a Code cell (cell numver is 19)---
```python
def train_dataset(batch_size):
    N_SAMPLES = LABELS.shape[0]
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))
    while True:
        # Shuffle Indices
        np.random.shuffle(IDXS)
        # Iterate Over All Indices Once
        for idxs in IDXS.reshape(-1, batch_size):
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output
            
            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)
            xs.mark_sharding(input_ids, mesh, (0, 1))
            xs.mark_sharding(attention_mask, mesh, (0, 1))
            xs.mark_sharding(labels, mesh, (0, 1))
            
            yield input_ids, attention_mask, labels

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# Load Model
```

---The following area is a Code cell (cell numver is 21)---
```python
# Load model for classification with 3 target label
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,
    num_labels=CFG.NUM_LABELS,
    torch_dtype=torch.bfloat16)

base_model.config.pretraining_tp = 1 

# Assign Padding TOKEN
base_model.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# Low-Rank Adaptation [LORA]
```

---The following area is a Code cell (cell numver is 23)---
```python
lora_config = LoraConfig(
    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices
    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations
    lora_dropout= CFG.DROPOUT, 
    bias='none',
    inference_mode=False,
    task_type=TaskType.SEQ_CLS,
    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection
```

---The following area is a Code cell (cell numver is 24)---
```python
# Create LoRa Model
model = get_peft_model(base_model, lora_config)
# Trainable Parameters
model.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 25)---
```python
# Number of TPU Nodes
num_devices = xr.global_runtime_device_count()
mesh_shape = (1, num_devices, 1)
device_ids = np.array(range(num_devices))
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))
# distribute model
partition_module(model, mesh)

print(f'num_devices: {num_devices}')
```

---The following area is a Code cell (cell numver is 26)---
```python
# Verfy The Trainable Layers
MODEL_LAYERS_ROWS = []
TRAINABLE_PARAMS = []
N_TRAINABLE_PARAMS = 0

for name, param in model.named_parameters():
    # Layer Parameter Count
    n_parameters = int(torch.prod(torch.tensor(param.shape)))
    # Only Trainable Layers
    if param.requires_grad:
        # Add Layer Information
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,
            'name': name,
            'dtype': param.data.dtype,
        })
        # Append Trainable Parameter
        TRAINABLE_PARAMS.append({ 'params': param })
        # Add Number Of Trainable Parameters"
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))

print(f"""
===============================
N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}
N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}
===============================
""")
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# Training
```

---The following area is a Code cell (cell numver is 28)---
```python
# LR & Optimizer
N_SAMPLES = len(train)
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)

# Cosine Learning Rate With Warmup
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)

print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')
```

---The following area is a Code cell (cell numver is 29)---
```python
# Set the data type for the optimizer's state (e.g., momentum buffers)
for state in OPTIMIZER.state.values():
    for k, v in state.items():
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:
            state[v] = v.to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 30)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)

print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')
print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')
print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')
```

---The following area is a Code cell (cell numver is 31)---
```python
%%time
# Dummy Prediction
with torch.no_grad():
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)
    
print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')
```

---The following area is a Code cell (cell numver is 32)---
```python
# Put Model In Train Mode
model.train()

# Loss Function, Cross Entropy
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)
```

---The following area is a Code cell (cell numver is 33)---
```python
st = time()
warnings.filterwarnings("error")
METRICS = {
    'loss': [],
    'accuracy': {'y_true': [], 'y_pred': [] }}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):
    ste = time()
    for step in range(STEPS_PER_EPOCH):
        # Zero Out Gradients
        OPTIMIZER.zero_grad()
        
        # Get Batch
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # Forward Pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # Logits Float32
        logits = outputs.logits.to(dtype=torch.float32)
        
        # Backward Pass
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))
        loss.backward()
        
        # optimizer step
        OPTIMIZER.step()
        xm.mark_step()
        
        # Update Learning Rate Scheduler
        lr_scheduler.step()
        
        # Update Metrics And Progress Bar
        METRICS['loss'].append(float(loss))
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()
        
        if (step + 1) % 200 == 0:  
            metrics = 'Âµ_loss: {:.3f}'.format(np.mean(METRICS['loss']))
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])
            metrics += ', Âµ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))
            lr = OPTIMIZER.param_groups[0]['lr']
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')
    
    print(f'\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )

    # If stopped, and to continue training in future on tpu we save model and optimizer
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    
    
    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')
```

---The following area is a Code cell (cell numver is 34)---
```python
plt.figure(figsize=(15, 6))
plt.plot(METRICS['loss'])    
plt.xlabel('Step per epoch')
plt.ylabel('Loss')
plt.title('Loss Plot step per epoch')    
plt.show()
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# Save Model
```

---The following area is a Code cell (cell numver is 36)---
```python
model = model.cpu()
torch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
# Conclusion 

There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!
```

** @@@ Jupyter Notebook numver 96, the number of votes :0 @@@ **

---The following area is a Code cell (cell numver is 0)---
```python
# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using "Save & Run All" 
# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session
```

---The following area is a Code cell (cell numver is 1)---
```python
# !pip install cupy-cuda11x>=12.0.0
# !pip install packaging>=22
# !pip install shapely>=2.0.1
# !pip install numpy<1.26
# !pip install scipy<1.12
!pip install transformers datasets accelerate peft

```

---The following area is a Code cell (cell numver is 2)---
```python
# You only need to run this once per machine
!pip install -q -U bitsandbytes
# !pip install -q -U git+https://github.com/huggingface/transformers.git
# !pip install -q -U git+https://github.com/huggingface/peft.git
# !pip install -q -U git+https://github.com/huggingface/accelerate.git
# !pip install -q -U datasets scipy ipywidgets
!pip install datasets
```

---The following area is a Code cell (cell numver is 3)---
```python
from datasets import load_dataset

train_dataset = load_dataset('csv', data_files='/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 4)---
```python
from huggingface_hub import notebook_login

notebook_login()
```

---The following area is a Code cell (cell numver is 5)---
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config,device_map="auto")
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    model_max_length=512,
    padding_side="left",
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data = pd.read_csv('../input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('../input/lmsys-chatbot-arena/test.csv')
submission_data = pd.read_csv('../input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
train_data.shape,test_data.shape,submission_data.shape
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

def trim_endings(custom_string):
    return custom_string[:-2][2:]

def count_newlines(custom_string):
    return custom_string.count('\\n')

def word_counts(custom_string):
    return len(custom_string.split())


def apply_transformations(df):
    
    df['prompt'] =  df['prompt'].map(process)
    df['response_a'] =  df['response_a'].map(process)
    df['response_b'] =  df['response_b'].map(process)
    
    df['prompt'] =  df['prompt'].map(trim_endings)
    df['response_a'] =  df['response_a'].map(trim_endings)
    df['response_b'] =  df['response_b'].map(trim_endings)
    
    df['res_a_line_count'] = df['response_a'].map(count_newlines).astype(str)
    df['res_b_line_count'] = df['response_b'].map(count_newlines).astype(str)
    
    df['res_a_word_count'] = df['response_a'].map(word_counts).astype(str)
    df['res_b_word_count'] = df['response_b'].map(word_counts).astype(str)
    
    return df

def create_summary(df):
    df['summary'] = 'User prompt: ' + df['prompt'] +  '\n\n Model A :\n' + df['response_a'] +'\n\n Model A length:\n' + df['res_a_word_count'] +'\n\n Model A Line Count:\n' + df['res_a_line_count'] +'\n\nModel B:\n'  + df['response_b'] +'\n\n \n\nModel B length:\n'  + df['res_b_word_count'] +'\n\n \n\nModel B Line Count:\n'  + df['res_b_line_count']
    return df
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data = apply_transformations(train_data)
train_data = create_summary(train_data)
test_data = apply_transformations(test_data)    
test_data = create_summary(test_data)
```

---The following area is a Code cell (cell numver is 11)---
```python
train_data['summary'][2]
```

---The following area is a Code cell (cell numver is 12)---
```python
train_data['labels'] = train_data[['winner_model_a','winner_model_b','winner_tie']].idxmax(axis=1)
train_data['labels']=train_data['label'].astype('category')
train_data['target']=train_data['label'].cat.codes
```

---The following area is a Code cell (cell numver is 13)---
```python
category_map = {code: category for code, category in enumerate(train_data['labels'].cat.categories)}
category_map
```

---The following area is a Code cell (cell numver is 14)---
```python
SEED = 7920
```

---The following area is a Code cell (cell numver is 15)---
```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score
train_df = train_data[['id','summary','target']]
test_df = train_data[['id','summary']]

df_train,df_val = train_test_split(train_df,test_size=0.2,random_state=SEED,stratify=train_df['target'])
```

---The following area is a Code cell (cell numver is 16)---
```python
from datasets import Dataset, DatasetDict
dataset_train = Dataset.from_pandas(df_train,preserve_index=False)
dataset_val = Dataset.from_pandas(df_val,preserve_index=False)
dataset_test = Dataset.from_pandas(test_df,preserve_index=False)
```

---The following area is a Code cell (cell numver is 17)---
```python
 dataset_train_shuffled = dataset_train.shuffle(seed=SEED)
```

---The following area is a Code cell (cell numver is 18)---
```python
dataset = DatasetDict({
    'train': dataset_train_shuffled,
#     'val': dataset_val,
#     'test': dataset_test
})
dataset
```

---The following area is a Code cell (cell numver is 19)---
```python
df_train.target.value_counts(normalize=True)
```

---The following area is a Code cell (cell numver is 20)---
```python
class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()
class_weights=torch.tensor(class_weights)
class_weights=class_weights/class_weights.sum()
class_weights
```

---The following area is a Code cell (cell numver is 21)---
```python
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    model_max_length=512,
    padding_side="left",
    add_eos_token=True)

tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 22)---
```python
MAX_LEN = 512
col_to_delete = ['summary']

def llama_preprocessing_function(examples):
    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)

tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)
tokenized_datasets = tokenized_datasets.rename_column("target", "labels")
tokenized_datasets.set_format("torch")
```

---The following area is a Code cell (cell numver is 23)---
```python
from peft import prepare_model_for_kbit_training

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)
```

---The following area is a Code cell (cell numver is 24)---
```python
def print_trainable_parameters(model):
    """
    Prints the number of trainable parameters in the model.
    """
    trainable_params = 0
    all_param = 0
    for _, param in model.named_parameters():
        all_param += param.numel()
        if param.requires_grad:
            trainable_params += param.numel()
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )

```

---The following area is a Code cell (cell numver is 25)---
```python
print(model)
```

---The following area is a Code cell (cell numver is 26)---
```python
from accelerate import FullyShardedDataParallelPlugin, Accelerator
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig

fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),
)

accelerator = Accelerator(fsdp_plugin=fsdp_plugin)
```

---The following area is a Code cell (cell numver is 27)---
```python
from peft import LoraConfig, get_peft_model

config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=[
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",
    lora_dropout=0.05,  # Conventional
    task_type="CAUSAL_LM",
)

model = get_peft_model(model, config)
print_trainable_parameters(model)

# Apply the accelerator. You can comment this out to remove the accelerator.
model = accelerator.prepare_model(model)
```

---The following area is a Code cell (cell numver is 28)---
```python
!pip install -q wandb -U

import wandb, os
wandb.login()

wandb_project = "mistral-finetune"
if len(wandb_project) > 0:
    os.environ["WANDB_PROJECT"] = wandb_project
```

---The following area is a Code cell (cell numver is 29)---
```python
if torch.cuda.device_count() > 1: # If more than 1 GPU
    model.is_parallelizable = True
    model.model_parallel = True
```

---The following area is a Code cell (cell numver is 30)---
```python
!nvcc --version
!pip uninstall torch torchvision torchaudio -y
!pip install torch==2.0.0+cu121 torchvision==0.15.0+cu121 torchaudio==2.0.0+cu121 -f https://download.pytorch.org/whl/nightly/cu121/torch_nightly.html

```

---The following area is a Code cell (cell numver is 31)---
```python
# Uninstall existing versions of torch, torchvision, and torchaudio
!pip uninstall torch torchvision torchaudio -y

# Install compatible versions with CUDA 11.2 support
!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu112/torch_stable.html

```

---The following area is a Code cell (cell numver is 32)---
```python
import os
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'

```

---The following area is a Code cell (cell numver is 33)---
```python
import torch

seed = 7920  # Replace with your desired seed

if torch.cuda.is_available():
    generator = torch.Generator(device='cuda').manual_seed(seed)
else:
    generator = torch.Generator().manual_seed(seed)

```

---The following area is a Code cell (cell numver is 34)---
```python
if torch.cuda.is_available():
  generator = torch.Generator('cuda').manual_seed(seed)
else:
  generator = torch.Generator().manual_seed(seed)
```

---The following area is a Code cell (cell numver is 35)---
```python
import transformers
class CustomTrainer(transformers.Trainer):
    def __init__(self, *args, class_weights=None, **kwargs):
        super().__init__(*args, **kwargs)
        # Ensure label_weights is a tensor
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)
        else:
            self.class_weights = None

    def compute_loss(self, model, inputs, return_outputs=False):
        # Extract labels and convert them to long type for cross_entropy
        labels = inputs.pop("labels").long()

        # Forward pass
        outputs = model(**inputs)

        # Extract logits assuming they are directly outputted by the model
        logits = outputs.get('logits')

        # Compute custom loss with class weights for imbalanced data handling
        if self.class_weights is not None:
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)
        else:
            loss = F.cross_entropy(logits, labels)

        return (loss, outputs) if return_outputs else loss
```

---The following area is a Code cell (cell numver is 36)---
```python
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}
```

---The following area is a Code cell (cell numver is 37)---
```python
training_args = transformers.TrainingArguments(
    output_dir = 'model_classification',
    learning_rate = 1e-4,
    per_device_train_batch_size = 8,
    per_device_eval_batch_size = 8,
    num_train_epochs = 1,
    weight_decay = 0.01,
    evaluation_strategy = 'epoch',
    save_strategy = 'epoch',
    load_best_model_at_end = True,
	logging_steps=10,
	max_steps=100,
    fp16=True,
    push_to_hub=False
)
```

---The following area is a Code cell (cell numver is 38)---
```python
collate_fn = transformers.DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 39)---
```python
trainer = CustomTrainer(
    model = model,
    args = training_args,
    train_dataset = tokenized_datasets['train'],
    tokenizer = tokenizer,
    data_collator = collate_fn,
    compute_metrics = compute_metrics,
    class_weights=class_weights,
)
```

---The following area is a Code cell (cell numver is 40)---
```python
!pip install torch
```

---The following area is a Code cell (cell numver is 41)---
```python
import transformers
from datetime import datetime
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'

# Check if CUDA is available
print("CUDA Available:", torch.cuda.is_available())
project = "mistral-finetune"
base_model_name = "mistral"
run_name = base_model_name + "-" + project
output_dir = "./" + run_name

tokenizer.pad_token = tokenizer.eos_token

trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_datasets['train'],
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=5,
        per_device_train_batch_size=2,
        gradient_checkpointing=True,
        gradient_accumulation_steps=4,
        max_steps=1000,
        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate
        logging_steps=50,
        fp16=False,
        optim="paged_adamw_8bit",
        logging_dir="./logs",        # Directory for storing logs
        save_strategy="steps",       # Save the model checkpoint every logging step
        save_steps=50,                # Save checkpoints every 50 steps
        evaluation_strategy="steps", # Evaluate the model every logging step
        eval_steps=50,               # Evaluate and save checkpoints every 50 steps
        do_eval=True,                # Perform evaluation at the end of training
        report_to="wandb",           # Comment this out if you don't want to use weights & baises
        run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}"          # Name of the W&B run (optional)
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

model.config.use_cache = False  # silence the warnings. Please re-enable for inference!
trainer.train()
```

---The following area is a Code cell (cell numver is 42)---
```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

base_model_id = "mistralai/Mistral-7B-v0.1"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,  # Mistral, same as before
    quantization_config=bnb_config,  # Same quantization config as before
    device_map="auto",
    trust_remote_code=True,
)

eval_tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,
    add_bos_token=True,
    trust_remote_code=True,
)
```

---The following area is a Code cell (cell numver is 43)---
```python
from peft import PeftModel

ft_model = PeftModel.from_pretrained(base_model, "mistral-viggo-finetune/checkpoint-1000")
```

---The following area is a Code cell (cell numver is 44)---
```python
eval_prompt = """Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']

### Target sentence:
Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?

### Meaning representation:
"""

model_input = eval_tokenizer(eval_prompt, return_tensors="pt").to("cuda")

ft_model.eval()
with torch.no_grad():
    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))
```

