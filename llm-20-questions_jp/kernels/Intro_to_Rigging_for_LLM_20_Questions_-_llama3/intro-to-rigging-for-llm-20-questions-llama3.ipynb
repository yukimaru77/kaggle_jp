{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8306f02",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションのためのベースライン提出物を作成するためのスターターノートブックです。主な目的は、Pythonパッケージ「rigging」を使用して、LLM（大規模言語モデル）を効率的にテーマに基づく質問応答のゲーム（20の質問）に活用することです。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "ノートブックでは、参加者が20の質問ゲームを戦略的にプレイするためのLLMエージェントを作成するためのフレームワークの設定と実装が行われています。このゲームは、限られた質問数で秘密のアイテムを特定しようとするもので、質問を生成する「質問者LLM」と「はい/いいえ」で回答する「回答者LLM」の協力が不可欠です。ノートブックでは、これらのエージェントを構築するためのツールや方法を示しています。\n",
    "\n",
    "### 使用している手法やライブラリ\n",
    "1. **rigging**：LLMとのインタラクションを簡素化するフレームワークで、異なるLLMバックエンドの交換や、LLMクエリの検証・再試行が可能です。\n",
    "2. **vLLM**：モデルをローカル環境でホスティングするためのサーバーで、特定のLLMモデルを読み込むために使用されます。\n",
    "3. **Hugging Face**：`snapshot_download`関数を利用して、モデルの重みをダウンロードします。\n",
    "4. **Pydantic**：出力のバリデーションとシリアライズを提供し、データモデルを定義するために使用されます。\n",
    "\n",
    "ノートブックでは、エージェントのセットアップ、モデルの重みのダウンロード、質問・回答の生成、回答のバリデーションといった様々なプロセスを示し、さらに最終提出物としてPythonスクリプトを作成し、モデルデータを圧縮して提出する際の手順も説明しています。\n",
    "\n",
    "このノートブックは参加者にさまざまな実践的な手法やツールを提供し、効率的な情報収集と演繹的推論が求められる競技において、大規模言語モデルの適用を促進するための基盤を築くことを目指しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d485a4",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、初心者がつまずく可能性がある、ノートブックに関連する専門用語の解説です。\n",
    "\n",
    "1. **リギング（Rigging）**:\n",
    "   - 専用の軽量なフレームワークで、さまざまなLLM（大規模言語モデル）とのインタラクションを管理するために使用されます。LLMをプロダクション環境で使う際のパイプラインを作成することを容易にする目的があります。\n",
    "\n",
    "2. **Pydantic**:\n",
    "   - Pythonのデータバリデーションライブラリで、データモデルを定義するのに使います。型ヒントとともにモデルを定義することで、データの整合性を保つための機能を提供します。\n",
    "\n",
    "3. **vLLM**:\n",
    "   - LLMをローカルなサービスとしてホスティングするためのツールです。大規模な言語モデルを効率的に実行するために設計されています。\n",
    "\n",
    "4. **ストップシーケンス（Stop Sequence）**:\n",
    "   - LLMが生成するテキストにおいて、生成を停止する特定のトークンやシーケンスのことです。これを指定することで、不要な生成結果を防ぐことができます。\n",
    "\n",
    "5. **アクティベーション対応重み量子化（Activation-Weighted Quantization）**:\n",
    "   - モデルの重みを圧縮する方法の一つで、出力の精度を維持しつつ、モデルのサイズを小さくするために用いられます。具体的には、重みを整数に置き換え、より少ないビット数で表現します。\n",
    "\n",
    "6. **ジョッキング（Forking）**:\n",
    "   - プロセスのコピーを作成する操作です。特定の状態から新しい質問やプロンプトを生成するために使用されています。\n",
    "\n",
    "7. **XML例（XML Examples）**:\n",
    "   - 特定のフォーマットや構造に従ってデータを表現する方法。ここではLLMに期待される出力の形式を示すために使用されます。\n",
    "\n",
    "8. **非同期（Asynchronous）**:\n",
    "   - プログラムの実行において、ある処理が完了するのを待たずに次の処理を開始できる方式です。特にI/Oを待つような遅延操作を効率的に扱うために重要です。\n",
    "\n",
    "9. **バリデーター（Validator）**:\n",
    "   - データの整合性を確認するための一連のルールや関数です。Pydanticにおける `field_validator` は、特定のフィールドが適切な形式や値を持つことを保証します。\n",
    "\n",
    "10. **エージェント（Agent）**:\n",
    "    - LLMが特定のタスクを遂行するために設計されたインスタンスやボットのことです。本ノートブックでは質問者と回答者のエージェントがそれぞれ自由に質問や回答を生成します。\n",
    "\n",
    "これらの用語は、このコンペティションやノートブックの内容に密接に関連しています。初心者はこれらのコンセプトをよく理解することで、ノートブックの理解を深めることができるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c46108",
   "metadata": {},
   "source": [
    "# LLM 20 Questions スターターとリギング\n",
    "\n",
    "このスターターノートブックでは、Pythonパッケージ「rigging」を使用して、コンペティション用のベースライン提出を作成する方法を示します。このセットアップでは、`llama3`量子化モデルをvLLMを使用して利用します。\n",
    "\n",
    "## 更新 **2024年6月10日**\n",
    "- rigging 2.0に対応するようにコードを更新しました。\n",
    "- 知っているキーワードを活用する非LLM質問エージェントを含めています。ただし、これはプライベートリーダーボードではうまく機能しません。回答エージェントは、riggingを介してLLMを使用します。\n",
    "\n",
    "## リギングとは何か？\n",
    "\n",
    "リギングは、Pydantic XMLに基づいた軽量のLLMインタラクションフレームワークです。目的は、プロダクションパイプラインでLLMを利用することをできるだけ簡単かつ効果的にすることです。リギングは、20の質問タスクに最適であり、以下のことができます：\n",
    "1. 異なるバックエンドLLMモデルの交換を簡単に扱うことができる。\n",
    "2. 期待される出力を確認し、成功するまで再試行するLLMクエリパイプラインを設計できる。\n",
    "3. 型ヒント、非同期サポート、pydanticバリデーション、シリアライズなど、モダンなPythonを使用する。\n",
    "\n",
    "リポジトリをここでスターしてください: https://github.com/dreadnode/rigging\n",
    "ドキュメントをこちらで読む: https://rigging.dreadnode.io/\n",
    "\n",
    "リギングは[dreadnode](https://www.dreadnode.io/)によって構築され、維持されています。私たちは日々の作業でリギングを使用しています。\n",
    "\n",
    "リギングパイプラインの例は次のようになります：\n",
    "```{python}\n",
    "chat = rg.get_generator('gpt-4o') \\\n",
    "    .chat(f\"南アメリカのAで始まるすべての国の名前を教えてください {Answer.xml_tags()} タグ。\") \\\n",
    "    .until_parsed_as(Answer) \\\n",
    "    .run() \n",
    "```\n",
    "\n",
    "生成器は、APIキーが環境変数として保存されていれば、ほとんどの主要なLLM APIでシームレスに作成できます。\n",
    "```\n",
    "export OPENAI_API_KEY=...\n",
    "export TOGETHER_API_KEY=...\n",
    "export TOGETHERAI_API_KEY=...\n",
    "export MISTRAL_API_KEY=...\n",
    "export ANTHROPIC_API_KEY=...\n",
    "```\n",
    "\n",
    "このコンペティションでは、モデルをローカルで実行する必要がありますが、幸運にもリギングはバックエンドでtransformersを使用してモデルを実行するサポートをしています。\n",
    "\n",
    "# セットアップ\n",
    "\n",
    "以下は、このノートブックのためのセットアップの一部です。ここでは：\n",
    "- Hugging FaceとKaggleのための秘密のトークンをロードします（オプション）。\n",
    "- 必要なパッケージをインストールします。\n",
    "- vLLMサーバーをテストするためのヘルパーユーティリティスクリプトを作成します。\n",
    "\n",
    "このノートブックは、Kaggleのシークレットを使用したいくつかの隠れたトークンを利用しています。これはオプションであり、コードを実行するために必要ではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:44:41.417183Z",
     "iopub.status.busy": "2024-06-18T18:44:41.416506Z",
     "iopub.status.idle": "2024-06-18T18:44:41.516324Z",
     "shell.execute_reply": "2024-06-18T18:44:41.51541Z",
     "shell.execute_reply.started": "2024-06-18T18:44:41.417155Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# Hugging Faceトークンを格納するための変数を定義します。\n",
    "HF_TOKEN: str | None  = None\n",
    "# KaggleのAPIキーを格納するための変数を定義します。\n",
    "KAGGLE_KEY: str | None = None\n",
    "# Kaggleのユーザー名を格納するための変数を定義します。\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    # KaggleのシークレットからHugging Faceトークンを取得します。\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    # KaggleのシークレットからKaggle APIキーを取得します。\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    # KaggleのシークレットからKaggleユーザー名を取得します。\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    # 例外が発生した場合は何もしません。\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e488a0",
   "metadata": {},
   "source": [
    "## パッケージインストール\n",
    "以下のパッケージをインストールします：\n",
    "- [rigging](https://github.com/dreadnode/rigging) コンペティション用のLLMパイプラインを作成するために使用します。\n",
    "- [vLLM](https://github.com/vllm-project/vllm) モデルをローカルで独立したサービスとしてホスティングするために使用します。\n",
    "\n",
    "また、[uv](https://github.com/astral-sh/uv)を使用します。これにより、これらのパッケージをはるかに早くインストールできます。\n",
    "\n",
    "**注意:** これらのパッケージは、`/kaggle/tmp/lib`ディレクトリにインストールしています。これは、コンペティションのセットアップの目的でこのパスからファイルを後で提出用のzipファイルに含める必要があるためです。また、vllmの依存関係も`/kaggle/tmp/srvlib`にインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:44:41.518147Z",
     "iopub.status.busy": "2024-06-18T18:44:41.517895Z",
     "iopub.status.idle": "2024-06-18T18:45:50.112765Z",
     "shell.execute_reply": "2024-06-18T18:45:50.111214Z",
     "shell.execute_reply.started": "2024-06-18T18:44:41.518124Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 依存関係のインストール（高速化のためのuv）\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "# riggingとkaggleパッケージを指定したディレクトリにインストールします。\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "# vllm、numpyパッケージを別の指定したディレクトリにインストールします。\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a58ba5",
   "metadata": {},
   "source": [
    "# LLMをローカルにダウンロード\n",
    "\n",
    "このコンペティションでは、モデルの重みと共にコードを提出する必要があるため、まずはHugging Faceの`snapshot_download`を使用してモデルの重みをダウンロードします。\n",
    "\n",
    "ダウンロードするモデルは`solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ`です。これは、コンペティションの要件を満たすサイズのアクティベーション対応重み量子化版のモデルです。\n",
    "\n",
    "**注意**: 通常の状況でリギングを使用する際にはこのステップは必要ありませんが、コンペティションの提出用zipに含めるために重みを別々にダウンロードしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:45:50.119389Z",
     "iopub.status.busy": "2024-06-18T18:45:50.119071Z",
     "iopub.status.idle": "2024-06-18T18:46:23.55105Z",
     "shell.execute_reply": "2024-06-18T18:46:23.550098Z",
     "shell.execute_reply.started": "2024-06-18T18:45:50.11936Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルをダウンロードする\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# モデルを保存するためのパスを設定します。\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# 既にモデルのパスが存在する場合は、そのディレクトリを削除します。\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "# モデルの保存先ディレクトリを作成します。\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "# Hugging Faceからモデルのスナップショットをダウンロードします。\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",  # ダウンロードするモデルのリポジトリID\n",
    "    ignore_patterns=\"original*\",  # 無視するパターンを指定します。\n",
    "    local_dir=g_model_path,  # モデルを保存するローカルディレクトリ\n",
    "    local_dir_use_symlinks=False,  # シンボリックリンクを使用しない設定\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # Hugging Faceトークンを取得します。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0447df13",
   "metadata": {},
   "source": [
    "モデルの重みは、`/kaggle/tmp/model/`に保存されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:26.312229Z",
     "iopub.status.busy": "2024-06-18T18:46:26.311603Z",
     "iopub.status.idle": "2024-06-18T18:46:27.343572Z",
     "shell.execute_reply": "2024-06-18T18:46:27.342399Z",
     "shell.execute_reply.started": "2024-06-18T18:46:26.312197Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /kaggle/tmp/model  # /kaggle/tmp/modelディレクトリ内のファイルとその詳細をリスト表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6049d763",
   "metadata": {},
   "source": [
    "# ヘルパーユーティリティファイル\n",
    "\n",
    "これらは、vLLMサーバーを起動するために使用するヘルパー関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:28.861984Z",
     "iopub.status.busy": "2024-06-18T18:46:28.861242Z",
     "iopub.status.idle": "2024-06-18T18:46:34.72919Z",
     "shell.execute_reply": "2024-06-18T18:46:34.728213Z",
     "shell.execute_reply.started": "2024-06-18T18:46:28.861946Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# vLLMサーバーを起動するためのヘルパー関数\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# 指定したポートが開いているかどうかを確認する関数\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)  # タイムアウトを1秒に設定\n",
    "            result = sock.connect_ex(('localhost', port))  # ポートに接続を試みる\n",
    "            if result == 0:  # 接続成功した場合\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False  # 接続失敗の場合はFalseを返す\n",
    "\n",
    "# プロセスを実行し、指定したポートが開くまで待機する関数\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str],  # 実行するコマンド\n",
    "    port: int,  # 確認するポート番号\n",
    "    env: dict[str, str] | None,  # 環境変数\n",
    "    timeout: int = 60,  # タイムアウトの秒数（デフォルトは60秒）\n",
    "    debug: bool = False  # デバッグモードのフラグ\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    # ポートが既に開いている場合はエラーを発生させる\n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"ポート {port} はすでに開いています\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},  # 環境変数を統合\n",
    "        stdout=subprocess.DEVNULL if not debug else None,  # デバッグオフの場合は出力を無効化\n",
    "        stderr=subprocess.DEVNULL if not debug else None,  # デバッグオフの場合はエラー出力を無効化\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()  # 開始時間を記録\n",
    "    while time.time() - start_time < timeout:  # タイムアウトまでループ\n",
    "        if check_port(port):  # ポートが開いたか確認\n",
    "            return popen  # ポートが開いた場合、プロセスを返す\n",
    "        time.sleep(1)  # 1秒待機\n",
    "    \n",
    "    popen.terminate()  # タイムアウトした場合はプロセスを終了\n",
    "    raise Exception(f\"プロセスは {timeout} 秒以内にポート {port} を開きませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9f74b",
   "metadata": {},
   "source": [
    "# vLLMサーバーを起動してテスト\n",
    "\n",
    "私たちのモデルはvLLMサーバーを使用してホスティングされます。以下では、このノートブックを起動して、Kaggle環境での動作を理解します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:34.731502Z",
     "iopub.status.busy": "2024-06-18T18:46:34.731125Z",
     "iopub.status.idle": "2024-06-18T18:46:34.740817Z",
     "shell.execute_reply": "2024-06-18T18:46:34.739992Z",
     "shell.execute_reply.started": "2024-06-18T18:46:34.73147Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLMのパスと設定\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "# ユーティリティモジュールを再読み込みします\n",
    "util = importlib.reload(util)\n",
    "\n",
    "# サーバーライブラリのパスを設定します\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "assert g_srvlib_path.exists()  # パスが存在することを確認します\n",
    "\n",
    "# モデルのパスを設定します\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "assert g_model_path.exists()  # パスが存在することを確認します\n",
    "\n",
    "# vLLMサーバーのポート番号とモデル名を設定します\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:36.925703Z",
     "iopub.status.busy": "2024-06-18T18:46:36.925325Z",
     "iopub.status.idle": "2024-06-18T18:47:02.966755Z",
     "shell.execute_reply": "2024-06-18T18:47:02.965667Z",
     "shell.execute_reply.started": "2024-06-18T18:46:36.925673Z"
    }
   },
   "outputs": [],
   "source": [
    "# サブプロセスを使用してvLLMサーバーを実行します\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",  # Pythonモジュールとして実行\n",
    "    \"vllm.entrypoints.openai.api_server\",  # vLLMのAPIサーバーを起動\n",
    "    \"--enforce-eager\",  # イagerを強制するオプション\n",
    "    \"--model\", str(g_model_path),  # モデルのパスを指定\n",
    "    \"--port\", str(g_vllm_port),  # サーバーがリッスンするポート番号を指定\n",
    "    \"--served-model-name\", g_vllm_model_name  # 提供するモデルの名前を指定\n",
    "],\n",
    "    g_vllm_port,  # 確認するポート番号\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},  # 環境変数にサーバーライブラリのパスを設定\n",
    "    debug=False  # デバッグモードをオフにします\n",
    ")\n",
    "\n",
    "print(\"vLLMが起動しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74767b41",
   "metadata": {},
   "source": [
    "llama3モデルが最初のTesla T4 GPUにロードされていることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:02.968876Z",
     "iopub.status.busy": "2024-06-18T18:47:02.968472Z",
     "iopub.status.idle": "2024-06-18T18:47:04.01652Z",
     "shell.execute_reply": "2024-06-18T18:47:04.01532Z",
     "shell.execute_reply.started": "2024-06-18T18:47:02.968842Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi  # NVIDIA GPUの状態を表示します。モデルが正しくロードされているか確認するために必要です。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a724e6d",
   "metadata": {},
   "source": [
    "## モデルの検証\n",
    "\n",
    "最初のリギングジェネレーターを作成しましょう。リギングでは、ジェネレーターが強力なLLMパイプラインを作成するための基盤となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:07.297877Z",
     "iopub.status.busy": "2024-06-18T18:47:07.297084Z",
     "iopub.status.idle": "2024-06-18T18:47:15.064385Z",
     "shell.execute_reply": "2024-06-18T18:47:15.06343Z",
     "shell.execute_reply.started": "2024-06-18T18:47:07.297837Z"
    }
   },
   "outputs": [],
   "source": [
    "# リギングとの接続\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# ライブラリのパスを追加\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログレベルをWARNINGに設定\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "# リギングジェネレーターを取得します\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # モデルの名前を指定します\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIのベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（例示）\n",
    "    \"stop=<|eot_id|>\"  # Llama用のストップシーケンスを指定\n",
    ")\n",
    "\n",
    "# ジェネレーターを使用してチャットを行います\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[リギングチャット]')\n",
    "print(type(answer), answer)  # 答えのタイプと内容を表示\n",
    "\n",
    "print()\n",
    "print('[LLMの応答のみ]')\n",
    "print(type(answer.last), answer.last)  # LLMの最後の応答を表示\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content  # 応答の内容を取得\n",
    "print('[LLMの応答を文字列として]')\n",
    "print(answer.last.content)  # 応答の内容を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375cf08",
   "metadata": {},
   "source": [
    "## 結果をpandasデータフレームに変換する\n",
    "\n",
    "`to_df()`メソッドを使用することで、チャット履歴を簡単にpandasデータフレームに変換できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.065705Z",
     "iopub.status.busy": "2024-06-18T18:47:15.065434Z",
     "iopub.status.idle": "2024-06-18T18:47:15.110363Z",
     "shell.execute_reply": "2024-06-18T18:47:15.109471Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.065681Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.to_df()  # チャットの応答をpandasデータフレームに変換して表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3497ff3a",
   "metadata": {},
   "source": [
    "## モデルパラメータの変更\n",
    "\n",
    "データベース接続文字列と同様に、リギングジェネレーターは、使用するプロバイダー、モデル、APIキー、生成パラメータなどを定義する文字列として表現できます。形式は以下の通りです：\n",
    "\n",
    "```\n",
    "<provider>!<model>,<**kwargs>\n",
    "```\n",
    "\n",
    "例えば、ここでは追加のパラメータを使用してモデルをロードします：\n",
    "- temperature=0.9\n",
    "- max_tokens=512\n",
    "\n",
    "これらの詳細については、ドキュメントをこちらでご覧ください: https://rigging.dreadnode.io/topics/generators/#overload-generation-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.111849Z",
     "iopub.status.busy": "2024-06-18T18:47:15.111525Z",
     "iopub.status.idle": "2024-06-18T18:47:15.221209Z",
     "shell.execute_reply": "2024-06-18T18:47:15.220415Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.111825Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータを変更してリギングジェネレーターを取得します\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # モデルの名前を指定します\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIのベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（例示）\n",
    "    \"temperature=0.9,max_tokens=512,\"  # 生成パラメータを指定\n",
    "    \"stop=<|eot_id|>\"  # Llama用のストップシーケンスを指定\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88d2e3",
   "metadata": {},
   "source": [
    "別の方法として、`rg.GenerateParams`クラスを使用してこれらのパラメータを設定することもできます。このクラスを使用すると、さまざまなモデルパラメータを設定できます：\n",
    "\n",
    "```\n",
    "rg.GenerateParams(\n",
    "    *,\n",
    "    temperature: float | None = None,\n",
    "    max_tokens: int | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    stop: list[str] | None = None,\n",
    "    presence_penalty: float | None = None,\n",
    "    frequency_penalty: float | None = None,\n",
    "    api_base: str | None = None,\n",
    "    timeout: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    extra: dict[str, typing.Any] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "詳細については、こちらのドキュメントをご覧ください: https://rigging.dreadnode.io/api/generator/#rigging.generator.GenerateParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.223081Z",
     "iopub.status.busy": "2024-06-18T18:47:15.222833Z",
     "iopub.status.idle": "2024-06-18T18:47:17.479867Z",
     "shell.execute_reply": "2024-06-18T18:47:17.478938Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.223059Z"
    }
   },
   "outputs": [],
   "source": [
    "# GenerateParamsを使用してパラメータを設定\n",
    "rg_params = rg.GenerateParams(\n",
    "    temperature=0.9,  # 温度を0.9に設定\n",
    "    max_tokens=512,  # 最大トークン数を512に設定\n",
    ")\n",
    "\n",
    "# ジェネレーターを使用してチャットを作成\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "\n",
    "# 質問「How is it going?」を投げて応答を取得\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "print(answer.last.content)  # LLMの応答を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abed461",
   "metadata": {},
   "source": [
    "また、チェーン内で`params`を使用してパラメータを設定することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:17.481882Z",
     "iopub.status.busy": "2024-06-18T18:47:17.481227Z",
     "iopub.status.idle": "2024-06-18T18:47:19.65719Z",
     "shell.execute_reply": "2024-06-18T18:47:19.656339Z",
     "shell.execute_reply.started": "2024-06-18T18:47:17.481845Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータを設定せずにチャットを作成\n",
    "base_chat = generator.chat()  # パラメータは設定しない\n",
    "\n",
    "# 質問「How is it going?」を投げて応答を取得（この時にパラメータを指定）\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature=0.9, max_tokens=512) \\  # 温度を0.9、最大トークン数を512に設定\n",
    "    .run()\n",
    "\n",
    "print(answer.last.content)  # LLMの応答を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860037f6",
   "metadata": {},
   "source": [
    "# パースされた出力の例\n",
    "\n",
    "次に、以下のパイプラインを作成します：\n",
    "1. `Answer`という名前のリギングモデルを作成します。これは、モデルの結果からパースすることが期待される出力を説明します。\n",
    "    - ここに一部のバリデーターを追加して、出力が`yes`または`no`のいずれかであることを保証します。\n",
    "    - これは完全にカスタマイズ可能です。\n",
    "    - ここでは、`validate_content`が応答が期待される出力（小文字で「yes」または「no」で始まる）に準拠していることを確認します。\n",
    "2. プロンプト内で`Answer.xml_example()`を使用して、LLMに出力がどのように見えるべきかを知らせることができます。\n",
    "3. 後で`.until_parsed_as(Answer)`を使用して、LLMの出力がここで定義した通りに抽出されることを確認します。\n",
    "\n",
    "**注意**: `until_parsed_as()`にはデフォルトで5である`max_rounds`パラメータを指定することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:19.659101Z",
     "iopub.status.busy": "2024-06-18T18:47:19.658668Z",
     "iopub.status.idle": "2024-06-18T18:47:19.668457Z",
     "shell.execute_reply": "2024-06-18T18:47:19.667558Z",
     "shell.execute_reply.started": "2024-06-18T18:47:19.659066Z"
    }
   },
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "# モデルクラスAnswerを定義します\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]  # 出力が\"yes\"または\"no\"であることを指定\n",
    "\n",
    "    # contentフィールドのバリデーションを定義します\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:  # 有効な値をリストで定義\n",
    "            if v.lower().startswith(valid):  # 値が小文字で\"yes\"または\"no\"で始まるか確認\n",
    "                return valid  # 有効な場合はその値を返す\n",
    "        raise ValueError(\"無効な回答です。'yes'または'no'である必要があります\")  # 無効な場合はエラーを発生させる\n",
    "\n",
    "    # XML形式の例を返すクラスメソッドを定義します\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"  # XMLタグで囲まれた例を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:19.669852Z",
     "iopub.status.busy": "2024-06-18T18:47:19.669536Z",
     "iopub.status.idle": "2024-06-18T18:47:19.752363Z",
     "shell.execute_reply": "2024-06-18T18:47:19.751366Z",
     "shell.execute_reply.started": "2024-06-18T18:47:19.669814Z"
    }
   },
   "outputs": [],
   "source": [
    "# XML例がどのように見えるかを確認します。これはプロンプトに使用できます。\n",
    "Answer.xml_example()  # AnswerモデルのXML例を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:21.724128Z",
     "iopub.status.busy": "2024-06-18T18:47:21.723509Z",
     "iopub.status.idle": "2024-06-18T18:47:33.30668Z",
     "shell.execute_reply": "2024-06-18T18:47:33.305798Z",
     "shell.execute_reply.started": "2024-06-18T18:47:21.724097Z"
    }
   },
   "outputs": [],
   "source": [
    "# ジェネレーターを再取得し、プロンプトを設定します\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # モデルの名前を指定\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIのベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（例示）\n",
    "    \"stop=<|eot_id|>\"  # Llama用のストップシーケンスを指定\n",
    ")\n",
    "\n",
    "keyword = 'Tom Hanks'  # 秘密の単語\n",
    "category = 'Famous Person'  # カテゴリ\n",
    "last_question = 'Is it a famous person?'  # 最後の質問\n",
    "\n",
    "# プロンプトを設定します\n",
    "prompt = f\"\"\"\\\n",
    "            このゲームの秘密の単語は \"{keyword}\" [{category}] です。\n",
    "\n",
    "            あなたは現在、上記の単語についての質問に答えています。\n",
    "\n",
    "            次の質問は \"{last_question}\" です。\n",
    "\n",
    "            上記のはい/いいえの質問に答え、以下の形式で回答してください：\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - あなたの回答は、上記のキーワードに基づいて正確であるべきです\n",
    "            - 常に「yes」または「no」で答えてください\n",
    "\n",
    "            答えは何ですか？\n",
    "\"\"\"\n",
    "\n",
    "# チャットを実行して応答を取得します\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)  # プロンプトを指定\n",
    "    .until_parsed_as(Answer, max_rounds=50)  # 50ラウンドまでパースを試みる\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print('=== 完全なチャット ===')\n",
    "print(chat)  # チャットの全体を表示\n",
    "\n",
    "print()\n",
    "print('=== LLMの応答のみ ===')\n",
    "print(chat.last)  # LLMの最後の応答を表示\n",
    "\n",
    "print()\n",
    "print('=== パースされた回答 ===')\n",
    "print(chat.last.parse(Answer).content)  # パースされた応答の内容を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96baeea",
   "metadata": {},
   "source": [
    "# リギングを使った質問者チャットパイプラインの作成\n",
    "\n",
    "次に、キーワードが何であるかを特定する手助けをする質問者パイプラインを作成します。\n",
    "\n",
    "まず、出力をパースするために使用する`Question`オブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:33.308069Z",
     "iopub.status.busy": "2024-06-18T18:47:33.307802Z",
     "iopub.status.idle": "2024-06-18T18:47:33.315461Z",
     "shell.execute_reply": "2024-06-18T18:47:33.314429Z",
     "shell.execute_reply.started": "2024-06-18T18:47:33.308044Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "# 空白を取り除くための型定義\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "# Questionモデルクラスを定義\n",
    "class Question(rg.Model):\n",
    "    content: str_strip  # contentフィールドに空白を取り除いた文字列を設定\n",
    "\n",
    "    # XML形式の例を返すクラスメソッドを定義\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()  # 例となる質問をXML形式で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:39.082592Z",
     "iopub.status.busy": "2024-06-18T18:47:39.082181Z",
     "iopub.status.idle": "2024-06-18T18:47:39.859978Z",
     "shell.execute_reply": "2024-06-18T18:47:39.859196Z",
     "shell.execute_reply.started": "2024-06-18T18:47:39.082562Z"
    }
   },
   "outputs": [],
   "source": [
    "# 基本チャットを作成し、質問を行います\n",
    "base = generator.chat(\"\"\"\\\n",
    "あなたは「20の質問」ゲームの才能あるプレイヤーです。あなたは正確で、集中力があり、構造的なアプローチを取ります。役に立つ質問を作成し、推測を行い、キーワードに関する質問に答えます。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# 質問を行うためのチャットを構築します\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "    あなたは現在、次の質問をしています。\n",
    "\n",
    "    質問を作成し、以下の形式で回答してください：\n",
    "    {Question.xml_example()}\n",
    "\n",
    "    - あなたの応答は、最も多くの情報を集めるための集中した質問であるべきです\n",
    "    - 質問は一般的なところから始めてください\n",
    "    - 残りの検索空間を二分しようと常に努めてください\n",
    "    - 前の質問と回答に注意を払ってください\n",
    "\n",
    "    次の質問は何ですか？\n",
    "    \"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)  # 質問のパースを試みる\n",
    ".run()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:39.861319Z",
     "iopub.status.busy": "2024-06-18T18:47:39.861048Z",
     "iopub.status.idle": "2024-06-18T18:47:39.884675Z",
     "shell.execute_reply": "2024-06-18T18:47:39.883442Z",
     "shell.execute_reply.started": "2024-06-18T18:47:39.861295Z"
    }
   },
   "outputs": [],
   "source": [
    "# 会話のデータフレーム表現を表示します\n",
    "question_chat.to_df()  # 質問チャットの内容をpandasデータフレームに変換して表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150a6aa2",
   "metadata": {},
   "source": [
    "現在、LLMの応答には質問が含まれており、以下のように質問をパースできると確信しています："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:46.02397Z",
     "iopub.status.busy": "2024-06-18T18:47:46.023601Z",
     "iopub.status.idle": "2024-06-18T18:47:46.029781Z",
     "shell.execute_reply": "2024-06-18T18:47:46.028875Z",
     "shell.execute_reply.started": "2024-06-18T18:47:46.02394Z"
    }
   },
   "outputs": [],
   "source": [
    "# LLMの応答から質問をパースします\n",
    "question = question_chat.last.parse(Question).content  # 最後の応答をQuestionモデルでパースし、内容を取得\n",
    "print(question)  # 取得した質問を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea67dda",
   "metadata": {},
   "source": [
    "# キーワードデータフレームを作成する\n",
    "**注意**: これは公開セットの可能なキーワードを知っているためにのみ機能します。これが最終リーダーボードでは機能しないことに注意してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:13:09.05161Z",
     "iopub.status.busy": "2024-06-18T19:13:09.050676Z",
     "iopub.status.idle": "2024-06-18T19:13:11.606383Z",
     "shell.execute_reply": "2024-06-18T19:13:11.605418Z",
     "shell.execute_reply.started": "2024-06-18T19:13:09.051574Z"
    }
   },
   "outputs": [],
   "source": [
    "# キーワードのリストを取得するためにファイルをダウンロードします\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py  # キーワードのPythonファイルを指定した名前でダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:13:13.191677Z",
     "iopub.status.busy": "2024-06-18T19:13:13.190866Z",
     "iopub.status.idle": "2024-06-18T19:13:14.141544Z",
     "shell.execute_reply": "2024-06-18T19:13:14.140544Z",
     "shell.execute_reply.started": "2024-06-18T19:13:13.19164Z"
    }
   },
   "outputs": [],
   "source": [
    "# ダウンロードしたキーワードファイルの先頭部分を表示します\n",
    "!head keywords_local.py  # keywords_local.pyファイルの最初の数行を表示して内容を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:08.09623Z",
     "iopub.status.busy": "2024-06-18T19:39:08.095864Z",
     "iopub.status.idle": "2024-06-18T19:39:08.157802Z",
     "shell.execute_reply": "2024-06-18T19:39:08.156896Z",
     "shell.execute_reply.started": "2024-06-18T19:39:08.096204Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.append('./')  # 現在のディレクトリをパスに追加\n",
    "from keywords_local import KEYWORDS_JSON  # ダウンロードしたキーワードデータをインポート\n",
    "\n",
    "# テキストの最初の単語を大文字にし、残りを小文字にする関数\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text  # テキストが空の場合はそのまま返す\n",
    "    return text[0].upper() + text[1:].lower()  # 最初の文字を大文字にし、それ以外を小文字にする\n",
    "\n",
    "# キーワードデータフレームを作成する関数\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    # JSON形式のキーワードデータを読み込む\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    # 各カテゴリの単語を辞書に格納\n",
    "    for d in keywords_dict:\n",
    "        words = [w['keyword'] for w in d['words']]  # 各単語を抽出\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]  # カテゴリと単語のタプルを作成\n",
    "        category_words_dict[d['category']] = words  # カテゴリ毎に単語リストを保持\n",
    "        all_words += words  # すべての単語を集約\n",
    "        all_cat_words += cat_word  # カテゴリと単語のタプルを集約\n",
    "\n",
    "    # データフレームを作成\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category','keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]  # 単語の最初の文字を抽出\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]  # 単語の第二の文字を抽出\n",
    "    keyword_df.to_parquet('keywords.parquet')  # データフレームをparquetファイルとして保存\n",
    "    \n",
    "# キーワードデータフレームを作成する関数を呼び出し\n",
    "create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:08.568525Z",
     "iopub.status.busy": "2024-06-18T19:39:08.56792Z",
     "iopub.status.idle": "2024-06-18T19:39:08.621277Z",
     "shell.execute_reply": "2024-06-18T19:39:08.620385Z",
     "shell.execute_reply.started": "2024-06-18T19:39:08.568491Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存したキーワードデータフレームを読み込み、サンプルを表示します\n",
    "keywords_df = pd.read_parquet('keywords.parquet')  # parquetファイルからデータフレームを読み込む\n",
    "keywords_df.sample(10)  # データフレームからランダムに10行をサンプル表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:09.267519Z",
     "iopub.status.busy": "2024-06-18T19:39:09.267113Z",
     "iopub.status.idle": "2024-06-18T19:39:09.314644Z",
     "shell.execute_reply": "2024-06-18T19:39:09.313676Z",
     "shell.execute_reply.started": "2024-06-18T19:39:09.267487Z"
    }
   },
   "outputs": [],
   "source": [
    "# 各カテゴリの出現回数をカウントして表示します\n",
    "keywords_df['category'].value_counts()  # カテゴリ毎の出現頻度を集計し表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe8a39a",
   "metadata": {},
   "source": [
    "# 最終提出用の`main.py`スクリプトを作成する\n",
    "\n",
    "私たちの最終提出物は、`main`ファイルを含む圧縮ディレクトリになります。以下にそのファイルの内容を示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:42.817674Z",
     "iopub.status.busy": "2024-06-18T20:07:42.817302Z",
     "iopub.status.idle": "2024-06-18T20:07:42.871384Z",
     "shell.execute_reply": "2024-06-18T20:07:42.870434Z",
     "shell.execute_reply.started": "2024-06-18T20:07:42.817645Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# メインエージェントファイル\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# パスの修正\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# ロギングのノイズを削減\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# 固定インポート\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# 定数\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    ")\n",
    "\n",
    "# タイプ\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です。'yes'または'no'である必要があります\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"質問\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"物事/場所\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# 関数\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # 最初の質問をオーバーライドしてキーワードバグを修正\n",
    "        return \"Are we playing 20 questions?\"\n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(\n",
    "                f\"\"\"\\\n",
    "                あなたは現在、次の質問をしています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づいて、次の最も有用なはい/いいえの質問をして、以下の形式で回答してください：\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - あなたの回答は、最も多くの情報を集めるための集中した質問であるべきです\n",
    "                - 質問は一般的なところから始めてください\n",
    "                - 残りの検索空間を二分しようと常に努めてください\n",
    "                - 前の質問と回答に注意を払ってください\n",
    "\n",
    "                次の質問は何ですか？\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'Is it a person?'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n",
    "        return \"yes\" # override until keyword bug is fixed.\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "    \n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # 5回ループして最も多い応答を取得\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "                    f\"\"\"\\\n",
    "                    Provide the best yes/no answer to the question about the keyword [{observation.keyword}] in the category [{observation.category}]\n",
    "\n",
    "                    [QUESTION] \"{last_question}\" [/QUESTION]\n",
    "\n",
    "                    Remember they keyword is [{observation.keyword}]\n",
    "\n",
    "                    Answer the yes/no question above and place it in the following format:\n",
    "                    {Answer.xml_example()}\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'Responses are {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% エラーが発生したため「はい」と答えます %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                あなたは現在、キーワードの情報に基づいて推測を行っています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づいて、キーワードの次に最良の推測を1つ作成し、以下の形式で回答してください：\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - 上記の履歴に基づいて繰り返しの推測を避けてください\n",
    "                - 推測は特定の人物、場所、または物であるべきです\n",
    "\n",
    "                あなたの推測は何ですか？\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'france'\n",
    "    \n",
    "# vLLMとジェネレーターの設定\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLMが起動しました\")\n",
    "except ValueError:\n",
    "    print('vLLMはすでに実行中です')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは「20の質問」ゲームの才能あるプレイヤーです。あなたは正確で、集中力があり、\n",
    "構造的なアプローチを取ります。役に立つ質問を作成し、推測を行い、キーワードに関する質問に答えます。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# エントリーポイント\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードの最初の文字は何ですか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは'{letters[0]}'で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\"または'{letters[-1]}'で始まりますか？\"\n",
    "    \n",
    "    return f\"キーワードの最初の文字は次のいずれかですか: {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# シンプルな質問者エージェント\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"Are we playing 20 questions?\",\n",
    "            \"Is the keyword a thing that is not a place?\",\n",
    "            \"Is the keyword a place?\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # 過去の回答に基づいてkeyword_dfをフィルタリングします\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"Is the keyword a thing that is not a place?\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"Is the keyword a place?\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('Does the keyword start '):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('sample letters', n_letters, sample_letters)\n",
    "        return sample_letters\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"以下のいずれかがキーワードですか？ {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # 推測した単語を除外\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # データフレームをリセット\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # フレームワーク内で実行する際の非同期ゲート\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28372f4f",
   "metadata": {},
   "source": [
    "# エージェントを自身に対してテストする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:44.085235Z",
     "iopub.status.busy": "2024-06-18T20:07:44.084879Z",
     "iopub.status.idle": "2024-06-18T20:07:45.842315Z",
     "shell.execute_reply": "2024-06-18T20:07:45.841352Z",
     "shell.execute_reply.started": "2024-06-18T20:07:44.085207Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最初の文字に関する質問を形式化する関数\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードの最初の文字は何ですか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは'{letters[0]}'で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\"または'{letters[-1]}'で始まりますか？\"\n",
    "    \n",
    "    return f\"キーワードの最初の文字は次のいずれかですか: {formatted_letters}?\"\n",
    "\n",
    "# 例として['a','b','c']を渡して、関数の結果を確認します\n",
    "format_first_letter_question(['a','b','c'])  \n",
    "\n",
    "import re\n",
    "\n",
    "# 質問から文字を抽出する関数\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"  # 文字列に含まれるシングルクォートで囲まれた文字を探す正規表現\n",
    "    matches = re.findall(pattern, question)  # 正規表現にマッチしたものをリストとして返す\n",
    "    return matches  # 抽出した文字のリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:45.84427Z",
     "iopub.status.busy": "2024-06-18T20:07:45.843989Z",
     "iopub.status.idle": "2024-06-18T20:07:45.888115Z",
     "shell.execute_reply": "2024-06-18T20:07:45.887152Z",
     "shell.execute_reply.started": "2024-06-18T20:07:45.844245Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自動再読み込みを設定し、必要なモジュールをインポートします\n",
    "%load_ext autoreload  # 自動再読み込みを有効にする\n",
    "%autoreload 2  # すべてのモジュールを変更時に自動で再読み込み\n",
    "from main import Observation, agent_fn, observe  # mainから必要なクラス・関数をインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:46.655031Z",
     "iopub.status.busy": "2024-06-18T20:07:46.6543Z",
     "iopub.status.idle": "2024-06-18T20:07:47.652045Z",
     "shell.execute_reply": "2024-06-18T20:07:47.650877Z",
     "shell.execute_reply.started": "2024-06-18T20:07:46.655Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLMが実行中かどうかを確認します\n",
    "!ps -aef | grep vllm  # プロセスのリストからvllmを検索して表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:48.447833Z",
     "iopub.status.busy": "2024-06-18T20:07:48.447029Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# キーワードデータフレームを読み込み、サンプルを抽出します\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "sample = keyword_df.sample(1)  # ランダムに1つのサンプルを取得\n",
    "\n",
    "# 観察オブジェクトを作成\n",
    "obs = Observation(step=0,\n",
    "    role='guesser',  # ロールを'guesser'に設定\n",
    "    turnType=\"ask\",  # ターンタイプを'ask'に設定\n",
    "    keyword=sample['keyword'].values[0],  # サンプルからキーワードを取得\n",
    "    category=sample['category'].values[0],  # サンプルからカテゴリを取得\n",
    "    questions=[],  # 質問のリストを初期化\n",
    "    answers=[],  # 回答のリストを初期化\n",
    "    guesses=[],  # 推測のリストを初期化\n",
    ")\n",
    "\n",
    "# 20回の質問と回答のループ\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)  # 質問を観察関数で取得\n",
    "    print(f'[{i} Question]: {question}')  # 質問を表示\n",
    "    obs.questions.append(question)  # 質問リストに追加\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)  # 回答を観察関数で取得\n",
    "    obs.answers.append(answer)  # 回答リストに追加\n",
    "    \n",
    "    # 正解を設定\n",
    "    if obs.questions[-1].startswith('Are we playing 20 questions?'):\n",
    "        gt_answer = answer  # 何でも良い\n",
    "    elif obs.questions[-1].startswith('Is the keyword a thing that is not a place?'):\n",
    "        gt_answer = 'yes' if sample['category'].values[0] == 'things' else 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword a place?'):\n",
    "        gt_answer = 'yes' if sample['category'].values[0] == 'place' else 'no'\n",
    "    elif obs.questions[-1].startswith('Does the keyword start'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])  # 質問から文字を抽出\n",
    "        gt_answer = 'yes' if obs.keyword[0] in letters_guess else 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword one of the following?'):\n",
    "        possible_kw = obs.questions[-1].replace('Is the keyword one of the following? ', '').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]\n",
    "        print(possible_kw)\n",
    "        gt_answer = 'yes' if obs.keyword in possible_kw else 'no'\n",
    "\n",
    "    print(f'[{i} Answer]: {answer} [True Answer]: {gt_answer}')  # 回答と正しい回答を表示\n",
    "    if answer != gt_answer:\n",
    "        break  # 回答が異なる場合はループを抜ける\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'\n",
    "    guess = await observe(obs)  # 推測を観察関数で取得\n",
    "    print(f'[{i} Guess]: {guess} - [Keyword]: {obs.keyword}')  # 推測とキーワードを表示\n",
    "    obs.guesses.append(guess)  # 推測リストに追加\n",
    "    if guess == obs.keyword:\n",
    "        print('GOT IT!')  # 正しい推測ができた場合のメッセージ\n",
    "        break\n",
    "        \n",
    "    obs.step += 1  # ステップを更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a346a78",
   "metadata": {},
   "source": [
    "# モデルとコードを圧縮して提出用に準備する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-06-18T20:05:09.682043Z",
     "iopub.status.busy": "2024-06-18T20:05:09.681791Z",
     "iopub.status.idle": "2024-06-18T20:05:12.711224Z",
     "shell.execute_reply": "2024-06-18T20:05:12.710045Z",
     "shell.execute_reply.started": "2024-06-18T20:05:09.682021Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提出用の圧縮に必要なパッケージをインストールします\n",
    "!apt install pigz pv  # pigz（並列圧縮用のgzip）とpv（仮想パイプ）をインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:16:49.684667Z",
     "iopub.status.busy": "2024-06-18T19:16:49.684329Z",
     "iopub.status.idle": "2024-06-18T19:19:49.999133Z",
     "shell.execute_reply": "2024-06-18T19:19:49.997937Z",
     "shell.execute_reply.started": "2024-06-18T19:16:49.684638Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデルとコードを圧縮して提出用のtar.gzファイルを作成します\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\  # 圧縮ファイルの作成\n",
    "    --dereference \\  # シンボリックリンクを解決\n",
    "    -C /kaggle/tmp model lib srvlib \\  # /kaggle/tmp内のモデル、lib、srvlibを追加\n",
    "    -C /kaggle/working main.py util.py \\  # /kaggle/working内のmain.pyとutil.pyを追加\n",
    "    -C /kaggle/working keywords.parquet  # keywords.parquetを追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:50.002318Z",
     "iopub.status.busy": "2024-06-18T19:19:50.001899Z",
     "iopub.status.idle": "2024-06-18T19:19:51.037256Z",
     "shell.execute_reply": "2024-06-18T19:19:51.036303Z",
     "shell.execute_reply.started": "2024-06-18T19:19:50.002277Z"
    }
   },
   "outputs": [],
   "source": [
    "# 作成したファイルやディレクトリの内容を表示します\n",
    "!ls -GFlash --color  # 詳細情報を色付きで表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdccd78",
   "metadata": {},
   "source": [
    "# Kaggle CLIを使用した提出\n",
    "\n",
    "オプションとして、ノートブックを再実行することなく、Kaggle CLIインターフェースを使用して提出することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:51.040706Z",
     "iopub.status.busy": "2024-06-18T19:19:51.040378Z",
     "iopub.status.idle": "2024-06-18T19:19:51.086269Z",
     "shell.execute_reply": "2024-06-18T19:19:51.085134Z",
     "shell.execute_reply.started": "2024-06-18T19:19:51.040676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle CLIを使用してコンペティションに提出するためのコマンドを示します（コメントアウトされています）\n",
    "# 環境変数にKaggleのユーザー名とAPIキーを設定し、以下のコマンドを実行します\n",
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"submit from notebook\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db0a894",
   "metadata": {},
   "source": [
    "# コメント \n",
    "\n",
    "> ## Bhanu Prakash M\n",
    "> \n",
    "> Hi [@robikscube](https://www.kaggle.com/robikscube),\n",
    "> \n",
    "> vLLMサーバーを実行する方法を教えてもらえますか？デバッグをtrueに設定した後、以下のエラーが発生します。\n",
    "> \n",
    "> INFO 06-18 21:44:58 selector.py:69] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
    "> \n",
    "> INFO 06-18 21:44:58 selector.py:32] Using XFormers backend.\n",
    "> \n",
    "> そして長いトレースバックエラーの最終文は\n",
    "> \n",
    "> ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla P100-PCIE-16GB GPU has compute capability 6.0. You can use float16 instead by explicitly setting the dtype flag in CLI, for example: --dtype=half\n",
    "> \n",
    "> \n",
    "> > ## Rob MullaTopic Author\n",
    "> > \n",
    "> どのモデルを実行しようとしていますか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > > ## Bhanu Prakash M\n",
    "> > > phi-3モデルをllamaフォーマットに変換された重みと共に使用しています。\n",
    "> > > \n",
    "> > > [https://huggingface.co/rhysjones/Phi-3-mini-mango-1-llamafied](https://huggingface.co/rhysjones/Phi-3-mini-mango-1-llamafied)\n",
    "> > > \n",
    "> > > これがそのモデルです。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Bhanu Prakash M\n",
    "> > > > [@robikscube](https://www.kaggle.com/robikscube) 何か更新はありますか？\n",
    "> > > >\n",
    "> > > > ## Rob MullaTopic Author\n",
    "> > > > ここで動作させました: [https://www.kaggle.com/code/robikscube/phi3-intro-to-rigging-for-llm-20-questions/](https://www.kaggle.com/code/robikscube/phi3-intro-to-rigging-for-llm-20-questions/)\n",
    "> > > > \n",
    "> > > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> \"Process did not open port 9999 within 120 seconds\"というエラーが出るのはなぜですか？ [@robikscube](https://www.kaggle.com/robikscube)\n",
    "> \n",
    "> \n",
    "> > ## Rob MullaTopic Author\n",
    "> > \n",
    "> ちょっと見てみます！教えてくれてありがとう。\n",
    "> > \n",
    "> \n",
    "> \n",
    "> > > ## OminousDude\n",
    "> > > ありがとうございます！リギングを使ってコードを高速化しようとしているが、使用しているモデルが時間配分に合わず、このコードが本当に役立っています。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> あなたのコードをテストしてみましたが、実行すると\"AttributeError: 'coroutine' object has no attribute 'last'\"という例外で失敗しました。このエラーに遭遇したことはありますか？\n",
    "> \n",
    "> \n",
    "> > ## Rob MullaTopic Author\n",
    "> > \n",
    "> 教えてくれてありがとう。この問題も確認しています。リギングは現在開発中で、この変更は最新のリリースから来たようです。ノートブックを更新してこの変更を修正するか、問題を解決するはずの古いバージョンのリギングを固定するかもしれません。\n",
    "> > \n",
    "> \n",
    "> > > ## OminousDude\n",
    "> > > わかりました。ありがとう！\n",
    "> > > \n",
    "> > > > ## OminousDude\n",
    "> > > > バージョン7は動作しますか？\n",
    "> > > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> 私はこれを自分のローカルマシンで試していますが、動作していません。理由がわかりますか？ [@robikscube](https://www.kaggle.com/robikscube) \n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> これがバグかどうかわからないが、このコードは「solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ」でしか動作していません。このコードを使ってより大きなバージョンのLlamaを試しているときに気づきました。\n",
    "> \n",
    "> "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
