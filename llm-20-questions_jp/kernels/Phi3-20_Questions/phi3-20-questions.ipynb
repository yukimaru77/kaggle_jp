{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc48f99f",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、「20の質問」ゲームに取り組むための言語モデルを実装するための準備を行っています。具体的には、MicrosoftのPhi-3 Miniモデルを使用して、質問を行い、その回答を適切に処理するエージェントを構築しています。\n",
    "\n",
    "### 問題への取り組み\n",
    "ノートブックの目的は、Kaggleの「LLM 20 Questions」コンペティションに参加するためのモデルを設定し、特に質問者（guesser）と回答者（answerer）の役割を制御するエージェントを作成することです。このエージェントは、ユーザーが思い浮かべる特定の「ターゲットワード」を答えるために、制約のある「はい」または「いいえ」の質問を投げかける必要があります。\n",
    "\n",
    "### 手法とライブラリ\n",
    "1. **パッケージのインストール**:\n",
    "   - `tqdm`: 進捗バーを表示するためのライブラリ。\n",
    "   - `pydantic`: データバリデーションおよび設定管理に使用。\n",
    "   - `transformers`: 言語モデルの操作、特にPhi-3モデルの読み込みと使用に利用。\n",
    "\n",
    "2. **モデルのセットアップ**:\n",
    "   - Hugging FaceからPhi-3モデルをダウンロードし、適切なディレクトリに保存します。\n",
    "   - `AutoModelForCausalLM`と`AutoTokenizer`を使用して、言語モデルのロードを行い、テキスト生成のためのパイプラインを設定します。\n",
    "\n",
    "3. **エージェントの実装**:\n",
    "   - `play`関数が定義されており、KaggleObservationとKaggleConfigのデータクラスを通じて、ゲームの状態を管理します。\n",
    "   - 質問者と回答者の役割に応じて異なるプロンプトを生成してモデルに問いかけることで、ゲームのロジックを実装しています。\n",
    "\n",
    "4. **提出ファイルの準備**:\n",
    "   - 最終的に、作成したコードを`main.py`というファイルに書き出し、提出用のディレクトリをtar.gz形式で圧縮する手順が含まれています。\n",
    "\n",
    "### 結論\n",
    "このノートブックは、明示的に設定されているライブラリと手順を用いて、20の質問ゲームにおけるAIエージェントの初期設定を行っており、将来的には実際のプレイに向けたモデルの精度を向上させるための基盤を築いています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad7da24",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連する初心者がつまずく可能性のある専門用語の解説を示します。これらは一般的に知られているものではないか、特定の文脈での理解が必要なものです。\n",
    "\n",
    "1. **tqdm**: Pythonでのプログレスバーを簡単に実装するためのライブラリです。ループ処理中に進捗状況を視覚的に表示することができ、処理の進行具合を確認できるのが特徴です。\n",
    "\n",
    "2. **pydantic**: Pythonのデータバリデーションおよび設定管理のためのライブラリで、データモデルを簡単に作成し、型検査を行うことができるため、データの整合性を保つのに役立ちます。\n",
    "\n",
    "3. **Attributes**: Pythonのクラスにおけるプロパティ。通常、クラスのインスタンスが持つ情報を定義します。例えば、dataclassを使うと、属性の定義を簡単に行えます。\n",
    "\n",
    "4. **pipeline**: Hugging FaceのTransformersライブラリの一部で、特定のタスクに対するモデルの推論を簡単に実行できるラッパーです。たとえば、テキスト生成、質問応答、分類などのタスクを簡単に処理できます。\n",
    "\n",
    "5. **AutoModelForCausalLM**: 自然言語処理モデルの一種で、因果言語モデリングタスク（次の単語を予測するタスク）のためのモデルを自動で選択し、インスタンス化するためのクラスです。\n",
    "\n",
    "6. **torch_dtype**: PyTorchにおけるデータ型を指定するための引数で、モデルの訓練や推論の際に使用されるデータ形式を設定します。例えば、FP16（16ビット浮動小数点数）やFP32（32ビット浮動小数点数）などがあります。\n",
    "\n",
    "7. **temperature**: 確率的生成モデルにおけるサンプリングの制約を調整するパラメータです。値が高いと多様な生成結果が可能になり、低いと決定的な生成になります。\n",
    "\n",
    "8. **yes-or-no question**: はい/いいえで答えられる質問。20の質問ゲームでは、情報を効率的に収集するためにこの形の質問が多用されます。\n",
    "\n",
    "9. **dataclass**: Pythonのクラスを簡素化するための構文で、イミュータブルなデータ構造を作成する際に有用です。属性を定義するだけで多くのボイラープレートコードを省略できます。\n",
    "\n",
    "10. **CUDA**: NVIDIAの並列計算プラットフォームおよびAPIで、GPUを用いて計算を加速する技術です。特にディープラーニングの訓練において、計算の時間を短縮するために頻繁に使用されます。\n",
    "\n",
    "11. **model.gguf**: モデルの保存形式の一つで、特定のフレームワークに依存せずにモデルデータを格納するために設計されています。\n",
    "\n",
    "12. **Kaggle Observation / Config**: このコンペティションでのゲームの状態や設定を保持するために使用されるデータ構造。各エピソードの進行状況や環境に関する情報を格納します。\n",
    "\n",
    "13. **play関数**: ゲームを実行するための主要な関数であり、観察データと設定に基づき、適切な行動を決定し、モデルを通じて結果を生成します。\n",
    "\n",
    "これらの用語は、特にJupyter Notebookの特定のコンテキストや実装において、理解に重要であることが多いです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f368c6f",
   "metadata": {},
   "source": [
    "# Phi-3との20の質問\n",
    "\n",
    "## 提出フォルダにパッケージをインストールする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なパッケージをインストールします。\n",
    "# - tqdm: 進捗バーを表示するためのライブラリ\n",
    "# - pydantic: データバリデーションおよび設定管理用ライブラリ\n",
    "# - transformers: 言語モデルを扱うためのライブラリ\n",
    "# 提出フォルダにインストールし、他の環境に影響を与えないようにします。\n",
    "\n",
    "pip install -U -t /kaggle/working/submission/lib tqdm pydantic transformers -qq  # 提出フォルダにパッケージをインストールします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b0131",
   "metadata": {},
   "source": [
    "## Llama.cppのセットアップ（現時点では未使用）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama.cpp用のPythonパッケージをインストールするためのコマンドです。\n",
    "# CMAKE_ARGSを設定して、CUDAを使用するように指定しています。\n",
    "# このパッケージは、現在使用されていませんが、将来的に必要になるかもしれません。\n",
    "# 指定されたディレクトリにインストールします。\n",
    "\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -t /kaggle/working/submission/lib llama-cpp-python \\\n",
    "#   --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なディレクトリを作成します。\n",
    "# -pオプションを使うことで、親ディレクトリが存在しない場合も一緒に作成されます。\n",
    "# ここでは、提出フォルダ内にphi3というサブディレクトリを作成します。\n",
    "\n",
    "mkdir -p /kaggle/working/submission/lib/phi3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging FaceからPhi-3モデルをダウンロードするためのコマンドです。\n",
    "# curlを使用して指定されたURLからファイルを取得し、\n",
    "# その内容を提出フォルダのphi3ディレクトリに\"model.gguf\"として保存します。\n",
    "\n",
    "# !curl -L \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf?download=true\" > \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポートします。\n",
    "# osとsysを使用して、環境に応じた重みファイルのパスを設定しています。\n",
    "\n",
    "# KAGGLE_AGENT_PATHはKaggleのエージェントのパスです。\n",
    "# もしこのパスが存在する場合、libディレクトリをPythonのモジュール検索パスに追加します。\n",
    "# その後、重みファイルのパスをKAGGLE_AGENT_PATH内のモデルファイルに設定します。\n",
    "# そのパスが存在しない場合は、提出フォルダのlibディレクトリを検索パスに追加し、重みファイルのパスをそこに設定します。\n",
    "\n",
    "import os, sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 現在の作業ディレクトリ内のファイルとフォルダのリストを表示します。\n",
    "# %lsコマンドを使用することで、現在の環境に保存されているすべてのファイルとディレクトリを確認できます。\n",
    "\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93c43",
   "metadata": {},
   "source": [
    "## 提出ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:16.018556Z",
     "iopub.status.busy": "2024-05-19T03:34:16.018251Z",
     "iopub.status.idle": "2024-05-19T03:34:16.035456Z",
     "shell.execute_reply": "2024-05-19T03:34:16.034339Z",
     "shell.execute_reply.started": "2024-05-19T03:34:16.018528Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 提出ファイルであるmain.pyを作成します。\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from typing import Literal, List\n",
    "import os, sys, json\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    os.chdir(os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    #WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    os.chdir(\"/kaggle/working/submission/lib\")\n",
    "    #WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "    \n",
    "print(f\"Current Directory is {os.getcwd()}. \\nFiles in here: {', '.join(os.listdir())}\")\n",
    "\n",
    "# モデルをインポートします。\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,\n",
    "    cache_dir=\"./huggingface\"\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=\"./huggingface\")\n",
    "hf_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask(prompt: str, max_new_tokens=100) -> str:\n",
    "    result = hf_llm(text_inputs=prompt, return_full_text=False, temperature=0.2, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "assert ask(\"<|user|>\\nHello!<|end|>\\n<|assistant|>\")\n",
    "\n",
    "@dataclass\n",
    "class KaggleObservation:\n",
    "  remainingOverageTime: int | float\n",
    "  step: int\n",
    "\n",
    "  questions: list[str]\n",
    "  answers: list[str]\n",
    "  guesses: list[str]\n",
    "\n",
    "  role: Literal[\"guesser\", \"answerer\"]\n",
    "  turnType: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "\n",
    "  keyword: str\n",
    "  category: str\n",
    "\n",
    "@dataclass\n",
    "class KaggleConfig:\n",
    "  episodeSteps: int\n",
    "  actTimeout: int | float\n",
    "  runTimeout: int | float\n",
    "  agentTimeout: int | float\n",
    "  __raw_path__: str\n",
    "\n",
    "# llm = Llama(\n",
    "#   model_path=WEIGHTS_PATH,  # GGUFファイルへのパス\n",
    "#   n_ctx=2048,  # 使用する最大シーケンス長 - シーケンスが長くなるほど多くのリソースが必要になります\n",
    "#   n_threads=4, # 使用するCPUスレッド数。システムや性能に応じて調整します\n",
    "#   n_gpu_layers=35, # GPUにオフロードするレイヤーの数。GPUアクセラレーションが使用できない場合は0に設定します。\n",
    "#   use_mlock=True, # RAMにメモリをロックし、ディスクにスワップされないようにするかどうか。大きなモデルには有用です\n",
    "#   use_mmap=False, #\n",
    "# )\n",
    "\n",
    "def get_context_prompt(observation: KaggleObservation) -> str:\n",
    "  questions = observation.questions\n",
    "  answers = observation.answers\n",
    "\n",
    "  history_prompt = \"\"\n",
    "  for index in range(len(max(questions, answers))):\n",
    "    history_prompt += f\"<|user|>\\n{questions[index]}<|end|>\\n\" if index < len(questions) else \"\"\n",
    "    history_prompt += f\"<|assistant|>\\n{answers[index]}<|end|>\\n\" if index < len(answers) else \"\"\n",
    "  #history_prompt += \"<|assistant|>\\n\"\n",
    "  \n",
    "  return history_prompt\n",
    "\n",
    "def get_guesser_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nLet's play 20 Questions. You are playing the role of the {observation.role.title()}.<|end|>\\n\"\n",
    "  prompt += get_context_prompt(observation)\n",
    "\n",
    "  if observation.turnType == \"ask\":\n",
    "    prompt += f\"<|user|>\\nTake a break, and ask a short yes-or-no question that would be useful to determine what the city I'm thinking about. Previous questions have been listed above. KEEP YOUR QUESTION ONE SENTENCE ONLY! Do not add any explaination to why you chose the question.<|end|>\\n\"\n",
    "  elif observation.turnType == \"guess\":\n",
    "    prompt += f\"<|user|>\\nNow, based on the information above, guess what city I'm thinking about, \" \n",
    "    prompt += f\"which aren't these: {', '.join(observation.guesses)}.\"\n",
    "    prompt += f\"Now, Make an informed guess, and only provide one word!<|end|>\\n\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid turnType: {observation.turnType}\\n\\n{observation}\")\n",
    "  \n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "def get_answerer_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nYou are a highly experienced tour guide specialized in the city {', '.join(observation.keyword.split(' '))}.\\n\"\n",
    "  prompt += \"You must answer a question about this city accurately, but only using the word **yes** or **no**.<|end|>\\n\"\n",
    "\n",
    "  prompt += f\"<|user|>{observation.questions[-1]}<|end|>\\n\"\n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "def play(obs, conf):\n",
    "  print(\"Observation: \" + json.dumps(obs, indent=2, ensure_ascii=False))\n",
    "  print(\"Confing: \" + json.dumps(conf, indent=2, ensure_ascii=False))\n",
    "  observation = KaggleObservation(**obs)\n",
    "  config = KaggleConfig(**conf)\n",
    "  if observation.role == \"guesser\":\n",
    "    prompt = get_guesser_prompt(observation)\n",
    "    result = ask(prompt, max_new_tokens=40).split(\"\\n\")[0].strip()#, stop=[\"<|end|>\"], max_tokens=256, temperature=0.5, echo=False)\n",
    "  elif observation.role == \"answerer\":\n",
    "    prompt = get_answerer_prompt(observation)\n",
    "    answer = ask(prompt, max_new_tokens=20)#, stop=[\"<|end|>\"], max_tokens=20, temperature=0.5, echo=False)\n",
    "    result = \"no\" if \"no\" in answer else \"yes\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid role: {observation.role}\\n\\n{observation}\")\n",
    "  print(f\"Result: {result}\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e7c28",
   "metadata": {},
   "source": [
    "## ちょっと確認中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:19.709658Z",
     "iopub.status.busy": "2024-05-19T03:34:19.708926Z",
     "iopub.status.idle": "2024-05-19T03:34:35.618916Z",
     "shell.execute_reply": "2024-05-19T03:34:35.617816Z",
     "shell.execute_reply.started": "2024-05-19T03:34:19.709623Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提出ファイルのmain.pyからすべての定義をインポートします。\n",
    "# これにより、main.pyの関数やクラスをこのセル内で使用できるようにします。\n",
    "\n",
    "from submission.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:35.621549Z",
     "iopub.status.busy": "2024-05-19T03:34:35.620903Z",
     "iopub.status.idle": "2024-05-19T03:34:37.664916Z",
     "shell.execute_reply": "2024-05-19T03:34:37.663875Z",
     "shell.execute_reply.started": "2024-05-19T03:34:35.621517Z"
    }
   },
   "outputs": [],
   "source": [
    "# play関数を呼び出して、仮のKaggleObservationおよびKaggleConfigオブジェクトを作成します。\n",
    "# assert文を使用して、play関数が正しく動作するか確認します。\n",
    "# ここでは、'guesser'のロールで質問をするターンを模擬しており、まだ質問や推測は行われていません。\n",
    "\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'guesser', \n",
    "  'turnType': 'ask', \n",
    "  'keyword': '', # 例: bangkok\n",
    "  'category': '', # 例: city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:37.666437Z",
     "iopub.status.busy": "2024-05-19T03:34:37.666159Z",
     "iopub.status.idle": "2024-05-19T03:34:38.716761Z",
     "shell.execute_reply": "2024-05-19T03:34:38.71582Z",
     "shell.execute_reply.started": "2024-05-19T03:34:37.666414Z"
    }
   },
   "outputs": [],
   "source": [
    "# play関数を呼び出し、今回は'answerer'のロールで、特定の質問に対して回答を返すターンを模擬します。\n",
    "# アサーションを使用して、play関数が正しく機能するかを確認します。\n",
    "# 質問として、「考えている都市は北アメリカにありますか？」が与えられています。\n",
    "\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [\"Is the city you're thinking of located in North America?\"], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'answerer', \n",
    "  'turnType': 'answer', \n",
    "  'keyword': '', # 例: bangkok\n",
    "  'category': '', # 例: city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8b8c5",
   "metadata": {},
   "source": [
    "## 提出するためにディレクトリをtar.gz形式でアーカイブする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tar.gzファイルを圧縮するためにpigzとpvというツールをインストールします。\n",
    "# インストールの出力は表示しないように/dev/nullにリダイレクトしています。\n",
    "\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 作業ディレクトリを/kaggle/working/に移動します。\n",
    "# ここでアーカイブを作成する準備を行います。\n",
    "\n",
    "%cd /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submissionディレクトリをtar.gz形式でアーカイブします。\n",
    "# pigzを使用して圧縮し、pvで進行状況を表示します。\n",
    "# -Cオプションは指定したディレクトリに移動してからアーカイブを作成するためのものです。\n",
    "# このコマンドでは、submission.tar.gzというファイルが作成されます。\n",
    "\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# アーカイブの作成が成功したことを示すメッセージを出力します。\n",
    "\n",
    "print(\"Success.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
