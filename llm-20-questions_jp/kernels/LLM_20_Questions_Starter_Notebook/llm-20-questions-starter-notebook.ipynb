{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8442f6e1",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyter Notebookã¯ã€Kaggleã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã€ŒLLM 20 Questionsã€ã«ãŠã‘ã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€20ã®è³ªå•ã‚²ãƒ¼ãƒ ã‚’åŠ¹æœçš„ã«ãƒ—ãƒ¬ã‚¤ã™ã‚‹ãŸã‚ã®è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ç”Ÿæˆã—ã€æœ€çµ‚çš„ã« `submission.tar.gz` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "### å•é¡Œã¨ç›®çš„\n",
    "ã€ŒLLM 20 Questionsã€ã¯ã€ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒè³ªå•ã‚’é€šã˜ã¦ç‰¹å®šã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ¨æ¸¬ã™ã‚‹ã‚²ãƒ¼ãƒ ã§ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€è³ªå•è€…ã¨å›ç­”è€…ã®å½¹å‰²ã‚’æŒã¤AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’é–‹ç™ºã—ã€åŠ¹ç‡çš„ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’æ¨æ¸¬ã™ã‚‹èƒ½åŠ›ã‚’é«˜ã‚ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "- **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ä¾å­˜é–¢ä¿‚**: \n",
    "  - `immutabledict` ã‚„ `sentencepiece` ãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ãƒ¢ãƒ‡ãƒ«ã®å‹•ä½œã«å¿…è¦ãªãƒ•ã‚¡ã‚¤ãƒ«ã‚’é©åˆ‡ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«é…ç½®ã—ã¾ã™ã€‚\n",
    "  - GitHubã‹ã‚‰`gemma_pytorch`ãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã€ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ãŸä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã•ã›ã¾ã™ã€‚\n",
    "\n",
    "- **ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¨åˆæœŸåŒ–**: \n",
    "  - ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«ã¯Gemmaãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã€ç‰¹ã«`GemmaForCausalLM`ã‚¯ãƒ©ã‚¹ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚ã“ã“ã§ã¯7BãŠã‚ˆã³2Bã®ãƒ¢ãƒ‡ãƒ«è¨­å®šãŒç”¨æ„ã•ã‚Œã¦ãŠã‚Šã€æŒ‡å®šã—ãŸãƒãƒªã‚¢ãƒ³ãƒˆã«å¿œã˜ã¦é©åˆ‡ãªãƒ¢ãƒ‡ãƒ«ã‚’å‹•çš„ã«é¸æŠã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "- **ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿**: \n",
    "  - `GemmaFormatter`ã‚¯ãƒ©ã‚¹ã‚’å®šç¾©ã—ã¦ã€ã‚²ãƒ¼ãƒ ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é©åˆ‡ã«ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã™ãŸã‚ã®æº–å‚™ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "- **ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹**: \n",
    "  - `GemmaQuestionerAgent`ã¨`GemmaAnswererAgent`ã¨ã„ã†2ã¤ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚¯ãƒ©ã‚¹ãŒå®Ÿè£…ã•ã‚Œã€è³ªå•è€…ã¨å›ç­”è€…ã®å½¹å‰²ã‚’ãã‚Œãã‚Œæ‹…å½“ã—ã¾ã™ã€‚ãã‚Œãã‚Œã®ã‚¯ãƒ©ã‚¹ã¯å†…éƒ¨ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç”Ÿæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰ã®å¿œç­”ã‚’è§£æã—ã¦æ­£ã—ã„è³ªå•ã‚„ç­”ãˆã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "\n",
    "### å‡ºåŠ›\n",
    "æœ€çµ‚çš„ã«ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯`submission.tar.gz`ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«æå‡ºã§ãã‚‹çŠ¶æ…‹ã«ã—ã¾ã™ã€‚ã“ã®æˆæœç‰©ã«ã¯ã€ã™ã¹ã¦ã®å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã¨å®Ÿè¡Œå¯èƒ½ãªã‚³ãƒ¼ãƒ‰ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ãŒå”èª¿ãƒ—ãƒ¬ã‚¤ã«åŸºã¥ã„ãŸæ¨ç†ã‚’è¡Œã†ãŸã‚ã®å‡ºç™ºç‚¹ã¨ã—ã¦æ©Ÿèƒ½ã—ã¾ã™ã€‚å‚åŠ è€…ã¯ã“ã®åŸºç›¤ã‚’å…ƒã«ã€è‡ªã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã‚„æˆ¦ç•¥ã‚’å®Ÿè£…ã—ã€ã•ã‚‰ã«æ”¹å–„ã™ã‚‹ã“ã¨ãŒæœŸå¾…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4528407",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã¯ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«é–¢é€£ã—ãŸå°‚é–€ç”¨èªã®ç°¡å˜ãªè§£èª¬ã§ã™ã€‚åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªãƒã‚¤ãƒŠãƒ¼ãªæ¦‚å¿µã‚„ç‰¹æœ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "1. **LLM (Large Language Model)**: å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ã£ã¦è¨“ç·´ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€äººé–“ã®ã‚ˆã†ã«æ–‡ç« ã‚’ç†è§£ã—ç”Ÿæˆã™ã‚‹èƒ½åŠ›ã‚’æŒã¤ã€‚ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ï¼ˆã“ã®å ´åˆã¯20ã®è³ªå•ã‚²ãƒ¼ãƒ ï¼‰ã«èª¿æ•´ã•ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã€‚\n",
    "\n",
    "2. **Few-shot examples**: ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦æ•°ä¾‹ã®å…¥åŠ›ã‚’ä¸ãˆã€ãã®ä¾‹ã‚’åŸºã«ãƒ¢ãƒ‡ãƒ«ãŒã‚¿ã‚¹ã‚¯ã‚’ç†è§£ã—ã¦å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚ä¾‹ãˆã°ã€è³ªå•ã¨ç­”ãˆã®ãƒšã‚¢ã‚’ç¤ºã™ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒãã®å½¢å¼ã‚’å­¦ã¶ã€‚\n",
    "\n",
    "3. **Gemma**: GoogleãŒé–‹ç™ºã—ãŸç‰¹å®šã®LLMã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã§ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã®ãŸã‚ã«ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã‚‹ã€‚gemma_pytorchã¯ãã®PyTorchå®Ÿè£…ã‚’å«ã‚€ãƒªãƒã‚¸ãƒˆãƒªã€‚\n",
    "\n",
    "4. **Causal LM (Causal Language Model)**: æ–‡è„ˆã‹ã‚‰æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«è‡ªå‹•å›å¸°ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã€‚éå»ã®æƒ…å ±ã‚’åŸºã«æœªæ¥ã®æƒ…å ±ã‚’ç”Ÿæˆã™ã‚‹èƒ½åŠ›ãŒã‚ã‚‹ã€‚\n",
    "\n",
    "5. **Tokenization**: æ–‡ã‚’æ§‹æˆã™ã‚‹å˜èªã‚„è¨˜å·ã‚’ã€Œãƒˆãƒ¼ã‚¯ãƒ³ã€ã¨å‘¼ã°ã‚Œã‚‹å°ã•ãªå˜ä½ã«åˆ†å‰²ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã¯ã€ãƒ¢ãƒ‡ãƒ«ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’ç†è§£ã—ã‚„ã™ãã™ã‚‹ãŸã‚ã®é‡è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§ã™ã€‚\n",
    "\n",
    "6. **Weight files (é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«)**: ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãŠã‘ã‚‹å­¦ç¿’çµæœï¼ˆãƒ¢ãƒ‡ãƒ«ãŒãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å­¦ã¶éç¨‹ã§å¾—ã‚‰ã‚Œã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼‰ã®ã“ã¨ã€‚ã“ã‚Œã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€è¨“ç·´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã™ã‚‹ãŸã‚ã«å¿…è¦ã§ã™ã€‚\n",
    "\n",
    "7. **Sampling parameters**: ç”Ÿæˆã™ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã®å¤šæ§˜æ€§ã‚„ä¸€è²«æ€§ã‚’èª¿æ•´ã™ã‚‹ãŸã‚ã®è¨­å®šï¼ˆä¾‹ï¼štemperature, top_k, top_pï¼‰ã€‚ã“ã‚Œã‚‰ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã®ã‚¹ã‚¿ã‚¤ãƒ«ã‚„å‰µé€ æ€§ãŒå¤‰ã‚ã‚‹ã€‚\n",
    "\n",
    "8. **Device (ãƒ‡ãƒã‚¤ã‚¹)**: ãƒ¢ãƒ‡ãƒ«ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã€‚å¤šãã®å ´åˆã€GPUï¼ˆã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ã‚¹å‡¦ç†ãƒ¦ãƒ‹ãƒƒãƒˆï¼‰ã‚’æ„å‘³ã—ã¾ã™ã€‚æ·±å±¤å­¦ç¿’ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚„æ¨è«–ã«ãŠã„ã¦ã¯ã€GPUã‚’åˆ©ç”¨ã™ã‚‹ã“ã¨ã§è¨ˆç®—é€Ÿåº¦ãŒå¤§å¹…ã«å‘ä¸Šã—ã¾ã™ã€‚\n",
    "\n",
    "9. **Interleave**: ç•°ãªã‚‹2ã¤ã®ãƒªã‚¹ãƒˆã®è¦ç´ ã‚’äº¤äº’ã«çµ„ã¿åˆã‚ã›ã‚‹æ“ä½œã€‚ã“ã“ã§ã¯ã€è³ªå•ã¨å›ç­”ã‚’äº¤äº’ã«çµåˆã—ã¦ã‚²ãƒ¼ãƒ ã®é€²è¡ŒçŠ¶æ³ã‚’ä¿æŒã—ã¾ã™ã€‚\n",
    "\n",
    "10. **Context manager**: Pythonã®æ©Ÿèƒ½ã§ã€ç‰¹å®šã®å‡¦ç†ãŒå§‹ã¾ã‚‹å‰ã«çŠ¶æ…‹ã‚’è¨­å®šã—ã€å‡¦ç†ãŒçµ‚ã‚ã£ãŸå¾Œã«å…ƒã®çŠ¶æ…‹ã«æˆ»ã™ãŸã‚ã®æ§‹æ–‡ã€‚ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ã«ä¾¿åˆ©ã§ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªã¯ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã ã‘ã§ãªãã€æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ã«ãŠã‘ã‚‹å®Ÿå‹™ã‚„ç ”ç©¶ã§ã‚‚ã‚ˆãä½¿ã‚ã‚Œã‚‹æ¦‚å¿µã§ã™ã€‚åˆå¿ƒè€…ã¯ã“ã‚Œã‚‰ã‚’ç†è§£ã™ã‚‹ã“ã¨ã§ã€å­¦ç¿’ã‚’æ·±ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ef682",
   "metadata": {},
   "source": [
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**LLM 20 Questions** ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€`submission.tar.gz` ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ã€å³å´ã® **Submit to competition** è¦‹å‡ºã—ã‹ã‚‰ç›´æ¥æå‡ºã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã‚ã‚‹ã„ã¯ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ“ãƒ¥ãƒ¼ã‚¢ã‹ã‚‰ *Output* ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€`submission.tar.gz` ã‚’è¦‹ã¤ã‘ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ç«¶æŠ€ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®å·¦ä¸Šã«ã‚ã‚‹ **Submit Agent** ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€æå‡ºã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working  # ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece  # å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null  # gemma_pytorchãƒªãƒã‚¸ãƒˆãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™\n",
    "mkdir /kaggle/working/submission/lib/gemma/  # gemmaç”¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆã—ã¾ã™\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/  # gemmaã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç§»å‹•ã•ã›ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# è¨­å®š\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **é‡è¦:** ã‚³ãƒ¼ãƒ‰ãŒãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã®ä¸¡æ–¹ã§å‹•ä½œã™ã‚‹ã‚ˆã†ã«ã€ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’æ¬¡ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¾ã™\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))  # KAGGLE_AGENT_PATHã®libãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ‘ã‚¹ã«è¿½åŠ ã—ã¾ã™\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")  # æŒ‡å®šã®ãƒ‘ã‚¹ãŒå­˜åœ¨ã—ãªã„å ´åˆã€åˆ¥ã®ãƒ‘ã‚¹ã‚’è¿½åŠ ã—ã¾ã™\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b  # gemmaãƒ¢ãƒ‡ãƒ«è¨­å®šã®å–å¾—\n",
    "from gemma.model import GemmaForCausalLM  # gemmaãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHãŒå­˜åœ¨ã™ã‚‹å ´åˆ\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")  # é‡ã¿ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"  # ä»–ã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'  # ã‚¿ãƒ¼ãƒ³ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "    _end_token = '<end_of_turn>'  # ã‚¿ãƒ¼ãƒ³ã®çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ä¿å­˜\n",
    "        self._few_shot_examples = few_shot_examples  # Few-shotä¾‹ã®ä¿å­˜\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"  # ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ãƒ³ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "        self.reset()  # çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state  # ç¾åœ¨ã®çŠ¶æ…‹ã‚’è¿”ã™\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¿ãƒ¼ãƒ³ã®é–‹å§‹ã‚’è¿½åŠ \n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"  # ãƒ¢ãƒ‡ãƒ«ã‚¿ãƒ¼ãƒ³ã®é–‹å§‹ã‚’è¿½åŠ \n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"  # ã‚¿ãƒ¼ãƒ³ã®çµ‚äº†ã‚’è¿½åŠ \n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"  # çŠ¶æ…‹ã‚’åˆæœŸåŒ–\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’è¿½åŠ \n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # Few-shotä¾‹ã®ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]  # ã‚¿ãƒ¼ãƒ³ã®å®Ÿè¡Œé †åºã‚’æ±ºå®š\n",
    "        formatters = itertools.cycle(formatters)  # é †åºã‚’å¾ªç’°ã•ã›ã‚‹\n",
    "        for fmt, turn in zip(formatters, turns):  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã¨ã‚¿ãƒ¼ãƒ³ã‚’çµåˆ\n",
    "            fmt(turn)  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã§ã‚¿ãƒ¼ãƒ³ã‚’å‡¦ç†\n",
    "        return self\n",
    "\n",
    "\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆå®šç¾©\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸdtypeã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®torch dtypeã‚’è¨­å®šã—ã¾ã™ã€‚\"\"\"\n",
    "    torch.set_default_dtype(dtype)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®dtypeã‚’å¤‰æ›´\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)  # å…ƒã®dtypeã«æˆ»ã™\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒãƒªã‚¢ãƒ³ãƒˆã‚’ä¿å­˜\n",
    "        self._device = torch.device(device)  # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹ã‚’æŒ‡å®š\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’åˆæœŸåŒ–\n",
    "\n",
    "        print(\"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()  # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "        model_config.quant = \"quant\" in variant  # é‡å­åŒ–è¨­å®š\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):  # ãƒ‡ãƒ¼ã‚¿å‹ã‚’è¨­å®šã—ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            model = GemmaForCausalLM(model_config)  # ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')  # é‡ã¿ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆãƒ‘ã‚¹ã‚’è¨­å®š\n",
    "            model.load_weights(ckpt_path)  # é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "            self.model = model.to(self._device).eval()  # ãƒ¢ãƒ‡ãƒ«ã‚’æŒ‡å®šãƒ‡ãƒã‚¤ã‚¹ã«ç§»å‹•ã—è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«ã™ã‚‹\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹\n",
    "        prompt = str(self.formatter)  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã®å†…å®¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›\n",
    "        response = self._call_llm(prompt)  # LLMã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ¸¡ã—ã¦å¿œç­”ã‚’å¾—ã‚‹\n",
    "        response = self._parse_response(response, obs)  # å¿œç­”ã‚’è§£æ\n",
    "        print(f\"{response=}\")  # å¿œç­”ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½è±¡åŒ–\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {  # ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®š\n",
    "                'temperature': 0.01,  # æ¸©åº¦ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "                'top_p': 0.1,  # ãƒˆãƒƒãƒ—ç¢ºç‡\n",
    "                'top_k': 1,  # ãƒˆãƒƒãƒ—Kã®è¨­å®š\n",
    "        }\n",
    "        response = self.model.generate(  # ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰æ–°ã—ã„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆ\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,  # æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°\n",
    "            **sampler_kwargs,  # ãã®ä»–ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º\n",
    "        if match is None:\n",
    "            keyword = ''  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆ\n",
    "        else:\n",
    "            keyword = match.group().lower()  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å°æ–‡å­—ã«å¤‰æ›\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æŠ½è±¡åŒ–\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [  # ä¸å‡ä¸€ãªãƒªã‚¹ãƒˆã‚’äº¤äº’ã«çµåˆ\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ã‚²ãƒ¼ãƒ é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # è³ªå•ã¨å›ç­”ã‚’äº¤äº’ã«çµåˆ\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")  # è³ªå•ã™ã‚‹ã‚ˆã†æŒ‡ç¤º\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰äºˆæƒ³ã®æŒ‡ç¤º\n",
    "        self.formatter.start_model_turn()  # ãƒ¢ãƒ‡ãƒ«ã‚¿ãƒ¼ãƒ³ã‚’é–‹å§‹\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # è³ªå•ã‚’æŠ½å‡º\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®è³ªå•\n",
    "            else:\n",
    "                question = match.group()  # æŠ½å‡ºã—ãŸè³ªå•\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®äºˆæƒ³ã‚’è§£æ\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)  # ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # è¦ªã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # ã‚²ãƒ¼ãƒ é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # è³ªå•ã¨å›ç­”ã‚’äº¤äº’ã«çµåˆ\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")  # å›ç­”æŒ‡ç¤ºãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "        self.formatter.start_model_turn()  # ãƒ¢ãƒ‡ãƒ«ã‚¿ãƒ¼ãƒ³ã‚’é–‹å§‹\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # å›ç­”ã‚’è§£æ\n",
    "        return 'yes' if 'yes' in answer else 'no'  # \"yes\"ã§ã‚ã‚Œã°yesã€ãã®ä»–ã¯noã‚’è¿”ã™\n",
    "\n",
    "\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆ\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"  # ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®è¨­å®š\n",
    "\n",
    "few_shot_examples = [  # Few-shotä¾‹ã®è¨­å®š\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "# **é‡è¦:** ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«å®šç¾©ã—ã¾ã™ã€‚å¿…è¦ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã ã‘ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã€‚\n",
    "# ä¸¡æ–¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€OOMï¼ˆOut of Memoryï¼‰ã«ç¹‹ãŒã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # å›ç­”è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–\n",
    "    assert agent is not None, \"Agent not initialized.\"  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª\n",
    "\n",
    "    return agent  # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’è¿”ã™\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)  # è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¿œç­”ã‚’å–å¾—\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)  # è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¿œç­”ã‚’å–å¾—\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # å›ç­”è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å¿œç­”ã‚’å–å¾—\n",
    "    if response is None or len(response) <= 1:  # å¿œç­”ãŒç©ºã‹é•·ã•ãŒ1ä»¥ä¸‹ã®å ´åˆ\n",
    "        return \"yes\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å¿œç­”\n",
    "    else:\n",
    "        return response  # é€šå¸¸ã®å¿œç­”ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzã¨pvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2  # submission.tar.gzãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db73b66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ã‚³ãƒ¡ãƒ³ãƒˆ \n",
    "\n",
    "> ## Samar Elhissi\n",
    "> \n",
    "> ä¾‹ã‚’ã‚ã‚ŠãŒã¨ã†ã€ãƒ­ãƒ¼ã‚«ãƒ«ã§ãƒ†ã‚¹ãƒˆã™ã‚‹ã«ã¯ã©ã†ã™ã‚Œã°è‰¯ã„ã§ã™ã‹ï¼Ÿ\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Valentin Baltazar\n",
    "> > \n",
    "> > ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„â€¦ã“ã®LLMã¯å¤šãã®è¨ˆç®—ã‚’å¿…è¦ã¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å¼·åŠ›ãªGPUãŒå¿…è¦ã§ã™ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒã€ã¯ã‚‹ã‹ã«ç°¡å˜ã§ã™ã€‚\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Michael Kamal 92\n",
    "> \n",
    "> ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ã€few_shot_examplesã«ã¤ã„ã¦è³ªå•ã—ãŸã„ã®ã§ã™ãŒã€ã©ã®ã‚ˆã†ã«ä½œæˆã™ã‚Œã°è‰¯ã„ã§ã™ã‹ï¼Ÿ\n",
    "> > ä¾‹ãˆã°ã€( 'is it place?', 'yes-or-no' )ã®ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã‹ã€ãã‚Œã¨ã‚‚( 'is it place?', 'yes', ) ã®ã‚ˆã†ã«ã™ã‚Œã°è‰¯ã„ã§ã™ã‹ï¼Ÿãã‚Œã¨ã‚‚( 'is it place?', 'yes', 'Now guess the keyword' )ã§ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã¨ã‚‚( 'is it place?', 'no', 'Now guess the keyword', 'France' )ã§ã—ã‚‡ã†ã‹ï¼Ÿãã‚Œã¨ã‚‚( 'is it place?', 'yes', 'France' )ã§ã—ã‚‡ã†ã‹ï¼Ÿã©ã‚ŒãŒæ­£ã—ã„è³ªå•ã€å›ç­”ã€äºˆæ¸¬ã®ä½œã‚Šæ–¹ã§ã™ã‹ï¼Ÿ\n",
    "> \n",
    "> ã‚‚ã†ä¸€ã¤ã®è³ªå•ã§ã™ãŒã€Gemmaã¯few_shot_examplesã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "> ## Yukky_2801\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€ç§ã¯Kaggleã®åˆå¿ƒè€…ã§ã™ã€‚ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã—ãŸï¼š\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: å‰ã®ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã«å¤±æ•—ã—ãŸã¨çµ‚äº†ã—ã¾ã™ã€‚\n",
    "> \n",
    "> submission.tar.gzã‚’ã‚¨ãƒ©ãƒ¼ã¨å…±ã«æå‡ºã§ãã¾ã›ã‚“ã€‚ã©ã†ã„ã†ã“ã¨ã‹ã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€è§£æ±ºç­–ã‚’æä¾›ã—ã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> > ã‚‚ã¡ã‚ã‚“ã€Œgemma/pytorch/7b-it-quant/2ã€ã“ã®ãƒ‘ã‚¹ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒãã®ãƒ‘ã‚¹ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å³å´ã‚’è¦‹ã¦ã€gemmaãƒ¢ãƒ‡ãƒ«ãŒãã“ã®ãƒ‘ã‚¹ã¨ä¸€è‡´ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼‰ã€‚\n",
    ">   \n",
    "> > ## Aryan Singh\n",
    "> > > Gemma 7b-it-quant V2ã‚’è¿½åŠ ã™ã‚‹ã«ã¯ã€Add Inputæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "> > > \n",
    "> > > ã¾ãšã€ã“ã¡ã‚‰ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š[https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > \n",
    "> > \n",
    "> > > ## Talal Mufti\n",
    "> > > > ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒãã®ãƒ‘ã‚¹ã«ã‚ã‚‹ã“ã¨ã‚’ç¢ºèªã—ãŸå¾Œã€ä¾ç„¶ã¨ã—ã¦å•é¡ŒãŒç™ºç”Ÿã—ãŸãŸã‚ã€bashã‚³ãƒãƒ³ãƒ‰ã‚’å°‘ã—ä¿®æ­£ã—ã¾ã—ãŸã€‚å€‹äººçš„ã«ã¯ã€ã“ã‚ŒãŒç§ã«ã¨ã£ã¦ã†ã¾ãã„ãã¾ã—ãŸï¼š\n",
    "> > > > !tar --use-compress-program='pigz --fast --recursive | pv' -f submission.tar.gz -c /kaggle/working/submission . -c /kaggle/input/gemma/pytorch/7b-it-quant/2\n",
    "\n",
    "---\n",
    "\n",
    "> ## Muhammad Hadi13\n",
    "> \n",
    "> ãªãœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å®Ÿè¡Œã—ã¦ã„ã‚‹ã®ã«ã€å¸¸ã«1.35MBä»¥ä¸Šã®å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œãšã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§å¤±æ•—ã™ã‚‹ã®ã‹ã‚ã‹ã‚Šã¾ã›ã‚“ã€‚Ryanã®å‡ºåŠ›ã¯ç´„7GBã§ã—ãŸã€‚ã“ã®ä»¶ã«ã¤ã„ã¦åŠ©ã‘ãŒå¿…è¦ã§ã™ï¼ï¼\n",
    "> \n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> > tar: gemma/pytorch: Cannot stat: No such file or directory\n",
    "> \n",
    "> tar: å‰ã®ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã«çµ‚äº†ã—ã¾ã—ãŸã€‚\n",
    "\n",
    "> ## Aryan Singh\n",
    "> > > Gemma 7b-it-quant V2ã‚’äº‹å‰ã«è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "> > > \n",
    "> > > ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã™ã‚‹ã«ã¯ã€Add inputæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "> > > \n",
    "> > > ã¾ãšã€ã“ã¡ã‚‰ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼š[https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "\n",
    "---\n",
    "\n",
    "> ## Ship of Theseus\n",
    "> \n",
    "> Thank Ryan, greate work! Nice code to run on localhost and sharing to Kaggle Community\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## shiv_314\n",
    "> \n",
    "> çš†ã•ã‚“ï¼ä¸€ã¤åŠ©ã‘ãŒå¿…è¦ã§ã™ã€‚gemmaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚\n",
    "> \n",
    "> Pythonã®ãŸã‚ã«ã™ã§ã«æ­£ã—ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’è¿½åŠ ã—ã¾ã—ãŸãŒã€ãã‚Œã§ã‚‚åŒã˜å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚åŠ©ã‘ã¦ãã ã•ã„ï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## dedq\n",
    "> \n",
    "> Thank Ryan, greate work! Nice code to run on localhost and sharing to Kaggle Community\n",
    "\n",
    "---\n",
    "\n",
    "> ## Code Hacker\n",
    "> \n",
    "> ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«tar.gzã‚’æå‡ºã—ã‚ˆã†ã¨ã—ãŸãŒã€å¤±æ•—ã—ã¾ã—ãŸâ€¦\n",
    "\n",
    "> ## Code Hacker\n",
    "> > > ã“ã®ãƒ¢ãƒ‡ãƒ«ã«åŒæ„ã—ãªã‹ã£ãŸã€‚ä¸‹ã®èµ¤ã„ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯â€¦\n",
    "\n",
    "---\n",
    "\n",
    "> ## JAPerez\n",
    "> \n",
    "> Great work Ryan!\n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€ç§ã¯ã“ã®ã‚³ãƒ³ãƒšã®åˆå¿ƒè€…ã§ã™ã€‚ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦æå‡ºã—ã‚ˆã†ã¨ã—ã¾ã—ãŸã€‚\n",
    "> æå‡ºç‰©ã®ãƒ•ã‚¡ã‚¤ãƒ«ã§ã‚ã‚‹submission.tar.gzã‚’ã©ã“ã«ç½®ã‘ã°ã‚ˆã„ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
    "> Submit agentsãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸå¾Œã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãã®ã¾ã¾æå‡ºã™ã‚Œã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ \n",
    "> æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™\n",
    "> åŸºæœ¬çš„ãªè³ªå•ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€è¿”ä¿¡ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
    "\n",
    "> ## Kanchan Maurya\n",
    "> > > Submit agentsã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸå¾Œã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡ºã—ã¦ã„ã¾ã™ã€‚ãã‚Œã«æ™‚é–“ãŒã‹ã‹ã‚‹ã®ã¯ã€åˆæœŸã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒæ©Ÿèƒ½ã—ã¦ã„ã‚‹ãŸã‚ã§ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "> ## vj4science\n",
    "> \n",
    "> Thanks Ryan - this is a good head start to the competition! much appreciated!\n",
    "\n",
    "---\n",
    "\n",
    "> ## gb_kwon\n",
    "> \n",
    "> Thank you so much for your COOL guidelines!\n",
    "\n",
    "---\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> \n",
    "> main.pyã§ã€gemma_pytorchãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’æ¬¡ã®ã‚ˆã†ã«ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼šfrom gemma.configã€‚\n",
    "> \n",
    "> ã“ã‚Œã¯ç§ã«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ãŒã€gemmaã¨ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¯å‡ºã¾ã›ã‚“ã€‚\n",
    "> \n",
    "> è‡ªåˆ†ã®ãƒ­ãƒ¼ã‚«ãƒ«ã®gemmaãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’æ‰‹å‹•ã§æŒ‡å®šã™ã‚‹ã®ã¨ã€Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªåã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã®ã‚’è©¦ã¿ã¾ã—ãŸãŒã€‚ä½•ã‹ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "> ## Duy Thai\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯[@ryanholbrook](https://www.kaggle.com/ryanholbrook)ã€ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’è©¦ã—ãŸã¨ã“ã‚ã€ã€Œæ·»ä»˜ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«è¿½åŠ ã®æ‰‹é †ãŒå¿…è¦ã§ã™ã€‚è©³ç´°ã¯Modelãƒ‘ãƒãƒ«ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ã€ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã—ãŸã€‚ãã‚Œã«ã¤ã„ã¦ã¯ã©ã†ã™ã‚Œã°ã‚ˆã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "> \n",
    "> ãƒ‘ãƒãƒ«ã‚’é–‹ã„ãŸã¨ãã€ç§ã¯ã“ã‚Œã ã‘ã‚’è¦‹ã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> > > \"Models\"ã¸è¡Œãã€Gemmaã‚’æ¤œç´¢ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã¾ã™ã€‚\n",
    "\n",
    "> ## Duy Thai\n",
    "> > > ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## Kai_Huang\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€ç§ã¯Kaggleã®åˆå¿ƒè€…ã§ã™ã€‚ã‚ãªãŸã® !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2 ã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ãŸã¨ãã€æ¬¡ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã—ãŸï¼š\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: å‰ã®ã‚¨ãƒ©ãƒ¼ã®ãŸã‚ã«çµ‚äº†ã—ã¾ã—ãŸ\n",
    "> \n",
    "> ã©ã†ã„ã†ã“ã¨ã‹ã‚ã‹ã‚‰ãªã„ã®ã§ã™ãŒã€æ•™ãˆã¦ã„ãŸã ã‘ã¾ã™ã‹ï¼Ÿã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
    "\n",
    "> ## Kai_Huang\n",
    "> > > ã‚ã‚ã€ã‚ã‹ã‚Šã¾ã—ãŸã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«å…¥åŠ›ã—ã¦ã„ãªã‹ã£ãŸã®ã§ã™ã­ğŸ˜±\n",
    "\n",
    "> ## D Prince Armand KOUMI\n",
    "> > > ãƒ¢ãƒ‡ãƒ«ãŒãªã„å ´åˆã¯ã€è¿½åŠ ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "> ## Qusay AL-Btoush\n",
    "> \n",
    "> ã¨ã¦ã‚‚è‰¯ã„ã€Ryan \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
