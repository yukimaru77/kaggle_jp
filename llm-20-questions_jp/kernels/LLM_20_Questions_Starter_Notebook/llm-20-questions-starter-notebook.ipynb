{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04b0fb5",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã€ŒLLM 20 Questionsã€ã«å‚åŠ ã™ã‚‹ãŸã‚ã®AIã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ä½œæˆã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€è³ªå•è€…ï¼ˆè³ªå•ã‚’ã™ã‚‹å´ï¼‰ã¨å›ç­”è€…ï¼ˆã¯ã„ã¾ãŸã¯ã„ã„ãˆã§å›ç­”ã™ã‚‹å´ï¼‰ã®2ã¤ã®ã‚¿ã‚¤ãƒ—ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å•é¡Œã«å–ã‚Šçµ„ã‚€å†…å®¹\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’æ´»ç”¨ã—ã¦ã€Œ20ã®è³ªå•ã€ã‚²ãƒ¼ãƒ ã‚’åŠ¹æœçš„ã«ãƒ—ãƒ¬ã‚¤ã§ãã‚‹ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¯ã€ã¯ã„ã¾ãŸã¯ã„ã„ãˆã§å›ç­”ã§ãã‚‹è³ªå•ã‚’é€šã˜ã¦ã€ç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã«ãŠã„ã¦ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ä¸€é€£ã®è³ªå•ã‚’çµŒã¦ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¯ãƒ¼ãƒ‰ã‚’ç‰¹å®šã™ã‚‹èƒ½åŠ›ã‚’è©•ä¾¡ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "### ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "1. **Gemmaãƒ¢ãƒ‡ãƒ«**: æœ¬ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€GoogleãŒé–‹ç™ºã—ãŸGemmaã¨ã„ã†è¨€èªãƒ¢ãƒ‡ãƒ«ãŒä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚ç‰¹ã«ã€`gemma_pytorch`ãƒªãƒã‚¸ãƒˆãƒªã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ãŒã‚¯ãƒ­ãƒ¼ãƒ³ã•ã‚Œã€ãã®æ§‹æˆã¨é‡ã¿ãŒåˆ©ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "2. **PyTorch**: ãƒ¢ãƒ‡ãƒ«ã¯PyTorchã‚’ç”¨ã„ã¦å®Ÿè£…ã•ã‚Œã€GPUã«ã‚ˆã‚‹è¨ˆç®—ãŒå¯èƒ½ã§ã™ã€‚\n",
    "3. **ã‚µãƒ–ãƒŸãƒƒã‚·ãƒ§ãƒ³ç”Ÿæˆ**: æœ€çµ‚çš„ã«ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯`submission.tar.gz`ã¨ã„ã†ãƒ•ã‚¡ã‚¤ãƒ«ã«ã¾ã¨ã‚ã‚‰ã‚Œã€Kaggleã«æå‡ºã•ã‚Œã‚‹å½¢å¼ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### æ§‹æˆ\n",
    "- ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ã¯ã€ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«æ‰‹é †ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®åˆæœŸåŒ–ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆ`GemmaQuestionerAgent`ï¼‰ã¨å›ç­”è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆ`GemmaAnswererAgent`ï¼‰ã®ãã‚Œãã‚Œã®ã‚¯ãƒ©ã‚¹å®šç¾©ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "- è³ªå•ã¨å¿œç­”ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ­ã‚¸ãƒƒã‚¯ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### ã‚³ãƒ¼ãƒ‰å®Ÿè¡Œã®æ‰‹é †\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å®Ÿè¡Œã«ã‚ˆã‚Šã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚„ãƒ¢ãƒ‡ãƒ«ãŒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€æ§‹ç¯‰ã•ã‚ŒãŸã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒKaggleã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒå†…ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚æœ€å¾Œã«ã€ç”Ÿæˆã•ã‚ŒãŸã‚µãƒ–ãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒ•ã‚¡ã‚¤ãƒ«ã‚’Kaggleã«æå‡ºã™ã‚‹ãŸã‚ã®åœ§ç¸®å‡¦ç†ãŒè¡Œã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ã‚ˆã†ã«ã€ã“ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€20ã®è³ªå•ã‚²ãƒ¼ãƒ ã«åŸºã¥ã„ãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’æ§‹ç¯‰ãƒ»æå‡ºã™ã‚‹ãŸã‚ã®åŒ…æ‹¬çš„ãªæ‰‹é †ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243ce61",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã¯ã€Jupyter Notebookã®å†…å®¹ã«é–¢é€£ã™ã‚‹å°‚é–€ç”¨èªã®ç°¡å˜ãªè§£èª¬ã§ã™ã€‚åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªãƒã‚¤ãƒŠãƒ¼ãªç”¨èªã‚„å®Ÿå‹™çµŒé¨“ãŒãªã„ã¨é¦´æŸ“ã¿ã®ãªã„ã‚‚ã®ã«ç„¦ç‚¹ã‚’å½“ã¦ã¾ã—ãŸã€‚\n",
    "\n",
    "1. **ImmutableDict**:\n",
    "   - å¤‰æ›´ä¸å¯ãªè¾æ›¸å‹ã®ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã€‚é€šå¸¸ã®è¾æ›¸ã¯è¦ç´ ã®è¿½åŠ ã‚„å‰Šé™¤ãŒå¯èƒ½ã§ã™ãŒã€ImmutableDictã¯ç”Ÿæˆå¾Œã«ãã®è¦ç´ ã‚’å¤‰æ›´ã§ãã¾ã›ã‚“ã€‚ã“ã®ç‰¹æ€§ã«ã‚ˆã£ã¦ã€ãƒ‡ãƒ¼ã‚¿ã®æ•´åˆæ€§ã‚’ä¿è¨¼ã§ãã¾ã™ã€‚\n",
    "\n",
    "2. **sentencepiece**:\n",
    "   - è‡ªç„¶è¨€èªå‡¦ç†ã®ãŸã‚ã®ãƒ†ã‚­ã‚¹ãƒˆã®ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚æ–‡ã‚’ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ï¼ˆé€šå¸¸ã€å˜èªæœªæº€ã®å˜ä½ï¼‰ã«åˆ†å‰²ã—ã€è¨€èªãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚ç‰¹ã«ã€ä½ãƒªã‚½ãƒ¼ã‚¹è¨€èªã‚„æ–°ã—ã„å˜èªã‚’æ‰±ã†éš›ã«åŠ¹æœçš„ã§ã™ã€‚\n",
    "\n",
    "3. **CausalLM**:\n",
    "   - è¦³å¯Ÿã•ã‚ŒãŸæƒ…å ±ã‹ã‚‰æ¬¡ã«æ¥ã‚‹æƒ…å ±ã‚’äºˆæ¸¬ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã§ã™ã€‚å…¸å‹çš„ãªç”¨é€”ã«ã¯è¨€èªç”ŸæˆãŒã‚ã‚Šã€æ¬¡ã®å˜èªã‚’äºˆæ¸¬ã—ãªãŒã‚‰æ–‡ã‚’å½¢æˆã—ã¾ã™ã€‚ã€ŒCAUSALã€ã¯æ™‚é–“çš„ãªå› æœé–¢ä¿‚ã«é‡ãã‚’ç½®ã„ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "4. **ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®torch dtype**:\n",
    "   - PyTorchã«ãŠã‘ã‚‹ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ‡ãƒ¼ã‚¿å‹è¨­å®šã€‚Tensorï¼ˆå¤šæ¬¡å…ƒé…åˆ—ï¼‰ã®è¨ˆç®—ç²¾åº¦ã‚’æŒ‡å®šã—ã€float32ã‚„float64ãªã©ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’ä½¿ã£ã¦æ•°å€¤æ¼”ç®—ãŒè¡Œãˆã¾ã™ã€‚ã“ã®è¨­å®šã‚’å¤‰æ›´ã™ã‚‹ã“ã¨ã§ã€è¨ˆç®—åŠ¹ç‡ã‚„ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã«å½±éŸ¿ã‚’ä¸ãˆã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "5. **Few-shot examples**:\n",
    "   - è»¢ç§»å­¦ç¿’ã«ãŠã„ã¦ã€å°‘æ•°ã®ä¾‹ï¼ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ï¼‰ã§ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹æŠ€è¡“ã€‚ä¾‹ãˆã°ã€è³ªå•ã®ä¾‹ã¨ãã®å¿œç­”ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒæ–°ã—ã„è³ªå•å½¢å¼ã«é©å¿œã™ã‚‹èƒ½åŠ›ã‚’é«˜ã‚ã¾ã™ã€‚\n",
    "\n",
    "6. **interleave_unequal**:\n",
    "   - ç•°ãªã‚‹é•·ã•ã®2ã¤ã®ãƒªã‚¹ãƒˆã‚’äº¤äº’ã«çµ„ã¿åˆã‚ã›ã‚‹å‡¦ç†ã‚’è¡Œã†é–¢æ•°ã€‚ä¸€æ–¹ã®ãƒªã‚¹ãƒˆã«è¦ç´ ãŒæ®‹ã£ã¦ã„ã‚‹å ´åˆã€ã¾ã è¦ç´ ã‚’è¿½åŠ ã—ç¶šã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€è³ªå•ã¨å›ç­”ãªã©ã®ãƒšã‚¢ã‚’ç®¡ç†ã™ã‚‹ã®ã«æœ‰ç”¨ã§ã™ã€‚\n",
    "\n",
    "7. **sampler_kwargs**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ç”Ÿæˆãƒ—ãƒ­ã‚»ã‚¹ã«ãŠã„ã¦ã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹æ³•ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®å¼•æ•°ã€‚ç”Ÿæˆæ™‚ã®ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã«ã€æ¸©åº¦ï¼ˆç”Ÿæˆã®å¤šæ§˜æ€§ï¼‰ã€top_kï¼ˆè€ƒæ…®ã™ã‚‹ä¸Šä½kå€™è£œï¼‰ã€top_pï¼ˆç¢ºç‡ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ï¼‰ãªã©ã®è¨­å®šãŒå«ã¾ã‚Œã¾ã™ã€‚\n",
    "\n",
    "8. **ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º**:\n",
    "   - ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‹ã‚‰ç‰¹å®šã®æƒ…å ±ã‚’è­˜åˆ¥ã™ã‚‹å‡¦ç†ã€‚æ­£ç¢ºãªå˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«æ­£è¦è¡¨ç¾ãªã©ã‚’ç”¨ã„ã‚‹æ‰‹æ³•ãŒä½¿ã‚ã‚Œã€è‡ªç„¶è¨€èªå‡¦ç†ã§ã‚ˆãè¦‹ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚„æ·±å±¤å­¦ç¿’ã®æ–‡è„ˆã§ç‰¹ã«ç‰¹æœ‰ã®æ„å‘³ã‚’æŒã£ã¦ãŠã‚Šã€åˆå¿ƒè€…ã«ã¯ç†è§£ãŒé›£ã—ã„å ´åˆãŒã‚ã‚Šã¾ã™ãŒã€å°‚é–€ç”¨èªã‚’çŸ¥ã£ã¦ãŠãã“ã¨ã§ç†è§£ãŒæ·±ã¾ã‚‹ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db901b2",
   "metadata": {},
   "source": [
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€**LLM 20 Questions**ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä½œæˆãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã™ã‚‹ã¨ã€`submission.tar.gz`ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ã€‚ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®å³å´ã«ã‚ã‚‹**Submit to competition**è¦‹å‡ºã—ã‹ã‚‰ç›´æ¥ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡ºã§ãã¾ã™ã€‚ã‚ã‚‹ã„ã¯ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ãƒ“ãƒ¥ãƒ¼ã‚¢ã‹ã‚‰*Output*ã‚¿ãƒ–ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã€`submission.tar.gz`ã‚’è¦‹ã¤ã‘ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®ãƒ›ãƒ¼ãƒ ãƒšãƒ¼ã‚¸ã®å·¦ä¸Šã«ã‚ã‚‹**Submit Agent**ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã€æå‡ºã‚’å®Œäº†ã•ã›ã¦ãã ã•ã„ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# ä½œæ¥­ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã«ç§»å‹•ã—ã¾ã™\n",
    "cd /kaggle/working \n",
    "\n",
    "# immutabledict ã¨ sentencepiece ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece \n",
    "\n",
    "# gemma_pytorchãƒªãƒã‚¸ãƒˆãƒªã‚’GitHubã‹ã‚‰ã‚¯ãƒ­ãƒ¼ãƒ³ã—ã¾ã™\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null \n",
    "\n",
    "# gemmaãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆã—ã¾ã™\n",
    "mkdir /kaggle/working/submission/lib/gemma/ \n",
    "\n",
    "# gemma_pytorchã‹ã‚‰ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ–°ã—ãä½œæˆã—ãŸgemmaãƒ•ã‚©ãƒ«ãƒ€ã«ç§»å‹•ã—ã¾ã™\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **é‡è¦:** ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ç’°å¢ƒã®ä¸¡æ–¹ã§ã‚³ãƒ¼ãƒ‰ãŒå‹•ä½œã™ã‚‹ã‚ˆã†ã«ã€\n",
    "# ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’ã“ã®ã‚ˆã†ã«è¨­å®šã—ã¾ã™ã€‚\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®å®šç¾©\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"æŒ‡å®šã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å‹ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®torch dtypeã‚’è¨­å®šã—ã¾ã™ã€‚\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ä¸­\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚’é–‹å§‹\n",
    "        prompt = str(self.formatter)  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã®çŠ¶æ…‹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›\n",
    "        response = self._call_llm(prompt)  # LLMã‚’å‘¼ã³å‡ºã—ã¦å¿œç­”ã‚’å–å¾—\n",
    "        response = self._parse_response(response, obs)  # å¿œç­”ã‚’è§£æ\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,  # æ¸©åº¦è¨­å®š\n",
    "                'top_p': 0.1,  # ç¢ºç‡ã®ã‚«ãƒƒãƒˆã‚ªãƒ•\n",
    "                'top_k': 1,  # ãƒˆãƒƒãƒ—kã®è¨­å®š\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        self.formatter.user(\"20ã®è³ªå•ã‚’ãƒ—ãƒ¬ã‚¤ã—ã¾ã—ã‚‡ã†ã€‚ã‚ãªãŸã¯è³ªå•è€…ã®å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # è³ªå•ã¨å›ç­”ã‚’äº¤äº’ã«å–å¾—\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã«ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"ã¯ã„ã‹ã„ã„ãˆã§ç­”ãˆã‚‰ã‚Œã‚‹è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"ä»Šã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„ã€‚æ¨æ¸¬ã¯ãƒ€ãƒ–ãƒ«ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã§å›²ã‚“ã§ãã ã•ã„ã€‚\")\n",
    "        self.formatter.start_model_turn()  # ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ãƒ³ã‚’é–‹å§‹\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # è³ªå•ã‚’æŠ½å‡º\n",
    "            if match is None:\n",
    "                question = \"äººã§ã™ã‹ï¼Ÿ\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)  # æ¨æ¸¬ã‚’è§£æ\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"ä¸æ˜ãªã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ãƒªã‚»ãƒƒãƒˆ\n",
    "        self.formatter.user(f\"20ã®è³ªå•ã‚’ãƒ—ãƒ¬ã‚¤ã—ã¾ã—ã‚‡ã†ã€‚ã‚ãªãŸã¯å›ç­”è€…ã®å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ {obs.keyword} ã§ã€ã‚«ãƒ†ã‚´ãƒªãƒ¼ã¯ {obs.category} ã§ã™ã€‚\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # è³ªå•ã¨å›ç­”ã‚’äº¤äº’ã«å–å¾—\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã«ã‚¿ãƒ¼ãƒ³ã‚’é©ç”¨\n",
    "        self.formatter.user(f\"è³ªå•ã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ {obs.keyword} ã«é–¢ã™ã‚‹ã‚‚ã®ã§ã™ã€‚ã¯ã„ã‹ã„ã„ãˆã§ç­”ãˆã€ç­”ãˆã¯ãƒ€ãƒ–ãƒ«ã‚¢ã‚¹ã‚¿ãƒªã‚¹ã‚¯ã§å›²ã‚“ã§ãã ã•ã„ã€‚\")\n",
    "        self.formatter.start_model_turn()  # ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ãƒ³ã‚’é–‹å§‹\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # å¿œç­”ã‹ã‚‰ç­”ãˆã‚’è§£æ\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä½œæˆ\n",
    "system_prompt = \"ã‚ãªãŸã¯20ã®è³ªå•ã‚²ãƒ¼ãƒ ã‚’ãƒ—ãƒ¬ã‚¤ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚ŒãŸAIã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ã“ã®ã‚²ãƒ¼ãƒ ã§ã¯ã€ç­”ãˆã‚‹å´ãŒã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è€ƒãˆã€è³ªå•ã™ã‚‹å´ãŒã¯ã„ã¾ãŸã¯ã„ã„ãˆã®è³ªå•ã«ç­”ãˆã¾ã™ã€‚ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã¯ç‰¹å®šã®äººã€å ´æ‰€ã€ã¾ãŸã¯ç‰©ã§ã™ã€‚\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20ã®è³ªå•ã‚’ãƒ—ãƒ¬ã‚¤ã—ã¾ã—ã‚‡ã†ã€‚ã‚ãªãŸã¯è³ªå•è€…ã®å½¹å‰²ã‚’æœãŸã—ã¾ã™ã€‚æœ€åˆã®è³ªå•ã‚’ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"äººã§ã™ã‹ï¼Ÿ\", \"**ã„ã„ãˆ**\",\n",
    "    \"å ´æ‰€ã§ã™ã‹ï¼Ÿ\", \"**ã¯ã„**\",\n",
    "    \"å›½ã§ã™ã‹ï¼Ÿ\", \"**ã¯ã„** ä»Šã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„ã€‚\",\n",
    "    \"**ãƒ•ãƒ©ãƒ³ã‚¹**\", \"æ­£è§£ã§ã™ï¼\",\n",
    "]\n",
    "\n",
    "\n",
    "# **é‡è¦:** ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¨ã—ã¦å®šç¾©ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€\n",
    "# å¿…è¦ãªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã ã‘ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ä¸¡æ–¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã¨ã€OOMãŒç™ºç”Ÿã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)  # è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)  # è³ªå•è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # å›ç­”è€…ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã‚’å‘¼ã³å‡ºã™\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"ã¯ã„\"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå¿œç­”\n",
    "    else:\n",
    "        return response  # å¿œç­”ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzã¨pvã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2  \n",
    "# pigzã‚’ä½¿ç”¨ã—ã¦ã€submissionãƒ•ã‚©ãƒ«ãƒ€ã®å†…å®¹ã‚’åœ§ç¸®ã—ã€åŒæ™‚ã«pvã‚’ä½¿ã£ã¦é€²è¡ŒçŠ¶æ³ã‚’è¡¨ç¤ºã—ãªãŒã‚‰ã€\n",
    "# submission.tar.gzã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚ \n",
    "# ã¾ãŸã€gemma/pytorch/7b-it-quant/2ã‹ã‚‰ã‚‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d6a38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ã‚³ãƒ¡ãƒ³ãƒˆ \n",
    "\n",
    "> ## Samar Elhissi\n",
    "> \n",
    "> ä¾‹ã‚’ã‚ã‚ŠãŒã¨ã†ã€‚ãƒ­ãƒ¼ã‚«ãƒ«ã§ã®ãƒ†ã‚¹ãƒˆæ–¹æ³•ã¯ï¼Ÿ\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Valentin Baltazar\n",
    "> > \n",
    "> > ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„â€¦ã“ã‚Œã‚‰ã®LLMã¯å¤šãã®è¨ˆç®—ã‚’å¿…è¦ã¨ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯å¼·åŠ›ãªGPUãŒå¿…è¦ã§ã™ã€‚ã‚¯ãƒ©ã‚¦ãƒ‰ã‚’ä½¿ç”¨ã™ã‚‹æ–¹ãŒãšã£ã¨ç°¡å˜ã§ã™ã€‚\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Michael Kamal 92\n",
    "> \n",
    "> ã‚ã‚ŠãŒã¨ã†ã€‚few_shot_examplesã«ã¤ã„ã¦ãŠå°‹ã­ã—ãŸã„ã®ã§ã™ãŒã€ç§ã¯æ¬¡ã®ã‚ˆã†ã«ã—ãªã‘ã‚Œã°ãªã‚Šã¾ã›ã‚“ã‹ã€‚\n",
    "> \n",
    "> ã“ã‚Œã®ã‚ˆã†ã«    ('ã“ã‚Œã¯å ´æ‰€ã§ã™ã‹ï¼Ÿ', 'ã¯ã„ã¾ãŸã¯ã„ã„ãˆ')\n",
    "> \n",
    "> ã¾ãŸã¯ã“ã‚Œ    ('ã“ã‚Œã¯å ´æ‰€ã§ã™ã‹ï¼Ÿ', 'ã¯ã„',)\n",
    "> \n",
    "> ã¾ãŸã¯ã“ã‚Œ     ('ã“ã‚Œã¯å ´æ‰€ã§ã™ã‹ï¼Ÿ', 'ã¯ã„', 'ä»Šã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„')\n",
    "> \n",
    "> ã¾ãŸã¯ã“ã‚Œ     ('ã“ã‚Œã¯å ´æ‰€ã§ã™ã‹ï¼Ÿ', 'ã„ã„ãˆ', 'ä»Šã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¨æ¸¬ã—ã¦ãã ã•ã„', 'ãƒ•ãƒ©ãƒ³ã‚¹')\n",
    "> \n",
    "> ã¾ãŸã¯ã“ã‚Œ     ('ã“ã‚Œã¯å ´æ‰€ã§ã™ã‹ï¼Ÿ', 'ã¯ã„', 'ãƒ•ãƒ©ãƒ³ã‚¹')\n",
    "> \n",
    "> ã©ã‚ŒãŒè³ªå•ã€å›ç­”ã€æ¨æ¸¬ã‚’ä½œã‚‹ã®ã«æ­£ã—ã„ã§ã™ã‹ï¼Ÿ\n",
    "> \n",
    "> ã‚‚ã†1ã¤ã®è³ªå•ã¯ã€Gemmaã¯few_shot_examplesã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "> ## Yukky_2801\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€ç§ã¯Kaggleã®åˆå¿ƒè€…ã§ã™ã€‚ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ãŸã¨ãã€\n",
    "> \n",
    "> æ¬¡ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºã¾ã—ãŸï¼š\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒå¤±æ•—ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "> \n",
    "> ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã‚‹ãŸã‚ã€submission.tar.gzã‚’æå‡ºã§ãã¾ã›ã‚“ã€‚\n",
    "> \n",
    "> ã“ã‚Œã«ã¤ã„ã¦ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚è§£æ±ºç­–ã‚’æä¾›ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "> \n",
    "> > ## Andres H. Zapke\n",
    "> > \n",
    "> > æ˜ã‚‰ã‹ã«ã€ã‚ãªãŸã¯ã“ã®ãƒ‘ã‚¹ \"gemma/pytorch/7b-it-quant/2\" ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚ãã®ãƒ‘ã‚¹ã«ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å³å´ã‚’è¦‹ã¦ã€gemmaãƒ¢ãƒ‡ãƒ«ãŒãã“ã«ã‚ã‚‹ã‹ç¢ºèªã—ã€ãƒ‘ã‚¹ãŒä¸€è‡´ã™ã‚‹ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰ã€‚\n",
    "> > \n",
    "> > > ## Aryan Singh\n",
    "> > > \n",
    "> > > Add Input æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ã€Gemma 7b-it-quant V2 ã‚’è¿½åŠ ã—ã¦ãã ã•ã„ã€‚\n",
    "> > > \n",
    "> > > ã¾ãšã€ã“ã¡ã‚‰ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã¦ãã ã•ã„: [https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Talal Mufti\n",
    "> > > > \n",
    "> > > > ãƒ‘ã‚¹ã«ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã£ãŸã“ã¨ã‚’ç¢ºèªã—ãŸå¾Œã§ã‚‚å•é¡ŒãŒç™ºç”Ÿã—ãŸãŸã‚ã€bashã‚³ãƒãƒ³ãƒ‰ã‚’å°‘ã—ç·¨é›†ã—ã¾ã—ãŸã€‚å€‹äººçš„ã«ã¯ã€ã“ã‚ŒãŒã‚ˆã‚Šã†ã¾ãã„ãã¾ã—ãŸï¼š\n",
    "> > > > \n",
    "> > > > !tar --use-compress-program='pigz --fast --recursive | pv' -f submission.tar.gz -c /kaggle/working/submission . -c /kaggle/input/gemma/pytorch/7b-it-quant/2\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Muhammad Hadi13\n",
    "> \n",
    "> ãªãœãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚³ãƒ”ãƒ¼ã—ã¦å®Ÿè¡Œã—ã¦ã‚‚ã€1.35 MBä»¥ä¸Šã®å‡ºåŠ›ãŒç”Ÿæˆã•ã‚Œãªã„ã®ã‹åˆ†ã‹ã‚Šã¾ã›ã‚“ã€‚ã“ã‚Œã¯å¸¸ã«æ¤œè¨¼ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã§å¤±æ•—ã—ã¾ã™ã€‚Ryanã®å‡ºåŠ›ã¯ç´„7GBã§ã—ãŸã€‚åŠ©ã‘ã¦ãã ã•ã„ï¼ï¼ï¼\n",
    "\n",
    "> \n",
    "> > ## Muhammad Hadi13\n",
    "> > \n",
    "> > tar: gemma/pytorch: Cannot stat: No such file or directory\n",
    "> > \n",
    "> > 1.37MiB 0:00:00 [36.4MiB/s] [<=>                                               ]\n",
    "> > \n",
    "> > tar: ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒå¤±æ•—ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "> > \n",
    "> > ã“ã®ã‚¨ãƒ©ãƒ¼ã¯æå‡ºã‚»ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã§ç™ºç”Ÿã—ã¾ã™ã€‚\n",
    "> > \n",
    "> > > ## Aryan Singh\n",
    "> > > \n",
    "> > > ã¾ãšã€Gemma 7b-it-quant V2 ã‚’è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ \n",
    "> > > \n",
    "> > > ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¿½åŠ ã™ã‚‹æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ \n",
    "> > > \n",
    "> > > ã¾ãšã€ã“ã¡ã‚‰ã§ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã¦ãã ã•ã„: [https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Ship of Theseus\n",
    "> \n",
    "> ã‚ã‚ŠãŒã¨ã†Ryanã€ç´ æ™´ã‚‰ã—ã„ä»•äº‹ã§ã™ï¼ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ç´ æ™´ã‚‰ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã€Kaggleã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å…±æœ‰ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "> ## shiv_314\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ï¼ çš†ã•ã‚“ã€1ã¤åŠ©ã‘ãŒå¿…è¦ã§ã™ã€‚gemmaãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚\n",
    "> \n",
    "> æ­£ã—ã„ã‚·ã‚¹ãƒ†ãƒ ãƒ‘ã‚¹ã‚’Pythonã«è¨­å®šã™ã‚‹ã“ã¨ã¯ã§ãã¾ã—ãŸãŒã€ã„ã¾ã ã«åŒã˜å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚åŠ©ã‘ã¦ãã ã•ã„ï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## dedq\n",
    "> \n",
    "> ã‚ã‚ŠãŒã¨ã†Ryanã€ç´ æ™´ã‚‰ã—ã„ä»•äº‹ã§ã™ï¼ãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã™ã‚‹ãŸã‚ã®ç´ æ™´ã‚‰ã—ã„ã‚³ãƒ¼ãƒ‰ã§ã€Kaggleã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã«å…±æœ‰ã—ã¦ãã‚Œã¦ã‚ã‚ŠãŒã¨ã†ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "> ## Code Hacker\n",
    "> \n",
    "> ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«tar.gzã‚’æå‡ºã—ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€å¤±æ•—ã—ã¾ã—ãŸâ€¦\n",
    "\n",
    "> \n",
    "> > ## Code Hacker\n",
    "> > \n",
    "> > ãƒ¢ãƒ‡ãƒ«ã«åŒæ„ã—ã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚ä»¥ä¸‹ã®èµ¤ã„ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦ãã ã•ã„â€¦\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## JAPerez\n",
    "> \n",
    "> ç´ æ™´ã‚‰ã—ã„ä»•äº‹ã§ã™ã€Ryanï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€ç§ã¯ã“ã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®åˆå¿ƒè€…ã§ã™ã€‚ \n",
    "> \n",
    "> ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦æå‡ºã—ã‚ˆã†ã¨ã—ãŸã®ã§ã™ãŒã€\n",
    "> \n",
    "> submission.tar.gzãƒ•ã‚¡ã‚¤ãƒ«ã¯ã©ã“ã«ç½®ã‘ã°ã‚ˆã„ã§ã™ã‹ï¼Ÿ \n",
    "> \n",
    "> Submit Agentãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸå¾Œã€ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡ºã™ã‚‹ã ã‘ã§ã‚ˆã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ \n",
    "> \n",
    "> å°‘ã—æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n",
    "> \n",
    "> ç§ã®è³ªå•ã¯å°‘ã—åŸºæœ¬çš„ã«èã“ãˆã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€è¿”ä¿¡ã—ã¦ãã‚ŒãŸã‚‰æœ¬å½“ã«æ„Ÿè¬ã—ã¾ã™ï¼ \n",
    "\n",
    "> \n",
    "> > ## Kanchan Maurya\n",
    "> > > Submit agentsã‚’ã‚¯ãƒªãƒƒã‚¯ã—ãŸå¾Œã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æå‡ºã™ã‚‹ã“ã¨ã§ã†ã¾ãã„ãã¾ã™ã€‚æœ€åˆã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãŒå‹•ä½œã—ã¦ã„ã‚‹ãŸã‚ã€æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ã€‚\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## vj4science\n",
    "> \n",
    "> ã‚ã‚ŠãŒã¨ã†Ryan - ã“ã‚Œã¯ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã¸ã®è‰¯ã„ã‚¹ã‚¿ãƒ¼ãƒˆã§ã™ï¼ã¨ã¦ã‚‚æ„Ÿè¬ã—ã¦ã„ã¾ã™ï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## gb_kwon\n",
    "> \n",
    "> ã™ã”ãã‚¯ãƒ¼ãƒ«ãªã‚¬ã‚¤ãƒ‰ãƒ©ã‚¤ãƒ³ã‚’æœ¬å½“ã«ã‚ã‚ŠãŒã¨ã†ï¼\n",
    "\n",
    "---\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> \n",
    "> main.pyå†…ã§ã€æ¬¡ã®ã‚ˆã†ã«gemma_pytorchãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ï¼šfrom gemma.config.\n",
    "> \n",
    "> ã“ã‚ŒãŒç§ã«ã¯æ©Ÿèƒ½ã—ã¾ã›ã‚“ãŒã€gemmaã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã™ã‚‹ã¨ã‚¨ãƒ©ãƒ¼ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "> \n",
    "> ãƒ­ãƒ¼ã‚«ãƒ«ã®gemmaãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ãƒ‘ã‚¹ã‚’æ‰‹å‹•ã§æŒ‡å®šã—ãŸã‚Šã€Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªåã§ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ãŸã‚Šã—ã¾ã—ãŸã€‚ä½•ã‹ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "---\n",
    "\n",
    "> ## Duy Thai\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯[@ryanholbrook](https://www.kaggle.com/ryanholbrook)ã€ã‚ãªãŸã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’è©¦ã—ã¾ã—ãŸãŒã€\"An attached model requires additional steps to be accessed. See the Models panel for details.\"ã¨ã„ã†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒè¡¨ç¤ºã•ã‚Œã¾ã—ãŸã€‚ã©ã†ã™ã‚Œã°è‰¯ã„ã§ã—ã‚‡ã†ã‹ï¼Ÿ\n",
    "> \n",
    "> ãƒ‘ãƒãƒ«ã‚’é–‹ãã¨ã€æ¬¡ã®ã‚ˆã†ã«è¡¨ç¤ºã•ã‚Œã¾ã™ï¼š\n",
    "\n",
    "> > ## Andres H. Zapke\n",
    "> > > \"Models\"ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã«è¡Œãã€Gemmaã‚’æ¤œç´¢ã—ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ©ã‚¤ã‚»ãƒ³ã‚¹ã‚’å—ã‘å…¥ã‚Œã¦ãã ã•ã„ã€‚\n",
    "> > > \n",
    "\n",
    "> > > > ## Duy Thai\n",
    "> > > > ã‚ã‚ŠãŒã¨ã†ï¼\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Kai_Huang\n",
    "> \n",
    "> ã“ã‚“ã«ã¡ã¯ã€Kaggleã®åˆå¿ƒè€…ã§ã™ã€‚ã‚ãªãŸã®ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã‚ˆã†ã¨ã—ãŸéš›ã«ã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã—ãŸï¼š\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=>                                               ]\n",
    "> \n",
    "> tar: ä¸Šè¨˜ã®ã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šã€çµ‚äº†ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãŒå¤±æ•—ã«ãªã‚Šã¾ã—ãŸã€‚\n",
    "> \n",
    "> ã“ã‚Œã«ã¤ã„ã¦ã®ã‚¢ã‚¤ãƒ‡ã‚¢ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚ã‚ã‚ŠãŒã¨ã†ã”ã–ã„ã¾ã™ï¼\n",
    "> \n",
    "> > ## Kai_Huang\n",
    "> > > ã‚ã‚ã€ã‚ã‹ã‚Šã¾ã—ãŸã€‚ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ãƒ¢ãƒ‡ãƒ«ã‚’å…¥åŠ›ã—ã¦ã„ãªã‹ã£ãŸğŸ˜±\n",
    "> > > \n",
    "\n",
    "> > > > ## D Prince Armand KOUMI\n",
    "> > > > ãƒ¢ãƒ‡ãƒ«ãŒãªã•ã™ãã‚‹ã®ã§ã€1ã¤è¿½åŠ ã—ã‚ˆã†ã¨ã—ã¦ã¿ã¦ãã ã•ã„ã€‚\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Qusay AL-Btoush\n",
    "> \n",
    "> ã™ã”ãè‰¯ã„ã§ã™ã€Ryan \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
