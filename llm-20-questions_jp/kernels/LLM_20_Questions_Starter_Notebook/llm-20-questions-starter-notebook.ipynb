{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04b0fb5",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleのコンペティション「LLM 20 Questions」に参加するためのAIエージェントを作成するプロセスを示しています。本ノートブックでは、質問者（質問をする側）と回答者（はいまたはいいえで回答する側）の2つのタイプのエージェントが実装されています。\n",
    "\n",
    "### 問題に取り組む内容\n",
    "ノートブックは、言語モデルを活用して「20の質問」ゲームを効果的にプレイできるエージェントを構築することを目的としています。プレイヤーは、はいまたはいいえで回答できる質問を通じて、特定のキーワードを推測する必要があります。このゲームにおいて、エージェントは一連の質問を経て、ターゲットワードを特定する能力を評価されます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **Gemmaモデル**: 本ノートブックでは、Googleが開発したGemmaという言語モデルが使用されています。特に、`gemma_pytorch`リポジトリからモデルがクローンされ、その構成と重みが利用されます。\n",
    "2. **PyTorch**: モデルはPyTorchを用いて実装され、GPUによる計算が可能です。\n",
    "3. **サブミッション生成**: 最終的に、エージェントは`submission.tar.gz`というファイルにまとめられ、Kaggleに提出される形式になっています。\n",
    "\n",
    "### 構成\n",
    "- ノートブックには、ライブラリのインストール手順、エージェントの初期化、プロンプトのフォーマット、質問者エージェント（`GemmaQuestionerAgent`）と回答者エージェント（`GemmaAnswererAgent`）のそれぞれのクラス定義が含まれています。\n",
    "- 質問と応答のインタラクションを管理するためのロジックが実装されています。\n",
    "\n",
    "### コード実行の手順\n",
    "ノートブックの実行により、必要なライブラリやモデルがダウンロードされ、構築されたエージェントがKaggleのシミュレーション環境内で機能します。最後に、生成されたサブミッションファイルをKaggleに提出するための圧縮処理が行われます。\n",
    "\n",
    "このように、このJupyterノートブックは、20の質問ゲームに基づいた言語モデルのエージェントを構築・提出するための包括的な手順を提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243ce61",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に関連する専門用語の簡単な解説です。初心者がつまずきそうなマイナーな用語や実務経験がないと馴染みのないものに焦点を当てました。\n",
    "\n",
    "1. **ImmutableDict**:\n",
    "   - 変更不可な辞書型のデータ構造。通常の辞書は要素の追加や削除が可能ですが、ImmutableDictは生成後にその要素を変更できません。この特性によって、データの整合性を保証できます。\n",
    "\n",
    "2. **sentencepiece**:\n",
    "   - 自然言語処理のためのテキストのトークン化ライブラリ。文をサブワード（通常、単語未満の単位）に分割し、言語モデルの効率を向上させます。特に、低リソース言語や新しい単語を扱う際に効果的です。\n",
    "\n",
    "3. **CausalLM**:\n",
    "   - 観察された情報から次に来る情報を予測するモデルの一種です。典型的な用途には言語生成があり、次の単語を予測しながら文を形成します。「CAUSAL」は時間的な因果関係に重きを置いています。\n",
    "\n",
    "4. **デフォルトのtorch dtype**:\n",
    "   - PyTorchにおけるデフォルトのデータ型設定。Tensor（多次元配列）の計算精度を指定し、float32やfloat64などのデータ型を使って数値演算が行えます。この設定を変更することで、計算効率やメモリ使用量に影響を与えることができます。\n",
    "\n",
    "5. **Few-shot examples**:\n",
    "   - 転移学習において、少数の例（トレーニングデータ）でモデルの性能を向上させる技術。例えば、質問の例とその応答を与えることで、モデルが新しい質問形式に適応する能力を高めます。\n",
    "\n",
    "6. **interleave_unequal**:\n",
    "   - 異なる長さの2つのリストを交互に組み合わせる処理を行う関数。一方のリストに要素が残っている場合、まだ要素を追加し続けることができます。このメソッドは、質問と回答などのペアを管理するのに有用です。\n",
    "\n",
    "7. **sampler_kwargs**:\n",
    "   - モデル生成プロセスにおいて、サンプリング方法を指定するための引数。生成時のランダム性を制御するために、温度（生成の多様性）、top_k（考慮する上位k候補）、top_p（確率のカットオフ）などの設定が含まれます。\n",
    "\n",
    "8. **キーワード抽出**:\n",
    "   - レスポンスから特定の情報を識別する処理。正確な単語やフレーズを見つけるために正規表現などを用いる手法が使われ、自然言語処理でよく見られます。\n",
    "\n",
    "これらの用語は、機械学習や深層学習の文脈で特に特有の意味を持っており、初心者には理解が難しい場合がありますが、専門用語を知っておくことで理解が深まるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db901b2",
   "metadata": {},
   "source": [
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。コンペティションの右側にある**Submit to competition**見出しから直接このファイルを提出できます。あるいは、ノートブックビューアから*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードしてください。コンペティションのホームページの左上にある**Submit Agent**をクリックして、ファイルをアップロードし、提出を完了させてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリに移動します\n",
    "cd /kaggle/working \n",
    "\n",
    "# immutabledict と sentencepiece パッケージをインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece \n",
    "\n",
    "# gemma_pytorchリポジトリをGitHubからクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null \n",
    "\n",
    "# gemmaフォルダを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/ \n",
    "\n",
    "# gemma_pytorchからのファイルを新しく作成したgemmaフォルダに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** ノートブックとシミュレーション環境の両方でコードが動作するように、\n",
    "# システムパスをこのように設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたデータ型にデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = str(self.formatter)  # フォーマッタの状態を文字列に変換\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出して応答を取得\n",
    "        response = self._parse_response(response, obs)  # 応答を解析\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,  # 温度設定\n",
    "                'top_p': 0.1,  # 確率のカットオフ\n",
    "                'top_k': 1,  # トップkの設定\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取得\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # フォーマッタにターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測はダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()  # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # 質問を抽出\n",
    "            if match is None:\n",
    "                question = \"人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)  # 推測を解析\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たします。キーワードは {obs.keyword} で、カテゴリーは {obs.category} です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取得\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # フォーマッタにターンを適用\n",
    "        self.formatter.user(f\"質問はキーワード {obs.keyword} に関するものです。はいかいいえで答え、答えはダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()  # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # 応答から答えを解析\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、答える側がキーワードを考え、質問する側がはいまたはいいえの質問に答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"人ですか？\", \"**いいえ**\",\n",
    "    \"場所ですか？\", \"**はい**\",\n",
    "    \"国ですか？\", \"**はい** 今、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバルとして定義します。これにより、\n",
    "# 必要なエージェントだけをロードできます。両方をロードすると、OOMが発生する可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントを呼び出す\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"  # デフォルト応答\n",
    "    else:\n",
    "        return response  # 応答を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzとpvをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2  \n",
    "# pigzを使用して、submissionフォルダの内容を圧縮し、同時にpvを使って進行状況を表示しながら、\n",
    "# submission.tar.gzとして保存します。 \n",
    "# また、gemma/pytorch/7b-it-quant/2からもファイルを追加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287d6a38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Samar Elhissi\n",
    "> \n",
    "> 例をありがとう。ローカルでのテスト方法は？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Valentin Baltazar\n",
    "> > \n",
    "> > ハードウェアがあるか確認してください…これらのLLMは多くの計算を必要とし、トレーニングとファインチューニングには強力なGPUが必要です。クラウドを使用する方がずっと簡単です。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Michael Kamal 92\n",
    "> \n",
    "> ありがとう。few_shot_examplesについてお尋ねしたいのですが、私は次のようにしなければなりませんか。\n",
    "> \n",
    "> これのように    ('これは場所ですか？', 'はいまたはいいえ')\n",
    "> \n",
    "> またはこれ    ('これは場所ですか？', 'はい',)\n",
    "> \n",
    "> またはこれ     ('これは場所ですか？', 'はい', '今、キーワードを推測してください')\n",
    "> \n",
    "> またはこれ     ('これは場所ですか？', 'いいえ', '今、キーワードを推測してください', 'フランス')\n",
    "> \n",
    "> またはこれ     ('これは場所ですか？', 'はい', 'フランス')\n",
    "> \n",
    "> どれが質問、回答、推測を作るのに正しいですか？\n",
    "> \n",
    "> もう1つの質問は、Gemmaはfew_shot_examplesでトレーニングされていますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## Yukky_2801\n",
    "> \n",
    "> こんにちは、私はKaggleの初心者です。あなたのノートブックを実行したとき、\n",
    "> \n",
    "> 次のエラーが出ました：\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: 上記のエラーにより、終了ステータスが失敗になりました。\n",
    "> \n",
    "> エラーが発生しているため、submission.tar.gzを提出できません。\n",
    "> \n",
    "> これについてのアイデアはありません。解決策を提供してください。\n",
    "\n",
    "> \n",
    "> > ## Andres H. Zapke\n",
    "> > \n",
    "> > 明らかに、あなたはこのパス \"gemma/pytorch/7b-it-quant/2\" にアクセスしようとしています。そのパスにファイルがあるか確認してください（ノートブックの右側を見て、gemmaモデルがそこにあるか確認し、パスが一致するかを確認してください）。\n",
    "> > \n",
    "> > > ## Aryan Singh\n",
    "> > > \n",
    "> > > Add Input 機能を使用して、Gemma 7b-it-quant V2 を追加してください。\n",
    "> > > \n",
    "> > > まず、こちらでライセンスを受け入れてください: [https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Talal Mufti\n",
    "> > > > \n",
    "> > > > パスにすべてのファイルがあったことを確認した後でも問題が発生したため、bashコマンドを少し編集しました。個人的には、これがよりうまくいきました：\n",
    "> > > > \n",
    "> > > > !tar --use-compress-program='pigz --fast --recursive | pv' -f submission.tar.gz -c /kaggle/working/submission . -c /kaggle/input/gemma/pytorch/7b-it-quant/2\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Muhammad Hadi13\n",
    "> \n",
    "> なぜファイルをコピーして実行しても、1.35 MB以上の出力が生成されないのか分かりません。これは常に検証エピソードで失敗します。Ryanの出力は約7GBでした。助けてください！！！\n",
    "\n",
    "> \n",
    "> > ## Muhammad Hadi13\n",
    "> > \n",
    "> > tar: gemma/pytorch: Cannot stat: No such file or directory\n",
    "> > \n",
    "> > 1.37MiB 0:00:00 [36.4MiB/s] [<=>                                               ]\n",
    "> > \n",
    "> > tar: 上記のエラーにより、終了ステータスが失敗になりました。\n",
    "> > \n",
    "> > このエラーは提出セルブロックで発生します。\n",
    "> > \n",
    "> > > ## Aryan Singh\n",
    "> > > \n",
    "> > > まず、Gemma 7b-it-quant V2 を追加する必要があります。 \n",
    "> > > \n",
    "> > > ノートブックでモデルを追加する機能を使用してください。 \n",
    "> > > \n",
    "> > > まず、こちらでライセンスを受け入れてください: [https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Ship of Theseus\n",
    "> \n",
    "> ありがとうRyan、素晴らしい仕事です！ローカルで実行するための素晴らしいコードで、Kaggleコミュニティに共有してくれてありがとう。\n",
    "\n",
    "---\n",
    "\n",
    "> ## shiv_314\n",
    "> \n",
    "> こんにちは！ 皆さん、1つ助けが必要です。gemmaパッケージでインポートエラーが発生しています。\n",
    "> \n",
    "> 正しいシステムパスをPythonに設定することはできましたが、いまだに同じ問題が発生しています。助けてください！\n",
    "\n",
    "---\n",
    "\n",
    "> ## dedq\n",
    "> \n",
    "> ありがとうRyan、素晴らしい仕事です！ローカルで実行するための素晴らしいコードで、Kaggleコミュニティに共有してくれてありがとう。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Code Hacker\n",
    "> \n",
    "> このノートブックの出力ファイルtar.gzを提出しようとしましたが、失敗しました…\n",
    "\n",
    "> \n",
    "> > ## Code Hacker\n",
    "> > \n",
    "> > モデルに同意していませんでした。以下の赤いボタンをクリックしてください…\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## JAPerez\n",
    "> \n",
    "> 素晴らしい仕事です、Ryan！\n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> こんにちは、私はこのコンペティションの初心者です。 \n",
    "> \n",
    "> あなたのノートブックを実行して提出しようとしたのですが、\n",
    "> \n",
    "> submission.tar.gzファイルはどこに置けばよいですか？ \n",
    "> \n",
    "> Submit Agentボタンをクリックした後、このファイルを提出するだけでよいでしょうか？ \n",
    "> \n",
    "> 少し時間がかかります。\n",
    "> \n",
    "> 私の質問は少し基本的に聞こえるかもしれませんが、返信してくれたら本当に感謝します！ \n",
    "\n",
    "> \n",
    "> > ## Kanchan Maurya\n",
    "> > > Submit agentsをクリックした後にファイルを提出することでうまくいきます。最初のシミュレーションが動作しているため、時間がかかります。\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## vj4science\n",
    "> \n",
    "> ありがとうRyan - これはコンペティションへの良いスタートです！とても感謝しています！\n",
    "\n",
    "---\n",
    "\n",
    "> ## gb_kwon\n",
    "> \n",
    "> すごくクールなガイドラインを本当にありがとう！\n",
    "\n",
    "---\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> \n",
    "> main.py内で、次のようにgemma_pytorchライブラリをインポートしています：from gemma.config.\n",
    "> \n",
    "> これが私には機能しませんが、gemmaをインポートするとエラーはありません。\n",
    "> \n",
    "> ローカルのgemmaモジュールのパスを手動で指定したり、Pythonライブラリ名でインポートしたりしました。何かアイデアはありますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## Duy Thai\n",
    "> \n",
    "> こんにちは[@ryanholbrook](https://www.kaggle.com/ryanholbrook)、あなたのノートブックを試しましたが、\"An attached model requires additional steps to be accessed. See the Models panel for details.\"というメッセージが表示されました。どうすれば良いでしょうか？\n",
    "> \n",
    "> パネルを開くと、次のように表示されます：\n",
    "\n",
    "> > ## Andres H. Zapke\n",
    "> > > \"Models\"のメニューに行き、Gemmaを検索し、モデルのライセンスを受け入れてください。\n",
    "> > > \n",
    "\n",
    "> > > > ## Duy Thai\n",
    "> > > > ありがとう！\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Kai_Huang\n",
    "> \n",
    "> こんにちは、Kaggleの初心者です。あなたのコードブロックを実行しようとした際に、以下のエラーが表示されました：\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=>                                               ]\n",
    "> \n",
    "> tar: 上記のエラーにより、終了ステータスが失敗になりました。\n",
    "> \n",
    "> これについてのアイデアはありません。ありがとうございます！\n",
    "> \n",
    "> > ## Kai_Huang\n",
    "> > > ああ、わかりました。ノートブックにモデルを入力していなかった😱\n",
    "> > > \n",
    "\n",
    "> > > > ## D Prince Armand KOUMI\n",
    "> > > > モデルがなさすぎるので、1つ追加しようとしてみてください。\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Qusay AL-Btoush\n",
    "> \n",
    "> すごく良いです、Ryan \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
