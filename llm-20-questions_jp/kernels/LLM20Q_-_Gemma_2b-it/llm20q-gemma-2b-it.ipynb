{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3f0c39",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、20の質問ゲームにおいて、人物、場所、物を特定するための大規模言語モデル（LLM）を使用したアプローチを提案しています。このプロジェクトでは、ターンごとのエージェントアプローチを採用し、質問を通じてターゲットキーワードを推測することを目指しています。\n",
    "\n",
    "### 問題定義\n",
    "- **問題**: 20の質問ゲームを通じて、キーワードのリストから特定の人物、場所、物を特定する。\n",
    "- **アプローチ**: 大規模言語モデルを用い、GIS座標や製品レビューなどのデータを活用して、質問に対する適切な応答を導く。\n",
    "\n",
    "### 考えられる解決策\n",
    "1. **モデルのファインチューニング**: ランドマークの写真やGIS座標でファインチューニングすることで、より正確な質問応答を可能にする。\n",
    "2. **テキストデータの活用**: 電話のテキスト会話データやウィキペディアの情報をReferし、特定の話し方やパターンを識別。\n",
    "3. **検索信号の最適化**: セグメンテーションに基づき、質問範囲内での迅速なキーワード検索を可能にする非監視モデルを使用。\n",
    "\n",
    "### コードの実装\n",
    "- **データ読み込み**: JSON形式のキーワードリストを読み込み、データフレームに格納する。\n",
    "- **エージェントの設計**: 質問者（QuestionerAgent）と回答者（AnswererAgent）を定義し、それぞれの役割に基づいてレスポンスを生成するメソッドを実装。\n",
    "- **モデルの初期化**: GemmaのカジュアルLMモデルを使用し、エージェントが機能するように構成する。\n",
    "- **環境テスト**: Kaggleの環境を構築し、4つのエージェントが対戦するテストを実行する。\n",
    "\n",
    "### 使用したライブラリ\n",
    "- **Pandas**: データフレームの操作。\n",
    "- **Torch**: モデルの実装。\n",
    "- **Kaggle Environments**: ゲーム環境を構築し、エージェントの対戦を可能にする。\n",
    "\n",
    "このNotebookは、20の質問ゲームをプレイし、エージェントを通じてでキーワードを特定するための新しいアプローチを提示しており、戦略的な質問応答を通じた推論能力の評価を可能にします。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1cc942",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提供されたJupyter Notebookに含まれる専門用語や概念の簡単な解説です。特に機械学習や深層学習の初心者がつまずきやすい、あまり知られていない用語や特定のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **GIS座標 (Geographic Information System Coordinates)**:\n",
    "   地理情報システム (GIS) とは、地理データを収集、保存、解析、表示するためのシステムです。GIS座標は、特定の地点を地球上の位置で表現するために使用される緯度と経度の組み合わせです。物理的な場所を特定するのに役立ちます。\n",
    "\n",
    "2. **ファインチューニング (Fine-tuning)**:\n",
    "   既に学習済みのモデルを特定のタスクに適応させるプロセスです。新しいデータでモデルを再学習させて精度を向上させる方法で、通常、完全な再トレーニングよりも早く、少ないデータで行われます。\n",
    "\n",
    "3. **検索信号 (Search Signal)**:\n",
    "   検索信号とは、特定の情報を探す際に役立つデータや特徴のことです。ここでは、質問者がキーワードを見つけるために必要な情報を指します。\n",
    "\n",
    "4. **逆引き (Reverse Lookup)**:\n",
    "   データベースやデータ構造において、特定のデータに基づいて関連する情報を見つけるプロセスです。ここでは、現在の使用モデルの重みや埋め込みを利用して、特定の質問に対する適切な回答を生成する方法を指します。\n",
    "\n",
    "5. **トークン (Token)**:\n",
    "   自然言語処理において、トークンは文章を構成する最小の単位です。単語や句読点などが該当し、モデルがテキストを理解するために用います。\n",
    "\n",
    "6. **モデルの重み (Model Weights)**:\n",
    "   ニューラルネットワークの各接続に付与される数値で、学習によって最適化されます。モデルの重みが異なると、同じ入力に対して異なる出力が得られます。\n",
    "\n",
    "7. **非監視モデル (Unsupervised Model)**:\n",
    "   ラベル付けされたデータを使用せずにデータのパターンや構造を学習するモデルです。クラスタリングや次元削減のタスクに一般的に使用されます。\n",
    "\n",
    "8. **エージェント (Agent)**:\n",
    "   特定のタスクを実行するために設計されたソフトウェアプログラムまたはモデルです。このノートブックでは、「質問者」エージェントと「回答者」エージェントがそれぞれ質問と回答を担当します。\n",
    "\n",
    "9. **温度 (Temperature)**:\n",
    "   確率分布のスプレッドを調整するパラメータで、この値が高いほど多様性のある出力が得られ、低いほど決定論的な出力になる傾向がある。生成モデルでは、創造性と一貫性のバランスを取るために使用されます。\n",
    "\n",
    "10. **少数ショット学習 (Few-shot Learning)**:\n",
    "    限られたデータポイントでモデルをトレーニングし、新しいタスクを学習させる能力です。あまりデータがない状況で有用であり、一般化能力を示します。\n",
    "\n",
    "これらの用語が初心者にとって参考になり、Jupyter Notebookへの理解を深める助けとなることを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f481c235",
   "metadata": {},
   "source": [
    "## 問題定義:\n",
    "20の質問ゲームをプレイしながら、キーワードのリストから人物、場所、物を特定するために、大規模言語モデルまたは新しいプログラミングアプローチを使用します。ターンごとのエージェントアプローチを採用します。  \n",
    "\n",
    "考えられる解決策:\n",
    "\n",
    "* 大規模言語モデルは、国、都市、州、そしてランドマークの近似を質問に対してより適切に調整するために、写真からのランドマークや都市のGIS座標の中心点でファインチューニングすることができます。\n",
    "\n",
    "* 電話のテキスト会話のデータセットにGIS座標を使用することも可能であり、変換された場所を用いて言語のローカリゼーション、特定の話し方のパターンや口語を特定することができるかもしれません。\n",
    "\n",
    "* 物に関しては、製品説明や製品レビューを使用することができるかもしれません。\n",
    "\n",
    "* 人に関しては、ウィキペディアやLinkedIn、デーティングサイトなどのソーシャルネットワーキングを参照することができます。\n",
    "\n",
    "* ファインチューニングなしの他のアプローチとしては、20の質問範囲内でキーワードにすぐに到達できるように、セグメンテーションに対して優れた検索信号を生み出すトークンを特定することができ、現在の使用モデルの重みや埋め込みに基づいて逆引きする非監視モデルに利用できます。\n",
    "\n",
    "例:\n",
    "* https://www.cia.gov/the-world-factbook/\n",
    "* https://public-nps.opendata.arcgis.com/datasets/nps::national-register-of-historic-places-points/explore\n",
    "\n",
    "\n",
    "## キーワードリスト\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:46:22.924486Z",
     "iopub.status.busy": "2024-06-08T05:46:22.924097Z",
     "iopub.status.idle": "2024-06-08T05:46:22.973208Z",
     "shell.execute_reply": "2024-06-08T05:46:22.972092Z",
     "shell.execute_reply.started": "2024-06-08T05:46:22.924455Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# モジュールを動的に読み込みます\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# keywords.pyからKEYWORDS_JSONをロードします\n",
    "keywords = SourceFileLoader(\"keywords\",'/kaggle/input/llm-20-questions/llm_20_questions/keywords.py').load_module()\n",
    "# JSON形式のキーワードデータを読み込みます\n",
    "df = json.loads(keywords.KEYWORDS_JSON)\n",
    "words = []\n",
    "\n",
    "# 各カテゴリーについてループ処理を行います\n",
    "for c in df:\n",
    "    # カテゴリー名と単語数を出力します\n",
    "    print(c['category'], len(c['words']))\n",
    "    # カテゴリー内の各単語についてさらにループ処理を行います\n",
    "    for w in c['words']:\n",
    "        # カテゴリー、キーワード、代替語、空のフィールド、緯度、経度をwordsリストに追加します\n",
    "        words.append([c['category'], w[\"keyword\"], w[\"alts\"], \"\", \"\", 0.0, 0.0])\n",
    "\n",
    "# データフレームを作成し、カラム名を設定します\n",
    "df = pd.DataFrame(words, columns=['Category','Word','Alternatives', \"Cat1\", \"Cat2\", \"Lat\", \"Lon\"])\n",
    "# データフレームの最終5行を表示します\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecaafa9",
   "metadata": {},
   "source": [
    "## テンプレートフォーマッティング\n",
    "* https://www.promptingguide.ai/models/gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:46:22.975677Z",
     "iopub.status.busy": "2024-06-08T05:46:22.975345Z",
     "iopub.status.idle": "2024-06-08T05:46:22.981114Z",
     "shell.execute_reply": "2024-06-08T05:46:22.980268Z",
     "shell.execute_reply.started": "2024-06-08T05:46:22.975647Z"
    }
   },
   "outputs": [],
   "source": [
    "template_special_tokens = ['<bos>','<start_of_turn>user','<start_of_turn>model','<end_of_turn>','<eos>']\n",
    "\n",
    "# エージェント1 - 推測者 (モデル？)\n",
    "# エージェント2 - 回答者 (ユーザー？)\n",
    "# エージェント3 - 推測者 (モデル？)\n",
    "# エージェント4 - 回答者 (ユーザー？)\n",
    "\n",
    "def agentx(obs, cfg):\n",
    "    # 観察結果のターンタイプに基づいてレスポンスを生成します\n",
    "    if obs.turnType == \"ask\": \n",
    "        response = \"\"  # 質問のターンの場合のレスポンス\n",
    "    elif obs.turnType == \"guess\": \n",
    "        response = \"\"  # 推測のターンの場合のレスポンス\n",
    "    elif obs.turnType == \"answer\": \n",
    "        response = \"\"  # 回答のターンの場合のレスポンス\n",
    "    else: \n",
    "        response = \"Am I Halucinating?  **YES**\"  # 他の不明なターンタイプに対するレスポンス\n",
    "    return response  # 生成したレスポンスを返します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bded1cd",
   "metadata": {},
   "source": [
    "## 20の質問提出モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:46:22.982593Z",
     "iopub.status.busy": "2024-06-08T05:46:22.982319Z",
     "iopub.status.idle": "2024-06-08T05:46:34.844839Z",
     "shell.execute_reply": "2024-06-08T05:46:34.842919Z",
     "shell.execute_reply.started": "2024-06-08T05:46:22.982569Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリを/kaggle/workingに移動します\n",
    "cd /kaggle/working\n",
    "# immutabledictとsentencepieceの最新バージョンをインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorchリポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# クローンしたgemmaのファイルを新しいディレクトリに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:50:54.946487Z",
     "iopub.status.busy": "2024-06-08T05:50:54.945851Z",
     "iopub.status.idle": "2024-06-08T05:50:54.9574Z",
     "shell.execute_reply": "2024-06-08T05:50:54.956464Z",
     "shell.execute_reply.started": "2024-06-08T05:50:54.946445Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "import torch, itertools, contextlib\n",
    "import os, sys, re, gc\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "gc.enable()\n",
    "\n",
    "# Kaggleエージェントパスを設定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "from gemma.config import get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# これは短縮され、ゲームメモリストレージで補完できます open('gamemaster.dat'), open('player.dat')\n",
    "class AgentFormatter:\n",
    "    def __init__(self, sp: str = None, fse: Iterable = None):\n",
    "        self._system_prompt = sp\n",
    "        self._few_shot_examples = fse\n",
    "        self._turn_user = f\"<start_of_turn>user\\n{{}}<end_of_turn>\\n\"\n",
    "        self._turn_model = f\"<start_of_turn>model\\n{{}}<end_of_turn>\\n\"\n",
    "        self.reset()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    \n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    \n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, sp=None, fse=None):\n",
    "        self._device = torch.device('cuda:0')\n",
    "        self.formatter = AgentFormatter(sp=sp, fse=fse)\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b()\n",
    "        model_config.tokenizer = WEIGHTS_PATH + '/tokenizer.model'\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            model.load_weights(WEIGHTS_PATH + '/gemma-2b-it.ckpt')\n",
    "            self.model = model.to(self._device).eval()\n",
    "            \n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(obs.turnType, response)\n",
    "        return response\n",
    "    \n",
    "    def _call_llm(self, prompt, max_nt=40, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {'temperature': 0.9, 'top_p': 0.9, 'top_k': 50,}\n",
    "        response = self.model.generate(\n",
    "            prompt, device=self._device, output_len=max_nt, **sampler_kwargs,)\n",
    "        return response\n",
    "    \n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response) #キーワードリストで最も近い一致をチェックし、正規表現を更新する\n",
    "        if match is None: keyword = ''\n",
    "        else: keyword = match.group().lower()\n",
    "        return keyword\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: question = \"Is it a place?\" #人物、場所、物のランダムな選択をする\n",
    "            else: question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            if guess is None or len(guess) <= 1: \n",
    "                return \"no guess\" #ランダムキーワード？\n",
    "            else: \n",
    "                return guess\n",
    "        elif obs.turnType == 'answer':\n",
    "            answer = self._parse_keyword(response)\n",
    "            return 'yes' if 'yes' in answer else 'no'\n",
    "        else: raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class QuestionerAgent(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"あなたは20の質問ゲームで質問者の役割を果たしています。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"あなたのターンは、人物、場所、物を探すために「はい」または「いいえ」で尋ねることです。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"あなたのターンは、人物、場所、物を推測することです。推測を二重アスタリスクで囲い、**yes**または**no**としてください。\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: question = \"Is it a place?\" #人物、場所、物のランダムな選択をする\n",
    "            else: question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            if guess is None or len(guess) <= 1: \n",
    "                return \"no guess\" \n",
    "            else: \n",
    "                return guess\n",
    "        elif obs.turnType == 'answer':\n",
    "            answer = self._parse_keyword(response)\n",
    "            return 'yes' if 'yes' in answer else 'no'\n",
    "        else: raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "class AnswererAgent(Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"あなたは20の質問ゲームで回答者の役割を果たしています。キーワードは{obs.keyword}で、カテゴリーは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"質問はキーワード{obs.keyword}についてで、カテゴリーは{obs.category}です。「はい」または「いいえ」と答え、二重アスタリスクで囲んでください（例：**yes**または**no**）.\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "sp = \"あなたは20の質問ゲームをプレイしています。このゲームでは、回答者がキーワードを考え、その後質問者の「はい」または「いいえ」の質問に答えます。キーワードは特定の人物、場所、または物です。\"\n",
    "fse = [\n",
    "    \"あなたは20の質問ゲームで質問者の役割を果たしています。最初の質問をして、人物、場所、物のキーワードを推測してください。\",\n",
    "    \"それは人物ですか？\", \"**no**\",\n",
    "    \"それは場所ですか？\", \"**yes**\",\n",
    "    \"それは国ですか？\", \"**yes** さて、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解!\",\n",
    "]\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = QuestionerAgent(sp=sp, fse=fse,)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = AnswererAgent(sp=sp, fse=fse,)\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\": response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\": response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\": response = get_agent('answerer')(obs)\n",
    "    \n",
    "    if response is None or len(response) <= 1: return \"no\" #デフォルトを変更\n",
    "    else: return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9e84b",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:46:59.550248Z",
     "iopub.status.busy": "2024-06-08T05:46:59.549885Z",
     "iopub.status.idle": "2024-06-08T05:47:27.472893Z",
     "shell.execute_reply": "2024-06-08T05:47:27.471795Z",
     "shell.execute_reply.started": "2024-06-08T05:46:59.550223Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q pygame\n",
    "!pip install -q 'kaggle_environments>=1.14.8'  # 新しい環境向けにKaggle環境をインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:51:00.453764Z",
     "iopub.status.busy": "2024-06-08T05:51:00.453414Z",
     "iopub.status.idle": "2024-06-08T05:51:00.463462Z",
     "shell.execute_reply": "2024-06-08T05:51:00.462658Z",
     "shell.execute_reply.started": "2024-06-08T05:51:00.453736Z"
    }
   },
   "outputs": [],
   "source": [
    "# コードを実行します\n",
    "%run submission/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-08T05:51:03.119441Z",
     "iopub.status.busy": "2024-06-08T05:51:03.119068Z",
     "iopub.status.idle": "2024-06-08T05:51:53.374014Z",
     "shell.execute_reply": "2024-06-08T05:51:53.372967Z",
     "shell.execute_reply.started": "2024-06-08T05:51:03.119415Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "\n",
    "# Kaggle環境を作成します\n",
    "env = make(\"llm_20_questions\", debug=True)\n",
    "agent = \"/kaggle/working/submission/main.py\"\n",
    "\n",
    "# 環境をリセットします\n",
    "env.reset()\n",
    "# エージェントを使って環境を実行します\n",
    "logs = env.run([agent, agent, agent, agent])\n",
    "# while not env.done: # テスト用に追加のステップをここに入れることができます\n",
    "# 環境の結果を表示します\n",
    "env.render(mode=\"ipython\", width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12184e5",
   "metadata": {},
   "source": [
    "# パッケージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T05:46:34.852949Z",
     "iopub.status.idle": "2024-06-08T05:46:34.85337Z",
     "shell.execute_reply": "2024-06-08T05:46:34.853168Z",
     "shell.execute_reply.started": "2024-06-08T05:46:34.85315Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzとpvをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-08T05:46:34.854563Z",
     "iopub.status.idle": "2024-06-08T05:46:34.855024Z",
     "shell.execute_reply": "2024-06-08T05:46:34.854796Z",
     "shell.execute_reply.started": "2024-06-08T05:46:34.854777Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/2b-it/2  # submissionディレクトリを圧縮してtar.gzファイルを作成します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d6fbb1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## DataDiva007\n",
    "> \n",
    "> ここでデータサイエンスプロジェクトに取り組むのがすごく中毒的です。とても楽しい時間を過ごしています！\n",
    "> \n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## SuM\n",
    "> \n",
    "> 素晴らしい仕事ですね [@jazivxt](https://www.kaggle.com/jazivxt)🎉\n",
    "> \n",
    "> 共有してくれてありがとう、スタートするための重要なインスピレーションを与えてくれます😃\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Neel Patel\n",
    "> \n",
    "> すごい仕事ですね、提出前にモデルをテストするのに非常に役立ちます。 :)\n",
    "\n",
    "> \n",
    "---\n",
    "\n",
    "> ## Hassan shahidi\n",
    "> \n",
    "> 優れたノートブックです [@jazivxt](https://www.kaggle.com/jazivxt)\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Prajwal Kanade\n",
    "> \n",
    "> 素晴らしく、非常に有益です、\n",
    "> \n",
    "> 優れた仕事です [@jazivxt](https://www.kaggle.com/jazivxt) \n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Utkarsh Jain\n",
    "> \n",
    "> 興味深いノートブック!! とても洞察に満ちています!!\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## huoyeqianxun\n",
    "> \n",
    "> CUDAのメモリ不足のようです…\n",
    "> \n",
    "> ノートブックで4つのエージェントを実行できませんか？？？\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
