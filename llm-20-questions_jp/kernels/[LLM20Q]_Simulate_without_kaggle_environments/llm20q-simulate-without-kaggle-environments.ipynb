{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ef5f62",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「20の質問」ゲームにおける質問者と回答者エージェントのテストを行うために設計されています。Kaggle環境のメモリ制限のため、ローカル環境でのテストが主な目的です。ノートブック内では、エージェントが同ゲームにおいて戦略的に質問を行い、回答を生成するプロセスをシミュレートしています。\n",
    "\n",
    "### 取り組む問題\n",
    "このノートブックは、言語モデルを活用して、対話式の情報収集ゲームである「20の質問」において、エージェントのパフォーマンスを評価することを目的としています。具体的に、質問者が有効な質問を行い、回答者が適切に応答することで、ターゲットワードを特定する能力が試されます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **ライブラリ**:\n",
    "  - `transformers`: Hugging Faceのライブラリを使用して、言語モデルを扱っています。\n",
    "  - `torch`: PyTorchライブラリを利用し、データの操作とGPUでの推論をサポートしています。\n",
    "  - `kaggle_secrets`: Kaggle内でのシークレット管理に対応しています。\n",
    "  - `pandas`と`json`: キーワードデータの処理及び構造化を行うために使用されています。\n",
    "\n",
    "- **主要な手法**:\n",
    "  - **トークナイゼーション**: `AutoTokenizer`を使用して、入力テキストをトークンに変換し、モデルが処理できる形式にしています。\n",
    "  - **モデルの生成**: `AutoModelForCausalLM`モデルを用いて、質問や推測に対する回答を生成します。\n",
    "  - **ゲームロジック**: `asker`や`answerer`メソッドを通じて、エージェントの質問者および回答者としての動作を定義し、対話を進めるロジックが実装されています。\n",
    "\n",
    "このノートブックは、作成したエージェントが「20の質問」を通じて、適切な質問や推測を行う能力をテストするために、キーワード情報を基にしたサンプルラウンドを実行します。また、各ラウンドではエージェントの出力をモニタリングし、勝利条件をチェックする仕組みも組み込まれています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecda8a01",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブック内で機械学習や深層学習の初心者がつまずきがちな専門用語についての簡単な解説です。特に、実務を経験していないと馴染みのない用語に焦点を当てています。\n",
    "\n",
    "1. **CUDA（Compute Unified Device Architecture）**\n",
    "   - NVIDIAが開発した並列計算のためのプラットフォームおよびAPIです。深層学習モデルのトレーニングや推論を高速化するために、GPUを使って計算を行うことができます。\n",
    "\n",
    "2. **トークナイザー（Tokenizer）**\n",
    "   - 自然言語処理（NLP）において、文を意味を持つ単位（トークン）に分解するプロセスです。たとえば、文章を単語や文字列に分けることによって、モデルがテキストデータを理解しやすくします。\n",
    "\n",
    "3. **Causal Language Model（因果言語モデル）**\n",
    "   - 文脈を考慮して、次に来る単語を生成するために設計されたモデルです。このモデルは、過去の単語をもとに次の単語を予測するため、因果関係を持っています。\n",
    "\n",
    "4. **デバイスマップ（Device Map）**\n",
    "   - モデルがどのハードウェアデバイス（CPUやGPU）上で動作するかを管理する仕組みです。特に大規模なモデルの場合、メモリ効率を考慮して複数のデバイスに分割して動かすことがあります。\n",
    "\n",
    "5. **終了トークン（End of Text Token, EOT）**\n",
    "   - テキスト生成モデルにおいて、生成が終了したことを示す特別なトークンです。このトークンが出力されると、モデルは生成を停止します。\n",
    "\n",
    "6. **推測（Guessing）**\n",
    "   - 20の質問ゲームにおいて、質問者がある単語を特定しようとする行為です。「推測する」という行為は、モデルが過去の質問に基づいて、元のキーワードを推測することを意味します。\n",
    "\n",
    "7. **観察クラス（Observation Class）**\n",
    "   - ゲームの状態を管理するためのクラスです。質問や回答、推測の情報を保持し、ゲームの進行に必要なデータを提供します。\n",
    "\n",
    "8. **エージェント（Agent）**\n",
    "   - 自律的に環境で行動するプログラムのことです。このコンテキストでは、質問者または回答者の役割を果たすAIが該当します。\n",
    "\n",
    "9. **メモリ効率的なSDP（Symmetric Distributed Processing）**\n",
    "   - メモリを節約しながら計算を効率的に行うためのCUDAの機能です。この機能を使用することで、GPUのメモリ使用量を削減できます。\n",
    "\n",
    "10. **代替キーワード（Alternative Keywords）**\n",
    "    - ゲーム内で使用できる別のキーワードのリストです。主なキーワードに関連する他の用語を提供し、推測を助ける役割を果たします。\n",
    "\n",
    "これらの用語は、特にJupyter Notebook内で特有の機能や概念に関連しているため、初心者が理解するためにはさらに詳しい説明が必要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc66d19",
   "metadata": {},
   "source": [
    "# Kaggle環境を使用せずにエージェントをテストできます\n",
    "kaggle_environmentsを使用したテストは、メモリ制限のために実施できません。  \n",
    "そのため、質問者と回答者エージェントをテストできるコードを作成しました。\n",
    "\n",
    "このエージェントは、以下に基づいて作成されました：  \n",
    "[llama3-8b- LLM20 Questions [LB 750]](https://www.kaggle.com/code/wouldyoujustfocus/llama3-8b-llm20-questions-lb-750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.837338Z",
     "iopub.status.busy": "2024-07-08T00:33:16.836975Z",
     "iopub.status.idle": "2024-07-08T00:33:16.860956Z",
     "shell.execute_reply": "2024-07-08T00:33:16.860154Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.837309Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# /kaggle/working/submission ディレクトリを作成します\n",
    "# -p オプションを使用することで、親ディレクトリが存在しない場合でも、必要なディレクトリをすべて作成します\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.863976Z",
     "iopub.status.busy": "2024-07-08T00:33:16.863686Z",
     "iopub.status.idle": "2024-07-08T00:33:16.874365Z",
     "shell.execute_reply": "2024-07-08T00:33:16.873336Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.86395Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# CUDAのメモリ効率的なSDPを無効にします\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# CUDAのフラッシュSDPを無効にします\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# エージェントのモデルパスを設定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# トークナイザーを初期化します\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# モデルを初期化します\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "# 終了トークンのIDを取得します\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    # 入力テンプレートをトークン化します\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # モデルから回答を生成します\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    # 生成結果の開始位置を特定します\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    # 終了トークンが生成結果に含まれているか確認します\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        # モードが適切であることを確認します\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"modeは以下のいずれかの値でなければなりません: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問者の役割を開始します\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 回答者の役割を開始します\n",
    "            output = self.answerer(obs)\n",
    "            # \"yes\"または\"no\"の応答を標準化します\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"\n",
    "            else:\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # 推測する役割を開始します\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者の「はい」または「いいえ」で答えます。\n",
    "        キーワードは特定の場所、または物です。\\n\n",
    "        \"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            ヒントとして、キーワードがモロッコである場合の動作例を示します:\n",
    "            例:\n",
    "            <あなた: それは場所ですか？\n",
    "            ユーザー: はい\n",
    "            あなた: それはヨーロッパにありますか？\n",
    "            ユーザー: いいえ\n",
    "            あなた: それはアフリカにありますか？\n",
    "            ユーザー: はい\n",
    "            あなた: そこに住んでいる人々の大半は肌の色が濃いですか？\n",
    "            ユーザー: いいえ\n",
    "            あなた: それはmで始まる国名ですか？\n",
    "            ユーザー: はい\n",
    "            あなた: それはモロッコですか？\n",
    "            ユーザー: はい。>\n",
    "\n",
    "            ユーザーが単語を選びました。最初の質問をしてください！\n",
    "            短くて簡潔に、1つの質問だけを与えてください。余分な言葉はいりません！\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            現在のゲームの状態は以下の通りです:\\n{conv}\n",
    "            会話に基づいて単語を推測できますか？単語だけを答えてください。余計なことはいりません。\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者の「はい」または「いいえ」で答えます。\n",
    "        キーワードは特定の場所、または物です。\\n\n",
    "        ユーザーの質問を理解し、プレイしているキーワードを理解することを確認してください。\n",
    "        現在、ユーザーが推測すべき単語は: \"{obs.keyword}\" で、カテゴリは \"{obs.category}\" です。\n",
    "        ヒントとして、キーワードがモロッコで、カテゴリが「場所」である場合の動作例を示します:\n",
    "        例:\n",
    "        <ユーザー: それは場所ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: それはヨーロッパにありますか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: それはアフリカにありますか？\n",
    "        あなた: はい\n",
    "        ユーザー: そこに住んでいる人々の大半は肌の色が濃いですか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: それはmで始まる国名ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: それはモロッコですか？\n",
    "        あなた: はい。>\n",
    "        \"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        # 質問の際の応答を生成します\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        # 推測の際の応答を生成します\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        # 回答の際の応答を生成します\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    # 応答がない場合、デフォルトで「yes」を返します\n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f375280c",
   "metadata": {},
   "source": [
    "## 次のコードをmain.pyの後に貼り付けてください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.87888Z",
     "iopub.status.busy": "2024-07-08T00:33:16.878588Z",
     "iopub.status.idle": "2024-07-08T00:33:18.283739Z",
     "shell.execute_reply": "2024-07-08T00:33:18.282643Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.878855Z"
    }
   },
   "outputs": [],
   "source": [
    "# keywords_local.pyという名前で、指定したURLからファイルをダウンロードします\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:18.286668Z",
     "iopub.status.busy": "2024-07-08T00:33:18.286284Z",
     "iopub.status.idle": "2024-07-08T00:37:18.311817Z",
     "shell.execute_reply": "2024-07-08T00:37:18.310968Z",
     "shell.execute_reply.started": "2024-07-08T00:33:18.286628Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from submission.main import agent\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# 観察クラスを定義します\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        self.step = 0  # 現在のステップ\n",
    "        self.role = \"guesser\"  # 役割は推測者\n",
    "        self.turnType = \"ask\"  # 現在のターンタイプは質問\n",
    "        self.keyword = \"Japan\"  # 予め設定されたキーワード（例として「日本」）\n",
    "        self.category = \"country\"  # キーワードのカテゴリ（例として「国」）\n",
    "        self.questions = []  # 質問のリスト\n",
    "        self.answers = []  # 回答のリスト\n",
    "        self.guesses = []  # 推測のリスト\n",
    "        \n",
    "# KEYWORDS_JSONからキーワードデータフレームを作成する関数\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON)  # JSON文字列を辞書に変換\n",
    "\n",
    "    keyword_list = []  # キーワードのリスト\n",
    "    category_list = []  # カテゴリのリスト\n",
    "    alts_list = []  # 代替キーワードのリスト\n",
    "\n",
    "    # JSONデータを反復処理してリストを作成\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):\n",
    "            keyword = json_data[i]['words'][j]['keyword']  # キーワードを抽出\n",
    "            keyword_list.append(keyword)  # リストに追加\n",
    "            category_list.append(json_data[i]['category'])  # カテゴリをリストに追加\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])  # 代替キーワードをリストに追加\n",
    "\n",
    "    # データフレームを作成\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])  # 列名を指定\n",
    "    data_pd['keyword'] = keyword_list  # キーワードリストをデータフレームに追加\n",
    "    data_pd['category'] = category_list  # カテゴリリストをデータフレームに追加\n",
    "    data_pd['alts'] = alts_list  # 代替リストをデータフレームに追加\n",
    "    \n",
    "    return data_pd  # 作成したデータフレームを返す\n",
    "    \n",
    "# KEYWORDS_JSONを使用してキーワードデータフレームを作成\n",
    "keywords_df = create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:37:18.313992Z",
     "iopub.status.busy": "2024-07-08T00:37:18.313331Z",
     "iopub.status.idle": "2024-07-08T00:48:16.991991Z",
     "shell.execute_reply": "2024-07-08T00:48:16.990969Z",
     "shell.execute_reply.started": "2024-07-08T00:37:18.313956Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観察インスタンスを作成します\n",
    "obs = Observation()\n",
    "cfg = \"_\"  # 構成用の任意の文字列を設定\n",
    "\n",
    "# キーワードデータフレームからサンプルを取得します\n",
    "sample_df = keywords_df.sample()\n",
    "obs.keyword = sample_df[\"keyword\"].values[0]  # サンプルのキーワードを観察オブジェクトに設定\n",
    "obs.category = sample_df[\"category\"].values[0]  # サンプルのカテゴリを観察オブジェクトに設定\n",
    "alts_list = sample_df[\"alts\"].values[0]  # サンプルの代替キーワードリストを取得\n",
    "alts_list.append(obs.keyword)  # オリジナルのキーワードを代替リストに追加\n",
    "\n",
    "# 現在のキーワードを表示します\n",
    "print(f\"キーワード: {obs.keyword}\")\n",
    "\n",
    "# 最大20ラウンド分のゲームを実行します\n",
    "for round in range(20):\n",
    "    obs.step = round + 1  # ステップを更新\n",
    "    \n",
    "    obs.role = \"guesser\"  # 役割を推測者に設定\n",
    "    obs.turnType = \"ask\"  # ターンタイプを質問に設定\n",
    "    question = agent(obs, cfg)  # 質問を生成\n",
    "    obs.questions.append(question)  # 質問を観察オブジェクトに追加\n",
    "    \n",
    "    obs.role = \"answerer\"  # 役割を回答者に設定\n",
    "    obs.turnType = \"answer\"  # ターンタイプを回答に設定\n",
    "    answer = agent(obs, cfg)  # 回答を生成\n",
    "    obs.answers.append(answer)  # 回答を観察オブジェクトに追加\n",
    "    \n",
    "    obs.role = \"guesser\"  # 役割を推測者に戻す\n",
    "    obs.turnType = \"guess\"  # ターンタイプを推測に設定\n",
    "    guess = agent(obs, cfg)  # 推測を生成\n",
    "    obs.guesses.append(guess)  # 推測を観察オブジェクトに追加\n",
    "    \n",
    "    # 現在のラウンド情報を表示します\n",
    "    print(f\"ラウンド: {round + 1}\")\n",
    "    print(f\"質問: {question}\")\n",
    "    print(f\"回答: {answer}\")\n",
    "    print(f\"推測: {guess}\")\n",
    "    \n",
    "    # 推測が代替リストに含まれているか確認します\n",
    "    if guess in alts_list:\n",
    "        print(\"勝利！！\")  # 勝利のメッセージを表示\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
