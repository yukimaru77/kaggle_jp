{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ce948d7",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebook「Qwen2-7b-it」では、言語モデルを用いて「20の質問」ゲームに取り組むエージェントを開発しています。具体的には、質問者と回答者の2つの役割を持つエージェントを作成し、与えられたキーワードをできるだけ少ない質問で推測することを目指しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "- **「20の質問」ゲーム**: プレイヤーが一つの特定の単語を推測するために、「はい」または「いいえ」で答えられる質問を通じて情報を収集します。このコンペティションでの目標は、エージェントが非常に効率的にターゲットとなる単語を推測することです。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **Transformers**: 「Hugging Face」から提供されるTransformersライブラリを使用して、事前学習された言語モデル「Qwen2-7b-instruct」を適用し、トークナイザーおよび因果言語モデルをロードします。\n",
    "2. **量子化技術**: `BitsAndBytesConfig`を用いて4ビットでのモデルの量子化を行い、メモリ効率を向上させています。これにより、計算リソースの使用が最適化されています。\n",
    "3. **テキスト生成パイプライン**: テキスト生成のためのパイプラインを設定し、生成されたテキストをベースに質問や推測を行っています。\n",
    "4. **ロジックの実装**: `Robot`クラスを設計し、質問者、回答者、推測者の3つのモードごとに処理を実行して、ゲームの状態に応じた質問や回答を生成します。\n",
    "5. **シミュレーション機能**: 提出する前に、シミュレーションを行うためのコードが用意されており、ゲームが実際にプレイされる過程を確認できるようになっています。\n",
    "\n",
    "このノートブックは、自然言語処理を利用した双方向の対話型AIエージェントを実装するための実用的な例を示しており、特にゲーム理論に基づく戦略的な質問と推測の能力を強調っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cd8288",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容で初心者がつまずきそうな専門用語の簡単な解説を示します。\n",
    "\n",
    "1. **Causal Language Model (因果言語モデル)**:\n",
    "   - 出力が生成される際に、与えられた過去の単語に基づいて未来の単語を予測するモデル。ルールとして、常に過去の情報だけを考慮し、その時点での単語に依存して次の単語を生成する。\n",
    "\n",
    "2. **Quantization (量子化)**:\n",
    "   - モデルのパラメータの精度を低下させ、より小さなビット数で表現する技術。これにより、計算資源の使用量が減り、モデルの推論速度が向上する。ただし、精度が多少犠牲になることがある。\n",
    "\n",
    "3. **BitsAndBytesConfig**:\n",
    "   - Hugginface Transformers内の量子化設定を行うためのクラス。複数の量子化オプション（例えば、ビット数や計算型）を指定し、効率的なモデル読み込みを助ける。\n",
    "\n",
    "4. **bfloat16**:\n",
    "   - 16ビットの浮動小数点数で、特に機械学習において計算の効率と精度のバランスをとるために使用される。このフォーマットは、GPUでの計算を高速化しつつ、一定の精度を保つのに有効。\n",
    "\n",
    "5. **Pipeline**:\n",
    "   - 機械学習の処理の一連のステップを簡単に実行できるフレームワーク。入力データを特定の処理順序で処理し、モデルの出力を得るための便利な機能。\n",
    "\n",
    "6. **Meme Efficient SDP**:\n",
    "   - \"Memory Efficient Low-Rank Approximation\"の略で、深層学習モデルをメモリ効率よく実装するための技術。特に大規模モデルでのメモリ消費を抑えることを目的とする。\n",
    "\n",
    "7. **Text Generation (テキスト生成)**:\n",
    "   - 自然言語処理におけるタスクであり、与えられた入力から意味のある文章を生成すること。次の単語を予測する生成モデルに基づく。\n",
    "\n",
    "8. **Parse (解析)**:\n",
    "   - 特定の構文やパターンに基づいてデータを処理し、意味を取り出す行為。ここでの応答解析は、生成されたテキストの中から特定の質問を抽出することを意味する。\n",
    "\n",
    "9. **Turn (ターン)**:\n",
    "   - ゲームや対話における一つのラウンドやステップを指す。質問や応答のやり取りが行われる単位。\n",
    "\n",
    "10. **Asking / Guessing / Answering**:\n",
    "    - ゲームの中での役割やモードを表す。質問をする「asking」、推測をする「guessing」、そして質問への応答を行う「answering」という三つのモード。\n",
    "\n",
    "これらの用語は、初心者には馴染みが薄く、実務においてもあまり接触することのない概念であり、理解に苦しむ可能性があります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c314c727",
   "metadata": {},
   "source": [
    "# Qwen2-7b-it\n",
    "参考のために、Qwen2-7b-itで作成されたエージェントを共有します。このエージェントは[LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)で良好な成果を上げています。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T06:51:36.62623Z",
     "iopub.status.busy": "2024-07-11T06:51:36.625521Z",
     "iopub.status.idle": "2024-07-11T06:52:15.1781Z",
     "shell.execute_reply": "2024-07-11T06:52:15.177279Z",
     "shell.execute_reply.started": "2024-07-11T06:51:36.626199Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "pip install bitsandbytes accelerate\n",
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.180517Z",
     "iopub.status.busy": "2024-07-11T06:52:15.179979Z",
     "iopub.status.idle": "2024-07-11T06:52:15.191147Z",
     "shell.execute_reply": "2024-07-11T06:52:15.190164Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.180481Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, pipeline\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # CUDAのメモリ効率の良いSDPを無効化\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"  # Kaggleエージェントのパスを指定\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # エージェントのパスが存在する場合\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")  # パスに基づいてモデルIDを設定\n",
    "else:\n",
    "    model_id = \"/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1\"  # デフォルトのモデルIDを設定\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)  # トークナイザーをモデルIDに基づいてロード\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(  # 量子化の設定を行う\n",
    "    load_in_4bit=True,  # 4ビットでの読み込みを有効にする\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 4ビット量子化のタイプをnf4に設定\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化を使用する\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # 計算のデータ型をbfloat16に設定\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(  # Causal Language Model（因果言語モデル）をロード\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,  # CPUメモリ使用量を低くする\n",
    "    quantization_config=quantization_config,  # 量子化設定を適用\n",
    "    torch_dtype=torch.float16,  # PyTorchのデータ型をfloat16に設定\n",
    ")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")  # テキスト生成のためのパイプラインを作成\n",
    "\n",
    "def parse_response(response):  # 応答を解析する関数\n",
    "    match = re.search(\".+?\\?\", response.replace('*', ''))  # 応答からクエスチョンを取得\n",
    "    if match is None:  # マッチが存在しない場合\n",
    "        question = 'キーワードは「s」で始まりますか？'  # デフォルトの質問を設定\n",
    "    else:\n",
    "        question = match.group()  # マッチした質問を取得\n",
    "    return question\n",
    "\n",
    "def generate_answer(chat_template):  # 応答を生成する関数\n",
    "    output = pipe(\n",
    "        chat_template,  # チャットテンプレートをパイプに渡す\n",
    "        max_new_tokens=32,  # 最大生成トークン数を32に設定\n",
    "        do_sample=False,  # サンプリングを無効にする\n",
    "#         temperature=0.01,  # 温度を設定（コメントアウト）\n",
    "#         top_p=0.1,  # top-pを設定（コメントアウト）\n",
    "#         top_k=1,  # top-kを設定（コメントアウト）\n",
    "        return_full_text=False,  # 完全なテキストを返さない\n",
    "    )[0][\"generated_text\"]  # 生成されたテキストを取得\n",
    "    output = re.sub('<end_of_turn>', '', output)  # テキストから'end_of_turn'を削除\n",
    "    return output\n",
    "\n",
    "\n",
    "class Robot:  # Robotクラスを定義\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):  # モードに基づいて処理を行うメソッド\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"modeはasking, answering, guessingのいずれかである必要があります。\"  # モードの確認\n",
    "        if mode == \"asking\":\n",
    "            output = self.asker(obs)  # 質問者の場合の処理\n",
    "        if mode == \"answering\":\n",
    "            output = self.answerer(obs)  # 回答者の場合の処理\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():  # 応答が「yes」の場合\n",
    "                output = \"yes\"  # 出力を「yes」に設定\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():  # 応答が「no」の場合\n",
    "                output = \"no\"  # 出力を「no」に設定\n",
    "            else:  # その他の場合\n",
    "                output = \"no\"  # デフォルトで「no」に設定\n",
    "        if mode == \"guessing\":\n",
    "            output = self.asker(obs)  # 推測者の場合の処理\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):  # 質問者の処理を行うメソッド\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは「20の質問」ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え出し、質問者のはい/いいえ質問に回答します。\n",
    "        キーワードは特定の「もの」または「場所」です。\n",
    "        \"\"\"\n",
    "        if obs.turnType ==\"ask\":  # 質問するターン\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            「20の質問」をプレイしましょう。質問者の役割を果たします。\n",
    "            はい/いいえの質問をしてください。\n",
    "            例に従ってください:\n",
    "            例: キーワードは「モロッコ」\n",
    "            <assistant: それは都市ですか？\n",
    "            user: いいえ\n",
    "            assistant: それは国ですか？\n",
    "            user: はい\n",
    "            assistant: アフリカにありますか？\n",
    "            user: はい\n",
    "            assistant: mで始まる国名ですか？\n",
    "            user: はい\n",
    "            assistant: それはモロッコですか？\n",
    "            user: はい。>\n",
    "            \"\"\"\n",
    "\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}\\n\"\"\"  # チャットテンプレートを作成\n",
    "\n",
    "            if len(obs.questions)>=1:  # 質問が1つ以上ある場合\n",
    "                chat_template += \"これまでの会話の履歴:\\n\"  # 会話履歴の追加\n",
    "                chat_template += \"<\\n\"\n",
    "                for q, a in zip(obs.questions, obs.answers):  # 質問と応答をループ\n",
    "                    chat_template += f\"assistant:\\n{q}\\n\"\n",
    "                    chat_template += f\"user:\\n{a}\\n\"\n",
    "                chat_template += \">\\n\"\n",
    "                    \n",
    "            chat_template += \"\"\"\n",
    "            ユーザーが選んだ単語について、最初の質問をしてください！\n",
    "            短く、冗長な表現を避けて、一つの質問だけをしてください.<end_of_turn>\n",
    "            <start_of_turn>assistant\\n\n",
    "            \"\"\"\n",
    "            output = generate_answer(chat_template)  # 応答を生成\n",
    "            output = parse_response(output)  # 応答を解析\n",
    "                \n",
    "        elif obs.turnType == \"guess\":  # 推測するターン\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):  # 質問と応答をループ\n",
    "                conv += f\"\"\"question:\\n{q}\\nanswer:\\n{a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            あなたの役割は、質問や回答に基づいてキーワードを予測することです。\n",
    "            例に従ってください:\n",
    "            例:\n",
    "            <question: それは都市ですか？\n",
    "            answer: いいえ\n",
    "            question: それは国ですか？\n",
    "            answer: はい\n",
    "            question: アフリカにありますか？\n",
    "            answer: はい\n",
    "            question: mで始まる国名ですか？\n",
    "            answer: はい\n",
    "            あなた: モロッコ>\n",
    "            \n",
    "            現在のゲームの状態は次の通りです:\\n{conv}\n",
    "            会話に基づいて、キーワードを推測してください。ただ、単語だけを答えて、冗長にしないでください<end_of_turn>\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{guess_prompt}\\n\"\"\"  # チャットテンプレートを作成\n",
    "            chat_template += \"<start_of_turn>you:\\n\"\n",
    "            output = generate_answer(chat_template)  # 応答を生成\n",
    "        return output\n",
    "    \n",
    "    def answerer(self, obs):  # 回答者の処理を行うメソッド\n",
    "        ask_prompt = f\"\"\"\n",
    "        あなたは「20の質問」ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え出し、質問者のはい/いいえ質問に回答します。\n",
    "        例に従ってください:\n",
    "        例: キーワードは「モロッコ」\n",
    "        <user: それは場所ですか？\n",
    "        あなた: はい\n",
    "        user: ヨーロッパにありますか？\n",
    "        あなた: いいえ\n",
    "        user: アフリカにありますか？\n",
    "        あなた: はい\n",
    "        user: mで始まる国名ですか？\n",
    "        あなた: はい\n",
    "        user: それはモロッコですか？\n",
    "        あなた: はい。>\n",
    "        \"{obs.keyword}\"について、次の質問に「はい」または「いいえ」で答えてください:\n",
    "        {obs.questions[-1]}\n",
    "        \"\"\"\n",
    "        chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}<end_of_turn>\\n\"\"\"  # チャットテンプレートを作成\n",
    "        chat_template += \"<start_of_turn>Assistant\\n\"\n",
    "        output = generate_answer(chat_template)  # 応答を生成\n",
    "        return output\n",
    "\n",
    "robot = Robot()  # Robotクラスのインスタンスを作成\n",
    "\n",
    "def agent(obs, cfg):  # エージェント関数\n",
    "    if obs.turnType ==\"ask\":  # 質問するターン\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":  # 推測するターン\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":  # 回答するターン\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:  # 応答が無効な場合\n",
    "        response = \"yes\"  # デフォルトの応答を「yes」に設定\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.192968Z",
     "iopub.status.busy": "2024-07-11T06:52:15.192649Z",
     "iopub.status.idle": "2024-07-11T06:52:22.612061Z",
     "shell.execute_reply": "2024-07-11T06:52:22.611154Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.192945Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:22.615033Z",
     "iopub.status.busy": "2024-07-11T06:52:22.614746Z",
     "iopub.status.idle": "2024-07-11T06:57:12.011816Z",
     "shell.execute_reply": "2024-07-11T06:57:12.010683Z",
     "shell.execute_reply.started": "2024-07-11T06:52:22.615005Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/qwen2/transformers/qwen2-7b-instruct . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.013563Z",
     "iopub.status.busy": "2024-07-11T06:57:12.013245Z",
     "iopub.status.idle": "2024-07-11T06:57:12.018443Z",
     "shell.execute_reply": "2024-07-11T06:57:12.017575Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.013535Z"
    }
   },
   "outputs": [],
   "source": [
    "# # tar.gzファイルの中身を確認するためのコード\n",
    "\n",
    "# import tarfile\n",
    "# tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")\n",
    "# for file in tar.getmembers():\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce385d8f",
   "metadata": {},
   "source": [
    "# シミュレーション\n",
    "エージェントを提出する前にシミュレーションを行いたい場合は、以下のコメントを解除して実行してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.019843Z",
     "iopub.status.busy": "2024-07-11T06:57:12.019546Z",
     "iopub.status.idle": "2024-07-11T06:57:12.027836Z",
     "shell.execute_reply": "2024-07-11T06:57:12.02707Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.019812Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.02921Z",
     "iopub.status.busy": "2024-07-11T06:57:12.028922Z",
     "iopub.status.idle": "2024-07-11T06:57:12.036912Z",
     "shell.execute_reply": "2024-07-11T06:57:12.036194Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.029187Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# from submission.main import agent\n",
    "# from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# class Observation:\n",
    "#     def __init__(self):\n",
    "#         self.step = 0  # ステップ番号を初期化\n",
    "#         self.role = \"guesser\"  # 役割を「推測者」に設定\n",
    "#         self.turnType = \"ask\"  # ターンタイプを「質問」に設定\n",
    "#         self.keyword = \"Japan\"  # キーワードを「日本」に設定\n",
    "#         self.category = \"country\"  # カテゴリを「国」に設定\n",
    "#         self.questions = []  # 質問リストを初期化\n",
    "#         self.answers = []  # 応答リストを初期化\n",
    "#         self.guesses = []  # 推測リストを初期化\n",
    "        \n",
    "# def create_keyword_df(KEYWORDS_JSON):  # キーワードデータフレームを作成する関数\n",
    "#     json_data = json.loads(KEYWORDS_JSON)  # JSONデータをロード\n",
    "\n",
    "#     keyword_list = []  # キーワードリストを初期化\n",
    "#     category_list = []  # カテゴリリストを初期化\n",
    "#     alts_list = []  # 代替リストを初期化\n",
    "\n",
    "#     for i in range(len(json_data)):  # JSONデータをループ\n",
    "#         for j in range(len(json_data[i]['words'])):  # 各キーワードをループ\n",
    "#             keyword = json_data[i]['words'][j]['keyword']  # キーワードを取得\n",
    "#             keyword_list.append(keyword)  # キーワードをリストに追加\n",
    "#             category_list.append(json_data[i]['category'])  # カテゴリをリストに追加\n",
    "#             alts_list.append(json_data[i]['words'][j]['alts'])  # 代替をリストに追加\n",
    "\n",
    "#     data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])  # データフレームを作成\n",
    "#     data_pd['keyword'] = keyword_list  # キーワード列を設定\n",
    "#     data_pd['category'] = category_list  # カテゴリ列を設定\n",
    "#     data_pd['alts'] = alts_list  # 代替列を設定\n",
    "    \n",
    "#     return data_pd\n",
    "    \n",
    "# keywords_df = create_keyword_df(KEYWORDS_JSON)  # キーワードデータフレームを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.038569Z",
     "iopub.status.busy": "2024-07-11T06:57:12.038129Z",
     "iopub.status.idle": "2024-07-11T06:57:12.049014Z",
     "shell.execute_reply": "2024-07-11T06:57:12.048315Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.038537Z"
    }
   },
   "outputs": [],
   "source": [
    "# obs = Observation()  # 観察インスタンスを作成\n",
    "# cfg = \"_\"\n",
    "\n",
    "# sample_df = keywords_df.sample()  # キーワードデータフレームからサンプルを取得\n",
    "# obs.keyword = sample_df[\"keyword\"].values[0]  # サンプルからキーワードを設定\n",
    "# obs.category = sample_df[\"category\"].values[0]  # サンプルからカテゴリを設定\n",
    "# alts_list = sample_df[\"alts\"].values[0]  # サンプルから代替リストを設定\n",
    "# alts_list.append(obs.keyword)  # キーワードを代替リストに追加\n",
    "\n",
    "# print(f\"キーワード: {obs.keyword}\")  # キーワードを表示\n",
    "\n",
    "# for round in range(20):  # 20ラウンドの繰り返し\n",
    "#     obs.step = round+1  # ステップを更新\n",
    "    \n",
    "#     obs.role = \"guesser\"  # 役割を「推測者」に設定\n",
    "#     obs.turnType = \"ask\"  # ターンタイプを「質問」に設定\n",
    "#     question = agent(obs, cfg)  # 質問を生成\n",
    "#     obs.questions.append(question)  # 質問をリストに追加\n",
    "    \n",
    "#     obs.role = \"answerer\"  # 役割を「回答者」に設定\n",
    "#     obs.turnType = \"answer\"  # ターンタイプを「回答」に設定\n",
    "#     answer = agent(obs, cfg)  # 応答を生成\n",
    "#     obs.answers.append(answer)  # 応答をリストに追加\n",
    "    \n",
    "#     obs.role = \"guesser\"  # 役割を「推測者」に設定\n",
    "#     obs.turnType = \"guess\"  # ターンタイプを「推測」に設定\n",
    "#     guess = agent(obs, cfg)  # 推測を生成\n",
    "#     obs.guesses.append(guess)  # 推測をリストに追加\n",
    "    \n",
    "#     print(f\"ラウンド: {round+1}\")  # ラウンド数を表示\n",
    "#     print(f\"質問: {question}\")  # 質問を表示\n",
    "#     print(f\"応答: {answer}\")  # 応答を表示\n",
    "#     print(f\"推測: {guess}\")  # 推測を表示\n",
    "    \n",
    "#     if guess in alts_list:  # 推測が代替リストに存在する場合\n",
    "#         print(\"勝利!!\")  # 勝利メッセージを表示\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 51944,
     "sourceId": 62188,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
