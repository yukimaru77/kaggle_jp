{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3a651f",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションにおいて、Qwen2-7b-itモデルを使用したエージェントを作成するための実装を示しています。Notebookの内容は、具体的にはエージェントが「20の質問ゲーム」において質問と推測を行い、適切に応答する仕組みを構築することに取り組んでいます。\n",
    "\n",
    "### 問題の概要\n",
    "本エージェントは、言語モデルを使用して、ユーザーから提示されたキーワードに関連する質問を生成し、それに対して「はい」または「いいえ」で回答する役割を果たします。また、エージェントは求められた際にキーワードを推測する能力も持っています。このようにして、最小限の質問回数でキーワードを特定することが求められます。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "1. **ライブラリのインストール**:\n",
    "   - `bitsandbytes` や `accelerate` を用いてCUDAのメモリ効率を向上させ、モデルの実行効率を高めています。\n",
    "   - `transformers` ライブラリを使用して、事前訓練済みモデルとトークナイザーを便利に読み込んでいます。\n",
    "\n",
    "2. **モデルの設定**:\n",
    "   - `AutoTokenizer` でトークナイザーを初期化し、`AutoModelForCausalLM` を使用してQwen2-7b-itモデルを読み込みます。モデルは量子化設定を利用してメモリ使用量を削減しつつ、効率よく動作するようにしています。\n",
    "\n",
    "3. **エージェントの実装**:\n",
    "   - `Robot` クラスを定義し、質問と推測を行うためのメソッド (`asker`, `answerer`) を実装しています。これにより、質問を生成したり、回答を行ったりする機能を持ったエージェントを構築しています。\n",
    "   - `agent` 関数でエージェントが観察情報に基づき適切な動作を取るようになっています。\n",
    "\n",
    "4. **シミュレーション機能**:\n",
    "   - コメントアウトされたコードブロックがあり、実際のゲームをシミュレーションするための環境を設定することが可能です。この部分では、観察者のクラスを定義し、ゲームのルールに則ってエージェントのアクションを試行することができます。\n",
    "\n",
    "このNotebook全体として、Qwen2-7b-itモデルを活用し、「20の質問ゲーム」を成功裏にプレイするためのAIエージェントの実装と、そのための環境設定、シミュレーション機能を包含しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc56be4",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提供されたJupyter Notebookに関連する専門用語の簡単な解説です。初心者がつまずきそうなものに焦点を当てていますが、一般的に知られている用語は省いています。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **メモリ効率の良いSDP (Stochastic Dynamic Programming)**:\n",
    "   - メモリ効率を考慮した動的プログラミングの手法で、メモリ消費を最小限に抑えながら計算を行うことを指します。特に大きなモデルを扱う際に役立ちます。\n",
    "\n",
    "2. **量子化 (Quantization)**:\n",
    "   - モデルのサイズを削減し、推論を高速化するために、モデルの重みを小さなビット数で表現する手法です。これにより、メモリ使用量が低減し、計算速度が向上します。特に「4-bit量子化」は一般的な手法です。\n",
    "\n",
    "3. **BitsAndBytesConfig**:\n",
    "   - 量子化のための設定を行うためのクラスで、どのようにモデルを量子化するかを指定します。パラメータには、量子化のタイプや計算時のデータ型が含まれます。\n",
    "\n",
    "4. **Pipeline**:\n",
    "   - Hugging Face Transformersライブラリにおいて、特定のタスク（例えばテキスト生成、感情分析など）を行うための簡潔なインターフェースです。モデルとトークナイザーを組み合わせて使うことができます。\n",
    "\n",
    "5. **チャットテンプレート (Chat Template)**:\n",
    "   - 生成したいテキストの形式や構造を示すために使用するフレームワークです。具体的には、システムの指示やユーザーとの対話履歴を含むことがあります。\n",
    "\n",
    "6. **レギュライズ (Regularization)**:\n",
    "   - モデルの複雑さを減少させ、過学習を防ぐために使用される手法です。特に、ニューラルネットワークでは、ドロップアウトやL2正則化などがあります。\n",
    "\n",
    "7. **デヴ (Dev)**:\n",
    "   - 開発環境を指し、モデルのトレーニングや動作確認、デバッグを行うための設定や仮想環境です。\n",
    "\n",
    "8. **エージェント (Agent)**:\n",
    "   - ゲームやシミュレーションにおいて、特定の役割を持ち、自律的に行動するプログラムやモデルです。このノートブックでは、質問者と回答者のエージェントが実装されています。\n",
    "\n",
    "9. **オブザベーション (Observation)**:\n",
    "   - エージェントが環境から得られる情報や状態を指します。このクラスは、ゲームの進行に必要な情報（質問、回答、キーワードなど）を管理します。\n",
    "\n",
    "10. **サンプリング (Sampling)**:\n",
    "    - モデルから生成される出力を選択する際の手法です。通常、確率的な方法に基づいており、生成するテキストの多様性を向上させます。たとえば、`do_sample`オプションによって制御されます。\n",
    "\n",
    "この解説リストは、機械学習や深層学習において一般的に使われる用語以外の、特定の技術や手法に焦点を当てたものです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0af272b",
   "metadata": {},
   "source": [
    "# Qwen2-7b-it\n",
    "参考のために、[LLMリーダーボード](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)で良い成績を収めているQwen2-7b-itを用いて作成されたエージェントを共有します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T06:51:36.62623Z",
     "iopub.status.busy": "2024-07-11T06:51:36.625521Z",
     "iopub.status.idle": "2024-07-11T06:52:15.1781Z",
     "shell.execute_reply": "2024-07-11T06:52:15.177279Z",
     "shell.execute_reply.started": "2024-07-11T06:51:36.626199Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# /kaggle/working/submissionというディレクトリを作成します。\n",
    "mkdir -p /kaggle/working/submission\n",
    "# bitsandbytesとaccelerateというライブラリをインストールします。\n",
    "pip install bitsandbytes accelerate\n",
    "# transformersライブラリを最新バージョンにアップグレードします。\n",
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.180517Z",
     "iopub.status.busy": "2024-07-11T06:52:15.179979Z",
     "iopub.status.idle": "2024-07-11T06:52:15.191147Z",
     "shell.execute_reply": "2024-07-11T06:52:15.190164Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.180481Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "# submission/main.pyというファイルに追記します\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, pipeline\n",
    "\n",
    "# CUDAを使用したメモリ効率の良いSDPを有効にします\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# Kaggleエージェントのパスが存在するか確認します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1\"\n",
    "\n",
    "# トークナイザーをモデルIDから読み込みます\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# 量子化設定を定義します\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "# モデルを読み込む際の設定をします\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "# テキスト生成のためのパイプラインを作成します\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "def parse_response(response):\n",
    "    # レスポンスから質問を抽出します\n",
    "    match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "    if match is None:\n",
    "        question = 'キーワードは「s」という文字で始まりますか？'\n",
    "    else:\n",
    "        question = match.group()\n",
    "    return question\n",
    "\n",
    "def generate_answer(chat_template):\n",
    "    # 与えられたチャットテンプレートに基づいて回答を生成します\n",
    "    output = pipe(\n",
    "        chat_template,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "#         temperature=0.01,\n",
    "#         top_p=0.1,\n",
    "#         top_k=1,\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"]\n",
    "    # 出力から<end_of_turn>を削除します\n",
    "    output = re.sub('<end_of_turn>', '', output)\n",
    "    return output\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        # モードに応じた処理を行います\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"modeはasking, answering, guessingのいずれかでなければなりません\"\n",
    "        if mode == \"asking\":\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"\n",
    "            else:\n",
    "                output = \"no\"\n",
    "        if mode == \"guessing\":\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        # 質問者としての役割を果たします\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。\n",
    "        このゲームでは、回答者がキーワードを考え、質問者の「はい」または「いいえ」の質問に答えます。\n",
    "        キーワードは具体的な「物」または「場所」です。\n",
    "        \"\"\"\n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            20の質問を始めましょう。あなたは質問者の役割を果たします。\n",
    "            「はい」または「いいえ」で答えられる質問をしてください。\n",
    "            例として、以下のように進行します：\n",
    "            例：キーワードは「モロッコ」であるとします：\n",
    "            <assistant: それは都市ですか？\n",
    "            user: いいえ\n",
    "            assistant: それは国ですか？\n",
    "            user: はい\n",
    "            assistant: アフリカにありますか？\n",
    "            user: はい\n",
    "            assistant: その国名はmで始まりますか？\n",
    "            user: はい\n",
    "            assistant: モロッコですか？\n",
    "            user: はい。>\n",
    "            \"\"\"\n",
    "\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}\\n\"\"\"\n",
    "\n",
    "            if len(obs.questions)>=1:\n",
    "                chat_template += \"これまでの会話の履歴です：\\n\"\n",
    "                chat_template += \"<\\n\"\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"assistant:\\n{q}\\n\"\n",
    "                    chat_template += f\"user:\\n{a}\\n\"\n",
    "                chat_template += \">\\n\"\n",
    "                    \n",
    "            chat_template += \"\"\"\n",
    "            ユーザーが選んだ単語について、最初の質問をしてください！\n",
    "            短く簡潔に、質問は一つだけで、余計な言葉は使わないでください！<end_of_turn>\n",
    "            <start_of_turn>assistant\\n\n",
    "            \"\"\"\n",
    "#             print(chat_template)\n",
    "            output = generate_answer(chat_template) \n",
    "            output = parse_response(output)\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"質問:\\n{q}\\n回答:\\n{a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            あなたの役割は質問と回答に基づいてキーワードを予測することです。\n",
    "            進行の例は以下の通りです：\n",
    "            例：\n",
    "            <質問: それは都市ですか？\n",
    "            回答: いいえ\n",
    "            質問: それは国ですか？\n",
    "            回答: はい\n",
    "            質問: アフリカにありますか？\n",
    "            回答: はい\n",
    "            質問: その国名はmで始まりますか？\n",
    "            回答: はい\n",
    "            あなた: モロッコ>\n",
    "            \n",
    "            現在のゲームの状況は以下の通りです:\\n{conv}\n",
    "            この会話に基づいて、単語を予測してください。余計な言葉は使わずにください<end_of_turn>\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{guess_prompt}\\n\"\"\"\n",
    "            chat_template += \"<start_of_turn>あなた:\\n\"\n",
    "#             print(chat_template)\n",
    "            output = generate_answer(chat_template)        \n",
    "        return output\n",
    "    \n",
    "    def answerer(self, obs):\n",
    "        # 質問に対する回答を提供します\n",
    "        ask_prompt = f\"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者の「はい」または「いいえ」の質問に答えます。\n",
    "        進行の例は以下の通りです：\n",
    "        例：キーワードが「モロッコ」の場合\n",
    "        <user: それは場所ですか？\n",
    "        あなた: はい\n",
    "        user: ヨーロッパにありますか？\n",
    "        あなた: いいえ\n",
    "        user: アフリカにありますか？\n",
    "        あなた: はい\n",
    "        user: その国名はmで始まりますか？\n",
    "        あなた: はい\n",
    "        user: モロッコですか？\n",
    "        あなた: はい。>\n",
    "        「{obs.keyword}」に関して、以下の質問には「はい」または「いいえ」で答えてください：\n",
    "        {obs.questions[-1]}\n",
    "        \"\"\"\n",
    "        chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}<end_of_turn>\\n\"\"\"\n",
    "        chat_template += \"<start_of_turn>アシスタント\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "# ロボットのインスタンスを作成します\n",
    "robot = Robot()\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    # エージェントが行う処理\n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"はい\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.192968Z",
     "iopub.status.busy": "2024-07-11T06:52:15.192649Z",
     "iopub.status.idle": "2024-07-11T06:52:22.612061Z",
     "shell.execute_reply": "2024-07-11T06:52:22.611154Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.192945Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします。出力は表示しないようにします。\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:22.615033Z",
     "iopub.status.busy": "2024-07-11T06:52:22.614746Z",
     "iopub.status.idle": "2024-07-11T06:57:12.011816Z",
     "shell.execute_reply": "2024-07-11T06:57:12.010683Z",
     "shell.execute_reply.started": "2024-07-11T06:52:22.615005Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzを使用して高速に圧縮し、pvで進行状況を表示しながらsubmission.tar.gzというアーカイブを作成します\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/qwen2/transformers/qwen2-7b-instruct . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.013563Z",
     "iopub.status.busy": "2024-07-11T06:57:12.013245Z",
     "iopub.status.idle": "2024-07-11T06:57:12.018443Z",
     "shell.execute_reply": "2024-07-11T06:57:12.017575Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.013535Z"
    }
   },
   "outputs": [],
   "source": [
    "# tar.gzファイルの中身を確認するためのコードです\n",
    "\n",
    "import tarfile\n",
    "# submission.tar.gzファイルを開きます\n",
    "tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")\n",
    "# アーカイブ内のファイル名をすべて印刷します\n",
    "for file in tar.getmembers():\n",
    "    print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb296ce",
   "metadata": {},
   "source": [
    "# シミュレーション\n",
    "エージェントを提出する前にシミュレーションを行いたい場合は、以下のコメントを解除して実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.019843Z",
     "iopub.status.busy": "2024-07-11T06:57:12.019546Z",
     "iopub.status.idle": "2024-07-11T06:57:12.027836Z",
     "shell.execute_reply": "2024-07-11T06:57:12.02707Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.019812Z"
    }
   },
   "outputs": [],
   "source": [
    "# keywords_local.pyというファイルを指定したURLからダウンロードします\n",
    "# !wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.02921Z",
     "iopub.status.busy": "2024-07-11T06:57:12.028922Z",
     "iopub.status.idle": "2024-07-11T06:57:12.036912Z",
     "shell.execute_reply": "2024-07-11T06:57:12.036194Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.029187Z"
    }
   },
   "outputs": [],
   "source": [
    "# モジュールをインポートします\n",
    "# import json\n",
    "# import pandas as pd\n",
    "# submission.mainモジュールからエージェントをインポートします\n",
    "# from submission.main import agent\n",
    "# keywords_localからKEYWORDS_JSONをインポートします\n",
    "# from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# 観察用のクラスを定義します\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        self.step = 0  # ステップカウンタ\n",
    "        self.role = \"guesser\"  # ロールは「推測者」\n",
    "        self.turnType = \"ask\"  # ゲームのターンタイプは「質問」\n",
    "        self.keyword = \"Japan\"  # キーワードの初期値\n",
    "        self.category = \"country\"  # カテゴリーの初期値\n",
    "        self.questions = []  # 質問のリスト\n",
    "        self.answers = []  # 回答のリスト\n",
    "        self.guesses = []  # 推測のリスト\n",
    "        \n",
    "# KEYWORDS_JSONを使ってキーワードのデータフレームを作成する関数を定義します\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON)  # JSONデータをロードします\n",
    "\n",
    "    keyword_list = []  # キーワードのリスト\n",
    "    category_list = []  # カテゴリーのリスト\n",
    "    alts_list = []  # 代替案のリスト\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):\n",
    "            keyword = json_data[i]['words'][j]['keyword']  # キーワードを取得\n",
    "            keyword_list.append(keyword)  # リストに追加\n",
    "            category_list.append(json_data[i]['category'])  # カテゴリーを追加\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])  # 代替案を追加\n",
    "\n",
    "    # データフレームを作成します\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n",
    "    data_pd['keyword'] = keyword_list\n",
    "    data_pd['category'] = category_list\n",
    "    data_pd['alts'] = alts_list\n",
    "    \n",
    "    return data_pd  # 作成したデータフレームを返します\n",
    "    \n",
    "# KEYWORDS_JSONを基にキーワードのデータフレームを作成します\n",
    "# keywords_df = create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.038569Z",
     "iopub.status.busy": "2024-07-11T06:57:12.038129Z",
     "iopub.status.idle": "2024-07-11T06:57:12.049014Z",
     "shell.execute_reply": "2024-07-11T06:57:12.048315Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.038537Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観察インスタンスを作成します\n",
    "# obs = Observation()\n",
    "# コンフィグは \"_\" で初期化します\n",
    "# cfg = \"_\"\n",
    "\n",
    "# keywords_dfからサンプルを取得します\n",
    "# sample_df = keywords_df.sample()\n",
    "# 観察にキーワードとカテゴリーを設定します\n",
    "# obs.keyword = sample_df[\"keyword\"].values[0]\n",
    "# obs.category = sample_df[\"category\"].values[0]\n",
    "# 代替案リストを取得します\n",
    "# alts_list = sample_df[\"alts\"].values[0]\n",
    "# キーワードを代替案リストに追加します\n",
    "# alts_list.append(obs.keyword)\n",
    "\n",
    "# 選ばれたキーワードを表示します\n",
    "# print(f\"keyword:{obs.keyword}\")\n",
    "\n",
    "# 20ラウンドまでのゲームをシミュレーションします\n",
    "# for round in range(20):\n",
    "#     obs.step = round+1  # ラウンド数を更新\n",
    "    \n",
    "#     obs.role = \"guesser\"  # ロールを「推測者」に設定\n",
    "#     obs.turnType = \"ask\"  # ターンタイプを「質問」に設定\n",
    "#     question = agent(obs, cfg)  # エージェントに質問をさせます\n",
    "#     obs.questions.append(question)  # 質問を記録します\n",
    "    \n",
    "#     obs.role = \"answerer\"  # ロールを「回答者」に設定\n",
    "#     obs.turnType = \"answer\"  # ターンタイプを「回答」に設定\n",
    "#     answer = agent(obs, cfg)  # エージェントに回答をさせます\n",
    "#     obs.answers.append(answer)  # 回答を記録します\n",
    "    \n",
    "#     obs.role = \"guesser\"  # ロールを「推測者」に戻します\n",
    "#     obs.turnType = \"guess\"  # ターンタイプを「推測」に設定\n",
    "#     guess = agent(obs, cfg)  # エージェントに推測をさせます\n",
    "#     obs.guesses.append(guess)  # 推測を記録します\n",
    "    \n",
    "#     # ラウンドの進行状況を表示します\n",
    "#     print(f\"round: {round+1}\")\n",
    "#     print(f\"question: {question}\")\n",
    "#     print(f\"answer: {answer}\")\n",
    "#     print(f\"guess: {guess}\")\n",
    "    \n",
    "#     # 推測が代替案リストに含まれているかをチェックします\n",
    "#     if guess in alts_list:\n",
    "#         print(\"勝ちました!!\")  # 勝利した場合のメッセージ\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 51944,
     "sourceId": 62188,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
