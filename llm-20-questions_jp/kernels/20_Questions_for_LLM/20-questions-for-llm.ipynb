{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "170d1344",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「20の質問 LLM コンペティション」に参加するための問題解決手法を示しています。このコンペティションの目標は、AIが秘密の単語を当てるために最大20の質問に答える「20の質問」ゲームをプレイすることです。\n",
    "\n",
    "ノートブックでは、具体的には以下の内容に取り組んでいます：\n",
    "\n",
    "1. **問題設定**: 参加者が考えた秘密の単語を、質問者LLMが質問を通じて推測するという構造のゲームです。このゲームは、限られた質問回数の中でターゲット単語を当てるために戦略的かつ効率的な情報収集を行う必要があります。\n",
    "\n",
    "2. **使用技術**:\n",
    "   - **ライブラリ**: `transformers`ライブラリを使用し、事前学習済みのRoBERTaモデル（\"deepset/roberta-base-squad2\"）を利用して、質問応答タスクを実行します。\n",
    "   - **GPU利用**: 質問応答パイプラインは、GPUを利用して処理速度を向上させています。\n",
    "\n",
    "3. **手法**:\n",
    "   - **プロンプト生成**: `generate_prompt`関数が、キーワードとそのカテゴリー、大陸などのコンテキスト情報をもとにプロンプトを生成します。\n",
    "   - **応答生成**: `get_answer_from_model`関数を使用して、生成したプロンプトに対する質問の回答をモデルから取得します。\n",
    "\n",
    "4. **実行**: `main`関数内で実際にキーワード「Mount Everest」を使用し、関連する質問をリスト化して、それぞれの質問に対するモデルの回答を表示します。これにより、モデルが受け取った質問に対してどのように応答するかが確認できます。\n",
    "\n",
    "このノートブックは、言語モデルを利用して「20の質問」ゲームに取り組むための基礎を築くものであり、特に自然言語処理の技術を用いて、AIが人間のゲームプレイのように推論する能力を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a478363f",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に示すのは、上記のJupyter Notebookにおいて初心者がつまずきそうな専門用語の簡単な解説です。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **RoBERTa (Robustly optimized BERT approach)**:\n",
    "   - BERTに基づいた自然言語処理モデルで、事前学習を強化し、いくつかのハイパーパラメータやトレーニングテクニックを最適化している。QA (質問応答) タスクに特化したファインチューニングが施されている。\n",
    "\n",
    "2. **トークナイザー**:\n",
    "   - テキストを入力として受け取り、それをモデルが理解できる形式（トークン）に変換するツール。単語や文の区切りを認識し、ID番号に変換する。\n",
    "\n",
    "3. **パイプライン**:\n",
    "   - 特定のタスク（例: 質問応答）を実行するための簡易的なインターフェース。モデルのロード、トークン化、入力の生成、出力の取得を一連の処理としてまとめている。\n",
    "\n",
    "4. **ファインチューニング**:\n",
    "   - 事前学習されたモデルを特定のデータセットやタスクに対して最適化するプロセス。通常、大規模なデータセットで学習したモデルを用いて、より少ないデータセットで再学習すること。\n",
    "\n",
    "5. **プロンプト**:\n",
    "   - モデルに入力するための情報や質問の表現。コンテキスト情報も含めてモデルに提供することで、適切な応答を得るための手がかりを与える。\n",
    "\n",
    "6. **コンテキスト**:\n",
    "   - モデルが質問に答えるために使用する情報のこと。通常は、モデルが質問を理解し応答するための背景情報を指す。\n",
    "\n",
    "7. **GPU (Graphics Processing Unit)**:\n",
    "   - 大量の計算を並列処理するために特化したプロセッサ。深層学習モデルのトレーニングや推論において計算速度を大幅に向上させるために使用される。\n",
    "\n",
    "8. **除外情報 (Negate)**:\n",
    "   - 特定の条件や選択肢から排除する情報。特定のカテゴリに含まれないことを示すことで、モデルの推測や質問の精度を高めるために用いられる。\n",
    "\n",
    "9. **応答 (Response)**:\n",
    "   - モデルが質問に基づいて生成した答えのこと。この場合、質問応答タスクにおいて生成された具体的な情報を指す。\n",
    "\n",
    "10. **エピソード**:\n",
    "    - コンペティションの進行中に行われる1回のゲームセッション。ゲームのルールに従い、質問と応答を繰り返すことで進行する。\n",
    "\n",
    "これらの用語の理解を深めることで、ノートブックに記載された内容や全体的なコンペティションのアプローチをよりよく理解できると思います。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0b77ba",
   "metadata": {},
   "source": [
    "### 「20の質問 LLM コンペティション」は、人工知能、自然言語処理、ゲーム理論の分野を結びつける革新的な挑戦です。このコンペティションは、参加者の一人が秘密の単語を考え、他の参加者が最大20のはい/いいえの質問を通じてそれを推測するという古典的な「20の質問」ゲームを中心に展開されます。このシンプルでありながら魅力的なゲームが、高度なAIコンペティションの基盤となっています。\n",
    "\n",
    "<figure>\n",
    "        <img src=\"https://www.kaggle.com/competitions/61247/images/header\" alt =\"Audio Art\" style='width:800px;height:500px;'>\n",
    "        <figcaption>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-04T08:47:05.564045Z",
     "iopub.status.busy": "2024-08-04T08:47:05.563775Z",
     "iopub.status.idle": "2024-08-04T08:47:13.977278Z",
     "shell.execute_reply": "2024-08-04T08:47:13.97615Z",
     "shell.execute_reply.started": "2024-08-04T08:47:05.56402Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# 事前学習済みのRoBERTaモデルとトークナイザーをロードします\n",
    "model_id = \"deepset/roberta-base-squad2\"  # QA用にファインチューニングされたRoBERTaモデル\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # トークナイザーの初期化\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)  # モデルの初期化\n",
    "\n",
    "# GPU用の質問応答パイプラインを初期化します\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)  # device=0はGPUを使用することを確認します\n",
    "\n",
    "def generate_prompt(keyword, category, negate, continent):\n",
    "    if category in negate:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a {category}. {negate[category]} \"\n",
    "                  f\"This word has first letter {keyword[0]}. This {category} is located in {continent}.\")\n",
    "    else:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a thing. It is not a city. \"\n",
    "                  f\"It is not a country. It is not a landmark. This word has first letter {keyword[0]}.\")\n",
    "    return prompt  # 生成したプロンプトを返します\n",
    "\n",
    "def get_answer_from_model(prompt, question):\n",
    "    # モデルから回答を生成します\n",
    "    response = qa_pipeline(question=question, context=prompt)  # 質問とプロンプトを使用して応答を取得\n",
    "    answer = response['answer']  # 応答から答えを取得します\n",
    "    return answer  # 答えを返します\n",
    "\n",
    "def main():\n",
    "    keyword = \"Mount Everest\"  # キーワードを設定します（ここではエベレスト）\n",
    "    category = \"mountain\"  # カテゴリーを設定します（ここでは山）\n",
    "    negate = {\"mountain\": \"It is not a country, city, or person.\"}  # 除外情報を設定します\n",
    "    continent = \"Asia\"  # 大陸を設定します（ここではアジア）\n",
    "\n",
    "    prompt = generate_prompt(keyword, category, negate, continent)  # プロンプトを生成します\n",
    "\n",
    "    questions = [\n",
    "        \"Is this a country or city?\",  # 質問リストの最初の質問\n",
    "        \"No, it is a mountain. What is the name of this mountain?\",  # 次の質問\n",
    "        \"Where is this place?\"  # 最後の質問\n",
    "    ]\n",
    "\n",
    "    # 構築したプロンプトを表示します\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # 各質問を繰り返します\n",
    "    for question in questions:\n",
    "        # モデルから答えを取得します\n",
    "        answer = get_answer_from_model(prompt, question)\n",
    "        \n",
    "        # フォーマットされた出力を表示します\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Answer: '{answer}'\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # メイン関数を実行します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T08:47:17.974574Z",
     "iopub.status.busy": "2024-08-04T08:47:17.973938Z",
     "iopub.status.idle": "2024-08-04T08:47:17.982056Z",
     "shell.execute_reply": "2024-08-04T08:47:17.98108Z",
     "shell.execute_reply.started": "2024-08-04T08:47:17.974542Z"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# 事前学習済みのRoBERTaモデルとトークナイザーをロードします\n",
    "model_id = \"deepset/roberta-base-squad2\"  # QA用にファインチューニングされたRoBERTaモデル\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # トークナイザーの初期化\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)  # モデルの初期化\n",
    "\n",
    "# GPU用の質問応答パイプラインを初期化します\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)  # device=0はGPUを使用することを確認します\n",
    "\n",
    "def generate_prompt(keyword, category, negate, continent):\n",
    "    if category in negate:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a {category}. {negate[category]} \"\n",
    "                  f\"This word has first letter {keyword[0]}. This {category} is located in {continent}.\")\n",
    "    else:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a thing. It is not a city. \"\n",
    "                  f\"It is not a country. It is not a landmark. This word has first letter {keyword[0]}.\")\n",
    "    return prompt  # 生成したプロンプトを返します\n",
    "\n",
    "def get_answer_from_model(prompt, question):\n",
    "    # モデルから回答を生成します\n",
    "    response = qa_pipeline(question=question, context=prompt)  # 質問とプロンプトを使用して応答を取得\n",
    "    answer = response['answer']  # 応答から答えを取得します\n",
    "    return answer  # 答えを返します\n",
    "\n",
    "def main():\n",
    "    keyword = \"Mount Everest\"  # キーワードを設定します（ここではエベレスト）\n",
    "    category = \"mountain\"  # カテゴリーを設定します（ここでは山）\n",
    "    negate = {\"mountain\": \"It is not a country, city, or person.\"}  # 除外情報を設定します\n",
    "    continent = \"Asia\"  # 大陸を設定します（ここではアジア）\n",
    "\n",
    "    prompt = generate_prompt(keyword, category, negate, continent)  # プロンプトを生成します\n",
    "\n",
    "    questions = [\n",
    "        \"Is this a country or city?\",  # 質問リストの最初の質問\n",
    "        \"No, it is a mountain. What is the name of this mountain?\",  # 次の質問\n",
    "        \"Where is this place?\"  # 最後の質問\n",
    "    ]\n",
    "\n",
    "    # 構築したプロンプトを表示します\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # 各質問を繰り返します\n",
    "    for question in questions:\n",
    "        # モデルから答えを取得します\n",
    "        answer = get_answer_from_model(prompt, question)\n",
    "        \n",
    "        # フォーマットされた出力を表示します\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Answer: '{answer}'\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  # メイン関数を実行します\n",
    "\"\"\"\n",
    "\n",
    "# コードをsubmission.pyに保存します\n",
    "with open(\"/kaggle/working/submission.py\", \"w\") as f:\n",
    "    f.write(code)  # コードを書き込む"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
