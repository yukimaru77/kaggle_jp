{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "387810c5",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「20の質問」コンペティションに参加するためのエージェント（ロボット）を作成する方法を示しています。ノートブックは、特に初心者向けに設計されており、ゲーム内で質問者、回答者、推測者の役割を果たすことができるモデルの実装に焦点を当てています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "このノートブックは、言語モデル（LLM）を用いて「20の質問」ゲームをプレイするエージェントを構築するという課題に取り組んでいます。ユーザーが選んだ単語を質問を通じて推測し、また、その質問に対して「はい」または「いいえ」で回答する必要があります。\n",
    "\n",
    "### 使用している手法とライブラリ\n",
    "1. **ライブラリ**:\n",
    "   - `transformers`: Hugging FaceのTransformersライブラリを使用して、事前訓練された言語モデルをロードする。\n",
    "   - `torch`: PyTorchを使用して、モデルの推論を行います。\n",
    "   - `kaggle_secrets`: Kaggle環境でのシークレット管理に使用。\n",
    "\n",
    "2. **モデル初期化**:\n",
    "   - `AutoTokenizer` と `AutoModelForCausalLM` を利用して、モデルとトークナイザーを初期化。\n",
    "\n",
    "3. **エージェントの設計**:\n",
    "   - `Robot`クラスを定義し、質問者、回答者、推測者のそれぞれの役割を果たすメソッドを提供。\n",
    "   - 質問は「はい」または「いいえ」で答える形式とし、適切なプロンプトを与えて生成する質問を制御。\n",
    "\n",
    "4. **エージェントの実行**:\n",
    "   - `agent`関数において、ゲームの状態に応じて適切なモード（質問、推測、回答）を設定し、ロボットが正しい応答を生成するように設計されている。\n",
    "\n",
    "5. **ローカルテスト**:\n",
    "   - 環境内でのシミュレーションとローカルテストが可能なコードが含まれています。\n",
    "\n",
    "ノートブックは最後に、提出用のアーカイブファイルを作成するためのコードも含まれており、コンペティションにエージェントを提出する際の手順に従っています。\n",
    "\n",
    "全体として、このノートブックは20の質問ゲームを効果的にプレイできるエージェントを実装するための明確な手順を提供しており、使用するライブラリや手法を詳細に解説しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35a3ee7",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に初心者がつまずきそうな専門用語の簡単な解説を示します。特に、実務を経験していないと馴染みのないものや、ノートブック特有のドメイン知識に焦点を当てました。\n",
    "\n",
    "1. **LLM (Large Language Model)**:\n",
    "   大規模言語モデルの略。大量のテキストデータを基に学習され、文脈を理解し、テキスト生成や質問応答などのタスクを遂行できるAIモデル。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**:\n",
    "   テキストをトークンと呼ばれる小さな単位（単語やサブワードなど）に分割するツール。モデルが入力を理解しやすくするために、テキストを数値に変換するプロセスの一部。\n",
    "\n",
    "3. **CUDA (Compute Unified Device Architecture)**:\n",
    "   NVIDIA社によって開発された並列コンピューティングアーキテクチャ。GPUを用いた高速な計算を可能にし、大規模なデータ処理や深層学習を効率的に行うことができる。\n",
    "\n",
    "4. **メモリ効率 (Memory Efficiency)**:\n",
    "   モデルが使用するメモリの効率的な管理を指す。特にGPUでの計算時に、メモリ使用量を抑える工夫が施されている。\n",
    "\n",
    "5. **バイノーム (Binomial)**:\n",
    "   二項分布を用いた統計的手法に関連する話題で、通常は成功・失敗の2つの結果が存在する状況を表す。\n",
    "\n",
    "6. **エピソード (Episode)**:\n",
    "   強化学習などの文脈で使われる概念。ある一連の行動や試行を指し、特定のタスクを完了するまでの過程を示す。\n",
    "\n",
    "7. **最大新トークン (max_new_tokens)**:\n",
    "   モデルが生成する際の最大トークン数。このパラメータによって、生成される応答の長さを制限することができる。\n",
    "\n",
    "8. **`torch_dtype`**:\n",
    "   PyTorchと呼ばれる深層学習フレームワークにおいて、テンソルのデータ型を指定する引数。\"bfloat16\"は、浮動小数点数の一種で、特に深層学習での計算精度と性能のバランスを考慮して使用される。\n",
    "\n",
    "9. **os.path.exists**:\n",
    "   Pythonのosモジュールを使ったファイルやディレクトリの存在確認に用いる関数。指定されたパスが存在するかをチェックすることができる。\n",
    "\n",
    "10. **assert**:\n",
    "    Pythonのキーワードで、条件が真であることを確認するために使用される。条件が偽の場合には、エラーを引き起こしてプログラムを終了させる。\n",
    "\n",
    "11. **ヒント (Hint)**:\n",
    "    ゲームや問題を解決するための手がかり。特にこのコンペティションの文脈では、ユーザーが単語を推測する際に利用する際のアシストを指す。\n",
    "\n",
    "12. **tar.gz**:\n",
    "    複数のファイルをまとめるために使用される圧縮ファイル形式の一つ。tarはアーカイブ形式、gzはgzip圧縮を示す。\n",
    "\n",
    "これらは、初心者が理解する上で役立つ情報を提供し、機械学習や深層学習のコンテキストでも重要な概念です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cfac86",
   "metadata": {},
   "source": [
    "# このイントロダクションは初心者向けです\n",
    "## このコンペティションで必要なことは？\n",
    "\n",
    "**要約：20の質問ゲームをプレイできるモデルを作成すること**\n",
    "\n",
    "ロボットが話し、歩き、踊ることができるように構築する必要があると想像してみてください。あなたの仕事は、そのロボットが各役割を適切に行い、ユーザーが好きな役割を選択して他の役割に干渉することなく実行できるようにすることです（例えば、3つのボタンを追加できます）。\n",
    "\n",
    "これはまさに、このコンペティションであなたに求められていることです。質問者、回答者、推測者の3つの役割を果たすことができるエージェント/ロボット（llmモデル）を構築する必要があります。\n",
    "\n",
    "こちらの概要セクションを読んだ場合、提出したエージェントは、別の参加者が提出したエージェントと対戦することになります。このデュオの中で、あなたのエージェントは質問者と推測者の役割を果たすか、回答者の役割を果たします。役割とバイノーム（2人組）は、裏での環境によっていくつかの条件に基づいて選択され、あなたはそれについて心配する必要はありません。\n",
    "\n",
    "あなたがすべきことは、環境が回答者の役割をプレイすることを決定したとき、あなたのエージェントが「はい」または「いいえ」で答えることを確認することだけです。他の回答をすることは、ゲームに負けることになります。\n",
    "\n",
    "詳細はこちらで確認できます [こちら](https://www.kaggle.com/competitions/llm-20-questions/overview) \n",
    "\n",
    "## 提出方法は？\n",
    "\n",
    "コードはmain.pyという1つのファイルにまとめなければならず、このファイルにはエージェント/ロボットのコードと、obsおよびcfgを引数とする必須の関数が含まれている必要があります。これは、裏で環境があなたのエージェントを実行するために使用されます。\n",
    "\n",
    "私たちのコード（他のノートブックに比べて非常にシンプルです）では、この関数に「agent」という名前を付け、他のロジックを「robot」という名前のクラスに入れます。\n",
    "\n",
    "環境はオフラインモードでコードを実行し、/kaggle/inputディレクトリにアクセスできないため、必要なパッケージとmain.pyファイルを1つのtar.gzファイルにロード/コピーする必要があります。\n",
    "\n",
    "裏でtar.gzファイルは「/kaggle_simulations/agent/」フォルダの下に展開されるため、私たちは実行環境に応じてパスを調整するコードを追加します（下記を参照）。\n",
    "\n",
    "コードをローカルでテストすることもできます。ローカルテストについては、以下の「ローカルテスト」セクションを参照してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:30:07.460467Z",
     "iopub.status.busy": "2024-06-25T14:30:07.460168Z",
     "iopub.status.idle": "2024-06-25T14:30:07.484708Z",
     "shell.execute_reply": "2024-06-25T14:30:07.483728Z",
     "shell.execute_reply.started": "2024-06-25T14:30:07.460437Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:26:54.32383Z",
     "iopub.status.busy": "2024-06-25T21:26:54.323407Z",
     "iopub.status.idle": "2024-06-25T21:30:25.229808Z",
     "shell.execute_reply": "2024-06-25T21:30:25.228768Z",
     "shell.execute_reply.started": "2024-06-25T21:26:54.323795Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "# CUDAのメモリ効率を有効にする\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Kaggleのエージェントパスを設定\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# トークナイザーとモデルを初期化\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "# 回答を生成する関数\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"modeはこれらの値のいずれかでなければなりません：asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問者の役割を始める\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 回答者の役割を始める\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"   \n",
    "            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n",
    "                output = \"yes\"  # 明確な回答が無かった場合は「yes」にする\n",
    "        if mode == \"guessing\":\n",
    "            # 推測者の役割を始める\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"あなたは有能なAIアシスタントであり、20の質問ゲームをプレイするのが非常に得意です。\n",
    "        ユーザーは単語を考えるつもりです。それは次の3つのカテゴリのいずれかです：\n",
    "        1. 場所\n",
    "        2. 人物\n",
    "        3. 物\n",
    "        これらのオプションに焦点を当てて、検索範囲を狭めるための賢い質問をしてください。\\n\"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"あなたの役割は、彼に20の質問以内で単語を見つけることです。あなたの質問は有効であるためには「はい」または「いいえ」の回答のみでなければなりません。\n",
    "            ヒントとして、以下にどのように機能すべきかの例があります。キーワードがモロッコであると仮定します：\n",
    "            例：\n",
    "            <あなた：それは場所ですか？\n",
    "            ユーザー：はい\n",
    "            あなた：ヨーロッパにありますか？\n",
    "            ユーザー：いいえ\n",
    "            あなた：アフリカにありますか？\n",
    "            ユーザー：はい\n",
    "            あなた：そこで生きている人々の大半は肌が暗いですか？\n",
    "            ユーザー：いいえ\n",
    "            ユーザー：mから始まる国名ですか？\n",
    "            あなた：はい\n",
    "            あなた：モロッコですか？\n",
    "            ユーザー：はい。>\n",
    "\n",
    "            ユーザーが単語を選びました。最初の質問をしてください！\n",
    "            短く簡潔にし、余計な言葉は使わず、一つの質問だけをしてください！\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"これまでのゲームの状態は次の通りです:\\n{conv}\n",
    "            会話に基づいて、単語を推測できますか？単語だけを教えてください、余計な言葉はいりません。\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"あなたは有能なAIアシスタントであり、20の質問ゲームをプレイするのが非常に得意です。\n",
    "        ユーザーの役割は、あなたに対して最大20の質問を使って単語を推測することです。あなたの回答は「はい」または「いいえ」でなければなりません。その他の回答は無効であり、それによりゲームに負けます。\n",
    "        ユーザーは常に次の3つのカテゴリのいずれかに属する単語を推測することになります：\n",
    "        1. 場所\n",
    "        2. 人物\n",
    "        3. 物\n",
    "        ユーザーの質問を理解し、あなたがプレイしているキーワードを把握してください。\n",
    "        現在、ユーザーが推測する必要がある単語は「{obs.keyword}」で、カテゴリは「{obs.category}」です。\n",
    "        ヒントとして、以下にどのように機能すべきかの例があります。キーワードがモロッコでカテゴリ「場所」であると仮定します：\n",
    "        例：\n",
    "        <ユーザー：それは場所ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：ヨーロッパにありますか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：アフリカにありますか？\n",
    "        あなた：はい\n",
    "        ユーザー：そこで生きている人々の大半は肌が暗いですか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：mから始まる国名ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：モロッコですか？\n",
    "        あなた：はい。>\"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4fd9a",
   "metadata": {},
   "source": [
    "# ローカルテスト <a id=\"lc\"></a>\n",
    "\n",
    "ローカルでテストするには、まず上のセルの「%%writefile -a submission/main.py」をコメントアウトし、このセルを実行します。\n",
    "次に、以下のセルをコメントアウト解除して実行してください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:31:20.53805Z",
     "iopub.status.busy": "2024-06-25T21:31:20.537013Z",
     "iopub.status.idle": "2024-06-25T21:31:21.897103Z",
     "shell.execute_reply": "2024-06-25T21:31:21.896084Z",
     "shell.execute_reply.started": "2024-06-25T21:31:20.538008Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from kaggle_environments import make\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# game_output = env.run(agents=[agent, agent, agent, agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:42:24.806653Z",
     "iopub.status.busy": "2024-06-25T21:42:24.805663Z",
     "iopub.status.idle": "2024-06-25T21:42:24.873565Z",
     "shell.execute_reply": "2024-06-25T21:42:24.872711Z",
     "shell.execute_reply.started": "2024-06-25T21:42:24.806602Z"
    }
   },
   "outputs": [],
   "source": [
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67358746",
   "metadata": {},
   "source": [
    "# 提出ファイル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:14:28.205431Z",
     "iopub.status.busy": "2024-06-25T16:14:28.204763Z",
     "iopub.status.idle": "2024-06-25T16:14:31.046479Z",
     "shell.execute_reply": "2024-06-25T16:14:31.045488Z",
     "shell.execute_reply.started": "2024-06-25T16:14:28.205393Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:14:47.111039Z",
     "iopub.status.busy": "2024-06-25T16:14:47.110649Z",
     "iopub.status.idle": "2024-06-25T16:20:02.504823Z",
     "shell.execute_reply": "2024-06-25T16:20:02.503606Z",
     "shell.execute_reply.started": "2024-06-25T16:14:47.110992Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/llama-3/transformers/8b-chat-hf . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T16:20:02.507126Z",
     "iopub.status.busy": "2024-06-25T16:20:02.506741Z",
     "iopub.status.idle": "2024-06-25T16:22:51.972507Z",
     "shell.execute_reply": "2024-06-25T16:22:51.971476Z",
     "shell.execute_reply.started": "2024-06-25T16:20:02.507093Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tar.gzファイルの中身を確認する\n",
    "\n",
    "# import tarfile\n",
    "# tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")\n",
    "# for file in tar.getmembers():\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9d6ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## davide\n",
    "> \n",
    "> 素晴らしい仕事です。共有してくれてありがとうございます！\n",
    "> \n",
    "> ただ質問ですが、このコードはkeyword.pyファイルを全く使用していないのですか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > [@davidemariani](https://www.kaggle.com/davidemariani) いいえ\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## dhruvyadav89300\n",
    "> \n",
    "> ノートブックを説明してくれてありがとう。とても役に立ちました。\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > それを聞いて嬉しいです :)\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Chazzyie\n",
    "> \n",
    "> あなたの作業を共有してくれてありがとう。シンプルで明確です 🤩\n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Matthew S Farmer\n",
    "> \n",
    "> シンプルなノートブックですね。FYI、このコンペティションでは「人物」というカテゴリが削除されました。それはあなたのエージェントがキーワードの検索空間を少しでも保つのに役立つかもしれません。 \n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Matthew S Farmer\n",
    "> > \n",
    "> > [https://www.kaggle.com/competitions/llm-20-questions/discussion/512955#2884981](https://www.kaggle.com/competitions/llm-20-questions/discussion/512955#2884981)\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > おお、ありがとうございます。知らなかったです！\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## BK\n",
    "> \n",
    "> 共有してくれてありがとう！あなたのノートブックが大好きです :)\n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Eslam Mohamed\n",
    "> \n",
    "> 素晴らしい仕事です！あなたの説明は最高です。\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > それを聞いて嬉しいです！\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## 古月方源\n",
    "> \n",
    "> ローカルテストが成功しました\n",
    "> \n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## 古月方源\n",
    "> \n",
    "> 最新の3.1を成功裏に実行しましたが、提出後にエラーが発生し続けました\n",
    "> \n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Sandeep Sharma\n",
    "> \n",
    "> ノートブックをありがとう。しかし、テストしようとすると「1つの位置引数 'cfg' が不足しています」と表示されます。\n",
    "> \n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Toshi\n",
    "> \n",
    "> 素晴らしいノートブックです。\n",
    "> \n",
    "> ローカルでテストする場合、どのバージョンのpytorchをインストールすべきですか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > [@mst923](https://www.kaggle.com/mst923) あなたのパソコンでローカルという意味ですか？良いGPUがない限り、動作しないでしょう\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## kothiwsk28\n",
    "> \n",
    "> 共有してくれてありがとう！このノートブックに3つのデータセット（数字認識、タイタニック、ハウスプライス）を保持しておく特別な理由がありますか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > いいえ、削除した方が良いです。ノートブックには、競技にやってくる初心者を助けるために追加しました。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Mask\n",
    "> \n",
    "> 役に立つノートブックですね。\n",
    "> \n",
    "> 私はKaggleに新しく、フォークしたりコピーしたりしたときに、実際に提出ボタンが見つかりません！あなたのノートブックに付属している他の競技（数字認識者とタイタニック）があり、そのノートブックでLlamaを使用してすべてのコードをコピーしたとき、モデルパスが正しくないというエラーが表示されましたが、正しいです！\n",
    "> \n",
    "> 何が間違っているのかよくわかりません！\n",
    "> \n",
    "> あなたのノートブックをフォークして、エラーなしに提出する方法を教えてもらえますか？私はここでは新しいので、使い方を知りたいです。\n",
    "> \n",
    "> 提出ボタンがないのは非常に奇妙で、モデルディレクトリが正しく表示されているのに、正しくないと言われています！\n",
    "> \n",
    "> \n",
    "> > ## Maoshenli\n",
    "> > \n",
    "> > 私も同じ問題に直面しました。解決しましたか？😄\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > ## jehlum\n",
    "> > \n",
    "> > モデルを右側の入力タブから追加しましたか？\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > 皆さん、llamaモデルを使用するための条件を受け入れ、アクセスを許可されていることを確認してください。この問題を解決し、ノートブックが正しく実行されれば、右上の3つの点に行き、「コンペティションに提出」をクリックしてください。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Lucas\n",
    "> \n",
    "> コードをありがとうございます。提出しましたが、LBで600点しか得られませんでした。750点を獲得するにはどうすれば良いですか？Llama3 8bを微調整しましたか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Krens\n",
    "> > \n",
    "> > ゲームを提出したとき、初期スコアは600ポイントでした。スコアは他の参加者のエージェントとのゲームをプレイすることでのみ変動します。さらに、分散は比較的大きく、それほど安定していません。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## zerojin\n",
    "> \n",
    "> 作業を共有してくれてありがとう。質問があります。使用しているllama3モデルは微調整されていますか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > 私のデータで微調整されていますか？答えはいいえです。Kaggleハブのローカルモデルのパスを使用していることがわかります。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## tingyu516\n",
    "> \n",
    "> こんにちは。このコードを提出して「Validation Episode failed」というエラーメッセージを受け取りました。どうすれば良いですか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > エージェントのログをダウンロードして、何が起こったのかを確認できます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Matthew S Farmer\n",
    "> \n",
    "> このコードは競技のGPU上でメモリ不足にならずに実行されるのはなぜですか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > そうですね、私も心配でしたが、うまくいきました。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## i_am_nothing\n",
    "> \n",
    "> データソースである数字認識者を削除するにはどうすればよいですか？\n",
    "> \n",
    "> \n",
    "> > ## Chazzyie\n",
    "> > \n",
    "> > 3つの点をクリックして、数字認識者フォルダーを削除を選択できます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## Eslam Mohamed\n",
    "> \n",
    "> 非常に優れたプレゼンテーションです！明確さと詳細のレベルは最高です。\n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> 私はこの競技に参加したばかりの初心者です。\n",
    "> \n",
    "> 質問が基本すぎる場合はごめんなさい。\n",
    "> \n",
    "> このコードをcolabで実行しようとしていますが、モジュールや提出コードのために実行できません。このコードはKaggleプラットフォームでしか実行できませんか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic著者\n",
    "> > \n",
    "> > こんにちは、いいえ、Google Colabでは実行できません。モデルのローカルパスを使用しているため、Google Colabを使用する場合はHuggingfaceからモデルをダウンロードし、Kaggle環境パッケージをインストールしてください。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## philipha2\n",
    "> > > \n",
    "> > > 返信どうもありがとうございます！\n",
    "> > > \n",
    "> > > 通常、あなたはどのようにモデルを実行しますか？\n",
    "> > > \n",
    "> > > Kaggleのリソース（GPU）を使用しているのですか？それとも自分のGPUを使用しているのですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > ## kaoutarTopic著者\n",
    "> > > \n",
    "> > > 私は個人的なGPUを持っていません。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Krens\n",
    "> > > > \n",
    "> > > > > \n",
    "> > > > Kaggleのノートブック内の最初のセルのコードを実行すると、Kaggleノートブックの入力ファイルを自動的にダウンロードして正しいファイルパスを作成します。\n",
    "> > > > \n",
    "> > > > \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
