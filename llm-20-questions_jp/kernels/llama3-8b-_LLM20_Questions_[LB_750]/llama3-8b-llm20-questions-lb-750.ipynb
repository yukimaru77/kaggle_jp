{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7672a60d",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、「20の質問ゲーム」に参加するためのエージェントを構築するプロセスを示しています。このコンペティションでは、参加者が質問者、回答者、および推測者の役割を果たすAIエージェントを作ることが求められます。このノートブックは主にPythonプログラムを使用しており、特にPyTorchとTransformersライブラリを活用しています。\n",
    "\n",
    "## 問題に取り組む内容\n",
    "ノートブックは、与えられた環境でゲームを進行させるエージェントを開発することに焦点を当てています。エージェントは、ユーザーが考えている単語を特定するために、限られた質問回数内で「はい」または「いいえ」で答えられる質問を行います。エージェントの設計は、質問、推測、および回答の各モードに基づいています。\n",
    "\n",
    "## 使用している手法やライブラリ\n",
    "以下の技術が用いられています：\n",
    "- **Transformers**: 自然言語処理のためのモデルを構築するために使用されます。\n",
    "  - AutoTokenizerとAutoModelForCausalLMを使用して、事前に学習した言語モデルをロードし、質問生成と応答生成を行います。\n",
    "- **PyTorch**: モデルのトレーニングと計算をサポートするために用いられます。\n",
    "- **Kaggleの秘密情報管理**: シークレット情報を安全に扱うためのライブラリとして使用されています。\n",
    "\n",
    "また、コードは`main.py`にまとめられており、エージェントの動作を制御する`agent`関数が重要な役割を果たしています。この関数は、観察データ（obs）と設定データ（cfg）に基づいて、エージェントの動作を決定します。\n",
    "\n",
    "### 主要な機能\n",
    "- **質問者（asker）**: ユーザーに対して有効な質問を生成します。\n",
    "- **回答者（answerer）**: ユーザーの質問に「はい」または「いいえ」で答えます。\n",
    "- **推測者（guesser）**: 過去の質問と回答を基に、ターゲットの単語を推測します。\n",
    "\n",
    "このノートブックは、エージェントのロジックを簡潔に示し、対戦相手との相互作用を通じて学習させるための手法を提供しています。また、コードのローカルテストを実行する方法についても言及があり、Kaggle環境での動作を確認できるようになっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae8bf61d",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebook内の専門用語に関する簡単な解説です。初心者の方がつまずきそうな、あまり一般的でない用語や、このノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **エージェント (Agent)**:\n",
    "   - 20の質問ゲームにおいて、質問する役割を持つ AI モデルやプログラムのこと。ここでは、質問者、回答者、推測者の役割を持つエージェントが構築されます。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**:\n",
    "   - テキストを数値形式に変換するためのツール。自然言語処理では、文章を単語やサブワードに分割して、それぞれに対応するIDを割り当てます。モデルが理解できるようにするために必須です。\n",
    "\n",
    "3. **自動モデル (AutoModel)**:\n",
    "   - Hugging Face Transformersライブラリにおいて、特定のモデルを自動的に読み込むためのクラス。指定したモデルの種類に応じて、適切なアーキテクチャや設定が適用されます。\n",
    "\n",
    "4. **EOTトークン (End of Text Token)**:\n",
    "   - テキストデータの終端を示す特別なトークン。このトークンは、モデルに出力が終了したことを知らせるために使用され、生成したテキストをデコードする際に重要です。\n",
    "\n",
    "5. **CUDA**:\n",
    "   - NVIDIAの並列計算プラットフォームおよびAPIで、GPUを用いて計算を加速するための技術。PyTorchなどのディープラーニングフレームワークでは、GPUで効率的に処理を行う上で重要です。\n",
    "\n",
    "6. **メモリ効率 (Memory Efficient)**:\n",
    "   - GPUやCPUのメモリを少なく使用して計算を行う手法。このコンペでは、メモリ消費を最小限に抑えるための設定が行われています。\n",
    "\n",
    "7. **質問のターン (Turn)**:\n",
    "   - ゲームの進行におけるステージや段階のこと。質問者が質問をするターン、回答者が答えを返すターンなど、各プレイヤの役割に基づいて進行します。\n",
    "\n",
    "8. **観察 (Observation)**:\n",
    "   - ゲームの進行中にエージェントが受け取る情報のこと。これには、現在のターンの状況や過去の質問と回答が含まれます。\n",
    "\n",
    "9. **ランダム選択 (Random Selection)**:\n",
    "   - 同程度のスキルを持つボットが対戦する際に、相手を無作為に選ぶプロセス。これは、エージェントのパフォーマンスを公平に評価するために行われます。\n",
    "\n",
    "10. **ローカルテスト (Local Test)**:\n",
    "    - 自分の環境やPCでコードの動作を確認するためのテスト。このノートブックでは、Kaggleの環境ではなくローカルでエージェントをテストする方法が説明されています。\n",
    "\n",
    "11. **ファインチューニング (Fine-Tuning)**:\n",
    "    - 事前学習済みのモデルを特定のデータセットやタスクに合わせて再調整するプロセス。コンペの文脈では、最初からトレーニングするのではなく、既存のモデルを改良することが重要です。\n",
    "\n",
    "この解説が、初心者の方がノートブックの内容を理解する手助けとなることを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60d0fb",
   "metadata": {},
   "source": [
    "# このイントロダクションは初心者のためのものです\n",
    "## このコンペティションで必要なことは？\n",
    "\n",
    "**要するに：20の質問ゲームをプレイできるモデルを作成することです**\n",
    "\n",
    "あなたが話し、歩き、踊ることができるロボットを作らなければならないと想像してください。あなたの仕事は、そのロボットがそれぞれの役割を適切に果たすことを確認し、ユーザーが希望する役割を選んで実行できるようにすることです。たとえば、3つのボタンを追加することができます。\n",
    "\n",
    "これが、このコンペティションで期待されていることです。あなたは、質問者、回答者、推測者の3つの役割をプレイできるエージェント/ロボット（LLMモデル）を構築する必要があります。\n",
    "\n",
    "[こちら](https://www.kaggle.com/competitions/llm-20-questions/overview)の概要セクションを読んだことがあるなら、提出するエージェントは他の参加者が提出したエージェントと対戦することが述べられています。このデュオでは、あなたのエージェントは質問者と推測者の役割をアクティブにするか、回答者になることができます。役割とペアは、いくつかの条件に基づいて、裏で行われている環境によって選択されます。そのため、あなたはそれについて心配する必要はありません。\n",
    "\n",
    "あなたがするべきことは、環境が回答者の役割をプレイすることを決定した際に、あなたのエージェントが「はい」または「いいえ」と答えなければならないことを確認するだけです。その他の答えをすると、ゲームに負けてしまいます。\n",
    "\n",
    "詳しくは[こちら](https://www.kaggle.com/competitions/llm-20-questions/overview)をご覧ください。 \n",
    "\n",
    "\n",
    "## どのように提出するか？\n",
    "\n",
    "あなたのコードはmain.pyという1つのファイルにまとめる必要があります。このファイルにはエージェント/ロボットのコードと、obsとcfgというパラメータを受け取る必須関数が含まれている必要があります。この関数は、背後の環境によってエージェントを実行するために使用されます。\n",
    "\n",
    "私たちのコード（他のノートブックと比較して非常にシンプルです）では、この関数を「agent」と名付け、他のロジックを「robot」というクラスに置くことにします。\n",
    "\n",
    "環境はオフラインモードでコードを実行し、/kaggle/inputディレクトリにアクセスできないため、必要なパッケージとmain.pyファイルを1つのtar.gzファイルにロード/コピーする必要があります。\n",
    "\n",
    "裏で、tar.gzファイルは「/kaggle_simulations/agent/」フォルダの下に解凍されます。これが、実行環境に応じてパスを調整するコードを追加する理由です（下記参照）。\n",
    "\n",
    "ローカルでコードをテストすることができます。下記の[ローカルテスト](#lc)セクションを参照してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:30:07.460467Z",
     "iopub.status.busy": "2024-06-25T14:30:07.460168Z",
     "iopub.status.idle": "2024-06-25T14:30:07.484708Z",
     "shell.execute_reply": "2024-06-25T14:30:07.483728Z",
     "shell.execute_reply.started": "2024-06-25T14:30:07.460437Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# submissionというディレクトリを作成します\n",
    "# -pオプションは、親ディレクトリが存在しない場合にも作成することを意味します\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:26:54.32383Z",
     "iopub.status.busy": "2024-06-25T21:26:54.323407Z",
     "iopub.status.idle": "2024-06-25T21:30:25.229808Z",
     "shell.execute_reply": "2024-06-25T21:30:25.228768Z",
     "shell.execute_reply.started": "2024-06-25T21:26:54.323795Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "# transformersライブラリから必要なクラスをインポート\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch  # PyTorchをインポート\n",
    "from kaggle_secrets import UserSecretsClient  # Kaggleの秘密情報を扱うライブラリをインポート\n",
    "import os  # OS操作のためのライブラリをインポート\n",
    "import sys  # システム関連の操作を行うためのライブラリをインポート\n",
    "import shutil  # 高水準のファイル操作を行うためのライブラリをインポート\n",
    "\n",
    "# CUDAのメモリ効率を有効にする設定\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Kaggle環境のパスを設定\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")  # Kaggleエージェントのパスが存在する場合\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # 初期モデルパス\n",
    "\n",
    "# トークナイザーとモデルの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]  # EOTトークンのIDを取得\n",
    "\n",
    "# 回答を生成する関数\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")  # 入力をトークナイズしてCUDAデバイスに移す\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()  # モデルによる生成\n",
    "    start_gen = inp_ids.input_ids.shape[1]  # 入力の長さ\n",
    "    out_ids = out_ids[start_gen:]  # 生成されたトークンの取得\n",
    "    if id_eot in out_ids:  # EOTトークンがあれば\n",
    "        stop = out_ids.tolist().index(id_eot)  # EOTトークンの位置を特定\n",
    "        out = tokenizer.decode(out_ids[:stop])  # トークンをデコード\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)  # トークンをデコード\n",
    "    return out  # 生成された回答を返す\n",
    "\n",
    "# Robotクラス\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        # 有効なモードであることを確認\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問者の役割を開始\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 答え手の役割を開始\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"  # \"yes\"と返す\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"  # \"no\"と返す\n",
    "            if (\"yes\" not in output.lower() and \"no\" not in output.lower()):\n",
    "                output = \"yes\"  # 有効な回答でない場合はデフォルトで\"yes\"を返す\n",
    "        if mode == \"guessing\":\n",
    "            # 推測者の役割を開始\n",
    "            output = self.asker(obs)\n",
    "        return output  # 生成された出力を返す\n",
    "    \n",
    "    # 質問者の役割\n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"あなたは役立つAIアシスタントであり、20の質問ゲームが得意です。\n",
    "        ユーザーが考えている単語は、以下の3つのカテゴリのいずれかである必要があります：\n",
    "        1. 場所\n",
    "        2. 人\n",
    "        3. 物\n",
    "        したがって、これらの選択肢に焦点を合わせて質問を考え、検索範囲を狭める賢い質問をしてください\\n\"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":  # 質問するターン\n",
    "            ask_prompt = sys_prompt + \"\"\"あなたの役割は、最大20の質問で単語を見つけることです。あなたの質問は有効であるためには「はい」または「いいえ」で答えられる必要があります。\n",
    "            これを助けるために、キーワードがモロッコだと仮定した場合の例を示します：\n",
    "            例：\n",
    "            <あなた: それは場所ですか？\n",
    "            ユーザー: はい\n",
    "            あなた: ヨーロッパにありますか？\n",
    "            ユーザー: いいえ\n",
    "            あなた: アフリカにありますか？\n",
    "            ユーザー: はい\n",
    "            あなた: そこに住んでいる人々の多くは肌の色が暗いですか？\n",
    "            ユーザー: いいえ\n",
    "            ユーザー: mで始まる国名ですか？\n",
    "            あなた: はい\n",
    "            あなた: モロッコですか？\n",
    "            ユーザー: はい。>\"\"\"\n",
    "\n",
    "            # ユーザーが単語を選んだら、最初の質問をしてください！\n",
    "            ask_prompt += \"ユーザーが単語を選んだので、最初の質問をしてください！短く、冗長にならないでください。1つの質問だけを提示してください。\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            # これまでの質問と回答があればそれを追加\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":  # 推測するターン\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"  # 会話履歴を収集\n",
    "            guess_prompt =  sys_prompt + f\"\"\"これまでのゲームの状況は以下の通りです:\\n{conv}\n",
    "            この会話に基づいて、単語を推測してください。単語だけを返してください、冗長な回答は不要です。\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output  # 生成された質問や推測を返す\n",
    "        \n",
    "    # 答え手の役割\n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"あなたは役立つAIアシスタントであり、20の質問ゲームが得意です。\n",
    "        ユーザーの役割は、最大20の質問で単語を推測することです。あなたの回答は「はい」または「いいえ」でなければなりません。他の回答は無効であり、あなたはゲームに負けてしまいます。\n",
    "        ユーザーが推測する単語は常に以下の3つのカテゴリのいずれかに属します：\n",
    "        1. 場所\n",
    "        2. 人\n",
    "        3. 物\n",
    "        したがって、ユーザーの質問を理解し、どのキーワードで遊んでいるかを理解していることを確認してください。\n",
    "        現在、ユーザーが推測すべき単語は: \"{obs.keyword}\"で、カテゴリは\"{obs.category}\"です。\n",
    "        これを助けるために、キーワードがモロッコでありカテゴリが「場所」である場合の例を示します：\n",
    "        例：\n",
    "        <ユーザー: それは場所ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: ヨーロッパにありますか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: アフリカにありますか？\n",
    "        あなた: はい\n",
    "        ユーザー: そこに住んでいる人々の多くは肌の色が暗いですか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: mで始まる国名ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: モロッコですか？\n",
    "        あなた: はい。>\"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"  # 最初の質問を追加\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:  # 以前の回答があれば追加\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)  # 回答を生成\n",
    "        return output  # 生成された回答を返す\n",
    "    \n",
    "robot = Robot()  # Robotのインスタンスを生成\n",
    "\n",
    "# エージェント関数\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    # 現在のターンの種類に応じて処理を分ける\n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    # 返答が無い場合や長さが1以下の場合はデフォルトで\"yes\"を返す\n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response  # 答えを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c5f174",
   "metadata": {},
   "source": [
    "# ローカルテスト <a id=\"lc\"></a>\n",
    "\n",
    "ローカルでテストするには、まず上のセルの「%%writefile -a submission/main.py」をコメントアウトし、そのセルを実行します。\n",
    "次に、以下のセルのコメントを解除して実行してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:31:20.53805Z",
     "iopub.status.busy": "2024-06-25T21:31:20.537013Z",
     "iopub.status.idle": "2024-06-25T21:31:21.897103Z",
     "shell.execute_reply": "2024-06-25T21:31:21.896084Z",
     "shell.execute_reply.started": "2024-06-25T21:31:20.538008Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# kaggle_environmentsライブラリから環境を作成するための関数をインポート\n",
    "# \"llm_20_questions\"環境を作成し、デバッグモードを有効にします\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "# エージェントを4つ使ってゲームを実行し、その出力を取得します\n",
    "# game_output = env.run(agents=[agent, agent, agent, agent])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:42:24.806653Z",
     "iopub.status.busy": "2024-06-25T21:42:24.805663Z",
     "iopub.status.idle": "2024-06-25T21:42:24.873565Z",
     "shell.execute_reply": "2024-06-25T21:42:24.872711Z",
     "shell.execute_reply.started": "2024-06-25T21:42:24.806602Z"
    }
   },
   "outputs": [],
   "source": [
    "# 環境の結果をIPythonモードで表示します\n",
    "# 表示の幅を600ピクセル、高さを500ピクセルに設定します\n",
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16518e99",
   "metadata": {},
   "source": [
    "# 提出ファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:14:28.205431Z",
     "iopub.status.busy": "2024-06-25T16:14:28.204763Z",
     "iopub.status.idle": "2024-06-25T16:14:31.046479Z",
     "shell.execute_reply": "2024-06-25T16:14:31.045488Z",
     "shell.execute_reply.started": "2024-06-25T16:14:28.205393Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvを非表示でインストールします\n",
    "# pigzは圧縮を、pvはデータの進行状況を表示するためのツールです\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:14:47.111039Z",
     "iopub.status.busy": "2024-06-25T16:14:47.110649Z",
     "iopub.status.idle": "2024-06-25T16:20:02.504823Z",
     "shell.execute_reply": "2024-06-25T16:20:02.503606Z",
     "shell.execute_reply.started": "2024-06-25T16:14:47.110992Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzを使って、submission.tar.gzという名前でファイルを圧縮します\n",
    "# -Cオプションは指定されたディレクトリに移動してから処理を行います\n",
    "# まず/kaggle/input/llama-3/transformers/8b-chat-hfから全ファイルを圧縮し、その後/kaggle/working/submissionからも全ファイルを圧縮します\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/llama-3/transformers/8b-chat-hf . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T16:20:02.507126Z",
     "iopub.status.busy": "2024-06-25T16:20:02.506741Z",
     "iopub.status.idle": "2024-06-25T16:22:51.972507Z",
     "shell.execute_reply": "2024-06-25T16:22:51.971476Z",
     "shell.execute_reply.started": "2024-06-25T16:20:02.507093Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# tar.gzファイルの中身を確認するためのコードです\n",
    "\n",
    "# tarfileライブラリをインポート\n",
    "# import tarfile\n",
    "# tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")  # tar.gzファイルをオープン\n",
    "# tar.getmembers()でファイルのメンバーを取得し、それぞれのファイル名を表示します\n",
    "# for file in tar.getmembers():\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb53a9",
   "metadata": {},
   "source": [
    "# コメント\n",
    "\n",
    "> ## davide\n",
    "> \n",
    "> すばらしい作業ですね、共有してくれてありがとう！\n",
    "> \n",
    "> ちょっと質問ですが、このコードはキーワード.pyファイルを全く使用していないのですか？\n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > [@davidemariani](https://www.kaggle.com/davidemariani) いいえ\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## dhruvyadav89300\n",
    "> \n",
    "> ノートブックの説明をありがとう、とても助かりました。\n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > それを聞けて嬉しいです :)\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Chazzyie\n",
    "> \n",
    "> あなたの作品を共有してくれてありがとう、シンプルで明確です🤩\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Matthew S Farmer\n",
    "> \n",
    "> シンプルで良いノートブックですね。ちょっとお知らせですが、「人」というカテゴリはコンペティションから削除されました。これがあなたのエージェントがキーワードの検索空間に留まるのに役立つかもしれません。\n",
    "> \n",
    "> \n",
    "> > ## Matthew S Farmer\n",
    "> > \n",
    "> > [https://www.kaggle.com/competitions/llm-20-questions/discussion/512955#2884981](https://www.kaggle.com/competitions/llm-20-questions/discussion/512955#2884981)\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > ありがとうございます、そのことは知りませんでした！\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## BK\n",
    "> \n",
    "> 共有してくれてありがとう！あなたのノートブックが大好きです :)\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Eslam Mohamed\n",
    "> \n",
    "> 素晴らしい仕事です！説明の明瞭さはトップレベルです。\n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > それを聞けて嬉しいです！\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## 古月方源\n",
    "> \n",
    "> ローカルテストは成功しました\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## 古月方源\n",
    "> \n",
    "> 最新の3.1を正常に実行しましたが、提出後にエラーが発生していました\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Sandeep Sharma\n",
    "> \n",
    "> ノートブックをありがとう。しかし、テストしようとすると、「1つの位置引数 'cfg' が欠落しています」と表示されます。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Toshi\n",
    "> \n",
    "> 良いノートブックですね。\n",
    "> \n",
    "> ローカルでテストする場合、どのバージョンのPyTorchをインストールすればよいですか？\n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > [@mst923](https://www.kaggle.com/mst923) あなたの言う「ローカル」とは自分のコンピュータのことですか？良いGPUがない限り、動作しません。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## kothiwsk28\n",
    "> \n",
    "> 共有してくれてありがとう！ノートブックにこの3つのデータセット（数字認識、タイタニック、ハウスプライス）を置いている特別な理由はありますか？\n",
    "> \n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > いいえ、削除した方がいいです。初心者がこれらのコンペティションページに訪れることを助けるために追加しました。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Mask\n",
    "> \n",
    "> 役立つノートブックです。\n",
    "> \n",
    "> Kaggle初心者で、これをコピー/フォークしたとき、提出ボタンが見つかりませんでした！ノートブックに他の競技（数字認識やタイタニック）が付随しています。また、Llamaを使用して完全なノートブックを作成し、そこにコードをコピーしようとしたとき、モデルパスが正しくないというエラーが出ましたが、実際には正しいです！\n",
    "> \n",
    "> 何が問題か理解できません！\n",
    "> \n",
    "> あなたのノートブックをフォークして、エラーなしで提出する方法を教えてもらえますか？私はここでは新しいので、うまくいくか見たいです。\n",
    "> \n",
    "> 提出ボタンが見当たらないのは奇妙で、モデルディレクトリが間違っていることが示されますが、実際にはそうではありません！\n",
    "> \n",
    "> > ## Maoshenli\n",
    "> > \n",
    "> > 私も同じ問題に遭遇しました。それを解決しましたか？😀\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > ## jehlum\n",
    "> > \n",
    "> > 右側の入力タブからモデルを追加しましたか？\n",
    "> > \n",
    "> > \n",
    "> > > ## kaoutarTopic Author\n",
    "> > \n",
    "> > > 皆さん、Llamaモデル使用の条件に同意し、使用を許可されていることを確認してください。この問題を解決し、ノートブックが正常に動作すれば、右上の3つのドットをクリックして「コンペティションに提出」を選択してください。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Lucas\n",
    "> \n",
    "> コードをありがとうございます。提出しましたが、LBで600点しか獲得できませんでした。プレトレーニングされたllama3 8bを使用しました。750を取得するにはどうすればいいですか？llama3 8bをファインチューニングしましたか？\n",
    "> \n",
    "> > ## Krens\n",
    "> > \n",
    "> > ゲームを提出したとき、初期スコアは600ポイントでした。このスコアは、他の参加者のエージェントとゲームをプレイして初めて変わります。また、分散が比較的大きく、あまり安定していません。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## zerojin\n",
    "> \n",
    "> ご自身の作品を共有してくださり、ありがとうございます。質問があります。使用しているllama3モデルはファインチューニングされていますか？\n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > 私のデータでファインチューニングされたのですか？答えはいいえです。Kaggleハブにあるローカルモデルのパスを使用していることがわかります。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## tingyu516\n",
    "> \n",
    "> こんにちは、このコードを提出するときに「検証エピソードが失敗しました」というエラーメッセージが表示されました。どうすればよいですか？\n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > エージェントのログをダウンロードし、何が起こったのかを確認できます。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Matthew S Farmer\n",
    "> \n",
    "> このコードは、競技用GPUでメモリ不足にならずに実行できますか？\n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > はい、私も心配でしたが、うまくいきました。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## i_am_nothing\n",
    "> \n",
    "> データソースの数字認識を削除する方法は？\n",
    "> \n",
    "> > ## Chazzyie\n",
    "> > \n",
    "> > 三つのドットをクリックし、数字認識フォルダの削除を選択できます。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Eslam Mohamed\n",
    "> \n",
    "> 非常に優れたプレゼンテーション！明晰さと詳細レベルはトップクラスです。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> このコンペティションでは初心者です。\n",
    "> \n",
    "> 質問が基本的すぎたらごめんなさい。\n",
    "> \n",
    "> このコードをColabで実行しようとしていますが、一部のモジュールと提出コードのために実行できません。これはKaggleプラットフォームでのみ実行可能ですか？\n",
    "> \n",
    "> > ## kaoutarTopic Author\n",
    "> > \n",
    "> > こんにちは、Google Colabでは実行できません。なぜなら、Llamaモデルのローカルパスを使用しているからです。Google Colabを使用したい場合は、Hugging Faceからモデルをダウンロードし、Kaggle envパッケージをインストールする必要があります。\n",
    "> > \n",
    "> > \n",
    "> > > ## philipha2\n",
    "> > > \n",
    "> > > お気遣いの返信をありがとうございます！\n",
    "> > > \n",
    "> > > 通常、モデルはどのように実行しますか？\n",
    "> > > \n",
    "> > > あなたはKaggleのリソース（GPU）のみを使用していますか？それとも自分のGPUを使用していますか？\n",
    "> > > \n",
    "> > > > ## kaoutarTopic Author\n",
    "> > > > \n",
    "> > > > 個人のGPUは持っていません。\n",
    "> > > > \n",
    "> > > > ## Krens\n",
    "> > > > \n",
    "> > > > \n",
    "> > > > このオプションを選択してからColabの最初のセルでコードを実行すると、Kaggleノートブックの入力ファイルを自動的にダウンロードし、正しいファイルパス（/kaggle/input/...）を作成できます。\n",
    "> > > > \n",
    "> > > > \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 861823,
     "sourceId": 3004,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 26502,
     "sourceId": 3136,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 868283,
     "sourceId": 5407,
     "sourceType": "competition"
    },
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
