{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99c2698e",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "以下は、与えられたJupyter Notebookの要約です。\n",
    "\n",
    "このノートブックは、Kaggleの「20の質問」ゲームをプレイするためのAIエージェントを構築することを目的としています。主に、動的に生成される質問と推測を通じてターゲットワードを当てることを目指し、Gemmaモデルを活用しています。\n",
    "\n",
    "### 問題\n",
    "ノートブックは、「20の質問」ゲームにおいて、AIエージェントがユーザーに対して質問をし、回答を元にターゲットワードを推測するプロセスを自動化し、効率的に行うためのシステムを構築するという課題に取り組んでいます。\n",
    "\n",
    "### 手法およびライブラリ\n",
    "1. **環境セットアップ**:\n",
    "   - `immutabledict`、`sentencepiece`、`kaggle_environments`といった必要なライブラリをインストール。\n",
    "   - GitHubから`gemma_pytorch`リポジトリをクローンし、必要なファイルを作業ディレクトリに移動します。\n",
    "\n",
    "2. **Gemmaモデルの使用**:\n",
    "   - Gemmaモデルは、因果言語モデルとして使用され、その設定やトークナイザーをインポート。\n",
    "   - モデルの重みをロードし、ターンごとに質問および回答を処理するための構造を設計しています。\n",
    "\n",
    "3. **フォーマッタークラスの定義**:\n",
    "   - `GemmaFormatter`クラスにより、ユーザーとモデルのターンを管理し、システムプロンプトや少数ショットの例をもとにインタラクティブなセッションを形成します。\n",
    "\n",
    "4. **質問・推測の解析**:\n",
    "   - `_parse_response`や`_parse_keyword`といった関数を定義し、AIが生成したレスポンスから質問や推測を適切に抽出し処理しています。\n",
    "\n",
    "5. **モデルの生成および実行**:\n",
    "   - 実際にいくつかのGemmaモデル（\"2b\"、\"2b-it\"、\"7b-it-quant\"など）をロードし、生成したプロンプトに基づいてレスポンスを取得。\n",
    "   - モデルによる生成結果を解析し、最終結果を表示するための処理を行なっています。\n",
    "\n",
    "このノートブックは、効率的な情報収集や推測によって「20の質問」ゲームの戦略を実行するためのAIエージェントの設計と実装に役立てられています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f793258",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提示されたJupyter Notebookに関連する専門用語の簡単な解説です。初心者にとって馴染みの薄いものや、このノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **immutabledict**\n",
    "   - 変更できない辞書型のデータ構造を提供するPythonのパッケージです。一度作成した内容を変更できないため、データの保護やハッシュ可能な値としての利用に役立ちます。\n",
    "\n",
    "2. **sentencepiece**\n",
    "   - 自然言語処理で用いられるサブワードトークナイザーです。特にニューラルネットワークモデルの入力データ準備に使われ、トークン化の際に語彙サイズを減少させ、より柔軟なモデルの学習を可能にします。\n",
    "\n",
    "3. **kaggle_environments**\n",
    "   - KaggleのAI環境をシミュレートするためのライブラリで、エージェント間の競争を含むさまざまな環境を構築する助けになります。このゲームでは、エージェントが相互に作用し、対戦形式で実行されます。\n",
    "\n",
    "4. **contextlib**\n",
    "   - Pythonの標準ライブラリの一部で、コンテキストマネージャーを使用するためのユーティリティが含まれています。`with`ステートメントとともに使用され、リソース管理（開放やクローズ）を簡素化します。\n",
    "\n",
    "5. **Causal LM (Causal Language Model)**\n",
    "   - 因果関係を持った言語モデルで、次に予測される単語の確率を前の単語のみに基づいて計算するモデルです。一般に、生成タスクに使用され、テキスト生成や対話システムに適しています。\n",
    "\n",
    "6. **Tokenization**\n",
    "   - テキストを小さな単位（トークン）に分割するプロセスです。これにより、モデルが理解し処理しやすい形式になります。トークンは通常、単語、サブワード、さらには文字レベルかもしれません。\n",
    "\n",
    "7. **温度 (Temperature)**\n",
    "   - 確率的サンプリングのパラメータであり、出力の多様性を制御します。低い温度はより決定的な出力を生成し、高い温度はランダム性を増やし、さまざまな出力を引き出します。\n",
    "\n",
    "8. **Top-k Sampling**\n",
    "   - モデルが予測したトークンの中から上位k個の選択肢を考慮して、ランダムに選択するサンプリング手法です。これにより、出力結果の多様性が向上します。\n",
    "\n",
    "9. **Top-p Sampling (Nucleus Sampling)**\n",
    "   - 予測された確率分布から、累積確率がpを超える最小のトークンセットを選び、その中からランダムに選ぶ手法です。より自然なテキスト生成が可能です。\n",
    "\n",
    "10. **Quantization (量子化)**\n",
    "    - モデルのパラメータを小さなデータ型に変換するプロセスです。これにより、メモリの使用量が減り、推論速度が向上します。特にデバイスの制約がある場合に有効です。\n",
    "\n",
    "11. **Iterable (イテラブル)**\n",
    "    - 繰り返し処理が可能なオブジェクトのことです。リストやタプルのようなコレクションや、カスタムオブジェクトを含みます。Pythonでは`__iter__`メソッドを持つオブジェクトはイテラブルと見なされます。\n",
    "\n",
    "12. **Context Manager (コンテキストマネージャ)**\n",
    "    - リソースを管理するための構文で、リソース（ファイル、ネットワークコネクションなど）の開放を自動的に行います。`with`文を使用して、リソースの利用範囲を明確に定義できます。\n",
    "\n",
    "以上の用語は、Jupyter Notebookの理解を深めるために役立つはずです。特に、機械学習や深層学習においては、これらの概念をしっかり把握することが重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:46:55.794817Z",
     "iopub.status.busy": "2024-06-28T13:46:55.794466Z",
     "iopub.status.idle": "2024-06-28T13:47:25.200385Z",
     "shell.execute_reply": "2024-06-28T13:47:25.199204Z",
     "shell.execute_reply.started": "2024-06-28T13:46:55.794784Z"
    }
   },
   "outputs": [],
   "source": [
    "# 環境をセットアップする\n",
    "# immutabledict と sentencepiece パッケージをインストールします\n",
    "!pip install -q -U immutabledict sentencepiece \n",
    "# kaggle_environmentsの必要なバージョンをインストールします\n",
    "!pip install -q 'kaggle_environments>=1.14.8'\n",
    "# GitHubからgemma_pytorchリポジトリをクローンします\n",
    "!git clone https://github.com/google/gemma_pytorch.git\n",
    "# 作業用のgemmaディレクトリを作成します\n",
    "!mkdir /kaggle/working/gemma/\n",
    "# クローンしたgemma_pytorchからgemmaフォルダ内にファイルを移動します\n",
    "!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:25.203258Z",
     "iopub.status.busy": "2024-06-28T13:47:25.202653Z",
     "iopub.status.idle": "2024-06-28T13:47:31.779874Z",
     "shell.execute_reply": "2024-06-28T13:47:31.779106Z",
     "shell.execute_reply.started": "2024-06-28T13:47:25.203217Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なモジュールをインポートします\n",
    "import sys \n",
    "# gemma_pytorchのパスをシステムパスに追加します\n",
    "sys.path.append(\"/kaggle/working/gemma_pytorch/\") \n",
    "import contextlib\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import kaggle_environments\n",
    "import itertools\n",
    "# Gemmaの設定をインポートします\n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "# Gemmaモデルをインポートします\n",
    "from gemma.model import GemmaForCausalLM\n",
    "# トークナイザーをインポートします\n",
    "from gemma.tokenizer import Tokenizer\n",
    "# Iterable型をインポートします\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.781426Z",
     "iopub.status.busy": "2024-06-28T13:47:31.781003Z",
     "iopub.status.idle": "2024-06-28T13:47:31.791789Z",
     "shell.execute_reply": "2024-06-28T13:47:31.790918Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.7814Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaFormatter:\n",
    "    # ターンの開始と終了を示すトークン\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        # システムプロンプトと少数ショットの例をセットします\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        # ユーザーとモデルのターンのフォーマットを定義します\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()  # 状態をリセットします\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 現在の状態を返します\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        # ユーザーのプロンプトを状態に追加します\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        # モデルのプロンプトを状態に追加します\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        # ユーザーのターンを開始します\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        # モデルのターンを開始します\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        # 現在のターンを終了します\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        # 状態を初期化します\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  # システムプロンプトがあれば追加します\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # 少数ショットの例を適用します\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # ターンを適用し、どのエージェントから始まるかによってフォーマッタを決定します\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)  # フォーマッタを循環させます\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)  # フォーマッタを使用してプロンプトを追加します\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.794995Z",
     "iopub.status.busy": "2024-06-28T13:47:31.794426Z",
     "iopub.status.idle": "2024-06-28T13:47:31.808707Z",
     "shell.execute_reply": "2024-06-28T13:47:31.80787Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.794963Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(VARIANT, device):\n",
    "    # ウェイトファイルのパスを設定します\n",
    "    WEIGHTS_PATH = f'/kaggle/input/gemma/pytorch/{VARIANT}/2' \n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(dtype: torch.dtype):\n",
    "        \"\"\"デフォルトのtorchデータ型を指定されたdtypeに設定します。\"\"\"\n",
    "        torch.set_default_dtype(dtype)  # デフォルト型を設定\n",
    "        yield  # 処理を進める\n",
    "        torch.set_default_dtype(torch.float)  # デフォルト型をfloatに戻す\n",
    "\n",
    "    # モデルの設定を定義します。\n",
    "    model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()  # バリアントに応じた設定を取得\n",
    "    model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザーのパスを設定\n",
    "    model_config.quant = \"quant\" in VARIANT  # 量子化の設定\n",
    "\n",
    "    # モデルのインスタンス化\n",
    "    with _set_default_tensor_type(model_config.get_dtype()):  # 指定されたdtypeに設定した状態でモデルを生成\n",
    "        model = GemmaForCausalLM(model_config)  # Gemmaモデルを初期化\n",
    "        ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{VARIANT}.ckpt')  # チェックポイントのパスを設定\n",
    "        model.load_weights(ckpt_path)  # ウェイトをロード\n",
    "        model = model.to(device).eval()  # 指定されたデバイスにモデルを移動し、評価モードに設定\n",
    "        \n",
    "    return model  # モデルを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.809865Z",
     "iopub.status.busy": "2024-06-28T13:47:31.809558Z",
     "iopub.status.idle": "2024-06-28T13:47:31.821925Z",
     "shell.execute_reply": "2024-06-28T13:47:31.821138Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.809814Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_keyword(response: str):\n",
    "    # レスポンスからキーワードを抽出します\n",
    "    match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # **に囲まれた部分を検索\n",
    "    if match is None:\n",
    "        keyword = ''  # マッチがなければ空の文字列を設定\n",
    "    else:\n",
    "        keyword = match.group().lower()  # マッチした部分を小文字に変換\n",
    "    return keyword  # 抽出したキーワードを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.823345Z",
     "iopub.status.busy": "2024-06-28T13:47:31.823021Z",
     "iopub.status.idle": "2024-06-28T13:47:31.832614Z",
     "shell.execute_reply": "2024-06-28T13:47:31.831874Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.823315Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_response(response: str, obs: dict):\n",
    "    # レスポンスと観測データに基づいて質問または推測を解析します\n",
    "    if obs['turnType'] == 'ask':  # ターンの種類が'ask'の場合\n",
    "        # レスポンスから最初の質問文を抽出します\n",
    "        match = re.search(\".+?\\?\", response.replace('*', ''))  # アスタリスクを削除して質問を検索\n",
    "        if match is None:\n",
    "            question = \"Is it a person?\"  # 質問が見つからない場合のデフォルト質問\n",
    "        else:\n",
    "            question = match.group()  # マッチした質問文を取得\n",
    "        return question  # 質問を返す\n",
    "    elif obs['turnType'] == 'guess':  # ターンの種類が'guess'の場合\n",
    "        guess = _parse_keyword(response)  # キーワードを解析して取得\n",
    "        return guess  # 推測を返す\n",
    "    else:\n",
    "        # 未知のターンタイプの場合はエラーを発生させます\n",
    "        raise ValueError(\"Unknown turn type:\", obs['turnType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.834217Z",
     "iopub.status.busy": "2024-06-28T13:47:31.83377Z",
     "iopub.status.idle": "2024-06-28T13:47:31.842001Z",
     "shell.execute_reply": "2024-06-28T13:47:31.841138Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.834187Z"
    }
   },
   "outputs": [],
   "source": [
    "def interleave_unequal(x, y):\n",
    "    # 2つのイテラブルxとyを交互に組み合わせます\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y)  # xとyを長さの異なる部分も考慮してペアにする\n",
    "        for item in pair if item is not None  # Noneでないアイテムのみを抽出\n",
    "    ]  # 交互に組み合わせたリストを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.843806Z",
     "iopub.status.busy": "2024-06-28T13:47:31.843109Z",
     "iopub.status.idle": "2024-06-28T13:47:31.851451Z",
     "shell.execute_reply": "2024-06-28T13:47:31.850649Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.843774Z"
    }
   },
   "outputs": [],
   "source": [
    "# システムプロンプトを設定します\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "# 少数ショットの例を定義します\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",  # ゲームの開始を促すメッセージ\n",
    "    \"Is it a thing?\", \"**no**\",  # 質問とその回答\n",
    "    \"Is it a country?\", \"**yes**\",  # 質問とその回答\n",
    "    \"Is it Europe?\", \"**yes** Now guess the keyword.\",  # 質問に対する回答と推測への移行\n",
    "    \"**France**\", \"Correct!\",  # 推測とその結果\n",
    "]\n",
    "\n",
    "# GemmaFormatterのインスタンスを作成します\n",
    "formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.852794Z",
     "iopub.status.busy": "2024-06-28T13:47:31.85249Z",
     "iopub.status.idle": "2024-06-28T13:47:31.864509Z",
     "shell.execute_reply": "2024-06-28T13:47:31.863739Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.852772Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観測データを定義します\n",
    "obs = {\n",
    "    'turnType': 'ask',  # 現在のターンの種類は'ask'（質問）であることを示します\n",
    "    'questions': [  # 質問のリスト\n",
    "        'Is it a living entity?',  # 質問1\n",
    "        'Is it man-made?',  # 質問2\n",
    "        'Can it be held in a single hand?'  # 質問3\n",
    "    ],\n",
    "    'answers': [  # 回答のリスト\n",
    "        'no',  # 質問1に対する回答\n",
    "        'yes',  # 質問2に対する回答\n",
    "        'yes'  # 質問3に対する回答\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.868247Z",
     "iopub.status.busy": "2024-06-28T13:47:31.867968Z",
     "iopub.status.idle": "2024-06-28T13:47:31.877294Z",
     "shell.execute_reply": "2024-06-28T13:47:31.876427Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.868204Z"
    }
   },
   "outputs": [],
   "source": [
    "# フォーマッターをリセットします\n",
    "formatter.reset()\n",
    "# ゲームの開始をユーザーに伝えます\n",
    "formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "# 質問と回答を交互に組み合わせます\n",
    "turns = interleave_unequal(obs['questions'], obs['answers'])\n",
    "# ターンを適用します。モデルから開始することを指定します\n",
    "formatter.apply_turns(turns, start_agent='model')\n",
    "# ターンの種類に応じて、適切なプロンプトを追加します\n",
    "if obs['turnType'] == 'ask':\n",
    "    formatter.user(\"Please ask a yes-or-no question.\")  # 質問を促すメッセージ\n",
    "elif obs['turnType'] == 'guess':\n",
    "    formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")  # キーワードの推測を促します\n",
    "# モデルのターンを開始します\n",
    "formatter.start_model_turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.87862Z",
     "iopub.status.busy": "2024-06-28T13:47:31.878349Z",
     "iopub.status.idle": "2024-06-28T13:47:31.887707Z",
     "shell.execute_reply": "2024-06-28T13:47:31.886764Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.878596Z"
    }
   },
   "outputs": [],
   "source": [
    "# フォーマッターの状態を文字列に変換します\n",
    "prompt = str(formatter)\n",
    "prompt  # 生成したプロンプトを表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.890731Z",
     "iopub.status.busy": "2024-06-28T13:47:31.888869Z",
     "iopub.status.idle": "2024-06-28T13:47:31.895711Z",
     "shell.execute_reply": "2024-06-28T13:47:31.895002Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.890706Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用するマシンタイプを設定します（GPUを指定）\n",
    "MACHINE_TYPE = \"cuda\" \n",
    "# 指定したマシンタイプに基づいてデバイスを作成します\n",
    "device = torch.device(MACHINE_TYPE)\n",
    "# 生成するテキストの最大トークン数を設定します\n",
    "max_new_tokens = 32\n",
    "# サンプリングのパラメータを設定します\n",
    "sampler_kwargs = {\n",
    "    'temperature': 0.01,  # 出力の温度（ランダムさを制御）\n",
    "    'top_p': 0.1,  # 確率的サンプリングのパラメータ\n",
    "    'top_k': 1,  # 上位k個のトークンから選択する\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573a3e4b",
   "metadata": {},
   "source": [
    "# Gemma 2b V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.897079Z",
     "iopub.status.busy": "2024-06-28T13:47:31.896743Z",
     "iopub.status.idle": "2024-06-28T13:48:09.346136Z",
     "shell.execute_reply": "2024-06-28T13:48:09.345216Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.897046Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルをロードします\n",
    "model = load_model(\"2b\", device)\n",
    "\n",
    "# プロンプトに基づいてレスポンスを生成します\n",
    "response = model.generate(\n",
    "    prompt,  # 入力プロンプト\n",
    "    device=device,  # 使用するデバイス\n",
    "    output_len=max_new_tokens,  # 出力するトークンの最大数\n",
    "    **sampler_kwargs  # サンプリングのパラメータを展開して渡します\n",
    ")\n",
    "# 生成されたレスポンスを解析します\n",
    "response = _parse_response(response, obs)\n",
    "# 解析したレスポンスを出力します\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e89e03",
   "metadata": {},
   "source": [
    "# Gemma 2b-it V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:48:09.347623Z",
     "iopub.status.busy": "2024-06-28T13:48:09.34733Z",
     "iopub.status.idle": "2024-06-28T13:48:44.807608Z",
     "shell.execute_reply": "2024-06-28T13:48:44.806732Z",
     "shell.execute_reply.started": "2024-06-28T13:48:09.347597Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"2b-it\"モデルをロードします\n",
    "model = load_model(\"2b-it\", device)\n",
    "\n",
    "# プロンプトに基づいてレスポンスを生成します\n",
    "response = model.generate(\n",
    "    prompt,  # 入力プロンプト\n",
    "    device=device,  # 使用するデバイス\n",
    "    output_len=max_new_tokens,  # 出力するトークンの最大数\n",
    "    **sampler_kwargs  # サンプリングのパラメータを展開して渡します\n",
    ")\n",
    "# 生成されたレスポンスを解析します\n",
    "response = _parse_response(response, obs)\n",
    "# 解析したレスポンスを出力します\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b2667c",
   "metadata": {},
   "source": [
    "# Gemma 7b-it-quant V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:48:44.809002Z",
     "iopub.status.busy": "2024-06-28T13:48:44.808712Z",
     "iopub.status.idle": "2024-06-28T13:49:57.08955Z",
     "shell.execute_reply": "2024-06-28T13:49:57.08861Z",
     "shell.execute_reply.started": "2024-06-28T13:48:44.808978Z"
    }
   },
   "outputs": [],
   "source": [
    "# \"7b-it-quant\"モデルをロードします\n",
    "model = load_model(\"7b-it-quant\", device)\n",
    "\n",
    "# プロンプトに基づいてレスポンスを生成します\n",
    "response = model.generate(\n",
    "    prompt,  # 入力プロンプト\n",
    "    device=device,  # 使用するデバイス\n",
    "    output_len=max_new_tokens,  # 出力するトークンの最大数\n",
    "    **sampler_kwargs  # サンプリングのパラメータを展開して渡します\n",
    ")\n",
    "# 生成されたレスポンスを解析します\n",
    "response = _parse_response(response, obs)\n",
    "# 解析したレスポンスを出力します\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0358c5f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## dedq\n",
    "> \n",
    "> コードが明確で、動作が速いです。Kaggleのメンバーに最高の仕事を共有してくれてありがとう！\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5305,
     "sourceId": 11357,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
