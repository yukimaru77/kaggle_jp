{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfb5b79",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、「20の質問」ゲームに挑戦するための言語モデルの開発に関連しており、特定のキーワードを当てるための質問と推測を行うエージェントを構築しています。以下に、ノートブックの主な内容を要約します。\n",
    "\n",
    "### 問題の取り組み\n",
    "ノートブックは、言語モデルを使用して、質問応答形式で特定のキーワードを推測するAIエージェントを構築することを目的としています。特に、ユーザーが設定したカテゴリや特定の特徴に基づいて、質問を生成し、それに対する応答を取得して推測を行います。このプロジェクトは、Kaggleの「LLM 20 Questions」コンペティションに参加するために設計されています。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **データ処理**:\n",
    "   - `pandas`: データ操作を行い、キーワードデータを管理します。\n",
    "   - JSON形式のキーワードデータを辞書として読み込み、DataFrame形式に変換しています。\n",
    "\n",
    "2. **ビジュアライゼーション**:\n",
    "   - `matplotlib`: クエスチョンやアンサーを視覚化するために使用され、描画機能を実装しています。\n",
    "\n",
    "3. **モデルの構築**:\n",
    "   - `transformers`: Hugging FaceのTransformersライブラリからT5モデルを用いることが考慮されています。また、別の言語モデル「Llama-3-Smaug-8B」を使用してYes/No質問の応答を生成しています。\n",
    "\n",
    "4. **対話システムの実装**:\n",
    "   - 多層的な質問生成や推測アルゴリズムを通じて、エージェントは、質問に対する応答を生成するほか、さらに次の質問を決定します。これには、質問の種類（例えば、地名や国名、文化財など）や特徴（大陸や文字の最初など）に基づく処理があります。\n",
    "\n",
    "5. **リソース管理**:\n",
    "   - `gc`および`torch.cuda`: メモリ管理を行い、GPUリソースのクリアを実施しています。\n",
    "\n",
    "### 結論\n",
    "このノートブックは「20の質問」ゲームのAIエージェントを開発するための基盤を提供しており、特定の対象を当てるための戦略的な質問を自動的に生成し、言語モデルによって得られた情報を効果的に利用します。また、モデルとトークナイザーの読み込み、応答処理のロジック、エラーハンドリングについても詳細に記載されており、実用的なプロジェクトとして集約されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adca7c",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づいて、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説です。初心者なレベルですが、少しマイナーな用語や特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **immutable_dict**: イミュータブル（不変）な辞書を提供するデータ構造で、作成後に内容を変更できません。この特性は、データ整合性を保つために役立ちます。\n",
    "\n",
    "2. **sentencepiece**: Googleが開発したオープンソースのトークン化ツールです。主に、ニューラルネットワークの入力としてテキストを効果的に処理するために使用されます。モデリングの際にサブワード単位での処理を支援します。\n",
    "\n",
    "3. **T5Tokenizer / T5ForConditionalGeneration**: T5（Text-to-Text Transfer Transformer）モデル用のトークナイザーおよび条件付き生成モデルです。テキストを生成や要約、翻訳など様々なタスクに利用できる柔軟性があります。\n",
    "\n",
    "4. **pip install -U -t**: Pythonのパッケージマネージャであるpipを使って、指定したフォルダにパッケージをインストールするコマンド。`-U`オプションは既存のパッケージをアップグレードすることを意味します。`-t`オプションはインストール先のディレクトリを指定します。\n",
    "\n",
    "5. **maximum new tokens**: モデルが生成する新しいトークンの最大数を指定するパラメータで、これにより生成された応答の長さを制限します。\n",
    "\n",
    "6. **torch.no_grad()**: PyTorchにおいて、勾配計算を無効にするための文脈マネージャです。推論時にメモリ使用量を削減し、計算時間を短縮します。\n",
    "\n",
    "7. **torch.cuda.amp.autocast()**: 自動混合精度（Automatic Mixed Precision）を有効にするための文脈マネージャで、計算を半精度で行うことにより性能を向上させることができます。リソースの効率を良くし、深層学習のトレーニングや推論を加速します。\n",
    "\n",
    "8. **get_yes_no()関数**: 与えられた質問に対して「はい」または「いいえ」で応答するための関数です。キーワードやその属性に基づいてプロンプトを組み立て、モデルに入力します。\n",
    "\n",
    "9. **Observationクラス**: エージェントのターンに関する情報（質問、回答、キーワードなど）を管理するデータ構造を定義するクラスで、システムの状態を保持します。\n",
    "\n",
    "10. **garbage collection (gc)**: 使用されなくなったメモリリソースを自動的に解放するプロセスです。メモリリークを防ぎ、アプリケーションのパフォーマンスを向上させます。\n",
    "\n",
    "11. **Assertions**: コード内の条件が本当に真であるかどうかを確認するためのテスト機能。エラーが発生した場合には、デバッグの助けとなります。\n",
    "\n",
    "これらの説明は、初心者がノートブックを理解するのに役立つ関連用語をより明確にするためのものです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-04T17:22:06.807107Z",
     "iopub.status.busy": "2024-08-04T17:22:06.806802Z",
     "iopub.status.idle": "2024-08-04T17:22:07.79464Z",
     "shell.execute_reply": "2024-08-04T17:22:07.793304Z",
     "shell.execute_reply.started": "2024-08-04T17:22:06.807081Z"
    }
   },
   "outputs": [],
   "source": [
    "# このPython 3環境には、多くの便利な分析ライブラリがインストールされています\n",
    "# これは、kaggle/pythonのDockerイメージに基づいています: https://github.com/kaggle/docker-python\n",
    "# 例えば、いくつかの便利なパッケージを読み込むことができます\n",
    "\n",
    "import numpy as np # 線形代数を扱うためのライブラリ\n",
    "import pandas as pd # データ処理やCSVファイルの入出力を行うライブラリ（例: pd.read_csv）\n",
    "\n",
    "# 入力データファイルは読み取り専用の \"../input/\" ディレクトリにあります\n",
    "# 例えば、これを実行すると（実行ボタンをクリックするかShift+Enterを押すと）入力ディレクトリ内のすべてのファイルが一覧表示されます\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename)) # 各ファイルのパスを表示します\n",
    "\n",
    "# 現在のディレクトリ (/kaggle/working/) に最大20GBまで書き込むことができ、そのデータは \"Save & Run All\" を使用してバージョンを作成する際に出力として保存されます\n",
    "# また、一時ファイルを /kaggle/temp/ にも書き込むことができますが、現在のセッションの外には保存されません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:24:58.601352Z",
     "iopub.status.busy": "2024-08-04T17:24:58.60032Z",
     "iopub.status.idle": "2024-08-04T17:25:11.988458Z",
     "shell.execute_reply": "2024-08-04T17:25:11.987468Z",
     "shell.execute_reply.started": "2024-08-04T17:24:58.601316Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working # 作業ディレクトリに移動\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece # immutabledictとsentencepieceを作業ディレクトリ内の特定のライブラリフォルダにインストールします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null # gemma_pytorchリポジトリをクローンします（出力は非表示）\n",
    "mkdir /kaggle/working/submission/lib/gemma/ # gemma用のディレクトリを作成します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/ # クローンしたファイルを指定のディレクトリに移動します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:27:53.698669Z",
     "iopub.status.busy": "2024-08-04T17:27:53.698244Z",
     "iopub.status.idle": "2024-08-04T17:27:58.293566Z",
     "shell.execute_reply": "2024-08-04T17:27:58.292771Z",
     "shell.execute_reply.started": "2024-08-04T17:27:53.698638Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import json # JSONデータを扱うためのライブラリをインポート\n",
    "import os # OSに関する機能を提供するライブラリをインポート\n",
    "import pandas as pd # データ操作のためのpandasライブラリをインポート\n",
    "import random # ランダムな値を生成するためのライブラリをインポート\n",
    "import string # 文字列操作のためのライブラリをインポート\n",
    "import torch # PyTorchライブラリをインポート（深層学習用）\n",
    "\n",
    "from os import path # OSのpath機能をインポート\n",
    "from pathlib import Path # パス操作のためのPathクラスをインポート\n",
    "from random import choice # ランダムに選択するためのchoice関数をインポート\n",
    "from string import Template # テンプレート文字列操作のためのTemplateクラスをインポート\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration # T5モデル用のトークナイザーとモデルをインポート\n",
    "\n",
    "# キーワードのJSONデータを定義\n",
    "KEYWORDS_JSON = \"\"\"\n",
    "[\n",
    "  {\n",
    "    \"category\": \"country\", # カテゴリ: 国\n",
    "    \"words\": [\n",
    "      {\n",
    "        \"keyword\": \"afghanistan\", # キーワード: アフガニスタン\n",
    "        \"alts\": [] # 代替単語: なし\n",
    "      },\n",
    "      {\n",
    "        \"keyword\": \"albania\", # キーワード: アルバニア\n",
    "        \"alts\": [] # 代替単語: なし\n",
    "      },\n",
    "      {\n",
    "        \"keyword\": \"algeria\", # キーワード: アルジェリア\n",
    "        \"alts\": [] # 代替単語: なし\n",
    "      },\n",
    "      ...\n",
    "      {\n",
    "        \"keyword\": \"zimbabwe\", # キーワード: ジンバブエ\n",
    "        \"alts\": [] # 代替単語: なし\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"city\", # カテゴリ: 都市\n",
    "    \"words\": [\n",
    "      {\n",
    "        \"keyword\": \"amsterdam netherlands\", # キーワード: アムステルダム, オランダ\n",
    "        \"alts\": [\"amsterdam\", \"amsterdam holland\"] # 代替単語: アムステルダム, アムステルダム・ホラント\n",
    "      },\n",
    "      ...\n",
    "      {\n",
    "        \"keyword\": \"zurich switzerland\", # キーワード: チューリッヒ, スイス\n",
    "        \"alts\": [\"zurich\"] # 代替単語: なし\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  ...\n",
    "]\n",
    "\"\"\" # JSONの内容の終わり"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:28:44.925238Z",
     "iopub.status.busy": "2024-08-04T17:28:44.92465Z",
     "iopub.status.idle": "2024-08-04T17:28:44.955705Z",
     "shell.execute_reply": "2024-08-04T17:28:44.954593Z",
     "shell.execute_reply.started": "2024-08-04T17:28:44.925192Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_data = json.loads(KEYWORDS_JSON) # JSON形式のデータをPythonの辞書として読み込む\n",
    "\n",
    "# DataFrameの行を格納するリストを作成\n",
    "rows = []\n",
    "\n",
    "# JSONの構造を行にフラット化する\n",
    "for item in keywords_data:\n",
    "    category = item[\"category\"] # カテゴリを取得\n",
    "    for word in item[\"words\"]:\n",
    "        rows.append({\n",
    "            \"keyword\": word[\"keyword\"], # キーワードを取得\n",
    "            \"category\": category, # カテゴリを設定\n",
    "            \"continent\": \"\",  # 大陸データがあれば追加可能\n",
    "            \"first_letter\": word[\"keyword\"][0].upper()  # 最初の文字を大文字に変換\n",
    "        })\n",
    "\n",
    "# DataFrameを作成\n",
    "keywords = pd.DataFrame(rows) # フラット化したデータからDataFrameを作成\n",
    "keywords.head() # DataFrameの最初の5行を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:28:59.064722Z",
     "iopub.status.busy": "2024-08-04T17:28:59.064326Z",
     "iopub.status.idle": "2024-08-04T17:28:59.381764Z",
     "shell.execute_reply": "2024-08-04T17:28:59.380555Z",
     "shell.execute_reply.started": "2024-08-04T17:28:59.06469Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def wrap_text(text, max_width, font_size):\n",
    "    words = text.split() # テキストを単語に分割\n",
    "    lines = [] # 行を格納するリスト\n",
    "    line = \"\" # 行の初期化\n",
    "    for word in words:\n",
    "        test_line = f\"{line} {word}\".strip() # 次の行をテスト\n",
    "        if len(test_line) * font_size > max_width and line: # 最大幅を超えたら\n",
    "            lines.append(line) # 現在の行を保存\n",
    "            line = word # 新しい行を開始\n",
    "        else:\n",
    "            line = test_line # 行を更新\n",
    "    if line:\n",
    "        lines.append(line) # 最後の行を保存\n",
    "    return lines # 行を返す\n",
    "\n",
    "def renderer(context):\n",
    "    # コンテキストから必要な情報を取得\n",
    "    act = context['act']\n",
    "    agents = context['agents']\n",
    "    environment = context['environment']\n",
    "    frame = context['frame']\n",
    "    height = context.get('height', 800) # 高さの取得\n",
    "    interactive = context['interactive']\n",
    "    is_interactive = context['is_interactive']\n",
    "    parent = context['parent']\n",
    "    step = context['step']\n",
    "    update = context['update']\n",
    "    width = context.get('width', 1200) # 幅の取得\n",
    "\n",
    "    # 共通の寸法\n",
    "    max_width = 1200\n",
    "    max_height = 800\n",
    "    canvas_size = min(height, width) # 最小のキャンバスサイズ\n",
    "    unit = 8\n",
    "    offset = canvas_size * 0.1 if canvas_size > 400 else unit / 2 # オフセットの計算\n",
    "    cell_size = (canvas_size - offset * 2) / 3 # セルサイズの計算\n",
    "\n",
    "    # 図と軸を作成\n",
    "    fig, ax = plt.subplots(figsize=(width / 100, height / 100)) # 図のサイズを設定\n",
    "    ax.set_xlim(0, width) # x軸の範囲を設定\n",
    "    ax.set_ylim(0, height) # y軸の範囲を設定\n",
    "    ax.invert_yaxis() # y軸の反転\n",
    "    ax.set_axis_off() # 軸を非表示\n",
    "\n",
    "    # 現在のステップの状態を取得\n",
    "    try:\n",
    "        steps = environment['steps']\n",
    "        state = steps[step] # 現在のステップを取得\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"Error accessing environment steps: {e}\") # エラーメッセージ\n",
    "        return\n",
    "\n",
    "    print(\"State:\", state) # ステート情報を表示\n",
    "\n",
    "    if not isinstance(state, dict):\n",
    "        print(\"State is not properly defined.\") # ステートが不正な場合のエラーメッセージ\n",
    "        return\n",
    "\n",
    "    # ステートから情報を抽出\n",
    "    try:\n",
    "        observation = state['observation']\n",
    "        team1_index = len(observation['questions']) - 1\n",
    "        team2_index = len(observation['questions']) - 1\n",
    "\n",
    "        team1_question = observation['questions'][team1_index] if team1_index >= 0 else \"\"\n",
    "        team2_question = observation['questions'][team2_index] if team2_index >= 0 else \"\"\n",
    "\n",
    "        team1_answer = observation['answers'][team1_index] if len(observation['questions']) == len(observation['answers']) and team1_index >= 0 else \"\"\n",
    "        team2_answer = observation['answers'][team2_index] if len(observation['questions']) == len(observation['answers']) and team2_index >= 0 else \"\"\n",
    "\n",
    "        team1_guess = observation['guesses'][team1_index] if len(observation['questions']) == len(observation['guesses']) and team1_index >= 0 else \"\"\n",
    "        team2_guess = observation['guesses'][team2_index] if len(observation['questions']) == len(observation['guesses']) and team2_index >= 0 else \"\"\n",
    "\n",
    "        team1_reward = str(state['reward']) if state['reward'] != 0 else \"\"\n",
    "        team2_reward = str(state['reward']) if state['reward'] != 0 else \"\"\n",
    "    except (IndexError, KeyError) as e:\n",
    "        print(f\"Error extracting data from state: {e}\") # エラーメッセージ\n",
    "        return\n",
    "\n",
    "    info = environment.get('info', {})\n",
    "    team1_text = info.get('TeamNames', ['Team 1'])[0] # チーム1の名前を取得\n",
    "    team2_text = info.get('TeamNames', ['Team 2'])[1] # チーム2の名前を取得\n",
    "\n",
    "    # キャンバスパラメータ\n",
    "    padding = 20\n",
    "    row_width = (min(max_width, width) - padding * 3 - 100) / 2 # 行の幅を計算\n",
    "    label_x = padding # ラベルのx座標\n",
    "    team1_x = padding + 100 # チーム1のx座標\n",
    "    team2_x = padding * 2 + row_width + 100 # チーム2のx座標\n",
    "    line_height = 40 # 行の高さ\n",
    "    label_y = 120 # ラベルのy座標\n",
    "    question_y = 160 # 質問のy座標\n",
    "    answer_y = 200 # 回答のy座標\n",
    "    guess_y = 240 # 推測のy座標\n",
    "    score_y = 280 # スコアのy座標\n",
    "\n",
    "    # フォントサイズ\n",
    "    font_size = 20 # フォントのサイズ\n",
    "\n",
    "    # テキストを描画\n",
    "    try:\n",
    "        # キーワード行\n",
    "        ax.text(label_x, line_height, f\"Keyword: {observation['keyword']}\", fontsize=font_size, color='#FFFFFF') # キーワードを描画\n",
    "\n",
    "        # チーム行\n",
    "        ax.text(team1_x, line_height, team1_text, fontsize=font_size, color='#FFFFFF') # チーム1の名前を描画\n",
    "        ax.text(team2_x, line_height, team2_text, fontsize=font_size, color='#FFFFFF') # チーム2の名前を描画\n",
    "\n",
    "        # 質問行\n",
    "        ax.text(label_x, question_y, \"Question:\", fontsize=font_size, color='#FFFFFF') # 質問ラベルを描画\n",
    "        wrapped_text1 = wrap_text(team1_question, row_width, font_size) # チーム1の質問をラップ\n",
    "        wrapped_text2 = wrap_text(team2_question, row_width, font_size) # チーム2の質問をラップ\n",
    "        for i, line in enumerate(wrapped_text1):\n",
    "            ax.text(team1_x, question_y - i * line_height, line, fontsize=font_size, color='#FFFFFF') # チーム1のラップされた質問を描画\n",
    "        for i, line in enumerate(wrapped_text2):\n",
    "            ax.text(team2_x, question_y - i * line_height, line, fontsize=font_size, color='#FFFFFF') # チーム2のラップされた質問を描画\n",
    "\n",
    "        # 回答行\n",
    "        ax.text(label_x, answer_y, \"Answer:\", fontsize=font_size, color='#FFFFFF') # 回答ラベルを描画\n",
    "        ax.text(team1_x, answer_y, team1_answer, fontsize=font_size, color='#FFFFFF') # チーム1の回答を描画\n",
    "        ax.text(team2_x, answer_y, team2_answer, fontsize=font_size, color='#FFFFFF') # チーム2の回答を描画\n",
    "\n",
    "        # 推測行\n",
    "        ax.text(label_x, guess_y, \"Guess:\", fontsize=font_size, color='#FFFFFF') # 推測ラベルを描画\n",
    "        ax.text(team1_x, guess_y, team1_guess, fontsize=font_size, color='#FFFFFF') # チーム1の推測を描画\n",
    "        ax.text(team2_x, guess_y, team2_guess, fontsize=font_size, color='#FFFFFF') # チーム2の推測を描画\n",
    "\n",
    "        # 報酬行\n",
    "        ax.text(label_x, score_y, \"Reward:\", fontsize=font_size, color='#FFFFFF') # 報酬ラベルを描画\n",
    "        ax.text(team1_x, score_y, team1_reward, fontsize=font_size, color='#FFFFFF') # チーム1の報酬を描画\n",
    "        ax.text(team2_x, score_y, team2_reward, fontsize=font_size, color='#FFFFFF') # チーム2の報酬を描画\n",
    "\n",
    "        plt.show() # グラフを表示\n",
    "    except Exception as e:\n",
    "        print(f\"Error during rendering: {e}\") # 描画中のエラーメッセージ\n",
    "\n",
    "# テスト用のコンテキストの例\n",
    "context = {\n",
    "    'act': lambda x: print(f\"Action: {x}\"), # アクションを処理するための関数\n",
    "    'agents': [],\n",
    "    'environment': {\n",
    "        'steps': [\n",
    "            {'observation': {'questions': ['What is AI?', 'Define ML.'], 'answers': ['Artificial Intelligence', 'Machine Learning'], 'guesses': ['AI', 'ML'], 'keyword': 'AI'}, 'reward': 10}, # ステップデータ\n",
    "        ],\n",
    "        'info': {'TeamNames': ['Team Alpha', 'Team Beta']} # チーム名情報\n",
    "    },\n",
    "    'frame': None,\n",
    "    'height': 800, # 高さ\n",
    "    'interactive': True,\n",
    "    'is_interactive': lambda: True,\n",
    "    'parent': None,\n",
    "    'step': 0,\n",
    "    'update': lambda: None,\n",
    "    'width': 1200 # 幅\n",
    "}\n",
    "\n",
    "renderer(context) # 描画関数を呼び出す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:29:07.156268Z",
     "iopub.status.busy": "2024-08-04T17:29:07.155923Z",
     "iopub.status.idle": "2024-08-04T17:31:35.139258Z",
     "shell.execute_reply": "2024-08-04T17:31:35.138294Z",
     "shell.execute_reply.started": "2024-08-04T17:29:07.156242Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd # pandasをインポート\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoConfig # Transformersライブラリから必要なクラスをインポート\n",
    "import torch # PyTorchライブラリをインポート\n",
    "import numpy as np # NumPyライブラリをインポート\n",
    "import gc # ガーベジコレクションを管理するためのライブラリをインポート\n",
    "import os # OSに関する機能を提供するライブラリをインポート\n",
    "\n",
    "# グローバル変数および設定\n",
    "model_name = \"abacusai/Llama-3-Smaug-8B\"  # メモリの問題が発生する場合は小さいモデルを検討\n",
    "VERBOSE = True  # デバッグ出力用にTrueに設定\n",
    "\n",
    "# GPUサポートでモデルとトークナイザーをロードする関数\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    try:\n",
    "        config = AutoConfig.from_pretrained(model_name) # モデルの設定をロード\n",
    "        model = AutoModelForCausalLM.from_pretrained( # 条件付き生成用のモデルをロード\n",
    "            model_name,\n",
    "            config=config,\n",
    "            trust_remote_code=True, # リモートコードを信頼\n",
    "            device_map=\"auto\"  # 適切なデバイスにモデルを配置\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name) # トークナイザーをロード\n",
    "        model.eval()  # モデルを評価モードに設定\n",
    "        return model, tokenizer # モデルとトークナイザーを返す\n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"Error loading model: {e}\") # エラーメッセージ\n",
    "        return None, None # エラーが発生した場合はNoneを返す\n",
    "\n",
    "# Yes/No質問用にLLMと対話する関数\n",
    "def get_yes_no(question, keyword, tokenizer, model):\n",
    "    try:\n",
    "        # プロンプトメッセージを構築\n",
    "        if keyword in keywords.keyword.values: # キーワードが存在する場合\n",
    "            row = keywords.loc[keywords.keyword == keyword].iloc[0] # キーワードに対応する行を取得\n",
    "            category = row.category # カテゴリを取得\n",
    "            continent = row.continent # 大陸情報を取得\n",
    "            negate = { # 否定要素を定義\n",
    "                \"city\": \"It is not a country. It is not a landmark.\",\n",
    "                \"country\": \"It is not a city. It is not a landmark.\",\n",
    "                \"landmark\": \"It is not a city. It is not a country.\",\n",
    "            }\n",
    "            prompt = f\"We are playing 20 questions. The keyword is {keyword}. It is a {category}. {negate[category]} This word has first letter {keyword[0]}. This {category} is located in {continent}. {question}\"\n",
    "        else:\n",
    "            prompt = f\"We are playing 20 questions. The keyword is {keyword}. It is a thing. It is not a city. It is not a country. It is not a landmark. This word has first letter {keyword[0]}. {question}\"\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"Answer yes or no to the following question and nothing else.\"}, # システムメッセージ\n",
    "            {\"role\": \"user\", \"content\": prompt} # ユーザープロンプト\n",
    "        ]\n",
    "\n",
    "        # チャットテンプレートを適用し、トークン化\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to('cuda') # トークン化した入力をGPUに転送\n",
    "\n",
    "        pad_token_id = tokenizer.pad_token_id # パディングトークンIDを取得\n",
    "        if pad_token_id is None:\n",
    "            pad_token_id = tokenizer.eos_token_id # Noneの場合はEOSトークンIDを使用\n",
    "\n",
    "        # 混合精度を使用して応答を生成\n",
    "        with torch.no_grad(), torch.cuda.amp.autocast():\n",
    "            generated_ids = model.generate(\n",
    "                model_inputs.input_ids,\n",
    "                attention_mask=model_inputs.attention_mask,\n",
    "                pad_token_id=pad_token_id,\n",
    "                max_new_tokens=1 # 新しいトークンの最大数を設定\n",
    "            )\n",
    "        \n",
    "        generated_ids = [\n",
    "            output_ids[len(model_inputs.input_ids[0]):] for output_ids in generated_ids # 生成されたIDを抽出\n",
    "        ]\n",
    "\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0] # 応答をデコード\n",
    "        if not \"yes\" in response.lower(): # 応答が「yes」でなければ\n",
    "            response = \"no\" # 「no」と返す\n",
    "        else:\n",
    "            response = \"yes\" # 「yes」と返す\n",
    "\n",
    "        return response # 最終応答を返す\n",
    "    \n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"Error during agent execution: {e}\") # エラーメッセージ\n",
    "        return \"no\" # エラー時は「no」を返す\n",
    "    \n",
    "def agent_fn(obs, cfg): # エージェント関数\n",
    "    global keywords # グローバル変数としてキーワードを参照\n",
    "\n",
    "    # 応答変数の初期化\n",
    "    response = \"no\"  # 応答のデフォルト値を設定\n",
    "    \n",
    "    try:\n",
    "        # キーワードが正しくロードされていることを確認\n",
    "        if keywords.empty: # キーワードが空の場合\n",
    "            response = \"Unable to process due to missing keywords.\" # エラーメッセージを設定\n",
    "            return response\n",
    "\n",
    "        # 初期化が必要な変数\n",
    "        cat_guess = 0  # カテゴリ推測用カウンタ\n",
    "        con_guess = 0  # 大陸推測用カウンタ\n",
    "        let_guess = 0  # 最初の文字推測用カウンタ\n",
    "        category_yes = [] # 「はい」回答のカテゴリリスト\n",
    "        category_no = [] # 「いいえ」回答のカテゴリリスト\n",
    "        continent_yes = [] # 「はい」回答の大陸リスト\n",
    "        continent_no = [] # 「いいえ」回答の大陸リスト\n",
    "        first_letter_yes = [] # 「はい」回答の最初の文字リスト\n",
    "        first_letter_no = [] # 「いいえ」回答の最初の文字リスト\n",
    "        extra_guess = None # 追加の推測\n",
    "\n",
    "        categories = [\"city\", \"country\", \"landmark\"] # カテゴリのリスト\n",
    "        continents = [\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\", \"Australia\"] # 大陸のリスト\n",
    "\n",
    "        if obs.turnType == \"ask\": # 問いかけの場合\n",
    "            if (cat_guess < 3) and (len(category_yes) == 0): # カテゴリの推測\n",
    "                response = f\"Is the keyword the name of a {categories[cat_guess]}?\" # 質問の構築\n",
    "                cat_guess += 1 # カテゴリカウンタの増加\n",
    "            elif (con_guess < 6) and (len(continent_yes) == 0): # 大陸の推測\n",
    "                category = \"place\" # 場所として初期化\n",
    "                if len(category_yes) == 1: \n",
    "                    category = category_yes[0] # 既存のカテゴリをセット\n",
    "                response = f\"Is the {category} located in {continents[con_guess]}?\" # 質問の構築\n",
    "                con_guess += 1 # 大陸カウンタの増加\n",
    "            else:\n",
    "                IDX = keywords.category.isin(category_yes) # カテゴリのインデックス\n",
    "                IDX = IDX & (keywords.continent.isin(continent_yes)) # 大陸のインデックス\n",
    "                first_letters = list(keywords.loc[IDX, \"first_letter\"].value_counts().index.values) # 最初の文字のリスト\n",
    "                if let_guess < len(first_letters): # 最初の文字がリストにある場合\n",
    "                    response = f\"Does the keyword begin with the letter {first_letters[let_guess]}?\" # 質問の構築\n",
    "                else:\n",
    "                    IDX = keywords.guess == 0 # 推測が初めての場合\n",
    "                    if len(category_yes) > 0: IDX = IDX & (keywords.category.isin(category_yes)) # 「はい」回答対象カテゴリをフィルタ\n",
    "                    if len(category_no) > 0: IDX = IDX & (~keywords.category.isin(category_no)) # 「いいえ」回答対象カテゴリをフィルタ\n",
    "                    if len(continent_yes) > 0: IDX = IDX & (keywords.continent.isin(continent_yes)) # 「はい」回答対象大陸をフィルタ\n",
    "                    if len(continent_no) > 0: IDX = IDX & (~keywords.continent.isin(continent_no)) # 「いいえ」回答対象大陸をフィルタ\n",
    "                    if len(first_letter_yes) > 0: IDX = IDX & (keywords.first_letter.isin(first_letter_yes)) # 「はい」回答対象最初の文字をフィルタ\n",
    "                    if len(first_letter_no) > 0: IDX = IDX & (~keywords.first_letter.isin(first_letter_no)) # 「いいえ」回答対象最初の文字をフィルタ\n",
    "\n",
    "                    try:\n",
    "                        guess = keywords.loc[IDX].sample(1).index.values[0] # 推測を抽出\n",
    "                        keywords.loc[guess, 'guess'] = 1 # 推測マーク\n",
    "                        response = keywords.loc[guess, \"keyword\"] # 推測結果を取得\n",
    "                    except:\n",
    "                        response = np.random.choice(keywords.keyword.values) # ランダムな推測を生成\n",
    "                    extra_guess = response # 追加推測にセット\n",
    "                    response = f\"Is it {response}?\" # 質問の構築\n",
    "                let_guess += 1 # 最初の文字カウンタの増加\n",
    "\n",
    "        elif obs.turnType == \"guess\": # 推測の場合\n",
    "            category_yes = [] # 「はい」回答のカテゴリリストを初期化\n",
    "            category_no = [] # 「いいえ」回答のカテゴリリストを初期化\n",
    "            for k in range(cat_guess): # カテゴリ推測に対する回答を取得\n",
    "                if obs.answers[k] == \"yes\":\n",
    "                    category_yes.append(categories[k]) # 「はい」回答を追加\n",
    "                else:\n",
    "                    category_no.append(categories[k]) # 「いいえ」回答を追加\n",
    "            if (cat_guess == 3) and (len(category_yes) == 0): # 全てのカテゴリが「いいえ」だった場合\n",
    "                category_yes = [\"city\", \"country\", \"landmark\"] # すべてのカテゴリを対象にする\n",
    "                category_no = [] # 「いいえ」リストをクリア\n",
    "\n",
    "            continent_yes = [] # 大陸「はい」リストを初期化\n",
    "            continent_no = [] # 大陸「いいえ」リストを初期化\n",
    "            for k in range(con_guess): # 大陸推測に対する回答を取得\n",
    "                if obs.answers[k + cat_guess] == \"yes\":\n",
    "                    continent_yes.append(continents[k]) # 「はい」回答を追加\n",
    "                else:\n",
    "                    continent_no.append(continents[k]) # 「いいえ」回答を追加\n",
    "            if (con_guess == 6) and (len(continent_yes) == 0): # 全ての大陸が「いいえ」だった場合\n",
    "                continent_yes = [\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\", \"Australia\"] # すべての大陸を対象にする\n",
    "                continent_no = [] # 「いいえ」リストをクリア\n",
    "\n",
    "            first_letter_yes = [] # 最初の文字「はい」リストを初期化\n",
    "            first_letter_no = [] # 最初の文字「いいえ」リストを初期化\n",
    "            for k in range(let_guess): # 最初の文字推測に対する回答を取得\n",
    "                if k >= len(first_letters): continue # 文字数がオーバーしている場合、スキップ\n",
    "                if obs.answers[k + cat_guess + con_guess] == \"yes\":\n",
    "                    first_letter_yes.append(first_letters[k]) # 「はい」回答を追加\n",
    "                else:\n",
    "                    first_letter_no.append(first_letters[k]) # 「いいえ」回答を追加\n",
    "\n",
    "            # フィルタリング条件を適用\n",
    "            IDX = keywords.guess == 0\n",
    "            if len(category_yes) > 0: IDX = IDX & (keywords.category.isin(category_yes))\n",
    "            if len(category_no) > 0: IDX = IDX & (~keywords.category.isin(category_no))\n",
    "            if len(continent_yes) > 0: IDX = IDX & (keywords.continent.isin(continent_yes))\n",
    "            if len(continent_no) > 0: IDX = IDX & (~keywords.continent.isin(continent_no))\n",
    "            if len(first_letter_yes) > 0: IDX = IDX & (keywords.first_letter.isin(first_letter_yes))\n",
    "            if len(first_letter_no) > 0: IDX = IDX & (~keywords.first_letter.isin(first_letter_no))\n",
    "\n",
    "            try:\n",
    "                guess = keywords.loc[IDX].sample(1).index.values[0] # 推測を抽出\n",
    "                keywords.loc[guess, 'guess'] = 1 # 推測マーク\n",
    "                response = keywords.loc[guess, \"keyword\"] # 推測結果を取得\n",
    "            except:\n",
    "                response = np.random.choice(keywords.keyword.values) # ランダムな推測を生成\n",
    "\n",
    "            if (let_guess > 0) and (let_guess >= len(first_letters)) and (obs.answers[-1] == \"yes\"): # 最後の回答が「はい」の場合\n",
    "                response = extra_guess # 追加の推測を応答\n",
    "\n",
    "        elif obs.turnType == \"answer\": # 回答の場合\n",
    "            if obs.keyword.lower() in obs.questions[-1].lower(): # キーワードが質問に含まれている場合\n",
    "                response = \"yes\" # 「はい」と応答\n",
    "            else:\n",
    "                response = get_yes_no(obs.questions[-1], obs.keyword, tokenizer, model) # LLMに問い合わせ\n",
    "\n",
    "        else:\n",
    "            response = \"yes\"  # すべて外れた場合のデフォルト応答\n",
    "\n",
    "    except Exception as e:\n",
    "        if VERBOSE:\n",
    "            print(f\"Error during agent execution: {e}\") # エラーメッセージ\n",
    "\n",
    "    return response  # 応答を常に返す\n",
    "\n",
    "class Observation: # 観察クラスの定義\n",
    "    def __init__(self, turnType, answers=None, keyword=None, questions=None):\n",
    "        self.turnType = turnType\n",
    "        self.answers = answers or [] # 回答リスト\n",
    "        self.keyword = keyword\n",
    "        self.questions = questions or [] # 質問リスト\n",
    "\n",
    "# 使用例\n",
    "if __name__ == \"__main__\":\n",
    "    model, tokenizer = load_model_and_tokenizer(model_name) # モデルとトークナイザーをロード\n",
    "    \n",
    "    observation_object = Observation( # 観察オブジェクトを作成\n",
    "        turnType=\"ask\",\n",
    "        questions=[\"Is it a country?\"]  # 質問の例\n",
    "    )\n",
    "    configuration_object = {}  # 必要な設定パラメータを追加\n",
    "    \n",
    "    response = agent_fn(observation_object, configuration_object) # エージェント関数を呼び出す\n",
    "    print(f\"Agent response: {response}\") # エージェントの応答を表示\n",
    "\n",
    "    # リソースをクリーンアップ\n",
    "    del model, tokenizer\n",
    "    gc.collect() # ガーベジコレクションを実行\n",
    "    torch.cuda.empty_cache() # CUDAキャッシュをクリア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:33:05.171424Z",
     "iopub.status.busy": "2024-08-04T17:33:05.170704Z",
     "iopub.status.idle": "2024-08-04T17:33:11.672209Z",
     "shell.execute_reply": "2024-08-04T17:33:11.671121Z",
     "shell.execute_reply.started": "2024-08-04T17:33:05.17139Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null # pigzとpvパッケージをインストール（出力は非表示）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T17:35:22.361678Z",
     "iopub.status.busy": "2024-08-04T17:35:22.361022Z",
     "iopub.status.idle": "2024-08-04T17:37:15.077861Z",
     "shell.execute_reply": "2024-08-04T17:37:15.076612Z",
     "shell.execute_reply.started": "2024-08-04T17:35:22.361638Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf /kaggle/working/submission.tar.gz -C /kaggle/input/gemma-2/pytorch/gemma-2-2b-it/1 . # データを圧縮し、出力される圧縮ファイルを作成するコマンド"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 76277,
     "modelInstanceId": 72240,
     "sourceId": 85979,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
