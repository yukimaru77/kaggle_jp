{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83bad167",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのエージェントを開発することを目的としています。このエージェントは、質問を行い、回答を受け取り、推測をします。主に、次のような問題に取り組んでいます：\n",
    "\n",
    "1. **モデルのセットアップと接続**: Hugging Faceのモデル（Meta-Llama-3-8B-Instruct-hf-AWQ）をダウンロードし、vLLMを用いてAPIサーバーを起動します。\n",
    "2. **質問者エージェントの実装**: 質問を行うためのロジック、回答を処理するためのロジック、推測を生成するためのロジックがそれぞれ実装されています。\n",
    "\n",
    "使用されている主な手法とライブラリは以下の通りです：\n",
    "- **Hugging Face Hub**: モデルのスナップショットをダウンロードするために使用。\n",
    "- **vLLM**: 高速化を図るためのモデルサーバー。\n",
    "- **Rigging**: エージェントが質問を生成し、回答を解析するためのライブラリ。\n",
    "- **Pydantic**: データモデルを定義するのに使用し、各種質問、回答、推測を構造化しています。\n",
    "\n",
    "Notebookは、依存関係のインストール、モデルのダウンロード、vLLMサーバーの起動、質問・回答・推測のロジックを含むメインエージェントの実装を行い、最終的にKaggleコンペティションへの提出を行うコードから構成されています。また、ユーザーのシークレット情報を扱うためのコードも含まれており、KaggleのAPIを介して提出を行うことも可能です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a97767",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に関連する専門用語の簡単な解説です。特にマイナーなものや実務経験がないと馴染みのないものを中心に説明します。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **Hugging Face トークン (HF_TOKEN)**:\n",
    "   Hugging Faceは、多くの深層学習モデル（特に自然言語処理のモデル）を提供するプラットフォームです。HF_TOKENはそのサービスにアクセスするための認証用トークンで、ユーザーが自分のアカウントにアクセスしてモデルをダウンロードしたり、APIを利用したりする際に使用されます。\n",
    "\n",
    "2. **rigging**:\n",
    "   これは、AIモデルのサービングや生成に関連するライブラリです。riggingは、さまざまなLLM（大規模言語モデル）を管理し、効率的に利用するための機能を提供します。特にAPI経由でモデルを呼び出す際に便利です。\n",
    "\n",
    "3. **vllm**:\n",
    "   vllmは、AIモデルを高速でサーブ（提供）するためのフレームワークです。このライブラリは、特に大規模な言語モデルの性能を良く引き出すために設計されています。vllmを使用すると、効率的にリクエストを処理し、応答を生成することができます。\n",
    "\n",
    "4. **シンボリックリンク (symlink)**:\n",
    "   シンボリックリンクは、ファイルシステム内で他のファイルやディレクトリへの参照を示す特別なタイプのファイルです。シンボリックリンクを使用すると、実際のファイルの場所を移動しても、リンクを通じてそのファイルにアクセスできるため、便利です。\n",
    "\n",
    "5. **サーバーリッスンポート**:\n",
    "   サーバーが外部からのリクエストを受け付けるために使用するポートです。このポート番号は、サーバーが特定のリクエスト（例えばHTTPトラフィック）を待ち受ける際に必要です。このノートブックでは、ポート9999がvllmのために指定されています。\n",
    "\n",
    "6. **Pydantic**:\n",
    "   Pydanticは、データの検証と設定を行うためのPythonライブラリです。特に、型ヒントを用いてデータモデルを定義し、そのデータが正しいかどうかを検証する際に使用されます。Pythonのデータクラスに似ていますが、追加のバリデーション機能を提供します。\n",
    "\n",
    "7. **XML形式の例 (xml_example)**:\n",
    "   XML（拡張マークアップ言語）は、データを構造的に表現するためのフォーマットです。`xml_example`メソッドは、特定のモデルがどのようにXML形式で表現されるべきかを示すサンプルを提供します。\n",
    "\n",
    "8. **サーバーサイドでの処理 (APIサーバー実行)**:\n",
    "   APIサーバーは、クライアントからのリクエストに対して応答を返すためのソフトウェアです。ノートブック内のvllmは、このAPIサーバーとして機能し、他のプログラムやサービスからの要求に対してAIモデルを通じて応答を生成します。\n",
    "\n",
    "これらの説明は、機械学習や深層学習の初心者が遭遇しそうな専門用語の理解を助けるためのものです。他の一般的な用語やコンセプトについては、知識をお持ちのことと考えますので省略しました。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シークレット（オプション）\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# HF_TOKENを文字列またはNoneで初期化します\n",
    "HF_TOKEN: str | None  = None\n",
    "# KAGGLE_KEYを文字列またはNoneで初期化します\n",
    "KAGGLE_KEY: str | None = None\n",
    "# KAGGLE_USERNAMEを文字列またはNoneで初期化します\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    # シークレットからHF_TOKENを取得します\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    # シークレットからKAGGLE_KEYを取得します\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    # シークレットからKAGGLE_USERNAMEを取得します\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    # エラーが発生した場合は何もしません（パスします）\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 依存関係（速度向上のためのuv）\n",
    "\n",
    "# uvをインストールします\n",
    "!pip install uv\n",
    "\n",
    "# riggingとkaggleをアップグレードインストールします\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\  # 実行中のPythonのパスを指定します\n",
    "    --target /kaggle/tmp/lib \\  # インストール先ディレクトリを指定します\n",
    "    rigging==1.3.0 \\  # riggingパッケージのバージョン1.3.0を指定します\n",
    "    kaggle\n",
    "\n",
    "# vllmをアップグレードインストールします\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\  # 実行中のPythonのパスを指定します\n",
    "    --target /kaggle/tmp/srvlib \\  # 別のインストール先ディレクトリを指定します\n",
    "    vllm  # vllmパッケージをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのダウンロード\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# モデルを保存するパスを指定します\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# 既にモデルのパスが存在する場合は、そのディレクトリを削除します\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "# 新しいディレクトリを作成します\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "# モデルのスナップショットをダウンロードします\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",  # ダウンロードするモデルのIDを指定します\n",
    "    ignore_patterns=\"original*\",  # 無視するファイルパターンを指定します\n",
    "    local_dir=g_model_path,  # ダウンロード先のローカルディレクトリを指定します\n",
    "    local_dir_use_symlinks=False,  # シンボリックリンクは使用しません\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # Hugging Faceのトークンを取得します\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# vLLMサーバーを開始するためのヘルパー関数\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# ポートが使用中かどうかを確認する関数\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        # ソケットを作成してポートをチェックします\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)  # タイムアウトを1秒に設定\n",
    "            result = sock.connect_ex(('localhost', port))  # 指定したポートに接続を試みる\n",
    "            if result == 0:  # 接続成功（ポートがオープン）\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass  # エラーが発生した場合は無視\n",
    "    \n",
    "    return False  # ポートがオープンでない場合\n",
    "\n",
    "# プロセスを実行し、指定したポートが開くのを待つ関数\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        # 既にポートが開いている場合はエラーを返します\n",
    "        raise ValueError(f\"ポート {port} はすでに開いています\")\n",
    "        \n",
    "    # コマンドを実行します\n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},  # 環境変数を結合します\n",
    "        stdout=subprocess.DEVNULL,  # 標準出力を無視します\n",
    "        stderr=subprocess.DEVNULL   # 標準エラーを無視します\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # タイムアウトまでポートが開くのを待ちます\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen  # ポートが開いた場合、プロセスを返します\n",
    "        time.sleep(1)  # 1秒待ちます\n",
    "    \n",
    "    popen.terminate()  # タイムアウトした場合、プロセスを終了します\n",
    "    raise Exception(f\"プロセスは {timeout} 秒以内にポート {port} を開きませんでした。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vLLMの起動を検証する（オプション - ビルドを速くするためにコメントアウト）\n",
    "\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import util\n",
    "\n",
    "# util = importlib.reload(util)  # utilモジュールを再読み込みします\n",
    "\n",
    "# g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")  # srvlibのパスを指定します\n",
    "# assert g_srvlib_path.exists()  # パスが存在することを確認します\n",
    "\n",
    "# g_model_path = Path(\"/kaggle/tmp/model\")  # モデルのパスを指定します\n",
    "# assert g_model_path.exists()  # パスが存在することを確認します\n",
    "\n",
    "# g_vllm_port = 9999  # vLLMがリッスンするポートを指定します\n",
    "# g_vllm_model_name = \"custom\"  # サーブするモデルの名前を指定します\n",
    "\n",
    "# # vLLMサーバーを開始します\n",
    "\n",
    "# vllm = util.run_and_wait_for_port([\n",
    "#     \"python\", \"-m\",  # Pythonモジュールとして実行します\n",
    "#     \"vllm.entrypoints.openai.api_server\",  # vLLMのAPIサーバーを指定します\n",
    "#     \"--enforce-eager\",  # 即時実行を強制します\n",
    "#     \"--model\", str(g_model_path),  # モデルのパスを指定します\n",
    "#     \"--port\", str(g_vllm_port),  # 使用するポートを指定します\n",
    "#     \"--served-model-name\", g_vllm_model_name  # サーブされるモデルの名前を指定します\n",
    "# ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})  # ポートが開くのを待ちながら実行します\n",
    "\n",
    "# print(\"vLLMが起動しました\")  # サーバーが正常に起動したことを表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Riggingに接続します（オプション - ビルドを速くするためにコメントアウト）\n",
    "\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/tmp/lib\")  # ライブラリパスを追加します\n",
    "\n",
    "# logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ロギングのレベルをWARNINGに設定します\n",
    "\n",
    "# import rigging as rg  # riggingモジュールをインポートします\n",
    "\n",
    "# generator = rg.get_generator(\n",
    "#     f\"openai/{g_vllm_model_name},\"  # 使用するモデル名を指定します\n",
    "#     f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIベースURLを指定します\n",
    "#     \"api_key=sk-1234,\"  # APIキーを指定します\n",
    "#     \"stop=<|eot_id|>\"  # Llamaモデルでは特別な停止トークンが必要です\n",
    "# )\n",
    "# chat = generator.chat(\"Say Hello!\").run()  # \"Say Hello!\"というメッセージでチャットを開始します\n",
    "\n",
    "# print(chat.last)  # チャットの最後の応答を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# メインエージェントファイル\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# パスの修正\n",
    "\n",
    "g_working_path = Path('/kaggle/working')  # 作業ディレクトリのパス\n",
    "g_input_path = Path('/kaggle/input')  # 入力データのパス\n",
    "g_temp_path = Path(\"/kaggle/tmp\")  # 一時ファイルのパス\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")  # エージェントのパス\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"  # モデルのパス\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"  # サーバーライブラリのパス\n",
    "g_lib_path = g_temp_path / \"lib\"  # ライブラリのパス\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"  # エージェントパスが存在する場合、ライブラリパスを更新\n",
    "    g_model_path = g_agent_path / \"model\"  # モデルパスを更新\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"  # サーバーライブラリのパスを更新\n",
    "\n",
    "sys.path.insert(0, str(g_lib_path))  # ライブラリパスをシステムパスに追加\n",
    "\n",
    "# ロギングの設定\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログレベルをWARNINGに設定\n",
    "\n",
    "# インポート\n",
    "\n",
    "import util  # 利用するモジュール\n",
    "import rigging as rg  # riggingモジュールをインポート\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # Pydanticライブラリから必要なクラスをインポート\n",
    "\n",
    "# 定数\n",
    "\n",
    "g_vllm_port = 9999  # vLLMがリッスンするポート\n",
    "g_vllm_model_name = \"custom\"  # 使用するモデル名\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\"  # モデル名を指定\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIベースURLを指定\n",
    "    \"api_key=sk-1234,\"  # APIキーを指定\n",
    "    \"stop=<|eot_id|>\"  # Llamaモデルでは特別な停止トークンが必要\n",
    ")\n",
    "\n",
    "# タイプ\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]  # スペースをトリムした文字列\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int  # 手順の番号\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]  # プレイヤーの役割\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]  # プレイヤーのターンのタイプ\n",
    "    keyword: str  # キーワード\n",
    "    category: str  # カテゴリー\n",
    "    questions: list[str]  # 質問のリスト\n",
    "    answers: list[str]  # 回答のリスト\n",
    "    guesses: list[str]  # 推測のリスト\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])  # 質問、回答、推測がすべて空かを確認\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")  # 過去の質問、回答、推測をまとめて返す\n",
    "\n",
    "    def get_history_as_xml(self, *, include_guesses: bool = False) -> str:\n",
    "        return \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if include_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "        ) if not self.empty else \"まだなし.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]  # 回答は\"yes\"または\"no\"\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):  # 有効な回答の検証\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です。'yes'または'no'でなければなりません\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"  # XML形式の例\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip  # 質問内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip  # 推測内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"**thing/place/person**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "# 関数\n",
    "\n",
    "def ask(base: rg.PendingChat, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # 最初の質問をオーバーライドして不可解なバグを修正\n",
    "        return \"私たちは20の質問をプレイしていますか？\"\n",
    "    \n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            あなたは現在、次の質問を尋ねています。\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            上記の履歴に基づいて、最も役立つはい/いいえの質問を次の形式で尋ねてください:\n",
    "            {Question.xml_example()}\n",
    "\n",
    "            - あなたの回答は、最も多くの情報を収集するための焦点を絞った質問であるべきです\n",
    "            - 最初は一般的な質問から始めてください\n",
    "            - 残された検索空間をバイセクトしようと常に試みてください\n",
    "            - 過去の質問と回答に注意してください\n",
    "\n",
    "            始める前に、利用可能な場合はゲーム履歴の分析を文書化し、\n",
    "            その後、質問を書いてください。\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Question, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Question).content\n",
    "\n",
    "\n",
    "def answer(base: rg.PendingChat, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"キーワードが回答者に提供されていません\", file=sys.stderr)\n",
    "        return \"yes\"  # 不可解なバグを修正するためにオーバーライド\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            このゲームの秘伝の言葉は\"{observation.keyword}\"です [{observation.category}]\n",
    "\n",
    "            あなたは上記の言葉に関する質問に答えています。\n",
    "\n",
    "            次の質問は\"{last_question}\"です。\n",
    "\n",
    "            上記のはい/いいえの質問に答え、次の形式で提示してください:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - あなたの回答は、上記のキーワードに正確であるべきです\n",
    "            - 常に\"yes\"または\"no\"で答えてください\n",
    "\n",
    "            答えは何ですか？\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Answer, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Answer).content\n",
    "\n",
    "\n",
    "def guess(base: rg.PendingChat, observation: Observation) -> str:\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            あなたは現在、キーワードの情報に基づいた推測を行っています。\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            上記の履歴に基づいて、キーワードに対する次の最良の推測を1つ作成し、次の形式で示してください:\n",
    "            {Guess.xml_example()}\n",
    "\n",
    "            - 上記の履歴に基づいて再推測は避けてください\n",
    "            - 推測は特定の人物、場所、または物であるべきです\n",
    "\n",
    "            始める前に、利用可能な場合はゲーム履歴の分析を文書化し、\n",
    "            その後、推測を書いてください。\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Guess, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "        \n",
    "    return chat.last.parse(Guess).content\n",
    "\n",
    "# vLLMとGeneratorの初期化\n",
    "\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",  # APIサーバーを指定\n",
    "    \"--enforce-eager\",  # 即時実行を強制\n",
    "    \"--model\", str(g_model_path),  # モデルのパスを指定\n",
    "    \"--port\", str(g_vllm_port),  # 使用するポートを指定\n",
    "    \"--served-model-name\", g_vllm_model_name  # サーブするモデルの名前を指定\n",
    "], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "print(\"vLLMが起動しました\")  # サーバーが正常に起動したことを表示\n",
    "\n",
    "generator = rg.get_generator(g_generator_id)  # Generatorの初期化\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの才能あるプレイヤーです。あなたは正確で、焦点を絞り、構造化されたアプローチを持っています。あなたは有用な質問を作成し、推測を行い、またはキーワードに関する質問に答えます。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# エントリーポイント\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)  # 観察オブジェクトを作成\n",
    "    \n",
    "    try:\n",
    "        match observation.turnType:  # プレイヤーのターンのタイプに基づいて処理\n",
    "            case \"ask\":\n",
    "                return ask(base, observation)  # 質問を行う\n",
    "            case \"answer\":\n",
    "                return answer(base, observation)  # 回答を行う\n",
    "            case \"guess\":\n",
    "                return guess(base, observation)  # 推測を行う\n",
    "            case _:\n",
    "                raise ValueError(\"未知のターンタイプです\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)  # エラーを表示\n",
    "        raise  # エラーを再スロー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv  # pigzとpvをインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast' \\  # pigzを使って圧縮します（高速モード）\n",
    "    -cf submission.tar.gz \\  # tar.gz形式でアーカイブを作成します\n",
    "    --dereference \\  # シンボリックリンクを解決して参照を保持します\n",
    "    -C /kaggle/tmp model lib srvlib \\  # /kaggle/tmpディレクトリからモデル、ライブラリ、サーバーライブラリを追加します\n",
    "    -C /kaggle/working main.py util.py  # /kaggle/workingディレクトリからmain.pyとutil.pyを追加します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!KAGGLE_USERNAME={KAGGLE_USERNAME} \\  # Kaggleユーザー名を設定します\n",
    " KAGGLE_KEY={KAGGLE_KEY} \\  # Kaggle APIキーを設定します\n",
    " kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"Updates\"  # コンペティションに提出します（\"Updates\"というメッセージを追加）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e917586",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> こんにちは、あなたのコードをテストしていたのですが、実行したら「AttributeError: 'coroutine' object has no attribute 'last'」という例外が発生して失敗しました。このエラーに遭遇したことはありますか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Rob Mulla\n",
    "> > \n",
    "> > [@max1mum](https://www.kaggle.com/max1mum) - これは、以前のバージョンとの互換性がない変更を含むriggingの新しいリリースが原因です。\n",
    "> > \n",
    "> > パッケージの固定バージョンを使用してみてください。インストールセルを次のように変更すれば、動作するはずです。\n",
    "> > \n",
    "> > ```\n",
    "> > # 依存関係（速度向上のためのuv）\n",
    "> > !pip install uv==0.1.45\n",
    "> > \n",
    "> > !uv pip install -U \\\n",
    "> >     --python $(which python) \\\n",
    "> >     --target /kaggle/tmp/lib \\\n",
    "> >     rigging==1.1.1 \\\n",
    "> >     kaggle\n",
    "> > \n",
    "> > !uv pip install -U \\\n",
    "> >     --python $(which python) \\\n",
    "> >     --target /kaggle/tmp/srvlib \\\n",
    "> >     vllm==0.4.2\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## OminousDude\n",
    "> > > \n",
    "> > > ありがとうございます!!!\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
