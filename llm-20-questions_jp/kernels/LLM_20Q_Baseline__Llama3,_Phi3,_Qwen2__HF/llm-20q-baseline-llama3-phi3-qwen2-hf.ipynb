{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89cd527a",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "以下のノートブックは、Kaggleの「LLM 20 Questions」コンペティションにおけるベースラインモデルの構築を目的としています。主に、特定の言葉を当てるための「20の質問」ゲームへのAIエージェントの参加を支援するもので、特定の言語モデルを利用しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "ノートブックは、プレイヤーが「はい」または「いいえ」で質問を行い、ターゲットとなる言葉を推測するゲームを支援するために、言語モデルを使用して効果的かつ戦略的な質問と推測を行うためのエージェントを構築しています。このゲームでは、効率よく情報を収集し、限られた質問を通じて答えを導き出す能力が求められます。\n",
    "\n",
    "### 使用している手法およびライブラリ\n",
    "1. **モデルの選択と依存関係のインストール**:\n",
    "   - `bitsandbytes`と`accelerate`、最新の`transformers`ライブラリを使用して、モデルの量子化と効率的なメモリ管理を行います。\n",
    "   - HuggingFaceから選択した言語モデル（LLAMA3、Phi-3、Qwen-2バリアント）を使用します。\n",
    "\n",
    "2. **モデルのダウンロードと設定**:\n",
    "   - HuggingFaceからモデルをダウンロードし、量子化された設定でロードします。これにより、ストレージ使用量を削減し、メモリ効率を最適化します。\n",
    "   \n",
    "3. **プロンプト設計**:\n",
    "   - AI エージェント用のプロンプトを設計し、質問、推測、回答の各モードに応じたプロンプトを構築しています。プロンプトは、モデルにコンテキストを提供し、効果的な質問や推測を行うために必要です。\n",
    "\n",
    "4. **AIエージェントの実装**:\n",
    "   - モデルから生成されるレスポンスに基づいて、エージェントが質問する機能 (asker)、推測する機能 (guesser)、および答える機能 (answerer) を持つクラス `Robot` を定義しています。\n",
    "\n",
    "5. **シミュレーションとテスト**:\n",
    "   - Kaggle Environmentsを使用して、AIエージェントとダムエージェントとの対戦をシミュレーションし、ゲームの結果やエージェントの性能を確認します。\n",
    "\n",
    "このノートブックは、言語モデルを用いて「20の質問」ゲームでの対話の流れを形成し、ゲームの戦略を最適化するための基盤を提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96036885",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "こちらのJupyter Notebookに関連する専門用語とその簡単な解説を以下に示します。特に初心者が混乱しやすい点や、このノートブック特有の用語にフォーカスしました。\n",
    "\n",
    "1. **トークナイザー (Tokenizer)**:\n",
    "   - テキストをトークン（単語やフレーズなどの基本的な単位）に分割するためのツールです。モデルはトークン処理を行うため、自然言語を数値的な表現に変換します。ノートブックにおいては、`tokenizer.apply_chat_template` というメソッドを用いて特別なトークンが適用されています。\n",
    "\n",
    "2. **量子化 (Quantization)**:\n",
    "   - モデルのパラメータを低精度（例えば、16ビットや8ビット）で表現することを指します。これにより、メモリ使用量を削減し、計算速度を向上させる目的があります。ノートブックでは、モデルをメモリに量子化してロードすることでストレージを節約しています。\n",
    "\n",
    "3. **HuggingFace モデルハブ**:\n",
    "   - HuggingFaceが提供する、さまざまなトランスフォーマーベースのモデルが保存されているオンラインリポジトリです。ユーザーはここからモデルを検索し、ダウンロードして使用することができます。\n",
    "\n",
    "4. **リモートコードの信頼 (trust_remote_code)**:\n",
    "   - 外部ソースからのコードを実行する際の安全性を確保するためのパラメータです。信頼できるソースからのコードである場合に設定して、リモートからのモデルや定義を読み込むことを許可します。\n",
    "\n",
    "5. **メモリ効率の良い分散処理 (Memory-efficient distributed processing)**:\n",
    "   - 複数のGPUやノードを用いて大規模なデータやモデルを効率良く処理する方法であり、メモリの使用量を抑えながら計算を分散させることを指します。ノートブック内では、この機能が無効にされています。\n",
    "\n",
    "6. **ターン終了トークン (Turn terminators)**:\n",
    "   - 会話の一部が終了したことを示すトークンで、モデルが次の反応を生成する際にどの時点で終わったかを認識するために使用されます。ノートブックでは、追加のターン終了トークンを定義して管理しています。\n",
    "\n",
    "7. **プロンプト (Prompt)**:\n",
    "   - モデルに対して出力を生成させるための入力テキストや指示のことです。ノートブックでは、「20の質問」ゲームのルールや目的に基づいた指示が与えられ、モデルがその指示に従って応答を生成します。\n",
    "\n",
    "8. **Kaggle 環境 (Kaggle environment)**:\n",
    "   - Kaggleのコンペティションやプロジェクトに特化したシミュレーション環境であり、エージェントが対戦するゲーム上での実行環境を指します。これは、Kaggleの`kaggle_environments`ライブラリを使用して構築されます。\n",
    "\n",
    "これらの解説が、Jupyter Notebookを利用する際の理解を助け、学習に役立つことを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de1bcee",
   "metadata": {},
   "source": [
    "# LLM 20 Questions ベースライン\n",
    "\n",
    "私は `tokenizer.apply_chat_template` を使用して特別なトークンを自動的に適用したので、モデルやプロンプトを便利に変更できます。\n",
    "\n",
    "\n",
    "サポートされているモデル:\n",
    "- `LLAMA3 バリアント`\n",
    "- `Phi-3 バリアント`\n",
    "- `Qwen-2 バリアント`\n",
    "\n",
    "## 前提条件\n",
    "アクセラレーターを GPU T4 に設定してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-01T02:42:36.432851Z",
     "iopub.status.busy": "2024-08-01T02:42:36.432507Z",
     "iopub.status.idle": "2024-08-01T02:43:16.74253Z",
     "shell.execute_reply": "2024-08-01T02:43:16.741715Z",
     "shell.execute_reply.started": "2024-08-01T02:42:36.432822Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# submissionという名前のディレクトリを作成します。\n",
    "mkdir -p /kaggle/working/submission\n",
    "\n",
    "# modelという名前の一時ディレクトリを作成します。\n",
    "mkdir -p /tmp/model\n",
    "\n",
    "# bitsandbytesとaccelerateをインストールします。\n",
    "pip install -q bitsandbytes accelerate\n",
    "\n",
    "# transformersを最新版にアップグレードしてインストールします。\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a878b",
   "metadata": {},
   "source": [
    "## モデルのダウンロード\n",
    "\n",
    "### HuggingFace ログイン\n",
    "\n",
    "1. HuggingFace アクセストークンを発行します（https://huggingface.co/settings/tokens）。\n",
    "\n",
    "2. HuggingFace アクセストークンをシークレットに追加します。シークレットは `Add-ons -> secrets` で見つけることができます。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot 2024-08-01 at 11.40.17 AM.png](attachment:fb5805e5-566e-41f1-ba50-0d9f9fade571.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.744239Z",
     "iopub.status.busy": "2024-08-01T02:43:16.743959Z",
     "iopub.status.idle": "2024-08-01T02:43:16.882488Z",
     "shell.execute_reply": "2024-08-01T02:43:16.88177Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.744215Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# HuggingFaceのトークンを保存する変数を初期化します。\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "# シークレットからHuggingFaceトークンを取得します。\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    # トークンの取得に失敗した場合は何もしません。\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64804d00",
   "metadata": {},
   "source": [
    "### モデルの選択\n",
    "\n",
    "希望するモデルを [HuggingFace モデルハブ](https://huggingface.co/models) から見つけ、そのモデル名を次のコマンドで使用してください。\n",
    "\n",
    "サポートされているモデル:\n",
    "- `LLAMA3 バリアント`\n",
    "- `Phi-3 バリアント`\n",
    "- `Qwen-2 バリアント`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.883834Z",
     "iopub.status.busy": "2024-08-01T02:43:16.883562Z",
     "iopub.status.idle": "2024-08-01T02:43:16.887672Z",
     "shell.execute_reply": "2024-08-01T02:43:16.886858Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.883809Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用するモデルのリポジトリIDを設定します。\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c6343",
   "metadata": {},
   "source": [
    "### HuggingFace経由でモデルをダウンロード\n",
    "\n",
    "ディスク使用量を削減するために、モデルを `/tmp/model` にダウンロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.890807Z",
     "iopub.status.busy": "2024-08-01T02:43:16.890312Z",
     "iopub.status.idle": "2024-08-01T02:46:44.781238Z",
     "shell.execute_reply": "2024-08-01T02:46:44.78017Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.890775Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# モデルの保存先パスを設定します。\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "\n",
    "# 既にモデルのパスが存在する場合は、そのディレクトリを削除します。\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "\n",
    "# モデルを保存するための新しいディレクトリを作成します。\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "# HuggingFaceからモデルをダウンロードします。\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,  # 使用するモデルのリポジトリID\n",
    "    ignore_patterns=\"original*\",  # 元のファイルを無視するためのパターン\n",
    "    local_dir=g_model_path,  # ダウンロード先のローカルディレクトリ\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # HuggingFaceトークン\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:44.783114Z",
     "iopub.status.busy": "2024-08-01T02:46:44.78267Z",
     "iopub.status.idle": "2024-08-01T02:46:45.823393Z",
     "shell.execute_reply": "2024-08-01T02:46:45.82208Z",
     "shell.execute_reply.started": "2024-08-01T02:46:44.783078Z"
    }
   },
   "outputs": [],
   "source": [
    "# /tmp/model ディレクトリ内のファイルとフォルダをリスト表示します。\n",
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66994dc7",
   "metadata": {},
   "source": [
    "### 量子化されたモデルを保存\n",
    "\n",
    "ダウンロードしたモデルをメモリに量子化してロードします。  \n",
    "これにより、ストレージを節約できます。\n",
    "\n",
    "さらに、保存されたモデルはすでに量子化されているため、`main.py`では `bitsandbytes` パッケージを使用する必要はありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:45.825488Z",
     "iopub.status.busy": "2024-08-01T02:46:45.825093Z",
     "iopub.status.idle": "2024-08-01T02:47:26.827498Z",
     "shell.execute_reply": "2024-08-01T02:47:26.826601Z",
     "shell.execute_reply.started": "2024-08-01T02:46:45.825451Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルをメモリにロードします。\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# メモリ効率の良い分散処理を無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# ダウンロードされたモデルのパスを設定します。\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "# 量子化の設定を行います。\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,  # 4ビットでのロードを有効にします。\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算時のデータ型を浮動小数点16ビットに設定します。\n",
    ")\n",
    "\n",
    "# 量子化されたモデルを事前に読み込みます。\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,  # 読み込むモデルのパス\n",
    "    quantization_config = bnb_config,  # 量子化設定\n",
    "    torch_dtype = torch.float16,  # モデルのデータタイプを浮動小数点16ビットに設定します。\n",
    "    device_map = \"auto\",  # デバイスマッピングを自動設定します。\n",
    "    trust_remote_code = True,  # リモートコードの信頼を有効にします。\n",
    ")\n",
    "\n",
    "# 対応するトークナイザーを読み込みます。\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:26.829232Z",
     "iopub.status.busy": "2024-08-01T02:47:26.82879Z",
     "iopub.status.idle": "2024-08-01T02:47:44.162649Z",
     "shell.execute_reply": "2024-08-01T02:47:44.161658Z",
     "shell.execute_reply.started": "2024-08-01T02:47:26.829206Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルを提出用ディレクトリに保存します。\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")  # モデルを指定のパスに保存します。\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")  # トークナイザーを指定のパスに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.164433Z",
     "iopub.status.busy": "2024-08-01T02:47:44.164008Z",
     "iopub.status.idle": "2024-08-01T02:47:44.529886Z",
     "shell.execute_reply": "2024-08-01T02:47:44.529036Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.164407Z"
    }
   },
   "outputs": [],
   "source": [
    "# メモリからモデルをアンロードします。\n",
    "import gc, torch\n",
    "\n",
    "# モデルとトークナイザーを削除します。\n",
    "del model, tokenizer\n",
    "\n",
    "# ガーベジコレクションを実行してメモリを解放します。\n",
    "gc.collect()\n",
    "\n",
    "# CUDAキャッシュを空にします。\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653773a",
   "metadata": {},
   "source": [
    "## エージェント\n",
    "\n",
    "### プロンプト\n",
    "\n",
    "プロンプトは [Anthropic プロンプトライブラリ](https://docs.anthropic.com/en/prompt-library/library) から参照されます。\n",
    "\n",
    "プロンプトは2つの部分で構成されています：\n",
    "- `system_prompt`: これはカテゴリを決定するための最初の質問です。\n",
    "- `chat_history`: これはモデルにコンテキストを提供するためのチャット履歴です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.531437Z",
     "iopub.status.busy": "2024-08-01T02:47:44.531097Z",
     "iopub.status.idle": "2024-08-01T02:47:45.526013Z",
     "shell.execute_reply": "2024-08-01T02:47:45.524961Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.531412Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "# プロンプトを管理するための関数を定義します。\n",
    "\n",
    "def asker_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    ask_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイすることに熟練した役立つAIアシスタントです。\n",
    "ユーザーが考えている言葉を当てるために質問をするのがあなたの役割です。\n",
    "はい/いいえの質問をすることによって可能性を絞り込んでください。\n",
    "段階的に考え、最も有益な質問をするようにしてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # チャット履歴の追加\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})  # 質問\n",
    "        message.append({\"role\": \"user\", \"content\": a})  # 回答\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    guess_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイすることに熟練した役立つAIアシスタントです。\n",
    "ユーザーが考えている言葉を当てるのがあなたの役割です。\n",
    "段階的に考えてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    # チャット履歴の構築\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "    \n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"これまでのゲームの状態は次の通りです:\\n{chat_history}\n",
    "        この会話に基づいて、言葉を当てることができますか？言葉だけを答えて、冗長な説明は不要です。\"\"\"\n",
    "    )\n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    prompt = f\"\"\"あなたは「20の質問」ゲームをプレイすることに熟練した役立つAIアシスタントです。\n",
    "ユーザーが考えている言葉を当てるのを助けるために質問に答えるのがあなたの役割です。\n",
    "あなたの回答は「はい」または「いいえ」でなければなりません。\n",
    "キーワードは: \"{obs.keyword}\" です、それはカテゴリ: \"{obs.category}\" のものです。\n",
    "ユーザーがキーワードを推測するのを助けるために正確な回答を提供してください。\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # チャット履歴の追加\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})  # 最初の質問\n",
    "    \n",
    "    if len(obs.answers) >= 1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})  # 回答\n",
    "            message.append({\"role\": \"user\", \"content\": q})  # 次の質問\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c983e1",
   "metadata": {},
   "source": [
    "### エージェント\n",
    "\n",
    "より多くのLLMモデルを追加するには、ターン終了トークンを終端子リストに追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.530407Z",
     "iopub.status.busy": "2024-08-01T02:47:45.530085Z",
     "iopub.status.idle": "2024-08-01T02:47:45.539942Z",
     "shell.execute_reply": "2024-08-01T02:47:45.538701Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.53038Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "# メモリ効率の良い分散処理を無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# Kaggleのエージェントパスを設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "# モデルを事前に読み込みます。\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",  # デバイスマッピングを自動設定します。\n",
    "    trust_remote_code=True,  # リモートコードの信頼を有効にします。\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 使用するターン終了トークンを指定します。\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "# 追加のターン終了トークンの可能性\n",
    "# llama3, phi3, gwen2 の順で\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "# ポテンシャルターン終了トークンをterminatorsリストに追加します。\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "# レスポンスを生成する関数\n",
    "def generate_response(chat):\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)  # 入力テンソルを作成します。\n",
    "    outputs = model.generate(inputs, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)  # モデルから出力を生成します。\n",
    "    response = outputs[0][inputs.shape[-1]:]  # 出力の一部を取り出します。\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)  # デコードして文字列に戻します。\n",
    "\n",
    "    return out\n",
    "\n",
    "# Robotクラスを定義します。\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問する役割を実行します。\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 回答する役割を実行します。\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # 推測する役割を実行します。\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        input = asker_prompt(obs)  # 質問用のプロンプトを取得します。\n",
    "        output = generate_response(input)  # レスポンスを生成します。        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        input = guesser_prompt(obs)  # 推測用のプロンプトを取得します。\n",
    "        output = generate_response(input)  # レスポンスを生成します。\n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)  # 回答用のプロンプトを取得します。\n",
    "        output = generate_response(input)  # レスポンスを生成します。\n",
    "        return output\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "# エージェント関数を定義します。\n",
    "def agent(obs, cfg):\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)  # 質問の場合\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)  # 推測の場合\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)  # 回答の場合\n",
    "\n",
    "    # レスポンスが空または短すぎる場合には「yes」とする。\n",
    "    if response == None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "\n",
    "    return response  # レスポンスを返します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf90d1",
   "metadata": {},
   "source": [
    "## シミュレーション\n",
    "\n",
    "### pygameのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.541971Z",
     "iopub.status.busy": "2024-08-01T02:47:45.541077Z",
     "iopub.status.idle": "2024-08-01T02:48:02.91265Z",
     "shell.execute_reply": "2024-08-01T02:48:02.911458Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.541938Z"
    }
   },
   "outputs": [],
   "source": [
    "# pygameをインストールします。\n",
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.914553Z",
     "iopub.status.busy": "2024-08-01T02:48:02.914201Z",
     "iopub.status.idle": "2024-08-01T02:48:02.921087Z",
     "shell.execute_reply": "2024-08-01T02:48:02.920176Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.914522Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile dumb.py\n",
    "\n",
    "# ダムエージェントを定義します。\n",
    "def dumb_agent(obs, cfg):\n",
    "    \n",
    "    # エージェントが推測者で、ターンタイプが「ask」の場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"それはカモですか？\"  # 質問を返します。\n",
    "    # エージェントが推測者で、ターンタイプが「guess」の場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"カモ\"  # 推測を返します。\n",
    "    # エージェントが回答者の場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"いいえ\"  # 回答を返します。\n",
    "    \n",
    "    return response  # レスポンスを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.922495Z",
     "iopub.status.busy": "2024-08-01T02:48:02.922131Z",
     "iopub.status.idle": "2024-08-01T02:49:45.023058Z",
     "shell.execute_reply": "2024-08-01T02:49:45.022131Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.922463Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# kaggle_environmentsライブラリを使用して環境を作成します。\n",
    "from kaggle_environments import make\n",
    "\n",
    "# \"llm_20_questions\" 環境をデバッグモードで作成します。\n",
    "env = make(\"llm_20_questions\", debug=True)\n",
    "\n",
    "# 2つのメインエージェントと2つのダムエージェントでゲームを実行します。\n",
    "game_output = env.run(agents=[\"submission/main.py\", \"submission/main.py\", \"dumb.py\", \"dumb.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:51:55.095662Z",
     "iopub.status.busy": "2024-08-01T02:51:55.095291Z",
     "iopub.status.idle": "2024-08-01T02:51:55.156977Z",
     "shell.execute_reply": "2024-08-01T02:51:55.155859Z",
     "shell.execute_reply.started": "2024-08-01T02:51:55.095636Z"
    }
   },
   "outputs": [],
   "source": [
    "# ゲームの結果をIPython環境で表示します。\n",
    "env.render(mode=\"ipython\", width=600, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64082156",
   "metadata": {},
   "source": [
    "## エージェントの提出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:45.091747Z",
     "iopub.status.busy": "2024-08-01T02:49:45.091472Z",
     "iopub.status.idle": "2024-08-01T02:49:51.432444Z",
     "shell.execute_reply": "2024-08-01T02:49:51.431242Z",
     "shell.execute_reply.started": "2024-08-01T02:49:45.091723Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします。出力は表示しません。\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:51.435019Z",
     "iopub.status.busy": "2024-08-01T02:49:51.434137Z",
     "iopub.status.idle": "2024-08-01T02:51:27.838366Z",
     "shell.execute_reply": "2024-08-01T02:51:27.836645Z",
     "shell.execute_reply.started": "2024-08-01T02:49:51.434979Z"
    }
   },
   "outputs": [],
   "source": [
    "# submissionディレクトリを圧縮し、tar.gzファイルを作成します。\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 追加の操作は行われていません。必要に応じてコードを追加してください。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
