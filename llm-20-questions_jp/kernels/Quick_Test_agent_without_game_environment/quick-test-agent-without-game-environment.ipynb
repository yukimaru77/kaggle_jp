{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2c3fc7e",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleのコンペティション「LLM 20 Questions」に参加するためのエージェントを開発することを目的としています。具体的には、20の質問ゲームの質問者（質問を行うAI）と回答者（「はい」または「いいえ」で答えるAI）の役割を果たす2種類のエージェントを実装しています。\n",
    "\n",
    "### 問題の概要\n",
    "ノートブックは、質問者エージェントおよび回答者エージェントがどうにかしてターゲットワードを推測できるように設計されています。エージェントは、過去の質問と回答をもとに次の質問を生成したり、与えられた質問に適切に答えたりする必要があります。このゲームの制約に基づき、効率的な情報収集と推論が求められます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **Gemmaモデル**: ノートブックでは、`gemma_pytorch`リポジトリからGemma（Causal Language Model）を使用しています。これにより、モデルは質問応答を行い、ターゲットワードの推測を助けます。\n",
    "- **PyTorch**: モデルを構築し、深層学習の計算を行うために使用されています。GPUアクセラレーターを活用して効率的に処理を行います。\n",
    "- **クラス設計**:\n",
    "    - `GemmaFormatter`クラス: プロンプトのフォーマッティングを行います。ゲームのターンごとにユーザーとモデルの入力を管理します。\n",
    "    - `GemmaAgent`クラス: ゲームのエージェントのベースクラスで、質問者エージェント(`GemmaQuestionerAgent`)と回答者エージェント(`GemmaAnswererAgent`)の機能が含まれます。これらのクラスは、ゲームのセッションを管理し、応答を生成する役割を担っています。\n",
    "- **関数**: `agent_fn`は、観察オブジェクト（ゲームの状態）に基づいて、エージェントを呼び出し、適切な回答を得る関数です。\n",
    "  \n",
    "### 実装手順\n",
    "1. 必要なライブラリのインストールとリポジトリのクローンを行う。\n",
    "2. システムパスを設定し、モデルの重みをロードする。\n",
    "3. `GemmaAgent`クラスのインスタンスを生成し、質問者として機能する場合と回答者として機能する場合の両方を設定できるようにする。\n",
    "4. 20の質問ゲームのロジックを実装し、それをテストするための観察オブジェクトを準備する。\n",
    "\n",
    "### 結論\n",
    "このノートブックは、20の質問ゲームにおける質問生成および回答生成のための強化されたエージェントを開発するための基盤を提供し、Gemmaという言語モデルを利用して、効果的なゲームプレイを実現しようとしています。これは、自然言語処理と機械学習技術の実用的な応用例であり、特に制約のある環境での戦略的な応答生成に焦点を当てています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a609a",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "このJupyter Notebook内で初心者がつまずきそうな専門用語や概念について、簡単に解説します。\n",
    "\n",
    "1. **マジックコマンド**: Jupyter Notebook内で特別な機能を提供するコマンドで、通常のPythonコードとは異なる方法で実行されます。例えば、`%%bash`はBashシェルコマンドを実行するために使用されます。\n",
    "\n",
    "2. **immutabledict**: Pythonの標準の辞書型ではなく、変更不可能な辞書の実装です。データの不変性が保証され、プログラムの安全性が向上します。特に、ハッシュ可能性が必要な場合に使用されます。\n",
    "\n",
    "3. **sentencepiece**: 自然言語処理において、文章をトークン化するためのライブラリで、特にサブワードトークン化を使用します。これは、単語の部分を考慮することで、未知の単語にも対応可能になります。\n",
    "\n",
    "4. **GemmaForCausalLM**: 特定の言語モデルであり、因果関係に基づいて次の単語を予測するためのものです。ここでは、質問応答のために設計されたモデルとして使用されています。\n",
    "\n",
    "5. **contextlib**: Pythonの標準ライブラリで、コンテキストマネージャを作成するためのユーティリティを提供します。リソースを管理しやすくするために使用されます。\n",
    "\n",
    "6. **dtype**: データ型（data type）を指します。PyTorchの`torch.dtype`は、テンソルが保持するデータの型を指定するのに使われます。例としては、整数型や浮動小数点型などがあります。\n",
    "\n",
    "7. **量子化 (quantization)**: 深層学習モデルのサイズを縮小し、推論速度を向上させるために使用される技術です。モデルの重みを少ないビット数（通常は8ビット）で表現します。これによりメモリ使用量が削減されますが、精度の低下が懸念されます。\n",
    "\n",
    "8. **トークナイザー**: テキストをトークン（単語やサブワード）に変換するコンポーネントです。モデルにデータを供給する際の前処理ステップとして重要です。\n",
    "\n",
    "9. **ask, answer, guess**: これらは「ターンタイプ」と呼ばれ、エージェントが何をするかを示す状態を表します。それぞれ質問をする、質問の答えを出す、または答えを推測することを意味します。\n",
    "\n",
    "10. **エージェント**: 特定のタスクを実行するために設計されたプログラムまたはモデルです。このノートブックでは、質問をするエージェント（質問者）と、答えを提供するエージェント（回答者）が登場します。\n",
    "\n",
    "11. **プロンプト**: モデルに入力を与えるためのテキストや情報です。プロンプトの設計は、応答の質や関連性に大きな影響を与えます。\n",
    "\n",
    "12. **few-shot examples**: モデルが少数の例から学習し、新たなタスクを遂行するための技術です。これにより、少量のデータからでも効果的にパフォーマンスを発揮できるようになります。\n",
    "\n",
    "これらの用語や概念は、特に実務経験のない初心者にとっては直感的でないことがありますので、理解を深めるためにこれらの解説が役立つでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-24T17:22:53.868142Z",
     "iopub.status.busy": "2024-06-24T17:22:53.86777Z",
     "iopub.status.idle": "2024-06-24T17:22:53.882128Z",
     "shell.execute_reply": "2024-06-24T17:22:53.88122Z",
     "shell.execute_reply.started": "2024-06-24T17:22:53.868112Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "あなたの提出コードはここに記述する必要があります。 \n",
    "提出コードと.pyファイルを書き出すためのマジックコマンドをコメントアウトしてください。\n",
    "シミュレーションされたゲームの値を渡すことができるobsクラスを作成するコードブロックを末尾に追加します。\n",
    "GPUアクセラレーターを有効にして実行してください。\n",
    "参照のために、下記にスターターノートブックが追加されています。\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db77a7",
   "metadata": {},
   "source": [
    "## スターターノートブックの例\n",
    "テスターをより早く見るために、2bに変更しただけです。 \n",
    "\n",
    "\n",
    "https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook?kernelSessionId=178755035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T17:22:53.884654Z",
     "iopub.status.busy": "2024-06-24T17:22:53.883806Z",
     "iopub.status.idle": "2024-06-24T17:23:07.867585Z",
     "shell.execute_reply": "2024-06-24T17:23:07.866482Z",
     "shell.execute_reply.started": "2024-06-24T17:22:53.884628Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリに移動します\n",
    "cd /kaggle/working\n",
    "\n",
    "# immutabledict と sentencepiece を指定したフォルダーにインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "\n",
    "# gemma_pytorchリポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "\n",
    "# gemma用の新しいディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "# gemma_pytorchのファイルを指定したディレクトリに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T17:23:07.869823Z",
     "iopub.status.busy": "2024-06-24T17:23:07.869512Z",
     "iopub.status.idle": "2024-06-24T17:23:11.133124Z",
     "shell.execute_reply": "2024-06-24T17:23:11.132237Z",
     "shell.execute_reply.started": "2024-06-24T17:23:07.869797Z"
    }
   },
   "outputs": [],
   "source": [
    "#%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** システムパスをこのように設定し、ノートブックとシミュレーション環境の両方でコードが動作するようにします。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマッティング\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"デフォルトのtorchのdtypeを指定されたdtypeに設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を担います。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を担います。キーワードは {obs.keyword} で、カテゴリは {obs.category} です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"この質問はキーワード {obs.keyword} に関するものです。はいまたはいいえで答えてください。答えは二重アスタリスクで囲んでください、例: **yes** または **no**。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを思いつき、質問者がはいまたはいいえの質問をします。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を担います。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** では、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバル変数として定義し、必要なエージェントだけをロードします。\n",
    "# 両方をロードすると、OOMになる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30000b81",
   "metadata": {},
   "source": [
    "提出ファイルの作成がコメントアウトされました"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T17:23:11.134838Z",
     "iopub.status.busy": "2024-06-24T17:23:11.134419Z",
     "iopub.status.idle": "2024-06-24T17:23:11.139062Z",
     "shell.execute_reply": "2024-06-24T17:23:11.137894Z",
     "shell.execute_reply.started": "2024-06-24T17:23:11.134811Z"
    }
   },
   "outputs": [],
   "source": [
    "# !apt install pigz pv > /dev/null\n",
    "# pigzとpvをインストールしますが、出力は表示しません\n",
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "# 提出物のtar.gzアーカイブを作成します、圧縮はpigzを使用してスピードアップし、pvで進行状況を表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58658c7",
   "metadata": {},
   "source": [
    "## クイックテスト\n",
    "\n",
    "これは質問者と推測者エージェントに適用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T17:43:00.268252Z",
     "iopub.status.busy": "2024-06-24T17:43:00.267511Z",
     "iopub.status.idle": "2024-06-24T17:43:14.394535Z",
     "shell.execute_reply": "2024-06-24T17:43:14.393556Z",
     "shell.execute_reply.started": "2024-06-24T17:43:00.268223Z"
    }
   },
   "outputs": [],
   "source": [
    "class obs(object):\n",
    "    '''\n",
    "    このクラスはゲームからの観察を表します。 \n",
    "    turnTypeは['ask','answer','guess']のいずれかである必要があります。\n",
    "    テストしたいキーワードを選択してください。\n",
    "    キーワードのカテゴリを選択してください。\n",
    "    質問は文字列のリストにできます。エージェントに過去の質問の履歴を渡す場合に使用します。\n",
    "    回答も文字列のリストにできます。\n",
    "    responseを使用して質問を回答者エージェントに渡します。\n",
    "    '''\n",
    "    def __init__(self, turnType, keyword, category, questions, answers):\n",
    "        self.turnType = turnType\n",
    "        self.keyword = keyword\n",
    "        self.category = category\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "\n",
    "#エージェントに渡すobsの例。ここに独自のobsを渡してください。\n",
    "obs1 = obs(turnType='ask', \n",
    "          keyword='paris', \n",
    "          category='place', \n",
    "          questions=['これは人ですか？', 'これは場所ですか？', 'これは国ですか？', 'これは都市ですか？'], \n",
    "          answers=['いいえ', 'はい', 'いいえ', 'はい'])\n",
    "\n",
    "question = agent_fn(obs1, None)\n",
    "print(question)\n",
    "\n",
    "#出力としてエージェントからの応答を得る必要があります。 \n",
    "#これは提出エラーの助けにはなりませんが、エージェントの応答を評価するのには役立ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-24T17:43:24.321032Z",
     "iopub.status.busy": "2024-06-24T17:43:24.320631Z",
     "iopub.status.idle": "2024-06-24T17:43:38.431991Z",
     "shell.execute_reply": "2024-06-24T17:43:38.431022Z",
     "shell.execute_reply.started": "2024-06-24T17:43:24.320995Z"
    }
   },
   "outputs": [],
   "source": [
    "obs2 = obs(turnType='guess', \n",
    "          keyword='paris', \n",
    "          category='place', \n",
    "          questions=['これは人ですか？', 'これは場所ですか？', 'これは国ですか？', 'これは都市ですか？', 'キーワードは雨が多いですか？'], \n",
    "          answers=['いいえ', 'はい', 'いいえ', 'はい', 'いいえ'])\n",
    "\n",
    "agent_fn(obs2, None)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
