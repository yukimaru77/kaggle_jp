{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ec1c550",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、「20の質問」ゲームに特化した言語モデルを実装するためのエージェントを構築しています。具体的には、質問者エージェント（`Llama3QuestionerAgent`）と回答者エージェント（`Llama3AnswererAgent`）の2つのエージェントを作成し、ゲームルールに従って対話を行えるようにします。\n",
    "\n",
    "### 問題の取り組み\n",
    "- **目的**: 20の質問ゲームをプレイするためのAIエージェントを開発し、ターゲットワードに関連する質問を生成し、回答を提供する。\n",
    "- **ゲームのルール**: 質問者エージェントがターゲットワードに対して質問をし、回答者エージェントが「はい」または「いいえ」で応答します。この両者の協力によって、できるだけ早くターゲットを特定することを目指します。\n",
    "\n",
    "### 手法とライブラリ\n",
    "1. **Transformersライブラリ**:\n",
    "   - `AutoTokenizer`と`AutoModelForCausalLM`を用いて、事前訓練された言語モデルをロードします。具体的には、LLAMA-3モデルを使用して、そのプロンプトや出力形式を管理します。\n",
    "2. **PyTorch**:\n",
    "   - モデルの生成時には、PyTorchを利用して効率的に計算を実行します。特に、メモリ効率を高めるためにSDP（システムデータパラメータ）を調整します。\n",
    "3. **状態管理**:\n",
    "   - `Llama3Formatter`クラスを使って、ユーザーの入力やモデルの出力、システムのプロンプトを状態として管理し、ゲームの流れをスムーズにします。\n",
    "\n",
    "### エージェントの構成\n",
    "- **質問者エージェント**:\n",
    "  - ゲームの進行に従い、対象の単語が「場所」であるかを特定したり、単語の最初や二文字目の範囲を絞り込んだりします。\n",
    "- **回答者エージェント**:\n",
    "  - 質問に対して「はい」または「いいえ」といった応答をし、ターゲットの単語に対する情報を提供します。\n",
    "\n",
    "### その他の構成要素\n",
    "- **出力の保存**: 最後に、生成したエージェントのコードを一つのtar.gzファイルにアーカイブし、次のステップでの提出に備えるために整理しています。\n",
    "- **エラーハンドリング**: 応答が無効な場合にはデフォルトの「はい」を返すなど、冗長性のある設計がなされています。\n",
    "\n",
    "このノートの内容は、20の質問ゲームを自動化し、AIが協力して効率的にターゲットを推測する能力を示すものとなっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1903719",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebook内で使用されている専門用語の簡単な解説を示します。初心者でもある程度の知識がある前提のため、深く掘り下げた説明は省いています。\n",
    "\n",
    "1. **KAGGLE_AGENT_PATH**:\n",
    "   - Kaggleのシミュレーション環境内でエージェントのライブラリが格納されているパスを指定します。このパスを使うことで、ノートブックとシミュレーション環境でコードが正しく動作するように設定します。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**:\n",
    "   - テキストデータをモデルが理解可能なトークン（単語やサブワードなどの単位）に変換するためのツールです。モデルの入力として使用されるため、重要です。\n",
    "\n",
    "3. **Causal LM (因果言語モデル)**:\n",
    "   - 与えられたテキストの文脈に基づいて次の単語を予測するためのモデル。通常の言語モデルは全トークンを考慮しますが、因果モデルはこれに制約があり、現在のトークンより前のトークンのみを考慮します。\n",
    "\n",
    "4. **メモリ効率の良いSDP** (Memory Efficient SDP):\n",
    "   - メモリ効率を高めるための技術で、大規模な深層学習モデルのトレーニングや推論を行う際に必要とされるメモリの使用を削減します。\n",
    "\n",
    "5. **bfloat16**:\n",
    "   - 16ビットのブレイン浮動小数点形式で、深層学習においてメモリ使用量を抑えつつ、計算精度を維持するために用いられるデータ型です。\n",
    "\n",
    "6. **プロンプト (Prompt)**:\n",
    "   - モデルに対して提供する入力のこと。例えば質問やテーマなどを含み、モデルがどのように応答するかを決定づけます。\n",
    "\n",
    "7. **Few-shot examples**:\n",
    "   - モデルに少数の参考例（少数ショット例）を与え、特定のタスクにおいてどのように振る舞うべきかを学習させる手法。例を基にモデルが自己修正を行いやすくなります。\n",
    "\n",
    "8. **エージェント (Agent)**:\n",
    "   - ある特定のタスクを実行するために設計されたプログラムやモデルのこと。ここでは「質問者」や「回答者」としての特定のロールを持つエージェントが扱われています。\n",
    "\n",
    "9. **interleave_unequal**:\n",
    "   - 異なる長さのリストを交互に組み合わせるための関数。異なる情報源からの情報を組み合わせることにより、より効率的なデータ処理を行います。\n",
    "\n",
    "10. **Top-pサンプリング**:\n",
    "    - 確率分布に基づいて上位pの確率質量を持つ単語の中から新しいトークンをサンプリングする手法。生成する内容に多様性を持たせます。\n",
    "\n",
    "これらの用語は、このノートブックの理解や、機械学習・深層学習の実践において特に重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:45.176925Z",
     "iopub.status.busy": "2024-07-22T18:14:45.176479Z",
     "iopub.status.idle": "2024-07-22T18:14:45.188448Z",
     "shell.execute_reply": "2024-07-22T18:14:45.187358Z",
     "shell.execute_reply.started": "2024-07-22T18:14:45.176887Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 新しいディレクトリを作成します\n",
    "# -pオプションは、親ディレクトリも必要に応じて作成します\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:54.171348Z",
     "iopub.status.busy": "2024-07-22T18:14:54.170957Z",
     "iopub.status.idle": "2024-07-22T18:14:54.187649Z",
     "shell.execute_reply": "2024-07-22T18:14:54.186496Z",
     "shell.execute_reply.started": "2024-07-22T18:14:54.171301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 設定\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** このようにシステムパスを設定して、ノートブックとシミュレーション環境の両方でコードが正しく動作するようにします。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "# Transformersライブラリからトークナイザーとモデルをインポート\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False) # メモリ効率の良いSDPを無効に\n",
    "torch.backends.cuda.enable_flash_sdp(False) # フラッシュSDPを無効に\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"llama-3/transformers/8b-chat-hf/1\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class Llama3Formatter:\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self.reset()  # 初期状態をリセット\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self._state  # 現在の状態を辞書形式で取得\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state.append({'role': 'user', 'content': prompt})  # ユーザーの入力を状態に追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state.append({'role': 'assistant', 'content': prompt})  # モデルの出力を状態に追加\n",
    "        return self\n",
    "    \n",
    "    def system(self, prompt):\n",
    "        self._state.append({'role': 'system', 'content': prompt})  # システムのプロンプトを状態に追加\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = []  # 状態をリセット\n",
    "        if self._system_prompt is not None:\n",
    "            self.system(self._system_prompt)  # システムプロンプトがある場合は追加\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # Few-shot例を適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)  # エージェントを循環\n",
    "        for fmt, turn in zip(formatters, turns):  # プロンプトのターンを適用\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "class Llama3Agent:\n",
    "    def __init__(self, system_prompt=None, few_shot_examples=None):\n",
    "        self.formatter = Llama3Formatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)  # フォーマッタを初期化\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーを初期化\n",
    "        self.terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        \n",
    "        ### 元のモデルをロード\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            WEIGHTS_PATH,\n",
    "            device_map=\"auto\",  # デバイスの割り当てを自動で行う\n",
    "            torch_dtype=torch.bfloat16  # モデルのデータ型を指定\n",
    "        )\n",
    "\n",
    "    def __call__(self, obs, *args):  # エージェントの呼び出し\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = self.formatter.get_dict()  # プロンプトを取得\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出して応答を取得\n",
    "        response = self._parse_response(response, obs)  # 応答を解析\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # サブクラスで実装する必要がある\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            prompt,  # プロンプトをトークンに変換\n",
    "            add_generation_prompt=True,  # 生成プロンプトを追加\n",
    "            return_tensors=\"pt\",  # PyTorchテンソルとして返す\n",
    "        ).to(self.model.device)  # モデルのデバイスに移動\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,  # エンコードしたプロンプト\n",
    "            max_new_tokens=max_new_tokens,  # 最大のトークン数\n",
    "            eos_token_id=self.terminators,  # 終了トークン\n",
    "            do_sample=True,  # サンプリングを有効にする\n",
    "            temperature=0.6,  # サンプリング温度\n",
    "            top_p=0.9  # Top-pサンプリング\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)  # 応答をデコード\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # 特殊なキーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()  # 小文字に変換\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # サブクラスで実装する必要がある\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class Llama3QuestionerAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "        self.category_determined = False  # カテゴリが決定されたかどうか\n",
    "        self.is_place = False  # 場所かどうか\n",
    "        self.first_char_range = (0, 25)  # AからZまでのインデックス\n",
    "        self.second_char_range = None  # 二文字目の範囲\n",
    "        self.final_guess = None  # 最終的な推測\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        global guesses\n",
    "\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # ユーザーの質問と回答を交互に配置\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ターンを適用\n",
    "\n",
    "        if not self.category_determined:\n",
    "            self.formatter.user(\"Is it a place?\")  # 最初の質問\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2  # 中間インデックスを計算\n",
    "                mid_char = chr(65 + mid_index)  # インデックスをアルファベットに変換（0 → A, 1 → B, ... 25 → Z）\n",
    "                self.formatter.user(f\"Does the keyword start with a letter before {mid_char}?\")  # 中間文字より前かどうかを質問\n",
    "            elif self.final_guess is None:\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2  # 二文字目の範囲の中間インデックス\n",
    "                mid_char = chr(65 + mid_index)\n",
    "                self.formatter.user(f\"Does the second letter of the keyword come before {mid_char}?\")  # 二文字目に関する質問\n",
    "            else:\n",
    "                self.formatter.user(f\"Is the keyword **{self.final_guess}**?\")  # 最終的な推測の質問\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if not self.category_determined:\n",
    "            answer = self._parse_keyword(response)  # 応答からキーワードを解析\n",
    "            self.is_place = (answer == 'yes')  # 質問が「はい」の場合、場所であることを示す\n",
    "            self.category_determined = True  # カテゴリが決定\n",
    "            return \"Is it a place?\"  # 次の質問\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                answer = self._parse_keyword(response)  # 応答からキーワードを解析\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2  # 中間インデックスを計算\n",
    "                if answer == 'yes':\n",
    "                    self.first_char_range = (self.first_char_range[0], mid_index)  # 範囲を狭める\n",
    "                else:\n",
    "                    self.first_char_range = (mid_index + 1, self.first_char_range[1])  # 範囲を変更\n",
    "\n",
    "                if self.first_char_range[0] == self.first_char_range[1]:  # 範囲が一つの文字に絞られた場合\n",
    "                    self.second_char_range = (0, 25)  # 二文字目の範囲をリセット\n",
    "                    return f\"Does the keyword start with {chr(65 + self.first_char_range[0])}?\"  # 絞られた文字で質問\n",
    "                else:\n",
    "                    mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the keyword start with a letter before {mid_char}?\"  # 中間文字に基づき質問\n",
    "            elif self.final_guess is None:\n",
    "                answer = self._parse_keyword(response)  # 応答からキーワードを解析\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.second_char_range = (self.second_char_range[0], mid_index)  # 範囲を狭める\n",
    "                else:\n",
    "                    self.second_char_range = (mid_index + 1, self.second_char_range[1])  # 範囲を変更\n",
    "\n",
    "                if self.second_char_range[0] == self.second_char_range[1]:  # 範囲が一つの文字に絞られた場合\n",
    "                    first_char = chr(65 + self.first_char_range[0])\n",
    "                    second_char = chr(65 + self.second_char_range[0])\n",
    "                    self.final_guess = first_char + second_char  # 最終推測を更新\n",
    "                    return f\"Does the keyword start with {first_char}{second_char}?\"  # 最終推測で質問\n",
    "                else:\n",
    "                    mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the second letter of the keyword come before {mid_char}?\"  # 二文字目に関する質問\n",
    "            else:\n",
    "                answer = self._parse_keyword(response)  # 応答からキーワードを解析\n",
    "                if answer == 'yes':\n",
    "                    return f\"The keyword is **{self.final_guess}**.\"  # 正しい推測\n",
    "                else:\n",
    "                    self.final_guess = None  # 推測をリセット\n",
    "                    return \"Let's continue guessing.\"  # ゲームを続ける\n",
    "\n",
    "\n",
    "class Llama3AnswererAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # ユーザーの質問と回答を交互に配置\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ターンを適用\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")  # 応答の形式を指示\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # 応答からキーワードを解析\n",
    "        return 'yes' if 'yes' in answer else 'no'  # 「はい」または「いいえ」で返す\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"You are a very smart AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific place or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is the category of the keyword place?\", \"**no**\",\n",
    "    \"Is it a food?\", \"**yes** Now guess the keyword in the category things.\",\n",
    "    \"**Veggie Burger**\", \"Correct.\",  # Few-shot例の設定\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義して、必要なエージェントだけをロードします。\n",
    "# 両方をロードすると、OOMになる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Llama3QuestionerAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Llama3AnswererAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"  # エージェントが初期化されているか確認\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "turnRound = 1  # ラウンド数を初期化\n",
    "guesses = []  # 推測のリストを初期化\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turnRound\n",
    "    global guesses\n",
    "\n",
    "    if obs.turnType == \"ask\":  # 質問の場合\n",
    "        if turnRound == 1:\n",
    "            response = \"Is it a place?\"  # 最初の質問\n",
    "        else:\n",
    "            response = get_agent('questioner')(obs)  # 質問エージェントから応答を取得\n",
    "    elif obs.turnType == \"guess\":  # 推測の場合\n",
    "        response = get_agent('questioner')(obs)  # 質問エージェントから応答を取得\n",
    "        turnRound += 1  # ラウンド数を増やす\n",
    "        guesses.append(response)  # 推測をリストに追加\n",
    "    elif obs.turnType == \"answer\":  # 回答の場合\n",
    "        response = get_agent('answerer')(obs)  # 回答エージェントから応答を取得\n",
    "        turnRound += 1  # ラウンド数を増やす\n",
    "    if response is None or len(response) <= 1:  # 応答が無効な場合\n",
    "        return \"yes\"  # デフォルトの「はい」を返す\n",
    "    else:\n",
    "        return response  # 有効な応答を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:15.133496Z",
     "iopub.status.busy": "2024-07-22T18:15:15.132612Z",
     "iopub.status.idle": "2024-07-22T18:15:23.195997Z",
     "shell.execute_reply": "2024-07-22T18:15:23.194844Z",
     "shell.execute_reply.started": "2024-07-22T18:15:15.133456Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzとpvパッケージをインストールします\n",
    "# pigzは高速なgzip互換の圧縮ツールで、pvはパイプの進行状況を表示するツールです\n",
    "# インストールの出力は非表示に設定しています（> /dev/null）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:24.886486Z",
     "iopub.status.busy": "2024-07-22T18:15:24.88602Z",
     "iopub.status.idle": "2024-07-22T18:21:46.738456Z",
     "shell.execute_reply": "2024-07-22T18:21:46.736412Z",
     "shell.execute_reply.started": "2024-07-22T18:15:24.886445Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ llama-3/transformers/8b-chat-hf/1\n",
    "# tarコマンドを使用して、submission.tar.gzという名前のアーカイブを作成します\n",
    "# --use-compress-programオプションでpigzを指定し、圧縮を高速化しています\n",
    "# -Cオプションでディレクトリを変更し、submissionとllama-3の内容をアーカイブに追加します\n",
    "# pvを使用して進行状況を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:23:33.154199Z",
     "iopub.status.busy": "2024-07-22T03:23:33.153786Z",
     "iopub.status.idle": "2024-07-22T03:23:33.166077Z",
     "shell.execute_reply": "2024-07-22T03:23:33.164667Z",
     "shell.execute_reply.started": "2024-07-22T03:23:33.154165Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 新しいディレクトリを作成します\n",
    "# -pオプションは、必要に応じて親ディレクトリも作成します\n",
    "mkdir -p /kaggle/working/submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
