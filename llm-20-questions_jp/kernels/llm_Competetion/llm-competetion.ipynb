{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f57198ab",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのエージェントを作成するプロセスが示されています。エージェントは、「20の質問」ゲームをプレイするために設計されており、ユーザーの質問に対して「はい」または「いいえ」で答える役割を持つ「回答者LLM」と、その質問を行い、ターゲットを推測する役割の「質問者LLM」で構成されています。最終的に、実行すると提出用の`submission.tar.gz`ファイルが生成されます。\n",
    "\n",
    "### 問題解決のアプローチ\n",
    "1. **環境設定とライブラリのインストール**:\n",
    "   - `immutabledict`や`sentencepiece`といったライブラリをインストールし、Googleの`gemma_pytorch`リポジトリからモデルの一部を克隆します。\n",
    "\n",
    "2. **エージェントの定義**:\n",
    "   - **GemmaFormatter**クラスが実装されており、プロンプトの構成を担当します。また、質問者と回答者のターンを管理するためのメソッドを提供します。\n",
    "   - **GemmaAgent**を基にした`GemmaQuestionerAgent`および`GemmaAnswererAgent`クラスが実装され、質問者と回答者の具体的な動作を定義します。\n",
    "\n",
    "3. **モデルの初期化**:\n",
    "   - `GemmaForCausalLM`を使用してモデルが初期化され、与えられた重みを読み込みます。モデルはCUDAデバイス上で動作するように設定されています。\n",
    "\n",
    "4. **セッション管理**:\n",
    "   - 各エージェントはセッションを開始し、ユーザーからの観察（`obs`）データに基づいて適切なプロンプトを生成します。これにより、質問の生成やキーワードの推測が行われます。\n",
    "\n",
    "5. **エージェントの呼び出し**:\n",
    "   - `agent_fn`関数がエージェントを選択し、観察データに基づいてそれぞれのエージェント（質問者または回答者）を呼び出します。\n",
    "\n",
    "### 使用されているライブラリ\n",
    "- **torch**: PyTorchライブラリで深層学習モデルの構築と学習を支援します。\n",
    "- **gemma.pytorch**: モデルの設定と実行を支援するために使用されるGoogleのリポジトリからのライブラリです。\n",
    "\n",
    "このNotebookは、20の質問ゲームを効率良くプレイするためのLLMのエージェントを構築するためのフレームワークを提供し、ユーザーの質問に対する適切な応答を生成する方法を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b5f714",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連する、機械学習・深層学習において初心者がつまずきそうなマイナーな専門用語の解説を示します。\n",
    "\n",
    "1. **GemmaFormatter**:\n",
    "   - 「フォーマッタ」は、入力データ（この場合はユーザーとモデルのターン）を特定の形式に変換するクラスです。言語モデルに与えるプロンプトの構造を定義し、質問や応答を体系的に管理します。\n",
    "\n",
    "2. **GemmaForCausalLM**:\n",
    "   - これは、因果的言語モデル（Causal Language Model）を実装するクラスであり、過去の単語に基づいて次の単語を予測するために設計されています。このノートブックでは、特定のタスク（20の質問ゲーム）に応じた応答を生成するために使用されます。\n",
    "\n",
    "3. **サンプリング (sampling)**:\n",
    "   - モデルが出力する可能性のある次の単語（トークン）を選ぶための手法のことです。このノートブックでは、`temperature`, `top_p`, `top_k` などのパラメータを使用してサンプリング戦略をチューニングします。これにより生成されるテキストの多様性を調整します。\n",
    "\n",
    "4. **量子化 (quantization)**:\n",
    "   - モデルの重みを圧縮してメモリの使用を削減する技術です。このノートブックでは、モデルが「量子化」されているかどうかを指定するために用いられます。特に、ディープラーニングモデルのデプロイ時に効率を向上させるために重要です。\n",
    "\n",
    "5. **interleave_unequal関数**:\n",
    "   - 2つのリストの要素を交互に組み合わせる関数です。しかし、リストの長さが異なる場合でも、両方のリストから要素を組み合わせて新しいリストを作成します。この機能は、質問と回答のペアリングに役立ちます。\n",
    "\n",
    "6. **contextlib.contextmanager**:\n",
    "   - Pythonのコンテキストマネージャを作成するためのユーティリティです。特定のリソース（この場合はデフォルトのテンソル型）を管理し、一時的な変更を行い後に元に戻すことを容易にします。これは、メモリ管理や状態管理の効率を向上させるのに役立ちます。\n",
    "\n",
    "7. **reモジュール (re module)**:\n",
    "   - 正規表現を使用するためのPythonの標準ライブラリの一部です。このノートブックでは、質問内容や回答内容を解析し、特定のパターンを見つけ出すために使用されています。\n",
    "\n",
    "これらは、初心者にとってはあまり馴染みのない用語や概念である可能性が高く、実務を通じて初めて理解できるものが多いです。各用語がどういった役割を果たしているのか、またどのようにコード内で利用されているのかを抑えておくことが、理解を深める助けになります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T03:19:31.917628Z",
     "iopub.status.busy": "2024-05-30T03:19:31.917171Z",
     "iopub.status.idle": "2024-05-30T03:19:32.027093Z",
     "shell.execute_reply": "2024-05-30T03:19:32.025795Z",
     "shell.execute_reply.started": "2024-05-30T03:19:31.917589Z"
    }
   },
   "outputs": [],
   "source": [
    "# このノートブックは、LLM 20 Questions コンペティションにおけるエージェント作成プロセスを示しています。\n",
    "# このノートブックを実行すると、直接提出できる submission.tar.gz ファイルが生成されます。\n",
    "\n",
    "%%bash\n",
    "# 作業環境の設定\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "%%writefile submission/main.py\n",
    "# エージェント用のメインスクリプト\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# ノートブックとシミュレーション環境のためのシステムパスを設定\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "# 定数の定義\n",
    "WEIGHTS_PATH = os.path.join(\n",
    "    KAGGLE_AGENT_PATH if os.path.exists(KAGGLE_AGENT_PATH) else \"/kaggle/input\",\n",
    "    \"gemma/pytorch/7b-it-quant/2\"\n",
    ")\n",
    "\n",
    "# プロンプト形式設定クラス\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"  # ユーザーのターン\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"  # モデルのターン\n",
    "        self.reset()  # 初期化\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state  # 表示用\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)  # ユーザーのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)  # モデルのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"  # ユーザーのターンを開始\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"  # モデルのターンを開始\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"  # ターンを終了\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"  # 状態のリセット\n",
    "        if self._system_prompt:\n",
    "            self.user(self._system_prompt)  # システムプロンプトを設定\n",
    "        if self._few_shot_examples:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # Few-shot例を適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)  # フォーマッタを循環させる\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)  # 各ターンを適用\n",
    "        return self\n",
    "\n",
    "# エージェントクラスの定義\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"与えられたデータ型にデフォルトのtorchのdtypeを設定する。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)  # デフォルトデータ型をfloatに戻す\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)  # デバイス設定\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()  # モデル設定を取得\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザのパス\n",
    "        model_config.quant = \"quant\" in variant  # 量子化の設定\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):  # デフォルトデータ型の設定\n",
    "            self.model = GemmaForCausalLM(model_config)  # モデルのインスタンス化\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{variant}.ckpt')  # チェックポイントのパス\n",
    "            self.model.load_weights(ckpt_path)  # 重みを読み込む\n",
    "            self.model.to(self._device).eval()  # モデルをデバイスに移動し評価モードに設定\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = str(self.formatter)  # プロンプトを文字列として取得\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出す\n",
    "        return self._parse_response(response, obs)  # レスポンスを解析して返す\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # 未実装のメソッド\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        sampler_kwargs = sampler_kwargs or {  # サンプリング設定\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードの解析\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # 未実装のメソッド\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"2つのリストを交互に組み合わせるが、長さが異なる場合も考慮する。\"\"\"\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタのリセット\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たしています。\")  # 初期プロンプトを設定\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に組み合わせ\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいまたはいいえで答えられる質問をしてください。\")  # 質問の場合\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"さて、キーワードを推測してください。推測は二重アスタリスクで囲んでください。\")  # 推測の場合\n",
    "        self.formatter.start_model_turn()  # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # 質問の解析\n",
    "            return match.group() if match else \"それは人ですか？\"\n",
    "        elif obs.turnType == 'guess':\n",
    "            return self._parse_keyword(response)  # キーワードを解析して返す\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタのリセット\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たしています。キーワードは {obs.keyword} で、カテゴリは {obs.category} です。\")  # 初期プロンプトを設定\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に組み合わせ\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ターンを適用\n",
    "        self.formatter.user(f\"この質問はキーワード {obs.keyword} に関するものです。はいまたはいいえで答えてください。答えは二重アスタリスクで囲んでください。\")  # 回答のプロンプトを設定\n",
    "        self.formatter.start_model_turn()  # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # キーワードを解析\n",
    "        return 'yes' if 'yes' in answer else 'no'  # 答えを返す\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者のはいまたはいいえでの質問に対して答えます。キーワードは特定の人、場所、または物です。\"\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たしています。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** さて、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "# グローバルエージェント変数 - 並行して両方のエージェントを読み込まないようにする\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "\n",
    "    if agent is None:  # エージェントが未初期化の場合\n",
    "        if name == 'questioner':\n",
    "            agent = GemmaQuestionerAgent(\n",
    "                device='cuda:0',  # デバイス設定\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "        elif name == 'answerer':\n",
    "            agent = GemmaAnswererAgent(\n",
    "                device='cuda:0',  # デバイス設定\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"  # エージェントの存在を確認\n",
    "    return agent  # エージェントを返す\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType in [\"ask\", \"guess\"]:\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントを呼び出す\n",
    "    return response if response and len(response) > 1 else \"はい\"  # レスポンスが有効であれば返す"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
