{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ee3535",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleのコンペティション「LLM 20 Questions」に参加するためのエージェントを作成するプロセスを示しています。具体的には、質問者と回答者の役割を果たすエージェントを実装し、言語モデル（LLM）を使用して質問-回答ゲームを進行することを目的としています。\n",
    "\n",
    "### 問題の概要\n",
    "このノートブックは、言語モデルを活用して「20の質問」ゲームを自動化するエージェントを構築するためのフレームワークを提供しています。エージェントは、与えられたゲームの状況に基づき、質問を生成し、またその質問に対する「はい」または「いいえ」での応答を提供する必要があります。最終的には、ターゲット単語を最速で当てることが求められます。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "- **Gemma**: このノートブックではGoogleの`gemma_pytorch`ライブラリを使用しています。このライブラリは、因果言語モデルの訓練と推論に特化しています。\n",
    "- **ImmutableDict**と**SentencePiece**: 質問と回答を効率的に処理するために`immutabledict`と`sentencepiece`パッケージをインストールしています。`immutabledict`は変更不可の辞書を提供し、`sentencepiece`はトークナイザとして使用されます。\n",
    "- **PyTorch**: 言語モデルの実行にはPyTorchが利用されており、GPU上での処理が行われています。\n",
    "\n",
    "### 主なセクション\n",
    "1. **環境のセットアップ**: 必要なパッケージをインストールし、`gemma_pytorch`ライブラリを含むディレクトリを作成します。\n",
    "   \n",
    "2. **エージェント定義**:\n",
    "   - `GemmaAgent`クラスは、言語モデルとやり取りし、プロンプトを生成して応答を得るための基本機能を持ちます。\n",
    "   - `GemmaQuestionerAgent`と`GemmaAnswererAgent`クラスは、それぞれ質問者と回答者のエージェントを実装しています。これらのクラスは各ターンの処理、プロンプトのフォーマッティング、モデルからの応答解析を行います。\n",
    "\n",
    "3. **エージェントの動作**: \n",
    "   - ゲームの状態を表すデータをもとに、エージェントのメソッドを呼び出し、適切な応答を生成します。\n",
    "   - 各エージェントは、ターンタイプ（質問、推測、回答）に応じて異なるロジックを実行します。\n",
    "\n",
    "4. **ファイルの圧縮と提出準備**: 最後に、生成されたファイルと必要なモデルファイルを一つのtarボールに圧縮し、Kaggleへの提出を容易にします。\n",
    "\n",
    "このノートブックは、言語モデルを使用した「20の質問」ゲームの実行に必要なエージェントの構築方法を詳細に説明しており、コメントが豊富に含まれているため、実際の実装の概念を理解しやすくなっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04047a12",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、あなたが示したJupyterノートブックの内容から、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説です。特にマイナーなものや実務を経験していないと馴染みのないもの、ノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **Gemma**:\n",
    "   - このノートブック内で使用されている言語モデルの名称。本コンペティションにおける20の質問ゲームに特化したモデル。\n",
    "\n",
    "2. **Causal LM (因果言語モデル)**:\n",
    "   - 文章を生成する際に、過去の単語やトークンのみを考慮して次に来る単語を予測するタイプのモデル。自然言語生成において広く使われる。\n",
    "\n",
    "3. **量子化 (Quantization)**:\n",
    "   - モデルの重みや活性化値を低ビットの整数に変換する技術。これにより、モデルのサイズを縮小し、計算を効率化することができる。`get_config_for_2b`や`get_config_for_7b`はそれぞれ異なるモデルの設定を取得するための関数。\n",
    "\n",
    "4. **Interleave (インターリーブ)**:\n",
    "   - 2つのリストの要素を交互に並べる操作。リストの長さが異なる場合は、短い方のリストが終わるまで要素を交互に並べ、残りはNoneで埋める。\n",
    "\n",
    "5. **サンプリング (Sampling)**:\n",
    "   - 言語モデルからの出力生成の際に、次に生成するトークンを決定する方法。一般的には、確率的に選択されたトークンが使用され、様々なサンプリング手法（例：温度設定、top-k、top-p）がある。\n",
    "\n",
    "6. **Few-shot examples**:\n",
    "   - モデルに特定のタスクを教えるために提供される少数の例。この場合、プレイするゲームの例が含まれている。\n",
    "\n",
    "7. **デフォルトのテンソル型 (Default Tensor Type)**:\n",
    "   - PyTorchにおいて、テンソルを作成する際に使用されるデフォルトのデータ型。特定のデータ型を指定することで、メモリの使用量や計算速度を最適化することができる。\n",
    "\n",
    "8. **contextlib.contextmanager**:\n",
    "   - Pythonの標準ライブラリに含まれるモジュールで、資源の管理を簡易化するための文脈管理を提供する。特にリソースが必要な時、使い終わった時に自動的に解放されるべき場合に役立つ。\n",
    "\n",
    "9. **tar (Tape ARchiver)**:\n",
    "   - Unix系のオペレーティングシステムで、ファイルをアーカイブするためのコマンド。複数のファイルを一つのファイルにまとめる際に使用される。\n",
    "\n",
    "10. **pigz**:\n",
    "    - gzip（GNU zip）の並列実装であり、マルチコアCPUを使ってフィアルを圧縮するためのツール。大容量のファイルを迅速に圧縮できる。\n",
    "\n",
    "11. **pv (Pipe Viewer)**:\n",
    "    - パイプのデータを監視し、その進捗や統計情報を表示するためのツール。これにより、長時間かかる処理の進行状況を確認できる。\n",
    "\n",
    "これらの用語は、ノートブック内で特有のコンテキストや技術的な背景を持つため、初心者にとっては理解するのが難しいかもしれません。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb08f95a",
   "metadata": {},
   "source": [
    "[スタートノートブック](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook)を進める中で、私は友人のChatGPTと共同でこの完全に文書化されたバージョンを作成しました。コードは100％同じで、変更はありませんが、コメントが役に立つことを期待しています。\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。このファイルは右側の**コンペティションに提出**という見出しから直接提出できます。あるいは、ノートブックビューワーから*Output*タブをクリックして`submission.tar.gz`を見つけ、ダウンロードします。競技ホームページの左上にある**エージェントを提出**をクリックして、ファイルをアップロードし、提出を完了させます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:06:48.488574Z",
     "iopub.status.busy": "2024-06-06T05:06:48.488156Z",
     "iopub.status.idle": "2024-06-06T05:07:03.276374Z",
     "shell.execute_reply": "2024-06-06T05:07:03.275345Z",
     "shell.execute_reply.started": "2024-06-06T05:06:48.488539Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405ab0a",
   "metadata": {},
   "source": [
    "上記のセルは、必要なPythonパッケージをインストールし、エージェントで使用するために`gemma_pytorch`ライブラリを準備することで、環境をセットアップします。これにより、すべての必要な依存関係が提出ファイルにバンドルされます：\n",
    "\n",
    "- `%%bash`はJupyterノートブックで使用されるセルマジックコマンドであり、セル全体がbashスクリプトとして実行されるべきであることを示します。これにより、`%%bash`以降のすべての行が、Pythonコードではなく、シェル内でbashコマンドとして解釈されて実行されます。\n",
    "\n",
    "- `cd /kaggle/working`は、現在の作業ディレクトリを`/kaggle/working`に変更します。このディレクトリはKaggle環境内の作業ディレクトリであり、ファイルを保存したり、ワークスペースを設定したりできます。\n",
    "\n",
    "- `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`は、`immutabledict`および`sentencepiece` Pythonパッケージをインストールします。\n",
    "   - `-q`はインストールプロセスからの出力を抑制します（クワイエットモード）。\n",
    "   - `-U`は、すでにインストールされているパッケージをアップグレードします。\n",
    "   - `-t /kaggle/working/submission/lib`は、パッケージをインストールする対象ディレクトリを指定します。これにより、依存関係が提出パッケージに含まれるようになります。\n",
    "\n",
    "- `git clone https://github.com/google/gemma_pytorch.git > /dev/null`は、GitHubから現在のディレクトリ（`/kaggle/working`）に[`gemma_pytorch`リポジトリ](https://github.com/google/gemma_pytorch)をクローンします。\n",
    "   - `> /dev/null`は`git clone`コマンドの出力を`/dev/null`にリダイレクトし、実質的にその出力をノートブックの出力から隠します。`/dev/null`は、Unix系オペレーティングシステムにおける特別なファイルで、データを記録すると捨てられ、取り出すことができません。これは、出力を抑制し、ノートブックの出力を整理して不必要な詳細を避ける方法です。\n",
    "\n",
    "- `mkdir /kaggle/working/submission/lib/gemma/`は、`submission/lib`ディレクトリ内に`gemma`という名前の新しいディレクトリを作成します。このディレクトリには、`gemma_pytorch`リポジトリからのファイルが格納されます。\n",
    "\n",
    "- `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`は、クローンした`gemma_pytorch`リポジトリの`gemma`ディレクトリ内のすべてのファイルを、新しく作成した`submission/lib/gemma/`ディレクトリに移動します。これにより、`gemma_pytorch`リポジトリから必要なファイルが提出パッケージに含まれるようになります。\n",
    "\n",
    "これらはインストールされるパッケージです：\n",
    "\n",
    "**immutabledict**は、変更不可の辞書を提供するパッケージです。変更不可の辞書は、作成後に辞書が変更されないことを保証する必要があるときに便利です。これは、特に構成、定数、または並行プログラミングで作業する際に重要です。\n",
    "\n",
    "**sentencepiece**は、gemmaのトークナイザーです。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:08.002368Z",
     "iopub.status.busy": "2024-06-06T05:07:08.001976Z",
     "iopub.status.idle": "2024-06-06T05:07:08.023358Z",
     "shell.execute_reply": "2024-06-06T05:07:08.02231Z",
     "shell.execute_reply.started": "2024-06-06T05:07:08.002336Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で機能するように、システムパスをこのように設定してください。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# テンソル操作とモデル設定のハンドリングに必要なモジュールをインポートします。\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルウェイトのパスを定義します。\n",
    "# スクリプトがKaggleシミュレーション環境で実行されているか確認します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # Kaggleシミュレーション環境で実行されている場合、ウェイトパスを適切に設定します。\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # ローカルまたは別の環境で実行されている場合、ローカル入力ディレクトリを使用します。\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    20の質問ゲームのプロンプトと応答をフォーマットするためのクラス。\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        オプションのシステムプロンプトと少数のサンプルを使用してGemmaFormatterを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        プロンプトの現在の状態を返します。\n",
    "        \n",
    "        Returns:\n",
    "            str: 現在のフォーマットされた状態。\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にユーザーのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): ユーザーのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にモデルのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): モデルのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        新しいユーザーターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        新しいモデルターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        現在のターンを終了します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        フォーマッタを初期状態にリセットします。\n",
    "        提供された場合はシステムプロンプトと少数のサンプルも含みます。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        フォーマッタに一連のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            turns (Iterable): 適用するターンのシーケンス。\n",
    "            start_agent (str): 'user'または'model'のどちらが最初に開始されるかを指定します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    PyTorchにおけるデフォルトのテンソル型を設定するためのコンテキストマネージャ。\n",
    "\n",
    "    このコンテキストマネージャは、一時的に指定されたdtypeにデフォルトのテンソル型を設定し、\n",
    "    コンテキストを抜けると元のデフォルトのテンソル型に戻します。\n",
    "\n",
    "    Args:\n",
    "        dtype (torch.dtype): 設定するデフォルトのテンソル型。\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 指定されたdtypeにデフォルトのtorch dtypeを設定します。\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # デフォルトのテンソル型をfloatに戻します\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Gemma言語モデルとインタラクションするエージェントの基本クラス。\n",
    "    このクラスは、モデルの初期化、プロンプトのフォーマット、応答の生成を処理します。\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        指定された設定でGemmaAgentを初期化します。\n",
    "\n",
    "        Args:\n",
    "            variant (str): 使用するモデルバリアント（例：'7b-it-quant'）。\n",
    "            device (str): モデルを実行するデバイス（例：'cuda:0'はGPU）。\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        # バリアントに基づいてモデルの設定を取得\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # デフォルトのテンソル型を設定し、モデルを初期化\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        新しい観察を扱い、セッションを開始し、プロンプトを生成し、応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            *args: 追加の引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        エージェントの新しいセッションを開始します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        提供されたプロンプトを使用して言語モデルから応答を生成します。\n",
    "\n",
    "        Args:\n",
    "            prompt (str): 言語モデルに対する入力プロンプト。\n",
    "            max_new_tokens (int): 生成する最大新トークン数。\n",
    "            **sampler_kwargs: サンプリングプロセスに対する追加のキーワード引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        モデルの応答からキーワードを抽出します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "\n",
    "        Returns:\n",
    "            str: 抽出されたキーワード、見つからない場合は空の文字列。\n",
    "        \"\"\"\n",
    "        # ダブルアスタリスク（**）で囲まれた部分文字列を見つけます。\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        観察に基づいてモデルの応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    2つのリストxとyをインターリーブし、欠損値のためにNoneで埋めます。\n",
    "    \n",
    "    この関数は、2つのリストの要素をインターリーブします。リストの長さが異なる場合は、\n",
    "    欠損値のためにNoneを使用し、最終結果からこれらのNone値を除外します。\n",
    "\n",
    "    Args:\n",
    "        x (list): インターリーブする最初のリスト。\n",
    "        y (list): インターリーブする2番目のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: None値を除外してインターリーブされたxとyの要素を持つリスト。\n",
    "    \n",
    "    例:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの質問者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて質問を生成し、言語モデルからの応答を解析して\n",
    "    質問プロセスを進めます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaQuestionerAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        質問者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 受け取った対応する回答のリスト。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいまたはいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測はダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        ゲームのターンのタイプに基づいてモデルの応答を解析します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \n",
    "        Returns:\n",
    "            str: ターンタイプに基づいて解析された質問または推測。\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: 観察内のターンタイプが不明である場合。\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの回答者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて「はい」または「いいえ」の回答を提供し、\n",
    "    言語モデルからの応答を解析して回答プロセスを続けます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaAnswererAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        回答者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 与えられた対応する回答のリスト。\n",
    "                - obs.keyword (str): 質問者が推測しているキーワード。\n",
    "                - obs.category (str): キーワードのカテゴリ。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"その質問はキーワード{obs.keyword}についてで、カテゴリは{obs.category}です。「はい」または「いいえ」で答えて、回答をダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        モデルの応答を解析して「はい」または「いいえ」の回答を抽出します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "        \n",
    "        Returns:\n",
    "            str: 'yes'（回答に「yes」が含まれている場合）、それ以外は「no」。\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者の質問に「はい」または「いいえ」で答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "# **重要:** 必要なエージェントを一つだけロードできるようにエージェントをグローバルとして定義します。\n",
    "# 両方のエージェントをロードすると、メモリ不足になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    提供された名前に基づいて、適切なエージェント（質問者または回答者）を初期化して返します。\n",
    "    \n",
    "    この関数はグローバル変数を使用して、エージェントのインスタンスが一つだけ作成されることを保証します。\n",
    "    エージェントがまだ初期化されていない場合、提供された名前に基づいてGemmaQuestionerAgentまたはGemmaAnswererAgentの新しいインスタンスを作成します。\n",
    "    \n",
    "    Args:\n",
    "        name (str): 初期化するエージェントの名前（'questioner'または'answerer'）。\n",
    "\n",
    "    Returns:\n",
    "        GemmaAgent: GemmaQuestionerAgentまたはGemmaAnswererAgentのインスタンス。\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: エージェント名が認識されない場合、またはエージェントの初期化に失敗した場合。\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "   ターンのタイプを判定し、適切なエージェントを呼び出して応答を生成します。\n",
    "    \n",
    "    この関数は観察内のターンタイプを調べ、その対応するエージェント（質問者または回答者）を使用して応答を生成します。\n",
    "    応答がNoneまたは空である場合、\"yes\"を返します。\n",
    "    \n",
    "    Args:\n",
    "        obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            - obs.turnType (str): 現在のターンのタイプ（'ask'、'guess'、または'answer'）。\n",
    "        cfg (dict): エージェントの設定（現在の実装では使用されていません）。\n",
    "\n",
    "    Returns:\n",
    "        str: エージェントによって生成された応答、または応答がNoneまたは空である場合は\"yes\"。\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:10.340786Z",
     "iopub.status.busy": "2024-06-06T05:07:10.340376Z",
     "iopub.status.idle": "2024-06-06T05:07:18.728324Z",
     "shell.execute_reply": "2024-06-06T05:07:18.726981Z",
     "shell.execute_reply.started": "2024-06-06T05:07:10.340755Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714ac3e7",
   "metadata": {},
   "source": [
    "上記のセルは、高速圧縮のための`pigz`と、進捗監視のための`pv`をインストールします：\n",
    "\n",
    "- **apt install pigz pv**: `pigz`（gzipの並列実装）と`pv`（パイプビューア、データのパイプラインを通る進捗を監視可能）をインストールします。\n",
    "- **> /dev/null**: コマンドの出力を`/dev/null`にリダイレクトし、出力を抑制してノートブックをクリーンに保ちます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:08:42.811284Z",
     "iopub.status.busy": "2024-06-06T05:08:42.810305Z",
     "iopub.status.idle": "2024-06-06T05:11:31.295042Z",
     "shell.execute_reply": "2024-06-06T05:11:31.29284Z",
     "shell.execute_reply.started": "2024-06-06T05:08:42.811242Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8b5dfd",
   "metadata": {},
   "source": [
    "上記のセルは、提出ディレクトリと必要なモデルファイルを圧縮tarボール（`submission.tar.gz`）内にパッケージ化します。このtarボールには以下が含まれます：\n",
    "   - `/kaggle/working/submission`内のすべてのファイル。\n",
    "   - `/kaggle/input`からの`gemma/pytorch/7b-it-quant/2`ディレクトリ。\n",
    "   \n",
    "詳細な内訳：\n",
    "- **--use-compress-program='pigz --fast --recursive | pv'**: 圧縮に`pigz`を使用するように指定し、複数のCPUコアを利用してプロセスを高速化します。`--fast`オプションは迅速な圧縮を保証し、`--recursive`はディレクトリを再帰的に処理します。出力は`pv`を通してパイプされ、進捗を監視します。\n",
    "- **-cf submission.tar.gz**: `submission.tar.gz`という名前のファイルを作成します。\n",
    "- **-C /kaggle/working/submission**: アーカイブにファイルを追加する前に、ディレクトリを`/kaggle/working/submission`に変更します。\n",
    "- **.**: 現在のディレクトリ（オプションの`-C`によって`/kaggle/working/submission`になっています）からすべてのファイルをアーカイブに追加します。\n",
    "- **-C /kaggle/input/**: より多くのファイルを追加する前に、`/kaggle/input/`ディレクトリに変更します。\n",
    "- **gemma/pytorch/7b-it-quant/2**: `/kaggle/input/`から`gemma/pytorch/7b-it-quant/2`ディレクトリとその内容をアーカイブに追加します。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11220,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
