{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c43fd0be",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションに向けたエージェントの作成プロセスを示しています。具体的には、質問者と回答者の役割を持つ2つのエージェントを実装するための手法を扱っています。\n",
    "\n",
    "### 問題の概要\n",
    "「20の質問」ゲームで、高度な言語モデル（LLM）を活用し、質問を通じてターゲットワードを特定することが求められます。このゲームの目的は、各チームの質問者ができるだけ少ない質問でターゲットワードを推測することです。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **エージェントの実装**: `GemmaQuestionerAgent`と`GemmaAnswererAgent`がそれぞれ質問を生成し、回答を提供するエージェントです。これにより、ゲームの規則やフローに沿った対応を行います。\n",
    "- **Gemmaライブラリ**: Googleの`gemma_pytorch`リポジトリからクローンしたライブラリを使用します。これは、指定された設定でのモデルの初期化やトークン化を支援します。\n",
    "- **必要なパッケージ**: `immutabledict`と`sentencepiece`がインストールされており、特に不変辞書形式とトークナイザーとして機能します。\n",
    "- **PyTorch**: モデルのトレーニングや推論に使用され、`torch`ライブラリを通じてTensor操作が行われます。\n",
    "\n",
    "### プロセスの流れ\n",
    "1. **環境設定**: 必要なライブラリをインストールし、ゲームプレイのために必要なファイルを適切なディレクトリに配置します。\n",
    "2. **セッションの開始**: 質問者または回答者エージェントのセッションが開始され、ゲーム状態に基づいてフォーマットされたプロンプトを生成します。\n",
    "3. **応答の生成**: エージェントがプロンプトを使用して言語モデルからの応答を受け取り、それを解析して次のアクションを決定します。\n",
    "4. **提出ファイルの作成**: 最終的に、すべての必要なファイルを圧縮し、コンペティションへの提出用のアーカイブファイルを作成します。\n",
    "\n",
    "このノートブックは、エージェントの役割を明確に分割し、各エージェントがゲーム内で機能するための具体的な構造を提供しています。また、モデルがどのように操作され、互いに作用するかに焦点を当てています。最終的に生成された`submission.tar.gz`ファイルをコンペティションに提出することで、エージェントのパフォーマンスを測定します。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f565d3b6",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容を踏まえた初心者がつまずきそうな専門用語の解説を示します。\n",
    "\n",
    "1. **immutabledict**:\n",
    "   - 不変の辞書を提供するPythonのライブラリです。一度作成されると内容を変更できず、設定や定数を誤って変更してしまうのを防ぐために使われます。データが変更されることが予期されない場合に便利です。\n",
    "\n",
    "2. **sentencepiece**:\n",
    "   - Googleが開発したトークナイザーで、テキストをサブワード単位に分割します。これにより、未知の単語への対応が容易になり、特に多言語や事前学習モデルでよく利用されます。通常の単語分割器に比べ、より柔軟で効率的なトークナイゼーションを実現します。\n",
    "\n",
    "3. **contextlib**:\n",
    "   - コンテクスト管理を行うためのモジュールで、with文を使ってリソース管理を効率的に行えます。特定の処理を行った後、自動的に後処理を実行するために使用される場合が多いです。\n",
    "\n",
    "4. **torch.dtype**:\n",
    "   - PyTorchにおけるデータ型の表現です。例えば、`torch.float`, `torch.int`などがあり、テンソルのデータ型を指定するために使われます。効率的なメモリの使用や計算性能の最適化につながります。\n",
    "\n",
    "5. **quant**:\n",
    "   - モデルの重みや出力を整数や小数の固定ビット数に圧縮して、メモリ使用量を削減する技術です。量子化は特に、デプロイ時のパフォーマンス向上やリソース制約のある環境（例：モバイルデバイス）で重要です。\n",
    "\n",
    "6. **interleave_unequal**:\n",
    "   - 二つの異なる長さのリストを交差させる関数です。長さが異なる場合、短い方のリストの不足分には`None`で埋めることで、両者を交互に並べる処理を行います。データ処理やコレクション操作において役立つ技術です。\n",
    "\n",
    "7. **@contextlib.contextmanager**:\n",
    "   - Pythonのデコレーターで、関数をコンテクストマネージャとして使用できるようにします。これにより、リソースを準備し、使用後にクリーンアップする処理を自動化できます。特に、ファイル操作やネットワークリソース管理で便利です。\n",
    "\n",
    "8. **GemmaForCausalLM**:\n",
    "   - Gemmaモデルの因果言語モデル（Causal Language Model）に関するクラスで、シーケンスデータに基づいて次に来るトークンを予測するために訓練されています。このクラスは、特定のタスクに応じてカスタマイズされたモデルを提供します。\n",
    "\n",
    "9. **load_weights**:\n",
    "   - 事前に訓練されたモデルの重みを読み込むメソッドです。これにより、モデルが知識をもとに新しいデータに対して効果的に応答できるようになります。\n",
    "\n",
    "10. **max_new_tokens**:\n",
    "    - モデルから生成される新しいトークンの最大数を指定するパラメータです。この設定によって出力の長さを制御し、必要に応じて短いまたは長い応答を生成することができます。\n",
    "\n",
    "以上の用語は、ノートブック内で使われている特有のコンセプトや一般的ではないが知識として重要なものを中心に選びました。これらを理解することで、ノートブック全体の流れや機能が把握しやすくなるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f740f4ee",
   "metadata": {},
   "source": [
    "[スターターノートブック](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook)を進めながら、友人のChatGPTと共同でこの完全にドキュメント化されたバージョンを作成しました。コードは100%同じで変更はありませんが、コメントが役に立つことを願っています。\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。このファイルは、右側の**コンペティションに提出**という見出しから直接提出できます。あるいは、ノートブックのビューアから*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードします。コンペティションホームページの左上にある**エージェントを提出**をクリックして、ファイルをアップロードし、提出を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:06:48.488574Z",
     "iopub.status.busy": "2024-06-06T05:06:48.488156Z",
     "iopub.status.idle": "2024-06-06T05:07:03.276374Z",
     "shell.execute_reply": "2024-06-06T05:07:03.275345Z",
     "shell.execute_reply.started": "2024-06-06T05:06:48.488539Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# カレントディレクトリを作業ディレクトリに移動します\n",
    "cd /kaggle/working\n",
    "\n",
    "# immutabledictとsentencepieceライブラリを指定されたディレクトリにインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "\n",
    "# GitHubからgemma_pytorchリポジトリをクローンします。出力を非表示にします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "\n",
    "# 新しいディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "# クローンしたリポジトリ内のgemmaのファイルを新しいディレクトリに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/ \n",
    "\n",
    "# 上記のコマンドは、エージェントのために必要なライブラリとコードを準備しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff1d3e",
   "metadata": {},
   "source": [
    "上記のセルでは、必要なPythonパッケージをインストールし、エージェントで使用するために`gemma_pytorch`ライブラリを準備することで環境を整えています。これにより、すべての必要な依存関係が提出ファイルにまとめられます。\n",
    "\n",
    "- `%%bash`はJupyterノートブックでのセルマジックコマンドであり、セル全体をbashスクリプトとして実行することを示します。つまり、`%%bash`の後のすべての行は、Pythonコードとしてではなく、シェル内でbashコマンドとして解釈され、実行されます。\n",
    "\n",
    "- `cd /kaggle/working`は、カレントディレクトリを`/kaggle/working`に変更します。このディレクトリはKaggle環境内の作業ディレクトリであり、ファイルを保存したりワークスペースを設定したりできます。\n",
    "\n",
    "- `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`は、`immutabledict`と`sentencepiece`というPythonパッケージをインストールします。\n",
    "   - `-q`はインストールプロセスの出力を抑制します（静かなモード）。\n",
    "   - `-U`は既にインストールされている場合にパッケージをアップグレードします。\n",
    "   - `-t /kaggle/working/submission/lib`は、パッケージをインストールする対象ディレクトリを指定します。これにより、依存関係が提出パッケージに含まれることが保証されます。\n",
    "\n",
    "- `git clone https://github.com/google/gemma_pytorch.git > /dev/null`は、GitHubから[`gemma_pytorch`リポジトリ](https://github.com/google/gemma_pytorch)をカレントディレクトリ（`/kaggle/working`）にクローンします。\n",
    "   - `> /dev/null`は`git clone`コマンドの出力を`/dev/null`にリダイレクトし、ノートブックの出力から隠します。`/dev/null`はUnix系オペレーティングシステムにおける特別なファイルであり、データが書き込まれると破棄され、取得できません。これはあくまで出力を抑制する手段であり、ノートブックの出力をクリーンに保ち、不必要な詳細を省くことができます。\n",
    "\n",
    "- `mkdir /kaggle/working/submission/lib/gemma/`は、`submission/lib`ディレクトリ内に`gemma`という新しいディレクトリを作成します。このディレクトリには、`gemma_pytorch`リポジトリからのファイルが格納されます。\n",
    "\n",
    "- `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`は、クローンした`gemma_pytorch`リポジトリ内の`gemma`ディレクトリに存在するすべてのファイルを、新たに作成した`submission/lib/gemma`ディレクトリに移動します。これにより、提出パッケージに必要なファイルが含まれることが保証されます。\n",
    "\n",
    "インストールされたパッケージは以下の通りです：\n",
    "\n",
    "**immutabledict**は、不変の辞書を提供するパッケージです。不変の辞書は、一度作成された後に辞書が変更できないことを確実にする必要がある場合に便利です。これは、特定のアプリケーション（設定、定数、あるいは同時プログラミングの作業を行う際）でデータ構造に対する accidental な変更を防ぐのに特に重要です。\n",
    "\n",
    "**sentencepiece**は、gemmaのトークナイザーです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:08.002368Z",
     "iopub.status.busy": "2024-06-06T05:07:08.001976Z",
     "iopub.status.idle": "2024-06-06T05:07:08.023358Z",
     "shell.execute_reply": "2024-06-06T05:07:08.02231Z",
     "shell.execute_reply.started": "2024-06-06T05:07:08.002336Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 環境設定\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックでもシミュレーション環境でも動作するように、システムパスを設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# テンソル操作とモデル設定のために必要なモジュールをインポートします\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルの重みのパスを定義します。\n",
    "# スクリプトがKaggleシミュレーション環境で実行されているかどうかを確認します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # Kaggleシミュレーション環境で実行している場合、重みのパスを設定します\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # ローカルまたは別の環境で実行している場合、重みのためのローカル入力ディレクトリを使用します\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    20質問ゲーム用のプロンプトと応答をフォーマットするためのクラス。\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        システムプロンプトと少数ショットの例（オプション）でGemmaFormatterを初期化します。\n",
    "        \n",
    "        引数:\n",
    "            system_prompt (str): システム用の初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        プロンプトの現在の状態を返します。\n",
    "        \n",
    "        戻り値:\n",
    "            str: 現在のフォーマット済みの状態。\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にユーザーのプロンプトを追加します。\n",
    "        \n",
    "        引数:\n",
    "            prompt (str): ユーザーのプロンプト。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にモデルのプロンプトを追加します。\n",
    "        \n",
    "        引数:\n",
    "            prompt (str): モデルのプロンプト。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        新しいユーザーターンを開始します。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        新しいモデルターンを開始します。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        現在のターンを終了します。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        フォーマッタを初期状態にリセットします。\n",
    "        システムプロンプトや少数ショットの例が提供されている場合は、それらもリセットします。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        フォーマッタに一連のターンを適用します。\n",
    "        \n",
    "        引数:\n",
    "            turns (Iterable): 適用するターンのシーケンス。\n",
    "            start_agent (str): 最初に『user』または『model』のどちらが開始するかを指定します。\n",
    "        \n",
    "        戻り値:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    PyTorchにおけるデフォルトテンソル型を設定するためのコンテクストマネージャ。\n",
    "    \n",
    "    このコンテクストマネージャは、指定されたdtypeにデフォルトテンソル型を一時的に設定し、\n",
    "    コンテキストを終了する際に元のデフォルトテンソル型を復元します。\n",
    "    \n",
    "    引数:\n",
    "        dtype (torch.dtype): 設定するデフォルトテンソル型。\n",
    "    \n",
    "    戻り値:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # デフォルトtorch dtypeを指定されたdtypeに設定します。\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # デフォルトテンソル型をfloatに復元します\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Gemma言語モデルと対話するためのエージェントの基本クラス。\n",
    "    このクラスはモデルの初期化、プロンプトのフォーマット、応答生成を処理します。\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        指定された設定でGemmaAgentを初期化します。\n",
    "        \n",
    "        引数:\n",
    "            variant (str): 使用するモデルのバリアント（例： '7b-it-quant'）。\n",
    "            device (str): モデルを実行するデバイス（例： 'cuda:0'はGPU）。\n",
    "            system_prompt (str): システム用の初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        # バリアントに基づいてモデルの設定を取得します\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # デフォルトテンソル型を設定し、モデルを初期化します\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        新しい観察を処理し、セッションを開始してプロンプトを生成し、応答を解析します。\n",
    "        \n",
    "        引数:\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "            *args: 追加の引数。\n",
    "        \n",
    "        戻り値:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        エージェントの新しいセッションを開始します。\n",
    "        \n",
    "        引数:\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "        \n",
    "        例外:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        提供されたプロンプトを使用して、言語モデルから応答を生成します。\n",
    "        \n",
    "        引数:\n",
    "            prompt (str): 言語モデルへの入力プロンプト。\n",
    "            max_new_tokens (int): 生成する新しいトークンの最大数。\n",
    "            **sampler_kwargs: サンプリングプロセスのための追加のキーワード引数。\n",
    "        \n",
    "        戻り値:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        モデルの応答からキーワードを抽出します。\n",
    "        \n",
    "        引数:\n",
    "            response (str): モデルの応答。\n",
    "        \n",
    "        戻り値:\n",
    "            str: 抽出されたキーワード。キーワードが見つからない場合は空の文字列。\n",
    "        \"\"\"\n",
    "        # ダブルアスタリスク（**）で囲まれたサブストリングを見つけます。\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        観察に基づいてモデルの応答を解析します。\n",
    "        \n",
    "        引数:\n",
    "            response (str): モデルの応答。\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "        \n",
    "        例外:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    二つのリストxとyをインタリーブし、長さが異なる場合は欠けている値にはNoneを埋めます。\n",
    "    \n",
    "    この関数は二つのリストを受け取り、その要素をインタリーブします。リストの長さが異なる場合は、\n",
    "    欠けている値にはNoneを使用し、最終結果からこれらのNone値を除外します。\n",
    "\n",
    "    引数:\n",
    "        x (list): インタリーブする最初のリスト。\n",
    "        y (list): インタリーブする二つ目のリスト。\n",
    "\n",
    "    戻り値:\n",
    "        list: xとyの要素がインタリーブされたリスト（None値は除外）。\n",
    "    \n",
    "    例:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20質問ゲームでの質問者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントはゲーム状態に基づいて質問を生成し、\n",
    "    言語モデルからの応答を解析して質問プロセスを続行します。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaQuestionerAgentを初期化します。\n",
    "        \n",
    "        引数:\n",
    "            *args: 基底クラス初期化子に渡す位置引数。\n",
    "            **kwargs: 基底クラス初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        質問者エージェントの新しいセッションを開始します。\n",
    "        フォーマッタをリセットし、初期プロンプトと前のターンを適用します。\n",
    "        \n",
    "        引数:\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前に行った質問のリスト。\n",
    "                - obs.answers (list): 受け取った回答のリスト。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいまたはいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今こそキーワードを推測してください。あなたの推測をダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        ゲーム内のターンのタイプに基づいてモデルの応答を解析します。\n",
    "        \n",
    "        引数:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \n",
    "        戻り値:\n",
    "            str: ターンタイプに基づく解析された質問または推測。\n",
    "        \n",
    "        例外:\n",
    "            ValueError: 観察内のターンタイプが不明な場合。\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20質問ゲームでの回答者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントはゲーム状態に基づいてはい・いいえの回答を提供し、\n",
    "    言語モデルからの応答を解析して回答プロセスを続行します。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaAnswererAgentを初期化します。\n",
    "        \n",
    "        引数:\n",
    "            *args: 基底クラス初期化子に渡す位置引数。\n",
    "            **kwargs: 基底クラス初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        回答者エージェントの新しいセッションを開始します。\n",
    "        フォーマッタをリセットし、初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        引数:\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前に行った質問のリスト。\n",
    "                - obs.answers (list): 与えられた同様の応答のリスト。\n",
    "                - obs.keyword (str): 質問者が推測しようとしているキーワード。\n",
    "                - obs.category (str): キーワードのカテゴリ。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。はい・いいえで答えてください。あなたの答えをダブルアスタリスクで囲んでください（例: **はい**または**いいえ**）。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        モデルの応答を解析してはい・いいえの回答を抽出します。\n",
    "        \n",
    "        引数:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "        \n",
    "        戻り値:\n",
    "            str: 応答に「はい」が含まれていれば'yes'、そうでなければ'no'。\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを思い浮かべ、質問者がはい・いいえの質問をすることで、それに答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい**今、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義しますので、必要なエージェントだけをロードします。\n",
    "# 両方をロードすると、OOM（Out Of Memory）になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    提供された名前に基づいて適切なエージェント（質問者または回答者）を初期化して返します。\n",
    "    \n",
    "    この関数はグローバル変数を使用して、エージェントのインスタンスが一度だけ作成されることを保証します。\n",
    "    エージェントがまだ初期化されていない場合は、提供された名前に基づいて\n",
    "    GemmaQuestionerAgentまたはGemmaAnswererAgentの新しいインスタンスを生成します。\n",
    "    \n",
    "    引数:\n",
    "        name (str): 初期化するエージェントの名前（'questioner'または'answerer'）。\n",
    "    \n",
    "    戻り値:\n",
    "        GemmaAgent: GemmaQuestionerAgentまたはGemmaAnswererAgentのインスタンス。\n",
    "    \n",
    "    例外:\n",
    "        AssertionError: エージェント名が認識されないか、エージェントの初期化に失敗した場合。\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "    ターンの種類を判定し、適切なエージェントを呼び出して応答を生成します。\n",
    "    \n",
    "    この関数は観察のターンタイプを調べ、それに応じて対応するエージェント\n",
    "    （質問者または回答者）を利用して応答を生成します。応答がNoneまたは空である場合は「はい」を返します。\n",
    "    \n",
    "    引数:\n",
    "        obs (dict): ゲーム状態情報を含む観察辞書。\n",
    "            - obs.turnType (str): 現在のターンのタイプ（'ask'、'guess'、または'answer'）。\n",
    "        cfg (dict): エージェントの設定（現在の実装では使用されません）。\n",
    "\n",
    "    戻り値:\n",
    "        str: エージェントによって生成された応答、または応答がNoneまたは空の場合は「はい」。\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:10.340786Z",
     "iopub.status.busy": "2024-06-06T05:07:10.340376Z",
     "iopub.status.idle": "2024-06-06T05:07:18.728324Z",
     "shell.execute_reply": "2024-06-06T05:07:18.726981Z",
     "shell.execute_reply.started": "2024-06-06T05:07:10.340755Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null\n",
    "# 上記のコマンドは、`pigz`と`pv`という二つのパッケージをインストールします。\n",
    "# `pigz`は並列圧縮ツールで、gzipの並列版です。\n",
    "# `pv`はパイプビューで、データの転送速度や進捗状況を表示するための便利なツールです。\n",
    "# 実行結果はノートブックの出力に表示されないようにしています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450c798",
   "metadata": {},
   "source": [
    "上記のセルでは、高速圧縮のために`pigz`を、進捗監視のために`pv`をインストールします：\n",
    "\n",
    "- **apt install pigz pv**: `pigz`（gzipの並列実装）と`pv`（Pipe Viewer、データがパイプラインを通過する進捗状況を監視するためのツール）をインストールします。\n",
    "- **> /dev/null**: コマンドの出力を`/dev/null`にリダイレクトし、出力を抑制することでノートブックをクリーンに保ちます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:08:42.811284Z",
     "iopub.status.busy": "2024-06-06T05:08:42.810305Z",
     "iopub.status.idle": "2024-06-06T05:11:31.295042Z",
     "shell.execute_reply": "2024-06-06T05:11:31.29284Z",
     "shell.execute_reply.started": "2024-06-06T05:08:42.811242Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "# 上記のコマンドは、指定されたディレクトリ内のファイルを圧縮して`submission.tar.gz`という名前のアーカイブファイルを作成します。\n",
    "# - `--use-compress-program='pigz --fast --recursive | pv'`: 圧縮には`pigz`を使用し、進捗状況を表示するために`pv`をパイプします。\n",
    "# - `-cf submission.tar.gz`: `submission.tar.gz`という名前で圧縮ファイルを作成します。\n",
    "# - `-C /kaggle/working/submission .`: `/kaggle/working/submission`ディレクトリ内の内容をアーカイブに追加します。\n",
    "# - `-C /kaggle/input/ gemma/pytorch/7b-it-quant/2`: 追加で`/kaggle/input/gemma/pytorch/7b-it-quant/2`ディレクトリの内容もアーカイブに追加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044aad92",
   "metadata": {},
   "source": [
    "上記のセルでは、提出ディレクトリと必要なモデルファイルを圧縮タールボール（`submission.tar.gz`）にパッケージ化しています。このタールボールには以下が含まれています：\n",
    "   - `/kaggle/working/submission`内のすべてのファイル。\n",
    "   - `/kaggle/input`からの`gemma/pytorch/7b-it-quant/2`ディレクトリ。\n",
    "   \n",
    "詳細な内訳：\n",
    "- **--use-compress-program='pigz --fast --recursive | pv'**: 圧縮には`pigz`を使用し、複数のCPUコアを利用して処理を高速化します。`--fast`オプションは迅速な圧縮を保証し、`--recursive`はディレクトリを再帰的に処理します。出力は`pv`を通してパイプされ、進捗状況がモニタリングされます。\n",
    "- **-cf submission.tar.gz**: `submission.tar.gz`という名前のファイルを作成します。\n",
    "- **-C /kaggle/working/submission**: アーカイブにファイルを追加する前に、`/kaggle/working/submission`ディレクトリに移動します。\n",
    "- **.**: 現在のディレクトリ（`-C`オプションにより`/kaggle/working/submission`となります）からすべてのファイルをアーカイブに追加します。\n",
    "- **-C /kaggle/input/**: さらにファイルを追加する前に、`/kaggle/input/`ディレクトリに移動します。\n",
    "- **gemma/pytorch/7b-it-quant/2**: `/kaggle/input/`から`gemma/pytorch/7b-it-quant/2`ディレクトリおよびその内容をアーカイブに追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 何も入力がありません。別のセルのリクエストをお願いします。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11220,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
