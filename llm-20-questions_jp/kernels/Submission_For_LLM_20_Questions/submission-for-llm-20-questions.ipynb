{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2fad3508",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、「20の質問」ゲームにおける言語モデル（LLM）のエージェントの構築に関するもので、特に質問者と回答者の二つのエージェントに焦点を当てています。具体的には、エージェントが与えられたキーワードに基づいて質問を行ったり、回答を返したりするロジックを実装しています。\n",
    "\n",
    "## 取り組む問題\n",
    "- **問題**: 「20の質問」ゲームにおいて、質問者エージェントと回答者エージェントが効果的に相互作用し、限られた質問数で正解のキーワードを推測する能力を向上させることに取り組んでいます。ゲームの制約に対応し、適切な質問と推測を行う必要があります。\n",
    "\n",
    "## 使用している手法・ライブラリ\n",
    "1. **ライブラリ**:\n",
    "   - `pandas`: データフレームを使用してキーワード情報を整理するために利用。\n",
    "   - `json`: JSON形式のデータを読み込むために使用。\n",
    "   - `torch`: 深層学習モデルのためのPyTorchライブラリ。\n",
    "   - `immutabledict`と`sentencepiece`: モデルの依存関係としてインポート。\n",
    "\n",
    "2. **クラスと構造**:\n",
    "   - **GemmaFormatter**: ユーザープロンプトやモデルレスポンスのフォーマットを管理し、現在の会話状態を保持。\n",
    "   - **Observation**: 課題に対する観察結果を管理し、過去の質問・回答・推測をトラック。\n",
    "   - **GemmaAgent**: 基底クラスとして、質問者と回答者エージェントの基本的な機能を提供。\n",
    "   - **GemmaQuestionerAgent**: 質問者エージェントとしての具体的なロジックを実装。\n",
    "   - **GemmaAnswererAgent**: 回答者エージェントとしてのロジックを実装。\n",
    "\n",
    "3. **システムプロンプトと少数ショット例**:\n",
    "   - システムプロンプトと少数ショット例は、エージェントの動作を導くために設定され、ゲームの目的やエージェントの役割について明示されています。\n",
    "\n",
    "4. **エージェントの実装**:\n",
    "   - エージェントは動的に初期化され、観察情報に基づいて適切な動作（質問する、回答する、推測する）を行います。\n",
    "\n",
    "このように、Notebookは「20の質問」ゲームを支援するためのLLMエージェントを構築するための多くの要素を組み合わせており、自然言語処理によるインタラクションを促進するために設計されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29396b73",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に関連する専門用語の簡単な解説です。特に、初心者がつまずきやすいマイナーな用語やドメイン特有の知識に焦点を当てています。\n",
    "\n",
    "1. **Gemma**:\n",
    "   - Gemmaは、特定の自然言語処理（NLP）タスク、特に因果推論（Causal Language Modeling）に焦点を当てた言語モデルです。このNotebookで使われるGemmaは、モデルの振る舞いを特定の目的に合わせて訓練・調整するために使用されています。\n",
    "\n",
    "2. **Causal LM (因果言語モデル)**:\n",
    "   - 過去の単語に基づいて次の単語を予測するタイプの言語モデルです。一方向に時間を考慮し、コンテキストの保持が重要です。\n",
    "\n",
    "3. **contextlib**:\n",
    "   - Pythonの標準ライブラリで、特定のコードブロックの前後で環境を管理するためのユーティリティを提供します。例えば、ファイルのオープンとクローズ、または特定の設定を一時的に変更するのに使われます。\n",
    "\n",
    "4. **Pydantic**:\n",
    "   - Pythonのデータバリデーションと設定管理を簡素化するためのライブラリです。データモデルを定義する際に、型ヒントを使って簡単にデータの検証や変換を行うことができます。\n",
    "\n",
    "5. **torch.set_default_dtype()**:\n",
    "   - PyTorchにおいて、全てのテンソルのデフォルトのデータ型を指定するためのメソッドです。デフォルトのデータ型を変えることで、メモリ使用量や計算の精度を調整することができます。\n",
    "\n",
    "6. **few-shot examples (数ショット例)**:\n",
    "   - モデルを訓練する際に、数少ないサンプルから学習するためのアプローチです。特に、NLPタスクにおいては、数例の入力と出力の組み合わせを用いて、モデルの応答を制御するのに役立ちます。\n",
    "\n",
    "7. **Observations (観察)**:\n",
    "   - ゲームの状態やエージェントのアクションを表すためのデータ構造です。ゲームの進行に応じた質問、回答、推測などが含まれ、エージェントがそれらの情報を用いて次のアクションを決定します。\n",
    "\n",
    "8. **interleave_unequal**:\n",
    "   - 二つのリストを交互に結合し、片方のリストが短い場合は対応する要素がない部分は無視する関数です。この動作により、異なる長さのリストを扱う際に便利です。\n",
    "\n",
    "9. **@property decorator**:\n",
    "   - クラス内のメソッドをプロパティとして扱うためのデコレータです。この機能を使うことで、クラスの属性に対してカスタムのゲッターを提供し、属性にアクセスする際に特定のロジックを組み込むことができます。\n",
    "\n",
    "10. **set_default_tensor_type**:\n",
    "    - PyTorchにおいて、全ての新しいテンソルに適用されるデフォルトのデータタイプを設定するためのコンテクストマネージャです。この機能を用いることで、特定のデータ型で処理を行なうことが容易になります。\n",
    "\n",
    "これらの用語は、特に初心者や実務経験のない人にとっての理解を助けるためのものです。各用語の背景や実際の利用方法について掘り下げて学ぶことをお勧めします。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T01:24:01.358181Z",
     "iopub.status.busy": "2024-08-02T01:24:01.3575Z",
     "iopub.status.idle": "2024-08-02T01:24:01.36274Z",
     "shell.execute_reply": "2024-08-02T01:24:01.361823Z",
     "shell.execute_reply.started": "2024-08-02T01:24:01.358148Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandasライブラリをpdという名前でインポートします\n",
    "# jsonライブラリをインポートします\n",
    "\n",
    "# キーワードをロードします\n",
    "from importlib.machinery import SourceFileLoader\n",
    "# keywords.pyファイルからキーワードを読み込みます\n",
    "keywords = SourceFileLoader(\"keywords\",'/kaggle/input/llm-20-questions/llm_20_questions/keywords.py').load_module()\n",
    "# JSON形式のキーワードデータを読み込みます\n",
    "df = json.loads(keywords.KEYWORDS_JSON)\n",
    "\n",
    "words = []\n",
    "# カテゴリごとにキーワードをループ処理します\n",
    "for cat in df:\n",
    "    # カテゴリ名とその単語数を表示します\n",
    "    print(cat['category'], len(cat['words']))\n",
    "    # 各キーワードをリストに追加します\n",
    "    for w in cat['words']:\n",
    "        words.append([cat['category'], w[\"keyword\"], w[\"alts\"], \"\", \"\", 0.0, 0.0])\n",
    "\n",
    "# キーワードをデータフレームに変換します\n",
    "df = pd.DataFrame(words, columns=['Category','Word','Alternatives', \"Cat1\", \"Cat2\", \"Lat\", \"Lon\"])\n",
    "# データフレームの最初の5行を表示します\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T01:24:01.36453Z",
     "iopub.status.busy": "2024-08-02T01:24:01.36423Z",
     "iopub.status.idle": "2024-08-02T01:24:13.972688Z",
     "shell.execute_reply": "2024-08-02T01:24:13.971914Z",
     "shell.execute_reply.started": "2024-08-02T01:24:01.364496Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリに移動します\n",
    "cd /kaggle/working\n",
    "# 作業ディレクトリ内のファイルをすべて削除します\n",
    "rm -rf ./*\n",
    "# immutabledictとsentencepieceを指定のディレクトリにインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorchリポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemmaのディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemma_pytorch内のファイルを新しいディレクトリに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T01:24:13.974606Z",
     "iopub.status.busy": "2024-08-02T01:24:13.974333Z",
     "iopub.status.idle": "2024-08-02T01:24:13.987585Z",
     "shell.execute_reply": "2024-08-02T01:24:13.986718Z",
     "shell.execute_reply.started": "2024-08-02T01:24:13.974584Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# # セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleエージェントのパスを設定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # Kaggle環境のライブラリパスをシステムパスに追加します\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    # ワーキングディレクトリのライブラリパスをシステムパスに追加します\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # モデルの重みパスを設定します\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    # 初期化\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    # 現在の会話状態を文字列として返します\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    # ユーザープロンプトを会話に追加します\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "        \n",
    "    # モデルのレスポンスを会話に追加します\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    # ユーザのターンの開始を記録します\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    # モデルのターンの開始を記録します\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    # 現在のターンの終了を記録します\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    # リセットメソッド\n",
    "    def reset(self):\n",
    "        # `_state`を空の文字列に初期化します\n",
    "        self._state = \"\"  \n",
    "\n",
    "        # 提供された場合はシステムプロンプトを追加します\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  \n",
    "            \n",
    "        # 提供された場合はfew-shot例を適用します\n",
    "        if self._few_shot_examples is not None: \n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 発言を使うエージェントの順番を決めます\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"デフォルトのtorchデータ型を指定されたdtypeに設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "from pydantic import BaseModel\n",
    "    \n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        # 質問、回答、推測がすべて空であるかをチェックします\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        # 過去の質問、回答、推測をまとめて返します\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_text(self, *, skip_guesses: bool = False, skip_answer: bool = False, skip_question: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            # 過去のやり取りの履歴を文字列として整形します\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"{'**Question:** ' + question if not skip_question else ''} -> {'**Answer:** ' + answer if not skip_answer else ''}\n",
    "            {'**Previous Guess:** ' + guess if not skip_guesses else ''}\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"まだありません。\"\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        \n",
    "        # モデル設定\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        \n",
    "        # モデルの初期化\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            \"\"\"TODO: モデルを変更してパフォーマンスを見てみる\"\"\"\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"TODO: LLMのパラメータを変更します\"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # レスポンスからキーワードを抽出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    # 2つのリストを交互に結合し、不足している要素を除外します\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\"\"\"TODO: 質問者を調整します\"\"\"\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # セッションを開始します\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をプレイします。あなたは質問者としての役割を担います。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        hints = self.turns_to_hints(turns)\n",
    "        if obs.turnType == 'ask':\n",
    "            if len(turns) == 0:\n",
    "                self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "            else:\n",
    "                self.formatter.user(f\"このリストのキーワードのヒントに基づいて、はいかいいえで答えられる質問をしてください: {hints}\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。あなたの推測を二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            # ユーザーの質問を抽出します\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "    \n",
    "    # 質問を肯定または否定のステートメントに変換するためにターンを変換します\n",
    "    def turns_to_hints(self, turns):\n",
    "        hints = []\n",
    "        for i in range(0, len(turns), 2):\n",
    "            question = turns[i]\n",
    "            answer = turns[i+1]\n",
    "            # hints.append(f\"Question: {question} Answer: {answer}\")\n",
    "            \n",
    "            if question.startswith('is it'):\n",
    "                item = question[6:-1]  # \"is it \"の後と\"?\"の前の部分を抽出します\n",
    "                if answer == 'yes':\n",
    "                    transformed_statement = f\"それは{item}です\"\n",
    "                else:\n",
    "                    transformed_statement = f\"それは{item}ではありません\"\n",
    "            elif question.startswith('does'):\n",
    "                item = question[5:-1]  # \"does \"の後と\"?\"の前の部分を抽出します\n",
    "                if answer == 'yes':\n",
    "                    transformed_statement = f\"それは{item}をします\"\n",
    "                else:\n",
    "                    transformed_statement = f\"それは{item}をしません\"\n",
    "            else:\n",
    "                transformed_statement = f\"認識できない質問形式: {question}\"\n",
    "\n",
    "            hints.append(transformed_statement)\n",
    "\n",
    "        return hints\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        # キーワードとカテゴリーに基づいて初期ユーザー発言をフォーマットします\n",
    "        self.formatter.user(f\"20の質問をプレイします。あなたは回答者の役割を担います。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"このキーワード{obs.keyword}とそのカテゴリ{obs.category}に関する質問です。はいまたはいいえで答え、あなたの答えを二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスから回答を取得します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"\"\"あなたは20の質問ゲームをプレイするためのAIアシスタントです。 \n",
    "このゲームでは、回答者がキーワードを考え、質問者がはいかいいえで質問します。キーワードは特定の人、場所、または物です。あなたの目的は、できるだけ少ないターンで秘密のキーワードを推測し、勝つことです。\"\"\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイします。あなたは質問者の役割を担います。最初の質問をしてください。\",\n",
    "    \"人ですか？\", \"**いいえ**\",\n",
    "    \"場所ですか？\", \"**はい**\",\n",
    "    \"国ですか？\", \"**はい** 今、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解!\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義して、必要なエージェントのみをロードします。\n",
    "# 両方をロードすると、メモリ不足になる可能性があります。\n",
    "\n",
    "# エージェント変数を初期化します\n",
    "agent = None\n",
    "\n",
    "# 要求されたエージェントに基づいて適切なエージェントを取得するための関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # エージェントが初期化されておらず、要求されたエージェントが「質問者」の場合\n",
    "    if agent is None and name == 'questioner':\n",
    "        # GemmaQuestionerAgentを特定のパラメータで初期化します\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',  # 計算用のデバイス\n",
    "            system_prompt=system_prompt,  # エージェントのシステムプロンプト\n",
    "            few_shot_examples=few_shot_examples,  # エージェントの振る舞いを導く例\n",
    "        )\n",
    "    # エージェントが初期化されておらず、要求されたエージェントが「回答者」の場合\n",
    "    elif agent is None and name == 'answerer':\n",
    "        # GemmaAnswererAgentを同じパラメータで初期化します\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    \n",
    "    # エージェントが初期化されていることを確認します\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    # 初期化されたエージェントを返します\n",
    "    return agent\n",
    "\n",
    "# 観察に基づいてインタラクションを処理するための関数\n",
    "def agent_fn(obs, cfg):\n",
    "    # 観察が質問するためのものである場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        # 「質問者」エージェントを使用して観察に応答します\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観察が推測するためのものである場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        # 「質問者」エージェントを使用して観察に応答します\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観察が回答を提供するためのものである場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        # 「回答者」エージェントを使用して観察に応答します\n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    # エージェントからのレスポンスがNoneまたは非常に短い場合\n",
    "    if response is None or len(response) <= 1:\n",
    "        # ポジティブなレスポンス（「はい」）を仮定します\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        # エージェントから受け取ったレスポンスを返します\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T01:24:13.988829Z",
     "iopub.status.busy": "2024-08-02T01:24:13.988595Z",
     "iopub.status.idle": "2024-08-02T01:26:41.336196Z",
     "shell.execute_reply": "2024-08-02T01:26:41.335101Z",
     "shell.execute_reply.started": "2024-08-02T01:24:13.988808Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします\n",
    "!apt install pigz pv > /dev/null\n",
    "# # 提出物をtar.gz形式で圧縮します（圧縮プログラムとしてpigzを使用します）\n",
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/gemma/pytorch/7b-it-quant/2\n",
    "\n",
    "# 提出物をtar.gz形式で圧縮します（圧縮プログラムとしてpigzを使用します）\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T01:26:41.33948Z",
     "iopub.status.busy": "2024-08-02T01:26:41.339055Z",
     "iopub.status.idle": "2024-08-02T01:26:41.345677Z",
     "shell.execute_reply": "2024-08-02T01:26:41.34463Z",
     "shell.execute_reply.started": "2024-08-02T01:26:41.339436Z"
    }
   },
   "outputs": [],
   "source": [
    "# obsというクラスを定義します\n",
    "#     '''\n",
    "#     このクラスは、ゲームからの観察を表します。 \n",
    "#     turnTypeは['ask','answer','guess']のいずれかでなければなりません。\n",
    "#     テストしたいキーワードを選択します。\n",
    "#     キーワードのカテゴリを選択します。\n",
    "#     質問は文字列のリストです。エージェントに質問履歴を渡す場合。\n",
    "#     回答は文字列のリストです。\n",
    "#     responseを使用して、質問を回答エージェントに渡します。\n",
    "#     '''\n",
    "#     def __init__(self, turnType, keyword, category, questions, answers):\n",
    "#         self.turnType = turnType\n",
    "#         self.keyword = keyword\n",
    "#         self.category = category\n",
    "#         self.questions = questions\n",
    "#         self.answers = answers\n",
    "\n",
    "# #エージェントに渡すためのobsの例。自分のobsをここに渡します。\n",
    "# obs1 = obs(turnType='ask', \n",
    "#           keyword='paris', \n",
    "#           category='place', \n",
    "#           questions=['人ですか？', '場所ですか？', '国ですか？', '都市ですか？'], \n",
    "#           answers=['いいえ', 'はい', 'いいえ', 'はい'])\n",
    "\n",
    "# question = agent_fn(obs1, None)\n",
    "# print(question)\n",
    "\n",
    "# #エージェントからのレスポンスが出力されるはずです。 \n",
    "# #提出エラーの解決には役立ちませんが、エージェントのレスポンスを評価するのに役立ちます。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 3301,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
