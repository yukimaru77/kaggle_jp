{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc79d5a7",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのAIエージェントを構築する過程を示しています。ノートブックは、主に「20の質問」ゲームでの質問者と回答者のロールを持つエージェントを作成するためのコードと手法を提供しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "このノートブックの目的は、AIが「20の質問」ゲームをプレイできるようにすることです。具体的には、質問者エージェントと回答者エージェントが、それぞれ質問を投げかけ、はい/いいえの回答を得ることで、対象のキーワードを推測する能力を持つエージェントを設計しています。エージェントが正確に、かつ迅速に答えを導き出すためには、効果的な質問戦略と情報収集能力が求められます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **Gemma**: プロジェクトの中核には、Gemmaというライブラリが含まれており、特に自然言語生成（NLG）に特化したモデル（GemmaForCausalLM）が利用されています。\n",
    "- **PyTorch**: ディープラーニングフレームワークとしてPyTorchが使用され、モデルのトレーニングと推論が行われます。\n",
    "- **Bashスクリプト**: ノートブック内での環境設定やライブラリのインストールにBashスクリプトが使用されており、GemmaリポジトリをGitHubからクローンするための手順が含まれています。\n",
    "- **Torchのテンソル設定**: コードはデフォルトのテンソル型を指定するコンテキストマネージャを使用しており、モデルの初期化時に適切なデータ型が確保されています。\n",
    "\n",
    "### 構造\n",
    "ノートブックは以下の構成要素を含みます：\n",
    "1. **環境設定**: 必要なライブラリのインストールやリポジトリのクローンが行われる部分。\n",
    "2. **エージェントクラス**: `GemmaAgent`という基本クラスと、そのサブクラスとして質問者と回答者用のエージェントが定義されており、ゲームのロジックに基づいた動作が実装されています。\n",
    "3. **エージェントの管理**: グローバルに定義されたエージェントを管理し、必要に応じてインスタンスを取得するための関数も実装されています。\n",
    "\n",
    "### 全体の流れ\n",
    "ノートブックを実行すると、まずは必要なライブラリがインストールされ、Gemmaのモデルが初期化されます。その後、質問者エージェントや回答者エージェントが機能するように定義され、最終的に圧縮ファイルが生成されて提出用のファイル（`submission.tar.gz`）が作成されます。このプロセスは、Kaggleコンペティションにおけるエージェント提出を意図しています。\n",
    "\n",
    "このノートブックは、特にNLPを利用したゲーム戦略の構築において、実践的な例を通して学ぶための素晴らしい出発点です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4d12e",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyterノートブックの内容に関連する専門用語の解説です。これにより、機械学習や深層学習の初心者がつまずきやすい部分を補足します。\n",
    "\n",
    "1. **エージェント (Agent)**:\n",
    "   - マシンラーニングや強化学習において、環境内で行動を選択し、報酬を最大化することを目的とした存在。ここでは、質問者または回答者の役割を担うAIシステムを指します。\n",
    "\n",
    "2. **プロンプト (Prompt)**:\n",
    "   - モデルに対して与える入力文。質問や指示などが含まれ、AIがそれに基づいて応答を生成します。特に、LLM（大規模言語モデル）では適切なプロンプトが重要な結果を生む要因となります。\n",
    "\n",
    "3. **Few-Shot Learning**:\n",
    "   - 与えられたわずかな例（ショット）から学習し、未知のデータに対する予測を行う手法。モデルは少数の例を利用して新しいタスクに対応できるよう進化します。\n",
    "\n",
    "4. **モデル (Model)**:\n",
    "   - 機械学習アルゴリズムの結果として得られる数理的構造。訓練データをもとに学習を行い、予測や分類を行う能力を持つもの。\n",
    "\n",
    "5. **トークナイザー (Tokenizer)**:\n",
    "   - テキストを構成する最小単位（トークン）に分割するプロセスやツール。言語モデルがテキストを理解するために、単語やサブワード、文字レベルなどのトークン化を行います。\n",
    "\n",
    "6. **サンプリング手法 (Sampling Methods)**:\n",
    "   - モデルからの出力を生成する際に使用される手法。例として、temperature（温度）、top-p、top-kがあり、生成の多様性や一貫性に影響を与えます。\n",
    "\n",
    "7. **コンテキストマネージャ (Context Manager)**:\n",
    "   - 一時的な環境を管理するためのフレームワーク。リソースの割当てや解放、エラーハンドリングを容易にします。\n",
    "\n",
    "8. **ユーザーターン (User Turn) と モデルターン (Model Turn)**:\n",
    "   - 20の質問ゲームにおけるターンの表現。ユーザーが発言する際のターンとモデルがそれに応じて返答する際のターンです。\n",
    "\n",
    "9. **スコア評価を更新 (Update Skill Evaluation)**:\n",
    "   - ボットのパフォーマンスを評価し、そのスコアや信頼性を数値化して更新するプロセス。通常、対戦結果に基づいて調整されます。\n",
    "\n",
    "10. **エピソード (Episode)**:\n",
    "    - 1回のゲームプレイを指し、これには一連のターンとやり取りが含まれます。エージェントはこのエピソードを通じて学習し、改善を図ります。\n",
    "\n",
    "これらの用語は、ノートブック内のコードやコメントの文脈において重要な役割を果たしており、理解を深めるために知識として備えておくと良いでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ec12ce",
   "metadata": {},
   "source": [
    "2024年5月20日\n",
    "\n",
    "* **ノートブックの提出を試みましたが、エラー*Validation Episode failed*が返されました。**\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。ノートブックビューワーで*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードしてください。コンペティションのホームページ左上にある**Submit Agent**をクリックして、ファイルをアップロードし、提出を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリに移動します\n",
    "cd /kaggle/working\n",
    "\n",
    "# 必要なライブラリをインストールします。-qオプションは出力を抑えます。\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "\n",
    "# GemmaのPyTorchリポジトリをGitHubからクローンします。出力は/dev/nullにリダイレクトされ、表示されません。\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "\n",
    "# Gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "# クローンしたGemmaのファイルを新しく作成したディレクトリに移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6902d5",
   "metadata": {},
   "source": [
    "**上記のコードのChatGPTによる説明**\n",
    "\n",
    "このコードはBashスクリプトで、作業環境を設定し、必要なPythonライブラリをインストールし、特定のライブラリ（GitHubリポジトリから）を使用するための準備を行います。以下は各行の詳細な説明です：\n",
    "1. `%%bash`:\n",
    "   - これはJupyterノートブックで使用されるマジックコマンドで、次の行はBashコマンドとして解釈されることを示します。\n",
    "2. `cd /kaggle/working`:\n",
    "   - 現在のディレクトリを`/kaggle/working`に変更します。このディレクトリはKaggleのカーネル（ノートブック）で計算中にファイルを保存したりアクセスしたりするためによく使用されます。\n",
    "3. `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`:\n",
    "   - `pip`を使用して、`immutabledict`および`sentencepiece`の最新バージョンを静かに（`-q`）インストールします（`install`）。\n",
    "   - `-t /kaggle/working/submission/lib`オプションは、ライブラリをインストールするターゲットディレクトリを指定します。この場合は`/kaggle/working/submission/lib`です。\n",
    "4. `git clone https://github.com/google/gemma_pytorch.git > /dev/null`:\n",
    "   - GitHubから`gemma_pytorch`リポジトリをクローンします。\n",
    "   - `> /dev/null`部分は、出力を`/dev/null`にリダイレクトし、クローンコマンドの出力を抑制して、ノートブックやスクリプトの出力をすっきりと保ちます。\n",
    "5. `mkdir /kaggle/working/submission/lib/gemma/`:\n",
    "   - `/kaggle/working/submission/lib/`内に`gemma`という名前の新しいディレクトリを作成します。このディレクトリにはクローンしたリポジトリのファイルが保存されます。\n",
    "6. `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`:\n",
    "   - クローンした`gemma_pytorch`リポジトリ内の`gemma`ディレクトリから必要なすべてのファイル（`*`）を、新しく作成した`/kaggle/working/submission/lib/gemma/`ディレクトリに移動します。\n",
    "\n",
    "### 概要\n",
    "- スクリプトは作業ディレクトリを設定します。\n",
    "- 特定のパスに`immutabledict`と`sentencepiece`ライブラリをインストールします。\n",
    "- GitHubからリポジトリをクローンします。\n",
    "- クローンしたリポジトリのファイルのためのディレクトリを作成します。\n",
    "- クローンしたリポジトリから必要なファイルを作成したディレクトリに移動します。\n",
    "\n",
    "このセットアップは、Kaggleノートブックでの機械学習タスクやその他の計算タスクに必要な特定の依存関係やライブラリを準備するために使用される可能性があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 設定\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** これに従ってシステムパスを設定してください。これにより、ノートブックとシミュレーション環境の両方でコードが機能します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"デフォルトのtorch dtypeを指定されたdtypeに設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はい/いいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"さあ、キーワードを推測してみてください。推測は二重のアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"未知のターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をしましょう。あなたは答える側の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"そのキーワード{obs.keyword}に関する質問です。はいまたはいいえで答えてください。答えは二重のアスタリスクで囲んでください、例えば**はい**または**いいえ**。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'はい' if 'はい' in answer else 'いいえ'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e8b8ce",
   "metadata": {},
   "source": [
    "**上記のコードのChatGPTによる説明**\n",
    "\n",
    "このスクリプトは、20の質問ゲームをプレイするために設計されたAIエージェントの設定と動作を定義しています。エージェントは、質問者または回答者のいずれかになります。以下はコードの各部分の詳細な説明です：\n",
    "\n",
    "### 1. ファイル作成\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "```\n",
    "- このJupyterノートブックのマジックコマンドは、以下のコードを`submission`ディレクトリ内の`main.py`というファイルに書き込みます。\n",
    "\n",
    "### 2. 設定\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "```\n",
    "- 必要なモジュールをインポートします。\n",
    "- ノートブックとKaggleシミュレーション環境の両方での互換性を確保するため、必要なライブラリが格納されているディレクトリをシステムパスに設定します。\n",
    "\n",
    "### 3. ライブラリと設定のインポート\n",
    "```python\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "```\n",
    "- 追加のライブラリをインポートします。\n",
    "- 環境に基づいてモデルの重みのパスを定義します。\n",
    "\n",
    "### 4. プロンプトフォーマットクラス\n",
    "```python\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "```\n",
    "- `GemmaFormatter`は、定義済みのトークンを使用してユーザーとモデルの間の会話をフォーマットします。\n",
    "- メソッドは、ユーザーとモデルのプロンプトを追加し、ターンを開始または終了し、会話の状態をリセットする機能を提供します。\n",
    "\n",
    "### 5. エージェント定義\n",
    "```python\n",
    "import re\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "            }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "```\n",
    "- `GemmaAgent`クラスは、モデルを初期化し、プロンプトをフォーマットし、モデルとのインタラクションを処理します。\n",
    "- `_set_default_tensor_type`コンテキストマネージャは、PyTorchのデフォルトテンソルタイプを一時的に設定します。\n",
    "- `__call__`メソッドは、観察（`obs`）に基づいてモデルからの応答を生成します。\n",
    "- `_start_session`および `_parse_response`メソッドは、サブクラスによって実装されることを意図しているプレースホルダーです。\n",
    "- `interleave_unequal`関数は、2つのリストから要素をインタリーブし、長い方のリストからの残りの要素でギャップを埋めます。\n",
    "\n",
    "### 6. 質問者と回答者エージェント\n",
    "```python\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "```\n",
    "- `GemmaQuestionerAgent`と`GemmaAnswererAgent`は`GemmaAgent`のサブクラスであり、それぞれの役割に合わせた`_start_session`および`_parse_response`メソッドが実装されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者によるはい/いいえの質問に応じて応答します。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** さあ、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義して、必要なエージェントだけを読み込むようにします。\n",
    "# 両方を読み込むと、OOM（メモリオーバー）に繋がる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d539e65",
   "metadata": {},
   "source": [
    "**上記のコードのChatGPTによる説明**\n",
    "このコードの部分は、20の質問ゲームをプレイするために使用されるAIエージェントの作成と管理を担当しています。エージェントのインスタンス化方法と、提供された観察に基づいてゲームのロジックをどのように扱うかを定義しています。以下は詳細な説明です。\n",
    "\n",
    "### 1. システムプロンプトと少数ショットの例の定義\n",
    "```python\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者によるはい/いいえの質問に応じて応答します。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** さあ、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解!\",\n",
    "]\n",
    "```\n",
    "- `system_prompt`: AIに20の質問ゲームのコンテキストを提供します。\n",
    "- `few_shot_examples`: AIがゲームの形式と応答方法を理解するのに役立つ一連の例。この少数ショット学習により、モデルはいくつかの例から一般化できます。\n",
    "\n",
    "### 2. グローバルエージェント変数\n",
    "```python\n",
    "agent = None\n",
    "```\n",
    "- 現在のエージェントを保存するためのグローバル変数`agent`を定義します。これにより、エージェントを複数回再読み込みすることを避け、メモリオーバー（OOM）エラーを引き起こすのを防ぎます。\n",
    "\n",
    "### 3. 適切なエージェントを取得する関数\n",
    "```python\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "```\n",
    "- `get_agent(name: str)`: 指定されたエージェント（`questioner`または`answerer`）のインスタンスを返す関数。\n",
    "    - グローバル変数`agent`が`None`で、要求されたエージェントのタイプが`questioner`の場合、`GemmaQuestionerAgent`のインスタンスを作成します。\n",
    "    - 同様に、エージェントのタイプが`answerer`の場合、`GemmaAnswererAgent`のインスタンスを作成します。\n",
    "    - 両方のエージェントは`system_prompt`と`few_shot_examples`で初期化されます。\n",
    "    - エージェントが常に返されることを保証し、初期化試行後に`agent`が`None`でないことを確認します。\n",
    "\n",
    "### 4. メインエージェント関数\n",
    "```python\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "- `agent_fn(obs, cfg)`: 現在のゲーム状態（`obs`）に基づいてエージェントとのインタラクションを処理するメイン関数。\n",
    "    - `obs.turnType`が`\"ask\"`の場合、`questioner`エージェントを呼び出して質問を生成します。\n",
    "    - `obs.turnType`が`\"guess\"`の場合、再び`questioner`エージェントを呼び出して推測を行います。\n",
    "    - `obs.turnType`が`\"answer\"`の場合、`answerer`エージェントを呼び出して回答を生成します。\n",
    "    - エージェントは必要な役割に基づいて`get_agent`を使用して取得されます。\n",
    "    - エージェントの応答が`None`または短すぎる場合は`\"はい\"`を返します。\n",
    "    - それ以外の場合は、エージェントによって生成された応答を返します。\n",
    "\n",
    "### 概要\n",
    "- このコードセグメントは、20の質問をプレイするためのAIエージェントを初期化し、管理します。\n",
    "- エージェント用のシステムプロンプトと例を定義します。\n",
    "- メモリの問題を避けるために、エージェントのインスタンスを1つだけ読み込むことを保証します。\n",
    "- `get_agent`関数は、必要に応じたエージェントタイプを動的に初期化します。\n",
    "- `agent_fn`関数はゲームのロジックを処理し、現在のターンタイプに基づいて適切なエージェントを呼び出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします。出力は/dev/nullにリダイレクトされ、表示されません。\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de83345d",
   "metadata": {},
   "source": [
    "**上記のコードのChatGPTによる説明**\n",
    "このコード行は、`apt`パッケージマネージャを使用して2つのパッケージ`pigz`と`pv`をインストールするシェルコマンドです。以下は詳細な説明です。\n",
    "\n",
    "### コマンドの内訳\n",
    "```bash\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "#### 1. `!`\n",
    "- 行の先頭にある`!`は、このコマンドがシェルで実行されるべきことを示します。これはJupyterノートブックで利用可能な機能で、コードセルから直接シェルコマンドを実行することができます。\n",
    "\n",
    "#### 2. `apt install pigz pv`\n",
    "- `apt`: これはAdvanced Package Tool（APT）のコマンドラインインターフェースで、DebianベースのLinuxディストリビューション（例：Ubuntu）で使用されるパッケージ管理システムです。\n",
    "- `install`: このサブコマンドは、`apt`に1つまたは複数のパッケージをインストールしたいことを指示します。\n",
    "- `pigz`: これは最初にインストールするパッケージの名前です。`pigz`とは「gzipの並列実装」という意味です。これは、データを圧縮するためのツールで、従来の`gzip`よりも迅速に圧縮を行うために複数のCPUコアを使用します。\n",
    "- `pv`: これは2番目にインストールするパッケージの名前です。`pv`は「Pipe Viewer」の略で、パイプラインを通過するデータの進行状況を監視するための端末ベースのツールです。データ転送の進行状況、スループットレート、推定完了時間などを視覚的に表示します。\n",
    "\n",
    "#### 3. `> /dev/null`\n",
    "- このコマンドの部分は、標準出力（stdout）を`/dev/null`にリダイレクトします。\n",
    "- `/dev/null`は、Unix系オペレーティングシステムにおける特別なファイルで、書き込まれたすべてのデータを破棄します。これは、表示したくない出力を抑制するためによく使用されます。\n",
    "- この文脈では、`> /dev/null`により、`apt install`コマンドによって生成される出力（例：ダウンロード進行状況、インストールメッセージなど）がターミナルやJupyterノートブックに表示されないようにします。\n",
    "\n",
    "### コマンドの目的\n",
    "このコマンドの目的は、`pigz`と`pv`パッケージを静かにインストールすることです。出力を表示しないようにすることで、インストールログで出力が clutter されないようにできます。\n",
    "\n",
    "### 実用的な使用例\n",
    "- **`pigz`**: 大きなファイルを迅速に圧縮する必要がある場合に、複数のCPUコアを活用するのに便利です。\n",
    "- **`pv`**: 長時間実行されるプロセスにおいて、データ転送やパイプラインの進行状況を監視するのに役立ちます。\n",
    "\n",
    "このコマンドを使用することで、これらのツールが次の使用のためにシステムに利用可能であることを確認しながら、インストールメッセージで出力が clutter されることを防ぎます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 圧縮プログラムとしてpigzを使用し、pvによる進捗表示を行いながら、submission.tar.gzファイルを作成します。\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a0dd5",
   "metadata": {},
   "source": [
    "**上記のコードのChatGPTによる説明**\n",
    "このシェルコマンドは、`tar`コマンドを使用して圧縮されたtarball（`submission.tar.gz`）を作成し、`pigz`と`pv`を組み合わせて高速かつ監視された圧縮を行います。以下はコマンドの各部分の詳細な説明です。\n",
    "\n",
    "### コマンドの内訳\n",
    "```bash\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "#### 1. `!`\n",
    "- これは、後続のコマンドがシェルで実行されるべきことを示します。これはJupyterノートブックで利用可能な特徴です。\n",
    "\n",
    "#### 2. `tar`\n",
    "- `tar`コマンドは、tarballアーカイブを作成、維持、変更、および抽出するために使用されます。\n",
    "\n",
    "#### 3. `--use-compress-program='pigz --fast --recursive | pv'`\n",
    "- このオプションは、使用する圧縮プログラムを指定します。この場合、`pv`を通じてパイプで接続された`pigz`を使用します。\n",
    "- `'pigz --fast --recursive'`: `pigz`（`gzip`の並列実装）を使用して圧縮します。`--fast`フラグは、圧縮率よりも速度を優先することを`pigz`に指示し、`--recursive`はディレクトリを再帰的に処理することを保証します。\n",
    "- `'| pv'`: `pigz`の出力を`pv`にパイプして、圧縮の進行状況を監視します。`pv`はデータ転送の進行状況を表示し、長時間実行されるプロセスに便利です。\n",
    "\n",
    "#### 4. `-cf submission.tar.gz`\n",
    "- `-c`: 新しいアーカイブを作成します。\n",
    "- `-f submission.tar.gz`: 作成するアーカイブファイルの名前を指定します（`submission.tar.gz`）。\n",
    "\n",
    "#### 5. `-C /kaggle/working/submission .`\n",
    "- `-C /kaggle/working/submission`: アーカイブにファイルを追加する前に、ディレクトリ`/kaggle/working/submission`に移動します。\n",
    "- `.`: 現在のディレクトリ（`-C`オプションによって`/kaggle/working/submission`となっている）をアーカイブに追加します。\n",
    "\n",
    "#### 6. `-C /kaggle/input/ gemma/pytorch/7b-it-quant/2`\n",
    "- `-C /kaggle/input/`: アーカイブにファイルを追加する前に、ディレクトリ`/kaggle/input/`に移動します。\n",
    "- `gemma/pytorch/7b-it-quant/2`: アーカイブに`/kaggle/input/`に相対的な`gemma/pytorch/7b-it-quant/2`ディレクトリを追加します。\n",
    "\n",
    "### コマンドの目的\n",
    "このコマンドは、以下を含む圧縮tarball（`submission.tar.gz`）を作成するために使用されます：\n",
    "1. `/kaggle/working/submission`からのすべてのファイルとディレクトリ。\n",
    "2. `/kaggle/input/`からの`gemma/pytorch/7b-it-quant/2`ディレクトリ。\n",
    "\n",
    "`pigz`を使用して圧縮することにより、並列処理による高速な圧縮速度の恩恵を受けます。`pv`は圧縮プロセスの視覚的な進捗インジケーターを提供します。\n",
    "\n",
    "### 実用的な使用例\n",
    "- **圧縮速度**: `pigz`は複数のCPUコアを利用することで圧縮プロセスを高速化します。\n",
    "- **進捗監視**: `pv`は圧縮の進行状況を監視するのに役立ち、プロセスにどのくらいの時間がかかるのかを見るのが容易になります。\n",
    "- **特定のファイルのアーカイブ**: `-C`を使用することで、現在の作業ディレクトリを変更することなく特定のディレクトリをアーカイブに含めることができ、複数の場所からファイルを追加できる柔軟性があります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42eee38",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Matin Mahmoudi ✨\n",
    "> \n",
    "> 実装のための素晴らしいスタートノートブックで、とても有益です [@regisvargas](https://www.kaggle.com/regisvargas)。あなたの他の作品も確認しましたし、全てにアップボートしました。素晴らしい、友人よ。\n",
    "> \n",
    "> \n",
    "> > ## Regis Vargas（トピック作成者）\n",
    "> > \n",
    "> > 励ましの言葉、ありがとうございます。私は多くの困難に直面していますが、一日一日自分を奮い立たせようとしています。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
