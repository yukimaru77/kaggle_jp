{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30f68f5d",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「20の質問」ゲームコンペティションに向けたエージェントの構築を目的としています。特定のユーザー入力に応じて質問を生成したり、答えを提供したりするための言語モデル（LLM）を使用しています。このプロジェクトは、「Gemma」という事前学習済みモデルを利用しており、特にマルチターンの対話における情報の取得と推測に焦点を当てています。\n",
    "\n",
    "### 提出物の内容\n",
    "1. **環境設定とライブラリのインストール**:\n",
    "   - Tensor Processing Unit (TPU) と Graphics Processing Unit (GPU) のセットアップをしています。\n",
    "   - `immutabledict`と`sentencepiece`ライブラリおよび`gemma_pytorch`リポジトリがインストールされています。\n",
    "\n",
    "2. **エージェントのクラス設計**:\n",
    "   - `GemmaFormatter`, `GemmaAgent`, `GemmaQuestionerAgent`, `GemmaAnswererAgent`などのクラスが定義されており、これにより質問者と回答者の役割を担うエージェントが実装されています。\n",
    "   - 各エージェントは、ユーザーの入力を受け取り、内部状態を保持し、適切な応答を生成します。\n",
    "\n",
    "3. **応答生成の方法**:\n",
    "   - パラメータ調整（例：temperature、top_p、top_k）を用いて生成プロセスを制御し、確率的に応答を生成します。\n",
    "   - 正しいキーワードを推測するための解析機能も含まれています。\n",
    "\n",
    "4. **テストと実行**:\n",
    "   - Kaggle環境で20の質問ゲームをシミュレーションするために、`kaggle_environments`ライブラリを用いています。\n",
    "\n",
    "5. **エージェントの相互作用**:\n",
    "   - 質問者と回答者のターンを管理し、質問を交互に生成し合う仕組みが実装されています。\n",
    "\n",
    "### 使用されているライブラリ\n",
    "- **PyTorch**: 学習モデルの実行と管理に利用。\n",
    "- **Gemma**: 特定の事前学習モデルを使用するためのライブラリとして活用。\n",
    "- **kaggle_environments**: Kaggleの環境をシミュレーションし、エージェントのパフォーマンスを確認するために使用。\n",
    "\n",
    "このノートブックは、「20の質問」ゲームの戦略的なプレイをサポートするためのインフラを整備しており、特に言語モデルがどのように自動的に質問し、回答し、ゲームを進行させるかに注力しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129e3c4c",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語の解説を示します。\n",
    "\n",
    "1. **immutabledict**:\n",
    "   - Pythonの辞書（dict）と似ていますが、変更ができない（immutable）特性を持つデータ構造です。この特性により、辞書の内容を誤って変更することを防ぎ、ハッシュ可能なオブジェクトとして使用することができます。\n",
    "\n",
    "2. **sentencepiece**:\n",
    "   - 自然言語処理におけるテキストのトークナイゼーションのためのアルゴリズムです。文章をサブワードやトークンに分割することで、より柔軟なモデルの学習を可能にします。特に、形態素解析を必要としない言語処理に役立ちます。\n",
    "\n",
    "3. **TPU（Tensor Processing Unit）**:\n",
    "   - Googleが開発した特化型ハードウェアで、特にディープラーニングモデルのトレーニングや推論を高速化するために設計されています。一般的なGPUよりも効率的に行列演算を行うことができます。\n",
    "\n",
    "4. **torch.FloatTensor**:\n",
    "   - PyTorchのデータ構造の一つで、浮動小数点数のテンソルを表します。モデルの入力データやモデルの重みを格納するために使用されます。\n",
    "\n",
    "5. **GemmaForCausalLM**:\n",
    "   - Gemmaライブラリの一部で、因果言語モデル（Causal Language Model）を実装したクラスです。因果モデルは、過去の情報に基づいて次の単語を予測するために使用されます。\n",
    "\n",
    "6. **prompt**:\n",
    "   - モデルがテキストを生成するための入力文や形態のことです。システムプロンプトはモデルの出力を導くための指示となります。\n",
    "\n",
    "7. **context manager**:\n",
    "   - Pythonの構文の一つで、特定のオブジェクトを一時的に取り扱う際に、そのオブジェクトが自動的に破棄されるようにすることができます。省メモリ化や、リソースの管理に役立ちます。\n",
    "\n",
    "8. **interleave**:\n",
    "   - 二つのリストやシーケンスを交互に結合する操作のことです。異なるデータのペアを作成する際に使われます。\n",
    "\n",
    "9. **sampler_kwargs**:\n",
    "   - モデルの出力生成時に使用するための追加的な引数です。例えば、生成するテキストの多様性を制御するためのパラメータ（temperature、top_pなど）が含まれます。\n",
    "\n",
    "10. **re.search**:\n",
    "   - 正規表現を用いて文字列の検索を行うための関数です。特定のパターンが文字列内に存在するかをチェックし、結果として見つかった部分を返します。\n",
    "\n",
    "11. **floatとdouble**:\n",
    "   - 数値型の一つで、floatは単精度（32ビット）を、doubleは倍精度（64ビット）を表します。計算の精度に影響を与えます。\n",
    "\n",
    "12. **Kaggleエージェント**:\n",
    "   - Kaggleプラットフォームで実行されるエージェントプログラムで、特定の課題を解決するための戦略的なアルゴリズムを実装します。この場合、20の質問ゲームにおける質問者や回答者の役割を担います。\n",
    "\n",
    "これらの用語は、機械学習の具体的な実装やコードの理解において重要な概念であり、特に実務未経験の初心者にとっては馴染みが薄いかもしれません。理解を深めるために、それぞれの実際の使用例や関連性をさらに調べると良いでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 現在の作業ディレクトリを/kaggle/workingに移動する\n",
    "cd /kaggle/working\n",
    "# immutabledictとsentencepieceパッケージをインストールする\n",
    "# -q:出力を最小限にする、-U:アップグレード、-t:指定したディレクトリにインストール\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorchリポジトリをGitHubからクローンする（出力は非表示にする）\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成する\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemma_pytorchリポジトリからgemmaのファイルを新しく作成したディレクトリに移動する\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU（Tensor Processing Unit）用の環境設定を行うためのスクリプトをダウンロードする\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# ダウンロードしたスクリプトを実行し、必要なAPTパッケージ（libomp5とlibopenblas-dev）をインストールする\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU（Graphics Processing Unit）用のデバイスを設定する\n",
    "# デバイスがCUDA（GPU）を利用可能であればcuda:0を使用し、それ以外の場合はCPUを使用する\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TPU（Tensor Processing Unit）用のデバイスを設定する\n",
    "device = xm.xla_device()\n",
    "# デフォルトのテンソルタイプをtorch.FloatTensorに設定する\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "import torch, itertools, contextlib\n",
    "import os, sys, re\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggleエージェントのパスを定義する\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# Kaggleエージェントのパスが存在する場合\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # ライブラリのパスをシステムパスに追加する\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    # 重みのパスを設定する\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    # その他の状況では、別のライブラリのパスを設定する\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "# gemmaライブラリから設定とモデルをインポートする\n",
    "from gemma.config import get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# GemmaFormatterクラスの定義\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    def __init__(self, sp: str = None, fse: Iterable = None):\n",
    "        self._system_prompt = sp\n",
    "        self._few_shot_examples = fse\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 質問者と回答者のターンを交互に適用する\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    # デフォルトのテンソルタイプを設定するためのコンテキストマネージャ\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "# GemmaAgentクラスの定義\n",
    "class GemmaAgent:\n",
    "    def __init__(self, sp=None, fse=None):\n",
    "        self._device = xm.xla_device()  # デバイスをTPUに設定する\n",
    "        self.formatter = GemmaFormatter(sp=sp, fse=fse)  # フォーマッタを初期化\n",
    "        print(\"モデルを初期化中\")\n",
    "        model_config = get_config_for_2b()  # モデルの設定を取得\n",
    "        model_config.tokenizer = WEIGHTS_PATH + '/tokenizer.model'  # トークナイザーのパスを設定\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)  # モデルを初期化\n",
    "            model.load_weights(WEIGHTS_PATH + '/gemma-2b-it.ckpt')  # 重みをロード\n",
    "            self.model = model.to(self._device).eval()  # モデルを指定したデバイスに移動し評価モードにする\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = str(self.formatter)  # プロンプトとしてフォーマッタの状態を取得\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出し\n",
    "        response = self._parse_response(response, obs)  # 応答を解析\n",
    "        print(f\"{response=}\")  # 応答を出力\n",
    "        return response\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # サブクラスで実装すべきメソッド\n",
    "    def _call_llm(self, prompt, max_nt=40, **sampler_kwargs):\n",
    "        # LLMを呼び出して応答を生成する\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {'temperature': 0.8, 'top_p': 0.9, 'top_k': 60,}\n",
    "        response = self.model.generate(\n",
    "            prompt, device=self._device, output_len=max_nt, **sampler_kwargs,)\n",
    "        return response\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 応答からキーワードを解析する\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None: \n",
    "            keyword = ''\n",
    "        else: \n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # サブクラスで実装すべきメソッド\n",
    "\n",
    "# 2つのイテラブルを交互に結合する関数\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "# GemmaQuestionerAgentクラスの定義\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に格納\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # フォーマッタにターンを適用\n",
    "        # ターンのタイプに応じたプロンプトを設定する\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを推測してください。推測を二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 応答を解析する\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: \n",
    "                question = \"それは人ですか？\"  # 人、場所、物の中からランダムに選択\n",
    "            else: \n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else: \n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "# GemmaAnswererAgentクラスの定義\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}でカテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に格納\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # フォーマッタにターンを適用\n",
    "        self.formatter.user(f\"質問はキーワード{obs.keyword}に関するものです。はいかいいえで答えて、答えを二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# システムプロンプトと例のセットを定義する\n",
    "sp = \"あなたは20の質問ゲームをプレイしています。このゲームでは、回答者がキーワードを考え、質問者のはい・いえの質問に答えます。キーワードは特定の人、場所、または物です。\"\n",
    "fse = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "agent = None\n",
    "\n",
    "# エージェントを取得する関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(sp=sp, fse=fse,)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(sp=sp, fse=fse,)\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # エージェントの応答を取得する\n",
    "    if obs.turnType == \"ask\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\": \n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1: \n",
    "        return \"はい\"\n",
    "    else: \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82da8e3",
   "metadata": {},
   "source": [
    "# テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pygameライブラリをインストールする\n",
    "!pip install -q pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggleの環境を作成するために必要なライブラリをインポートする\n",
    "from kaggle_environments import make\n",
    "# 20の質問ゲームのKaggle環境を作成する\n",
    "env = make(\"llm_20_questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コードを実行する\n",
    "%run submission/main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 修正が必要です ****\n",
    "# 環境で質問者エージェントとランダムエージェントを実行する\n",
    "# env.run([get_agent('questioner'), \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 環境の状態をIPythonモードで表示する\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c74bd9",
   "metadata": {},
   "source": [
    "# パッケージ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pigzとpvパッケージをインストールする（出力は非表示にする）\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pigzを使用して、submissionディレクトリを圧縮し、進行状況を表示しながらtar.gzファイルを作成する\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/2b-it/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
