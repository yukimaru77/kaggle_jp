{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7cc7205",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、Kaggleにおける「20の質問」ゲーム用にGemmaモデルを使用するためのガイドです。主な目的は、新しいユーザーがGemmaを設定し、使用する際の手順を示すことです。本ノートブックは、LLM（大規模言語モデル）の利用を容易にする助けとなり、特にGemmaの実装に焦点を当てています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "ノートブックは、Gemmaモデルのインストールから、モデルの読み込み、簡単な質問応答の実行に至るまでの全プロセスをステップバイステップで解説しています。特に、Gemmaを活用して「20の質問」形式のゲームにおけるインタラクションを実現することに特化しています。\n",
    "\n",
    "### 使用している手法とライブラリ\n",
    "1. **ライブラリのインストール**:\n",
    "    - `immutabledict`と`sentencepiece`などのPythonパッケージを使用しています。これらは、Gemmaモデルの動作に必要な依存関係です。\n",
    "\n",
    "2. **Gemmaのクローンと設定**:\n",
    "    - Gitを使用して`gemma_pytorch`リポジトリをクローンし、関連ファイルを指定のディレクトリに移動しています。\n",
    "\n",
    "3. **モデルの読み込み**:\n",
    "    - PyTorchを使用し、GemmaモデルをGPUに読み込む設定がなされています。`GemmaForCausalLM`クラスを用いてモデルの設定と重量の読み込みを行っています。\n",
    "\n",
    "4. **質問応答の実施**:\n",
    "    - モデルの`generate`メソッドを使用して、プロンプトに対する応答を生成しています。この部分では、出力の多様性を制御するための`temperature`や`top_p`、`top_k`の設定が行われています。\n",
    "\n",
    "5. **インストラクションチューニング**:\n",
    "    - モデルに「20の質問」ゲームのAIアシスタントとしての行動を促すプロンプトを用いることで、具体的なタスクに特化した応答を得る技術を実装しています。\n",
    "\n",
    "このノートブックは、Gemmaモデルの設置から質問応答の実行まで、一連のプロセスを分かりやすく解説しており、実際の使用例を通じて読者に対して実践的なガイドを提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5320aa",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提供されたJupyter Notebookの内容を基に、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説です。\n",
    "\n",
    "1. **Gemma**:\n",
    "   - 特定のLLM（大規模言語モデル）で、生成AIタスク用に設計されたモデル。このノートブックでは、Gemmaモデルを用いて「20の質問」ゲームのような対話形式のアプリケーションを構築するための手順が示されています。\n",
    "\n",
    "2. **プロンプト (prompt)**:\n",
    "   - モデルに与える入力文。モデルはこのプロンプトに基づいて出力を生成するため、プロンプトの設計は結果に大きな影響を与えます。\n",
    "\n",
    "3. **トークナイザー**:\n",
    "   - テキストをモデルが理解できる形式（トークン）に変換するツール。トークンは、単語や文の構成要素を小さく分割したもので、モデルはこれらのトークンを処理します。\n",
    "\n",
    "4. **temperature**:\n",
    "   - 生成するテキストの多様性を制御するパラメーター。低い値（例: 0.01）を設定すると、モデルはより規則的で確実な（ただし創造的ではない）出力を生成し、高い値では多様で創造的な出力を生成します。\n",
    "\n",
    "5. **top_p (nucleus sampling)**:\n",
    "   - 確率的サンプリング手法の一つで、累積確率がtop_pの値を超えるトークンのみを考慮して生成を行います。これにより、予測されるトークンの中からより有効な選択肢を絞ることができます。\n",
    "\n",
    "6. **top_k**:\n",
    "   - サンプリング手法で、モデルが次のトークンとして選ぶことができる可能性のある上位k個のトークンに基づいて出力を生成します。これによって出力の多様性が制御されます。\n",
    "\n",
    "7. **CUDA**:\n",
    "   - NVIDIAのGPUを利用するためのプラットフォーム。PyTorchでは、モデルをCUDA対応デバイスに移動させることで、計算を高速化できます。\n",
    "\n",
    "8. **immutabledict**:\n",
    "   - 変更不可能な辞書型データ構造。通常の辞書と異なり、内容を変更できないため、データの不変性が保証されます。\n",
    "\n",
    "9. **instruction tuning**:\n",
    "   - モデルに特定のタスクを実行させるための調整手法。プロンプトエンジニアリングと密接に関連しており、ユーザーの要求に基づいてモデルの出力を最適化します。\n",
    "\n",
    "10. **evaluation mode (評価モード)**:\n",
    "    - モデルが訓練されずにデータを処理する際の状態。評価モードでは、ドロップアウトやバッチ正規化などの特定の訓練時の操作が無効になります。\n",
    "\n",
    "これらの用語は、初心者が特定のコンセプトを理解する際に役立つ基盤となる情報です。ノートブックの内容を理解するためや、Gemmaモデルを活用する際に重要な要素です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa060dc",
   "metadata": {},
   "source": [
    "このノートブックは、KaggleでGemmaを使用するのが初めての方のためのものです。  \n",
    "その使い方をステップバイステップで見ていきましょう。\n",
    "\n",
    "**謝辞**  \n",
    "このノートブックは[LLM 20 Questions スターターノートブック](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook)に基づいています。\n",
    "\n",
    "始める前に、GPUの設定を確認してください。\n",
    "\n",
    "<img width=\"400px\" src=attachment:9319be98-0139-41f9-a741-6b606b7cadc6.png>\n",
    "\n",
    "---\n",
    "\n",
    "## Gemmaモデルの追加\n",
    "\n",
    "1. `+ Add Input`ボタンをクリックします。\n",
    "<img src=attachment:6dabe5ed-9e7e-437b-be73-ee0c3b3a2afe.png width=\"400\">\n",
    "\n",
    "2. `Gemma`モデルを検索します。\n",
    "<img src=attachment:68d9d1fa-84f4-4c37-acf9-f5cb2b825fcb.png width=\"400\">\n",
    "\n",
    "3. パラメーターを設定します。（私は*2b-it*を使用しました。なぜなら*7b*はテストが遅すぎるからです）\n",
    "<img src=attachment:c7f1744e-08df-4ba7-ab61-4b0cd4e4e6fd.png width=\"400\">\n",
    "\n",
    "4. モデルが追加されたことが確認できます。\n",
    "<img src=attachment:056f294d-eee3-4bbb-b68b-6bc991a9c746.png width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:20:28.755136Z",
     "iopub.status.busy": "2024-07-12T06:20:28.754795Z",
     "iopub.status.idle": "2024-07-12T06:20:28.781595Z",
     "shell.execute_reply": "2024-07-12T06:20:28.780519Z",
     "shell.execute_reply.started": "2024-07-12T06:20:28.755109Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 入力ファイルは`/kaggle/input`ディレクトリにあります。\n",
    "# `tree`コマンドを使用して、指定したディレクトリ内のファイルとフォルダーの構造を表示します。\n",
    "tree /kaggle/input/gemma/pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467bdc46",
   "metadata": {},
   "source": [
    "# Gemmaをコーディングするための設定\n",
    "\n",
    "Gemmaモデルを使用するためには、いくつかの設定が必要です。\n",
    "\n",
    "## 依存関係のインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:22:35.489033Z",
     "iopub.status.busy": "2024-07-12T06:22:35.48834Z",
     "iopub.status.idle": "2024-07-12T06:22:50.198234Z",
     "shell.execute_reply": "2024-07-12T06:22:50.197376Z",
     "shell.execute_reply.started": "2024-07-12T06:22:35.489002Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# `/kaggle/working/submission/lib`ディレクトリにPythonパッケージをインストールします。\n",
    "mkdir -p /kaggle/working/submission/lib\n",
    "# immutabledictとsentencepieceパッケージを指定したディレクトリにインストールします。\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "\n",
    "# `gemma_pytorch`リポジトリをダウンロードして、`/kaggle/working/submission/lib`ディレクトリに移動します。\n",
    "cd /kaggle/working\n",
    "# gitを使用してgemma_pytorchリポジトリをクローンします。出力は非表示にします。\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemma_pytorchからgemmaのファイルを指定したディレクトリに移動します。\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:22:51.821292Z",
     "iopub.status.busy": "2024-07-12T06:22:51.820345Z",
     "iopub.status.idle": "2024-07-12T06:22:51.862287Z",
     "shell.execute_reply": "2024-07-12T06:22:51.861431Z",
     "shell.execute_reply.started": "2024-07-12T06:22:51.821259Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 結果を確認します。\n",
    "# `tree`コマンドを使用して、指定したディレクトリ内のファイルとフォルダーの構造を表示します。\n",
    "tree /kaggle/working/submission/lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:25:49.971923Z",
     "iopub.status.busy": "2024-07-12T06:25:49.971287Z",
     "iopub.status.idle": "2024-07-12T06:25:49.976324Z",
     "shell.execute_reply": "2024-07-12T06:25:49.975257Z",
     "shell.execute_reply.started": "2024-07-12T06:25:49.971892Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# `/kaggle/working/submission/lib`ディレクトリ内のライブラリを使用するために、モジュールのパスを追加します。\n",
    "# これにより、指定したディレクトリからライブラリをインポートできるようになります。\n",
    "sys.path.insert(0, \"/kaggle/working/submission/lib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c7a091",
   "metadata": {},
   "source": [
    "## Gemmaモデルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:35:36.82969Z",
     "iopub.status.busy": "2024-07-12T06:35:36.829336Z",
     "iopub.status.idle": "2024-07-12T06:35:36.835864Z",
     "shell.execute_reply": "2024-07-12T06:35:36.834928Z",
     "shell.execute_reply.started": "2024-07-12T06:35:36.829661Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# グローバルパラメーターの設定\n",
    "# 使用可能な場合はGPUを使用し、そうでなければCPUを使用します。\n",
    "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "# 使用するGemmaモデルのバリアントを指定します。\n",
    "VARIANT = \"2b-it\"\n",
    "# VARIANT = \"7b-it-quant\"  # 他のバリアントをコメントアウトとして使用できます。\n",
    "# モデルの重みが格納されているパスを指定します。\n",
    "WEIGHTS_PATH = f\"/kaggle/input/gemma/pytorch/{VARIANT}/2\" # 入力モデルパス\n",
    "\n",
    "# 現在のデバイスとモデルバリアントを表示します。\n",
    "print(\"current device is\", DEVICE)\n",
    "print(\"current Gemma model variant is\", VARIANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:35:37.114006Z",
     "iopub.status.busy": "2024-07-12T06:35:37.113721Z",
     "iopub.status.idle": "2024-07-12T06:35:40.834872Z",
     "shell.execute_reply": "2024-07-12T06:35:40.833859Z",
     "shell.execute_reply.started": "2024-07-12T06:35:37.113984Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデル設定を行います\n",
    "def get_model_config(variant):\n",
    "    # バリアントが\"2b\"の場合、2bモデル用の設定を取得します。\n",
    "    if \"2b\" in variant:\n",
    "        return get_config_for_2b()\n",
    "    # バリアントが\"7b\"の場合、7bモデル用の設定を取得します。\n",
    "    if \"7b\" in variant:\n",
    "        return get_config_for_7b()\n",
    "    # サポートされていないバリアントの場合、エラーを発生させます。\n",
    "    raise ValueError(f\"Not supported variant - {variant}\")\n",
    "\n",
    "# モデルの設定を取得します\n",
    "model_config = get_model_config(VARIANT)\n",
    "# トークナイザーのモデルファイルのパスを設定します\n",
    "model_config.tokenizer = f\"{WEIGHTS_PATH}/tokenizer.model\"\n",
    "# VARIANTに\"quant\"が含まれているかどうかを設定します\n",
    "model_config.quant = \"quant\" in VARIANT\n",
    "# デフォルトのデータ型をモデル設定に基づいて設定します\n",
    "torch.set_default_dtype(model_config.get_dtype())\n",
    "\n",
    "# Gemmaモデルを読み込みます\n",
    "gemma_model = GemmaForCausalLM(model_config)\n",
    "# モデルの重みを指定したパスから読み込みます\n",
    "gemma_model.load_weights(f\"{WEIGHTS_PATH}/gemma-{VARIANT}.ckpt\")\n",
    "# モデルを指定したデバイスに移動し、評価モードに設定します\n",
    "gemma_model = gemma_model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3b63e8",
   "metadata": {},
   "source": [
    "## Gemmaの使用\n",
    "\n",
    "### 簡単なQA（質問応答）\n",
    "\n",
    "`generate`メソッドを使用して、モデルを利用することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:37:45.814383Z",
     "iopub.status.busy": "2024-07-12T06:37:45.813806Z",
     "iopub.status.idle": "2024-07-12T06:37:48.814026Z",
     "shell.execute_reply": "2024-07-12T06:37:48.813056Z",
     "shell.execute_reply.started": "2024-07-12T06:37:45.814353Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"Let me know how to use you.\"\n",
    "\n",
    "# gemma_modelのgenerateメソッドを使用して、モデルの回答を生成します。\n",
    "model_answer = gemma_model.generate(\n",
    "    prompts=prompt,  # モデルに与えるプロンプト\n",
    "    device=torch.device(DEVICE),  # 使用するデバイスを指定\n",
    "    output_len=100,  # 出力する最大トークン数\n",
    "    temperature=0.01,  # 出力の多様性を制御する温度\n",
    "    top_p=0.1,  # 確率的サンプルのトップPフィルタリング\n",
    "    top_k=1,  # トップKフィルタリングの設定\n",
    ")\n",
    "\n",
    "# モデルの回答を表示します\n",
    "print(model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54d2332",
   "metadata": {},
   "source": [
    "### インストラクションチューニングによるQA\n",
    "\n",
    "インストラクションチューニングを使用して、プロンプトエンジニアリングを行うことができます。  \n",
    "詳細なコードは、[スターターノートブック](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook)の`GemmaFormatter`クラスで確認できます。\n",
    "\n",
    "参考文献: https://ai.google.dev/gemma/docs/formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:44:23.61528Z",
     "iopub.status.busy": "2024-07-12T06:44:23.614886Z",
     "iopub.status.idle": "2024-07-12T06:44:23.621332Z",
     "shell.execute_reply": "2024-07-12T06:44:23.620262Z",
     "shell.execute_reply.started": "2024-07-12T06:44:23.615248Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"あなたは「20の質問」ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者の「はい」または「いいえ」の質問に応じます。そのキーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "# プロンプトを作成します\n",
    "prompt = \"<start_of_turn>user\\n\"  # ユーザーのターンの開始を示す\n",
    "prompt += f\"{system_prompt}<end_of_turn>\\n\"  # システムプロンプトを追加し、ターンの終了を示す\n",
    "prompt += \"<start_of_turn>model\\n\"  # モデルのターンの開始を示す\n",
    "\n",
    "# 作成したプロンプトを表示します\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-12T06:44:23.858165Z",
     "iopub.status.busy": "2024-07-12T06:44:23.857815Z",
     "iopub.status.idle": "2024-07-12T06:44:26.954729Z",
     "shell.execute_reply": "2024-07-12T06:44:26.953599Z",
     "shell.execute_reply.started": "2024-07-12T06:44:23.858138Z"
    }
   },
   "outputs": [],
   "source": [
    "model_answer = gemma_model.generate(\n",
    "    prompts=prompt,  # 上で作成したプロンプトをモデルに渡します\n",
    "    device=torch.device(DEVICE),  # 使用するデバイスを指定\n",
    "    output_len=100,  # 出力する最大トークン数\n",
    "    temperature=0.01,  # 出力の多様性を制御する温度\n",
    "    top_p=0.1,  # 確率的サンプルのトップPフィルタリング\n",
    "    top_k=1,  # トップKフィルタリングの設定\n",
    ")\n",
    "\n",
    "# モデルの回答を表示します\n",
    "print(model_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5692f699",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "これで、コンペティションのためにGemmaモデルを使用する方法がわかることを願っています。  \n",
    "このノートブックがスターターノートブックを理解するのに役立つことを願っています。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
