{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74095c6f",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、Kaggleの「LLM 20 Questions」コンペティションにおけるエージェント作成プロセスを示しています。具体的には、質問者と回答者の役割を果たすエージェントを構築し、ターゲットワードを推測するためのロジックを実装しています。\n",
    "\n",
    "### 問題に取り組む内容\n",
    "ノートブックでは、「20の質問」ゲームのエージェントを作成する過程が中心テーマです。これには、質問をする「質問者エージェント」と、「はい」または「いいえ」で回答する「回答者エージェント」を含む2つのエージェントを構築することが目的とされています。また、効率的な質問の生成と応答の解析を行い、ゲームを進行させることが求められています。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **PyTorch**: モデリングに使用される深層学習フレームワークです。エージェントはPyTorch上で動作するGemma言語モデルを利用しています。\n",
    "- **Gemma**: Googleが提供する大規模言語モデルであり、特に因果推論と会話形式のプロンプトに対する応答生成に用いられます。\n",
    "- **Forumatted conversation**: GemmaFormatterクラスを用いて、ユーザーとモデルのターンを構成するフォーマット機能を実装しています。このクラスは会話の状態を保持し、適切なプロンプトを生成します。\n",
    "- **エージェントの設計**: `GemmaAgent`クラスとそのサブクラス（`GemmaQuestionerAgent`および`GemmaAnswererAgent`）が定義され、エージェントの初期化、プロンプト処理、応答生成などの機能が実装されています。\n",
    "- **ユーティリティ関数**: 質問と応答を交互に組み合わせるための`interleave_unequal`関数や、エージェントを取得するための`get_agent`関数が用意されています。\n",
    "\n",
    "### 生成されるアウトプット\n",
    "ノートブックの実行により、`submission.tar.gz`というファイルが生成され、このファイルはKaggleのコンペティションに提出するために必要です。このプロセスにおいて、エージェントが大会の規則に従って機能するかどうかを確認するためのテストが行われます。\n",
    "\n",
    "全体として、このノートブックは、言語モデルを活用してインタラクティブなゲーム環境を作成する方法を体系的に示しています。環境構築からエージェントの実装、最終的な提出準備までの一連の流れをカバーしています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc37891",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連するが、初心者がつまずく可能性のある専門用語や概念を解説します。これにより、ノートブック特有のフレームワークや技術に対する理解を深める手助けとなることを目指しています。\n",
    "\n",
    "1. **エージェント (Agent)**:\n",
    "   - 機械学習や強化学習の文脈では、エージェントは環境と相互作用しながら行動を学習する主体を指します。このコンペティションでは、質問者（GemmaQuestionerAgent）と回答者（GemmaAnswererAgent）の2種類のエージェントが存在します。\n",
    "\n",
    "2. **プロンプト (Prompt)**:\n",
    "   - 言語モデルに対して、出力を生成するために与える入力文のことです。このノートブックでは、エージェントの振る舞いや特定のタスクを指示するために使用されます。\n",
    "\n",
    "3. **`GemmaFormatter`**:\n",
    "   - エージェントが生成する会話のフォーマットを担当するクラスです。ユーザーとモデルの対話を構造化し、ターンの開始と終了を示すトークンを使用して、整然とした会話形式に整えます。\n",
    "\n",
    "4. **量子化 (Quantization)**:\n",
    "   - モデルのパラメータを低いビット幅（例: 32ビット浮動小数点から8ビット整数）に変換するプロセスです。これにより、モデルのメモリ使用量を削減し、推論速度を向上させることができます。特にリソースが制限されている環境での使用がますます重要になっています。\n",
    "\n",
    "5. **サンプリング (Sampling)**:\n",
    "   - モデルからの出力を生成する際の手法のことです。「トップK」や「トップP」サンプリングなどのテクニックが用いられ、生成するトークンを選択する際のランダム性の度合いを調整します。これにより、生成されるテキストの多様性を制御します。\n",
    "\n",
    "6. **コンテキストマネージャ (Context Manager)**:\n",
    "   - Pythonにおける特別な構文で、特定のコードブロックの前後で特定の設定や状態を制御するために使用されます。本ノートブックでは、PyTorchのテンソルのデフォルトデータ型を一時的に設定するために使用されています。\n",
    "\n",
    "7. **イテラブル (Iterable)**:\n",
    "   - Pythonにおいて、ループ可能なオブジェクト（リスト、タプル、辞書など）を指します。このノートブックでは、特にターンを適用するためのデータ型を示す際に用いられています。\n",
    "\n",
    "8. **正規表現 (Regular Expression)**:\n",
    "   - 特定のパターンに基づいた文字列検索を行うための強力なツールです。このノートブックでは、応答から質問やキーワードを抽出する際に使用されています。\n",
    "\n",
    "9. **自動微分 (Automatic Differentiation)**:\n",
    "   - 深層学習のフレームワーク（この場合、PyTorch）で重要な特徴で、モデルのパラメータに関する勾配を自動的に計算する機能です。これは、最適化アルゴリズムが効率的に働くために必要です。\n",
    "\n",
    "10. **チェックポイント (Checkpoint)**:\n",
    "   - モデルの状態を保存するためのスナップショットです。トレーニング中に定期的にモデルの重みやバイアスを保存し、後で再開したり評価したりするために使用されます。\n",
    "\n",
    "これらの用語は、専門用語の多い機械学習および深層学習の分野において、特に具体的な実装やフレームワークに関連しており、初心者が理解するのが難しいことがあります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293a7f4f",
   "metadata": {},
   "source": [
    "こんにちは皆さん、私はコードをいくつかの部分に分け、chatGPTから生成されたコメントや説明を追加しました。\n",
    "\n",
    "このリンクのGemmaの例（https://www.kaggle.com/models/google/gemma/PyTorch/7b-it-quant/2）は、初心者にとって役立つかもしれません。\n",
    "\n",
    "このノートブックが皆さんの役に立つことを願っています。\n",
    "\n",
    "---\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。このファイルは、右側の**コンペティションに提出**の見出しから直接提出できます。あるいは、ノートブックビューアから*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードします。コンペティションのホームページの左上にある**エージェントを提出**をクリックして、ファイルをアップロードし、提出を行ってください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:25.021619Z",
     "iopub.status.busy": "2024-05-25T15:58:25.020891Z",
     "iopub.status.idle": "2024-05-25T15:58:38.22655Z",
     "shell.execute_reply": "2024-05-25T15:58:38.225563Z",
     "shell.execute_reply.started": "2024-05-25T15:58:25.021585Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working # 作業ディレクトリに移動します\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece # 必要なパッケージをインストールします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null # GitHubからgemma_pytorchリポジトリをクローンします（出力は表示しません）\n",
    "mkdir /kaggle/working/submission/lib/gemma/ # gemma用のディレクトリを作成します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/ # gemmaのファイルを新しいディレクトリに移動します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33908425",
   "metadata": {},
   "source": [
    "# パート1: ファイル作成\n",
    "\n",
    "- `%%writefile`は、ノートブックにこの行の下にあるすべてをファイルに書き込むよう指示します。\n",
    "- `submission/main.py`は、ファイルが書き込まれるパスです。もし`submission`というディレクトリが存在しない場合は、作成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:41.461435Z",
     "iopub.status.busy": "2024-05-25T15:58:41.46109Z",
     "iopub.status.idle": "2024-05-25T15:58:41.465484Z",
     "shell.execute_reply": "2024-05-25T15:58:41.46457Z",
     "shell.execute_reply.started": "2024-05-25T15:58:41.461408Z"
    }
   },
   "outputs": [],
   "source": [
    "# #%%writefile submission/main.py\n",
    "# # セットアップ\n",
    "import os # オペレーティングシステムに関する機能を提供するモジュールをインポートします\n",
    "import sys # Pythonのインタプリタに関する機能を提供するモジュールをインポートします"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffd427f",
   "metadata": {},
   "source": [
    "# パート2: 必要なライブラリのインポートとウェイトパスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:42.15402Z",
     "iopub.status.busy": "2024-05-25T15:58:42.153406Z",
     "iopub.status.idle": "2024-05-25T15:58:45.700446Z",
     "shell.execute_reply": "2024-05-25T15:58:45.699681Z",
     "shell.execute_reply.started": "2024-05-25T15:58:42.153989Z"
    }
   },
   "outputs": [],
   "source": [
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\" # Kaggleエージェントのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH): # Kaggleエージェントのパスが存在するか確認します\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib')) # 存在する場合、ライブラリパスをシステムパスに追加します\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\") # 存在しない場合、別のライブラリパスを追加します\n",
    "\n",
    "import contextlib # コンテキストマネージャを使いやすくするモジュールをインポートします\n",
    "import os # オペレーティングシステムに関する機能を提供するモジュールをインポートします\n",
    "import sys # Pythonのインタプリタに関する機能を提供するモジュールをインポートします\n",
    "from pathlib import Path # パス操作のためのモジュールをインポートします\n",
    "\n",
    "import torch # PyTorchライブラリをインポートします\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b # Gemmaの設定用関数をインポートします\n",
    "from gemma.model import GemmaForCausalLM # Gemmaのモデルをインポートします\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH): # Kaggleエージェントのパスが存在するか確認します\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\") # 存在する場合、ウェイトパスを設定します\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\" # 存在しない場合、別のウェイトパスを設定します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73929e1e",
   "metadata": {},
   "source": [
    "# パート3: GemmaFormatterによるプロンプトフォーマット\n",
    "\n",
    "- `GemmaFormatter`: ユーザーとモデルの間の会話をフォーマットするためのもので、ターンの開始と終了を示すために事前に定義されたトークンを使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.702729Z",
     "iopub.status.busy": "2024-05-25T15:58:45.702182Z",
     "iopub.status.idle": "2024-05-25T15:58:45.714485Z",
     "shell.execute_reply": "2024-05-25T15:58:45.713694Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.702695Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools # イテレータの操作を支援するモジュールをインポートします\n",
    "from typing import Iterable # Iterableなデータ型を扱うための型ヒントをインポートします\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>' # ターンの開始を示すトークン\n",
    "    _end_token = '<end_of_turn>' # ターンの終了を示すトークン\n",
    "\n",
    "    # 初期化\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt # システムプロンプトを設定します\n",
    "        self._few_shot_examples = few_shot_examples # 少数の例を設定します\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\" # ユーザターンのフォーマット\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\" # モデルターンのフォーマット\n",
    "        self.reset() # 状態をリセットします\n",
    "\n",
    "    # 現在の会話の状態を文字列として返します\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    # ユーザープロンプトを会話に追加するメソッド\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt) # プロンプトをユーザターンに追加します\n",
    "        return self\n",
    "        \n",
    "    # モデルの応答を会話に追加するメソッド\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt) # プロンプトをモデルターンに追加します\n",
    "        return self\n",
    "\n",
    "    # ユーザターンの開始を示すメソッド\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\" # ユーザターンの開始マーカーを追加します\n",
    "        return self\n",
    "\n",
    "    # モデルターンの開始を示すメソッド\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\" # モデルターンの開始マーカーを追加します\n",
    "        return self\n",
    "\n",
    "    # 現在のターンの終了を示すメソッド\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\" # 終了マーカーを追加します\n",
    "        return self\n",
    "\n",
    "    # リセットメソッド\n",
    "    def reset(self):\n",
    "        # `_state`を空文字列で初期化します\n",
    "        self._state = \"\"  \n",
    "\n",
    "        # 提供されている場合、システムプロンプトを追加します。\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  \n",
    "            \n",
    "        # 提供されている場合、少数の例にターンを適用します\n",
    "        if self._few_shot_examples is not None: \n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user') # 初めのエージェントをユーザーに設定します\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 初めのエージェントに応じたフォーマッターの順序を設定します\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters) # フォーマッターをサイクルします\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn) # 各ターンにフォーマッターを適用します\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683d9412",
   "metadata": {},
   "source": [
    "### 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.715692Z",
     "iopub.status.busy": "2024-05-25T15:58:45.715435Z",
     "iopub.status.idle": "2024-05-25T15:58:45.730863Z",
     "shell.execute_reply": "2024-05-25T15:58:45.730057Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.715671Z"
    }
   },
   "outputs": [],
   "source": [
    "# システムプロンプトと少数の例でフォーマッタを初期化します\n",
    "formatter = GemmaFormatter(\n",
    "    system_prompt=\"This is a system prompt.\", # システムプロンプトを設定します\n",
    "    few_shot_examples=[\"Example question?\", \"Example answer.\"] # 少数の例を設定します\n",
    ")\n",
    "\n",
    "# ユーザターンを追加します\n",
    "formatter.user(\"What is the capital of France?\") # 「フランスの首都は何ですか？」というユーザーの質問を追加します\n",
    "\n",
    "# モデルターンを追加します\n",
    "formatter.model(\"The capital of France is Paris.\") # モデルの応答「フランスの首都はパリです」を追加します\n",
    "\n",
    "# フォーマットされた会話を表示します\n",
    "print(formatter) # 作成された会話の状態を出力します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e457170c",
   "metadata": {},
   "source": [
    "`GemmaFormatter`クラスは、会話を一貫した方法で構造化しフォーマットするのに役立ち、ターンが適切にマークされ組織されることを保証します。\n",
    "\n",
    "# パート4: エージェントの定義とユーティリティ\n",
    "\n",
    "- `_set_default_tensor_type`コンテキストマネージャは、一時的にPyTorchテンソルのデフォルトデータ型を指定された型に設定し、コンテキストマネージャを使用するコードブロックが実行された後に元のtorch.floatにリセットします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.7338Z",
     "iopub.status.busy": "2024-05-25T15:58:45.733075Z",
     "iopub.status.idle": "2024-05-25T15:58:45.741278Z",
     "shell.execute_reply": "2024-05-25T15:58:45.740545Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.733776Z"
    }
   },
   "outputs": [],
   "source": [
    "import re # 正規表現操作のためのモジュールをインポートします\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"与えられたdtypeにデフォルトのtorchのdtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype) # デフォルトのデータ型を指定された型に設定します\n",
    "    yield # コンテキストブロック内のコードを実行します\n",
    "    torch.set_default_dtype(torch.float) # 実行後、デフォルトのデータ型をtorch.floatにリセットします"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c3bdc5",
   "metadata": {},
   "source": [
    "### 例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.742569Z",
     "iopub.status.busy": "2024-05-25T15:58:45.742251Z",
     "iopub.status.idle": "2024-05-25T15:58:45.76648Z",
     "shell.execute_reply": "2024-05-25T15:58:45.765652Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.742535Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([1.0, 2.0]).dtype # 現在のデフォルトのデータ型でテンソルを作成し、そのデータ型を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.767743Z",
     "iopub.status.busy": "2024-05-25T15:58:45.767473Z",
     "iopub.status.idle": "2024-05-25T15:58:45.772646Z",
     "shell.execute_reply": "2024-05-25T15:58:45.771803Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.767721Z"
    }
   },
   "outputs": [],
   "source": [
    "with _set_default_tensor_type(torch.float64): # デフォルトのデータ型をtorch.float64に設定した状態で以下を実行します\n",
    "    print(torch.tensor([1.0, 2.0]).dtype) # 指定されたデータ型でテンソルを作成し、そのデータ型を表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46f28699",
   "metadata": {},
   "source": [
    "# パート5: 基本GemmaAgentクラス\n",
    "\n",
    "GemmaAgentクラスは以下の目的で設計されています：\n",
    "\n",
    "- 言語モデルを初期化し、設定する。\n",
    "- プロンプトと応答をフォーマットし、処理する。\n",
    "- コンテキストマネージャを使用してテンソルのデータ型を一時的に設定する。\n",
    "- フォーマットされたプロンプトに基づいて応答を生成するためにモデルと対話する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.949245Z",
     "iopub.status.busy": "2024-05-25T15:58:45.948631Z",
     "iopub.status.idle": "2024-05-25T15:58:45.960292Z",
     "shell.execute_reply": "2024-05-25T15:58:45.959431Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.949221Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant # モデルのバリアントを設定します\n",
    "        self._device = torch.device(device) # 使用するデバイス（GPUまたはCPU）を設定します\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples) # フォーマッタを初期化します\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        \n",
    "        # モデルの設定\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b() # バリアントに応じてモデルの設定を取得します\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\") # トークナイザーのパスを設定します\n",
    "        model_config.quant = \"quant\" in variant # 量子化設定を行います\n",
    "        \n",
    "        # モデルの初期化\n",
    "        with _set_default_tensor_type(model_config.get_dtype()): # モデルのデフォルトデータ型を設定します\n",
    "            model = GemmaForCausalLM(model_config) # 言語モデルを初期化します\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt') # チェックポイントのパスを設定します\n",
    "            model.load_weights(ckpt_path) # チェックポイントからモデルの重みをロードします\n",
    "            self.model = model.to(self._device).eval() # モデルをデバイスに移動し、評価モードにします\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs) # セッションを開始します\n",
    "        prompt = str(self.formatter) # フォーマットされたプロンプトを取得します\n",
    "        response = self._call_llm(prompt) # モデルにプロンプトを渡して応答を生成します\n",
    "        response = self._parse_response(response, obs) # 応答を解析します\n",
    "        print(f\"{response=}\") # 生成された応答を表示します\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError # サブクラスで実装する必要があります\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = { # サンプラーの引数のデフォルト値を設定します\n",
    "                'temperature': 0.01, # 温度パラメータ\n",
    "                'top_p': 0.1, # トップPサンプリング\n",
    "                'top_k': 1, # トップKサンプリング\n",
    "        }\n",
    "        response = self.model.generate( # モデルに応じた新しいトークンを生成します\n",
    "            prompt,\n",
    "            device=self._device, # 使用するデバイスを指定します\n",
    "            output_len=max_new_tokens, # 生成するトークン数を指定します\n",
    "            **sampler_kwargs, # 他のサンプラー引数を渡します\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response) # 応答からキーワードを解析します\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower() # キーワードを小文字に変換します\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError # サブクラスで実装する必要があります"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4128fa47",
   "metadata": {},
   "source": [
    "# パート6: GemmaQuestionerAgentクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:46.723064Z",
     "iopub.status.busy": "2024-05-25T15:58:46.722291Z",
     "iopub.status.idle": "2024-05-25T15:58:46.732867Z",
     "shell.execute_reply": "2024-05-25T15:58:46.731969Z",
     "shell.execute_reply.started": "2024-05-25T15:58:46.723034Z"
    }
   },
   "outputs": [],
   "source": [
    "def interleave_unequal(x, y):\n",
    "    # 2つのリスト（xとy）を交互に組み合わせ、片方が短い場合は残りを無視します\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化を行います\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセットします\n",
    "        self.formatter.user(\"20の質問を始めましょう。あなたは質問者の役割を演じています。\") # ユーザーターンを追加します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に組み合わせます\n",
    "        self.formatter.apply_turns(turns, start_agent='model') # フォーマッタにターンを適用します\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\") # 質問を促します\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今度はキーワードを推測してください。推測は二重アスタリスクで囲んでください。\") # 推測を促します\n",
    "        self.formatter.start_model_turn() # モデルターンを開始します\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', '')) # 応答から質問を解析します\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\" # デフォルトの質問を設定します\n",
    "            else:\n",
    "                question = match.group() # 見つかった質問を取得します\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response) # 推測されたキーワードを解析します\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType) # 不正なターンタイプに対するエラーを発生させます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd75a42",
   "metadata": {},
   "source": [
    "GemmaQuestionerAgent:\n",
    "- `__init__` ： 親クラスのコンストラクタを呼び出してエージェントを設定します。\n",
    "- `_start_session` : 質問と回答を交互に組み合わせ、会話のフォーマットを設定します。\n",
    "- `_parse_response` : エージェントが質問をしているか推測をしているかによって、モデルの応答を異なる方法で解釈します。\n",
    "\n",
    "# パート7: GemmaAnswererAgentクラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:47.844243Z",
     "iopub.status.busy": "2024-05-25T15:58:47.843475Z",
     "iopub.status.idle": "2024-05-25T15:58:47.850945Z",
     "shell.execute_reply": "2024-05-25T15:58:47.850005Z",
     "shell.execute_reply.started": "2024-05-25T15:58:47.844215Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化を行います\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセットします\n",
    "        self.formatter.user(f\"20の質問を始めましょう。あなたは回答者の役割を演じています。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\") # ユーザーターンを追加します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に組み合わせます\n",
    "        self.formatter.apply_turns(turns, start_agent='user') # フォーマッタにターンを適用します\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。はいかいいえで答えてください。答えは二重アスタリスクで囲んでください（例: **yes** または **no**）。\") # 答えの形式を指示します\n",
    "        self.formatter.start_model_turn() # モデルターンを開始します\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response) # 応答からキーワードを解析します\n",
    "        return 'yes' if 'yes' in answer else 'no' # 応答に「yes」が含まれていれば'yes'を、そうでなければ'no'を返します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2434c965",
   "metadata": {},
   "source": [
    "# パート8: エージェントの作成と関数定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:49.041213Z",
     "iopub.status.busy": "2024-05-25T15:58:49.040862Z",
     "iopub.status.idle": "2024-05-25T15:58:49.051556Z",
     "shell.execute_reply": "2024-05-25T15:58:49.05063Z",
     "shell.execute_reply.started": "2024-05-25T15:58:49.041185Z"
    }
   },
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者のはい・いいえの質問に応じて答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問を始めましょう。あなたは質問者の役割を演じています。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** さて、キーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義して、必要なエージェントだけをロードします。\n",
    "# 両方をロードすると、アウトオブメモリ（OOM）になる可能性があります。\n",
    "\n",
    "# エージェント変数を初期化します\n",
    "agent = None\n",
    "\n",
    "# 名前に基づいて適切なエージェントを取得する関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # エージェントが初期化されておらず、要求されたエージェントが「質問者」である場合\n",
    "    if agent is None and name == 'questioner':\n",
    "        # 特定のパラメータでGemmaQuestionerAgentを初期化します\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',  # 計算用デバイス\n",
    "            system_prompt=system_prompt,  # エージェントのシステムプロンプト\n",
    "            few_shot_examples=few_shot_examples,  # エージェントの行動をガイドする例\n",
    "        )\n",
    "    # エージェントが初期化されておらず、要求されたエージェントが「回答者」である場合\n",
    "    elif agent is None and name == 'answerer':\n",
    "        # 同じパラメータでGemmaAnswererAgentを初期化します\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    \n",
    "    # エージェントが初期化されていることを確認します\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    # 初期化されたエージェントを返します\n",
    "    return agent\n",
    "\n",
    "# 観測に基づいて相互作用を処理する関数\n",
    "def agent_fn(obs, cfg):\n",
    "    # 観測が質問をするためのものである場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        # 「質問者」エージェントを取得して観測に応答します\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観測が推測をするためのものである場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        # 「質問者」エージェントを取得して観測に応答します\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観測が回答を提供するためのものである場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        # 「回答者」エージェントを取得して観測に応答します\n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    # エージェントからの応答がNoneまたは非常に短い場合\n",
    "    if response is None or len(response) <= 1:\n",
    "        # ポジティブな応答（「はい」）と仮定します\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        # エージェントから受け取った応答を返します\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccec8d6",
   "metadata": {},
   "source": [
    "1. **GemmaFormatterクラス**: このクラスは、ゲームのプロンプトをフォーマットする役割を担っています。ユーザーとモデルのターンを構成し、ユーザーとモデルのターンを開始し、ターンを終了し、状態をリセットし、ターンを適用するメソッドを持っています。エージェントに対するプロンプトの一貫したフォーマットを保証します。\n",
    "\n",
    "2. **GemmaAgentクラス**: ゲームにおける一般的なエージェントを表す抽象クラスです。初期化、呼び出し、セッションの開始、言語モデル（LLM）の呼び出し、応答の解析、デフォルトテンソルタイプの設定といった共通のメソッドと属性を定義しています。\n",
    "\n",
    "3. **GemmaQuestionerAgentクラス**と**GemmaAnswererAgentクラス**: これらのクラスはGemmaAgentから継承され、質問者と回答者のエージェントの特定の動作を実装しています。エージェントの動作をカスタマイズするために、`_start_session`および`_parse_response`メソッドをオーバーライドしています。\n",
    "\n",
    "4. **interleave_unequal関数**: これは異なる長さの2つのリストを交互に組み合わせる関数です。ゲーム内で質問と回答を交互に組み合わせるために使用されます。\n",
    "\n",
    "5. **get_agent関数**: この関数は、入力された名前（「質問者」または「回答者」）に基づいて適切なエージェントを初期化して返します。エージェントのインスタンスが1つだけ作成され再利用されることを保証します。\n",
    "\n",
    "6. **agent_fn関数**: この関数はゲームのエントリーポイントとして機能します。観測のターンタイプ（「ask」、「guess」、「answer」）に基づいて使用するエージェントのタイプを決定し、対応するエージェントの`__call__`メソッドを呼び出して応答を生成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:06.686412Z",
     "iopub.status.busy": "2024-05-25T11:17:06.685976Z",
     "iopub.status.idle": "2024-05-25T11:17:06.693459Z",
     "shell.execute_reply": "2024-05-25T11:17:06.691475Z",
     "shell.execute_reply.started": "2024-05-25T11:17:06.686368Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt install pigz pv > /dev/null # pigzとpvをインストールします（出力は表示しません）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:11.751057Z",
     "iopub.status.busy": "2024-05-25T11:17:11.750579Z",
     "iopub.status.idle": "2024-05-25T11:17:11.756935Z",
     "shell.execute_reply": "2024-05-25T11:17:11.755388Z",
     "shell.execute_reply.started": "2024-05-25T11:17:11.751024Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/gemma/pytorch/7b-it-quant/2 \n",
    "# submissionディレクトリと指定のgemmaモデルパスを圧縮してsubmission.tar.gzファイルを作成します。圧縮にはpigzを使用し、進行状況をpvで表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d97e953",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Mohamed MZAOUALI\n",
    "> \n",
    "> 今はずっと理解できるようになりました。ありがとうございます！\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
