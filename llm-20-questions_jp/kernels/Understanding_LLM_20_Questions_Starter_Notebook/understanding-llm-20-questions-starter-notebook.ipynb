{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "604e44a7",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleのコンペティション「LLM 20 Questions」でのエージェント作成プロセスを解説しています。具体的には、言語モデルを使用して「20の質問」ゲームをプレイするための質問者（質問する役割）と回答者（回答する役割）のエージェントを構築することを目的としています。\n",
    "\n",
    "### 問題の取り組み:\n",
    "- **目的**: 「20の質問」ゲームにおいて、参加者が効率的に情報を収集し、ターゲットとなる単語をできるだけ少ない質問で推測できるようにするエージェントを設計・実装する。\n",
    "- **ゲームルール**: エージェントは互いに質問と答えを通じて、最終的なキーワードを導き出します。\n",
    "\n",
    "### 手法とライブラリ:\n",
    "1. **ライブラリのインポート**: PyTorchとGemmaモデル（GoogleのオープンソースLLM）を使用し、必要なライブラリをインストールして、モデルの設定を行います。\n",
    "2. **GemmaFormatterクラス**: ユーザーとモデル間の会話をフォーマットするためのクラスで、ターンの開始と終了を示すトークンを使用して構成を整えます。\n",
    "3. **GemmaAgentクラス**: ゲームをプレイするための基本的なエージェントクラスで、プロンプトと応答の処理、モデルとのインタラクションの管理を行います。\n",
    "4. **GemmaQuestionerAgentクラスとGemmaAnswererAgentクラス**: それぞれ質問者と回答者の動作を具現化するために、GemmaAgentを継承し、特定の機能を実装します。\n",
    "5. **エージェントの管理**: `get_agent`関数を使用して、適切なエージェントを初期化し、`agent_fn`関数で観察に基づいてエージェントが適切に応答するように設定します。\n",
    "\n",
    "最終的に、ノートブックを実行することで作成される`submission.tar.gz`ファイルには、エージェントに関連するコードが含まれ、Kaggleのコンペティションに直接提出できます。この進行状況が他の参加者にも参考になることを重視して、Gemmaの例を提供する工夫もされています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b08ecd5",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter notebookの内容を基に、機械学習・深層学習の初心者がつまずきそうな専門用語の解説です。特に、実務や特定のドメインにおいてあまり知られていない用語や概念に焦点を当てています。\n",
    "\n",
    "1. **GemmaFormatter**:\n",
    "   - このクラスは、与えられたプロンプトや応答を一定のフォーマットに整形します。「ターン」とは、ユーザーやモデルがコントリビュートする一連のやり取りを指します。このフォーマッタは、会話の構造を保つためにスタートやエンドトークンを使います。\n",
    "\n",
    "2. **Causal Language Model (CLM)**:\n",
    "   - この種のモデルは、対象となる文の一部を生成する際に、その前のトークンのみを考慮する方法で学習されています。たとえば、「私が食べた果物は」は次に「りんご」や「バナナ」といった単語の生成を対象とした文脈を考慮するため、時系列データの生成に特化しています。\n",
    "\n",
    "3. **interleave_unequal関数**:\n",
    "   - 異なる長さの2つのリストを交互に取り出して新しいリストを作成する関数です。不均等なデータを処理したい場合、特に質問と回答のリストを統合する目的で使用されます。\n",
    "\n",
    "4. **エージェント (Agent)**:\n",
    "   - このコンペティション文脈におけるエージェントは、特定のタスク（この場合は「20の質問」ゲーム）を行うために設計されたプログラムであり、環境とインタラクションして学習や推論を行います。\n",
    "\n",
    "5. **コンテキストマネージャ (Context Manager)**:\n",
    "   - Pythonの`with`ステートメントと一緒に使うことで、リソースの管理（例えば、ファイルのオープンとクローズなど）を簡潔に行うための構文です。このノートブックでは、PyTorchのデフォルトのテンソルデータ型を一時的に変更するために用いられています。\n",
    "\n",
    "6. **Few-shot learning**:\n",
    "   - これは、数少ない例から学習する能力を指します。`few_shot_examples`は、Agentがどのように質問や回答を生成するかの刺激として与えられ、その分、学習の効率を上げるために使われるものです。\n",
    "\n",
    "7. **重み (Weights)**:\n",
    "   - 深層学習モデルにおける重みは、ニューラルネットワークの各接続に関連するパラメータで、モデルのパフォーマンスを調整する役割を持ちます。このノートブックでは、`GemmaForCausalLM`で重みを読み込む部分がありますが、これによりモデルがトレーニングされた内容を反映します。\n",
    "\n",
    "8. **トークン (Token)**:\n",
    "   - 自然言語処理におけるトークンとは、文や語を分析可能な最小単位に分割することを指します。通常は単語やサブワードに対応しますが、モデルが入力を扱うための形式です。\n",
    "\n",
    "9. **Tensors**:\n",
    "   - PyTorchで扱われるデータの基本的な形式で、多次元配列を表します。深層学習では、画像やテキストなど、様々なデータを処理するために用いられます。\n",
    "\n",
    "10. **Model Configuration**:\n",
    "    - モデルの設定を指し、どのようにモデルを構築し、運用するかに関する情報（たとえば、エンコーディング手法や入力のサイズなど）を含んでいます。このノートブックでは、`get_config_for_2b`や`get_config_for_7b`といった関数呼び出しで設定を取得しています。\n",
    "\n",
    "これらの用語はいずれも、自身で実装や実務経験を積まないと直面しにくいため、理解に役立つでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887800e",
   "metadata": {},
   "source": [
    "皆さんこんにちは、私はコードをいくつかの部分に分割し、chatGPTから生成されたコメントと説明を追加しました。\n",
    "\n",
    "このリンクにあるGemmaの例（https://www.kaggle.com/models/google/gemma/PyTorch/7b-it-quant/2）は初心者にとって役立つかもしれません。\n",
    "\n",
    "このノートブックが役立つことを願っています。\n",
    "\n",
    "---\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると`submission.tar.gz`ファイルが生成されます。このファイルは、右側の**コンペティションに提出**ヘッダーから直接送信できます。あるいは、ノートブックビュワーから*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードします。競技のホームページの左上にある**Submit Agent**をクリックして、ファイルをアップロードし、提出を行ってください。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:25.021619Z",
     "iopub.status.busy": "2024-05-25T15:58:25.020891Z",
     "iopub.status.idle": "2024-05-25T15:58:38.22655Z",
     "shell.execute_reply": "2024-05-25T15:58:38.225563Z",
     "shell.execute_reply.started": "2024-05-25T15:58:25.021585Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da11c1",
   "metadata": {},
   "source": [
    "# Part 1: ファイル作成\n",
    "\n",
    "- `%%writefile`は、ノートブックに次の行のすべてをファイルに書き込むよう指示します。\n",
    "- `submission/main.py`は、ファイルが書き込まれるパスです。もしディレクトリ`submission`が存在しない場合は、新しく作成されます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:41.461435Z",
     "iopub.status.busy": "2024-05-25T15:58:41.46109Z",
     "iopub.status.idle": "2024-05-25T15:58:41.465484Z",
     "shell.execute_reply": "2024-05-25T15:58:41.46457Z",
     "shell.execute_reply.started": "2024-05-25T15:58:41.461408Z"
    }
   },
   "outputs": [],
   "source": [
    "# #%%writefile submission/main.py\n",
    "# # セットアップ\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be6085",
   "metadata": {},
   "source": [
    "# Part 2: 必要なライブラリのインポートとウェイトパスの設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:42.15402Z",
     "iopub.status.busy": "2024-05-25T15:58:42.153406Z",
     "iopub.status.idle": "2024-05-25T15:58:45.700446Z",
     "shell.execute_reply": "2024-05-25T15:58:45.699681Z",
     "shell.execute_reply.started": "2024-05-25T15:58:42.153989Z"
    }
   },
   "outputs": [],
   "source": [
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81148b",
   "metadata": {},
   "source": [
    "# Part 3: GemmaFormatterによるプロンプトのフォーマット\n",
    "\n",
    "- `GemmaFormatter`：ユーザーとモデル間の会話をフォーマットし、ターンの開始と終了を示す事前定義されたトークンを使用します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.702729Z",
     "iopub.status.busy": "2024-05-25T15:58:45.702182Z",
     "iopub.status.idle": "2024-05-25T15:58:45.714485Z",
     "shell.execute_reply": "2024-05-25T15:58:45.713694Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.702695Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    # 初期化\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    # 会話の現在の状態を文字列として返します。\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    # ユーザープロンプトを会話に追加するメソッド\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "        \n",
    "    # モデルの応答を会話に追加するメソッド\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    # ユーザーのターンの開始を示すメソッド\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    # モデルのターンの開始を示すメソッド\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    # 現在のターンの終了を示すメソッド\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    # リセットメソッド\n",
    "    def reset(self):\n",
    "        # `_state`を空の文字列に初期化\n",
    "        self._state = \"\"  \n",
    "\n",
    "        # 提供された場合、システムプロンプトを追加します。\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  \n",
    "            \n",
    "        # 提供された場合、few-shotの例を適用します。\n",
    "        if self._few_shot_examples is not None: \n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d81f7b",
   "metadata": {},
   "source": [
    "###  例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.715692Z",
     "iopub.status.busy": "2024-05-25T15:58:45.715435Z",
     "iopub.status.idle": "2024-05-25T15:58:45.730863Z",
     "shell.execute_reply": "2024-05-25T15:58:45.730057Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.715671Z"
    }
   },
   "outputs": [],
   "source": [
    "# システムプロンプトとfew-shot例と共にフォーマッタを初期化\n",
    "formatter = GemmaFormatter(\n",
    "    system_prompt=\"これはシステムプロンプトです。\",\n",
    "    few_shot_examples=[\"例の質問？\", \"例の回答。\"]\n",
    ")\n",
    "\n",
    "# ユーザーターンを追加\n",
    "formatter.user(\"フランスの首都はどこですか？\")\n",
    "\n",
    "# モデルターンを追加\n",
    "formatter.model(\"フランスの首都はパリです。\")\n",
    "\n",
    "# フォーマットされた会話を出力\n",
    "print(formatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece06458",
   "metadata": {},
   "source": [
    "`GemmaFormatter`クラスは、会話を一貫した方法で構造化およびフォーマットするのに役立ち、ターンが適切にマークされ、整理されることを保証します。\n",
    "\n",
    "# Part 4: エージェント定義とユーティリティ\n",
    "\n",
    "- `_set_default_tensor_type`コンテキストマネージャは、一時的にPyTorchのテンソルのデフォルトデータ型を指定されたタイプに設定し、そのブロックのコードが実行された後に再びtorch.floatにリセットされます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.7338Z",
     "iopub.status.busy": "2024-05-25T15:58:45.733075Z",
     "iopub.status.idle": "2024-05-25T15:58:45.741278Z",
     "shell.execute_reply": "2024-05-25T15:58:45.740545Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.733776Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたdtypeにtorchのデフォルトdtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066072f",
   "metadata": {},
   "source": [
    "### 例\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.742569Z",
     "iopub.status.busy": "2024-05-25T15:58:45.742251Z",
     "iopub.status.idle": "2024-05-25T15:58:45.76648Z",
     "shell.execute_reply": "2024-05-25T15:58:45.765652Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.742535Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([1.0, 2.0]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.767743Z",
     "iopub.status.busy": "2024-05-25T15:58:45.767473Z",
     "iopub.status.idle": "2024-05-25T15:58:45.772646Z",
     "shell.execute_reply": "2024-05-25T15:58:45.771803Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.767721Z"
    }
   },
   "outputs": [],
   "source": [
    "with _set_default_tensor_type(torch.float64):\n",
    "    print(torch.tensor([1.0, 2.0]).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6997e5",
   "metadata": {},
   "source": [
    "# Part 5: 基本GemmaAgentクラス\n",
    "\n",
    "GemmaAgentクラスは以下を目的としています:\n",
    "\n",
    "- 言語モデルを初期化・設定します。\n",
    "- プロンプトと応答をフォーマットおよび処理します。\n",
    "- テンソルデータタイプを一時的に設定するためにコンテキストマネージャを使用します。\n",
    "- フォーマットされたプロンプトに基づいてモデルと対話し、応答を生成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.949245Z",
     "iopub.status.busy": "2024-05-25T15:58:45.948631Z",
     "iopub.status.idle": "2024-05-25T15:58:45.960292Z",
     "shell.execute_reply": "2024-05-25T15:58:45.959431Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.949221Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        \n",
    "        # モデル設定\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        \n",
    "        # モデル\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a28e9",
   "metadata": {},
   "source": [
    "# Part 6: GemmaQuestionerAgentクラス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:46.723064Z",
     "iopub.status.busy": "2024-05-25T15:58:46.722291Z",
     "iopub.status.idle": "2024-05-25T15:58:46.732867Z",
     "shell.execute_reply": "2024-05-25T15:58:46.731969Z",
     "shell.execute_reply.started": "2024-05-25T15:58:46.723034Z"
    }
   },
   "outputs": [],
   "source": [
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えてください質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"未知のターンタイプ: \", obs.turnType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2542f4",
   "metadata": {},
   "source": [
    "GemmaQuestionerAgent:\n",
    "- `__init__`：親クラスのコンストラクタを呼び出してエージェントを設定します。\n",
    "- `_start_session`：質問と回答を交互に並べ、会話のフォーマットを設定します。\n",
    "- `_parse_response`：エージェントが質問をしているのか、推測をしているのかによってモデルの応答を異なる方法で解釈します。\n",
    "\n",
    "# Part 7: GemmaAnswererAgentクラス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:47.844243Z",
     "iopub.status.busy": "2024-05-25T15:58:47.843475Z",
     "iopub.status.idle": "2024-05-25T15:58:47.850945Z",
     "shell.execute_reply": "2024-05-25T15:58:47.850005Z",
     "shell.execute_reply.started": "2024-05-25T15:58:47.844215Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"このキーワードは{obs.keyword}に関連する質問です。はいかいいえで答えてください。回答は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00ab49",
   "metadata": {},
   "source": [
    "# Part 8: エージェントの作成と関数定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:49.041213Z",
     "iopub.status.busy": "2024-05-25T15:58:49.040862Z",
     "iopub.status.idle": "2024-05-25T15:58:49.051556Z",
     "shell.execute_reply": "2024-05-25T15:58:49.05063Z",
     "shell.execute_reply.started": "2024-05-25T15:58:49.041185Z"
    }
   },
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはいかいいえの質問に応じます。キーワードは特定の人物、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義してくださいので、必要なエージェントだけをロードできます。両方をロードするとおそらくOOM（メモリ不足）になります。\n",
    "\n",
    "# エージェント変数を初期化\n",
    "agent = None\n",
    "\n",
    "# 名前に基づいて適切なエージェントを取得する関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # エージェントが初期化されておらず、要求されたエージェントが\"質問者\"の場合\n",
    "    if agent is None and name == 'questioner':\n",
    "        # 特定のパラメータでGemmaQuestionerAgentを初期化\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',  # 計算用のデバイス\n",
    "            system_prompt=system_prompt,  # エージェントのシステムプロンプト\n",
    "            few_shot_examples=few_shot_examples,  # エージェントの行動を指導するための例\n",
    "        )\n",
    "    # エージェントが初期化されておらず、要求されたエージェントが\"回答者\"の場合\n",
    "    elif agent is None and name == 'answerer':\n",
    "        # 同じパラメータでGemmaAnswererAgentを初期化\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    \n",
    "    # エージェントが初期化されていることを確認\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    # 初期化されたエージェントを返す\n",
    "    return agent\n",
    "\n",
    "# 観察に基づいてインタラクションを処理するための関数\n",
    "def agent_fn(obs, cfg):\n",
    "    # 観察が質問するためのものである場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        # 質問するエージェントを取得して観察に応答する\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観察が推測するためのものである場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        # 質問するエージェントを取得して観察に応答する\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # 観察が回答を提供するためのものである場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        # 回答するエージェントを取得して観察に応答する\n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    # エージェントからの応答がNoneまたは非常に短いものである場合\n",
    "    if response is None or len(response) <= 1:\n",
    "        # ポジティブな応答（\"はい\"）を仮定\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        # エージェントから受け取った応答を返す\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2095b",
   "metadata": {},
   "source": [
    "1. **GemmaFormatterクラス**: このクラスは、ゲームのプロンプトのフォーマットを扱います。ユーザーとモデルのターンを構成し、ターンの開始・終了、状態のリセット、ターンの適用に関するメソッドを持っています。エージェントのためのプロンプトの一貫したフォーマットを保証します。\n",
    "\n",
    "2. **GemmaAgentクラス**: ゲーム内の一般的なエージェントを表す抽象クラスです。初期化、呼び出し、セッションの開始、言語モデル（LLM）の呼び出し、応答の解析、デフォルトのテンソルタイプの設定など、共通のメソッドと属性を定義します。\n",
    "\n",
    "3. **GemmaQuestionerAgentクラス**と**GemmaAnswererAgentクラス**: これらのクラスはGemmaAgentから継承し、それぞれ質問者と回答者エージェントの特定の動作を実装します。ターンタイプに応じてモデルのレスポンスをカスタマイズするために、`_start_session`と`_parse_response`メソッドをオーバーライドします。\n",
    "\n",
    "4. **interleave_unequal関数**: この関数は、長さの異なる2つのリストを交互に並べます。ゲーム内で質問と回答を交互にするために使用されます。\n",
    "\n",
    "5. **get_agent関数**: この関数は、入力名（「questioner」または「answerer」）に基づいて適切なエージェントを初期化し、返します。エージェントは一度だけ作成され再利用されることが保証されます。\n",
    "\n",
    "6. **agent_fn関数**: この関数は、ゲームのエントリーポイントとして機能します。観察のターンタイプに応じて使用するエージェントのタイプを決定し、適切なエージェントの`__call__`メソッドを呼び出して応答を生成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:06.686412Z",
     "iopub.status.busy": "2024-05-25T11:17:06.685976Z",
     "iopub.status.idle": "2024-05-25T11:17:06.693459Z",
     "shell.execute_reply": "2024-05-25T11:17:06.691475Z",
     "shell.execute_reply.started": "2024-05-25T11:17:06.686368Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:11.751057Z",
     "iopub.status.busy": "2024-05-25T11:17:11.750579Z",
     "iopub.status.idle": "2024-05-25T11:17:11.756935Z",
     "shell.execute_reply": "2024-05-25T11:17:11.755388Z",
     "shell.execute_reply.started": "2024-05-25T11:17:11.751024Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e231e12f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Mohamed MZAOUALI\n",
    "> \n",
    "> より良く理解できました、どうもありがとう!\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
