{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23021436",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのシンプルなスタートを提供します。主な目的は、言語モデル（LLM）を用いて「20の質問」ゲームをプレイするための基本的なフレームワークを提供することです。\n",
    "\n",
    "### 問題と取り組んでいる内容\n",
    "ノートブックは、プレイヤーが有限の質問回数でターゲットワード（動物、植物、物体など）を特定する「20の質問」ゲームを効率的にプレイするために、質問者LLMと回答者LLMを活用するアプローチを取っています。実際に選手が質問を行い、その回答に基づいて推測を行うプロセスを自動化しています。\n",
    "\n",
    "### 手法とライブラリ\n",
    "1. **使用ライブラリ**:\n",
    "   - `torch`: PyTorchライブラリを用いてモデルの構築とGPUでの計算を管理しています。\n",
    "   - `gemma`: Googleが提供するGemmaモデルを使い、因果推論モデルとしての機能を利用しています。\n",
    "   - その他のPythonライブラリ（`immutabledict`、`sentencepiece`）がインストールされ、モデルの運用を支援しています。\n",
    "\n",
    "2. **モデル準備**:\n",
    "   - GitHubから`gemma_pytorch`リポジトリをクローンし、必要なライブラリを作業ディレクトリに移動して設定しています。\n",
    "   - 重みファイルパスを設定し、Gemmaモデルの初期化を行います。\n",
    "\n",
    "3. **エージェントクラス**:\n",
    "   - `Agents`クラスが定義され、3つの主要なエージェント（質問エージェント、推測エージェント、回答エージェント）が実装されています。これらはモデルに基づいて質問や推測を行い、適切な応答を生成します。\n",
    "   - 各エージェントは与えられた情報を元に動作し、ゲームの進行に合わせて最適なアクションをとります。\n",
    "\n",
    "4. **プロンプトエンジニアリング**:\n",
    "   - 各エージェントは、文脈に応じたプロンプトを生成し、そのプロンプトを使用してLLMに問い合わせ、応答を得る形式をとっています。\n",
    "\n",
    "このノートブックは、ユーザーが簡単にプロンプトエンジニアリングを開始し、提案された改善点や問題提起を通じてさらなる発展を促すことを目的としています。また、実行に必要なコードに加えて、簡潔に試すためのテスト用の例も含まれています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc7b8b5",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebook内で扱われている機械学習・深層学習の関連用語について、初心者がつまずきそうな専門用語の簡単な解説です。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **プロンプトエンジニアリング (Prompt Engineering)**  \n",
    "   これは、AIモデル、特に言語モデルに対して入力の形式（プロンプト）を工夫して、期待される出力を得る技術です。質問の仕方や文の構造を調整することで、モデルの応答の質を高めることが目的となります。\n",
    "\n",
    "2. **immutabledict**  \n",
    "   イミュータブル（不変）な辞書構造を持つデータ型です。Pythonの辞書は通常、変更可能ですが、`immutabledict`は一度作成すると内容を変更できないため、ハッシュ可能でより安全に使用できます。\n",
    "\n",
    "3. **sentencepiece**  \n",
    "   テキストデータをトークン化するためのライブラリです。生のテキストをサブワードやトークンに分割し、言語モデルの学習に使うためのデータ準備を行います。特に、ニューラルネットワークモデルのトレーニングにおいて便利です。\n",
    "\n",
    "4. **Gemma (GemmaForCausalLM)**  \n",
    "   確率的生成モデルである「因果言語モデル」を実装したライブラリやクラスです。特に自然言語生成タスクに使用され、多段階の生成が可能です。ここでは、質問応答のエージェントとして機能します。\n",
    "\n",
    "5. **データクラス (Dataclass)**  \n",
    "   Python 3.7以降で導入された、簡単に変更可能なデータ構造を作成するための構文です。パラメータを属性として定義し、イミュータブルなオブジェクトを容易に構築できます。これは、データを簡潔に表現するのに便利です。\n",
    "\n",
    "6. **ターンタイプ (Turn Type)**  \n",
    "   ゲームの状態に応じてエージェントの行動を識別するための変数です。「質問する」「推測する」「回答する」の3種類があり、それぞれのエージェントがどの役割を果たすかを決定します。\n",
    "\n",
    "7. **サンプリング (Sampling)**  \n",
    "   モデルが生成するテキストの選択肢を制御する方法です。このノートブックでは、`temperature`、`top_p`、`top_k`というパラメータがサンプリングに使われ、生成されるテキストの多様性や創造性を調整します。\n",
    "\n",
    "8. **コンテキストマネージャ (Context Manager)**  \n",
    "   リソース（例えばファイルやメモリ）の管理を簡単に行うためのPythonの機能です。ここでは、学習時に指定されたデータ型を設定する際に用いられています。\n",
    "\n",
    "9. **推測エージェント (Guess Agent)**  \n",
    "   質問応答ゲームにおける特定の役割で、これまでの質問と回答に基づいてキーワードを推測するエージェントです。他のエージェントとの相互作用を通じて進行します。\n",
    "\n",
    "10. **エンティティ (Entity)**  \n",
    "    テキストやデータから特定の情報（名前、地名、組織名など）を抽出するプロセスで、質問応答や情報検索システムで重要な役割を果たします。\n",
    "\n",
    "初心者がこれらの用語を理解できると、ノートブックの内容やその実装に関連する部分がより明確になるでしょう。また、特に実務経験がない場合、コンペティションに特化した知識を得ることは良い学習機会になります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b25625e",
   "metadata": {},
   "source": [
    "# シンプルスタートを示すノートブック\n",
    "\n",
    "優れた「LLM 20質問スターターノートブック」を基にしています。  \n",
    "\n",
    "しかし、これはプロンプトエンジニアリングを迅速に始めるために、はるかに簡素化されています。  \n",
    "\n",
    "「LLM 20質問スターターノートブック」ほど高度ではなく、プロンプトはコンペティション用に提供された「LLM 20質問」ノートブックから直接取得されています。  \n",
    "\n",
    "最後には、あなたのプロンプトをテストするためのセルがあります。  \n",
    "\n",
    "問題や改善の提案は歓迎します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-22T10:58:08.236875Z",
     "iopub.status.busy": "2024-06-22T10:58:08.236492Z",
     "iopub.status.idle": "2024-06-22T10:58:22.292725Z",
     "shell.execute_reply": "2024-06-22T10:58:22.291888Z",
     "shell.execute_reply.started": "2024-06-22T10:58:08.236838Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 作業ディレクトリに移動します\n",
    "cd /kaggle/working\n",
    "\n",
    "# immutabledictとsentencepieceパッケージをインストールします。\n",
    "# -qオプションは出力を抑制し、-Uはパッケージをアップグレードします。\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "\n",
    "# gemma_pytorchリポジトリをGitHubからクローンします。\n",
    "# 出力を/dev/nullにリダイレクトすることで表示を抑制します。\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "\n",
    "# gemmaフォルダを作成します。\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "# gemma_pytorchから中身を移動します。\n",
    "# 移動させることで、gemmaライブラリを作成したlibフォルダに配置します。\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6df5319",
   "metadata": {},
   "source": [
    "**モデルを追加するには「Add Input」を使用します**  \n",
    "これは環境の右側に見つかります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:13:23.842116Z",
     "iopub.status.busy": "2024-06-22T12:13:23.841789Z",
     "iopub.status.idle": "2024-06-22T12:13:23.867408Z",
     "shell.execute_reply": "2024-06-22T12:13:23.866335Z",
     "shell.execute_reply.started": "2024-06-22T12:13:23.84209Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleエージェントのパスを定義します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# Kaggleエージェントのパスが存在するか確認します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # 存在する場合、ライブラリのパスを挿入します\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    # 存在しない場合、作業スペース内のライブラリのパスを挿入します\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, List\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 重みファイルのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "\n",
    "# オブジェクトの内容を把握するためのデータクラスを設定します\n",
    "@dataclass\n",
    "class ObsData:\n",
    "    answers: List[\"str\"]\n",
    "    category: str\n",
    "    keyword: str\n",
    "    questions: List[\"str\"]\n",
    "    turn_type: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "    \n",
    "\n",
    "class Agents:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        # 7Bモデルはかなり大きいので、GPUセッションを要求する方が良いです\n",
    "        self._device = torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self._device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    def answer_agent(self, question: str, category: str, keyword: str) -> Literal[\"yes\", \"no\"]:\n",
    "        info_prompt = \"\"\"あなたは20の質問のゲームで非常に正確な回答者です。質問者が推測しようとしているキーワードは[カテゴリの {category} {keyword}]です。\"\"\"\n",
    "        answer_question_prompt = f\"\"\"次の質問に「はい」、「いいえ」、またはわからない場合は「多分」で答えてください: {question}\"\"\"\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(category=category, keyword=keyword),\n",
    "            answer_question_prompt\n",
    "        )\n",
    "        \n",
    "        return self._call_llm(prompt)\n",
    "\n",
    "    def ask_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        info_prompt = \"\"\"あなたは20の質問のゲームをプレイしていて、質問をしながらキーワードを特定しようとしています。それは実在の人、場所、または物か架空のものでしょう。\\nここまでの知識:\\n{q_a_thread}\"\"\"\n",
    "        questions_prompt = \"\"\"1つの「はい」または「いいえ」の質問をしてください。\"\"\"\n",
    "\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            questions_prompt\n",
    "        )\n",
    "\n",
    "        return self._call_llm(prompt)\n",
    "\n",
    "    \n",
    "    def guess_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # 期待される回答は**で囲まれた形式です\n",
    "        info_prompt = \"\"\"あなたは20の質問のゲームをプレイしていて、質問をしながらキーワードを特定しようとしています。それは実在の人、場所、または物か架空のものでしょう。\\nここまでの知識:\\n{q_a_thread}\"\"\"\n",
    "        guess_prompt = \"\"\"キーワードを推測してください。正確な単語/フレーズのみを返してください。たとえば、キーワードが[パリ]だと思うなら、[私はキーワードがパリだと思います]や[キーワードはパリですか？]とは返さず、単に[パリ]と答えてください。\"\"\"\n",
    "\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "        \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            guess_prompt\n",
    "        )\n",
    "\n",
    "        return f\"**{self._call_llm(prompt)}**\"\n",
    "    \n",
    "    def _call_llm(self, prompt: str):\n",
    "        self._set_model()\n",
    "        \n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=100,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "         \n",
    "    def _set_model(self):\n",
    "        if self.model is None:\n",
    "            print(\"まだモデルが設定されていないため、設定します\")\n",
    "            model_config = get_config_for_7b()\n",
    "            model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "            model_config.quant = True\n",
    "\n",
    "            # コンテキストマネージャを使用しないとスタックが溢れます\n",
    "            with self._set_default_tensor_type(model_config.get_dtype()):\n",
    "                model = GemmaForCausalLM(model_config)\n",
    "                ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-7b-it-quant.ckpt')\n",
    "                model.load_weights(ckpt_path)\n",
    "                self.model = model.to(self._device).eval()\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(self, dtype: torch.dtype):\n",
    "        \"\"\"指定されたdtypeをデフォルトのtorch dtypeに設定します。\"\"\"\n",
    "        torch.set_default_dtype(dtype)\n",
    "        yield\n",
    "        torch.set_default_dtype(torch.float)\n",
    "\n",
    "# エントリポイント。このため、名前とパラメータは事前に設定されています\n",
    "def agent_fn(obs, cfg) -> str:\n",
    "    obs_data = ObsData(\n",
    "        turn_type=obs.turnType,\n",
    "        questions=obs.questions,\n",
    "        answers=obs.answers,\n",
    "        keyword=obs.keyword,\n",
    "        category=obs.category\n",
    "    )\n",
    "    \n",
    "    if obs_data.turn_type == \"ask\":\n",
    "        response = agents.ask_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"guess\":\n",
    "        response = agents.guess_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"answer\":\n",
    "        response = agents.answer_agent(question=obs.questions[-1], category=obs.category, keyword=obs.keyword)\n",
    "    \n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# モデルが1回だけ設定されるようにエージェントクラスをインスタンス化します\n",
    "agents = Agents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:13:27.586335Z",
     "iopub.status.busy": "2024-06-22T12:13:27.585486Z",
     "iopub.status.idle": "2024-06-22T12:14:15.05232Z",
     "shell.execute_reply": "2024-06-22T12:14:15.051301Z",
     "shell.execute_reply.started": "2024-06-22T12:13:27.586289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 手動で回答エージェントを実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"answer\",  # ターンタイプを「回答」に設定します\n",
    "        questions=[\"Is it a place\"],  # 質問のリスト\n",
    "        answers=[],  # まだ回答は存在しないため空のリスト\n",
    "        keyword=\"Antartica\",  # 推測するキーワード\n",
    "        category=\"Place\"  # カテゴリーは「場所」\n",
    "    )\n",
    "\n",
    "# agent_fnを呼び出して反応を印刷します\n",
    "print(agent_fn(obs_data, {}))  # エージェントの反応を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:14:39.010378Z",
     "iopub.status.busy": "2024-06-22T12:14:39.009974Z",
     "iopub.status.idle": "2024-06-22T12:15:20.042528Z",
     "shell.execute_reply": "2024-06-22T12:15:20.041506Z",
     "shell.execute_reply.started": "2024-06-22T12:14:39.010344Z"
    }
   },
   "outputs": [],
   "source": [
    "# 手動で質問エージェントを実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"ask\",  # ターンタイプを「質問」に設定します\n",
    "        questions=[\"Is it a place?\"],  # 質問のリスト\n",
    "        answers=[\"Yes\"],  # 既に得られた回答\n",
    "        keyword=\"\",  # キーワードはまだ指定されていないため空\n",
    "        category=\"\"  # カテゴリーも指定されていないため空\n",
    "    )\n",
    "\n",
    "# agent_fnを呼び出して反応を印刷します\n",
    "print(agent_fn(obs_data, {}))  # エージェントの反応を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:15:51.259776Z",
     "iopub.status.busy": "2024-06-22T12:15:51.259369Z",
     "iopub.status.idle": "2024-06-22T12:16:32.830569Z",
     "shell.execute_reply": "2024-06-22T12:16:32.829575Z",
     "shell.execute_reply.started": "2024-06-22T12:15:51.259747Z"
    }
   },
   "outputs": [],
   "source": [
    "# 手動で推測エージェントを実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"guess\",  # ターンタイプを「推測」に設定します\n",
    "        questions=[\"Is it a place?\", \"Is it in the northern hemisphere?\", \"Is it a city?\", \"Is it icy?\"],  # 質問のリスト\n",
    "        answers=[\"Yes\", \"No\", \"No\", \"Yes\"],  # 各質問に対する回答リスト\n",
    "        keyword=\"\",  # キーワードはまだ指定されていないため空\n",
    "        category=\"\"  # カテゴリーも指定されていないため空\n",
    "    )\n",
    "\n",
    "# agent_fnを呼び出して反応を印刷します\n",
    "print(agent_fn(obs_data, {}))  # エージェントの反応を表示します"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
