{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8351745b",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleコンペティション「LLM 20 Questions」に向けたスターターコードです。特に、`phi3`言語モデルを使用して、ゲームの基礎的な機能を実装することを目的としています。ノートブックは、Pythonパッケージ「rigging」を使用してエージェントを構築し、`vLLM`を用いてモデルをローカルでホスティングします。この取り組みは、20の質問ゲームのための「質問者」と「回答者」モデルを作成するためのものです。\n",
    "\n",
    "### アプローチと手法\n",
    "- **riggingライブラリ**: これは軽量のLLMインタラクションフレームワークで、異なるLLMモデルを簡単に切り替え、クエリパイプラインを設計するのを助けます。このNotebookでは、`rigging`を使用してエージェントの質問と応答の管理を行っています。\n",
    "  \n",
    "- **vLLMライブラリ**: このライブラリは、モデルをローカルでホスティングするために使われます。ノートブックで`vLLM`サーバーを立ち上げる構成が含まれており、APIインターフェイスを介してモデルを利用できます。\n",
    "\n",
    "- **データの準備**: Hugging Faceからモデルの重みをダウンロードし、Kaggleの秘密管理機能を利用してAPIトークンを安全に扱います。また、キーワードデータをパラケットファイルとして作成・管理します。キーワードについての質問を生成し、その回答を管理するためのデータフレームが作成されます。\n",
    "\n",
    "- **エージェントの設計**: 属性を定義した`Observation`クラスと、質問・回答・推測を管理するためのパイプラインが設計されています。また、カテゴリーに基づいてキーワードをフィルタリングするためのシンプルな質問者エージェントが実装されています。\n",
    "\n",
    "###ライブラリ\n",
    "- **Pydantic**: 型注釈とデータの検証を行うために使用されています。\n",
    "- **Numpy/Pandas**: データフレームを操作するためのライブラリ。質問履歴やキーワードデータの管理に用いられます。\n",
    "\n",
    "このノートブックは、基本的な構成を提供し、特に質問の生成や推測、回答を通じて、20の質問の戦略を展開するための強力な基盤を築いています。さらに、LLMの能力を活かして、質問の内容を改善するための戦略的なアプローチが盛り込まれています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a46fb0b",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookに登場するが初心者がつまずきそうな専門用語の簡単な解説を列挙します。\n",
    "\n",
    "1. **Rigging**:\n",
    "   - 言語モデル（LLM）とのインタラクションを行うためのフレームワーク。これにより、異なるLLMモデルへの接続や出力のチェック、再試行ロジックを簡単に実装できる。\n",
    "\n",
    "2. **vLLM**:\n",
    "   - LLMをローカルで実行するためのサービスで、高速なモデルホスティングを提供する。特に、リソースを効率的に使用して、モデルのインスタンスを生成・管理する。\n",
    "\n",
    "3. **Pydantic**:\n",
    "   - Pythonのデータバリデーション及び設定管理を行うためのライブラリ。特に、データの整合性を保つために型ヒントを使用し、簡潔なクラス定義で複雑なバリデーションを行うことができる。\n",
    "\n",
    "4. **Subprocess**:\n",
    "   - Pythonで外部コマンドを実行するためのモジュール。このモジュールを使用すると、Pythonプログラムからシェルコマンドを実行し、入力・出力を管理することができる。\n",
    "\n",
    "5. **ChatPipeline**:\n",
    "   - LLMに対する対話セッションを構築するための一連の手続き。特定のプロンプトに基づいて質問を行い、その応答を処理するプロセスを指す。\n",
    "\n",
    "6. **Observation**:\n",
    "   - ゲームのステータスや履歴を保持するためのデータ構造。質問や回答の履歴を管理し、エージェントが次の質問を決定するのに役立つ。\n",
    "\n",
    "7. **XML形式**:\n",
    "   - データを構造化するためのマークアップ言語。特に、LLMに対する特定の形式での出力や入力を定義するのに用いられる。\n",
    "\n",
    "8. **Numpy**:\n",
    "   - 数値計算を効率的に行うためのPythonライブラリ。特に多次元配列（ndarray）を操作するのに最適化されている。\n",
    "\n",
    "9. **Streamlit**:\n",
    "   - データアプリケーションを簡単に作成するためのフレームワーク。特に機械学習のモデルを迅速にデモンストレーションするために使用される。\n",
    "\n",
    "10. **Latency**:\n",
    "    - システムが要求に応じて反応するまでの時間。特にリアルタイムシステムや対話エージェントで重要な指標となる。\n",
    "\n",
    "11. **Metadata**:\n",
    "    - データについての情報（データのデータ）。例えば、データ作成日時、データソースなどが含まれることがある。\n",
    "\n",
    "12. **Docker**:\n",
    "    - ソフトウェアをコンテナ化して、開発・デプロイを行うためのプラットフォーム。環境構築を容易にし、異なるシステム間での移植性を高める。\n",
    "\n",
    "13. **Git Submodules**:\n",
    "    - 他のGitリポジトリを自身のリポジトリ内に含めるための機能。他のプロジェクトのコードを参照するのに役立つ。\n",
    "\n",
    "14. **parquet形式**:\n",
    "    - 列指向のデータファイルフォーマットで、特にビッグデータ処理の中で効率的にデータを保存・読み込むために利用される。\n",
    "\n",
    "これらの用語は、ノートブックの内容および機械学習・深層学習の実務において重要な概念やツール、技術に関連しています。初心者がこれらを理解することで、プロジェクトや競技に参加する際の助けとなるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff90209",
   "metadata": {},
   "source": [
    "# LLM 20 Questions スターター（Rigging [Phi3]を使用）\n",
    "\n",
    "## このノートブックは、以前のノートブックの改訂版です。@bhanupmの要求に応じて、`llama3`モデルを`phi3`モデルに置き換えました。元のノートブックは[こちら](https://www.kaggle.com/code/robikscube/phi3-intro-to-rigging-for-llm-20-questions/)で見つけることができます。\n",
    "\n",
    "私の初期テストでは、llama3バージョンの方がパフォーマンスが良いですが、より良いプロンプトを用いることで改善される可能性があります。\n",
    "\n",
    "このスターターノートブックは、Pythonパッケージ「rigging」を使用して、コンペティションの基準提出を作成する方法を示しています。このセットアップでは、vLLMを使用して`phi3`量子化モデルを使用します。\n",
    "\n",
    "## 更新 **2024年6月10日**\n",
    "- rigging 2.0に対応するようにコードを更新しました\n",
    "- プライベートリーダーボードではうまく機能しないことに注意しながら、既知のキーワードを利用する非LLM質問エージェントを含めました。回答エージェントはrigging経由でLLMを使用します。\n",
    "\n",
    "## Riggingとは？\n",
    "\n",
    "Riggingは、Pydantic XMLに基づいた軽量のLLMインタラクションフレームワークです。目的は、LLMをプロダクションパイプラインで利用する際にできるだけ簡単かつ効果的にすることです。Riggingは20の質問タスクに完全に適しており、以下のことができます：\n",
    "1. 異なるバックエンドLLMモデルを簡単に切り替えることができます。\n",
    "2. 期待される出力をチェックし、成功するまで再試行するLLMクエリパイプラインを設計できます。\n",
    "3. 型ヒント、非同期サポート、Pydanticバリデーション、シリアル化など、現代的なPythonを活用しています。\n",
    "\n",
    "こちらからリポジトリにスターを付けてください：https://github.com/dreadnode/rigging  \n",
    "ドキュメントはここで読むことができます：https://rigging.dreadnode.io/\n",
    "\n",
    "Riggingは[dreadnode](https://www.dreadnode.io/)によって構築され、維持されています。私たちは日々の作業でこれを使用しています。\n",
    "\n",
    "例として、riggingパイプラインは次のようになります：\n",
    "```{python}\n",
    "chat = rg.get_generator('gpt-4o') \\\n",
    "    .chat(f\"南アメリカのすべての国の名前で、Aで始まるものを提供してください {Answer.xml_tags()} タグ。\") \\\n",
    "    .until_parsed_as(Answer) \\\n",
    "    .run() \n",
    "```\n",
    "\n",
    "大半の主要なLLM APIとシームレスに生成器を作成することができるため、APIキーが環境変数として保存されている限り可能です。\n",
    "```\n",
    "export OPENAI_API_KEY=...\n",
    "export TOGETHER_API_KEY=...\n",
    "export TOGETHERAI_API_KEY=...\n",
    "export MISTRAL_API_KEY=...\n",
    "export ANTHROPIC_API_KEY=...\n",
    "```\n",
    "\n",
    "このコンペティションでは、モデルをローカルで実行する必要があり、幸いなことに、riggingはバックエンドでtransformersを使用してモデルを実行するサポートがあります。\n",
    "\n",
    "# セットアップ\n",
    "\n",
    "以下は、このノートブックのいくつかの設定です。ここでは：\n",
    "- Hugging FaceおよびKaggleのためのシークレットトークンを読み込みます（オプション）\n",
    "- 必要なパッケージをインストールします\n",
    "- vLLMサーバーをテストするためのヘルパユーティリティスクリプトを作成します\n",
    "\n",
    "このノートブックでは、Kaggleのシークレットを使用していくつかの隠れたトークンを使用しています。これはオプションであり、コードを実行するために必須ではありません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:54:57.226986Z",
     "iopub.status.busy": "2024-06-25T14:54:57.226154Z",
     "iopub.status.idle": "2024-06-25T14:54:57.336873Z",
     "shell.execute_reply": "2024-06-25T14:54:57.336172Z",
     "shell.execute_reply.started": "2024-06-25T14:54:57.22696Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# Hugging Faceのトークンを格納するための変数\n",
    "HF_TOKEN: str | None  = None\n",
    "# Kaggleのキーを格納するための変数\n",
    "KAGGLE_KEY: str | None = None\n",
    "# Kaggleのユーザー名を格納するための変数\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    # Kaggleの秘密からHugging Faceトークンを取得\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    # Kaggleの秘密からKaggleキーを取得\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    # Kaggleの秘密からユーザー名を取得\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    # もし何かエラーが発生した場合は何もしない（エラーを無視）\n",
    "    pass\n",
    "\n",
    "# 上記のコードは、Kaggleの秘密管理機能を利用して、必要なAPIキーやトークンを安全に取得するために使用されます。\n",
    "# もし秘密が正常に取得できなかった場合、エラー処理として何も行わずにスクリプトの実行を続行します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9060a7",
   "metadata": {},
   "source": [
    "## パッケージのインストール\n",
    "私たちは以下のパッケージをインストールします：\n",
    "- [rigging](https://github.com/dreadnode/rigging)：このコンペティションのために私たちのLLMパイプラインを作成するために使用します。\n",
    "- [vLLM](https://github.com/vllm-project/vllm)：モデルをローカルで独立したサービスとしてホスティングするために使用します。\n",
    "\n",
    "また、[uv](https://github.com/astral-sh/uv)も使用します。これは、これらのパッケージをより迅速にインストールするのに役立ちます。\n",
    "\n",
    "**注意:** これらのパッケージは`/kaggle/tmp/lib`ディレクトリにインストールします。これはコンペティションのセットアップの目的のためであり、後で提出用のzipファイルにこのパスのファイルを含める必要があります。また、vllmの依存関係を`/kaggle/tmp/srvlib`にもインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T14:54:57.338592Z",
     "iopub.status.busy": "2024-06-25T14:54:57.338297Z",
     "iopub.status.idle": "2024-06-25T14:56:11.727967Z",
     "shell.execute_reply": "2024-06-25T14:56:11.726422Z",
     "shell.execute_reply.started": "2024-06-25T14:54:57.33855Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 依存関係（速度向上のためのuv）\n",
    "# uvライブラリを使用してインストールの速度を向上させるために、指定のバージョンをインストールします。\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "# riggingとkaggleライブラリを指定のターゲットにインストールします。\n",
    "# --targetオプションを使用して特定のディレクトリにインストールします。\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "# vllmとnumpyライブラリを別のターゲットにインストールします。\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4\n",
    "\n",
    "# 上記のコードは、特定のターゲットディレクトリに依存関係をインストールするために使用されます。\n",
    "# uvを使用することで、インストールプロセスが速くなり、効率的にライブラリを管理できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1933707a",
   "metadata": {},
   "source": [
    "# LLMをローカルにダウンロード\n",
    "\n",
    "このコンペティションでは、モデルの重みを含めたコードを提出する必要があるため、まず`snapshot_download`を使用してモデルの重みをダウンロードします。\n",
    "\n",
    "私たちは`solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ`をダウンロードします。これは、コンペティションの要件を満たすのに十分小さな、アクティベーションアウェアの重み量子化バージョンのモデルです。\n",
    "\n",
    "**注意**: 通常の状況でriggingを使用する場合、このステップは必要ありませんが、コンペティションのために重みを別にダウンロードして、提出用のzipファイルに含めることができるようにしています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T14:56:40.058151Z",
     "iopub.status.busy": "2024-06-25T14:56:40.057745Z",
     "iopub.status.idle": "2024-06-25T14:57:26.232868Z",
     "shell.execute_reply": "2024-06-25T14:57:26.231829Z",
     "shell.execute_reply.started": "2024-06-25T14:56:40.05811Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルをダウンロード\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# モデルの格納先パスを設定\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# もし指定したパスがすでに存在する場合は、その内容を削除\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "# 新しいモデル格納先のディレクトリを作成\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "# Hugging Faceからモデルのスナップショットをダウンロード\n",
    "snapshot_download(\n",
    "    repo_id=\"rhysjones/Phi-3-mini-mango-1-llamafied\",  # ダウンロードするモデルのリポジトリID\n",
    "    ignore_patterns=\"original*\",  # 無視するファイルパターン\n",
    "    local_dir=g_model_path,  # モデル格納先のローカルディレクトリ\n",
    "    local_dir_use_symlinks=False,  # シンボリックリンクを使用しない\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # トークンはグローバル変数から取得\n",
    ")\n",
    "\n",
    "# このコードは、指定したモデルをHugging Faceからダウンロードし、指定したディレクトリに保存します。\n",
    "# 既存のモデルファイルがあれば削除し、新しいモデル用のディレクトリを作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbc6447",
   "metadata": {},
   "source": [
    "モデルの重みは`/kaggle/tmp/model/`に保存されていることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:43.927105Z",
     "iopub.status.busy": "2024-06-25T14:57:43.926241Z",
     "iopub.status.idle": "2024-06-25T14:57:44.886775Z",
     "shell.execute_reply": "2024-06-25T14:57:44.885813Z",
     "shell.execute_reply.started": "2024-06-25T14:57:43.927071Z"
    }
   },
   "outputs": [],
   "source": [
    "# 保存されたモデルの重みを確認するために、指定したディレクトリ内のファイル一覧を表示します。\n",
    "!ls -l /kaggle/tmp/model\n",
    "\n",
    "# 上記のコマンドは、'/kaggle/tmp/model'ディレクトリにあるファイルやフォルダの詳細情報（権限、サイズ、最終更新日など）をリスト表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca594e4f",
   "metadata": {},
   "source": [
    "# ヘルパユーティリティファイル\n",
    "\n",
    "これらは、vLLMサーバーを起動するために使用するヘルパー関数です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:49.332077Z",
     "iopub.status.busy": "2024-06-25T14:57:49.331727Z",
     "iopub.status.idle": "2024-06-25T14:57:49.339771Z",
     "shell.execute_reply": "2024-06-25T14:57:49.338936Z",
     "shell.execute_reply.started": "2024-06-25T14:57:49.332048Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# vLLMサーバーを起動するためのヘルパー\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "# 指定したポートが使用中かどうかをチェックする関数\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)  # 1秒以内に応答がない場合はタイムアウト\n",
    "            result = sock.connect_ex(('localhost', port))  # ローカルホストの指定ポートに接続を試みる\n",
    "            if result == 0:  # 結果が0ならポートは開いている\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass  # ソケットエラーが発生した場合は無視\n",
    "    \n",
    "    return False  # ポートが開いていない場合はFalseを返す\n",
    "\n",
    "# コマンドを実行し、指定したポートが開くのを待つ関数\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str],  # 実行するコマンドのリスト\n",
    "    port: int,  # チェックするポート番号\n",
    "    env: dict[str, str] | None,  # 環境変数\n",
    "    timeout: int = 60,  # タイムアウト時間（秒）\n",
    "    debug: bool = False,  # デバッグモードのフラグ\n",
    ") -> subprocess.Popen:  # subprocess.Popenオブジェクトを返す\n",
    "\n",
    "    if check_port(port):  # ポートがすでに開いている場合\n",
    "        raise ValueError(f\"Port {port} is already open\")  # エラーを発生させる\n",
    "        \n",
    "    # コマンドを非同期で実行\n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},  # 環境変数を結合\n",
    "        stdout=subprocess.DEVNULL if not debug else None,  # デバッグモードでない場合は標準出力を無視\n",
    "        stderr=subprocess.DEVNULL if not debug else None,  # デバッグモードでない場合はエラー出力を無視\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()  # 開始時間を記録\n",
    "    while time.time() - start_time < timeout:  # タイムアウトまでループ\n",
    "        if check_port(port):  # ポートが開いたら\n",
    "            return popen  # Popenオブジェクトを返す\n",
    "        time.sleep(1)  # 1秒待つ\n",
    "    \n",
    "    popen.terminate()  # タイムアウトした場合はプロセスを終了\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")  # エラーを発生させる\n",
    "\n",
    "# 上記のコードは、vLLMサーバーを起動し、指定したポートが開くのを待つためのヘルパー関数を定義しています。\n",
    "# これらの関数を使うことで、サーバーの起動状況を確認し、問題がなければ次の処理に進むことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5648ace7",
   "metadata": {},
   "source": [
    "# vLLMサーバーをテストのために起動する\n",
    "\n",
    "私たちのモデルはvLLMサーバーを使用してホスティングされます。以下では、Kaggle環境でどのように動作するかを理解するためにノートブックを起動します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:51.596509Z",
     "iopub.status.busy": "2024-06-25T14:57:51.596145Z",
     "iopub.status.idle": "2024-06-25T14:57:51.603805Z",
     "shell.execute_reply": "2024-06-25T14:57:51.602922Z",
     "shell.execute_reply.started": "2024-06-25T14:57:51.596481Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLMのパスと設定。\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "# utilモジュールを再読み込みします。\n",
    "util = importlib.reload(util)\n",
    "\n",
    "# vLLMライブラリのパスを設定\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "# 指定したパスが存在することを確認\n",
    "assert g_srvlib_path.exists()\n",
    "\n",
    "# モデルのパスを設定\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# 指定したパスが存在することを確認\n",
    "assert g_model_path.exists()\n",
    "\n",
    "# vLLMサーバーで使用するポート番号とモデル名を設定\n",
    "g_vllm_port = 9999  # 使用するポート番号\n",
    "g_vllm_model_name = \"custom\"  # モデル名\n",
    "\n",
    "# このコードは、vLLMサーバーを運用するために必要なパスと設定を定義しています。\n",
    "# これにより、サーバーの起動時やモデルの読み込み時に適切なリソースを指定できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:31.209345Z",
     "iopub.status.busy": "2024-06-25T15:05:31.208642Z",
     "iopub.status.idle": "2024-06-25T15:05:49.239702Z",
     "shell.execute_reply": "2024-06-25T15:05:49.238752Z",
     "shell.execute_reply.started": "2024-06-25T15:05:31.209307Z"
    }
   },
   "outputs": [],
   "source": [
    "# subprocessを使用してvLLMサーバーを実行\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",  # vLLMのAPIサーバーをモジュールとして起動\n",
    "    \"--enforce-eager\",  # イagerモードを強制するオプション\n",
    "    \"--model\", str(g_model_path),  # 使用するモデルのパスを指定\n",
    "    \"--port\", str(g_vllm_port),  # 使用するポート番号を指定\n",
    "    \"--served-model-name\", g_vllm_model_name,  # 提供されるモデルの名前\n",
    "    \"--dtype=half\"  # データ型をhalf精度に設定\n",
    "],\n",
    "    g_vllm_port,  # ポート番号を指定\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},  # PYTHONPATHにsrvlibのパスを追加\n",
    "    debug=False  # デバッグモードを無効にする\n",
    ")\n",
    "\n",
    "print(\"vLLMが起動しました\")  # サーバーが正常に起動したことを通知するメッセージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b79fe6d",
   "metadata": {},
   "source": [
    "llama3モデルが最初のTesla T4 GPUにロードされていることが確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:54.05533Z",
     "iopub.status.busy": "2024-06-25T15:05:54.05498Z",
     "iopub.status.idle": "2024-06-25T15:05:55.109773Z",
     "shell.execute_reply": "2024-06-25T15:05:55.108601Z",
     "shell.execute_reply.started": "2024-06-25T15:05:54.055302Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPUの状態を確認するために、nvidia-smiコマンドを実行します。\n",
    "!nvidia-smi\n",
    "\n",
    "# このコマンドは、NVIDIAのGPUの使用状況やメモリの状態、実行中のプロセスを表示します。\n",
    "# モデルが適切にGPUにロードされていることを確認するために使用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0da8f1",
   "metadata": {},
   "source": [
    "## モデルの検証\n",
    "\n",
    "最初のriggingジェネレーターを作成しましょう。riggingでは、ジェネレーターは強力なLLMパイプラインを作成するための基盤となります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:57.177132Z",
     "iopub.status.busy": "2024-06-25T15:05:57.176719Z",
     "iopub.status.idle": "2024-06-25T15:05:57.605963Z",
     "shell.execute_reply": "2024-06-25T15:05:57.604998Z",
     "shell.execute_reply.started": "2024-06-25T15:05:57.177094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Riggingに接続する\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# ライブラリのパスをシステムパスに追加\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "# LiteLLMのログレベルを警告に設定\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "# ジェネレーターを取得する\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # vLLMモデルの名前\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（ダミーの例です）\n",
    "    \"stop=<|eot_id|>\"  # Llamaモデルにはいくつかの設定が必要\n",
    ")\n",
    "\n",
    "# ジェネレーターにチャットメッセージを送信\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[Rigging Chat]')\n",
    "print(type(answer), answer)  # 応答のタイプと内容を表示\n",
    "\n",
    "print()\n",
    "print('[LLM Response Only]')\n",
    "print(type(answer.last), answer.last)  # 最後の応答のタイプと内容を表示\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content  # 最後の応答の内容を取得\n",
    "print('[LLM Response as a String]')\n",
    "print(answer.last.content)  # 最後の応答の内容を表示\n",
    "\n",
    "# 上記のコードは、riggingを使用してvLLMサーバーに接続し、シンプルなチャット応答を取得するプロセスを示しています。\n",
    "# ユーザーからの入力に対してモデルがどのように応答するかを検証します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4772b54",
   "metadata": {},
   "source": [
    "## 結果をpandasデータフレームに変換\n",
    "\n",
    "`to_df()`メソッドを使用することで、チャット履歴を簡単にpandasデータフレームに変換することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:01.007133Z",
     "iopub.status.busy": "2024-06-25T15:06:01.00677Z",
     "iopub.status.idle": "2024-06-25T15:06:01.031837Z",
     "shell.execute_reply": "2024-06-25T15:06:01.030907Z",
     "shell.execute_reply.started": "2024-06-25T15:06:01.007105Z"
    }
   },
   "outputs": [],
   "source": [
    "# チャット応答をpandasデータフレームに変換する\n",
    "answer_df = answer.to_df()\n",
    "\n",
    "# データフレームの内容を表示する\n",
    "answer_df\n",
    "\n",
    "# 上記のコードは、チャットの応答をデータフレーム形式に変換し、内容を表示します。\n",
    "# これにより、応答履歴をより構造化された形式で管理できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0c1f6",
   "metadata": {},
   "source": [
    "## モデルパラメータの変更\n",
    "\n",
    "データベース接続文字列と同様に、Riggingのジェネレーターは、どのプロバイダー、モデル、APIキー、生成パラメータなどを使用するべきかを定義する文字列として表現できます。フォーマットは以下の通りです：\n",
    "\n",
    "```\n",
    "<provider>!<model>,<**kwargs>\n",
    "```\n",
    "\n",
    "ここでは、追加パラメータを使用してモデルをロードする例を示します：\n",
    "- temperature=0.9\n",
    "- max_tokens=512\n",
    "\n",
    "これらの詳細については、こちらのドキュメントを参照してください: https://rigging.dreadnode.io/topics/generators/#overload-generation-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:01.71593Z",
     "iopub.status.busy": "2024-06-25T15:06:01.715601Z",
     "iopub.status.idle": "2024-06-25T15:06:01.72054Z",
     "shell.execute_reply": "2024-06-25T15:06:01.719689Z",
     "shell.execute_reply.started": "2024-06-25T15:06:01.715906Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルを追加パラメータでロードするジェネレーターを取得\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # vLLMモデルの名前\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（ダミーの例です）\n",
    "    \"temperature=0.9,max_tokens=512,\"  # 生成時の温度と最大トークン数の設定\n",
    "    \"stop=<|eot_id|>\"  # Llamaモデルにはいくつかの設定が必要\n",
    ")\n",
    "\n",
    "# 上記のコードは、モデルを指定したパラメータ（temperatureやmax_tokensなど）でロードするためのジェネレーターを取得します。\n",
    "# この設定により、生成される応答の性質を調整することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d030eb",
   "metadata": {},
   "source": [
    "別の方法として、`rg.GenerateParams`クラスを使用してこれらのパラメータを設定することもできます。このクラスでは、さまざまなモデルパラメータを設定することができます：\n",
    "\n",
    "```\n",
    "rg.GenerateParams(\n",
    "    *,\n",
    "    temperature: float | None = None,  # 温度\n",
    "    max_tokens: int | None = None,  # 最大トークン数\n",
    "    top_k: int | None = None,  # トップkフィルタリング\n",
    "    top_p: float | None = None,  # トップpサンプリング\n",
    "    stop: list[str] | None = None,  # 停止トークンのリスト\n",
    "    presence_penalty: float | None = None,  # プレゼンスペナルティ\n",
    "    frequency_penalty: float | None = None,  # 頻度ペナルティ\n",
    "    api_base: str | None = None,  # APIのベースURL\n",
    "    timeout: int | None = None,  # タイムアウト時間\n",
    "    seed: int | None = None,  # シード値\n",
    "    extra: dict[str, typing.Any] = None,  # その他の追加パラメータ\n",
    ")\n",
    "```\n",
    "\n",
    "https://rigging.dreadnode.io/api/generator/#rigging.generator.GenerateParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:03.179319Z",
     "iopub.status.busy": "2024-06-25T15:06:03.178702Z",
     "iopub.status.idle": "2024-06-25T15:06:04.437702Z",
     "shell.execute_reply": "2024-06-25T15:06:04.43678Z",
     "shell.execute_reply.started": "2024-06-25T15:06:03.179289Z"
    }
   },
   "outputs": [],
   "source": [
    "# GenerateParamsを使用してモデルパラメータを設定\n",
    "rg_params = rg.GenerateParams(\n",
    "    temperature = 0.9,  # 温度を0.9に設定\n",
    "    max_tokens = 512,  # 最大トークン数を512に設定\n",
    ")\n",
    "\n",
    "# ジェネレーターに対してチャットを開始し、パラメータを適用\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "\n",
    "# 新たなプロンプトでチャットを実行\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "\n",
    "# 最後の応答の内容を表示\n",
    "print(answer.last.content)\n",
    "\n",
    "# 上記のコードは、rg.GenerateParamsを使用して設定したモデルパラメータを使って、\n",
    "# チャットを実行し、特定のプロンプトに対する応答を取得するプロセスを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1227ce8c",
   "metadata": {},
   "source": [
    "また、パラメータはチェーン内で`params`を使って設定することもできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:05.457476Z",
     "iopub.status.busy": "2024-06-25T15:06:05.456774Z",
     "iopub.status.idle": "2024-06-25T15:06:06.532498Z",
     "shell.execute_reply": "2024-06-25T15:06:06.531524Z",
     "shell.execute_reply.started": "2024-06-25T15:06:05.457446Z"
    }
   },
   "outputs": [],
   "source": [
    "# チェーン内でパラメータを設定してチャットを実行\n",
    "base_chat = generator.chat()  # パラメータは設定しない\n",
    "\n",
    "# 新たなプロンプトでチャットを実行し、パラメータを指定\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature = 0.9, max_tokens = 512) \\  # 温度と最大トークン数を設定\n",
    "    .run()\n",
    "\n",
    "# 最後の応答の内容を表示\n",
    "print(answer.last.content)\n",
    "\n",
    "# 上記のコードは、パラメータを直接チェーン内で設定し、\n",
    "# チャットを実行する方法を示しています。これにより、より柔軟に生成パラメータを指定できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e70602",
   "metadata": {},
   "source": [
    "# パースされた出力の例\n",
    "\n",
    "次に、以下のようなパイプラインを作成します：\n",
    "1. `Answer`というriggingモデルを作成します。これにより、モデルの結果からパースする期待される出力を説明します。\n",
    "    - 出力が`yes`または`no`のいずれかであることを確認するバリデーターを追加します。\n",
    "    - これは完全にカスタマイズ可能です。\n",
    "    - ここでの`validate_content`は、応答が期待される形式（小文字で「yes」または「no」で始まる）に準拠していることを保証します。\n",
    "2. プロンプト内で`Answer.xml_example()`を使用して、LLMに出力の期待する形式を伝えます。\n",
    "3. 後で`.until_parsed_as(Answer)`を使用して、ここで定義されたとおりにLLMの出力が抽出されることを確保します。\n",
    "\n",
    "**注意**: `until_parsed_as()`は、デフォルトで5の`max_rounds`パラメータを受け取ることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:21.347929Z",
     "iopub.status.busy": "2024-06-25T15:11:21.347101Z",
     "iopub.status.idle": "2024-06-25T15:11:21.356312Z",
     "shell.execute_reply": "2024-06-25T15:11:21.35534Z",
     "shell.execute_reply.started": "2024-06-25T15:11:21.347896Z"
    }
   },
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "# モデルAnswerを定義\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]  # 出力が\"yes\"または\"no\"であることを指定\n",
    "\n",
    "    # contentフィールドのバリデーションを行うメソッド\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:  # 有効な応答と比較\n",
    "            if v.lower().startswith(valid):  # 応答が小文字で\"yes\"または\"no\"で始まるか確認\n",
    "                return valid  # 有効な場合、その値を返す\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")  # 無効な場合はエラーを発生\n",
    "\n",
    "    # XML例を提供するクラスメソッド\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"  # XML形式の例を返す\n",
    "\n",
    "# 上記のコードは、期待される出力として\"yes\"または\"no\"を定義し、\n",
    "# それに基づいて応答をバリデーションするモデルAnswerを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:21.731091Z",
     "iopub.status.busy": "2024-06-25T15:11:21.730735Z",
     "iopub.status.idle": "2024-06-25T15:11:21.736827Z",
     "shell.execute_reply": "2024-06-25T15:11:21.735955Z",
     "shell.execute_reply.started": "2024-06-25T15:11:21.731064Z"
    }
   },
   "outputs": [],
   "source": [
    "# xml_exampleメソッドを使用して、XML形式の例を確認します。\n",
    "Answer.xml_example() \n",
    "\n",
    "# このコードは、モデルAnswerのXML例を表示し、プロンプトに使用できる期待される出力の形式を確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:22.351891Z",
     "iopub.status.busy": "2024-06-25T15:11:22.351188Z",
     "iopub.status.idle": "2024-06-25T15:11:22.689862Z",
     "shell.execute_reply": "2024-06-25T15:11:22.688976Z",
     "shell.execute_reply.started": "2024-06-25T15:11:22.351859Z"
    }
   },
   "outputs": [],
   "source": [
    "# ジェネレーターを設定し、プロンプトを定義\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\"  # vLLMモデルの名前\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\"  # APIベースURL\n",
    "    \"api_key=sk-1234,\"  # APIキー（ダミーの例です）\n",
    "    \"temperature=1.2\"  # 温度を1.2に設定\n",
    "    # \"stop=<|eot_id|>\"  # Llamaモデルにはいくつかの設定が必要\n",
    ")\n",
    "\n",
    "# キーワード、カテゴリ、最後の質問を設定\n",
    "keyword='Tom Hanks'\n",
    "category='Famous Person'\n",
    "last_question='Is it a famous person?'\n",
    "\n",
    "# プロンプトを定義\n",
    "prompt = f\"\"\"\\\n",
    "            このゲームの秘密の言葉は「{keyword}」です [{category}]\n",
    "\n",
    "            あなたは現在、上記の言葉についての質問に答えています。\n",
    "\n",
    "            次の質問は「{last_question}」です。\n",
    "\n",
    "            上のyes/noの質問に答え、次の形式で記入してください：\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - あなたの回答は上記のキーワードに基づき正確であるべきです\n",
    "            - いつも「yes」または「no」で答えてください\n",
    "\n",
    "            答えは何ですか？\n",
    "\"\"\"\n",
    "\n",
    "# チャットを実行し、パースされた応答を取得\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)  # プロンプトに基づくチャットを開始\n",
    "    .until_parsed_as(Answer, max_rounds=50)  # パースされた出力を取得するまで実行\n",
    "    .run()\n",
    ")\n",
    "\n",
    "# フルチャット履歴を表示\n",
    "print('=== 完全チャット ===')\n",
    "print(chat)\n",
    "\n",
    "print()\n",
    "# LLMの応答のみを表示\n",
    "print('=== LLM応答のみ ===')\n",
    "print(chat.last)\n",
    "\n",
    "print()\n",
    "# パースされた答えを表示\n",
    "print('=== パースされた回答 ===')\n",
    "print(chat.last.parse(Answer).content)  # パースした回答の内容を表示\n",
    "\n",
    "# 上記のコードは、指定したプロンプトに基づきLLMに質問を送り、応答を取得します。\n",
    "# その後、応答をパースして期待する形式に確認するプロセスを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ff40b1",
   "metadata": {},
   "source": [
    "# Riggingを使用した例の質問者チャットパイプラインを作成\n",
    "\n",
    "次に、キーワードが何であるかを判断するための質問者パイプラインを作成しましょう。\n",
    "\n",
    "最初に、出力をパースするために使用する`Question`オブジェクトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:31.378829Z",
     "iopub.status.busy": "2024-06-25T15:11:31.378051Z",
     "iopub.status.idle": "2024-06-25T15:11:31.388132Z",
     "shell.execute_reply": "2024-06-25T15:11:31.387242Z",
     "shell.execute_reply.started": "2024-06-25T15:11:31.378787Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "# 空白をトリムするための型アノテーションを定義\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "# Questionモデルを定義\n",
    "class Question(rg.Model):\n",
    "    content: str_strip  # 質問の内容（空白をトリムする）\n",
    "\n",
    "    # XMLの例を提供するクラスメソッド\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()  # XML形式で質問の例を返す\n",
    "\n",
    "# 上記のコードは、質問をパースするためのQuestionモデルを定義し、\n",
    "# 質問の内容が空白をトリムされることを保証します。また、XML形式の例も提供します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:32.156494Z",
     "iopub.status.busy": "2024-06-25T15:11:32.156098Z",
     "iopub.status.idle": "2024-06-25T15:11:36.412491Z",
     "shell.execute_reply": "2024-06-25T15:11:36.41159Z",
     "shell.execute_reply.started": "2024-06-25T15:11:32.156464Z"
    }
   },
   "outputs": [],
   "source": [
    "# 質問者のチャットを設定し、キーワードについて質問を行う\n",
    "base = generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの才能あるプレイヤーです。あなたは正確で、集中力があり、\n",
    "構造的なアプローチを持っています。あなたは有用な質問を作成し、推測を行うか、\n",
    "キーワードに関する質問に答えます。\n",
    "\"\"\")\n",
    "\n",
    "# 次の質問を行うためのチャットを実行\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "あなたは現在、次の質問を行っています。\n",
    "\n",
    "質問を作成し、次の形式で記入してください：\n",
    "{Question.xml_example()}\n",
    "\n",
    "- あなたの応答は、最も多くの情報を収集する集中した質問であるべきです\n",
    "- 質問を一般的に始めてください\n",
    "- 残りの検索空間を二分することを常に試みてください\n",
    "- 過去の質問と回答に注意を払ってください\n",
    "\n",
    "次の質問は何ですか？\n",
    "\"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)  # 質問をパースし、回復を試みる\n",
    ".run()\n",
    ")\n",
    "\n",
    "# 上記のコードは、質問者が未来の質問に基づいてゲームを進めるためのチャットを実行します。\n",
    "# それにより、キーワードに関する情報を収集するための集中した質問が生成されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:38.428599Z",
     "iopub.status.busy": "2024-06-25T15:11:38.428005Z",
     "iopub.status.idle": "2024-06-25T15:11:38.450345Z",
     "shell.execute_reply": "2024-06-25T15:11:38.449351Z",
     "shell.execute_reply.started": "2024-06-25T15:11:38.428565Z"
    }
   },
   "outputs": [],
   "source": [
    "# 会話のデータフレーム表現を表示\n",
    "question_chat.to_df()\n",
    "\n",
    "# このコードは、質問チャットの履歴をpandasデータフレームとして表示します。\n",
    "# これにより、会話の進行状況や内容を構造化された形式で確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d9d2b8",
   "metadata": {},
   "source": [
    "私たちは現在、LLMの応答が質問を含んでいることが確信でき、それに基づいて質問をパースすることができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:41.697665Z",
     "iopub.status.busy": "2024-06-25T15:11:41.696998Z",
     "iopub.status.idle": "2024-06-25T15:11:41.702788Z",
     "shell.execute_reply": "2024-06-25T15:11:41.701924Z",
     "shell.execute_reply.started": "2024-06-25T15:11:41.697632Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最後のチャットの応答をQuestionモデルを用いてパースし、質問を取得\n",
    "question = question_chat.last.parse(Question).content\n",
    "print(question)  # パースした質問を表示\n",
    "\n",
    "# 上記のコードは、最新の質問チャットから質問を抽出し、それを表示します。\n",
    "# これにより、LLMが生成した質問内容を確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41135af3",
   "metadata": {},
   "source": [
    "# キーワードデータフレームの作成\n",
    "**注意**: これは公開セットの可能なキーワードを知っているからこそ機能します。最終リーダーボードでは動作しません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:42.899784Z",
     "iopub.status.busy": "2024-06-25T15:11:42.89879Z",
     "iopub.status.idle": "2024-06-25T15:11:44.237713Z",
     "shell.execute_reply": "2024-06-25T15:11:44.236602Z",
     "shell.execute_reply.started": "2024-06-25T15:11:42.899737Z"
    }
   },
   "outputs": [],
   "source": [
    "# キーワードデータを取得するために、指定したURLからスクリプトをダウンロード\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "\n",
    "# このコマンドは、Kaggleのリポジトリからキーワード定義スクリプトをダウンロードし、\n",
    "# `keywords_local.py`という名前のファイルとして保存します。これにより、キーワードのリストにアクセスできます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:57.05108Z",
     "iopub.status.busy": "2024-06-25T15:11:57.050672Z",
     "iopub.status.idle": "2024-06-25T15:11:58.005916Z",
     "shell.execute_reply": "2024-06-25T15:11:58.004762Z",
     "shell.execute_reply.started": "2024-06-25T15:11:57.051046Z"
    }
   },
   "outputs": [],
   "source": [
    "# ダウンロードしたキーワードファイルの先頭部分を表示\n",
    "!head keywords_local.py\n",
    "\n",
    "# このコマンドは、`keywords_local.py`ファイルの最初の数行を表示し、ファイルの内容やキーワードのリストを確認します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:59.747293Z",
     "iopub.status.busy": "2024-06-25T15:11:59.746348Z",
     "iopub.status.idle": "2024-06-25T15:11:59.904356Z",
     "shell.execute_reply": "2024-06-25T15:11:59.903389Z",
     "shell.execute_reply.started": "2024-06-25T15:11:59.747254Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# カレントディレクトリにパスを追加\n",
    "sys.path.append('./')\n",
    "\n",
    "# KEYWORDS_JSONをインポート\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# テキストの最初の単語を大文字に変換する関数\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return text[0].upper() + text[1:].lower()\n",
    "\n",
    "# キーワードデータフレームを作成する関数\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)  # JSON形式のキーワードデータを辞書に変換\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    \n",
    "    for d in keywords_dict:  # 辞書内の各項目をループ\n",
    "        words = [w['keyword'] for w in d['words']]  # キーワードを抽出\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]  # カテゴリとキーワードのペアを作成\n",
    "        category_words_dict[d['category']] = words\n",
    "        all_words += words  # すべてのキーワードを追加\n",
    "        all_cat_words += cat_word  # すべてのカテゴリ・キーワードペアを追加\n",
    "\n",
    "    # データフレームを作成\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category', 'keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]  # 最初の文字を抽出\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]  # 2番目の文字を抽出\n",
    "    \n",
    "    # データフレームをparquet形式で保存\n",
    "    keyword_df.to_parquet('keywords.parquet')\n",
    "    \n",
    "# キーワードデータフレームを作成する関数を実行\n",
    "create_keyword_df(KEYWORDS_JSON)\n",
    "\n",
    "# 上記のコードは、JSONデータからキーワードデータフレームを作成し、parquetファイルとして保存します。\n",
    "# これにより、キーワードのカテゴリとそれに対応する単語の情報を管理できるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:12:00.416192Z",
     "iopub.status.busy": "2024-06-25T15:12:00.415824Z",
     "iopub.status.idle": "2024-06-25T15:12:00.564344Z",
     "shell.execute_reply": "2024-06-25T15:12:00.563422Z",
     "shell.execute_reply.started": "2024-06-25T15:12:00.416162Z"
    }
   },
   "outputs": [],
   "source": [
    "# 作成したキーワードデータフレームをparquetファイルから読み込み、サンプルを表示\n",
    "keywords_df = pd.read_parquet('keywords.parquet')\n",
    "keywords_df.sample(10)  # ランダムに10行を表示\n",
    "\n",
    "# このコードは、キーワードデータフレームを読み込み、その内容の一部をサンプルとして表示します。\n",
    "# これにより、データフレームの構造とキーワードのカテゴリ及びその内容を確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:12:03.242015Z",
     "iopub.status.busy": "2024-06-25T15:12:03.241664Z",
     "iopub.status.idle": "2024-06-25T15:12:03.261012Z",
     "shell.execute_reply": "2024-06-25T15:12:03.260034Z",
     "shell.execute_reply.started": "2024-06-25T15:12:03.241989Z"
    }
   },
   "outputs": [],
   "source": [
    "# 各カテゴリのキーワードのカウントを表示\n",
    "keywords_df['category'].value_counts()\n",
    "\n",
    "# このコードは、キーワードデータフレーム内の各カテゴリに含まれるキーワードの数をカウントし、\n",
    "# その結果を表示します。これにより、各カテゴリのキーワードの分布を確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a895fc",
   "metadata": {},
   "source": [
    "# 最終提出用の`main.py`スクリプトを作成\n",
    "\n",
    "私たちの最終提出は、`main`ファイルを含む圧縮ディレクトリになります。以下にそのファイルを示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:27:50.461711Z",
     "iopub.status.busy": "2024-06-25T21:27:50.461328Z",
     "iopub.status.idle": "2024-06-25T21:27:50.487868Z",
     "shell.execute_reply": "2024-06-25T21:27:50.486744Z",
     "shell.execute_reply.started": "2024-06-25T21:27:50.461679Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# メインエージェントファイル\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# パスの修正\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# ロギングのノイズを抑える\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# 固定インポート\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# 定数\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    ")\n",
    "\n",
    "# 型定義\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"question\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"thing/place\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# 関数\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # 最初の質問のバグ修正のためオーバーライド\n",
    "        return \"Are we playing 20 questions?\"\n",
    "    \n",
    "    \n",
    "    full_question = f\"\"\"\\\n",
    "                あなたは現在、次の質問を行っています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づき、次の最も有用なyes/noの質問を行い、\n",
    "                次の形式で記入してください：\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - あなたの応答は、最も多くの情報を収集する集中した質問であるべきです\n",
    "                - 質問を一般的に始めてください\n",
    "                - 残りの検索空間を二分することを常に試みてください\n",
    "                - 過去の質問と回答に注意を払ってください\n",
    "\n",
    "                次の質問は何ですか？\n",
    "                \"\"\"\n",
    "    \n",
    "    print(' ======質問中 ======')\n",
    "    print(full_question)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(full_question)\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'Is it a person?'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n",
    "        return \"yes\" # バグ修正のためオーバーライド\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "\n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # 5回ループして最も頻繁な応答を取得\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "                    f\"\"\"\n",
    "                    キーワード: [{observation.keyword}]\n",
    "\n",
    "                    質問: {last_question}\n",
    "\n",
    "                    yes または no で答え、形式は次の通りにしてください：<answer>yes</answer> または <answer>no</answer>\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'応答は {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% エラーが発生したためyesと回答します %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                あなたは現在、キーワードの情報に基づいて推測を行っています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づき、キーワードに対する次の最良の推測を生成し、\n",
    "                次の形式で記入してください：\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - 上記の履歴に基づいて重複する推測を避けてください\n",
    "                - 推測は特定の人、場所、または物であるべきです\n",
    "\n",
    "                あなたの推測は何ですか？\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'france'\n",
    "    \n",
    "# vLLMとジェネレーターの構成\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name,\n",
    "        \"--dtype=half\"\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLMの起動完了\")\n",
    "except ValueError:\n",
    "    print('vLLMは既に起動しています')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの才能あるプレイヤーです。あなたは正確で、集中力があり、\n",
    "構造的なアプローチを持っています。あなたは有用な質問を作成し、推測を行い、または\n",
    "キーワードに関する質問に答えます。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# エントリポイント\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは'{letters[0]}'の文字で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次の文字のいずれかで始まりますか {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# シンプルな質問者\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"20の質問ゲームをしていますか？\",\n",
    "            \"キーワードは物で、場所ではありませんか？\",\n",
    "            \"キーワードは場所ですか？\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # 過去の回答に基づいてkeyword_dfをフィルタリング\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"キーワードは物で、場所ではありませんか？\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"キーワードは場所ですか？\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('キーワードは次の文字で始まりますか？'):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('サンプル文字', n_letters, sample_letters)\n",
    "        return sample_letters # ', '.join(sample_letters)\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"次の中のキーワードはどれですか？ {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # 推測した単語を削除\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # データフレームをリセット\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # フレームワーク内で非同期を処理\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd44df9",
   "metadata": {},
   "source": [
    "# エージェントを自己対戦させてテストする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:27:50.490153Z",
     "iopub.status.busy": "2024-06-25T21:27:50.489818Z",
     "iopub.status.idle": "2024-06-25T21:27:50.500109Z",
     "shell.execute_reply": "2024-06-25T21:27:50.499088Z",
     "shell.execute_reply.started": "2024-06-25T21:27:50.490126Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最初の文字の質問をフォーマットする関数を定義\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは'{letters[0]}'の文字で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次の文字のいずれかで始まりますか {formatted_letters}?\"\n",
    "\n",
    "# 例として'a', 'b', 'c'の場合にフォーマットされた質問を表示\n",
    "format_first_letter_question(['a', 'b', 'c'])\n",
    "\n",
    "import re\n",
    "\n",
    "# 質問から文字を抽出する関数を定義\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"  # アポストロフィで囲まれた文字を検索\n",
    "    matches = re.findall(pattern, question)  # 正規表現を使ってマッチを抽出\n",
    "    return matches\n",
    "\n",
    "# 上記のコードは、キーワードの最初の文字に関する質問をフォーマットする方法を示し、\n",
    "# また、特定の質問から文字を抽出するための関数を定義しています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:53.891959Z",
     "iopub.status.busy": "2024-06-25T18:43:53.891614Z",
     "iopub.status.idle": "2024-06-25T18:43:53.944887Z",
     "shell.execute_reply": "2024-06-25T18:43:53.94407Z",
     "shell.execute_reply.started": "2024-06-25T18:43:53.891923Z"
    }
   },
   "outputs": [],
   "source": [
    "# 自動再読み込みを有効にする\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# main.pyから必要なクラスと関数をインポート\n",
    "from main import Observation, agent_fn, observe\n",
    "\n",
    "# 上記のコードは、Jupyter環境においてモジュールが変更された際に自動的に再読み込みを行い、\n",
    "# 最新の定義をインポートするための設定をしています。これにより、開発中のコード変更が即座に反映されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:53.94651Z",
     "iopub.status.busy": "2024-06-25T18:43:53.946184Z",
     "iopub.status.idle": "2024-06-25T18:43:54.938834Z",
     "shell.execute_reply": "2024-06-25T18:43:54.937806Z",
     "shell.execute_reply.started": "2024-06-25T18:43:53.946485Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLMが実行中かどうかを確認\n",
    "!ps -aef | grep vllm\n",
    "\n",
    "# このコマンドは、現在実行中のプロセスの中からvLLMに関連するプロセスをフィルタリングして表示します。\n",
    "# これにより、vLLMが正しく起動しているか確認できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:54.940887Z",
     "iopub.status.busy": "2024-06-25T18:43:54.940586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# キーワードデータフレームをparquetファイルから読み込む\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "# ランダムに1つのサンプルを取得\n",
    "sample = keyword_df.sample(1)\n",
    "\n",
    "# 観測データを作成\n",
    "obs = Observation(step=0,\n",
    "    role='guesser',  # 役割を'guesser'に設定\n",
    "    turnType=\"ask\",  # 最初は質問を行う\n",
    "    keyword=sample['keyword'].values[0],  # サンプルからキーワードを取得\n",
    "    category=sample['category'].values[0],  # サンプルからカテゴリを取得\n",
    "    questions=[],  # 質問のリストを初期化\n",
    "    answers=[],  # 回答のリストを初期化\n",
    "    guesses=[],  # 推測のリストを初期化\n",
    ")\n",
    "\n",
    "question_agent = None\n",
    "\n",
    "# 20ラウンドにわたって質問と回答を繰り返す\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)  # 質問を生成\n",
    "    print(f'[{i} Question]: {question}')\n",
    "    obs.questions.append(question)  # 質問を追加\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)  # 回答を生成\n",
    "    obs.answers.append(answer)  # 回答を追加\n",
    "    \n",
    "    # 各質問に対する正しい回答を生成\n",
    "    if obs.questions[-1].startswith('Are we playing 20 questions?'):\n",
    "        gt_answer = answer  # 任意の回答を記録\n",
    "    elif obs.questions[-1].startswith('Is the keyword a thing that is not a location?'):\n",
    "        if sample['category'].values[0] == 'things':\n",
    "            gt_answer = 'yes'  # 正解\n",
    "        else:\n",
    "            gt_answer = 'no'  # 不正解\n",
    "    elif obs.questions[-1].startswith('Is the keyword a place?'):\n",
    "        if sample['category'].values[0] == 'place':\n",
    "            gt_answer = 'yes'  # 正解\n",
    "        else:\n",
    "            gt_answer = 'no'  # 不正解\n",
    "    elif obs.questions[-1].startswith('Does the keyword start'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])  # 質問から文字を抽出\n",
    "        gt_answer = obs.keyword[0] in letters_guess\n",
    "        gt_answer = 'yes' if gt_answer else 'no'  # 正解かどうかを判断\n",
    "    elif obs.questions[-1].startswith('Is the keyword one of the following?'):\n",
    "        possible_kw = obs.questions[-1].replace('Is the keyword one of the following? ', '').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]  # キーワードのリストを整形\n",
    "        print(possible_kw)\n",
    "        gt_answer = obs.keyword in possible_kw  # サンプルキーワードが候補に含まれているか\n",
    "        gt_answer = 'yes' if gt_answer else 'no'  # 正解かどうかを判断\n",
    "\n",
    "    print(f'[{i} Answer]: {answer} [True Answer]: {gt_answer}')\n",
    "    if answer != gt_answer:  # 回答が正しいか確認\n",
    "        break\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'  # 推測を行うターンに変更\n",
    "    guess = await observe(obs)  # 推測を生成\n",
    "    print(f'[{i} Guess]: {guess} - [Keyword]: {obs.keyword}')\n",
    "    obs.guesses.append(guess)  # 推測を追加\n",
    "    if guess == obs.keyword:  # 推測が正解か確認\n",
    "        print('GOT IT!')\n",
    "        break\n",
    "        \n",
    "    obs.step += 1  # ステップを進める\n",
    "\n",
    "# 上記のコードは、キーワードについて20ターンにわたって質問、回答、推測を繰り返すエージェントの動作をシミュレーションします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:25:45.793242Z",
     "iopub.status.busy": "2024-06-25T18:25:45.79235Z",
     "iopub.status.idle": "2024-06-25T18:25:45.834748Z",
     "shell.execute_reply": "2024-06-25T18:25:45.833946Z",
     "shell.execute_reply.started": "2024-06-25T18:25:45.793208Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最後の質問を定義\n",
    "last_question = \"Does the keyword start with one of the letters 'a', 'n' or 'g'?\"\n",
    "# ジェネレーターを使用して答えるプロンプトを構築\n",
    "out = await generator.chat(f\"\"\"\\\n",
    "あなたは、キーワード[{obs.keyword}]に対して正確にyes/noの質問に答えます。このキーワードはカテゴリ[{obs.category}]に属します。\n",
    "\n",
    "[QUESTION] \"{last_question}\" [/QUESTION]\n",
    "\n",
    "キーワードは[{obs.keyword}]です - 質問にはこの特定のキーワードに正確に回答してください。\n",
    "\n",
    "上のyes/noの質問に答え、次の形式で記入してください：\n",
    "{Answer.xml_example()}\n",
    "\"\"\").run() \n",
    "\n",
    "# 上記のコードは、ジェネレーターを使用して特定の質問に対するyes/noの応答を生成するためのプロンプトを作成します。\n",
    "# これは、エージェントがキーワードに基づいて正確な回答を提供するのを助けるためのものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:27.467095Z",
     "iopub.status.busy": "2024-06-25T16:01:27.466766Z",
     "iopub.status.idle": "2024-06-25T16:01:29.430507Z",
     "shell.execute_reply": "2024-06-25T16:01:29.429725Z",
     "shell.execute_reply.started": "2024-06-25T16:01:27.467071Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最後の質問を定義\n",
    "last_question = \"Does the keyword start with one of the letters 'a', 'n' or 'g'?\"\n",
    "# ジェネレーターを使用して答えるプロンプトを構築\n",
    "out = await generator.chat(f\"\"\"\\\n",
    "あなたは20の質問ゲームをプレイしています。あなたのタスクは、特定のキーワードに関するyes/noの質問に正確に答えることです。\n",
    "\n",
    "キーワード: [{obs.keyword}]\n",
    "カテゴリ: [{obs.category}]\n",
    "\n",
    "質問: {last_question}\n",
    "\n",
    "指示:\n",
    "1. 回答する際は、キーワード[{obs.keyword}]のみを考慮してください。\n",
    "2. 質問には「yes」または「no」で答えてください。\n",
    "3. 答えを出す前に、正確性をダブルチェックしてください。\n",
    "4. 下記のXML形式で答えを提供してください。\n",
    "\n",
    "あなたの応答はこの正確な形式であるべきです：\n",
    "<answer>yes</answer>\n",
    "OR\n",
    "<answer>no</answer>\n",
    "\n",
    "さて、キーワード[{obs.keyword}]に対して質問に正確に答えてください：\n",
    "\"\"\").run() \n",
    "\n",
    "# 上記のコードは、ジェネレーターを使用してプレイヤーに特定の質問に対するyes/noの応答を正確に生成するよう指示するプロンプトを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:29.432014Z",
     "iopub.status.busy": "2024-06-25T16:01:29.431678Z",
     "iopub.status.idle": "2024-06-25T16:01:29.47228Z",
     "shell.execute_reply": "2024-06-25T16:01:29.471434Z",
     "shell.execute_reply.started": "2024-06-25T16:01:29.431977Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最後の出力の内容を表示\n",
    "print(out.prev[-1].content)\n",
    "\n",
    "# このコードは、生成された応答の前のやり取りの最後の内容を表示します。\n",
    "# これにより、LLMがどのように応答したかを確認することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:30.352451Z",
     "iopub.status.busy": "2024-06-25T16:01:30.351758Z",
     "iopub.status.idle": "2024-06-25T16:01:30.394972Z",
     "shell.execute_reply": "2024-06-25T16:01:30.393982Z",
     "shell.execute_reply.started": "2024-06-25T16:01:30.352412Z"
    }
   },
   "outputs": [],
   "source": [
    "# 最後の出力の内容を表示\n",
    "out.last\n",
    "\n",
    "# このコードは、生成された応答の最後の内容を表示します。\n",
    "# これにより、LLMの応答の詳細を確認することができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec880f38",
   "metadata": {},
   "source": [
    "# モデルとコードを提出用に圧縮\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.status.busy": "2024-06-25T15:37:49.840946Z",
     "iopub.status.idle": "2024-06-25T15:37:49.84127Z",
     "shell.execute_reply": "2024-06-25T15:37:49.841124Z",
     "shell.execute_reply.started": "2024-06-25T15:37:49.84111Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします\n",
    "!apt install pigz pv\n",
    "\n",
    "# このコマンドは、圧縮を高速化するためのpigz（並列実行のgzip）と\n",
    "# プログレスバーを表示するためのpvをインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:16:49.684667Z",
     "iopub.status.busy": "2024-06-18T19:16:49.684329Z",
     "iopub.status.idle": "2024-06-18T19:19:49.999133Z",
     "shell.execute_reply": "2024-06-18T19:19:49.997937Z",
     "shell.execute_reply.started": "2024-06-18T19:16:49.684638Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデルとコードを圧縮して提出用のtar.gzファイルを作成します\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\  # 指定したディレクトリから必要なフォルダを追加\n",
    "    -C /kaggle/working main.py util.py \\  # メインのPythonファイルとユーティリティファイルを追加\n",
    "    -C /kaggle/working keywords.parquet  # キーワードデータファイルを追加\n",
    "\n",
    "# このコマンドは、指定されたファイルとディレクトリを圧縮し、提出用のtar.gzファイルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:50.002318Z",
     "iopub.status.busy": "2024-06-18T19:19:50.001899Z",
     "iopub.status.idle": "2024-06-18T19:19:51.037256Z",
     "shell.execute_reply": "2024-06-18T19:19:51.036303Z",
     "shell.execute_reply.started": "2024-06-18T19:19:50.002277Z"
    }
   },
   "outputs": [],
   "source": [
    "# 作成したsubmission.tar.gzファイルを表示\n",
    "!ls -GFlash --color\n",
    "\n",
    "# このコマンドは、カラフルな表示で現在のディレクトリ内のファイルとフォルダをリストします。\n",
    "# これにより、提出用に圧縮したファイルが正しく作成されたか確認できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ce0d2",
   "metadata": {},
   "source": [
    "# Kaggle CLIを使用しての提出\n",
    "\n",
    "必要に応じて、ノートブックを再実行することなく、KaggleのCLIインターフェースを使用して提出することができます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:51.040706Z",
     "iopub.status.busy": "2024-06-18T19:19:51.040378Z",
     "iopub.status.idle": "2024-06-18T19:19:51.086269Z",
     "shell.execute_reply": "2024-06-18T19:19:51.085134Z",
     "shell.execute_reply.started": "2024-06-18T19:19:51.040676Z"
    }
   },
   "outputs": [],
   "source": [
    "# Kaggle CLIを使用してコンペティションに提出するコマンド（コメントアウトされています）\n",
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"submit from notebook\"\n",
    "\n",
    "# 上記のコマンドは、KaggleのCLIを使用してコンペティションに提出するためのものです。\n",
    "# KAGGLE_USERNAMEとKAGGLE_KEYは、Kaggleの認証情報を指定するために使用します。\n",
    "# submission.tar.gzファイルを提出し、メッセージを添えて送信します。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
