{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15b2c6e",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションにおけるAIエージェントの開発に取り組んでいます。このエージェントは、質問と推測を通じて指定されたキーワードを当てる「20の質問」ゲームをプレイするために設計されています。\n",
    "\n",
    "### 解決する問題\n",
    "Notebookは、AIエージェントが効率的に質問・回答を生成し、ターゲットキーワードを推測する能力を持つことを目指しています。このプロセスには、大規模言語モデル（LLM）の利用が含まれ、特に「gemma2」というモデルが用いられています。\n",
    "\n",
    "### 使用する手法やライブラリ\n",
    "1. **環境設定：** 必要なライブラリである`bitsandbytes`と`accelerate`がインストールされ、`transformers`ライブラリが最新バージョンにアップグレードされます。\n",
    "2. **Hugging Face Hubからモデルのダウンロード：** `snapshot_download`関数を用いて、gemma2モデルのスナップショットをHugging Faceからローカルにダウンロードします。\n",
    "3. **モデルのデバイスマッピング：** モデルを複数のデバイスに分散させ、メモリと計算リソースを効率化するための設定が行われます。\n",
    "4. **回答生成機能：** `generate_answer`という関数が設計され、テンプレートを元にモデルからの出力を生成します。この関数は、質問や推測に対する応答を得るために用いられます。\n",
    "5. **ロボットクラスの実装：** 質問、推測、回答を行うためのロボットクラスが設計され、エージェントのコアロジックが実装されています。\n",
    "6. **エージェントのインターフェース：** `agent`関数がエージェントのメイン処理を担当し、観察インスタンスを受け取り、現在のターンタイプに基づいて質問、推測、回答を行います。\n",
    "\n",
    "### まとめ\n",
    "このNotebookは、Kaggleの「LLM 20 Questions」コンペティション向けに設計されたAIエージェントの構築を目的とし、Hugging Faceのモデルを利用して質問と推測を行います。手法としては、ライブラリのインストール、モデルのダウンロード、デバイスマッピング、回答生成のためのロジック実装などが含まれています。ただし、出力ディレクトリが容量を超えたため、submission.tarファイルが作成できず、提出には失敗したことが報告されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455d28f0",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語を簡単に解説します。特に、ノートブック特有のドメイン知識や実務経験がないと馴染みがない用語に焦点を当てています。\n",
    "\n",
    "1. **submissionディレクトリ**:\n",
    "   - コンペティションに提出するためのファイルを格納するためのディレクトリ。ここには、提出するモデルやエージェントのコードが保存されます。\n",
    "\n",
    "2. **bitsandbytes**:\n",
    "   - 大規模モデルの量子化やメモリ効率を向上させるために使用されるライブラリで、特にGPUメモリを節約するための機能を提供します。\n",
    "\n",
    "3. **accelerate**:\n",
    "   - Transformersライブラリと組み合わせて使用することで、モデルのトレーニングや推論を高速化するためのライブラリ。特に、マルチデバイス環境での効率化が図れます。\n",
    "\n",
    "4. **huggingface_hub**:\n",
    "   - Hugging Faceが提供するモデルやデータセットのリポジトリサービス。モデルを簡単に共有・ダウンロードできるプラットフォームです。\n",
    "\n",
    "5. **snapshot_download**:\n",
    "   - Hugging Face Hubからモデルの特定の状態（スナップショット）をダウンロードする関数。モデルのバージョン管理ができます。\n",
    "\n",
    "6. **device_maps**:\n",
    "   - モデルのレイヤーをどのハードウェアデバイス（例えばGPU）にマッピングするかを指定するための設定。大規模モデルを複数のデバイスに分散させる際に有用です。\n",
    "\n",
    "7. **メモリ効率の良いSDP**:\n",
    "   - ストレージデータパターン（SDP）と呼ばれる技術で、計算リソースを効率的に使うための技術。メモリ管理を最適化し、特定のハードウェア環境でのパフォーマンスを向上させます。\n",
    "\n",
    "8. **BitsAndBytesConfig**:\n",
    "   - モデルの量子化設定を行うためのクラスで、特に8ビットでモデルを読み込む設定が含まれています。これにより、モデルが使用するメモリを大幅に削減できます。\n",
    "\n",
    "9. **quantization**:\n",
    "   - モデルのパラメータをより小さなデータ型（例えば、32ビット浮動小数点から8ビット整数）に変換するプロセス。これにより、メモリ使用量が少なくなり、速度が向上することがあります。\n",
    "\n",
    "10. **回帰チェックポイント**:\n",
    "    - モデルがトレーニング中に、メモリを節約するために中間層の出力を保存せずに、必要に応じて再計算を行う技術。これにより、GPUメモリの使用量を減少させることができます。\n",
    "\n",
    "11. **観察クラス**:\n",
    "    - ゲームの状態や進捗を追跡するためのクラス。ターンの情報や質問・回答・推測を記録します。\n",
    "\n",
    "12. **エージェント**:\n",
    "    - ゲーム内で特定の役割を持つプログラム。質問者や回答者の役割を果たし、相手の質問に対して適切な応答を生成します。\n",
    "\n",
    "これらの用語は、特にこのノートブック特有の文脈やコンペティションにおいて重要性が高く、初心者にとっては理解が難しい場合があります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-10T15:39:11.56763Z",
     "iopub.status.busy": "2024-07-10T15:39:11.56738Z",
     "iopub.status.idle": "2024-07-10T15:39:56.873551Z",
     "shell.execute_reply": "2024-07-10T15:39:56.872658Z",
     "shell.execute_reply.started": "2024-07-10T15:39:11.567607Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# submissionディレクトリを作成します。\n",
    "# -pオプションを使用すると、既に存在する場合はエラーを出さずにディレクトリを作成します。\n",
    "mkdir -p /kaggle/working/submission\n",
    "\n",
    "# bitsandbytesとaccelerateというライブラリをインストールします。\n",
    "pip install bitsandbytes accelerate\n",
    "\n",
    "# transformersライブラリを最新バージョンにアップグレードします。\n",
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba82868d",
   "metadata": {},
   "source": [
    "**秘密を保持:** アドオン > 秘密"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:39:56.875485Z",
     "iopub.status.busy": "2024-07-10T15:39:56.875218Z",
     "iopub.status.idle": "2024-07-10T15:39:57.049016Z",
     "shell.execute_reply": "2024-07-10T15:39:57.048069Z",
     "shell.execute_reply.started": "2024-07-10T15:39:56.875462Z"
    }
   },
   "outputs": [],
   "source": [
    "# kaggle_secretsパッケージからUserSecretsClientをインポートします。\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "# UserSecretsClientのインスタンスを作成します。\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "# HF_TOKENを文字列またはNoneとして初期化します。\n",
    "HF_TOKEN: str | None = None\n",
    "\n",
    "# シークレットから'hf_token'を取得しようとします。\n",
    "try:\n",
    "    # hf_tokenという名前の秘密を取得します。\n",
    "    HF_TOKEN = secrets.get_secret(\"hf_token\")\n",
    "except:\n",
    "    # 取得中にエラーが発生した場合は何もしません（pass）。\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb0ed41",
   "metadata": {},
   "source": [
    "**ローカルにLLMをダウンロードする。** gemma2は入力から呼び出すことで実行できなかったため、ローカルにダウンロードしました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:39:57.050416Z",
     "iopub.status.busy": "2024-07-10T15:39:57.050137Z",
     "iopub.status.idle": "2024-07-10T15:40:57.561902Z",
     "shell.execute_reply": "2024-07-10T15:40:57.560938Z",
     "shell.execute_reply.started": "2024-07-10T15:39:57.050392Z"
    }
   },
   "outputs": [],
   "source": [
    "# huggingface_hubからsnapshot_downloadをインポートします。\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "# モデルを保存するパスを設定します。\n",
    "g_model_path = Path(\"/kaggle/working/submission/model\")\n",
    "\n",
    "# 既にモデルのパスが存在する場合は、そのディレクトリを削除します。\n",
    "if g_model_path.exists():\n",
    "    # shutil.rmtreeを使って、ディレクトリとその中身を再帰的に削除します。\n",
    "    shutil.rmtree(g_model_path)\n",
    "\n",
    "# 親ディレクトリを含めてモデルの新しいディレクトリを作成します。\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "# Hugging Face Hubからモデルのスナップショットをダウンロードします。\n",
    "snapshot_download(\n",
    "    # リポジトリIDを指定します。\n",
    "    repo_id=\"google/gemma-2-9b-it\",\n",
    "    # \"original*\"という名前のパターンを無視します。\n",
    "    ignore_patterns=\"original*\",\n",
    "    # ダウンロード先のローカルディレクトリを指定します。\n",
    "    local_dir=g_model_path,\n",
    "    # シンボリックリンクを使用せずにローカルディレクトリを使用します。\n",
    "    local_dir_use_symlinks=False,\n",
    "    # グローバルなHF_TOKENをトークンとして使用します。なければNoneを使用します。\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:40:57.564206Z",
     "iopub.status.busy": "2024-07-10T15:40:57.563925Z",
     "iopub.status.idle": "2024-07-10T15:40:58.593106Z",
     "shell.execute_reply": "2024-07-10T15:40:58.592201Z",
     "shell.execute_reply.started": "2024-07-10T15:40:57.564182Z"
    }
   },
   "outputs": [],
   "source": [
    "# 指定したパスにあるファイルやディレクトリの詳細情報をリスト表示します。\n",
    "!ls -l /kaggle/working/submission/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf2118c",
   "metadata": {},
   "source": [
    "**モデルレイヤーのデバイスマッピング**\n",
    "大きなモデルを扱う際、レイヤーを複数のデバイスに分散させることで、メモリや計算リソースをより効率的に管理するのに役立ちます。device_maps変数は、モデルのどのレイヤーがどのデバイスに割り当てられているかを指定するために使用されます。\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)を使用することで、PyTorchにおけるメモリ効率の良いSDPを無効にすることができます。これは特定のモデルやデバッグ目的で必要になることがあります。この設定により、異なるハードウェア環境間でのパフォーマンスや一貫性を向上させることができます。\n",
    "\n",
    "[Gemma 2をプロンプトする方法](https://huggingface.co/blog/gemma2#how-to-prompt-gemma-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:40:58.595334Z",
     "iopub.status.busy": "2024-07-10T15:40:58.595041Z",
     "iopub.status.idle": "2024-07-10T15:40:58.797631Z",
     "shell.execute_reply": "2024-07-10T15:40:58.796439Z",
     "shell.execute_reply.started": "2024-07-10T15:40:58.595309Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "# 必要なライブラリをインポートします。\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig\n",
    "\n",
    "# PyTorchのメモリ効率的SDPを無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "# Kaggleエージェントのパスを設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    # 指定されたパスが存在しない場合は、ローカルのモデルパスを使用します。\n",
    "    model_id = \"/kaggle/working/submission/model\"\n",
    "\n",
    "# モデルレイヤーのデバイスマッピングを定義します。\n",
    "device_maps = [('model.layers.0', 0),\n",
    " ('model.layers.1', 0),\n",
    " ('model.layers.2', 0),\n",
    " ('model.layers.3', 0),\n",
    " ('model.layers.4', 0),\n",
    " ('model.layers.5', 0),\n",
    " ('model.layers.6', 0),\n",
    " ('model.layers.7', 0),\n",
    " ('model.layers.8', 0),\n",
    " ('model.layers.9', 0),\n",
    " ('model.layers.10', 0),\n",
    " ('model.layers.11', 0),\n",
    " ('model.layers.12', 0),\n",
    " ('model.layers.13', 0),\n",
    " ('model.layers.14', 0),\n",
    " ('model.layers.15', 0),\n",
    " ('model.layers.16', 0),\n",
    " ('model.layers.17', 0),\n",
    " ('model.layers.18', 0),\n",
    " ('model.layers.19', 1),\n",
    " ('model.layers.20', 1),\n",
    " ('model.layers.21', 1),\n",
    " ('model.layers.22', 1),\n",
    " ('model.layers.23', 1),\n",
    " ('model.layers.24', 1),\n",
    " ('model.layers.25', 1),\n",
    " ('model.layers.26', 1),\n",
    " ('model.layers.27', 1),\n",
    " ('model.layers.28', 1),\n",
    " ('model.layers.29', 1),\n",
    " ('model.layers.30', 1),\n",
    " ('model.layers.31', 1),\n",
    " ('model.layers.32', 1),\n",
    " ('model.layers.33', 1),\n",
    " ('model.layers.34', 1),\n",
    " ('model.layers.35', 1),\n",
    " ('model.layers.36', 1),\n",
    " ('model.layers.37', 1),\n",
    " ('model.layers.38', 1),\n",
    " ('model.layers.39', 1),\n",
    " ('model.layers.40', 1),\n",
    " ('model.layers.41', 1),\n",
    " ('model.embed_tokens', 1),\n",
    " ('model.layers', 1)]\n",
    "\n",
    "# 量子化設定を行います（8ビットで読み込みます）。\n",
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "# トークナイザーをモデルIDから初期化します。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id) \n",
    "# <eos>トークンのIDを取得します。\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<eos>\"])[0]\n",
    "\n",
    "# デバイスの設定を行います。\n",
    "device = {layer: gpu_mem for (layer, gpu_mem) in device_maps}\n",
    "\n",
    "# モデルの設定を読み込みます。\n",
    "config = AutoConfig.from_pretrained(model_id)\n",
    "config.gradient_checkpointing = True\n",
    "\n",
    "# モデルを初期化します。\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", quantization_config=quantization_config,\n",
    "                                             device_map=\"auto\", trust_remote_code=True, config=config)\n",
    "\n",
    "# 回答を生成する関数を定義します。\n",
    "def generate_answer(template):\n",
    "    # テンプレートをトークン化し、CUDAデバイスに移動させます。\n",
    "    input_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    # モデルから生成된出力を取得します。\n",
    "    output_ids = model.generate(**input_ids, max_new_tokens=15).squeeze()\n",
    "    # 生成の開始位置を取得します。\n",
    "    start_gen = input_ids.input_ids.shape[1]\n",
    "    # 出力から生成された部分を抽出します。\n",
    "    output_ids = output_ids[start_gen:]\n",
    "    if id_eot in output_ids:\n",
    "        stop = output_ids.tolist().index(id_eot)\n",
    "        output = tokenizer.decode(output_ids[:stop])\n",
    "    else:\n",
    "        output = tokenizer.decode(output_ids)\n",
    "    # 改行および特定のトークンを削除します。\n",
    "    output = re.sub('\\n', '', output)\n",
    "    output = re.sub(' <end_of_turn>', '', output)\n",
    "    output = re.sub('<end_of_turn>', '', output)\n",
    "    return output\n",
    "\n",
    "# ロボットクラスを定義します。\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    # モードに応じてロボットの動作を制御します。\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"\n",
    "            else:\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "\n",
    "    # 質問者の動作を定義します。\n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者がはい/いいえで質問します。\n",
    "        キーワードは特定の「物」または「場所」です。\n",
    "        \"\"\"\n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            20の質問を始めましょう。あなたは質問者の役割を果たしています。\n",
    "            はいまたはいいえで答えられる質問をしてください。\n",
    "            キーワードがモロッコだった場合の例を以下に示します:\n",
    "            例:\n",
    "            <あなた: それは国ですか？\n",
    "            ユーザー: はい\n",
    "            あなた: ヨーロッパにありますか？\n",
    "            ユーザー: いいえ\n",
    "            あなた: アフリカにありますか？\n",
    "            ユーザー: はい\n",
    "            あなた: そこに住んでいる人の大多数は肌の色が暗いですか？\n",
    "            ユーザー: いいえ\n",
    "            あなた: mで始まる国名ですか？\n",
    "            ユーザー: はい\n",
    "            あなた: それはモロッコですか？\n",
    "            ユーザー: はい。>\n",
    "            ユーザーが選んだ言葉です。最初の質問をしてください！\n",
    "            短く、冗長にならずに、一つの質問のみを提供してください。\n",
    "            \"\"\"\n",
    "        \n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}<end_of_turn>\\n\"\"\"\n",
    "            chat_template += \"<start_of_turn>model\\n\"\n",
    "\n",
    "            # 前の質問を記録する場合\n",
    "            if len(obs.questions) >= 1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<end_of_turn>\\n<start_of_turn>user\\n\"\n",
    "                    chat_template += f\"{a}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            これまでのゲームの進行状況は次の通りです:\\n{conv}\n",
    "            この会話に基づいて、単語を推測できますか？単語だけを教えてください。\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{guess_prompt}<end_of_turn>\\n\"\"\"\n",
    "            chat_template += \"<start_of_turn>model\\n\"\n",
    "\n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "    \n",
    "    # 回答者の動作を定義します。\n",
    "    def answerer(self, obs):\n",
    "        ask_prompt = f\"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを思いつき、質問者がはい/いいえで質問します。\n",
    "        キーワードは特定の場所または物です。\\n\n",
    "        ユーザーの質問を理解し、あなたがプレイしているキーワードを理解してください。\n",
    "        現在、ユーザーが推測するべき単語は: \"{obs.keyword}\"、カテゴリは\"{obs.category}\"です。\n",
    "        キーワードがモロッコというカテゴリ「場所」の場合の例を示します:\n",
    "        例:\n",
    "        <ユーザー: それは場所ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: ヨーロッパにありますか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: アフリカにありますか？\n",
    "        あなた: はい\n",
    "        ユーザー: そこに住んでいる人々の大多数は肌の色が暗いですか？\n",
    "        あなた: いいえ\n",
    "        ユーザー: mで始まる国名ですか？\n",
    "        あなた: はい\n",
    "        ユーザー: それはモロッコですか？\n",
    "        あなた: はい。>\n",
    "        \"\"\"\n",
    "        chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}<end_of_turn>\\n\"\"\"\n",
    "        chat_template += \"<start_of_turn>user\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}\"\n",
    "        chat_template += \"<start_of_turn>model\\n\"\n",
    "        # 質問と回答の履歴を追加します。\n",
    "        if len(obs.answers) >= 1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{q}<end_of_turn>\\n<start_of_turn>user\\n\"\n",
    "                chat_template += f\"{a}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "# ロボットのインスタンスを作成します。\n",
    "robot = Robot()\n",
    "\n",
    "# エージェントのメイン関数を定義します。\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    # 現在のターンのタイプに応じて処理を分岐します。\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)\n",
    "        \n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)\n",
    "        \n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)\n",
    "        \n",
    "    # 応答がNoneまたは短い場合はデフォルトで\"yes\"を返します。\n",
    "    if response is None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:40:58.799087Z",
     "iopub.status.busy": "2024-07-10T15:40:58.798823Z",
     "iopub.status.idle": "2024-07-10T15:41:00.206372Z",
     "shell.execute_reply": "2024-07-10T15:41:00.205574Z",
     "shell.execute_reply.started": "2024-07-10T15:40:58.799064Z"
    }
   },
   "outputs": [],
   "source": [
    "# 簡単なエージェントの関数を定義します。\n",
    "# def simple_agent1(obs, cfg):\n",
    "#     # エージェントが推測者で、ターンタイプが\"ask\"の場合\n",
    "#     if obs.turnType == \"ask\":\n",
    "#         # いくつかの質問のリストを定義します。\n",
    "#         response_list = [\n",
    "#             'それはアフリカにありますか？',\n",
    "#             'それはアメリカにありますか？',\n",
    "#             'それはアジアにありますか？',\n",
    "#             'それはオセアニアにありますか？',\n",
    "#             'それは東ヨーロッパにありますか？',\n",
    "#             'それは北ヨーロッパにありますか？',\n",
    "#             'それは南ヨーロッパにありますか？',\n",
    "#             'それは西ヨーロッパにありますか？',\n",
    "#             'それは日本ですか？'\n",
    "#         ]\n",
    "#         # 質問の数からリストの応答を選択します。\n",
    "#         # response = response_list[len(obs.questions)]\n",
    "#         # ランダムに質問を選択します。\n",
    "#         response = random.choice(response_list)\n",
    "#     elif obs.turnType == \"guess\":\n",
    "#         # 推測の場合の応答を設定します。\n",
    "#         response = \"アヒル\"\n",
    "#     elif obs.turnType == \"answer\":\n",
    "#         # はいまたはいいえのランダムな応答を選択します。\n",
    "#         response = random.choices([\"はい\", \"いいえ\"])[0]\n",
    "#     return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:41:00.208008Z",
     "iopub.status.busy": "2024-07-10T15:41:00.207714Z",
     "iopub.status.idle": "2024-07-10T15:41:01.705608Z",
     "shell.execute_reply": "2024-07-10T15:41:01.704802Z",
     "shell.execute_reply.started": "2024-07-10T15:41:00.207986Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# ランダムモジュールをインポートします。\n",
    "# import random\n",
    "# kaggle_environmentsからmake関数をインポートします。\n",
    "# from kaggle_environments import make\n",
    "\n",
    "# エージェントのスクリプトのパスを指定します。\n",
    "# agent = \"/kaggle/working/submission/main.py\"\n",
    "\n",
    "# 環境を作成します（デバッグモード）。\n",
    "# env = make(\"llm_20_questions\", debug=True)\n",
    "\n",
    "# エージェントを指定してゲームを実行します。\n",
    "# game_output = env.run(agents=[agent, simple_agent1, simple_agent1, simple_agent1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:41:01.707399Z",
     "iopub.status.busy": "2024-07-10T15:41:01.707041Z",
     "iopub.status.idle": "2024-07-10T15:41:02.480554Z",
     "shell.execute_reply": "2024-07-10T15:41:02.479157Z",
     "shell.execute_reply.started": "2024-07-10T15:41:01.707365Z"
    }
   },
   "outputs": [],
   "source": [
    "# 環境のレンダリングを行います（IPythonモードにて、幅600、高さ500）。\n",
    "# env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:41:02.482477Z",
     "iopub.status.busy": "2024-07-10T15:41:02.482077Z",
     "iopub.status.idle": "2024-07-10T15:41:05.287786Z",
     "shell.execute_reply": "2024-07-10T15:41:05.2869Z",
     "shell.execute_reply.started": "2024-07-10T15:41:02.482443Z"
    }
   },
   "outputs": [],
   "source": [
    "# wgetコマンドを使用して、指定したURLからkeywords_local.pyをダウンロードします。\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:41:05.291607Z",
     "iopub.status.busy": "2024-07-10T15:41:05.291296Z",
     "iopub.status.idle": "2024-07-10T15:42:07.62528Z",
     "shell.execute_reply": "2024-07-10T15:42:07.624291Z",
     "shell.execute_reply.started": "2024-07-10T15:41:05.291581Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from submission.main import agent\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# 観察クラスを定義します。\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        # ステップ数を初期化します。\n",
    "        self.step = 0\n",
    "        # 役割を\"guesser\"（推測者）に設定します。\n",
    "        self.role = \"guesser\"\n",
    "        # ターンタイプを\"ask\"（質問）に設定します。\n",
    "        self.turnType = \"ask\"\n",
    "        # キーワード（今回の例では\"Japan\"）を設定します。\n",
    "        self.keyword = \"Japan\"\n",
    "        # カテゴリーを\"country\"（国）に設定します。\n",
    "        self.category = \"country\"\n",
    "        # 質問と回答のリストを初期化します。\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.guesses = []\n",
    "        \n",
    "# KEYWORDS_JSONからキーワードのデータフレームを作成する関数を定義します。\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    # JSON形式のデータをロードします。\n",
    "    json_data = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    # キーワード、カテゴリ、代替語のリストを作成します。\n",
    "    keyword_list = []\n",
    "    category_list = []\n",
    "    alts_list = []\n",
    "\n",
    "    # JSONデータをループし、情報を収集します。\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):\n",
    "            keyword = json_data[i]['words'][j]['keyword']\n",
    "            keyword_list.append(keyword)\n",
    "            category_list.append(json_data[i]['category'])\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])\n",
    "\n",
    "    # pandasのデータフレームを作成します。\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n",
    "    data_pd['keyword'] = keyword_list\n",
    "    data_pd['category'] = category_list\n",
    "    data_pd['alts'] = alts_list\n",
    "    \n",
    "    return data_pd\n",
    "    \n",
    "# キーワードのデータフレームを生成します。\n",
    "keywords_df = create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:42:07.627207Z",
     "iopub.status.busy": "2024-07-10T15:42:07.626631Z",
     "iopub.status.idle": "2024-07-10T15:46:53.496045Z",
     "shell.execute_reply": "2024-07-10T15:46:53.49507Z",
     "shell.execute_reply.started": "2024-07-10T15:42:07.627171Z"
    }
   },
   "outputs": [],
   "source": [
    "# 観察のインスタンスを作成します。\n",
    "obs = Observation()\n",
    "cfg = \"_\"\n",
    "\n",
    "# キーワードデータフレームからサンプルを抽出します。\n",
    "sample_df = keywords_df.sample()\n",
    "\n",
    "# 抽出したサンプルからキーワードとカテゴリを設定します。\n",
    "obs.keyword = sample_df[\"keyword\"].values[0]\n",
    "obs.category = sample_df[\"category\"].values[0]\n",
    "alts_list = sample_df[\"alts\"].values[0]\n",
    "# 代替語リストに現在のキーワードを追加します。\n",
    "alts_list.append(obs.keyword)\n",
    "\n",
    "# 抽出したキーワードを表示します。\n",
    "print(f\"keyword: {obs.keyword}\")\n",
    "\n",
    "# 最大20ラウンドのゲームプレイを行います。\n",
    "for round in range(20):\n",
    "    # 現在のステップを更新します。\n",
    "    obs.step = round + 1\n",
    "    \n",
    "    # 推測者として質問を行います。\n",
    "    obs.role = \"guesser\"\n",
    "    obs.turnType = \"ask\"\n",
    "    question = agent(obs, cfg)\n",
    "    # 質問を記録します。\n",
    "    obs.questions.append(question)\n",
    "    \n",
    "    # 回答者としての応答を行います。\n",
    "    obs.role = \"answerer\"\n",
    "    obs.turnType = \"answer\"\n",
    "    answer = agent(obs, cfg)\n",
    "    # 回答を記録します。\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    # 再び推測者として推測を行います。\n",
    "    obs.role = \"guesser\"\n",
    "    obs.turnType = \"guess\"\n",
    "    guess = agent(obs, cfg)\n",
    "    # 推測を記録します。\n",
    "    obs.guesses.append(guess)\n",
    "    \n",
    "    # 現在のラウンドと質問・回答・推測を表示します。\n",
    "    print(f\"round: {round + 1}\")\n",
    "    print(f\"question: {question}\")\n",
    "    print(f\"answer: {answer}\")\n",
    "    print(f\"guess: {guess}\")\n",
    "    \n",
    "    # 推測がキーワードの代替語リストに含まれている場合、勝利メッセージを表示し、ゲームを終了します。\n",
    "    if guess in alts_list:\n",
    "        print(\"勝利!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec7ace0",
   "metadata": {},
   "source": [
    "**提出できませんでした...** 出力ディレクトリが容量を超えたため、tarファイルを作成できませんでした。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:46:53.49879Z",
     "iopub.status.busy": "2024-07-10T15:46:53.497553Z",
     "iopub.status.idle": "2024-07-10T15:46:53.504268Z",
     "shell.execute_reply": "2024-07-10T15:46:53.503391Z",
     "shell.execute_reply.started": "2024-07-10T15:46:53.498731Z"
    }
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします。出力を非表示にします。\n",
    "# !apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:46:53.505787Z",
     "iopub.status.busy": "2024-07-10T15:46:53.505413Z",
     "iopub.status.idle": "2024-07-10T15:46:53.515746Z",
     "shell.execute_reply": "2024-07-10T15:46:53.515003Z",
     "shell.execute_reply.started": "2024-07-10T15:46:53.505737Z"
    }
   },
   "outputs": [],
   "source": [
    "# tarコマンドを使用して、submissionディレクトリを圧縮してsubmission.tar.gzを作成します。\n",
    "# pigzを使用して圧縮を行い、pvで進行状況を表示します。\n",
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-10T15:46:53.517652Z",
     "iopub.status.busy": "2024-07-10T15:46:53.516865Z",
     "iopub.status.idle": "2024-07-10T15:46:53.531178Z",
     "shell.execute_reply": "2024-07-10T15:46:53.530504Z",
     "shell.execute_reply.started": "2024-07-10T15:46:53.517622Z"
    }
   },
   "outputs": [],
   "source": [
    "# tar.gzファイルの中身を確認するためのコードです。\n",
    "# import tarfile\n",
    "\n",
    "# tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")\n",
    "# # tarファイル内のすべてのファイルメンバーを表示します。\n",
    "# for file in tar.getmembers():\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8df68f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Yuang Wu\n",
    "> \n",
    "> 推測する際に一単語だけを出力する方法について教えてください…私はgemma 7b-it 3を使用しましたが、エージェントは推測時に複数の単語を生成し、質問も奇妙なものになっています。\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Valentin Baltazar\n",
    "> > \n",
    "> > 同じです…知りたいです。gemma2はそんなに優れているのでしょうか？\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## KasaharaTopic 著者\n",
    "> > > \n",
    "> > > 私もgemma7b-itを使用しているときに同じ問題がありました。gemma2は少し改善されたようですが、私の実験ではllama3が正しい長さの単語を出力するのに最適なモデルです。\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Yuang Wu\n",
    "> > > \n",
    "> > > そうですね、私はgemma 2 9bに変更したので、状況が改善されました。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## ravi tanwar\n",
    "> \n",
    "> このコンペティションへの提出は成功しましたか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## KasaharaTopic 著者\n",
    "> > \n",
    "> > submission.tarファイルを作成できませんでした。出力ディレクトリが容量を超えたため、提出できませんでした。\n",
    "> > \n",
    "> > たとえ提出できたとしても、このプロンプトは改善の余地があると思います。質問をうまく生成できていません。\n",
    "> > \n",
    "> > \n",
    "> > "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
