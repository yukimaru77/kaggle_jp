{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de6c713d",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、Kaggleの「20の質問」ゲームに挑むためのAIエージェントを開発しています。具体的には、言語モデルを利用した質問者（Questioner）と回答者（Answerer）のエージェントを設計し、ターゲットとなる単語をできるだけ少ない質問で推測する能力を実装しています。\n",
    "\n",
    "### 問題の説明\n",
    "コンペティションの「20の質問」ゲームは、質問者がターゲットとなる単語を推測するためにはい/いいえの質問を行うという推理ゲームであり、エージェントは戦略的に質問を展開しなければなりません。このノートブックは、AIエージェントがこのゲームをプレイできるように設計されています。\n",
    "\n",
    "### 使用している手法とライブラリ\n",
    "- **Transformersライブラリ**: `transformers`を使用して、あらかじめ訓練されたモデル（Llama3）をロードし、言語生成に利用しています。このモデルは、質問に基づいて回答を生成する能力を備えています。\n",
    "- **PyTorch**: モデルのセットアップと動作にPyTorchが使用され、特にGPUを使ってメモリ効率の良い実行を行うための設定がされています。\n",
    "- **カスタムフォーマッタ**: `Llama3Formatter`クラスや`Llama3Agent`クラスを使用して、プロンプトのフォーマットやデータの構造化を担当します。\n",
    "\n",
    "### エージェントの設計\n",
    "1. **質問者エージェント（Llama3QuestionerAgent）**: 質問者としての戦略を実装し、ターゲットとなる単語のカテゴリや文字の考慮を行いながら、解答を導出する能力を持っています。\n",
    "2. **回答者エージェント（Llama3AnswererAgent）**: 提供された質問に対して、適切に「はい」または「いいえ」を応答するように設計されています。\n",
    "\n",
    "エージェントは、状態をリセットし、過去の質問と回答をトラッキングしながら、プレイヤーとしてのロールを果たします。\n",
    "\n",
    "このノートブックは、AIエージェントが「20の質問」ゲームにおいてどのように振る舞うかをシミュレートするために、関連するコードやロジックを実装している重要なリソースとなっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8b9c3f",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づき、初心者がつまずきそうな専門用語の解説です。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **システムパス**:\n",
    "   - Pythonプログラムがモジュールやライブラリを探すためのディレクトリのリスト。システムパスを設定することで、特定のディレクトリからモジュールをインポートできるようにする。\n",
    "\n",
    "2. **トークナイザー**:\n",
    "   - 自然言語処理（NLP）において、テキストを単語やサブワードなどのトークンに分割するためのツール。モデルに入力する前に、テキストデータを適切な形式に変換する。\n",
    "\n",
    "3. **Causal LM**:\n",
    "   - 「因果言語モデル」の略で、特定の単語やトークンを生成するために、前のトークンを上下文として用いるモデル。主に生成タスクに使用される。\n",
    "\n",
    "4. **デバイスマップ**:\n",
    "   - 複数のGPUやCPUがある環境で、どのデバイス上で計算を実行するかを指定する方法。`device_map=\"auto\"`は、自動で最適なデバイスを選択するよう指示。\n",
    "\n",
    "5. **torch_dtype**:\n",
    "   - PyTorchにおいて、テンソルのデータ型を指定するための引数。`torch.bfloat16`は、一般的な32ビット浮動小数点数よりも少ないメモリでデータを表現できる形式で、特にトレーニングでの計算効率を向上させる。\n",
    "\n",
    "6. **メモリ効率を最適化する機能**:\n",
    "   - GPUのメモリ使用量を減らし、より大きなモデルやバッチサイズを扱うことが可能になるようにするオプション。`enable_mem_efficient_sdp`や`enable_flash_sdp`はそれに関連。\n",
    "\n",
    "7. **イテラブル**:\n",
    "   - ループなどで反復処理できるオブジェクト。リスト、タプル、辞書などが該当します。`Iterable`はPythonの型ヒントにも使われ、反復可能であることを示します。\n",
    "\n",
    "8. **プロンプト**:\n",
    "   - モデルに対して出力を生成させるための入力データ。文脈情報や質問など、モデルが応答を生成する際に参照する情報を提供します。\n",
    "\n",
    "9. **アスタリスク**:\n",
    "   - 特に回答フォーマットにおいて使われる記号。ユーザーからのテキストを強調するために用いられ、通常は太字として表示されます。\n",
    "\n",
    "10. **ターン**:\n",
    "    - 人間同士の対話における発話の一つ一つを指し、モデルとユーザ間の連続的なやりとりを管理するための概念。\n",
    "\n",
    "11. **最大新規トークン数**:\n",
    "    - モデルが生成できる新しいトークンの最大数を制限するパラメータ。過度に長い生成を防ぎ、必要な情報を効率的に取得するために使う。\n",
    "\n",
    "12. **不均一な要素の交互に取り出す関数**:\n",
    "    - 異なる長さの二つのリストから、可能な限り交互に要素を取り出す関数。この関数は、特定の操作をする際に有用です。\n",
    "\n",
    "これらの用語は、全体として自然言語処理や機械学習の文脈で頻繁に登場するものであり、特に初学者にとっては重要な概念を理解する助けとなるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57047db0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:45.176925Z",
     "iopub.status.busy": "2024-07-22T18:14:45.176479Z",
     "iopub.status.idle": "2024-07-22T18:14:45.188448Z",
     "shell.execute_reply": "2024-07-22T18:14:45.187358Z",
     "shell.execute_reply.started": "2024-07-22T18:14:45.176887Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e5d231",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"llama-3/transformers/8b-chat-hf/1\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class Llama3Formatter:\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self.reset()\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state.append({'role': 'user', 'content': prompt})\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state.append({'role': 'assistant', 'content': prompt})\n",
    "        return self\n",
    "    \n",
    "    def system(self, prompt):\n",
    "        self._state.append({'role': 'system', 'content': prompt})\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = []\n",
    "        if self._system_prompt is not None:\n",
    "            self.system(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "class Llama3Agent:\n",
    "    def __init__(self, system_prompt=None, few_shot_examples=None):\n",
    "        self.formatter = Llama3Formatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)\n",
    "        self.terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        \n",
    "        ### Load original model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            WEIGHTS_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = self.formatter.get_dict()\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class Llama3QuestionerAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.category_determined = False\n",
    "        self.is_place = False\n",
    "        self.first_char_range = (0, 25)  # A to Z in terms of index\n",
    "        self.second_char_range = None\n",
    "        self.final_guess = None\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        global guesses\n",
    "\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "\n",
    "        if not self.category_determined:\n",
    "            self.formatter.user(\"Is it a place?\")\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)  # Convert index to alphabet (0 -> A, 1 -> B, ..., 25 -> Z)\n",
    "                self.formatter.user(f\"Does the keyword start with a letter before {mid_char}?\")\n",
    "            elif self.final_guess is None:\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)\n",
    "                self.formatter.user(f\"Does the second letter of the keyword come before {mid_char}?\")\n",
    "            else:\n",
    "                self.formatter.user(f\"Is the keyword **{self.final_guess}**?\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if not self.category_determined:\n",
    "            answer = self._parse_keyword(response)\n",
    "            self.is_place = (answer == 'yes')\n",
    "            self.category_determined = True\n",
    "            return \"Is it a place?\"\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.first_char_range = (self.first_char_range[0], mid_index)\n",
    "                else:\n",
    "                    self.first_char_range = (mid_index + 1, self.first_char_range[1])\n",
    "\n",
    "                if self.first_char_range[0] == self.first_char_range[1]:\n",
    "                    self.second_char_range = (0, 25)  # Reset for second character\n",
    "                    return f\"Does the keyword start with {chr(65 + self.first_char_range[0])}?\"\n",
    "                else:\n",
    "                    mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the keyword start with a letter before {mid_char}?\"\n",
    "            elif self.final_guess is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.second_char_range = (self.second_char_range[0], mid_index)\n",
    "                else:\n",
    "                    self.second_char_range = (mid_index + 1, self.second_char_range[1])\n",
    "\n",
    "                if self.second_char_range[0] == self.second_char_range[1]:\n",
    "                    first_char = chr(65 + self.first_char_range[0])\n",
    "                    second_char = chr(65 + self.second_char_range[0])\n",
    "                    self.final_guess = first_char + second_char\n",
    "                    return f\"Does the keyword start with {first_char}{second_char}?\"\n",
    "                else:\n",
    "                    mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the second letter of the keyword come before {mid_char}?\"\n",
    "            else:\n",
    "                answer = self._parse_keyword(response)\n",
    "                if answer == 'yes':\n",
    "                    return f\"The keyword is **{self.final_guess}**.\"\n",
    "                else:\n",
    "                    self.final_guess = None\n",
    "                    return \"Let's continue guessing.\"\n",
    "\n",
    "\n",
    "class Llama3AnswererAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are a very smart AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific place or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is the category of the keyword place?\", \"**no**\",\n",
    "    \"Is it a food?\", \"**yes** Now guess the keyword in the category things.\",\n",
    "    \"**Veggie Burger**\", \"Correct.\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Llama3QuestionerAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Llama3AnswererAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "turnRound = 1\n",
    "guesses = []\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turnRound\n",
    "    global guesses\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        if turnRound == 1:\n",
    "            response = \"Is it a place?\" # First question\n",
    "        else:\n",
    "            response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "        turnRound += 1\n",
    "        guesses.append(response)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "        turnRound += 1\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# 準備\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境両方で動作するように、システムパスを下記のように設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False) # メモリ効率を最適化する機能を無効にします\n",
    "torch.backends.cuda.enable_flash_sdp(False) # フラッシュSDP機能を無効にします\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"llama-3/transformers/8b-chat-hf/1\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "# Llama3Formatterクラスを定義します\n",
    "class Llama3Formatter:\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self.reset()  # ステートをリセットします\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state.append({'role': 'user', 'content': prompt})  # ユーザーのプロンプトを追加します\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state.append({'role': 'assistant', 'content': prompt})  # モデルのプロンプトを追加します\n",
    "        return self\n",
    "    \n",
    "    def system(self, prompt):\n",
    "        self._state.append({'role': 'system', 'content': prompt})  # システムのプロンプトを追加します\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = []  # ステートを初期化します\n",
    "        if self._system_prompt is not None:\n",
    "            self.system(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)  # フォーマッタを循環させます\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)  # 各ターンにフォーマットを適用します\n",
    "        return self\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "class Llama3Agent:\n",
    "    def __init__(self, system_prompt=None, few_shot_examples=None):\n",
    "        self.formatter = Llama3Formatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーを初期化します\n",
    "        self.terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        \n",
    "        ### 元のモデルを読み込みます\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            WEIGHTS_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始します\n",
    "        prompt = self.formatter.get_dict()  # プロンプトを取得します\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出し結果を取得します\n",
    "        response = self._parse_response(response, obs)  # レスポンスを解析します\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # 未実装の状態でエラーを発生させます\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.model.device)  # モデルのデバイスにテンソルを移動します\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)  # 生成されたトークンをデコードします\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードを探します\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()  # 小文字に変換します\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # 未実装の状態でエラーを発生させます\n",
    "\n",
    "# 不均一な要素を交互に取り出す関数\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None  # 不均一な要素を交互に取り出します\n",
    "    ]\n",
    "\n",
    "class Llama3QuestionerAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.category_determined = False  # カテゴリーが決定されていない状態\n",
    "        self.is_place = False  # 場所かどうかのフラグ\n",
    "        self.first_char_range = (0, 25)  # AからZまでのインデックス範囲\n",
    "        self.second_char_range = None  # 2文字目の範囲\n",
    "        self.final_guess = None  # 最終的な推測\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        global guesses  # グローバル変数を参照\n",
    "\n",
    "        self.formatter.reset()  # フォーマッターをリセットします\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ユーザーに役割を指定します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取り出します\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # フォーマットを適用します\n",
    "\n",
    "        if not self.category_determined:\n",
    "            self.formatter.user(\"Is it a place?\")  # カテゴリーが決定されていない場合の質問\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2  # 中間インデックスの計算\n",
    "                mid_char = chr(65 + mid_index)  # インデックスをアルファベットに変換 (0 -> A, 1 -> B, ..., 25 -> Z)\n",
    "                self.formatter.user(f\"Does the keyword start with a letter before {mid_char}?\")\n",
    "            elif self.final_guess is None:\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)\n",
    "                self.formatter.user(f\"Does the second letter of the keyword come before {mid_char}?\")\n",
    "            else:\n",
    "                self.formatter.user(f\"Is the keyword **{self.final_guess}**?\")  # 最終推測を確認します\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if not self.category_determined:\n",
    "            answer = self._parse_keyword(response)  # レスポンスからキーワードを解析します\n",
    "            self.is_place = (answer == 'yes')  # 'yes'の場合は場所と判定\n",
    "            self.category_determined = True  # カテゴリーが決定したことを記録\n",
    "            return \"Is it a place?\"  # 質問を返します\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.first_char_range = (self.first_char_range[0], mid_index)  # 範囲を更新\n",
    "                else:\n",
    "                    self.first_char_range = (mid_index + 1, self.first_char_range[1])\n",
    "\n",
    "                if self.first_char_range[0] == self.first_char_range[1]:\n",
    "                    self.second_char_range = (0, 25)  # 2文字目の範囲をリセット\n",
    "                    return f\"Does the keyword start with {chr(65 + self.first_char_range[0])}?\"\n",
    "                else:\n",
    "                    mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the keyword start with a letter before {mid_char}?\"\n",
    "            elif self.final_guess is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.second_char_range = (self.second_char_range[0], mid_index)  # 2文字目の範囲を更新\n",
    "                else:\n",
    "                    self.second_char_range = (mid_index + 1, self.second_char_range[1])\n",
    "\n",
    "                if self.second_char_range[0] == self.second_char_range[1]:  # 最終推測を決定\n",
    "                    first_char = chr(65 + self.first_char_range[0])\n",
    "                    second_char = chr(65 + self.second_char_range[0])\n",
    "                    self.final_guess = first_char + second_char\n",
    "                    return f\"Does the keyword start with {first_char}{second_char}?\"\n",
    "                else:\n",
    "                    mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the second letter of the keyword come before {mid_char}?\"\n",
    "            else:\n",
    "                answer = self._parse_keyword(response)\n",
    "                if answer == 'yes':\n",
    "                    return f\"The keyword is **{self.final_guess}**.\"  # 最終的な答えを返します\n",
    "                else:\n",
    "                    self.final_guess = None  # 最終推測をリセット\n",
    "                    return \"Let's continue guessing.\"  # 推測を続けることを提案します\n",
    "\n",
    "\n",
    "class Llama3AnswererAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッターをリセットします\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # 回答者としての役割を指定します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取り出します\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # フォーマットを適用します\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\") #  yes/noの回答を要求\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # レスポンスからキーワードを解析します\n",
    "        return 'yes' if 'yes' in answer else 'no'  # 'yes'が含まれていれば'yes'を返します\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計された非常に賢いAIアシスタントです。このゲームでは、回答者がキーワードを考え、その回答者のyes-or-noの質問に対して質問者が応答します。キーワードは特定の場所または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is the category of the keyword place?\", \"**no**\",\n",
    "    \"Is it a food?\", \"**yes** Now guess the keyword in the category things.\",\n",
    "    \"**Veggie Burger**\", \"Correct.\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義します。これにより、必要なエージェントを一度だけ読み込むことができます。\n",
    "# 両方を読み込むと、メモリオーバーが発生する可能性があります。\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Llama3QuestionerAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Llama3AnswererAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "turnRound = 1\n",
    "guesses = []\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turnRound\n",
    "    global guesses\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        if turnRound == 1:\n",
    "            response = \"Is it a place?\" # 最初の質問\n",
    "        else:\n",
    "            response = get_agent('questioner')(obs)  # 質問者エージェントを呼び出します\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "        turnRound += 1  # 回合を増やします\n",
    "        guesses.append(response)  # 推測を追加\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントを呼び出します\n",
    "        turnRound += 1  # 回合を増やします\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"  # レスポンスが空または短い場合はyesを返します\n",
    "    else:\n",
    "        return response  # それ以外のレスポンスを返します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:54.171348Z",
     "iopub.status.busy": "2024-07-22T18:14:54.170957Z",
     "iopub.status.idle": "2024-07-22T18:14:54.187649Z",
     "shell.execute_reply": "2024-07-22T18:14:54.186496Z",
     "shell.execute_reply.started": "2024-07-22T18:14:54.171301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 準備\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境両方で動作するように、システムパスを下記のように設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False) # メモリ効率を最適化する機能を無効にします\n",
    "torch.backends.cuda.enable_flash_sdp(False) # フラッシュSDP機能を無効にします\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"llama-3/transformers/8b-chat-hf/1\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "# Llama3Formatterクラスを定義します\n",
    "class Llama3Formatter:\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self.reset()  # ステートをリセットします\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state.append({'role': 'user', 'content': prompt})  # ユーザーのプロンプトを追加します\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state.append({'role': 'assistant', 'content': prompt})  # モデルのプロンプトを追加します\n",
    "        return self\n",
    "    \n",
    "    def system(self, prompt):\n",
    "        self._state.append({'role': 'system', 'content': prompt})  # システムのプロンプトを追加します\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = []  # ステートを初期化します\n",
    "        if self._system_prompt is not None:\n",
    "            self.system(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)  # フォーマッタを循環させます\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)  # 各ターンにフォーマットを適用します\n",
    "        return self\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "class Llama3Agent:\n",
    "    def __init__(self, system_prompt=None, few_shot_examples=None):\n",
    "        self.formatter = Llama3Formatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーを初期化します\n",
    "        self.terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        \n",
    "        ### 元のモデルを読み込みます\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            WEIGHTS_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始します\n",
    "        prompt = self.formatter.get_dict()  # プロンプトを取得します\n",
    "        response = self._call_llm(prompt)  # LLMを呼び出し結果を取得します\n",
    "        response = self._parse_response(response, obs)  # レスポンスを解析します\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # 未実装の状態でエラーを発生させます\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.model.device)  # モデルのデバイスにテンソルを移動します\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)  # 生成されたトークンをデコードします\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードを探します\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()  # 小文字に変換します\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # 未実装の状態でエラーを発生させます\n",
    "\n",
    "# 不均一な要素を交互に取り出す関数\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None  # 不均一な要素を交互に取り出します\n",
    "    ]\n",
    "\n",
    "class Llama3QuestionerAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.category_determined = False  # カテゴリーが決定されていない状態\n",
    "        self.is_place = False  # 場所かどうかのフラグ\n",
    "        self.first_char_range = (0, 25)  # AからZまでのインデックス範囲\n",
    "        self.second_char_range = None  # 2文字目の範囲\n",
    "        self.final_guess = None  # 最終的な推測\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        global guesses  # グローバル変数を参照\n",
    "\n",
    "        self.formatter.reset()  # フォーマッターをリセットします\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ユーザーに役割を指定します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取り出します\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # フォーマットを適用します\n",
    "\n",
    "        if not self.category_determined:\n",
    "            self.formatter.user(\"Is it a place?\")  # カテゴリーが決定されていない場合の質問\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2  # 中間インデックスの計算\n",
    "                mid_char = chr(65 + mid_index)  # インデックスをアルファベットに変換 (0 -> A, 1 -> B, ..., 25 -> Z)\n",
    "                self.formatter.user(f\"Does the keyword start with a letter before {mid_char}?\")\n",
    "            elif self.final_guess is None:\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)\n",
    "                self.formatter.user(f\"Does the second letter of the keyword come before {mid_char}?\")\n",
    "            else:\n",
    "                self.formatter.user(f\"Is the keyword **{self.final_guess}**?\")  # 最終推測を確認します\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if not self.category_determined:\n",
    "            answer = self._parse_keyword(response)  # レスポンスからキーワードを解析します\n",
    "            self.is_place = (answer == 'yes')  # 'yes'の場合は場所と判定\n",
    "            self.category_determined = True  # カテゴリーが決定したことを記録\n",
    "            return \"Is it a place?\"  # 質問を返します\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.first_char_range = (self.first_char_range[0], mid_index)  # 範囲を更新\n",
    "                else:\n",
    "                    self.first_char_range = (mid_index + 1, self.first_char_range[1])\n",
    "\n",
    "                if self.first_char_range[0] == self.first_char_range[1]:\n",
    "                    self.second_char_range = (0, 25)  # 2文字目の範囲をリセット\n",
    "                    return f\"Does the keyword start with {chr(65 + self.first_char_range[0])}?\"\n",
    "                else:\n",
    "                    mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the keyword start with a letter before {mid_char}?\"\n",
    "            elif self.final_guess is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.second_char_range = (self.second_char_range[0], mid_index)  # 2文字目の範囲を更新\n",
    "                else:\n",
    "                    self.second_char_range = (mid_index + 1, self.second_char_range[1])\n",
    "\n",
    "                if self.second_char_range[0] == self.second_char_range[1]:  # 最終推測を決定\n",
    "                    first_char = chr(65 + self.first_char_range[0])\n",
    "                    second_char = chr(65 + self.second_char_range[0])\n",
    "                    self.final_guess = first_char + second_char\n",
    "                    return f\"Does the keyword start with {first_char}{second_char}?\"\n",
    "                else:\n",
    "                    mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the second letter of the keyword come before {mid_char}?\"\n",
    "            else:\n",
    "                answer = self._parse_keyword(response)\n",
    "                if answer == 'yes':\n",
    "                    return f\"The keyword is **{self.final_guess}**.\"  # 最終的な答えを返します\n",
    "                else:\n",
    "                    self.final_guess = None  # 最終推測をリセット\n",
    "                    return \"Let's continue guessing.\"  # 推測を続けることを提案します\n",
    "\n",
    "\n",
    "class Llama3AnswererAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッターをリセットします\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # 回答者としての役割を指定します\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に取り出します\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # フォーマットを適用します\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\") #  yes/noの回答を要求\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # レスポンスからキーワードを解析します\n",
    "        return 'yes' if 'yes' in answer else 'no'  # 'yes'が含まれていれば'yes'を返します\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計された非常に賢いAIアシスタントです。このゲームでは、回答者がキーワードを考え、その回答者のyes-or-noの質問に対して質問者が応答します。キーワードは特定の場所または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is the category of the keyword place?\", \"**no**\",\n",
    "    \"Is it a food?\", \"**yes** Now guess the keyword in the category things.\",\n",
    "    \"**Veggie Burger**\", \"Correct.\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義します。これにより、必要なエージェントを一度だけ読み込むことができます。\n",
    "# 両方を読み込むと、メモリオーバーが発生する可能性があります。\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Llama3QuestionerAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Llama3AnswererAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "turnRound = 1\n",
    "guesses = []\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turnRound\n",
    "    global guesses\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        if turnRound == 1:\n",
    "            response = \"Is it a place?\" # 最初の質問\n",
    "        else:\n",
    "            response = get_agent('questioner')(obs)  # 質問者エージェントを呼び出します\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "        turnRound += 1  # 回合を増やします\n",
    "        guesses.append(response)  # 推測を追加\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントを呼び出します\n",
    "        turnRound += 1  # 回合を増やします\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"  # レスポンスが空または短い場合はyesを返します\n",
    "    else:\n",
    "        return response  # それ以外のレスポンスを返します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290060b2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:15.133496Z",
     "iopub.status.busy": "2024-07-22T18:15:15.132612Z",
     "iopub.status.idle": "2024-07-22T18:15:23.195997Z",
     "shell.execute_reply": "2024-07-22T18:15:23.194844Z",
     "shell.execute_reply.started": "2024-07-22T18:15:15.133456Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19825a7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ llama-3/transformers/8b-chat-hf/1\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ llama-3/transformers/8b-chat-hf/1\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:24.886486Z",
     "iopub.status.busy": "2024-07-22T18:15:24.88602Z",
     "iopub.status.idle": "2024-07-22T18:21:46.738456Z",
     "shell.execute_reply": "2024-07-22T18:21:46.736412Z",
     "shell.execute_reply.started": "2024-07-22T18:15:24.886445Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ llama-3/transformers/8b-chat-hf/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764d8525",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:23:33.154199Z",
     "iopub.status.busy": "2024-07-22T03:23:33.153786Z",
     "iopub.status.idle": "2024-07-22T03:23:33.166077Z",
     "shell.execute_reply": "2024-07-22T03:23:33.164667Z",
     "shell.execute_reply.started": "2024-07-22T03:23:33.154165Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
