{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0fe1573",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは「LLM 20 Questions」コンペティションのためのベースラインモデルを実装しており、特に言語モデルを利用して「20の質問」ゲームのAIエージェントを構築することを目的としています。\n",
    "\n",
    "### 問題設定\n",
    "Notebookは、さまざまなLLM（大規模言語モデル）を利用し、限られた質問の中でターゲットとなる単語を推測するAIエージェントの作成に取り組んでいます。このエージェントは、質問を行い、回答を受け取り、推測を行うという一連の操作を通じて、効率的に情報を収集して正しい単語を導き出すことを目指します。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "1. **トークン化とモデルの設定**:\n",
    "   - `tokenizer.apply_chat_template`を使用して、モデルやプロンプトに特別なトークンを自動的に適用し、質問の生成をより効果的に行います。\n",
    "   - サポートしているモデルは、`LLAMA3`, `Phi-3`, `Qwen-2`の各バリエーションです。\n",
    "\n",
    "2. **ライブラリの利用**:\n",
    "   - `transformers`: Hugging Faceのライブラリを使用して、モデルのダウンロードやトークナイザーの設定を行います。\n",
    "   - `bitsandbytes`: メモリ使用の最適化に使用されるライブラリで、モデルを量子化する際に役立ちます。\n",
    "   - `pygame`: ゲームのシミュレーションに使用され、エージェントの動作を可視化するために利用されます。\n",
    "   - `kaggle_environments`: Kaggleの環境でエージェントの対戦を実行するために用います。\n",
    "\n",
    "3. **エージェントの構造**:\n",
    "   - エージェントは、質問を行う「質問者」、単語を推測する「推測者」、質問に「はい/いいえ」で応える「回答者」の3つの役割を持ち、各役割に応じたプロンプトを生成し、最適な応答を導くように設計されています。\n",
    "\n",
    "4. **モデルの量子化**:\n",
    "   - ダウンロードしたモデルをメモリに読み込み、4ビットの量子化を行うことで、ストレージの効率を向上させています。\n",
    "\n",
    "### 構成の概要\n",
    "Notebookの中では、まず必要なライブラリをインストールし、Hugging Faceからモデルをダウンロードして設定します。そして、エージェントの動作を定義するためのプロンプトを作成し、すべてのエージェントがゲーム内で適切に動作するような構造を持たせています。最後に、エージェントの提出用に必要なファイルを圧縮し、提出準備を整えます。\n",
    "\n",
    "このNotebookは、効率的な言語モデルを利用して「20の質問」ゲームに参加するための基盤を提供し、AIエージェントの開発における重要なステップを示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33096ca3",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づいて、機械学習・深層学習の初心者がつまずきやすい専門用語の簡単な解説です。\n",
    "\n",
    "1. **Tokenizer (トークナイザー)**\n",
    "   - テキストをモデルが理解可能な形（トークン）に変換する処理を行うツール。通常、文を単語やサブワードに分割し、それぞれにIDを割り当てます。\n",
    "\n",
    "2. **LLM (Large Language Model)**\n",
    "   - 大規模なデータセットで学習した自然言語処理モデル。多くのパラメータを持ち、文脈を理解し、自然なテキストを生成する能力があります。\n",
    "\n",
    "3. **HuggingFace Model Hub**\n",
    "   - 様々な事前学習されたモデルが提供されるプラットフォーム。モデルを簡単にダウンロードや利用できるAPIが用意されています。\n",
    "\n",
    "4. **量子化 (Quantization)**\n",
    "   - モデルのサイズを縮小し、計算効率を高めるために、パラメータのビット数を減らすプロセス。例えば、32ビット浮動小数点から8ビット整数に変換することが含まれます。\n",
    "\n",
    "5. **bitsandbytes**\n",
    "   - メモリ効率よく数量化されたモデルを扱うためのPythonライブラリ。低精度の計算を行いながら、GPUリソースを節約することができます。\n",
    "\n",
    "6. **SDP (Stochastic Dynamic Programming)**\n",
    "   - 最適化や強化学習に関連する手法。モデルが以前の状態と行動の情報を学習し、次の状態の選択を行う際に役立ちます。\n",
    "\n",
    "7. **プロンプト (Prompt)**\n",
    "   - モデルに対して与える指示や質問のこと。生成するテキストの文脈や条件を設定するために使用されます。\n",
    "\n",
    "8. **トークン (Token)**\n",
    "   - モデルに入力される最小単位の要素。単語や文字列の一部などを含む。モデルはこれを元に次のトークンを生成します。\n",
    "\n",
    "9. **デバイスマップ (Device Map)**\n",
    "   - モデルの各部分をどのデバイス（CPUやGPU）で実行するか管理するための設定。\n",
    "\n",
    "10. **ガーベジコレクション (Garbage Collection)**\n",
    "    - 使用しなくなったメモリを自動的に解放するプロセス。特にPythonでは、メモリリークを防ぐために重要です。\n",
    "\n",
    "11. **モデル識別子 (Model Identifier)**\n",
    "    - Hugging Face Model Hubにおける特定のモデルを一意に示す名前。通常はリポジトリ名で表現される。\n",
    "\n",
    "12. **ターン終了トークン (Turn Terminators)**\n",
    "    - モデルが応答を終了する際に使用する特定のトークン。会話のコンテキストを保持するのに役立つ。\n",
    "\n",
    "13. **メモリ効率 (Memory Efficiency)**\n",
    "    - プログラムやモデルが使用するメモリを最適化し、限られたリソースで最大のパフォーマンスを引き出すこと。\n",
    "\n",
    "これらの解説が、初心者が特定の用語に対する理解を深めるための助けになることを目的としています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65cf5b6",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# LLM 20 Questions Baseline\n",
    "\n",
    "You can change models and prompts conveniently since I used `tokenizer.apply_chat_template` to apply special tokens automatically. \n",
    "\n",
    "\n",
    "Supported models:\n",
    "- `LLAMA3 variants`\n",
    "- `Phi-3 variants`\n",
    "- `Qwen-2 variants`\n",
    "\n",
    "## Prerequisites\n",
    "Set accelerator to GPU T4\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LLM 20 Questions ベースライン\n",
    "\n",
    "`tokenizer.apply_chat_template`を使用して特別なトークンを自動的に適用したので、モデルやプロンプトを便利に変更できます。 \n",
    "\n",
    "\n",
    "サポートされているモデル:\n",
    "- `LLAMA3 バリエーション`\n",
    "- `Phi-3 バリエーション`\n",
    "- `Qwen-2 バリエーション`\n",
    "\n",
    "## 前提条件\n",
    "GPU T4をアクセラレータに設定してください。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8675d34f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-01T02:42:36.432851Z",
     "iopub.status.busy": "2024-08-01T02:42:36.432507Z",
     "iopub.status.idle": "2024-08-01T02:43:16.74253Z",
     "shell.execute_reply": "2024-08-01T02:43:16.741715Z",
     "shell.execute_reply.started": "2024-08-01T02:42:36.432822Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60153bc7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Download model\n",
    "\n",
    "### HuggingFace Login\n",
    "\n",
    "\n",
    "1. Issue HuggingFace access token (https://huggingface.co/settings/tokens)\n",
    "\n",
    "\n",
    "2. Add HuggingFace access token to secrets. You can find it in `Add-ons -> secrets`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot 2024-08-01 at 11.40.17 AM.png](attachment:fb5805e5-566e-41f1-ba50-0d9f9fade571.png)\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## モデルのダウンロード\n",
    "\n",
    "### HuggingFace ログイン\n",
    "\n",
    "\n",
    "1. HuggingFace アクセストークンを発行します (https://huggingface.co/settings/tokens)\n",
    "\n",
    "\n",
    "2. HuggingFace アクセストークンをシークレットに追加します。これは `Add-ons -> secrets` で見つけることができます。\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot 2024-08-01 at 11.40.17 AM.png](attachment:fb5805e5-566e-41f1-ba50-0d9f9fade571.png)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2b110",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.744239Z",
     "iopub.status.busy": "2024-08-01T02:43:16.743959Z",
     "iopub.status.idle": "2024-08-01T02:43:16.882488Z",
     "shell.execute_reply": "2024-08-01T02:43:16.88177Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.744215Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff47d4d7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Select Model\n",
    "\n",
    "Find your desired model from [HuggingFace Model Hub](https://huggingface.co/models) and use the model name in the next command.\n",
    "\n",
    "Supported models:\n",
    "- `LLAMA3 variants`\n",
    "- `Phi-3 variants`\n",
    "- `Qwen-2 variants`\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### モデルを選択\n",
    "\n",
    "[HuggingFace Model Hub](https://huggingface.co/models)から希望のモデルを見つけて、次のコマンドでモデル名を使用してください。\n",
    "\n",
    "サポートされているモデル:\n",
    "- `LLAMA3 バリエーション`\n",
    "- `Phi-3 バリエーション`\n",
    "- `Qwen-2 バリエーション`\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1f5aea",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.883834Z",
     "iopub.status.busy": "2024-08-01T02:43:16.883562Z",
     "iopub.status.idle": "2024-08-01T02:43:16.887672Z",
     "shell.execute_reply": "2024-08-01T02:43:16.886858Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.883809Z"
    }
   },
   "outputs": [],
   "source": [
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aff10e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Download Model via HuggingFace\n",
    "To reduce disk usage, download model in `/tmp/model`\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### HuggingFace経由でモデルをダウンロード\n",
    "ディスク使用量を削減するために、モデルを `/tmp/model` にダウンロードします。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ecef5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)  # 既存のモデルパスがあれば削除\n",
    "g_model_path.mkdir(parents=True)  # 新しいモデルパスを作成\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,  # モデル識別子\n",
    "    ignore_patterns=\"original*\",  # 指定したパターンを無視\n",
    "    local_dir=g_model_path,  # ダウンロード先のディレクトリ\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # HF_TOKENを取得\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.890807Z",
     "iopub.status.busy": "2024-08-01T02:43:16.890312Z",
     "iopub.status.idle": "2024-08-01T02:46:44.781238Z",
     "shell.execute_reply": "2024-08-01T02:46:44.78017Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.890775Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)  # 既存のモデルパスがあれば削除\n",
    "g_model_path.mkdir(parents=True)  # 新しいモデルパスを作成\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,  # モデル識別子\n",
    "    ignore_patterns=\"original*\",  # 指定したパターンを無視\n",
    "    local_dir=g_model_path,  # ダウンロード先のディレクトリ\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # HF_TOKENを取得\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8cbeb3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!ls -l /tmp/model\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!ls -l /tmp/model\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:44.783114Z",
     "iopub.status.busy": "2024-08-01T02:46:44.78267Z",
     "iopub.status.idle": "2024-08-01T02:46:45.823393Z",
     "shell.execute_reply": "2024-08-01T02:46:45.82208Z",
     "shell.execute_reply.started": "2024-08-01T02:46:44.783078Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21400f9a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Save quantized model\n",
    "Now, load downloaded model on memory with quantization.  \n",
    "This will save storage.\n",
    "\n",
    "\n",
    "Moreover, since the saved model has already been quantized, we do not need `bitsandbytes` package in `main.py`\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### 量子化されたモデルの保存\n",
    "ダウンロードしたモデルをメモリに読み込み、量子化を行います。  \n",
    "これにより、ストレージが節約されます。\n",
    "\n",
    "さらに、保存されたモデルはすでに量子化されているため、`main.py`では `bitsandbytes` パッケージは必要ありません。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06000ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# load model on memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# メモリにモデルを読み込む\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効化\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効化\n",
    "\n",
    "downloaded_model = \"/tmp/model\"  # ダウンロードしたモデルのパス\n",
    "\n",
    "# 量子化の設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,  # 4ビットで読み込む\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算データ型をfloat16に指定\n",
    ")\n",
    "\n",
    "# 事前学習モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,  # 読み込むモデルのパス\n",
    "    quantization_config = bnb_config,  # 量子化の設定\n",
    "    torch_dtype = torch.float16,  # データ型をfloat16に設定\n",
    "    device_map = \"auto\",  # デバイス自動設定\n",
    "    trust_remote_code = True,  # リモートコードを信頼する\n",
    ")\n",
    "\n",
    "# トークナイザーの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:45.825488Z",
     "iopub.status.busy": "2024-08-01T02:46:45.825093Z",
     "iopub.status.idle": "2024-08-01T02:47:26.827498Z",
     "shell.execute_reply": "2024-08-01T02:47:26.826601Z",
     "shell.execute_reply.started": "2024-08-01T02:46:45.825451Z"
    }
   },
   "outputs": [],
   "source": [
    "# メモリにモデルを読み込む\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効化\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効化\n",
    "\n",
    "downloaded_model = \"/tmp/model\"  # ダウンロードしたモデルのパス\n",
    "\n",
    "# 量子化の設定\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,  # 4ビットで読み込む\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算データ型をfloat16に指定\n",
    ")\n",
    "\n",
    "# 事前学習モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,  # 読み込むモデルのパス\n",
    "    quantization_config = bnb_config,  # 量子化の設定\n",
    "    torch_dtype = torch.float16,  # データ型をfloat16に設定\n",
    "    device_map = \"auto\",  # デバイス自動設定\n",
    "    trust_remote_code = True,  # リモートコードを信頼する\n",
    ")\n",
    "\n",
    "# トークナイザーの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c180d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# save model in submission directory\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# モデルを提出用ディレクトリに保存\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")  # モデル保存\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")  # トークナイザー保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:26.829232Z",
     "iopub.status.busy": "2024-08-01T02:47:26.82879Z",
     "iopub.status.idle": "2024-08-01T02:47:44.162649Z",
     "shell.execute_reply": "2024-08-01T02:47:44.161658Z",
     "shell.execute_reply.started": "2024-08-01T02:47:26.829206Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルを提出用ディレクトリに保存\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")  # モデル保存\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")  # トークナイザー保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d38c65",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# unload model from memory\n",
    "import gc, torch\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# メモリからモデルをアンロード\n",
    "import gc, torch\n",
    "del model, tokenizer  # モデルとトークナイザーを削除\n",
    "gc.collect()  # ガーベジコレクションを実行\n",
    "torch.cuda.empty_cache()  # CUDAメモリを空にする\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.164433Z",
     "iopub.status.busy": "2024-08-01T02:47:44.164008Z",
     "iopub.status.idle": "2024-08-01T02:47:44.529886Z",
     "shell.execute_reply": "2024-08-01T02:47:44.529036Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.164407Z"
    }
   },
   "outputs": [],
   "source": [
    "# メモリからモデルをアンロード\n",
    "import gc, torch\n",
    "del model, tokenizer  # モデルとトークナイザーを削除\n",
    "gc.collect()  # ガーベジコレクションを実行\n",
    "torch.cuda.empty_cache()  # CUDAメモリを空にする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2484e4e7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Agent\n",
    "\n",
    "### Prompts\n",
    "\n",
    "Prompts are reffered from the [Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library/library)\n",
    "\n",
    "Prompts are consisted of 2 parts:\n",
    "- `system_prompt`: This is the first question to determine category.\n",
    "- `chat_history`: This is the chat history to provide context for the model.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## エージェント\n",
    "\n",
    "### プロンプト\n",
    "\n",
    "プロンプトは[Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library/library)から参照されています。\n",
    "\n",
    "プロンプトは2つの部分で構成されています:\n",
    "- `system_prompt`: これはカテゴリを特定する最初の質問です。\n",
    "- `chat_history`: これはモデルにコンテキストを提供するチャット履歴です。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e968d30",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    ask_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to ask questions to the user to guess the word the user is thinking of.\n",
    "Narrow down the possibilities by asking yes/no questions.\n",
    "Think step by step and try to ask the most informative questions.\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # Chat history\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})\n",
    "        message.append({\"role\": \"user\", \"content\": a})\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    guess_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to guess the word the user is thinking of.\n",
    "Think step by step.\n",
    "\\n\"\"\"\n",
    "\n",
    "    # Chat history\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"so far, the current state of the game is as following:\\n{chat_history}\n",
    "        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to answer the questions of the user to help him guess the word you're thinking of.\n",
    "Your answers must be 'yes' or 'no'.\n",
    "The keyword is: \"{obs.keyword}\", it is of category: \"{obs.category}\"\n",
    "Provide accurate answers to help the user to guess the keyword.\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # Chat history\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})\n",
    "    \n",
    "    if len(obs.answers)>=1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})\n",
    "            message.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    ask_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーに質問をして、ユーザーが考えている単語を当てることです。\n",
    "「はい」または「いいえ」で答えられる質問をして可能性を絞り込みます。\n",
    "段階的に考え、最も情報量の多い質問をするようにしてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # チャット履歴\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})\n",
    "        message.append({\"role\": \"user\", \"content\": a})\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    guess_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーが考えている単語を当てることです。\n",
    "段階的に考えてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    # チャット履歴\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"現在のゲームの状況は以下の通りです:\\n{chat_history}\n",
    "        会話に基づいて、この単語を推測できますか？答えは単語のみで、詳細は不要です\"\"\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーが推測している単語を当てるのを手助けするために質問に答えることです。\n",
    "あなたの回答は「はい」または「いいえ」でなければなりません。\n",
    "キーワードは: \"{obs.keyword}\"、カテゴリは: \"{obs.category}\"です。\n",
    "ユーザーがキーワードを推測できるように正確な回答を提供してください。\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # チャット履歴\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})\n",
    "    \n",
    "    if len(obs.answers)>=1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})\n",
    "            message.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    return message\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.531437Z",
     "iopub.status.busy": "2024-08-01T02:47:44.531097Z",
     "iopub.status.idle": "2024-08-01T02:47:45.526013Z",
     "shell.execute_reply": "2024-08-01T02:47:45.524961Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.531412Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    ask_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーに質問をして、ユーザーが考えている単語を当てることです。\n",
    "「はい」または「いいえ」で答えられる質問をして可能性を絞り込みます。\n",
    "段階的に考え、最も情報量の多い質問をするようにしてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # チャット履歴\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})\n",
    "        message.append({\"role\": \"user\", \"content\": a})\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    guess_prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーが考えている単語を当てることです。\n",
    "段階的に考えてください。\n",
    "\\n\"\"\"\n",
    "\n",
    "    # チャット履歴\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"現在のゲームの状況は以下の通りです:\\n{chat_history}\n",
    "        会話に基づいて、この単語を推測できますか？答えは単語のみで、詳細は不要です\"\"\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # システムプロンプト\n",
    "    prompt = f\"\"\"あなたは「20の質問」ゲームをプレイする専門知識を持ったAIアシスタントです。\n",
    "あなたの役割は、ユーザーが推測している単語を当てるのを手助けするために質問に答えることです。\n",
    "あなたの回答は「はい」または「いいえ」でなければなりません。\n",
    "キーワードは: \"{obs.keyword}\"、カテゴリは: \"{obs.category}\"です。\n",
    "ユーザーがキーワードを推測できるように正確な回答を提供してください。\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # チャット履歴\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})\n",
    "    \n",
    "    if len(obs.answers)>=1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})\n",
    "            message.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    return message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d3cbd",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Agent\n",
    "\n",
    "To add more LLM models, add end-of-turn token to terminators list.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### エージェント\n",
    "\n",
    "複数のLLMモデルを追加するには、ターン終了トークンを終了トークンリストに追加します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb39f5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# specify end-of-turn tokens for your desired model\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "# Additional potential end-of-turn token\n",
    "# llama3, phi3, gwen2 by order\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "def generate_response(chat):\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n",
    "    response = outputs[0][inputs.shape[-1]:]\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # launch the guesser role\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        \n",
    "        input = asker_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        input = guesser_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)\n",
    "\n",
    "    if response == None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "\n",
    "    return response\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効化\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効化\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")  # エージェントパスが存在する場合\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"  # モデル保存パスを指定\n",
    "\n",
    "\n",
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,  # モデルのパスを指定\n",
    "    device_map=\"auto\",  # デバイス自動設定\n",
    "    trust_remote_code=True,  # リモートコードを信頼する\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)  # トークナイザーの読み込み\n",
    "\n",
    "# お望みのモデルに応じたターン終了トークンを指定\n",
    "terminators = [tokenizer.eos_token_id]  # デフォルトの終了トークン\n",
    "\n",
    "# 追加のターン終了トークンの候補\n",
    "# llama3, phi3, gwen2順\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "# 候補トークンをターン終了トークンに追加\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "def generate_response(chat):\n",
    "    # チャットをトークン化し、処理する\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    # モデルを介して新しいトークンを生成\n",
    "    outputs = model.generate(inputs, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n",
    "    response = outputs[0][inputs.shape[-1]:]  # 生成されたレスポンスを取得\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)  # デコードしてレスポンスを取得\n",
    "\n",
    "    return out  # レスポンスを返す\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"modeはこれらの値のいずれかのみ取ることができます: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問する役割を起動する\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 答える役割を起動する\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():  # 答えが「はい」の場合\n",
    "                output = \"yes\"  # 「はい」と設定\n",
    "            elif \"no\" in output.lower():  # 答えが「いいえ」の場合\n",
    "                output = \"no\"  # 「いいえ」と設定\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():  # どちらでもない場合\n",
    "                output = \"yes\"  # デフォルトで「はい」に設定\n",
    "        if mode == \"guessing\":\n",
    "            # 推測する役割を起動する\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        \n",
    "        input = asker_prompt(obs)  # 質問プロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        input = guesser_prompt(obs)  # 推測プロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)  # 答えプロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()  # Robotインスタンスの生成\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    # エージェントの動作を制御する関数\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)  # 質問する場合\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)  # 推測する場合\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)  # 答える場合\n",
    "\n",
    "    if response == None or len(response) <= 1:  # レスポンスがない場合\n",
    "        response = \"yes\"  # デフォルトで「はい」を返す\n",
    "\n",
    "    return response  # レスポンスを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.530407Z",
     "iopub.status.busy": "2024-08-01T02:47:45.530085Z",
     "iopub.status.idle": "2024-08-01T02:47:45.539942Z",
     "shell.execute_reply": "2024-08-01T02:47:45.538701Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.53038Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効化\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効化\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")  # エージェントパスが存在する場合\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"  # モデル保存パスを指定\n",
    "\n",
    "\n",
    "# モデルの読み込み\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,  # モデルのパスを指定\n",
    "    device_map=\"auto\",  # デバイス自動設定\n",
    "    trust_remote_code=True,  # リモートコードを信頼する\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)  # トークナイザーの読み込み\n",
    "\n",
    "# お望みのモデルに応じたターン終了トークンを指定\n",
    "terminators = [tokenizer.eos_token_id]  # デフォルトの終了トークン\n",
    "\n",
    "# 追加のターン終了トークンの候補\n",
    "# llama3, phi3, gwen2順\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "# 候補トークンをターン終了トークンに追加\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "def generate_response(chat):\n",
    "    # チャットをトークン化し、処理する\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    # モデルを介して新しいトークンを生成\n",
    "    outputs = model.generate(inputs, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n",
    "    response = outputs[0][inputs.shape[-1]:]  # 生成されたレスポンスを取得\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)  # デコードしてレスポンスを取得\n",
    "\n",
    "    return out  # レスポンスを返す\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"modeはこれらの値のいずれかのみ取ることができます: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問する役割を起動する\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 答える役割を起動する\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():  # 答えが「はい」の場合\n",
    "                output = \"yes\"  # 「はい」と設定\n",
    "            elif \"no\" in output.lower():  # 答えが「いいえ」の場合\n",
    "                output = \"no\"  # 「いいえ」と設定\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():  # どちらでもない場合\n",
    "                output = \"yes\"  # デフォルトで「はい」に設定\n",
    "        if mode == \"guessing\":\n",
    "            # 推測する役割を起動する\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        \n",
    "        input = asker_prompt(obs)  # 質問プロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        input = guesser_prompt(obs)  # 推測プロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)  # 答えプロンプトの生成\n",
    "        output = generate_response(input)  # レスポンス生成\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()  # Robotインスタンスの生成\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    # エージェントの動作を制御する関数\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)  # 質問する場合\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)  # 推測する場合\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)  # 答える場合\n",
    "\n",
    "    if response == None or len(response) <= 1:  # レスポンスがない場合\n",
    "        response = \"yes\"  # デフォルトで「はい」を返す\n",
    "\n",
    "    return response  # レスポンスを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f040ff3",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Simulation\n",
    "\n",
    "### Install pygame\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## シミュレーション\n",
    "\n",
    "### pygameをインストール\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cebacf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install pygame\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install pygame\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.541971Z",
     "iopub.status.busy": "2024-08-01T02:47:45.541077Z",
     "iopub.status.idle": "2024-08-01T02:48:02.91265Z",
     "shell.execute_reply": "2024-08-01T02:48:02.911458Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.541938Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a16c7c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile dumb.py\n",
    "\n",
    "def dumb_agent(obs, cfg):\n",
    "    \n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"Is it a duck?\"\n",
    "    # if agent is guesser and turnType is \"guess\"\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"duck\"\n",
    "    # if agent is the answerer\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"no\"\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile dumb.py\n",
    "\n",
    "def dumb_agent(obs, cfg):\n",
    "    \n",
    "    # エージェントが推測者でターンタイプが「ask」の場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"それはアヒルですか？\"  # 推測を尋ねる質問\n",
    "    # エージェントが推測者でターンタイプが「guess」の場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"アヒル\"  # アヒルと推測\n",
    "    # エージェントが回答者の場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"いいえ\"  # 疑問に対して「いいえ」と返答\n",
    "    \n",
    "    return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.914553Z",
     "iopub.status.busy": "2024-08-01T02:48:02.914201Z",
     "iopub.status.idle": "2024-08-01T02:48:02.921087Z",
     "shell.execute_reply": "2024-08-01T02:48:02.920176Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.914522Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile dumb.py\n",
    "\n",
    "def dumb_agent(obs, cfg):\n",
    "    \n",
    "    # エージェントが推測者でターンタイプが「ask」の場合\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"それはアヒルですか？\"  # 推測を尋ねる質問\n",
    "    # エージェントが推測者でターンタイプが「guess」の場合\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"アヒル\"  # アヒルと推測\n",
    "    # エージェントが回答者の場合\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"いいえ\"  # 疑問に対して「いいえ」と返答\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962a48c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\", debug=True)\n",
    "game_output = env.run(agents=[\"submission/main.py\", \"submission/main.py\", \"dumb.py\", \"dumb.py\"])\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\", debug=True)  # 環境を作成\n",
    "game_output = env.run(agents=[\"submission/main.py\", \"submission/main.py\", \"dumb.py\", \"dumb.py\"])  # エージェントを実行\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.922495Z",
     "iopub.status.busy": "2024-08-01T02:48:02.922131Z",
     "iopub.status.idle": "2024-08-01T02:49:45.023058Z",
     "shell.execute_reply": "2024-08-01T02:49:45.022131Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.922463Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\", debug=True)  # 環境を作成\n",
    "game_output = env.run(agents=[\"submission/main.py\", \"submission/main.py\", \"dumb.py\", \"dumb.py\"])  # エージェントを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d711995",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "env.render(mode=\"ipython\", width=600, height=700)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "env.render(mode=\"ipython\", width=600, height=700)  # ゲームの出力をレンダリング\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:51:55.095662Z",
     "iopub.status.busy": "2024-08-01T02:51:55.095291Z",
     "iopub.status.idle": "2024-08-01T02:51:55.156977Z",
     "shell.execute_reply": "2024-08-01T02:51:55.155859Z",
     "shell.execute_reply.started": "2024-08-01T02:51:55.095636Z"
    }
   },
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\", width=600, height=700)  # ゲームの出力をレンダリング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438a3cfb",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Submit Agent\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## エージェントの提出\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2970e08c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null  # 開いているプロセスを無視してパッケージをインストール\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:45.091747Z",
     "iopub.status.busy": "2024-08-01T02:49:45.091472Z",
     "iopub.status.idle": "2024-08-01T02:49:51.432444Z",
     "shell.execute_reply": "2024-08-01T02:49:51.431242Z",
     "shell.execute_reply.started": "2024-08-01T02:49:45.091723Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # 開いているプロセスを無視してパッケージをインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879ca93c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .  # 提出ファイルを圧縮してtarアーカイブを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:51.435019Z",
     "iopub.status.busy": "2024-08-01T02:49:51.434137Z",
     "iopub.status.idle": "2024-08-01T02:51:27.838366Z",
     "shell.execute_reply": "2024-08-01T02:51:27.836645Z",
     "shell.execute_reply.started": "2024-08-01T02:49:51.434979Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .  # 提出ファイルを圧縮してtarアーカイブを作成"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
