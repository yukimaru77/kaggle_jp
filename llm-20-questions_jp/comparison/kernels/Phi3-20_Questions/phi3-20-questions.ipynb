{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81bda22a",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「20の質問」コンペティションにおけるAIエージェントの開発を目的としています。具体的には、言語モデルである「Phi-3」を用いて、質問者と回答者の役割を持つ2つのエージェントが協力しながら、特定の単語を推測するゲームをプレイします。\n",
    "\n",
    "### 問題設定\n",
    "このノートブックは、「20の質問」というゲームにおいて、与えられた情報に基づき効率的に推測を行うAIエージェントを構築する問題に取り組んでいます。エージェントは、質問を通じて情報を引き出し、その情報に基づいて正しい答えを導き出さなければなりません。\n",
    "\n",
    "### 使用する手法やライブラリ\n",
    "1. **パッケージのインストール**: `tqdm`, `pydantic`, `transformers`ライブラリを使用して、プログラムの作業を容易にしています。これにより、モデルの読み込みやデータの構造定義が可能になります。\n",
    "   \n",
    "2. **モデルの設定**: Hugging Faceの`transformers`ライブラリを通じて、「Phi-3-mini-4k-instruct」モデルを使用する準備をしています。このモデルはテキスト生成能力を持ち、質問応答コンテキストでの操作に適しています。\n",
    "\n",
    "3. **データ管理**: `pydantic`を使ったデータクラスで、Kaggleの観察データや設定情報を管理しています。これにより、コードの可読性と保守性が向上します。\n",
    "\n",
    "4. **LLMのインタラクション**: 質問と応答を生成する`ask`関数を定義して、言語モデルにプロンプトを送り、生成されたテキストを得るプロセスを整備しています。\n",
    "\n",
    "5. **プレイ関数**: ゲームのロジックを処理する`play`関数では、受け取った観察データに基づいて質問者または回答者のプロンプトを生成し、適切な応答を得る仕組みが実装されています。\n",
    "\n",
    "### まとめ\n",
    "このJupyter Notebookは、「20の質問」ゲームのために「Phi-3」モデルを設定し、それを利用したAIエージェントを作成するための手順を示しています。使用される手法は、NLU (自然言語理解) および生成的AIの要素を取り入れたものであり、Kaggleコンペティションのルールを遵守しつつ、言語モデルの能力を最大限に活用しています。最終的には、提出ファイルを作成し、圧縮して送信する流れで完結します。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5fc941",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提示されたJupyterノートブックの内容から、機械学習・深層学習の初心者がつまずきそうな専門用語の解説です。\n",
    "\n",
    "1. **tqdm**:\n",
    "   - Pythonで使用されるプログレスバーライブラリ。ループの進捗状況を視覚的に表示し、長い処理の進捗を把握しやすくする。\n",
    "\n",
    "2. **pydantic**:\n",
    "   - データバリデーションや設定管理を簡素化するためのライブラリ。Pythonのデータクラスを利用して、型安全なデータ管理を提供する。\n",
    "\n",
    "3. **transformers**:\n",
    "   - Hugging Faceが提供するライブラリで、様々な自然言語処理モデル（BERT、GPT、T5など）を簡単に使用できるようにする。特に事前学習済みモデルを利用するためのシンプルなインターフェースを持つ。\n",
    "\n",
    "4. **gguf**:\n",
    "   - 特定のモデルファイル形式で、この場合はPhi-3が使用する重みや設定を持つファイル。一般的なTensorFlowやPyTorchのモデルとは異なる特定の形式。\n",
    "\n",
    "5. **device_map**:\n",
    "   - モデルをデプロイする際に、どのデバイス（CPUまたはGPU）で各部分を計算するかを指定するための設定。特に大規模モデルでは、メモリ使用量を最適化するために重要。\n",
    "\n",
    "6. **torch_dtype**:\n",
    "   - PyTorchで使用するデータ型を指定するオプション。モデルのメモリ使用量や計算精度に影響を与える。\n",
    "\n",
    "7. **pipeline**:\n",
    "   - transformersライブラリで提供される高レベルの機能で、特定の自然言語処理タスク（テキスト生成、分類など）を簡単に実行できるようにする。\n",
    "\n",
    "8. **max_new_tokens**:\n",
    "   - モデルが生成する新しいトークンの最大数を指定するパラメータ。出力の長さを制御するために重要。\n",
    "\n",
    "9. **dataclass**:\n",
    "   - Python 3.7以降で導入された構文で、クラスを簡単に作成できる機能。属性の定義と自動生成されるイニシャライザ、比較メソッドを持つ。\n",
    "\n",
    "10. **Literal**:\n",
    "    - Pythonの型ヒントで、特定のリテラル値のみを許容する型を定義するための機能。これにより、関数の引数として許可される値を制限できる。\n",
    "\n",
    "11. **ask**:\n",
    "    - 質問を行う関数で、与えられたプロンプトに基づいてモデルが生成する回答を取得する。\n",
    "\n",
    "12. **回帰的なモデル**:\n",
    "    - モデルが以前の入力と出力情報を基に次の出力を決定するプロセス。この文脈では、質問と回答のやり取りに関連している。\n",
    "\n",
    "13. **温度 (temperature)**:\n",
    "    - テキスト生成で使用されるパラメータで、出力の確率分布を調整する。高い値に設定すると多様性が増し、低い値ではより決定的な結果が得られる。\n",
    "\n",
    "14. **オーバーヘッドタイム (remainingOverageTime)**:\n",
    "    - 残りの追加時間を示す変数。特定のタスクに使用可能な残りのリソースを示す。\n",
    "\n",
    "15. **引数 (args)**:\n",
    "    - 関数に渡される入力値のこと。プログラムの動作を修正したり、異なるデータを処理したりするために使用する。\n",
    "\n",
    "これらの用語は、ノートブックの内容を理解するために重要ですが、初学者には馴染みが薄い可能性のあるものを中心に解説しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f937897",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Twenty Questions with Phi-3\n",
    "\n",
    "## Install packages to the submission folder\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# Phi-3との20の質問\n",
    "\n",
    "## 提出フォルダにパッケージをインストールする\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee174268",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "pip install -U -t /kaggle/working/submission/lib tqdm pydantic transformers -qq\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "pip install -U -t /kaggle/working/submission/lib tqdm pydantic transformers -qq\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U -t /kaggle/working/submission/lib tqdm pydantic transformers -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cf2173",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Llama.cpp setup (unused as of now)\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## Llama.cppの設定（現時点では未使用）\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b424452",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -t /kaggle/working/submission/lib llama-cpp-python \\\n",
    "#   --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -t /kaggle/working/submission/lib llama-cpp-python \\\n",
    "#   --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -t /kaggle/working/submission/lib llama-cpp-python \\\n",
    "#   --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8615775e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# mkdir -p /kaggle/working/submission/lib/phi3/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# mkdir -p /kaggle/working/submission/lib/phi3/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir -p /kaggle/working/submission/lib/phi3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccc5cf4",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# !curl -L \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf?download=true\" > \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# !curl -L \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf?download=true\" > \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf?download=true\" > \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f43af3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# import os, sys\n",
    "# KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "#     sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "# else:\n",
    "#     sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "#     WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# import os, sys\n",
    "# KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "#     sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "# else:\n",
    "#     sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "#     WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "#     sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "# else:\n",
    "#     sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "#     WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a5074",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%ls\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%ls\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1391d2",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Write the submission file\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 提出ファイルの作成\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e54379",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from typing import Literal, List\n",
    "import os, sys, json\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    os.chdir(os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    #WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    os.chdir(\"/kaggle/working/submission/lib\")\n",
    "    #WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "    \n",
    "print(f\"Current Directory is {os.getcwd()}. \\nFiles in here: {', '.join(os.listdir())}\")\n",
    "\n",
    "#Import model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,\n",
    "    cache_dir=\"./huggingface\"\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=\"./huggingface\")\n",
    "hf_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask(prompt: str, max_new_tokens=100) -> str:\n",
    "    result = hf_llm(text_inputs=prompt, return_full_text=False, temperature=0.2, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "assert ask(\"<|user|>\\nHello!<|end|>\\n<|assistant|>\")\n",
    "\n",
    "@dataclass\n",
    "class KaggleObservation:\n",
    "  remainingOverageTime: int | float\n",
    "  step: int\n",
    "\n",
    "  questions: list[str]\n",
    "  answers: list[str]\n",
    "  guesses: list[str]\n",
    "\n",
    "  role: Literal[\"guesser\", \"answerer\"]\n",
    "  turnType: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "\n",
    "  keyword: str\n",
    "  category: str\n",
    "\n",
    "@dataclass\n",
    "class KaggleConfig:\n",
    "  episodeSteps: int\n",
    "  actTimeout: int | float\n",
    "  runTimeout: int | float\n",
    "  agentTimeout: int | float\n",
    "  __raw_path__: str\n",
    "\n",
    "# llm = Llama(\n",
    "#   model_path=WEIGHTS_PATH,  # path to GGUF file\n",
    "#   n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "#   n_threads=4, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "#   n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    "#   use_mlock=True, # Whether to use mlock to lock the memory in RAM, preventing it from being swapped to disk. This is useful for large models that don't fit in RAM.\n",
    "#   use_mmap=False, #\n",
    "# )\n",
    "\n",
    "def get_context_prompt(observation: KaggleObservation) -> str:\n",
    "  questions = observation.questions\n",
    "  answers = observation.answers\n",
    "\n",
    "  history_prompt = \"\"\n",
    "  for index in range(len(max(questions, answers))):\n",
    "    history_prompt += f\"<|user|>\\n{questions[index]}<|end|>\\n\" if index < len(questions) else \"\"\n",
    "    history_prompt += f\"<|assistant|>\\n{answers[index]}<|end|>\\n\" if index < len(answers) else \"\"\n",
    "  #history_prompt += \"<|assistant|>\\n\"\n",
    "  \n",
    "  return history_prompt\n",
    "\n",
    "def get_guesser_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nLet's play 20 Questions. You are playing the role of the {observation.role.title()}.<|end|>\\n\"\n",
    "  prompt += get_context_prompt(observation)\n",
    "\n",
    "  if observation.turnType == \"ask\":\n",
    "    prompt += f\"<|user|>\\nTake a break, and ask a short yes-or-no question that would be useful to determine what the city I'm thinking about. Previous questions have been listed above. KEEP YOUR QUESTION ONE SENTENCE ONLY! Do not add any explaination to why you chose the question.<|end|>\\n\"\n",
    "  elif observation.turnType == \"guess\":\n",
    "    prompt += f\"<|user|>\\nNow, based on the information above, guess what city I'm thinking about, \" \n",
    "    prompt += f\"which aren't these: {', '.join(observation.guesses)}.\"\n",
    "    prompt += f\"Now, Make an informed guess, and only provide one word!<|end|>\\n\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid turnType: {observation.turnType}\\n\\n{observation}\")\n",
    "  \n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "def get_answerer_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nYou are a highly experienced tour guide specialized in the city {', '.join(observation.keyword.split(' '))}.\\n\"\n",
    "  prompt += \"You must answer a question about this city accurately, but only using the word **yes** or **no**.<|end|>\\n\"\n",
    "\n",
    "  prompt += f\"<|user|>{observation.questions[-1]}<|end|>\\n\"\n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "def play(obs, conf):\n",
    "  print(\"Observation: \" + json.dumps(obs, indent=2, ensure_ascii=False))\n",
    "  print(\"Confing: \" + json.dumps(conf, indent=2, ensure_ascii=False))\n",
    "  observation = KaggleObservation(**obs)\n",
    "  config = KaggleConfig(**conf)\n",
    "  if observation.role == \"guesser\":\n",
    "    prompt = get_guesser_prompt(observation)\n",
    "    result = ask(prompt, max_new_tokens=40).split(\"\\n\")[0].strip()#, stop=[\"<|end|>\"], max_tokens=256, temperature=0.5, echo=False)\n",
    "  elif observation.role == \"answerer\":\n",
    "    prompt = get_answerer_prompt(observation)\n",
    "    answer = ask(prompt, max_new_tokens=20)#, stop=[\"<|end|>\"], max_tokens=20, temperature=0.5, echo=False)\n",
    "    result = \"no\" if \"no\" in answer else \"yes\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid role: {observation.role}\\n\\n{observation}\")\n",
    "  print(f\"Result: {result}\")\n",
    "  return result\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from typing import Literal, List\n",
    "import os, sys, json\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    os.chdir(os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    #WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    os.chdir(\"/kaggle/working/submission/lib\")\n",
    "    #WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "    \n",
    "print(f\"現在のディレクトリ: {os.getcwd()}. \\nこのディレクトリ内のファイル: {', '.join(os.listdir())}\")\n",
    "\n",
    "# モデルのインポート\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,\n",
    "    cache_dir=\"./huggingface\"\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=\"./huggingface\")\n",
    "hf_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask(prompt: str, max_new_tokens=100) -> str:\n",
    "    result = hf_llm(text_inputs=prompt, return_full_text=False, temperature=0.2, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "assert ask(\"<|user|>\\nHello!<|end|>\\n<|assistant|>\")\n",
    "\n",
    "@dataclass\n",
    "class KaggleObservation:\n",
    "  remainingOverageTime: int | float\n",
    "  step: int\n",
    "\n",
    "  questions: list[str]\n",
    "  answers: list[str]\n",
    "  guesses: list[str]\n",
    "\n",
    "  role: Literal[\"guesser\", \"answerer\"]\n",
    "  turnType: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "\n",
    "  keyword: str\n",
    "  category: str\n",
    "\n",
    "@dataclass\n",
    "class KaggleConfig:\n",
    "  episodeSteps: int\n",
    "  actTimeout: int | float\n",
    "  runTimeout: int | float\n",
    "  agentTimeout: int | float\n",
    "  __raw_path__: str\n",
    "\n",
    "# llm = Llama(\n",
    "#   model_path=WEIGHTS_PATH,  # GGUFファイルへのパス\n",
    "#   n_ctx=2048,  # 使用する最大シーケンス長 - 長いシーケンスは多くのリソースを必要とします\n",
    "#   n_threads=4, # 使用するCPUスレッドの数、システムと結果のパフォーマンスに合わせて調整\n",
    "#   n_gpu_layers=35, # GPUにオフロードする層の数、GPUアクセラレーションが利用可能な場合は設定。利用できない場合は0に設定。\n",
    "#   use_mlock=True, # RAM内のメモリをロックし、ディスクにスワップされないようにするかどうか。 RAMに収まらない大きなモデルに有用。\n",
    "#   use_mmap=False, #\n",
    "# )\n",
    "\n",
    "def get_context_prompt(observation: KaggleObservation) -> str:\n",
    "  questions = observation.questions\n",
    "  answers = observation.answers\n",
    "\n",
    "  history_prompt = \"\"\n",
    "  for index in range(len(max(questions, answers))):\n",
    "    history_prompt += f\"<|user|>\\n{questions[index]}<|end|>\\n\" if index < len(questions) else \"\"\n",
    "    history_prompt += f\"<|assistant|>\\n{answers[index]}<|end|>\\n\" if index < len(answers) else \"\"\n",
    "  #history_prompt += \"<|assistant|>\\n\"\n",
    "  \n",
    "  return history_prompt\n",
    "\n",
    "def get_guesser_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\n20の質問をしましょう。あなたは{observation.role.title()}の役割を果たしています。<|end|>\\n\"\n",
    "  prompt += get_context_prompt(observation)\n",
    "\n",
    "  if observation.turnType == \"ask\":\n",
    "    prompt += f\"<|user|>\\n休憩して、私が考えている都市を特定するために役立つ短いはい/いいえの質問を一つ聞いてください。前の質問は上にリストされています。質問は一文だけでお願いします！ 質問の理由を追加しないでください。<|end|>\\n\"\n",
    "  elif observation.turnType == \"guess\":\n",
    "    prompt += f\"<|user|>\\n上の情報に基づいて、私が考えている都市を当ててください。\" \n",
    "    prompt += f\"これらの都市を除いて: {', '.join(observation.guesses)}。\"\n",
    "    prompt += f\"十分な情報を元に、一言で答えてください！<|end|>\\n\"\n",
    "  else:\n",
    "    raise ValueError(f\"無効なturnType: {observation.turnType}\\n\\n{observation}\")\n",
    "  \n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "def get_answerer_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nあなたは{', '.join(observation.keyword.split(' '))}の都市に特化した非常に経験豊富なツアーガイドです。\\n\"\n",
    "  prompt += \"この都市についての質問には、正確に答えなければなりませんが、**はい**または**いいえ**の言葉だけを使ってください。<|end|>\\n\"\n",
    "\n",
    "  prompt += f\"<|user|>{observation.questions[-1]}<|end|>\\n\"\n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "def play(obs, conf):\n",
    "  print(\"観察: \" + json.dumps(obs, indent=2, ensure_ascii=False))\n",
    "  print(\"設定: \" + json.dumps(conf, indent=2, ensure_ascii=False))\n",
    "  observation = KaggleObservation(**obs)\n",
    "  config = KaggleConfig(**conf)\n",
    "  if observation.role == \"guesser\":\n",
    "    prompt = get_guesser_prompt(observation)\n",
    "    result = ask(prompt, max_new_tokens=40).split(\"\\n\")[0].strip()#, stop=[\"<|end|>\"], max_tokens=256, temperature=0.5, echo=False)\n",
    "  elif observation.role == \"answerer\":\n",
    "    prompt = get_answerer_prompt(observation)\n",
    "    answer = ask(prompt, max_new_tokens=20)#, stop=[\"<|end|>\"], max_tokens=20, temperature=0.5, echo=False)\n",
    "    result = \"no\" if \"no\" in answer else \"yes\"\n",
    "  else:\n",
    "    raise ValueError(f\"無効な役割: {observation.role}\\n\\n{observation}\")\n",
    "  print(f\"結果: {result}\")\n",
    "  return result\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:16.018556Z",
     "iopub.status.busy": "2024-05-19T03:34:16.018251Z",
     "iopub.status.idle": "2024-05-19T03:34:16.035456Z",
     "shell.execute_reply": "2024-05-19T03:34:16.034339Z",
     "shell.execute_reply.started": "2024-05-19T03:34:16.018528Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from typing import Literal, List\n",
    "import os, sys, json\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    os.chdir(os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    #WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    os.chdir(\"/kaggle/working/submission/lib\")\n",
    "    #WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "    \n",
    "print(f\"現在のディレクトリ: {os.getcwd()}. \\nこのディレクトリ内のファイル: {', '.join(os.listdir())}\")\n",
    "\n",
    "# モデルのインポート\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,\n",
    "    cache_dir=\"./huggingface\"\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=\"./huggingface\")\n",
    "hf_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask(prompt: str, max_new_tokens=100) -> str:\n",
    "    result = hf_llm(text_inputs=prompt, return_full_text=False, temperature=0.2, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "assert ask(\"<|user|>\\nHello!<|end|>\\n<|assistant|>\")\n",
    "\n",
    "@dataclass\n",
    "class KaggleObservation:\n",
    "  remainingOverageTime: int | float\n",
    "  step: int\n",
    "\n",
    "  questions: list[str]\n",
    "  answers: list[str]\n",
    "  guesses: list[str]\n",
    "\n",
    "  role: Literal[\"guesser\", \"answerer\"]\n",
    "  turnType: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "\n",
    "  keyword: str\n",
    "  category: str\n",
    "\n",
    "@dataclass\n",
    "class KaggleConfig:\n",
    "  episodeSteps: int\n",
    "  actTimeout: int | float\n",
    "  runTimeout: int | float\n",
    "  agentTimeout: int | float\n",
    "  __raw_path__: str\n",
    "\n",
    "# llm = Llama(\n",
    "#   model_path=WEIGHTS_PATH,  # GGUFファイルへのパス\n",
    "#   n_ctx=2048,  # 使用する最大シーケンス長 - 長いシーケンスは多くのリソースを必要とします\n",
    "#   n_threads=4, # 使用するCPUスレッドの数、システムと結果のパフォーマンスに合わせて調整\n",
    "#   n_gpu_layers=35, # GPUにオフロードする層の数、GPUアクセラレーションが利用可能な場合は設定。利用できない場合は0に設定。\n",
    "#   use_mlock=True, # RAM内のメモリをロックし、ディスクにスワップされないようにするかどうか。 RAMに収まらない大きなモデルに有用。\n",
    "#   use_mmap=False, #\n",
    "# )\n",
    "\n",
    "def get_context_prompt(observation: KaggleObservation) -> str:\n",
    "  questions = observation.questions\n",
    "  answers = observation.answers\n",
    "\n",
    "  history_prompt = \"\"\n",
    "  for index in range(len(max(questions, answers))):\n",
    "    history_prompt += f\"<|user|>\\n{questions[index]}<|end|>\\n\" if index < len(questions) else \"\"\n",
    "    history_prompt += f\"<|assistant|>\\n{answers[index]}<|end|>\\n\" if index < len(answers) else \"\"\n",
    "  #history_prompt += \"<|assistant|>\\n\"\n",
    "  \n",
    "  return history_prompt\n",
    "\n",
    "def get_guesser_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\n20の質問をしましょう。あなたは{observation.role.title()}の役割を果たしています。<|end|>\\n\"\n",
    "  prompt += get_context_prompt(observation)\n",
    "\n",
    "  if observation.turnType == \"ask\":\n",
    "    prompt += f\"<|user|>\\n休憩して、私が考えている都市を特定するために役立つ短いはい/いいえの質問を一つ聞いてください。前の質問は上にリストされています。質問は一文だけでお願いします！ 質問の理由を追加しないでください。<|end|>\\n\"\n",
    "  elif observation.turnType == \"guess\":\n",
    "    prompt += f\"<|user|>\\n上の情報に基づいて、私が考えている都市を当ててください。\" \n",
    "    prompt += f\"これらの都市を除いて: {', '.join(observation.guesses)}。\"\n",
    "    prompt += f\"十分な情報を元に、一言で答えてください！<|end|>\\n\"\n",
    "  else:\n",
    "    raise ValueError(f\"無効なturnType: {observation.turnType}\\n\\n{observation}\")\n",
    "  \n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "def get_answerer_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nあなたは{', '.join(observation.keyword.split(' '))}の都市に特化した非常に経験豊富なツアーガイドです。\\n\"\n",
    "  prompt += \"この都市についての質問には、正確に答えなければなりませんが、**はい**または**いいえ**の言葉だけを使ってください。<|end|>\\n\"\n",
    "\n",
    "  prompt += f\"<|user|>{observation.questions[-1]}<|end|>\\n\"\n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "def play(obs, conf):\n",
    "  print(\"観察: \" + json.dumps(obs, indent=2, ensure_ascii=False))\n",
    "  print(\"設定: \" + json.dumps(conf, indent=2, ensure_ascii=False))\n",
    "  observation = KaggleObservation(**obs)\n",
    "  config = KaggleConfig(**conf)\n",
    "  if observation.role == \"guesser\":\n",
    "    prompt = get_guesser_prompt(observation)\n",
    "    result = ask(prompt, max_new_tokens=40).split(\"\\n\")[0].strip()#, stop=[\"<|end|>\"], max_tokens=256, temperature=0.5, echo=False)\n",
    "  elif observation.role == \"answerer\":\n",
    "    prompt = get_answerer_prompt(observation)\n",
    "    answer = ask(prompt, max_new_tokens=20)#, stop=[\"<|end|>\"], max_tokens=20, temperature=0.5, echo=False)\n",
    "    result = \"no\" if \"no\" in answer else \"yes\"\n",
    "  else:\n",
    "    raise ValueError(f\"無効な役割: {observation.role}\\n\\n{observation}\")\n",
    "  print(f\"結果: {result}\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b1952",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Just checkin\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 確認中\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62af81db",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from submission.main import *\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from submission.main import *\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:19.709658Z",
     "iopub.status.busy": "2024-05-19T03:34:19.708926Z",
     "iopub.status.idle": "2024-05-19T03:34:35.618916Z",
     "shell.execute_reply": "2024-05-19T03:34:35.617816Z",
     "shell.execute_reply.started": "2024-05-19T03:34:19.709623Z"
    }
   },
   "outputs": [],
   "source": [
    "from submission.main import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10627918",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'guesser', \n",
    "  'turnType': 'ask', \n",
    "  'keyword': '', #eg. bangkok\n",
    "  'category': '', #eg. city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'guesser', \n",
    "  'turnType': 'ask', \n",
    "  'keyword': '', #例: バンコク\n",
    "  'category': '', #例: 都市\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:35.621549Z",
     "iopub.status.busy": "2024-05-19T03:34:35.620903Z",
     "iopub.status.idle": "2024-05-19T03:34:37.664916Z",
     "shell.execute_reply": "2024-05-19T03:34:37.663875Z",
     "shell.execute_reply.started": "2024-05-19T03:34:35.621517Z"
    }
   },
   "outputs": [],
   "source": [
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'guesser', \n",
    "  'turnType': 'ask', \n",
    "  'keyword': '', #例: バンコク\n",
    "  'category': '', #例: 都市\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70230c4a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [\"Is the city you're thinking of located in North America?\"], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'answerer', \n",
    "  'turnType': 'answer', \n",
    "  'keyword': '', #eg. bangkok\n",
    "  'category': '', #eg. city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [\"あなたが考えている都市は北アメリカにありますか？\"], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'answerer', \n",
    "  'turnType': 'answer', \n",
    "  'keyword': '', #例: バンコク\n",
    "  'category': '', #例: 都市\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:37.666437Z",
     "iopub.status.busy": "2024-05-19T03:34:37.666159Z",
     "iopub.status.idle": "2024-05-19T03:34:38.716761Z",
     "shell.execute_reply": "2024-05-19T03:34:38.71582Z",
     "shell.execute_reply.started": "2024-05-19T03:34:37.666414Z"
    }
   },
   "outputs": [],
   "source": [
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [\"あなたが考えている都市は北アメリカにありますか？\"], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'answerer', \n",
    "  'turnType': 'answer', \n",
    "  'keyword': '', #例: バンコク\n",
    "  'category': '', #例: 都市\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbf3d4",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Archiving the directory into a tar.gz to submit\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## tar.gz形式でディレクトリをアーカイブして提出する\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ba707c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9062a1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%cd /kaggle/working/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%cd /kaggle/working/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98615c1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission .\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3a69e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(\"Success.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "print(\"成功しました。\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"成功しました。\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
