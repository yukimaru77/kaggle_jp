{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48e92fab",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションのためにエージェントを作成するプロセスを示しています。目指すのは、20の質問ゲームをプレイする能力を持つ言語モデル（LLM）を開発することです。このノートブックでは、特に質問者と回答者の役割を担うエージェントを実装しています。\n",
    "\n",
    "コーディングの初めに、必要なライブラリ（`immutabledict`や`sentencepiece`）のインストール、Gemma（Googleが開発したPyTorchの言語モデルフレームワーク）リポジトリのクローン、必要な構成ファイルの準備を行います。その後、`main.py`というスクリプトでエージェントのクラス構造を作成しています。\n",
    "\n",
    "主な手法として、以下の要素が挙げられます。\n",
    "\n",
    "1. **GemmaFormatter**: プロンプトのフォーマットを管理するクラス。ユーザーの質問とモデルの応答を整理する役割を果たします。\n",
    "2. **GemmaAgent**: 基本的なエージェントクラスで、LLMと対話し、その応答を処理する機能を提供します。このクラスは質問を生成し、応答を解析するためのメソッドを持っています。\n",
    "3. **GemmaQuestionerAgent**と**GemmaAnswererAgent**: 質問者と回答者の具体的なエージェントクラスで、それぞれの役割に特化した応答処理を行います。質問者はゲームにおける質問を生成し、回答者は与えられた質問に対して「はい」または「いいえ」で応答します。\n",
    "\n",
    "使用している主なライブラリは以下の通りです：\n",
    "- **PyTorch**: モデルの構築とトレーニングに利用。\n",
    "- **Gemma**: 言語モデルのための構成やクラスを含むフレームワークとして使われています。\n",
    "\n",
    "最終的に、このノートブックを実行することで、タスクを実行するために必要な`submission.tar.gz`ファイルが生成され、Kaggleのコンペティションに直接提出できるようになります。これにより、LLMが20の質問ゲームをどのように戦略的にプレイし、推論を行う能力を持てるかが実証されることを目指しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9e0701",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookに記載されている内容に基づく、初心者がつまずきそうな専門用語の解説です。特に、本ノートブック特有のドメイン知識や実務経験が求められる用語を中心にしています。\n",
    "\n",
    "1. **エージェント (Agent)**:\n",
    "   - 環境内で行動や意思決定を行う自律的なプログラムやシステム。ここでは「質問者エージェント」と「回答者エージェント」があり、それぞれ異なる役割を担っている。\n",
    "\n",
    "2. **プロンプト (Prompt)**:\n",
    "   - 言語モデルに与える入力文。質問や指示を含むテキストで、モデルの出力を誘導するために使用される。特に、モデルの応答のコンテキストを設定するために重要。\n",
    "\n",
    "3. **few-shot examples**:\n",
    "   - 言語モデルが新しいタスクを学習する際に、非常に少量の例（ショット）を用いてそのタスクを遂行する能力。特に、事前訓練されたモデルに追加学習を行わずに新しいタスクに対応する際に使われる。\n",
    "\n",
    "4. **Causal Language Model (因果言語モデル)**:\n",
    "   - 文脈に依存した次の単語を生成することに特化したモデル。ここではGemmaForCausalLMがその役割を果たす。文を逐次生成する際に、前の単語を考慮して次の単語を決定することが特徴。\n",
    "\n",
    "5. **でフォルトのtorch dtype (Default torch dtype)**:\n",
    "   - PyTorchにおけるデフォルトで使用されるデータ型。異なる精度（例えばfloat32、float64など）を扱う際に重要であり、メモリ使用量やモデルのパフォーマンスに影響を与える。\n",
    "\n",
    "6. **Kaggle Simulations (Kaggleシミュレーション)**:\n",
    "   - Kaggleプラットフォーム上でコンペティションを実行するための特別な環境。ここでは、コンペティションのエージェントを実行するためのパスが設定されている。\n",
    "\n",
    "7. **Interleave (インターリーブ)**:\n",
    "   - 2つの異なるシーケンスを交互に組み合わせる処理。ここでは、質問と答えを組み合わせるために使われている。\n",
    "\n",
    "8. **Tokenizer (トークナイザー)**:\n",
    "   - テキストを処理してモデルに入力できる形式（トークン）に変換する仕組み。言語モデルが理解できる形式にテキストを変換するため不可欠。\n",
    "\n",
    "9. **Max new tokens (最大新規トークン)**:\n",
    "   - モデルが生成する応答の長さを制限するためのパラメータ。過剰なトークン生成を防ぎ、モデルの出力を短く保つために使用される。\n",
    "\n",
    "10. **Top-p sampling (トップpサンプリング)**:\n",
    "    - 確率分布の上位p%に基づいて次のトークンを選択する手法。これにより、多様な出力が得られる。\n",
    "\n",
    "これらの用語を理解することで、ノートブックの内容や機械学習・深層学習での実務におけるエージェント設計の重要な概念が明らかになります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55533a8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# This notebook illustrates the agent creation process for the LLM 20 Questions competition.\n",
    "# Running this notebook produces a submission.tar.gz file, which you can submit directly.\n",
    "\n",
    "%%bash\n",
    "# Set up the working environment\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "%%writefile submission/main.py\n",
    "# Main script for the agent\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# Setup system path for both notebook and simulation environments\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "# Define constants\n",
    "WEIGHTS_PATH = os.path.join(\n",
    "    KAGGLE_AGENT_PATH if os.path.exists(KAGGLE_AGENT_PATH) else \"/kaggle/input\",\n",
    "    \"gemma/pytorch/7b-it-quant/2\"\n",
    ")\n",
    "\n",
    "# Prompt Formatting class\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "# Agent class definitions\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        return self._parse_response(response, obs)\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        sampler_kwargs = sampler_kwargs or {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            return match.group() if match else \"Is it a person?\"\n",
    "        elif obs.turnType == 'guess':\n",
    "            return self._parse_keyword(response)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give a yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "# Global agent variable to avoid loading both agents simultaneously\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "\n",
    "    if agent is None:\n",
    "        if name == 'questioner':\n",
    "            agent = GemmaQuestionerAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "        elif name == 'answerer':\n",
    "            agent = GemmaAnswererAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType in [\"ask\", \"guess\"]:\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    return response if response and len(response) > 1 else \"yes\"\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# このノートブックは、LLM 20 Questionsコンペティションのエージェント作成プロセスを示しています。\n",
    "# このノートブックを実行すると、直接提出できるsubmission.tar.gzファイルが生成されます。\n",
    "\n",
    "%%bash\n",
    "# 作業環境をセットアップします\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "%%writefile submission/main.py\n",
    "# エージェントのためのメインスクリプト\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# ノートブックとシミュレーション環境のシステムパスをセットアップします\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "# 定数を定義します\n",
    "WEIGHTS_PATH = os.path.join(\n",
    "    KAGGLE_AGENT_PATH if os.path.exists(KAGGLE_AGENT_PATH) else \"/kaggle/input\",\n",
    "    \"gemma/pytorch/7b-it-quant/2\"\n",
    ")\n",
    "\n",
    "# プロンプトフォーマット用のクラス\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "# エージェントクラスの定義\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたデータ型にデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        return self._parse_response(response, obs)\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        sampler_kwargs = sampler_kwargs or {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はい・いいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"次はキーワードを推測してください。推測を二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            return match.group() if match else \"それは人ですか？\"\n",
    "        elif obs.turnType == 'guess':\n",
    "            return self._parse_keyword(response)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。はい・いいえで答え、答えを二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# エージェント作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはい・いいえの質問に応答します。キーワードは特定の人物、場所、または物です。\"\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "# 並行して両方のエージェントを読み込まないためのグローバルエージェント変数\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "\n",
    "    if agent is None:\n",
    "        if name == 'questioner':\n",
    "            agent = GemmaQuestionerAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "        elif name == 'answerer':\n",
    "            agent = GemmaAnswererAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType in [\"ask\", \"guess\"]:\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    return response if response and len(response) > 1 else \"はい\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-30T03:19:31.917628Z",
     "iopub.status.busy": "2024-05-30T03:19:31.917171Z",
     "iopub.status.idle": "2024-05-30T03:19:32.027093Z",
     "shell.execute_reply": "2024-05-30T03:19:32.025795Z",
     "shell.execute_reply.started": "2024-05-30T03:19:31.917589Z"
    }
   },
   "outputs": [],
   "source": [
    "# このノートブックは、LLM 20 Questionsコンペティションのエージェント作成プロセスを示しています。\n",
    "# このノートブックを実行すると、直接提出できるsubmission.tar.gzファイルが生成されます。\n",
    "\n",
    "%%bash\n",
    "# 作業環境をセットアップします\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir -p /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "\n",
    "%%writefile submission/main.py\n",
    "# エージェントのためのメインスクリプト\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import contextlib\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# ノートブックとシミュレーション環境のシステムパスをセットアップします\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "# 定数を定義します\n",
    "WEIGHTS_PATH = os.path.join(\n",
    "    KAGGLE_AGENT_PATH if os.path.exists(KAGGLE_AGENT_PATH) else \"/kaggle/input\",\n",
    "    \"gemma/pytorch/7b-it-quant/2\"\n",
    ")\n",
    "\n",
    "# プロンプトフォーマット用のクラス\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "# エージェントクラスの定義\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたデータ型にデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        return self._parse_response(response, obs)\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        sampler_kwargs = sampler_kwargs or {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はい・いいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"次はキーワードを推測してください。推測を二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            return match.group() if match else \"それは人ですか？\"\n",
    "        elif obs.turnType == 'guess':\n",
    "            return self._parse_keyword(response)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。はい・いいえで答え、答えを二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# エージェント作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはい・いいえの質問に応答します。キーワードは特定の人物、場所、または物です。\"\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "# 並行して両方のエージェントを読み込まないためのグローバルエージェント変数\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "\n",
    "    if agent is None:\n",
    "        if name == 'questioner':\n",
    "            agent = GemmaQuestionerAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "        elif name == 'answerer':\n",
    "            agent = GemmaAnswererAgent(\n",
    "                device='cuda:0',\n",
    "                system_prompt=system_prompt,\n",
    "                few_shot_examples=few_shot_examples,\n",
    "            )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType in [\"ask\", \"guess\"]:\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    return response if response and len(response) > 1 else \"はい\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
