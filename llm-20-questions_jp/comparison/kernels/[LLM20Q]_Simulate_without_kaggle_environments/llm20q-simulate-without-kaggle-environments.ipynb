{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c67c32ab",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「20の質問」ゲーム用のエージェントをテストするためのコードを提供しています。具体的には、メモリ制限のために`kaggle_environments`を使用しない方法で、質問者（asker）と回答者（answerer）エージェントを実装しています。\n",
    "\n",
    "### 問題の概要\n",
    "ノートブックは、言語モデルを用いて「20の質問」ゲームをどのようにプレイするかを検証しています。このゲームは、質問者が1つのキーワードを推測するために、回答者から「はい」または「いいえ」で答えられる質問を投げかける形式で進行します。エージェントは、できるだけ少ない質問で正解にたどり着くことを目指します。\n",
    "\n",
    "### 手法とライブラリ\n",
    "1. **ライブラリ**:\n",
    "   - `transformers`: 特に`AutoTokenizer`と`AutoModelForCausalLM`クラスを使用して事前訓練済みの言語モデルを初期化します。\n",
    "   - `torch`: モデルの処理をGPU上で行うために使用します。\n",
    "\n",
    "2. **エージェントの設計**:\n",
    "   - エージェントは、インスタンス`Robot`から構成されています。このクラスは、質問する、応答する、推測するという異なる役割を持つメソッドを提供しています。\n",
    "   - `generate_answer`関数は、トークナイザーを使用してテキストをトークン化し、モデルに渡して出力を生成します。\n",
    "\n",
    "3. **ゲームの進行**:\n",
    "   - `Observation`クラスを使ってゲームの状態（現在のターン、役割、キーワードなど）を管理します。\n",
    "   - 各ラウンドにおいて、エージェントは質問、回答、推測を行い、その結果を表示します。\n",
    "\n",
    "4. **キーワードとカテゴリ**:\n",
    "   - 特定のキーワードとそれに関連するカテゴリをJSONデータから取得し、データフレームに変換します。\n",
    "\n",
    "このノートブック全体を通じて、エージェントは「20の質問」ゲームをプレイする能力を持ち、最終的には与えられたキーワードを推測することに挑戦します。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe1b3f",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連し、機械学習・深層学習の初心者がつまずきそうな専門用語の解説を列挙します。\n",
    "\n",
    "1. **kaggle_environments**:\n",
    "   - Kaggleの環境でエージェントの訓練やテストを行うためのライブラリです。コンペティションのルールや動作をシミュレーションするために用いられます。このノートブックでは、メモリ制限のために使用しないとされています。\n",
    "\n",
    "2. **エージェント**:\n",
    "   - ゲームやシミュレーションにおいて、行動を取る主体のことを指します。この文脈では、質問者と回答者の役割を持つ二つのエージェントを指し、相互に質問と回答を行います。\n",
    "\n",
    "3. **トークナイザー**:\n",
    "   - 自然言語処理(NLP)において、テキストデータをトークンと呼ばれる単位に分割するツールです。このトークンは通常、単語やサブワードの単位を持ち、モデルが理解できる形に変換する役割を果たします。\n",
    "\n",
    "4. **モデルID**:\n",
    "   - 特定の事前学習済みモデルを指し、モデルのパラメータやアーキテクチャを特定するための識別子です。このノートブックでは、エージェントが使用するモデルを指定しています。\n",
    "\n",
    "5. **CUDA**:\n",
    "   - NVIDIAのGPUを用いて計算を行うための API で、高速化のために利用されます。Deep Learningの訓練や推論を効率化するために用いられます。\n",
    "\n",
    "6. **メモリ効率的なSDP (Stochastic Dynamic Programming)**:\n",
    "   - モデルのメモリ使用量を削減するための手法です。深層学習において、大きなモデルを扱う際に重要な役割を果たします。この特定の文脈では、CUDAバックエンドで動作するメモリ効率的なアルゴリズムを無効にしています。\n",
    "\n",
    "7. **torch_dtype**:\n",
    "   - PyTorchにおいて、テンソルのデータ型を指定するための引数です。このノートブックでは、メモリ効率向上のために `torch.bfloat16` 型が指定されています。\n",
    "\n",
    "8. **特別なトークン (EOTトークン)**:\n",
    "   - モデルがトークンの生成を制御するために使用される特別なシンボルです。ここでは、会話の終了を示すために利用されています。\n",
    "\n",
    "9. **出力生成 (generate_answer)**:\n",
    "   - モデルが入力情報に基づいて出力を生成するプロセスです。この関数は、ユーザーの質問に対するモデルの回答を生成し、適切な形式にデコードします。\n",
    "\n",
    "10. **状態 (obs)**:\n",
    "    - ゲームの進行状況やエージェントが参照する情報が含まれたオブジェクトです。このノートブックでは、質問、回答、推測などの情報を含むクラス`Observation`として定義されています。\n",
    "\n",
    "11. **JSONデータ**:\n",
    "    - JavaScript Object Notationの略で、データを構造化するためのフォーマットです。このノートブックでは、キーワードやそのカテゴリに関する情報の保存とアクセスに使われています。\n",
    "\n",
    "12. **データフレーム (DataFrame)**:\n",
    "    - 複数のデータを表形式で扱うためのデータ構造で、Pandasライブラリの特徴的な構成です。このノートブックでは、キーワード、カテゴリ、同義語を整理するために使用されています。\n",
    "\n",
    "初心者の方は、専門用語の理解を深めることで、コードの動作や目的を把握しやすくなるでしょう。このような用語に慣れていくことで、機械学習や深層学習の文脈における理解が進むはずです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad57d60",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# You can test your agent without using kaggle_environments\n",
    "Testing using kaggle_environments cannot be done due to memory limitations.\n",
    "Therefore, I created a code that can test the Questioner and Answerer agents.  \n",
    "\n",
    "The agent was created based on the following:  \n",
    "[llama3-8b- LLM20 Questions [LB 750]](https://www.kaggle.com/code/wouldyoujustfocus/llama3-8b-llm20-questions-lb-750)\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# kaggle_environmentsを使用せずにエージェントをテストすることができます\n",
    "kaggle_environmentsを使用することはメモリの制限によりできません。\n",
    "そのため、質問者および回答者エージェントをテストするためのコードを作成しました。  \n",
    "\n",
    "このエージェントは以下に基づいて作成されました：  \n",
    "[llama3-8b- LLM20 Questions [LB 750]](https://www.kaggle.com/code/wouldyoujustfocus/llama3-8b-llm20-questions-lb-750)\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b688b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.837338Z",
     "iopub.status.busy": "2024-07-08T00:33:16.836975Z",
     "iopub.status.idle": "2024-07-08T00:33:16.860956Z",
     "shell.execute_reply": "2024-07-08T00:33:16.860154Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.837309Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0d67ac",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()\n",
    "    start_gen = inp_ids.input_ids.shape[1]\n",
    "    out_ids = out_ids[start_gen:]\n",
    "    if id_eot in out_ids:\n",
    "        stop = out_ids.tolist().index(id_eot)\n",
    "        out = tokenizer.decode(out_ids[:stop])\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)\n",
    "    return out\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            #launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            #launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"\n",
    "            else:\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            #launch the guesser role\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        You are an AI assistant designed to play the 20 Questions game. \n",
    "        In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner.\n",
    "        The keyword is a specific place, or thing.\\n\n",
    "        \"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            to help you, here's an example of how it should work assuming that the keyword is Morocco:\n",
    "            examle:\n",
    "            <you: is it a place?\n",
    "            user: yes\n",
    "            you: is it in europe?\n",
    "            user: no\n",
    "            you: is it in africa?\n",
    "            user: yes\n",
    "            you: do most people living there have dark skin?\n",
    "            user: no\n",
    "            you: is it a country name starting by m ?\n",
    "            user: yes\n",
    "            you: is it Morocco?\n",
    "            user: yes.>\n",
    "\n",
    "            the user has chosen the word, ask your first question!\n",
    "            please be short and not verbose, give only one question, no extra word!\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            so far, the current state of the game is as following:\\n{conv}\n",
    "            based on the conversation, can you guess the word, please give only the word, no verbosity around\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"\n",
    "        You are an AI assistant designed to play the 20 Questions game. \n",
    "        In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner.\n",
    "        The keyword is a specific place, or thing.\\n\n",
    "        so make sure you understand the user's question and you understand the keyword you're playig on.\n",
    "        for now the word that the user should guess is: \"{obs.keyword}\", it is of category \"{obs.category}\",\n",
    "        to help you, here's an example of how it should work assuming that the keyword is Morocco in the category \"place\":\n",
    "        examle:\n",
    "        <user: is it a place?\n",
    "        you: yes\n",
    "        user: is it in europe?\n",
    "        you: no\n",
    "        user: is it in africa?\n",
    "        you: yes\n",
    "        user: do most people living there have dark skin?\n",
    "        you: no\n",
    "        user: is it a country name starting by m ?\n",
    "        you: yes\n",
    "        user: is it Morocco?\n",
    "        you: yes.>\n",
    "        \"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率的なSDPを無効にします\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にします\n",
    "\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")  # エージェントパスが存在する場合のモデルID\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # デフォルトのモデルID\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # トークナイザーの初期化\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")  # モデルの初期化\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]  # モデルで使用する特別なトークンのIDを取得\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")  # 入力テンプレートをトークン化してCUDAに転送\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()  # モデルから出力を生成\n",
    "    start_gen = inp_ids.input_ids.shape[1]  # 入力のサイズを取得\n",
    "    out_ids = out_ids[start_gen:]  # 出力から入力のサイズを引く\n",
    "    if id_eot in out_ids:  # 特殊なトークンが出力に含まれる場合\n",
    "        stop = out_ids.tolist().index(id_eot)  # 特殊なトークンのインデックスを取得\n",
    "        out = tokenizer.decode(out_ids[:stop])  # 特殊なトークンまでデコード\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)  # デコード\n",
    "    return out  # 生成された出力を返す\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問者役を起動します\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 回答者役を起動します\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"  # 回答が「はい」の場合\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"  # 回答が「いいえ」の場合\n",
    "            else:\n",
    "                output = \"yes\"  # 不明な場合は「はい」とします\n",
    "        if mode == \"guessing\":\n",
    "            # 推測者役を起動します\n",
    "            output = self.asker(obs)  # 質問者の機能を呼び出します\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。\n",
    "        このゲームでは、回答者がキーワードを考え、質問者がイエスまたはノーの質問に答えます。\n",
    "        キーワードは特定の場所や物です。\\n\n",
    "        \"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            キーワードがモロッコであると仮定した場合、これはどう機能するかの例を示します：\n",
    "            例：\n",
    "            <あなた：これは場所ですか？\n",
    "            ユーザー：はい\n",
    "            あなた：ヨーロッパにありますか？\n",
    "            ユーザー：いいえ\n",
    "            あなた：アフリカにありますか？\n",
    "            ユーザー：はい\n",
    "            あなた：そこに住んでいるほとんどの人は肌が暗いですか？\n",
    "            ユーザー：いいえ\n",
    "            あなた：mで始まる国名ですか？\n",
    "            ユーザー：はい\n",
    "            あなた：それはモロッコですか？\n",
    "            ユーザー：はい。>\n",
    "\n",
    "            ユーザーは単語を選択しました。最初の質問をしてください！\n",
    "            短く簡潔に、1つの質問のみで、余分な言葉は使わないでください！\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"  # チャットのテンプレートを作成\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):  # 質問と回答を連結\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):  # 過去の質問と回答を取得\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            現在のゲームの状態は次のとおりです:\\n{conv}\n",
    "            会話に基づいて、単語を推測してください。余分な説明はしないで、単語のみを答えてください。\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"  # 推測用のテンプレートを作成\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者がイエスまたはノーの質問に答えます。\n",
    "        キーワードは特定の場所や物です。\\n\n",
    "        ユーザーの質問を理解できるようにし、あなたがプレイしているキーワードを理解してください。\n",
    "        現在、ユーザーが推測すべき単語は: \"{obs.keyword}\"、カテゴリは\"{obs.category}\"です。\n",
    "        キーワードがモロッコでカテゴリが「場所」であると仮定した場合、これはどう機能するかの例を示します：\n",
    "        例：\n",
    "        <ユーザー：これは場所ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：ヨーロッパにありますか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：アフリカにありますか？\n",
    "        あなた：はい\n",
    "        ユーザー：そこに住んでいるほとんどの人は肌が暗いですか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：mで始まる国名ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：それはモロッコですか？\n",
    "        あなた：はい。>\n",
    "        \"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"  # チャットのテンプレートを作成\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):  # 各質問と回答を連結\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)  # モデルに送信して出力を生成\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":  # 質問するターン\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":  # 推測するターン\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":  # 回答するターン\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:  # レスポンスが不正な場合\n",
    "        response = \"yes\"  # デフォルトのレスポンスを設定\n",
    "        \n",
    "    return response  # 最終レスポンスを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.863976Z",
     "iopub.status.busy": "2024-07-08T00:33:16.863686Z",
     "iopub.status.idle": "2024-07-08T00:33:16.874365Z",
     "shell.execute_reply": "2024-07-08T00:33:16.873336Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.86395Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率的なSDPを無効にします\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にします\n",
    "\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")  # エージェントパスが存在する場合のモデルID\n",
    "else:\n",
    "    model_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"  # デフォルトのモデルID\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)  # トークナイザーの初期化\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")  # モデルの初期化\n",
    "id_eot = tokenizer.convert_tokens_to_ids([\"<|eot_id|>\"])[0]  # モデルで使用する特別なトークンのIDを取得\n",
    "\n",
    "\n",
    "def generate_answer(template):\n",
    "    inp_ids = tokenizer(template, return_tensors=\"pt\").to(\"cuda\")  # 入力テンプレートをトークン化してCUDAに転送\n",
    "    out_ids = model.generate(**inp_ids,max_new_tokens=15).squeeze()  # モデルから出力を生成\n",
    "    start_gen = inp_ids.input_ids.shape[1]  # 入力のサイズを取得\n",
    "    out_ids = out_ids[start_gen:]  # 出力から入力のサイズを引く\n",
    "    if id_eot in out_ids:  # 特殊なトークンが出力に含まれる場合\n",
    "        stop = out_ids.tolist().index(id_eot)  # 特殊なトークンのインデックスを取得\n",
    "        out = tokenizer.decode(out_ids[:stop])  # 特殊なトークンまでデコード\n",
    "    else:\n",
    "        out = tokenizer.decode(out_ids)  # デコード\n",
    "    return out  # 生成された出力を返す\n",
    "    \n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # 質問者役を起動します\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # 回答者役を起動します\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"  # 回答が「はい」の場合\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"  # 回答が「いいえ」の場合\n",
    "            else:\n",
    "                output = \"yes\"  # 不明な場合は「はい」とします\n",
    "        if mode == \"guessing\":\n",
    "            # 推測者役を起動します\n",
    "            output = self.asker(obs)  # 質問者の機能を呼び出します\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。\n",
    "        このゲームでは、回答者がキーワードを考え、質問者がイエスまたはノーの質問に答えます。\n",
    "        キーワードは特定の場所や物です。\\n\n",
    "        \"\"\"\n",
    "    \n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            キーワードがモロッコであると仮定した場合、これはどう機能するかの例を示します：\n",
    "            例：\n",
    "            <あなた：これは場所ですか？\n",
    "            ユーザー：はい\n",
    "            あなた：ヨーロッパにありますか？\n",
    "            ユーザー：いいえ\n",
    "            あなた：アフリカにありますか？\n",
    "            ユーザー：はい\n",
    "            あなた：そこに住んでいるほとんどの人は肌が暗いですか？\n",
    "            ユーザー：いいえ\n",
    "            あなた：mで始まる国名ですか？\n",
    "            ユーザー：はい\n",
    "            あなた：それはモロッコですか？\n",
    "            ユーザー：はい。>\n",
    "\n",
    "            ユーザーは単語を選択しました。最初の質問をしてください！\n",
    "            短く簡潔に、1つの質問のみで、余分な言葉は使わないでください！\"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{ask_prompt}<|eot_id|>\"\"\"  # チャットのテンプレートを作成\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "            if len(obs.questions)>=1:\n",
    "                for q, a in zip(obs.questions, obs.answers):  # 質問と回答を連結\n",
    "                    chat_template += f\"{q}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                    chat_template += f\"{a}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):  # 過去の質問と回答を取得\n",
    "                conv += f\"\"\"質問: {q}\\n回答: {a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            現在のゲームの状態は次のとおりです:\\n{conv}\n",
    "            会話に基づいて、単語を推測してください。余分な説明はしないで、単語のみを答えてください。\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{guess_prompt}<|eot_id|>\"\"\"  # 推測用のテンプレートを作成\n",
    "            chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "                \n",
    "        output = generate_answer(chat_template)        \n",
    "        return output\n",
    "        \n",
    "        \n",
    "        \n",
    "    def answerer(self, obs):\n",
    "        sys_prompt = f\"\"\"\n",
    "        あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。 \n",
    "        このゲームでは、回答者がキーワードを考え、質問者がイエスまたはノーの質問に答えます。\n",
    "        キーワードは特定の場所や物です。\\n\n",
    "        ユーザーの質問を理解できるようにし、あなたがプレイしているキーワードを理解してください。\n",
    "        現在、ユーザーが推測すべき単語は: \"{obs.keyword}\"、カテゴリは\"{obs.category}\"です。\n",
    "        キーワードがモロッコでカテゴリが「場所」であると仮定した場合、これはどう機能するかの例を示します：\n",
    "        例：\n",
    "        <ユーザー：これは場所ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：ヨーロッパにありますか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：アフリカにありますか？\n",
    "        あなた：はい\n",
    "        ユーザー：そこに住んでいるほとんどの人は肌が暗いですか？\n",
    "        あなた：いいえ\n",
    "        ユーザー：mで始まる国名ですか？\n",
    "        あなた：はい\n",
    "        ユーザー：それはモロッコですか？\n",
    "        あなた：はい。>\n",
    "        \"\"\"\n",
    "        \n",
    "        chat_template = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\n{sys_prompt}<|eot_id|>\"\"\"  # チャットのテンプレートを作成\n",
    "        chat_template += \"<|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "        chat_template += f\"{obs.questions[0]}<|eot_id|>\"\n",
    "        chat_template += \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        if len(obs.answers)>=1:\n",
    "            for q, a in zip(obs.questions[1:], obs.answers):  # 各質問と回答を連結\n",
    "                chat_template += f\"{a}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n",
    "                chat_template += f\"{q}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "        output = generate_answer(chat_template)  # モデルに送信して出力を生成\n",
    "        return output\n",
    "    \n",
    "    \n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":  # 質問するターン\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":  # 推測するターン\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":  # 回答するターン\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:  # レスポンスが不正な場合\n",
    "        response = \"yes\"  # デフォルトのレスポンスを設定\n",
    "        \n",
    "    return response  # 最終レスポンスを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3530bb",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Paste the following code into your code after main.py\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 以下のコードを main.py の後に貼り付けてください\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ced180",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:16.87888Z",
     "iopub.status.busy": "2024-07-08T00:33:16.878588Z",
     "iopub.status.idle": "2024-07-08T00:33:18.283739Z",
     "shell.execute_reply": "2024-07-08T00:33:18.282643Z",
     "shell.execute_reply.started": "2024-07-08T00:33:16.878855Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d06af26",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "from submission.main import agent\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.role = \"guesser\"\n",
    "        self.turnType = \"ask\"\n",
    "        self.keyword = \"Japan\"\n",
    "        self.category = \"country\"\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.guesses = []\n",
    "        \n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    keyword_list = []\n",
    "    category_list = []\n",
    "    alts_list = []\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):\n",
    "            keyword = json_data[i]['words'][j]['keyword']\n",
    "            keyword_list.append(keyword)\n",
    "            category_list.append(json_data[i]['category'])\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])\n",
    "\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n",
    "    data_pd['keyword'] = keyword_list\n",
    "    data_pd['category'] = category_list\n",
    "    data_pd['alts'] = alts_list\n",
    "    \n",
    "    return data_pd\n",
    "    \n",
    "keywords_df = create_keyword_df(KEYWORDS_JSON)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import json\n",
    "import pandas as pd\n",
    "from submission.main import agent\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.role = \"guesser\"  # 最初の役割は推測者\n",
    "        self.turnType = \"ask\"  # 初期ターンは質問\n",
    "        self.keyword = \"Japan\"  # デフォルトのキーワード\n",
    "        self.category = \"country\"  # デフォルトのカテゴリ\n",
    "        self.questions = []  # 質問のリスト\n",
    "        self.answers = []  # 回答のリスト\n",
    "        self.guesses = []  # 推測のリスト\n",
    "        \n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON)  # JSONデータを読み込む\n",
    "\n",
    "    keyword_list = []  # キーワードのリスト\n",
    "    category_list = []  # カテゴリのリスト\n",
    "    alts_list = []  # 同義語リスト\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):  # 各単語をループ\n",
    "            keyword = json_data[i]['words'][j]['keyword']\n",
    "            keyword_list.append(keyword)  # キーワードを追加\n",
    "            category_list.append(json_data[i]['category'])  # カテゴリを追加\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])  # 同義語を追加\n",
    "\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])  # データフレームを作成\n",
    "    data_pd['keyword'] = keyword_list  # キーワードカラム\n",
    "    data_pd['category'] = category_list  # カテゴリカラム\n",
    "    data_pd['alts'] = alts_list  # 同義語カラム\n",
    "    \n",
    "    return data_pd  # データフレームを返す\n",
    "    \n",
    "keywords_df = create_keyword_df(KEYWORDS_JSON)  # データフレームを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:33:18.286668Z",
     "iopub.status.busy": "2024-07-08T00:33:18.286284Z",
     "iopub.status.idle": "2024-07-08T00:37:18.311817Z",
     "shell.execute_reply": "2024-07-08T00:37:18.310968Z",
     "shell.execute_reply.started": "2024-07-08T00:33:18.286628Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from submission.main import agent\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "class Observation:\n",
    "    def __init__(self):\n",
    "        self.step = 0\n",
    "        self.role = \"guesser\"  # 最初の役割は推測者\n",
    "        self.turnType = \"ask\"  # 初期ターンは質問\n",
    "        self.keyword = \"Japan\"  # デフォルトのキーワード\n",
    "        self.category = \"country\"  # デフォルトのカテゴリ\n",
    "        self.questions = []  # 質問のリスト\n",
    "        self.answers = []  # 回答のリスト\n",
    "        self.guesses = []  # 推測のリスト\n",
    "        \n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    json_data = json.loads(KEYWORDS_JSON)  # JSONデータを読み込む\n",
    "\n",
    "    keyword_list = []  # キーワードのリスト\n",
    "    category_list = []  # カテゴリのリスト\n",
    "    alts_list = []  # 同義語リスト\n",
    "\n",
    "    for i in range(len(json_data)):\n",
    "        for j in range(len(json_data[i]['words'])):  # 各単語をループ\n",
    "            keyword = json_data[i]['words'][j]['keyword']\n",
    "            keyword_list.append(keyword)  # キーワードを追加\n",
    "            category_list.append(json_data[i]['category'])  # カテゴリを追加\n",
    "            alts_list.append(json_data[i]['words'][j]['alts'])  # 同義語を追加\n",
    "\n",
    "    data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])  # データフレームを作成\n",
    "    data_pd['keyword'] = keyword_list  # キーワードカラム\n",
    "    data_pd['category'] = category_list  # カテゴリカラム\n",
    "    data_pd['alts'] = alts_list  # 同義語カラム\n",
    "    \n",
    "    return data_pd  # データフレームを返す\n",
    "    \n",
    "keywords_df = create_keyword_df(KEYWORDS_JSON)  # データフレームを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39486c97",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "obs = Observation()\n",
    "cfg = \"_\"\n",
    "\n",
    "sample_df = keywords_df.sample()\n",
    "obs.keyword = sample_df[\"keyword\"].values[0]\n",
    "obs.category = sample_df[\"category\"].values[0]\n",
    "alts_list = sample_df[\"alts\"].values[0]\n",
    "alts_list.append(obs.keyword)\n",
    "\n",
    "print(f\"keyword:{obs.keyword}\")\n",
    "\n",
    "for round in range(20):\n",
    "    obs.step = round+1\n",
    "    \n",
    "    obs.role = \"guesser\"\n",
    "    obs.turnType = \"ask\"\n",
    "    question = agent(obs, cfg)\n",
    "    obs.questions.append(question)\n",
    "    \n",
    "    obs.role = \"answerer\"\n",
    "    obs.turnType = \"answer\"\n",
    "    answer = agent(obs, cfg)\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    obs.role = \"guesser\"\n",
    "    obs.turnType = \"guess\"\n",
    "    guess = agent(obs, cfg)\n",
    "    obs.guesses.append(guess)\n",
    "    \n",
    "    print(f\"round: {round+1}\")\n",
    "    print(f\"question: {question}\")\n",
    "    print(f\"answer: {answer}\")\n",
    "    print(f\"guess: {guess}\")\n",
    "    \n",
    "    if guess in alts_list:\n",
    "        print(\"Win!!\")\n",
    "        break\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "obs = Observation()  # 観察インスタンスの作成\n",
    "cfg = \"_\"\n",
    "\n",
    "sample_df = keywords_df.sample()  # ランダムにサンプルを取得\n",
    "obs.keyword = sample_df[\"keyword\"].values[0]  # サンプルのキーワードを設定\n",
    "obs.category = sample_df[\"category\"].values[0]  # サンプルのカテゴリを設定\n",
    "alts_list = sample_df[\"alts\"].values[0]  # 同義語リストを取得\n",
    "alts_list.append(obs.keyword)  # キーワードを同義語リストに追加\n",
    "\n",
    "print(f\"keyword:{obs.keyword}\")  # キーワードを出力\n",
    "\n",
    "for round in range(20):  # 最大20ラウンドをループ\n",
    "    obs.step = round+1  # ステップを更新\n",
    "    \n",
    "    obs.role = \"guesser\"  # 現在の役割を推測者に設定\n",
    "    obs.turnType = \"ask\"  # 現在のターンを質問に設定\n",
    "    question = agent(obs, cfg)  # 質問を取得\n",
    "    obs.questions.append(question)  # 質問をリストに追加\n",
    "    \n",
    "    obs.role = \"answerer\"  # 現在の役割を回答者に設定\n",
    "    obs.turnType = \"answer\"  # 現在のターンを回答に設定\n",
    "    answer = agent(obs, cfg)  # 回答を取得\n",
    "    obs.answers.append(answer)  # 回答をリストに追加\n",
    "    \n",
    "    obs.role = \"guesser\"  # 再び推測者の役割に戻す\n",
    "    obs.turnType = \"guess\"  # 現在のターンを推測に設定\n",
    "    guess = agent(obs, cfg)  # 推測を取得\n",
    "    obs.guesses.append(guess)  # 推測をリストに追加\n",
    "    \n",
    "    print(f\"round: {round+1}\")  # ラウンド番号を出力\n",
    "    print(f\"question: {question}\")  # 質問を出力\n",
    "    print(f\"answer: {answer}\")  # 回答を出力\n",
    "    print(f\"guess: {guess}\")  # 推測を出力\n",
    "    \n",
    "    if guess in alts_list:  # 推測が同義語リストに含まれる場合\n",
    "        print(\"勝ちました!!\")  # 勝利メッセージを出力\n",
    "        break\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T00:37:18.313992Z",
     "iopub.status.busy": "2024-07-08T00:37:18.313331Z",
     "iopub.status.idle": "2024-07-08T00:48:16.991991Z",
     "shell.execute_reply": "2024-07-08T00:48:16.990969Z",
     "shell.execute_reply.started": "2024-07-08T00:37:18.313956Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = Observation()  # 観察インスタンスの作成\n",
    "cfg = \"_\"\n",
    "\n",
    "sample_df = keywords_df.sample()  # ランダムにサンプルを取得\n",
    "obs.keyword = sample_df[\"keyword\"].values[0]  # サンプルのキーワードを設定\n",
    "obs.category = sample_df[\"category\"].values[0]  # サンプルのカテゴリを設定\n",
    "alts_list = sample_df[\"alts\"].values[0]  # 同義語リストを取得\n",
    "alts_list.append(obs.keyword)  # キーワードを同義語リストに追加\n",
    "\n",
    "print(f\"keyword:{obs.keyword}\")  # キーワードを出力\n",
    "\n",
    "for round in range(20):  # 最大20ラウンドをループ\n",
    "    obs.step = round+1  # ステップを更新\n",
    "    \n",
    "    obs.role = \"guesser\"  # 現在の役割を推測者に設定\n",
    "    obs.turnType = \"ask\"  # 現在のターンを質問に設定\n",
    "    question = agent(obs, cfg)  # 質問を取得\n",
    "    obs.questions.append(question)  # 質問をリストに追加\n",
    "    \n",
    "    obs.role = \"answerer\"  # 現在の役割を回答者に設定\n",
    "    obs.turnType = \"answer\"  # 現在のターンを回答に設定\n",
    "    answer = agent(obs, cfg)  # 回答を取得\n",
    "    obs.answers.append(answer)  # 回答をリストに追加\n",
    "    \n",
    "    obs.role = \"guesser\"  # 再び推測者の役割に戻す\n",
    "    obs.turnType = \"guess\"  # 現在のターンを推測に設定\n",
    "    guess = agent(obs, cfg)  # 推測を取得\n",
    "    obs.guesses.append(guess)  # 推測をリストに追加\n",
    "    \n",
    "    print(f\"round: {round+1}\")  # ラウンド番号を出力\n",
    "    print(f\"question: {question}\")  # 質問を出力\n",
    "    print(f\"answer: {answer}\")  # 回答を出力\n",
    "    print(f\"guess: {guess}\")  # 推測を出力\n",
    "    \n",
    "    if guess in alts_list:  # 推測が同義語リストに含まれる場合\n",
    "        print(\"勝ちました!!\")  # 勝利メッセージを出力\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
