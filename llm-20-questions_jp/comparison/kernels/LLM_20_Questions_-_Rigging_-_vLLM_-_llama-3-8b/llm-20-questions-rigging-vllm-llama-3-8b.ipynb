{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "858020ce",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LLM 20 Questions」コンペティションにおける言語モデルの開発に関連しており、「20の質問」ゲームをプレイするためのエージェントを作成することに取り組んでいます。このノートブックでは、特に以下の問題にアプローチしています：\n",
    "\n",
    "1. **言語モデルの準備とセットアップ**: Hugging Faceからのモデルをダウンロードし、vLLMサーバーを起動して質問、回答、推測を行うための準備をしています。\n",
    "\n",
    "2. **エージェントの設計**: 質問や回答、推測を生成するためのメインロジックを実装しています。最初の質問を推測するためのアルゴリズムや、ゲームの履歴に基づいて次の質問を生成する機能が含まれています。\n",
    "\n",
    "使用されている主なライブラリや手法：\n",
    "- **Hugging Face Hub**: モデルをダウンロードするために使用しています。\n",
    "- **vLLM**: 言語モデルを扱うためのサーバーを提供し、APIサーバーとして機能します。\n",
    "- **Rigging**: 質問生成、回答生成、推測生成のためのロジックを構築するために使用されており、言語モデルとインタラクションします。\n",
    "- **Pydantic**: データモデルのバリデーションに用いられ、観測データや質問のフォーマットを確認するために使用されています。\n",
    "\n",
    "全体として、このノートブックは、言語モデルを活用して効果的に質問を行い、ゲームの進行に合わせて適切な回答と推測を生成するためのフレームワークを提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a3fffa",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語の簡単な解説をリストアップしました。特にこのノートブック固有のドメイン知識や、あまり一般的ではない概念に焦点を当てています。\n",
    "\n",
    "1. **Hugging Face**:\n",
    "   - オープンソースの機械学習ライブラリとモデルをホストするプラットフォームで、特に自然言語処理（NLP）モデルが多い。`huggingface_hub`はこのプラットフォームからモデルをダウンロードするためのライブラリの一つ。\n",
    "\n",
    "2. **Snapshot Download**:\n",
    "   - 特定のモデルの「スナップショット」をダウンロードすること。これは、モデルの特定のバージョンや状態を取得するための手法であり、開発中のコードや変更に影響されない安定した環境を提供する。\n",
    "\n",
    "3. **vLLM**:\n",
    "   - メモリと性能を最適化するために設計された大規模な言語モデルを扱うライブラリ。特に、低レイテンシと高スループットを求める用途に対応している。\n",
    "\n",
    "4. **Subprocess**:\n",
    "   - Pythonで他のプログラムやスクリプトを実行するためのモジュール。新しいプロセスを生成し、それに対して命令を渡したり、その出力を受け取ったりすることができる。\n",
    "\n",
    "5. **Socket Programming**:\n",
    "   - ネットワーク接続を確立するためのプログラミング手法。特定のポートを監視したり、データの受送信を行うために使用される。\n",
    "\n",
    "6. **BaseModel (Pydantic)**:\n",
    "   - Pydanticライブラリにおけるデータモデルの基底クラス。バリデーションや型注釈を簡単に扱えるようにする。`BaseModel`を使うことで、複雑なデータ構造を簡潔に定義できる。\n",
    "\n",
    "7. **Field Validator**:\n",
    "   - Pydanticの機能で、モデルのフィールドに対してカスタムなバリデーションロジックを定義できる。フィールドの値が適切であるかを確認するために使用される。\n",
    "\n",
    "8. **Annotation**:\n",
    "   - Pythonにおける型付けのための機能。特に、変数や関数の引数に対して期待されるデータ型を明示するために使用され、静的解析やIDEによる補完に役立つ。\n",
    "\n",
    "9. **Coroutine**:\n",
    "   - Pythonで非同期プログラミングを行うための関数。通常の関数と異なり、途中で一時的に処理を中断でき、後から再開することができる。この仕組みを通じて、非同期処理や非ブロッキング操作が実現される。\n",
    "\n",
    "10. **Rigging**:\n",
    "    - 複数の生成モデルを統合し、結果をまとめてつなぎ合わせるためのライブラリ。特に複数のAIモデルを連携させて活用する際に役立つ。\n",
    "\n",
    "11. **Eager Execution**:\n",
    "    - TensorFlowや他の機械学習フレームワークにおいて、計算グラフを事前に構築することなく、命令を即座に実行する方式。開発中のデバッグやプロトタイピングを容易にする。\n",
    "\n",
    "12. **Zip_longest**:\n",
    "    - Pythonのitertoolsモジュールにある関数で、複数のイテラブル（リストなど）の要素をまとめてタプルとして生成する。要素数が異なる場合でも、短いものには`fillvalue`で指定した値を埋めて扱うことができる。\n",
    "\n",
    "これらの用語は、初心者にとっては少し馴染みが薄いかもしれないため、理解を深めるのに役立つ情報です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f429365d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Secrets (optional)\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Secrets (optional)\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secrets (optional)\n",
    "\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6a11dc",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Dependencies (uv for speed)\n",
    "\n",
    "!pip install uv\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==1.3.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Dependencies (uv for speed)\n",
    "\n",
    "!pip install uv\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==1.3.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencies (uv for speed)\n",
    "\n",
    "!pip install uv\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==1.3.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98238f5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Download the model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Download the model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)  # モデル用のパスが存在する場合は、そのディレクトリを削除します。\n",
    "g_model_path.mkdir(parents=True)  # モデル用の新しいディレクトリを作成します。\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",  # Hugging Faceからモデルのスナップショットをダウンロードします。\n",
    "    ignore_patterns=\"original*\",  # 'original'というパターンを持つファイルは無視します。\n",
    "    local_dir=g_model_path,  # ダウンロード先のディレクトリを指定します。\n",
    "    local_dir_use_symlinks=False,  # シンボリックリンクは使用しません。\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # 必要なトークンを取得します。\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)  # モデル用のパスが存在する場合は、そのディレクトリを削除します。\n",
    "g_model_path.mkdir(parents=True)  # モデル用の新しいディレクトリを作成します。\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",  # Hugging Faceからモデルのスナップショットをダウンロードします。\n",
    "    ignore_patterns=\"original*\",  # 'original'というパターンを持つファイルは無視します。\n",
    "    local_dir=g_model_path,  # ダウンロード先のディレクトリを指定します。\n",
    "    local_dir_use_symlinks=False,  # シンボリックリンクは使用しません。\n",
    "    token=globals().get(\"HF_TOKEN\", None)  # 必要なトークンを取得します。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d808f4f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile util.py\n",
    "\n",
    "# Helpers for starting the vLLM server\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)\n",
    "            result = sock.connect_ex(('localhost', port))\n",
    "            if result == 0:\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"Port {port} is already open\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},\n",
    "        stdout=subprocess.DEVNULL,\n",
    "        stderr=subprocess.DEVNULL\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen\n",
    "        time.sleep(1)\n",
    "    \n",
    "    popen.terminate()\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile util.py\n",
    "\n",
    "# Helpers for starting the vLLM server\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:  # ポートが開いているか確認するための関数\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:  # ソケットを作成します。\n",
    "            sock.settimeout(1)  # タイムアウトの設定\n",
    "            result = sock.connect_ex(('localhost', port))  # 指定したポートに接続を試みます。\n",
    "            if result == 0:  # 接続成功\n",
    "                return True\n",
    "    except socket.error:  # ソケットエラーの場合\n",
    "        pass\n",
    "    \n",
    "    return False  # ポートが開いていない\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60\n",
    ") -> subprocess.Popen:  # 特定のポートが開くまでプロセスを実行する関数\n",
    "    \n",
    "    if check_port(port):  # 指定したポートがすでに開いている場合\n",
    "        raise ValueError(f\"Port {port} is already open\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},  # 環境変数を設定\n",
    "        stdout=subprocess.DEVNULL,  # 標準出力を無視\n",
    "        stderr=subprocess.DEVNULL  # 標準エラーを無視\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()  # 開始時間を記録\n",
    "    while time.time() - start_time < timeout:  # タイムアウトまで繰り返す\n",
    "        if check_port(port):  # ポートが開いたら\n",
    "            return popen  # プロセスを返す\n",
    "        time.sleep(1)  # 1秒待つ\n",
    "    \n",
    "    popen.terminate()  # タイムアウトした場合、プロセスを終了\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")  # エラーを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# Helpers for starting the vLLM server\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:  # ポートが開いているか確認するための関数\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:  # ソケットを作成します。\n",
    "            sock.settimeout(1)  # タイムアウトの設定\n",
    "            result = sock.connect_ex(('localhost', port))  # 指定したポートに接続を試みます。\n",
    "            if result == 0:  # 接続成功\n",
    "                return True\n",
    "    except socket.error:  # ソケットエラーの場合\n",
    "        pass\n",
    "    \n",
    "    return False  # ポートが開いていない\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60\n",
    ") -> subprocess.Popen:  # 特定のポートが開くまでプロセスを実行する関数\n",
    "    \n",
    "    if check_port(port):  # 指定したポートがすでに開いている場合\n",
    "        raise ValueError(f\"Port {port} is already open\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},  # 環境変数を設定\n",
    "        stdout=subprocess.DEVNULL,  # 標準出力を無視\n",
    "        stderr=subprocess.DEVNULL  # 標準エラーを無視\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()  # 開始時間を記録\n",
    "    while time.time() - start_time < timeout:  # タイムアウトまで繰り返す\n",
    "        if check_port(port):  # ポートが開いたら\n",
    "            return popen  # プロセスを返す\n",
    "        time.sleep(1)  # 1秒待つ\n",
    "    \n",
    "    popen.terminate()  # タイムアウトした場合、プロセスを終了\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")  # エラーを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2944da3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Validate vLLM startup (optional - commented for faster builds)\n",
    "\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import util\n",
    "\n",
    "# util = importlib.reload(util)\n",
    "\n",
    "# g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "# assert g_srvlib_path.exists()\n",
    "\n",
    "# g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# assert g_model_path.exists()\n",
    "\n",
    "# g_vllm_port = 9999\n",
    "# g_vllm_model_name = \"custom\"\n",
    "\n",
    "# # Start vLLM server\n",
    "\n",
    "# vllm = util.run_and_wait_for_port([\n",
    "#     \"python\", \"-m\",\n",
    "#     \"vllm.entrypoints.openai.api_server\",\n",
    "#     \"--enforce-eager\",\n",
    "#     \"--model\", str(g_model_path),\n",
    "#     \"--port\", str(g_vllm_port),\n",
    "#     \"--served-model-name\", g_vllm_model_name\n",
    "# ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "# print(\"vLLM Started\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Validate vLLM startup (optional - commented for faster builds)\n",
    "\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import util\n",
    "\n",
    "# util = importlib.reload(util)\n",
    "\n",
    "# g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "# assert g_srvlib_path.exists()  # srvlib のパスが存在することを確認\n",
    "\n",
    "# g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# assert g_model_path.exists()  # モデルのパスが存在することを確認\n",
    "\n",
    "# g_vllm_port = 9999  # vLLM サーバーが使用するポート番号\n",
    "# g_vllm_model_name = \"custom\"  # 使用するモデル名\n",
    "\n",
    "# # Start vLLM server\n",
    "\n",
    "# vllm = util.run_and_wait_for_port([\n",
    "#     \"python\", \"-m\",\n",
    "#     \"vllm.entrypoints.openai.api_server\",\n",
    "#     \"--enforce-eager\",\n",
    "#     \"--model\", str(g_model_path),  # モデルのパス\n",
    "#     \"--port\", str(g_vllm_port),  # ポート番号\n",
    "#     \"--served-model-name\", g_vllm_model_name  # サーブされるモデル名\n",
    "# ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "# print(\"vLLM Started\")  # vLLMサーバー開始のメッセージ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate vLLM startup (optional - commented for faster builds)\n",
    "\n",
    "# import importlib\n",
    "# from pathlib import Path\n",
    "# import util\n",
    "\n",
    "# util = importlib.reload(util)\n",
    "\n",
    "# g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "# assert g_srvlib_path.exists()  # srvlib のパスが存在することを確認\n",
    "\n",
    "# g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "# assert g_model_path.exists()  # モデルのパスが存在することを確認\n",
    "\n",
    "# g_vllm_port = 9999  # vLLM サーバーが使用するポート番号\n",
    "# g_vllm_model_name = \"custom\"  # 使用するモデル名\n",
    "\n",
    "# # Start vLLM server\n",
    "\n",
    "# vllm = util.run_and_wait_for_port([\n",
    "#     \"python\", \"-m\",\n",
    "#     \"vllm.entrypoints.openai.api_server\",\n",
    "#     \"--enforce-eager\",\n",
    "#     \"--model\", str(g_model_path),  # モデルのパス\n",
    "#     \"--port\", str(g_vllm_port),  # ポート番号\n",
    "#     \"--served-model-name\", g_vllm_model_name  # サーブされるモデル名\n",
    "# ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "# print(\"vLLM Started\")  # vLLMサーバー開始のメッセージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496d3edb",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Connect with Rigging (optional - commented for faster builds)\n",
    "\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "# logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# import rigging as rg\n",
    "\n",
    "# generator = rg.get_generator(\n",
    "#     f\"openai/{g_vllm_model_name},\" \\\n",
    "#     f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "#     \"api_key=sk-1234,\" \\\n",
    "#     \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    "# )\n",
    "# chat = generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "# print(chat.last)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Connect with Rigging (optional - commented for faster builds)\n",
    "\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "# logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログのレベルを設定\n",
    "\n",
    "# import rigging as rg\n",
    "\n",
    "# generator = rg.get_generator(\n",
    "#     f\"openai/{g_vllm_model_name},\" \\\n",
    "#     f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "#     \"api_key=sk-1234,\" \\\n",
    "#     \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    "# )\n",
    "# chat = generator.chat(\"Say Hello!\").run()  # チャットを実行\n",
    "\n",
    "# print(chat.last)  # 最後のチャットの内容を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect with Rigging (optional - commented for faster builds)\n",
    "\n",
    "# import sys\n",
    "# import logging\n",
    "\n",
    "# sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "# logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログのレベルを設定\n",
    "\n",
    "# import rigging as rg\n",
    "\n",
    "# generator = rg.get_generator(\n",
    "#     f\"openai/{g_vllm_model_name},\" \\\n",
    "#     f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "#     \"api_key=sk-1234,\" \\\n",
    "#     \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    "# )\n",
    "# chat = generator.chat(\"Say Hello!\").run()  # チャットを実行\n",
    "\n",
    "# print(chat.last)  # 最後のチャットの内容を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcb597d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile main.py\n",
    "\n",
    "# Main agent file\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Path fixups\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "\n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# Logging noise\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# Fixed imports\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# Constants\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    ")\n",
    "\n",
    "# Types\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, include_guesses: bool = False) -> str:\n",
    "        return \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if include_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "        ) if not self.empty else \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"**thing/place/person**\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "\n",
    "def ask(base: rg.PendingChat, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # override first question until keyword bug is fixed.\n",
    "        return \"Are we playing 20 questions?\"\n",
    "    \n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently asking the next question.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, ask the next most useful yes/no\n",
    "            question and place it in the following format:\n",
    "            {Question.xml_example()}\n",
    "\n",
    "            - Your response should be a focused question which will gather the most information\n",
    "            - Start general with your questions\n",
    "            - Always try to bisect the remaining search space\n",
    "            - Pay attention to previous questions and answers\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your question.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Question, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Question).content\n",
    "\n",
    "\n",
    "def answer(base: rg.PendingChat, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n",
    "        return \"yes\" # override until keyword bug is fixed.\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            The secret word for this game is \"{observation.keyword}\" [{observation.category}]\n",
    "\n",
    "            You are currently answering a question about the word above.\n",
    "\n",
    "            The next question is \"{last_question}\".\n",
    "\n",
    "            Answer the yes/no question above and place it in the following format:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - Your response should be accurate given the keyword above\n",
    "            - Always answer with \"yes\" or \"no\"\n",
    "\n",
    "            What is the answer?\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Answer, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Answer).content\n",
    "\n",
    "\n",
    "def guess(base: rg.PendingChat, observation: Observation) -> str:\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently making an informed guess of the keyword.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, produce a single next best guess\n",
    "            for the keyword and place it in the following format:\n",
    "            {Guess.xml_example()}\n",
    "\n",
    "            - Avoid repeat guesses based on the history above\n",
    "            - The guess should be a specific person, place, or thing\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your guess.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Guess, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "        \n",
    "    return chat.last.parse(Guess).content\n",
    "\n",
    "# vLLM and Generator\n",
    "\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),\n",
    "    \"--port\", str(g_vllm_port),\n",
    "    \"--served-model-name\", g_vllm_model_name\n",
    "], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "print(\"vLLM Started\")\n",
    "\n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Entrypoint\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    \n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "                return ask(base, observation)\n",
    "            case \"answer\":\n",
    "                return answer(base, observation)\n",
    "            case \"guess\":\n",
    "                return guess(base, observation)\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile main.py\n",
    "\n",
    "# Main agent file\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Path fixups\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():  # エージェント専用のパスが存在する場合\n",
    "    g_lib_path = g_agent_path / \"lib\"  # ライブラリパスを更新\n",
    "    g_model_path = g_agent_path / \"model\"  # モデルパスを更新\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"  # srvlibパスを更新\n",
    "\n",
    "sys.path.insert(0, str(g_lib_path))  # ライブラリパスをシステムパスに追加\n",
    "\n",
    "# Logging noise\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログのレベルを設定\n",
    "\n",
    "# Fixed imports\n",
    "\n",
    "import util # noqa  # ユーティリティをインポート\n",
    "import rigging as rg  # noqa  # riggingをインポート\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa  # Pydanticのインポート\n",
    "\n",
    "# Constants\n",
    "\n",
    "g_vllm_port = 9999  # vLLMが使用するポート番号\n",
    "g_vllm_model_name = \"custom\"  # カスタムモデル名\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaモデルには特別な設定が必要です\n",
    ")\n",
    "\n",
    "# Types\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]  # スペースをトリムするためのタイプ\n",
    "\n",
    "class Observation(BaseModel):  # 観測を表すモデル\n",
    "    step: int  # ステップ数\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]  # 役割（推測者または回答者）\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]  # ターンの種類\n",
    "    keyword: str  # キーワード\n",
    "    category: str  # カテゴリ\n",
    "    questions: list[str]  # 質問のリスト\n",
    "    answers: list[str]  # 回答のリスト\n",
    "    guesses: list[str]  # 推測のリスト\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])  # 質問、回答、推測が空かどうかを判定\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")  # 歴史を取得\n",
    "\n",
    "    def get_history_as_xml(self, *, include_guesses: bool = False) -> str:  # 歴史をXML形式で取得\n",
    "        return \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if include_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "        ) if not self.empty else \"none yet.\"  # 空でない場合はXMLを、そうでない場合は「まだなし」と返す\n",
    "\n",
    "\n",
    "class Answer(rg.Model):  # 回答を示すモデル\n",
    "    content: t.Literal[\"yes\", \"no\"]  # 回答は「はい」または「いいえ」\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:  # contentのバリデーション\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):  # 有効な場合\n",
    "                return valid  \n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")  # 無効な場合はエラーを投げる\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"  # XML形式の例\n",
    "\n",
    "\n",
    "class Question(rg.Model):  # 質問を表すモデル\n",
    "    content: str_strip  # 質問内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "class Guess(rg.Model):  # 推測を表すモデル\n",
    "    content: str_strip  # 推測内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"**thing/place/person**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "def ask(base: rg.PendingChat, observation: Observation) -> str:  # 質問を生成する関数\n",
    "    if observation.step == 0:\n",
    "        # override first question until keyword bug is fixed.\n",
    "        return \"Are we playing 20 questions?\"  # ゲームの確認をする質問\n",
    "    \n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently asking the next question.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, ask the next most useful yes/no\n",
    "            question and place it in the following format:\n",
    "            {Question.xml_example()}\n",
    "\n",
    "            - Your response should be a focused question which will gather the most information\n",
    "            - Start general with your questions\n",
    "            - Always try to bisect the remaining search space\n",
    "            - Pay attention to previous questions and answers\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your question.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Question, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Question).content  # 最後の質問内容を返す\n",
    "\n",
    "\n",
    "def answer(base: rg.PendingChat, observation: Observation) -> t.Literal[\"yes\", \"no\"]:  # 回答を生成する関数\n",
    "    if not observation.keyword:  # キーワードが提供されていない場合\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)  # エラーメッセージを表示して\n",
    "        return \"yes\"  # 仮に「はい」と返す（バグ修正まで）\n",
    "            \n",
    "    last_question = observation.questions[-1]  # 最後の質問を取得\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            The secret word for this game is \"{observation.keyword}\" [{observation.category}]\n",
    "\n",
    "            You are currently answering a question about the word above.\n",
    "\n",
    "            The next question is \"{last_question}\".\n",
    "\n",
    "            Answer the yes/no question above and place it in the following format:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - Your response should be accurate given the keyword above\n",
    "            - Always answer with \"yes\" or \"no\"\n",
    "\n",
    "            What is the answer?\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Answer, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Answer).content  # 最後の回答内容を返す\n",
    "\n",
    "\n",
    "def guess(base: rg.PendingChat, observation: Observation) -> str:  # 推測を生成する関数\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently making an informed guess of the keyword.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, produce a single next best guess\n",
    "            for the keyword and place it in the following format:\n",
    "            {Guess.xml_example()}\n",
    "\n",
    "            - Avoid repeat guesses based on the history above\n",
    "            - The guess should be a specific person, place, or thing\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your guess.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Guess, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "        \n",
    "    return chat.last.parse(Guess).content  # 最後の推測内容を返す\n",
    "\n",
    "# vLLM and Generator\n",
    "\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),  # モデルのパスを指定\n",
    "    \"--port\", str(g_vllm_port),  # ポート番号を指定\n",
    "    \"--served-model-name\", g_vllm_model_name  # サーブするモデル名を指定\n",
    "], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})  # vLLMを起動\n",
    "\n",
    "print(\"vLLM Started\")  # vLLM開始メッセージ\n",
    "\n",
    "generator = rg.get_generator(g_generator_id)  # ジェネレーターの取得\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")  # ゲームに関する初期メッセージ\n",
    "\n",
    "# Entrypoint\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:  # エージェントのメイン関数\n",
    "    observation = Observation(**obs.__dict__)  # 観測データをObservationオブジェクトに変換\n",
    "    \n",
    "    try:\n",
    "        match observation.turnType:  # ターンの種類に応じて処理を分岐\n",
    "            case \"ask\":\n",
    "                return ask(base, observation)  # 質問を生成\n",
    "            case \"answer\":\n",
    "                return answer(base, observation)  # 回答を生成\n",
    "            case \"guess\":\n",
    "                return guess(base, observation)  # 推測を生成\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")  # 不明なタイプの場合はエラーを投げる\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)  # エラーメッセージを表示\n",
    "        raise\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# Main agent file\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Path fixups\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():  # エージェント専用のパスが存在する場合\n",
    "    g_lib_path = g_agent_path / \"lib\"  # ライブラリパスを更新\n",
    "    g_model_path = g_agent_path / \"model\"  # モデルパスを更新\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"  # srvlibパスを更新\n",
    "\n",
    "sys.path.insert(0, str(g_lib_path))  # ライブラリパスをシステムパスに追加\n",
    "\n",
    "# Logging noise\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)  # ログのレベルを設定\n",
    "\n",
    "# Fixed imports\n",
    "\n",
    "import util # noqa  # ユーティリティをインポート\n",
    "import rigging as rg  # noqa  # riggingをインポート\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa  # Pydanticのインポート\n",
    "\n",
    "# Constants\n",
    "\n",
    "g_vllm_port = 9999  # vLLMが使用するポート番号\n",
    "g_vllm_model_name = \"custom\"  # カスタムモデル名\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaモデルには特別な設定が必要です\n",
    ")\n",
    "\n",
    "# Types\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]  # スペースをトリムするためのタイプ\n",
    "\n",
    "class Observation(BaseModel):  # 観測を表すモデル\n",
    "    step: int  # ステップ数\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]  # 役割（推測者または回答者）\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]  # ターンの種類\n",
    "    keyword: str  # キーワード\n",
    "    category: str  # カテゴリ\n",
    "    questions: list[str]  # 質問のリスト\n",
    "    answers: list[str]  # 回答のリスト\n",
    "    guesses: list[str]  # 推測のリスト\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])  # 質問、回答、推測が空かどうかを判定\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")  # 歴史を取得\n",
    "\n",
    "    def get_history_as_xml(self, *, include_guesses: bool = False) -> str:  # 歴史をXML形式で取得\n",
    "        return \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if include_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "        ) if not self.empty else \"none yet.\"  # 空でない場合はXMLを、そうでない場合は「まだなし」と返す\n",
    "\n",
    "\n",
    "class Answer(rg.Model):  # 回答を示すモデル\n",
    "    content: t.Literal[\"yes\", \"no\"]  # 回答は「はい」または「いいえ」\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:  # contentのバリデーション\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):  # 有効な場合\n",
    "                return valid  \n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")  # 無効な場合はエラーを投げる\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"  # XML形式の例\n",
    "\n",
    "\n",
    "class Question(rg.Model):  # 質問を表すモデル\n",
    "    content: str_strip  # 質問内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "class Guess(rg.Model):  # 推測を表すモデル\n",
    "    content: str_strip  # 推測内容\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"**thing/place/person**\").to_pretty_xml()  # XML形式の例\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "def ask(base: rg.PendingChat, observation: Observation) -> str:  # 質問を生成する関数\n",
    "    if observation.step == 0:\n",
    "        # override first question until keyword bug is fixed.\n",
    "        return \"Are we playing 20 questions?\"  # ゲームの確認をする質問\n",
    "    \n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently asking the next question.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, ask the next most useful yes/no\n",
    "            question and place it in the following format:\n",
    "            {Question.xml_example()}\n",
    "\n",
    "            - Your response should be a focused question which will gather the most information\n",
    "            - Start general with your questions\n",
    "            - Always try to bisect the remaining search space\n",
    "            - Pay attention to previous questions and answers\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your question.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Question, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Question).content  # 最後の質問内容を返す\n",
    "\n",
    "\n",
    "def answer(base: rg.PendingChat, observation: Observation) -> t.Literal[\"yes\", \"no\"]:  # 回答を生成する関数\n",
    "    if not observation.keyword:  # キーワードが提供されていない場合\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)  # エラーメッセージを表示して\n",
    "        return \"yes\"  # 仮に「はい」と返す（バグ修正まで）\n",
    "            \n",
    "    last_question = observation.questions[-1]  # 最後の質問を取得\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            The secret word for this game is \"{observation.keyword}\" [{observation.category}]\n",
    "\n",
    "            You are currently answering a question about the word above.\n",
    "\n",
    "            The next question is \"{last_question}\".\n",
    "\n",
    "            Answer the yes/no question above and place it in the following format:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - Your response should be accurate given the keyword above\n",
    "            - Always answer with \"yes\" or \"no\"\n",
    "\n",
    "            What is the answer?\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Answer, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "    return chat.last.parse(Answer).content  # 最後の回答内容を返す\n",
    "\n",
    "\n",
    "def guess(base: rg.PendingChat, observation: Observation) -> str:  # 推測を生成する関数\n",
    "    chat = (\n",
    "        base.fork(\n",
    "            f\"\"\"\\\n",
    "            You are currently making an informed guess of the keyword.\n",
    "\n",
    "            <game-history>\n",
    "            {observation.get_history_as_xml()}\n",
    "            </game-history>\n",
    "\n",
    "            Based on the history above, produce a single next best guess\n",
    "            for the keyword and place it in the following format:\n",
    "            {Guess.xml_example()}\n",
    "\n",
    "            - Avoid repeat guesses based on the history above\n",
    "            - The guess should be a specific person, place, or thing\n",
    "\n",
    "            Before you begin, document your analysis of the game history if available,\n",
    "            then write your guess.\n",
    "            \"\"\"\n",
    "        )\n",
    "        .until_parsed_as(Guess, attempt_recovery=True)\n",
    "        .run()\n",
    "    )\n",
    "        \n",
    "    return chat.last.parse(Guess).content  # 最後の推測内容を返す\n",
    "\n",
    "# vLLM and Generator\n",
    "\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),  # モデルのパスを指定\n",
    "    \"--port\", str(g_vllm_port),  # ポート番号を指定\n",
    "    \"--served-model-name\", g_vllm_model_name  # サーブするモデル名を指定\n",
    "], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})  # vLLMを起動\n",
    "\n",
    "print(\"vLLM Started\")  # vLLM開始メッセージ\n",
    "\n",
    "generator = rg.get_generator(g_generator_id)  # ジェネレーターの取得\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")  # ゲームに関する初期メッセージ\n",
    "\n",
    "# Entrypoint\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:  # エージェントのメイン関数\n",
    "    observation = Observation(**obs.__dict__)  # 観測データをObservationオブジェクトに変換\n",
    "    \n",
    "    try:\n",
    "        match observation.turnType:  # ターンの種類に応じて処理を分岐\n",
    "            case \"ask\":\n",
    "                return ask(base, observation)  # 質問を生成\n",
    "            case \"answer\":\n",
    "                return answer(base, observation)  # 回答を生成\n",
    "            case \"guess\":\n",
    "                return guess(base, observation)  # 推測を生成\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")  # 不明なタイプの場合はエラーを投げる\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)  # エラーメッセージを表示\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7391d5a1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv  # pigzおよびpvをインストール\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv  # pigzおよびpvをインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d9bd0e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py  # モデルやスクリプトを圧縮してtarファイルを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py  # モデルやスクリプトを圧縮してtarファイルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe27d2fa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    " KAGGLE_KEY={KAGGLE_KEY} \\\n",
    " kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"Updates\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    " KAGGLE_KEY={KAGGLE_KEY} \\\n",
    " kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"Updates\"  # コンペティションに提出\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    " KAGGLE_KEY={KAGGLE_KEY} \\\n",
    " kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"Updates\"  # コンペティションに提出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3b41ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## OminousDude\n",
    "> \n",
    "> こんにちは、あなたのコードをテストしていたのですが、実行したところ「AttributeError: 'coroutine' object has no attribute 'last'」という例外が発生しました。このエラーは以前に発生しましたか？\n",
    "> \n",
    "> \n",
    "> > ## Rob Mulla\n",
    "> > \n",
    "> > [@max1mum](https://www.kaggle.com/max1mum) - これは、riggingの新しいリリースが以前のバージョンに対していくつかの破壊的変更を加えたためです。\n",
    "> > \n",
    "> > パッケージの固定バージョンを使用してみてください。このようにインストールセルを変更すればうまくいくはずです。\n",
    "> > \n",
    "> > ```\n",
    "> > # Dependencies (uv for speed)\n",
    "> > !pip install uv==0.1.45\n",
    "> > \n",
    "> > !uv pip install -U \\\n",
    "> >     --python $(which python) \\\n",
    "> >     --target /kaggle/tmp/lib \\\n",
    "> >     rigging==1.1.1 \\\n",
    "> >     kaggle\n",
    "> > \n",
    "> > !uv pip install -U \\\n",
    "> >     --python $(which python) \\\n",
    "> >     --target /kaggle/tmp/srvlib \\\n",
    "> >     vllm==0.4.2\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## OminousDude\n",
    "> > > \n",
    "> > > ありがとうございます!!!\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
