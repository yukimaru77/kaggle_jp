{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8789572c",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションのために構築されたシンプルなスタートノートブックです。コンペティションの目標である、言語モデルを用いた「20の質問」ゲームのために、大幅に簡略化されたプロンプトエンジニアリングを迅速に始めるためのフレームワークを提供しています。具体的には、プロンプトは他のより高度なノートブックから取得され、モデルをテストするための部分が設けられています。\n",
    "\n",
    "### 取り組む問題\n",
    "このノートブックは、「20の質問」ゲームにおける質問者エージェントと回答者エージェントの両方を実装し、ゲームの進行において迅速かつ正確に情報を処理する問題に取り組んでいます。特に、推測、質問、回答の各ターンにおいて、適切なプロンプトを生成し、応答を得る能力が重要です。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **Gemmaモデル**: 必要なモデルとして`GemmaForCausalLM`が使用されます。このモデルは、具体的なタスクに最適化された言語モデルであり、質問に対する回答を生成します。\n",
    "2. **Torch**: 深層学習のためのフレームワークであり、モデルの学習と実行に使用されています。\n",
    "3. **ImmutableDictとSentencePiece**: 特定のライブラリがインストールされ、テキスト処理やメモリ管理の効率を高めるために使用されています。\n",
    "\n",
    "### コードの概要\n",
    "ノートブックには、エージェントが使用する異なるメソッド（`ask_agent`, `guess_agent`, `answer_agent`）が定義されており、これらがそれぞれ質問、推測、回答の役割を果たします。プロンプトは、ゲームの進行状況に応じてダイナミックに生成され、LLM（大規模言語モデル）を介して処理され、結果が返されます。\n",
    "\n",
    "ノートブックの後半部分では、エージェントの入力データを手動で構築し、さまざまなターン（質問、推測、回答）を実行するためのテストが行われています。これにより、実際のゲームシナリオでの動作を確認し、必要に応じて改善策を提案できるようになっています。\n",
    "\n",
    "このように、ノートブックは「20の質問」ゲームという特定の問題に対して、言語モデルを活用した効率的な対話システムの基盤を提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c605ab",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に関連する専門用語の簡単な解説です。特に初心者がつまずく可能性があるマイナーな用語や、実務経験がないと馴染みがないものに焦点を当てています。\n",
    "\n",
    "1. **プロンプトエンジニアリング**:\n",
    "   - テキスト入力（プロンプト）を工夫してAIモデルから期待される応答を得る技術。効果的なプロンプトは、モデルの応答の質を大きく改善することができる。\n",
    "\n",
    "2. **immutabledict**:\n",
    "   - イミュータブル（不変）な辞書の実装。辞書の内容を変更できないため、誤ってデータを変更してしまうリスクを減らすことができる。主にデータの整合性を保つために使用される。\n",
    "\n",
    "3. **sentencepiece**:\n",
    "   - サブワードトークナイザーで、テキストをより小さな意味的単位（サブワード）に分解する技術。特に言語モデルの前処理に利用され、新しい単語や形を柔軟に扱うことができる。\n",
    "\n",
    "4. **GemmaForCausalLM**:\n",
    "   - 特定の生成的因果言語モデル（Causal Language Model）で、出力がつながりを持ちながら生成されるように設計されている。主に自然言語生成や推論タスクに使用される。\n",
    "\n",
    "5. **量子化（Quantization）**:\n",
    "   - モデルのパラメータを低精度に変換して、メモリ使用量を削減し、推論速度を向上させるプロセス。「量子化フラグ」を設定することで、訓練済みの高精度モデルをより軽量なものに変換できる。\n",
    "\n",
    "6. **サンプリング温度（Temperature）**:\n",
    "   - モデルの出力のランダム性を制御するパラメータ。温度が高いほど多様性が増し、低いほど安定した予測が得られる。特に生成タスクでのクリエイティビティや多様性を調整するために重要。\n",
    "\n",
    "7. **top_pサンプリング**:\n",
    "   - 確率的サンプリング手法で、累積分布が指定した確率（top_p）を超えるトークンのうち、最も確率の高いものからサンプリングすることにより、多様な出力を選択する方法。\n",
    "\n",
    "8. **デフォルトテンソルタイプ**:\n",
    "   - PyTorchにおけるデフォルトのデータ型設定。特定の演算がどのデータ型で実行されるかに影響を与える。計算精度やメモリ使用効率に関連するため、使い方に注意が必要。\n",
    "\n",
    "9. **エージェント（Agent）**:\n",
    "   - 特定の役割を果たすプログラム。ここでは、質問者と回答者の役割を持ち、お互いにインタラクションするLLMを指す。\n",
    "\n",
    "10. **状態管理（State Management）**:\n",
    "   - 定義された変数やデータ（質問や回答など）の状態を維持するプロセス。ゲームの進行やモデルの応答に利用される。\n",
    "\n",
    "これらの用語は、ノートブックの実装や設定において重要な役割を果たしており、理解しておくことで全体の理解が深まります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bc7327",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Notebook to show Simple Start\n",
    "\n",
    "Based on the excllent \"LLM 20 Questions Starter Notebook\"  \n",
    "\n",
    "However this is much more simpliffied in order to help to get started with prompt engineering quickly.    \n",
    "\n",
    "It is not as advanced as the \"LLM 20 Questions Starter Notebook\" and the prompts are taken directly from the \"LLM 20 Questions\" notebook supplied for the competition.  \n",
    "\n",
    "At the end there are cells to help test your prompts. \n",
    "\n",
    "Any issues/suggestions for improvements are welcome.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# シンプルスタートを示すノートブック\n",
    "\n",
    "優れた「LLM 20 Questions Starter Notebook」に基づいています。\n",
    "\n",
    "ただし、プロンプトエンジニアリングを迅速に始めるために、はるかに簡略化されています。\n",
    "\n",
    "「LLM 20 Questions Starter Notebook」ほど高度ではなく、プロンプトはコンペティション用に提供された「LLM 20 Questions」ノートブックから直接取得されています。\n",
    "\n",
    "最後には、プロンプトをテストするためのセルがあります。\n",
    "\n",
    "問題や改善の提案があれば歓迎します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345e2f37",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "# immutabledictとsentencepieceを指定したディレクトリにインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorchリポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemmaパッケージのファイルを移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-06-22T10:58:08.236875Z",
     "iopub.status.busy": "2024-06-22T10:58:08.236492Z",
     "iopub.status.idle": "2024-06-22T10:58:22.292725Z",
     "shell.execute_reply": "2024-06-22T10:58:22.291888Z",
     "shell.execute_reply.started": "2024-06-22T10:58:08.236838Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "# immutabledictとsentencepieceを指定したディレクトリにインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorchリポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemmaパッケージのファイルを移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b35c4fe7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**Add the model using Add Input**  \n",
    "This is found on the right hand side of the enivronment\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**Add the model using Add Input**  \n",
    "環境の右側に見つけることができます\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ee74db",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# %%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, List\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "\n",
    "# Set our own dataclass so we know what is in the object\n",
    "@dataclass\n",
    "class ObsData:\n",
    "    answers: List[\"str\"]\n",
    "    category: str\n",
    "    keyword: str\n",
    "    questions: List[\"str\"]\n",
    "    turn_type: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "    \n",
    "\n",
    "class Agents:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        # It is much better if you request a GPU session as a 7B model is rather large\n",
    "        self._device = torch.device(\"cpu\")\n",
    "        if torch.cuda.is_available():\n",
    "            self._device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    def answer_agent(self, question: str, category: str, keyword: str) -> Literal[\"yes\", \"no\"]:\n",
    "        info_prompt = \"\"\"You are a very precise answerer in a game of 20 questions. The keyword that the questioner is trying to guess is [the {category} {keyword}]. \"\"\"\n",
    "        answer_question_prompt = f\"\"\"Answer the following question with only yes, no, or if unsure maybe: {question}\"\"\"\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(category=category, keyword=keyword),\n",
    "            answer_question_prompt\n",
    "        )\n",
    "        \n",
    "        return self._call_llm(prompt)\n",
    "\n",
    "    def ask_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        info_prompt = \"\"\"You are playing a game of 20 questions where you ask the questions and try to figure out the keyword, which will be a real or fictional person, place, or thing. \\nHere is what you know so far:\\n{q_a_thread}\"\"\"\n",
    "        questions_prompt = \"\"\"Ask one yes or no question.\"\"\"\n",
    "\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            questions_prompt\n",
    "        )\n",
    "\n",
    "        return self._call_llm(prompt)\n",
    "\n",
    "    \n",
    "    def guess_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # It is expected that the answer is surrounder by **         \n",
    "        info_prompt = \"\"\"You are playing a game of 20 questions where you ask the questions and try to figure out the keyword, which will be a real or fictional person, place, or thing. \\nHere is what you know so far:\\n{q_a_thread}\"\"\"\n",
    "        guess_prompt = \"\"\"Guess the keyword. Only respond with the exact word/phrase. For example, if you think the keyword is [paris], don't respond with [I think the keyword is paris] or [Is the kewyord Paris?]. Respond only with the word [paris].\"\"\"\n",
    "\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "        \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            guess_prompt\n",
    "        )\n",
    "\n",
    "        return f\"**{self._call_llm(prompt)}**\"\n",
    "    \n",
    "    def _call_llm(self, prompt: str):\n",
    "        self._set_model()\n",
    "        \n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,\n",
    "            'top_p': 0.1,\n",
    "            'top_k': 1,\n",
    "        }\n",
    "        \n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=100,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "         \n",
    "    def _set_model(self):\n",
    "        if self.model is None:\n",
    "            print(\"No model yet so setting up\")\n",
    "            model_config = get_config_for_7b()\n",
    "            model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "            model_config.quant = True\n",
    "\n",
    "            # Not using a context manager blows the stack\n",
    "            with self._set_default_tensor_type(model_config.get_dtype()):\n",
    "                model = GemmaForCausalLM(model_config)\n",
    "                ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-7b-it-quant.ckpt')\n",
    "                model.load_weights(ckpt_path)\n",
    "                self.model = model.to(self._device).eval()\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(self, dtype: torch.dtype):\n",
    "        \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "        torch.set_default_dtype(dtype)\n",
    "        yield\n",
    "        torch.set_default_dtype(torch.float)\n",
    "\n",
    "# The entry point so name and paramters are preset\n",
    "def agent_fn(obs, cfg) -> str:\n",
    "    obs_data = ObsData(\n",
    "        turn_type=obs.turnType,\n",
    "        questions=obs.questions,\n",
    "        answers=obs.answers,\n",
    "        keyword=obs.keyword,\n",
    "        category=obs.category\n",
    "    )\n",
    "    \n",
    "    if obs_data.turn_type == \"ask\":\n",
    "        response = agents.ask_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"guess\":\n",
    "        response = agents.guess_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"answer\":\n",
    "        response = agents.answer_agent(question=obs.questions[-1], category=obs.category, keyword=obs.keyword)\n",
    "    \n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "# Instantiate the agents class so the model is only set once\n",
    "agents = Agents()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# %%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleのエージェントのパスを指定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, List\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 重みファイルのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "\n",
    "# オブジェクトの内容を把握するために独自のデータクラスを定義します\n",
    "@dataclass\n",
    "class ObsData:\n",
    "    answers: List[\"str\"]  # 回答のリスト\n",
    "    category: str  # カテゴリ名\n",
    "    keyword: str  # キーワード\n",
    "    questions: List[\"str\"]  # 質問のリスト\n",
    "    turn_type: Literal[\"ask\", \"guess\", \"answer\"]  # ターンの種類（質問、推測、回答のいずれか）\n",
    "    \n",
    "\n",
    "class Agents:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        # 7Bモデルはかなり大きいのでGPUセッションをリクエストするのが理想的です\n",
    "        self._device = torch.device(\"cpu\")  # デフォルトはCPU\n",
    "        if torch.cuda.is_available():\n",
    "            self._device = torch.device(\"cuda:0\")  # GPUが利用可能なら使用します\n",
    "    \n",
    "    def answer_agent(self, question: str, category: str, keyword: str) -> Literal[\"yes\", \"no\"]:\n",
    "        # 質問者が推測しているキーワードについての情報を設定します\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームにおいて非常に正確な回答者です。質問者が推測しているキーワードは[カテゴリ: {category} キーワード: {keyword}]です。\"\"\"\n",
    "        answer_question_prompt = f\"\"\"以下の質問に「はい」、「いいえ」、または「わからない場合はmaybe」だけで答えてください: {question}\"\"\"\n",
    "    \n",
    "        # プロンプトを組み立てます\n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(category=category, keyword=keyword),\n",
    "            answer_question_prompt\n",
    "        )\n",
    "        \n",
    "        return self._call_llm(prompt)  # LLMを呼び出して応答を得る\n",
    "\n",
    "    def ask_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # 質問を行いキーワードを推測するための情報を設定します\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームをプレイしており、質問をしてキーワードを推測しようとしています。それは現実または架空の人物、場所、または物です。\\nここまでに知っている情報:\\n{q_a_thread}\"\"\"\n",
    "        questions_prompt = \"\"\"1つの「はい」または「いいえ」で答えられる質問をして下さい。\"\"\"\n",
    "\n",
    "        # 質問と回答の履歴を構築します\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            questions_prompt\n",
    "        )\n",
    "\n",
    "        return self._call_llm(prompt)  # LLMを呼び出して応答を得る\n",
    "\n",
    "    \n",
    "    def guess_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # 推測するための情報を設定します (答えは**で囲まれます)\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームをプレイしており、質問をしてキーワードを推測しようとしています。それは現実または架空の人物、場所、または物です。\\nここまでに知っている情報:\\n{q_a_thread}\"\"\"\n",
    "        guess_prompt = \"\"\"キーワードを推測してください。正確な単語/フレーズだけを返答します。例えば、キーワードが[パリ]だと思う場合は、[私はキーワードがパリだと思う]や[キーワードはパリですか？]ではなく、単語[パリ]だけで返答してください。\"\"\"\n",
    "\n",
    "        # 質問と回答の履歴を構築します\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "        \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            guess_prompt\n",
    "        )\n",
    "\n",
    "        return f\"**{self._call_llm(prompt)}**\"  # LLMを呼び出して応答を得る（答えは**で囲まれる）\n",
    "\n",
    "    def _call_llm(self, prompt: str):\n",
    "        self._set_model()  # モデルを設定\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,  # サンプリング温度\n",
    "            'top_p': 0.1,  # 上位確率\n",
    "            'top_k': 1,  # 上位kの制限\n",
    "        }\n",
    "        \n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,  # デバイス指定（GPUまたはCPU）\n",
    "            output_len=100,  # 出力の長さ\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "         \n",
    "    def _set_model(self):\n",
    "        if self.model is None:\n",
    "            print(\"モデルがまだ設定されていないため、設定中です。\")\n",
    "            model_config = get_config_for_7b()\n",
    "            model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザーのパス\n",
    "            model_config.quant = True  # 量子化フラグ\n",
    "\n",
    "            # コンテキストマネージャを使用して、スタックが吹き飛ばないようにします\n",
    "            with self._set_default_tensor_type(model_config.get_dtype()):\n",
    "                model = GemmaForCausalLM(model_config)\n",
    "                ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-7b-it-quant.ckpt')  # 重みのパス\n",
    "                model.load_weights(ckpt_path)  # 重みを読み込みます\n",
    "                self.model = model.to(self._device).eval()  # モデルをデバイスに移動して評価モードにします\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(self, dtype: torch.dtype):\n",
    "        \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "        torch.set_default_dtype(dtype)  # dtypeを設定\n",
    "        yield\n",
    "        torch.set_default_dtype(torch.float)  # デフォルトのdtypeをfloatに戻します\n",
    "\n",
    "# エントリーポイントのため、名前とパラメータが事前に設定されています\n",
    "def agent_fn(obs, cfg) -> str:\n",
    "    obs_data = ObsData(\n",
    "        turn_type=obs.turnType,\n",
    "        questions=obs.questions,\n",
    "        answers=obs.answers,\n",
    "        keyword=obs.keyword,\n",
    "        category=obs.category\n",
    "    )\n",
    "    \n",
    "    # ターンの種類に応じて適切なエージェントを呼び出します\n",
    "    if obs_data.turn_type == \"ask\":\n",
    "        response = agents.ask_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"guess\":\n",
    "        response = agents.guess_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"answer\":\n",
    "        response = agents.answer_agent(question=obs.questions[-1], category=obs.category, keyword=obs.keyword)\n",
    "    \n",
    "    # 応答がなかったり、長さが1以下の場合は「はい」を返します\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    \n",
    "    return response  # 得られた応答を返します\n",
    "\n",
    "# エージェントが1度だけ設定されるように、Agentsクラスをインスタンス化します\n",
    "agents = Agents()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:13:23.842116Z",
     "iopub.status.busy": "2024-06-22T12:13:23.841789Z",
     "iopub.status.idle": "2024-06-22T12:13:23.867408Z",
     "shell.execute_reply": "2024-06-22T12:13:23.866335Z",
     "shell.execute_reply.started": "2024-06-22T12:13:23.84209Z"
    }
   },
   "outputs": [],
   "source": [
    "# %%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleのエージェントのパスを指定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal, List\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 重みファイルのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "\n",
    "# オブジェクトの内容を把握するために独自のデータクラスを定義します\n",
    "@dataclass\n",
    "class ObsData:\n",
    "    answers: List[\"str\"]  # 回答のリスト\n",
    "    category: str  # カテゴリ名\n",
    "    keyword: str  # キーワード\n",
    "    questions: List[\"str\"]  # 質問のリスト\n",
    "    turn_type: Literal[\"ask\", \"guess\", \"answer\"]  # ターンの種類（質問、推測、回答のいずれか）\n",
    "    \n",
    "\n",
    "class Agents:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        \n",
    "        # 7Bモデルはかなり大きいのでGPUセッションをリクエストするのが理想的です\n",
    "        self._device = torch.device(\"cpu\")  # デフォルトはCPU\n",
    "        if torch.cuda.is_available():\n",
    "            self._device = torch.device(\"cuda:0\")  # GPUが利用可能なら使用します\n",
    "    \n",
    "    def answer_agent(self, question: str, category: str, keyword: str) -> Literal[\"yes\", \"no\"]:\n",
    "        # 質問者が推測しているキーワードについての情報を設定します\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームにおいて非常に正確な回答者です。質問者が推測しているキーワードは[カテゴリ: {category} キーワード: {keyword}]です。\"\"\"\n",
    "        answer_question_prompt = f\"\"\"以下の質問に「はい」、「いいえ」、または「わからない場合はmaybe」だけで答えてください: {question}\"\"\"\n",
    "    \n",
    "        # プロンプトを組み立てます\n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(category=category, keyword=keyword),\n",
    "            answer_question_prompt\n",
    "        )\n",
    "        \n",
    "        return self._call_llm(prompt)  # LLMを呼び出して応答を得る\n",
    "\n",
    "    def ask_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # 質問を行いキーワードを推測するための情報を設定します\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームをプレイしており、質問をしてキーワードを推測しようとしています。それは現実または架空の人物、場所、または物です。\\nここまでに知っている情報:\\n{q_a_thread}\"\"\"\n",
    "        questions_prompt = \"\"\"1つの「はい」または「いいえ」で答えられる質問をして下さい。\"\"\"\n",
    "\n",
    "        # 質問と回答の履歴を構築します\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "    \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            questions_prompt\n",
    "        )\n",
    "\n",
    "        return self._call_llm(prompt)  # LLMを呼び出して応答を得る\n",
    "\n",
    "    \n",
    "    def guess_agent(self, questions: List[str], answers: List[str]) -> str:\n",
    "        # 推測するための情報を設定します (答えは**で囲まれます)\n",
    "        info_prompt = \"\"\"あなたは「20の質問」ゲームをプレイしており、質問をしてキーワードを推測しようとしています。それは現実または架空の人物、場所、または物です。\\nここまでに知っている情報:\\n{q_a_thread}\"\"\"\n",
    "        guess_prompt = \"\"\"キーワードを推測してください。正確な単語/フレーズだけを返答します。例えば、キーワードが[パリ]だと思う場合は、[私はキーワードがパリだと思う]や[キーワードはパリですか？]ではなく、単語[パリ]だけで返答してください。\"\"\"\n",
    "\n",
    "        # 質問と回答の履歴を構築します\n",
    "        q_a_thread = \"\"\n",
    "        for i in range(0, len(answers)):\n",
    "            q_a_thread = \"{}Q: {} A: {}\\n\".format(\n",
    "                q_a_thread,\n",
    "                questions[i],\n",
    "                answers[i]\n",
    "            )\n",
    "        \n",
    "        prompt = \"{}{}\".format(\n",
    "            info_prompt.format(q_a_thread=q_a_thread),\n",
    "            guess_prompt\n",
    "        )\n",
    "\n",
    "        return f\"**{self._call_llm(prompt)}**\"  # LLMを呼び出して応答を得る（答えは**で囲まれる）\n",
    "\n",
    "    def _call_llm(self, prompt: str):\n",
    "        self._set_model()  # モデルを設定\n",
    "        sampler_kwargs = {\n",
    "            'temperature': 0.01,  # サンプリング温度\n",
    "            'top_p': 0.1,  # 上位確率\n",
    "            'top_k': 1,  # 上位kの制限\n",
    "        }\n",
    "        \n",
    "        return self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,  # デバイス指定（GPUまたはCPU）\n",
    "            output_len=100,  # 出力の長さ\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "         \n",
    "    def _set_model(self):\n",
    "        if self.model is None:\n",
    "            print(\"モデルがまだ設定されていないため、設定中です。\")\n",
    "            model_config = get_config_for_7b()\n",
    "            model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザーのパス\n",
    "            model_config.quant = True  # 量子化フラグ\n",
    "\n",
    "            # コンテキストマネージャを使用して、スタックが吹き飛ばないようにします\n",
    "            with self._set_default_tensor_type(model_config.get_dtype()):\n",
    "                model = GemmaForCausalLM(model_config)\n",
    "                ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-7b-it-quant.ckpt')  # 重みのパス\n",
    "                model.load_weights(ckpt_path)  # 重みを読み込みます\n",
    "                self.model = model.to(self._device).eval()  # モデルをデバイスに移動して評価モードにします\n",
    "    \n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(self, dtype: torch.dtype):\n",
    "        \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "        torch.set_default_dtype(dtype)  # dtypeを設定\n",
    "        yield\n",
    "        torch.set_default_dtype(torch.float)  # デフォルトのdtypeをfloatに戻します\n",
    "\n",
    "# エントリーポイントのため、名前とパラメータが事前に設定されています\n",
    "def agent_fn(obs, cfg) -> str:\n",
    "    obs_data = ObsData(\n",
    "        turn_type=obs.turnType,\n",
    "        questions=obs.questions,\n",
    "        answers=obs.answers,\n",
    "        keyword=obs.keyword,\n",
    "        category=obs.category\n",
    "    )\n",
    "    \n",
    "    # ターンの種類に応じて適切なエージェントを呼び出します\n",
    "    if obs_data.turn_type == \"ask\":\n",
    "        response = agents.ask_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"guess\":\n",
    "        response = agents.guess_agent(questions=obs.questions, answers=obs.answers)\n",
    "    if obs_data.turn_type == \"answer\":\n",
    "        response = agents.answer_agent(question=obs.questions[-1], category=obs.category, keyword=obs.keyword)\n",
    "    \n",
    "    # 応答がなかったり、長さが1以下の場合は「はい」を返します\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    \n",
    "    return response  # 得られた応答を返します\n",
    "\n",
    "# エージェントが1度だけ設定されるように、Agentsクラスをインスタンス化します\n",
    "agents = Agents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41683859",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Manually run the answer agent\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"answer\",\n",
    "        questions=[\"Is it a place\"],\n",
    "        answers=[],\n",
    "        keyword=\"Antartica\",\n",
    "        category=\"Place\"\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 回答エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"answer\",\n",
    "        questions=[\"はそれは場所ですか\"],\n",
    "        answers=[],  # 回答はまだありません\n",
    "        keyword=\"南極\",  # キーワード\n",
    "        category=\"場所\"  # カテゴリ\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:13:27.586335Z",
     "iopub.status.busy": "2024-06-22T12:13:27.585486Z",
     "iopub.status.idle": "2024-06-22T12:14:15.05232Z",
     "shell.execute_reply": "2024-06-22T12:14:15.051301Z",
     "shell.execute_reply.started": "2024-06-22T12:13:27.586289Z"
    }
   },
   "outputs": [],
   "source": [
    "# 回答エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"answer\",\n",
    "        questions=[\"はそれは場所ですか\"],\n",
    "        answers=[],  # 回答はまだありません\n",
    "        keyword=\"南極\",  # キーワード\n",
    "        category=\"場所\"  # カテゴリ\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7220b787",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Manually run the question agent\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"ask\",\n",
    "        questions=[\"Is it a place?\"],\n",
    "        answers=[\"Yes\"],\n",
    "        keyword=\"\",\n",
    "        category=\"\"\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 質問エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"ask\",\n",
    "        questions=[\"それは場所ですか？\"],  # 質問\n",
    "        answers=[\"はい\"],  # 回答\n",
    "        keyword=\"\",  # キーワードはまだ不明\n",
    "        category=\"\"  # カテゴリはまだ不明\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:14:39.010378Z",
     "iopub.status.busy": "2024-06-22T12:14:39.009974Z",
     "iopub.status.idle": "2024-06-22T12:15:20.042528Z",
     "shell.execute_reply": "2024-06-22T12:15:20.041506Z",
     "shell.execute_reply.started": "2024-06-22T12:14:39.010344Z"
    }
   },
   "outputs": [],
   "source": [
    "# 質問エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"ask\",\n",
    "        questions=[\"それは場所ですか？\"],  # 質問\n",
    "        answers=[\"はい\"],  # 回答\n",
    "        keyword=\"\",  # キーワードはまだ不明\n",
    "        category=\"\"  # カテゴリはまだ不明\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fc27fa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Manually run the guess agent\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str\n",
    "    \n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"guess\",\n",
    "        questions=[\"Is it a place?\", \"Is it in the northen hemisphere?\", \"Is it a city?\", \"Is it icy?\"],\n",
    "        answers=[\"Yes\", \"No\", \"No\", \"Yes\"],\n",
    "        keyword=\"\",\n",
    "        category=\"\"\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 推測エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"guess\",\n",
    "        questions=[\"それは場所ですか？\", \"北半球ですか？\", \"都市ですか？\", \"氷だらけですか？\"],  # 質問リスト\n",
    "        answers=[\"はい\", \"いいえ\", \"いいえ\", \"はい\"],  # 質問に対する回答\n",
    "        keyword=\"\",  # キーワードはまだ不明\n",
    "        category=\"\"  # カテゴリはまだ不明\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-22T12:15:51.259776Z",
     "iopub.status.busy": "2024-06-22T12:15:51.259369Z",
     "iopub.status.idle": "2024-06-22T12:16:32.830569Z",
     "shell.execute_reply": "2024-06-22T12:16:32.829575Z",
     "shell.execute_reply.started": "2024-06-22T12:15:51.259747Z"
    }
   },
   "outputs": [],
   "source": [
    "# 推測エージェントを手動で実行します\n",
    "@dataclass\n",
    "class ObsDataIn(ObsData):\n",
    "    turnType: str  # ターンの種類を追加します\n",
    "    \n",
    "# エージェントの入力データを構築します\n",
    "obs_data = ObsDataIn(\n",
    "        turn_type=\"\",\n",
    "        turnType=\"guess\",\n",
    "        questions=[\"それは場所ですか？\", \"北半球ですか？\", \"都市ですか？\", \"氷だらけですか？\"],  # 質問リスト\n",
    "        answers=[\"はい\", \"いいえ\", \"いいえ\", \"はい\"],  # 質問に対する回答\n",
    "        keyword=\"\",  # キーワードはまだ不明\n",
    "        category=\"\"  # カテゴリはまだ不明\n",
    "    )\n",
    "\n",
    "print(agent_fn(obs_data, {}))  # エージェント関数を呼び出します"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
