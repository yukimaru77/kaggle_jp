{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3de0fecf",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LLM 20 Questions」コンペティションにおけるAIエージェントの作成プロセスを示しています。具体的には、AIが「20の質問」ゲームをプレイするためのエージェント（質問者および回答者）を構築しています。このノートブックでは、エラーが発生した際のメッセージ（Validation Episode failed）から始まっていますが、正しく実行されると、提出用の`submission.tar.gz`ファイルが生成されます。\n",
    "\n",
    "### 主な課題\n",
    "ノートブックは、AIエージェントに与えられたターゲット単語を推測させ、各エージェントが適切に連携しながら質問を行ったり、回答を提供したりする問題に取り組んでいます。質問者エージェントは質問を生成し、回答者エージェントは「はい」または「いいえ」で答えます。このダイナミックなやり取りを通じて、できるだけ早くターゲット単語を特定することが目指されます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "ノートブックは以下の手法やライブラリを使用して問題にアプローチしています：\n",
    "\n",
    "1. **Pythonライブラリのインストール**: \n",
    "   - `immutabledict`と`sentencepiece`を使用して、言語モデルのパフォーマンス向上を図っています。\n",
    "   - GitHubから`gemma_pytorch`ライブラリをクローンし、AIエージェントに必要な設定を整えています。\n",
    "\n",
    "2. **GemmaFrameworkの使用**: \n",
    "   - `GemmaForCausalLM`を利用して言語モデルを構築し、AIエージェントを効果的に機能させるための基盤を提供しています。\n",
    "\n",
    "3. **エージェントクラスの定義**:\n",
    "   - `GemmaAgent`クラスをベースに、質問者エージェント（`GemmaQuestionerAgent`）と回答者エージェント（`GemmaAnswererAgent`）を定義。\n",
    "   - それぞれのクラスは、ユーザーからの入力を受け取り、ターゲットに基づいて動的に応答を生成します。\n",
    "\n",
    "4. **プロンプトフォーマッティング**:\n",
    "   - `GemmaFormatter`クラスを使用して、ユーザーとモデル間の会話の状態を管理し、適切にフォーマットされたプロンプトを生成します。\n",
    "\n",
    "5. **圧縮とファイル管理**:\n",
    "   - `pigz`と`pv`を利用して作成した圧縮ファイルを用いて、エージェントをKaggleに提出するための準備を整えています。これにより、ファイルの圧縮速度が向上し、その進行状況を確認することができます。\n",
    "\n",
    "このノートブック全体を通じて、KaggleのコンペティションにおけるLLMの適用と、AIエージェントの構築手法が詳細に示されています。成功すれば、AIは「20の質問」ゲームにおいて高いパフォーマンスを発揮することが期待されます。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744e6201",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyterノートブックに関連する機械学習・深層学習の専門用語に関する解説です。初心者がつまずきそうな専門用語に焦点を当てています。\n",
    "\n",
    "1. **LLM（Large Language Model）**:\n",
    "   - 大規模言語モデルの略称。トレーニングデータセットに大量のテキストデータを使用して訓練されたモデルで、自然言語処理において非常に高い性能を発揮します。通常、数億から数十億のパラメータを持ち、文脈を理解する能力があります。\n",
    "\n",
    "2. **Gemma**:\n",
    "   - Kaggleのコンペティションで使用される特定のモデル名です。このモデルは、効率的な言語生成を目的としたアーキテクチャに基づいており、特に因果的言語モデル（Causal Language Model）として設計されています。\n",
    "\n",
    "3. **few-shot examples**:\n",
    "   - モデルに与えて学習を助けるための少数の例。特に少数ショット学習は、十分なトレーニングデータがない場合でもモデルがタスクを学習するのに役立つ手法です。この手法は、新しいコンテキストでの応答を生成する際に非常に有用です。\n",
    "\n",
    "4. **プロンプト**:\n",
    "   - モデルに対して与えられる指示や質問のこと。言語モデルがどのように応答するかを決定するため、プロンプトの設計が重要です。システムプロンプトは特に、モデルがなすべき役割を定義するために使用されます。\n",
    "\n",
    "5. **contextlib**:\n",
    "   - Pythonの標準ライブラリの一部で、コンテキストマネージャを簡単に作成するための機能を提供します。リソース管理や一時的な設定のためにしばしば使用され、リソースの解放や状態の復元を自動で行います。\n",
    "\n",
    "6. **torch.device**:\n",
    "   - PyTorchにおけるデバイスの指定に使用されるクラス。計算をどのハードウェア（CPUまたはGPU）で行うかを示します。例えば、`cuda:0`は最初のGPUを指します。\n",
    "\n",
    "7. **量子化（Quantization）**:\n",
    "   - モデルのパラメータを減少させ、モデルのサイズを最適化し、推論速度を向上させるために数値の精度を下げる技術。これにより、リソースの限られたデバイスでも大規模なモデルを使用できるようになります。\n",
    "\n",
    "8. **itertools**:\n",
    "   - Pythonのライブラリで、効率的なループや組み合わせを生成するためのツールが含まれています。特に、`zip_longest`関数は、異なる長さのリストを組み合わせる際に使用され、リストの要素が不足している場合は`None`で埋めます。\n",
    "\n",
    "9. **状態管理（State Management）**:\n",
    "   - モデルやシステムの現在の状態を追跡・保存する技術。例えば、会話の流れやユーザーからの入力を保持することで、コンテキストを維持します。\n",
    "\n",
    "10. **アサーション（Assertion）**:\n",
    "    - プログラムの実行時に特定の条件が満たされているかを確認するための手法。条件が満たされない場合、エラーメッセージを表示し、プログラムを停止します。デバッグやテストに有用です。\n",
    "\n",
    "これらの用語は、ノートブックの実装において重要な役割を果たしており、理解しておくことが各コンポーネントの機能や相互作用を理解する助けになります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f954d2",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "May, 20 2024\n",
    "\n",
    "* **I've tried to submit the notebook:** returning the error *Validation Episode failed*.\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. From the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. \n",
    "\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "2024年5月20日\n",
    "\n",
    "* **ノートブックを提出しようとしましたが、*Validation Episode failed*というエラーが返ってきました。**\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。ノートブックビューアから、*Output*タブをクリックし、`submission.tar.gz`を見つけてダウンロードしてください。コンペティションのホームページの左上にある**Submit Agent**をクリックして、ファイルをアップロードして提出してください。 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c49cb5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74bf57fa",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**ChatGPT explanation of the code above**\n",
    "\n",
    "This code is a Bash script that sets up a working environment, installs necessary Python libraries, and prepares a specific library (from a GitHub repository) for use. Here is a detailed breakdown of each line:\n",
    "1. `%%bash`:\n",
    "   - This is a magic command used in Jupyter Notebooks to indicate that the following lines should be interpreted as Bash commands.\n",
    "2. `cd /kaggle/working`:\n",
    "   - This changes the current directory to `/kaggle/working`. This directory is often used in Kaggle kernels (notebooks) to store and access files during computation.\n",
    "3. `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`:\n",
    "   - This uses `pip` to quietly (`-q`) install (`install`) the latest versions (`-U` for upgrade) of the `immutabledict` and `sentencepiece` Python libraries.\n",
    "   - The `-t /kaggle/working/submission/lib` option specifies the target directory where the libraries should be installed, which is `/kaggle/working/submission/lib`.\n",
    "4. `git clone https://github.com/google/gemma_pytorch.git > /dev/null`:\n",
    "   - This clones the `gemma_pytorch` repository from GitHub.\n",
    "   - The `> /dev/null` part redirects the output to `/dev/null`, effectively silencing the output of the clone command to keep the notebook or script output clean.\n",
    "5. `mkdir /kaggle/working/submission/lib/gemma/`:\n",
    "   - This creates a new directory named `gemma` inside `/kaggle/working/submission/lib/`. This directory will be used to store files from the cloned repository.\n",
    "6. `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`:\n",
    "   - This moves all files (`*`) from the `gemma` directory inside the cloned `gemma_pytorch` repository to the newly created `gemma` directory in `/kaggle/working/submission/lib/`.\n",
    "### Summary\n",
    "- The script sets the working directory.\n",
    "- Installs `immutabledict` and `sentencepiece` libraries to a specific path.\n",
    "- Clones a repository from GitHub.\n",
    "- Creates a directory for the cloned repository's files.\n",
    "- Moves the necessary files from the cloned repository to the created directory.\n",
    "This setup might be used for preparing a custom environment with specific dependencies and libraries needed for a machine learning task or other computational tasks in a Kaggle notebook.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**上記のコードの説明**\n",
    "\n",
    "このコードは、作業環境を設定し、必要なPythonライブラリをインストールし、特定のライブラリ（GitHubリポジトリから）を使用するための準備を行うBashスクリプトです。以下は各行の詳細な説明です：\n",
    "1. `%%bash`:\n",
    "   - これはJupyterノートブックで使われるマジックコマンドで、以下の行がBashコマンドとして解釈されることを示します。\n",
    "2. `cd /kaggle/working`:\n",
    "   - 現在のディレクトリを`/kaggle/working`に変更します。このディレクトリは、Kaggleカーネル（ノートブック）で計算中にファイルを保存およびアクセスするためによく使用されます。\n",
    "3. `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`:\n",
    "   - `pip`を使用して、`immutabledict`と`sentencepiece`の最新バージョンを静かに（`-q`）インストールします（`-U`はアップグレードの意味）。\n",
    "   - `-t /kaggle/working/submission/lib`オプションは、ライブラリをインストールする対象ディレクトリを指定します。\n",
    "4. `git clone https://github.com/google/gemma_pytorch.git > /dev/null`:\n",
    "   - GitHubから`gemma_pytorch`リポジトリをクローンします。\n",
    "   - `> /dev/null`部分は、出力を`/dev/null`にリダイレクトし、クローンコマンドの出力をサイレントにしてノートブックやスクリプトの出力をきれいに保ちます。\n",
    "5. `mkdir /kaggle/working/submission/lib/gemma/`:\n",
    "   - `/kaggle/working/submission/lib/`内に、`gemma`という名前の新しいディレクトリを作成します。このディレクトリは、クローンしたリポジトリのファイルを保存するために使用されます。\n",
    "6. `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`:\n",
    "   - クローンした`gemma_pytorch`リポジトリ内の`gemma`ディレクトリからすべてのファイル（`*`）を新しく作成した`gemma`ディレクトリに移動します。\n",
    "### まとめ\n",
    "- このスクリプトは作業ディレクトリを設定します。\n",
    "- `immutabledict`と`sentencepiece`ライブラリを特定のパスにインストールします。\n",
    "- GitHubからリポジトリをクローンします。\n",
    "- クローンしたリポジトリのファイルを格納するためのディレクトリを作成します。\n",
    "- 必要なファイルを新しく作成したディレクトリに移動します。\n",
    "この設定は、機械学習タスクやKaggleノートブックでのその他の計算タスクに必要な特定の依存関係とライブラリを準備するために使用される可能性があります。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a4878c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で動作するように、システムパスをこのように設定してください\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        # モデルを呼び出して応答を生成します\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 応答から**で囲まれたキーワードを見つけ出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問を始めましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを予想してください。予想を**で囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"未知のターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問を始めましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}です。カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"キーワード{obs.keyword}に関する質問です。はいまたはいいえで答えてください。答えを**で囲んでください**。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 応答から解析したキーワードに基づいて「はい」または「いいえ」を返します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で動作するように、システムパスをこのように設定してください\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        # モデルを呼び出して応答を生成します\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 応答から**で囲まれたキーワードを見つけ出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問を始めましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを予想してください。予想を**で囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"未知のターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問を始めましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}です。カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"キーワード{obs.keyword}に関する質問です。はいまたはいいえで答えてください。答えを**で囲んでください**。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 応答から解析したキーワードに基づいて「はい」または「いいえ」を返します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06910d8b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**ChatGPT explanation of the code above**\n",
    "\n",
    "This script defines the setup and behavior of an AI agent designed to play the 20 Questions game. The agent can either be a questioner or an answerer. Here is a detailed explanation of each part of the code:\n",
    "### 1. File Creation\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "```\n",
    "- This Jupyter Notebook magic command writes the following code into a file named `main.py` in the `submission` directory.\n",
    "### 2. Setup\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "```\n",
    "- Import necessary modules.\n",
    "- Set the system path to include the directory with necessary libraries, ensuring compatibility both in notebooks and Kaggle simulation environments.\n",
    "### 3. Import Libraries and Configuration\n",
    "```python\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "```\n",
    "- Import additional libraries.\n",
    "- Define the path for model weights based on the environment.\n",
    "### 4. Prompt Formatting Class\n",
    "```python\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "```\n",
    "- `GemmaFormatter` formats the conversation between the user and the model using predefined tokens to mark the start and end of turns.\n",
    "- Methods allow adding user and model prompts, starting and ending turns, and resetting the conversation state.\n",
    "### 5. Agent Definitions\n",
    "```python\n",
    "import re\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "            }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "```\n",
    "- `GemmaAgent` class initializes the model, formats prompts, and handles model interactions.\n",
    "- `_set_default_tensor_type` context manager temporarily sets the default tensor type for PyTorch.\n",
    "- `__call__` method generates a response from the model based on the observation (`obs`).\n",
    "- `_start_session` and `_parse_response` methods are placeholders meant to be implemented by subclasses.\n",
    "- `interleave_unequal` function interleaves elements from two lists, filling gaps with the remaining elements from the longer list.\n",
    "### 6. Questioner and Answerer Agents\n",
    "```python\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "```\n",
    "- `GemmaQuestionerAgent` and `GemmaAnswererAgent` are subclasses of `GemmaAgent`, each implementing `_start_session` and `_parse_response` methods tailored to their roles in the game.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**上記のコードの説明**\n",
    "\n",
    "このスクリプトは、20の質問ゲームをプレイするために設計されたAIエージェントのセットアップと動作を定義します。エージェントは質問者または回答者のいずれかになります。コードの各部分についての詳細な説明は以下の通りです：\n",
    "### 1. ファイル作成\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "```\n",
    "- このJupyterノートブックのマジックコマンドは、以下のコードを`submission`ディレクトリ内の`main.py`というファイルに書き込みます。\n",
    "### 2. セットアップ\n",
    "```python\n",
    "import os\n",
    "import sys\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "```\n",
    "- 必要なモジュールをインポートします。\n",
    "- システムパスを指定して、ノートブックとKaggleシミュレーション環境の両方で必要なライブラリが利用できるようにします。\n",
    "### 3. ライブラリおよび設定のインポート\n",
    "```python\n",
    "import contextlib\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "```\n",
    "- 追加のライブラリをインポートします。\n",
    "- 環境に基づいてモデルの重みのパスを定義します。\n",
    "### 4. プロンプトフォーマッティングクラス\n",
    "```python\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "```\n",
    "- `GemmaFormatter`は、ユーザーとモデル間の会話をフォーマットする役割を果たします。会話のターンの開始と終了を示すために定義されたトークンを使用します。\n",
    "- ユーザーやモデルのプロンプトを追加したり、ターンの開始と終了を管理したり、会話の状態をリセットするためのメソッドがあります。\n",
    "### 5. エージェント定義\n",
    "```python\n",
    "import re\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "                }\n",
    "        # モデルを呼び出して応答を生成します\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # 応答から**で囲まれたキーワードを見つけ出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "```\n",
    "- `GemmaAgent`クラスは、モデルを初期化し、プロンプトをフォーマットし、モデルとのインタラクションを処理します。\n",
    "- `_set_default_tensor_type`コンテキストマネージャは、PyTorchのデフォルトテンソル型を一時的に設定します。\n",
    "- `__call__`メソッドは、観察値（`obs`）に基づいてモデルから応答を生成します。\n",
    "- `_start_session`と`_parse_response`メソッドは、サブクラスによって実装されることを想定しているプレースホルダです。\n",
    "- `interleave_unequal`関数は、2つのリストから要素を交互に並べ、それぞれのリストに多くの要素が含まれている場合は残りの要素で埋めます。\n",
    "### 6. 質問者と回答者エージェント\n",
    "```python\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問を始めましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを予想してください。予想を**で囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"未知のターンタイプ:\", obs.turnType)\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問を始めましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}です。カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"キーワード{obs.keyword}に関する質問です。はいまたはいいえで答えてください。答えを**で囲んでください**。\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # 応答から解析したキーワードに基づいて「はい」または「いいえ」を返します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b31b2c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはい/いいえ質問に応じます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問を始めましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** それではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントはグローバルに定義して、必要なエージェントのみをロードします。\n",
    "# 両方のエージェントを読み込むと、OOM（メモリ不足）になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # エージェントの役割に基づいて処理を呼び出します\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはい/いいえ質問に応じます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問を始めましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** それではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントはグローバルに定義して、必要なエージェントのみをロードします。\n",
    "# 両方のエージェントを読み込むと、OOM（メモリ不足）になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # エージェントの役割に基づいて処理を呼び出します\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7db60fe",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**ChatGPT explanation of the code above**\n",
    "This part of the code is responsible for creating and managing the AI agents used to play the 20 Questions game. It defines how to instantiate the agents and how they should handle the game logic based on the observations provided. Here's a detailed explanation:\n",
    "### 1. Define System Prompt and Few-Shot Examples\n",
    "```python\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "```\n",
    "- `system_prompt`: Provides context to the AI about the 20 Questions game.\n",
    "- `few_shot_examples`: A set of example interactions to help the AI understand the format of the game and how to respond. This is useful for few-shot learning, where the model is given a few examples to generalize from.\n",
    "### 2. Global Agent Variable\n",
    "```python\n",
    "agent = None\n",
    "```\n",
    "- A global variable `agent` is defined to store the current agent. This avoids reloading the agent multiple times, which can lead to out-of-memory (OOM) errors.\n",
    "### 3. Function to Get the Appropriate Agent\n",
    "```python\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "```\n",
    "- `get_agent(name: str)`: A function that returns an instance of the specified agent (`questioner` or `answerer`).\n",
    "    - If the global `agent` variable is `None` and the requested agent type is `questioner`, it creates an instance of `GemmaQuestionerAgent`.\n",
    "    - If the global `agent` variable is `None` and the requested agent type is `answerer`, it creates an instance of `GemmaAnswererAgent`.\n",
    "    - Both agent types are initialized with the `system_prompt` and `few_shot_examples`.\n",
    "    - Ensures that an agent is always returned and asserts that the `agent` is not `None` after the initialization attempt.\n",
    "### 4. Main Agent Function\n",
    "```python\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "- `agent_fn(obs, cfg)`: The main function that handles interactions with the agent based on the current game state (`obs`).\n",
    "    - If `obs.turnType` is `\"ask\"`, it calls the `questioner` agent to generate a question.\n",
    "    - If `obs.turnType` is `\"guess\"`, it calls the `questioner` agent to make a guess.\n",
    "    - If `obs.turnType` is `\"answer\"`, it calls the `answerer` agent to generate an answer.\n",
    "    - The agent is retrieved using `get_agent` based on the required role.\n",
    "    - If the agent's response is `None` or too short, it defaults to returning `\"yes\"`.\n",
    "    - Otherwise, it returns the response generated by the agent.\n",
    "### Summary\n",
    "- This code segment initializes and manages AI agents for playing 20 Questions.\n",
    "- It defines a system prompt and examples for the agent.\n",
    "- It ensures that only one instance of the agent is loaded to prevent memory issues.\n",
    "- The `get_agent` function dynamically initializes the correct agent type.\n",
    "- The `agent_fn` function handles the game logic, invoking the appropriate agent based on the current turn type.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**上記のコードの説明**\n",
    "このコード部分は、20の質問ゲームをプレイするために使用されるAIエージェントの作成と管理を担当しています。エージェントをインスタンス化する方法と、提供された観察に基づいてゲームロジックを処理する方法を定義します。以下は詳細な説明です：\n",
    "### 1. システムプロンプトと少数ショットの例の定義\n",
    "```python\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者からのはい/いいえ質問に応じます。キーワードは特定の人、場所、または物です。\"\n",
    "few_shot_examples = [\n",
    "    \"20の質問を始めましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** それではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "```\n",
    "- `system_prompt`: AIに20の質問ゲームについてのコンテキストを提供します。\n",
    "- `few_shot_examples`: AIがゲームの形式と応答の仕方を理解するのに役立つ一連の例を示します。この少数ショット学習は、モデルがいくつかの例から一般化するのに役立ちます。\n",
    "### 2. グローバルエージェント変数\n",
    "```python\n",
    "agent = None\n",
    "```\n",
    "- 現在のエージェントを保存するためのグローバル変数`agent`が定義されています。これにより、エージェントを複数回ロードせずに済み、メモリ不足（OOM）エラーを回避します。\n",
    "### 3. 適切なエージェントを取得するための関数\n",
    "```python\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "    return agent\n",
    "```\n",
    "- `get_agent(name: str)`: 指定されたエージェント（`questioner`または`answerer`）のインスタンスを返す関数です。\n",
    "    - グローバルな`agent`変数が`None`であり、リクエストされたエージェントタイプが`questioner`の場合、`GemmaQuestionerAgent`のインスタンスを作成します。\n",
    "    - 同様に、`answerer`の場合は`GemmaAnswererAgent`のインスタンスを作成します。\n",
    "    - 両方のエージェントタイプは、`system_prompt`と`few_shot_examples`を使って初期化されます。\n",
    "    - エージェントが必ず返されることを保証し、初期化を試みた後に`agent`が`None`でないことを確認します。\n",
    "### 4. 主なエージェント関数\n",
    "```python\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "- `agent_fn(obs, cfg)`: ゲームの現在の状態（`obs`）に基づいてエージェントとのインタラクションを処理する主関数です。\n",
    "    - `obs.turnType`が`\"ask\"`の場合、`questioner`エージェントを呼び出して質問を生成します。\n",
    "    - `obs.turnType`が`\"guess\"`の場合、`questioner`エージェントを呼び出して予想を生成します。\n",
    "    - `obs.turnType`が`\"answer\"`の場合、`answerer`エージェントを呼び出して答えを生成します。\n",
    "    - 必要に応じて`get_agent`を使用してエージェントを取得します。\n",
    "    - エージェントの応答が`None`または非常に短い場合、\"はい\"を返すようにデフォルト化します。\n",
    "    - そうでなければ、エージェントが生成した応答を返します。\n",
    "### まとめ\n",
    "- このコードセグメントは、20の質問ゲームをプレイするためのAIエージェントを初期化し管理します。\n",
    "- エージェントのためのシステムプロンプトと例を定義します。\n",
    "- メモリの問題を防ぐために、エージェントが1つだけロードされるようにします。\n",
    "- `get_agent`関数は必要なエージェントタイプを動的に初期化します。\n",
    "- `agent_fn`関数は、現在のターンタイプに基づいてエージェントを呼び出すゲームロジックを処理します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c39dc5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ff82eb",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**ChatGPT explanation of the code above**\n",
    "This line of code is a shell command that installs two packages, `pigz` and `pv`, using the `apt` package manager. Here’s a detailed explanation:\n",
    "### Breakdown of the Command\n",
    "```bash\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "#### 1. `!`\n",
    "- The `!` at the beginning of the line indicates that this command should be run in the shell. This is a feature available in Jupyter notebooks that allows you to run shell commands directly from a code cell.\n",
    "#### 2. `apt install pigz pv`\n",
    "- `apt`: This is the command-line interface for the Advanced Package Tool (APT), which is a package management system used by Debian-based Linux distributions (e.g., Ubuntu).\n",
    "- `install`: This subcommand tells `apt` that you want to install one or more packages.\n",
    "- `pigz`: This is the name of the first package to be installed. `pigz` stands for \"parallel implementation of gzip.\" It is a compression tool that uses multiple CPU cores to compress data faster than the traditional `gzip`.\n",
    "- `pv`: This is the name of the second package to be installed. `pv` stands for \"Pipe Viewer.\" It is a terminal-based tool that allows you to monitor the progress of data through a pipeline. It provides a visual display of data transfer progress, throughput rate, and estimated time to completion.\n",
    "#### 3. `> /dev/null`\n",
    "- This part of the command redirects the standard output (stdout) to `/dev/null`. \n",
    "- `/dev/null` is a special file in Unix-like operating systems that discards all data written to it. It is often used to suppress output that you do not want to display.\n",
    "- In this context, `> /dev/null` ensures that any output generated by the `apt install` command (e.g., download progress, installation messages) is not shown in the terminal or Jupyter notebook.\n",
    "### Purpose of the Command\n",
    "The purpose of this command is to install the `pigz` and `pv` packages quietly, without displaying any output. This can be useful in scripts or notebook cells where you want to perform installations without cluttering the output with installation logs.\n",
    "### Practical Uses\n",
    "- **`pigz`**: Useful when you need to compress large files quickly by leveraging multiple CPU cores.\n",
    "- **`pv`**: Helpful for monitoring the progress of data transfers or pipelines, especially in long-running processes. \n",
    "By using this command, you ensure that these tools are available on your system for subsequent use without the installation messages cluttering your output.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**上記のコードの説明**\n",
    "この1行のコードは、`apt`パッケージマネージャを用いて`pigz`と`pv`の2つのパッケージをインストールするシェルコマンドです。以下は詳細な説明です：\n",
    "### コマンドの内訳\n",
    "```bash\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "#### 1. `!`\n",
    "- 行の先頭にある`!`は、このコマンドがシェルで実行されるべきであることを示します。これは Jupyter ノートブックでの特徴で、コードセルから直接シェルコマンドを実行できます。\n",
    "#### 2. `apt install pigz pv`\n",
    "- `apt`: これは、Debian系Linuxディストリビューション（例：Ubuntu）で使用されるパッケージ管理システムであるAdvanced Package Tool（APT）のコマンドラインインターフェースです。\n",
    "- `install`: このサブコマンドは、1つまたは複数のパッケージをインストールしたいことをaptに指示します。\n",
    "- `pigz`: インストールする最初のパッケージの名前です。`pigz`は「gzipの並列実装」を意味します。これは、データを圧縮するためのツールであり、従来の`gzip`よりも速くデータを圧縮できます。\n",
    "- `pv`: インストールする2番目のパッケージの名前です。`pv`は「Pipe Viewer」を意味します。これは、パイプを介るデータの進行状況を監視できる端末ベースのツールです。データ転送の進捗、スループット率、および完了までの推定時間を表示します。\n",
    "#### 3. `> /dev/null`\n",
    "- このコマンドの部分は、標準出力（stdout）を`/dev/null`にリダイレクトします。\n",
    "- `/dev/null`は、Unix系オペレーティングシステムにおける特別なファイルであり、書き込まれたすべてのデータを破棄します。通常、表示したくない出力を抑制するために使用されます。\n",
    "- この文脈では、`> /dev/null`により、`apt install`コマンドによって生成されたいかなる出力（例：ダウンロードの進行状況、インストールメッセージ）が端末やJupyterノートブックに表示されないようにします。\n",
    "### コマンドの目的\n",
    "このコマンドの目的は、`pigz`と`pv`パッケージを静かにインストールすることです。出力を表示せずにインストールを実行することは、スクリプトやノートブックセルにおいて、インストールログで出力が混雑することを防ぐのに便利です。\n",
    "### 実用例\n",
    "- **`pigz`**: 大きなファイルを迅速に圧縮する際に、複数のCPUコアを活用できるので便利です。\n",
    "- **`pv`**: 長時間のプロセスにおいてデータの進行状況を監視する際に役立ちます。\n",
    "このコマンドを利用することで、後の処理のためにこれらのツールをシステムに利用可能にしつつ、インストールメッセージで出力を混乱させないようにしています。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0be7db5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b33e01",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "**ChatGPT explanation of the code above**\n",
    "This shell command creates a compressed tarball (`submission.tar.gz`) using the `tar` command, with the help of `pigz` and `pv` for faster and monitored compression. Here's a detailed breakdown of each part of the command:\n",
    "### Breakdown of the Command\n",
    "```bash\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "#### 1. `!`\n",
    "- Indicates that the following command should be run in the shell, which is a feature available in Jupyter notebooks.\n",
    "#### 2. `tar`\n",
    "- The `tar` command is used to create, maintain, modify, and extract files from a tarball archive.\n",
    "#### 3. `--use-compress-program='pigz --fast --recursive | pv'`\n",
    "- This option specifies the compression program to use, in this case, `pigz` piped through `pv`.\n",
    "- `'pigz --fast --recursive'`: Uses `pigz` (parallel implementation of `gzip`) for compression. The `--fast` flag tells `pigz` to prioritize speed over compression ratio, and `--recursive` ensures it handles directories recursively.\n",
    "- `'| pv'`: Pipes the output of `pigz` to `pv` for monitoring the progress of the compression. `pv` shows the progress of data transfer, which can be useful for long-running processes.\n",
    "#### 4. `-cf submission.tar.gz`\n",
    "- `-c`: Create a new archive.\n",
    "- `-f submission.tar.gz`: Specifies the name of the archive file to create (`submission.tar.gz`).\n",
    "#### 5. `-C /kaggle/working/submission .`\n",
    "- `-C /kaggle/working/submission`: Change to the directory `/kaggle/working/submission` before adding files to the archive.\n",
    "- `.`: Add the current directory (which is now `/kaggle/working/submission` due to the `-C` option) to the archive.\n",
    "#### 6. `-C /kaggle/input/ gemma/pytorch/7b-it-quant/2`\n",
    "- `-C /kaggle/input/`: Change to the directory `/kaggle/input/` before adding files to the archive.\n",
    "- `gemma/pytorch/7b-it-quant/2`: Add the `gemma/pytorch/7b-it-quant/2` directory (relative to `/kaggle/input/`) to the archive.\n",
    "### Purpose of the Command\n",
    "This command is used to create a compressed tarball (`submission.tar.gz`) that includes:\n",
    "1. All files and directories from `/kaggle/working/submission`.\n",
    "2. The `gemma/pytorch/7b-it-quant/2` directory from `/kaggle/input/`.\n",
    "By using `pigz` for compression, the command benefits from faster compression speeds due to parallel processing. `pv` provides a visual progress indicator for the compression process.\n",
    "### Practical Uses\n",
    "- **Compression Speed**: `pigz` speeds up the compression process by utilizing multiple CPU cores.\n",
    "- **Progress Monitoring**: `pv` helps in monitoring the progress of the compression, making it easier to see how long the process might take.\n",
    "- **Archiving Specific Files**: The use of `-C` allows including specific directories in the archive without changing the current working directory, making it flexible to add files from multiple locations.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "**上記のコードの説明**\n",
    "このシェルコマンドは、`pigz`と`pv`の助けを借りて、圧縮されたtarボール（`submission.tar.gz`）を作成します。以下はコマンドの各部分の詳細な内訳です：\n",
    "### コマンドの内訳\n",
    "```bash\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "#### 1. `!`\n",
    "- この文字は、次のコマンドがシェルで実行されるべきであることを示しています。これはJupyterノートブックで利用可能な機能です。\n",
    "#### 2. `tar`\n",
    "- `tar`コマンドは、アーカイブの作成、維持、変更、および抽出を行うために使用されます。\n",
    "#### 3. `--use-compress-program='pigz --fast --recursive | pv'`\n",
    "- このオプションは使用する圧縮プログラムを指定します。この場合は、`pigz`を`pv`でパイプで接続します。\n",
    "- `'pigz --fast --recursive'`: `pigz`（gzipの並列実装）を使用して圧縮します。`--fast`フラグは、圧縮率よりも速度を優先するように指示し、`--recursive`は、ディレクトリを再帰的に処理することを保証します。\n",
    "- `'| pv'`: `pigz`の出力を`pv`にパイプします。これにより圧縮の進行を監視できます。\n",
    "#### 4. `-cf submission.tar.gz`\n",
    "- `-c`: 新しいアーカイブを作成します。\n",
    "- `-f submission.tar.gz`: 作成するアーカイブファイルの名前を指定します（`submission.tar.gz`）。\n",
    "#### 5. `-C /kaggle/working/submission .`\n",
    "- `-C /kaggle/working/submission`: アーカイブにファイルを追加する前にディレクトリを`/kaggle/working/submission`に変更します。\n",
    "- `.`: 現在のディレクトリ（今は`/kaggle/working/submission`）をアーカイブに追加します。\n",
    "#### 6. `-C /kaggle/input/ gemma/pytorch/7b-it-quant/2`\n",
    "- `-C /kaggle/input/`: アーカイブに追加する前に、`/kaggle/input/`ディレクトリに変更します。\n",
    "- `gemma/pytorch/7b-it-quant/2`: `/kaggle/input/`内の`gemma/pytorch/7b-it-quant/2`ディレクトリをアーカイブに追加します。\n",
    "### コマンドの目的\n",
    "このコマンドは、以下を含む圧縮tarボール（`submission.tar.gz`）を作成するために使用されます：\n",
    "1. `/kaggle/working/submission`ディレクトリ内のすべてのファイルとディレクトリ。\n",
    "2. `/kaggle/input/`からの`gemma/pytorch/7b-it-quant/2`ディレクトリ。\n",
    "`pigz`を使用して圧縮することで、マルチCPUコアによるより高速な圧縮を実現し、`pv`が圧縮の進行を可視化します。\n",
    "### 実用例\n",
    "- **圧縮速度**: `pigz`は複数のCPUコアを活用することで、圧縮プロセスを加速させます。\n",
    "- **進行状況の監視**: `pv`は圧縮進行状況を監視し、プロセスの進捗を視覚的に示すのに役立ちます。\n",
    "- **特定のファイルのアーカイブ**: `-C`オプションを使用することで、現在の作業ディレクトリを変更することなく、アーカイブに特定のディレクトリを含めることができ、柔軟に複数の場所からファイルを追加されます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e10b46",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Matin Mahmoudi ✨\n",
    "> \n",
    "> 素晴らしいスタートノートブックで、実装に関する情報が豊富です [@regisvargas](https://www.kaggle.com/regisvargas)。あなたの他の作品もチェックし、すべてに投票しました。すごい、友達！\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Regis Vargas（トピック作成者）\n",
    "> > \n",
    "> > とても励みになるお言葉をありがとうございます。私は多くの困難に直面していますが、日々自分を押し上げる努力をしています。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
