{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7e449c6",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティション向けのエージェント作成プロセスを示しています。本ノートブックの主な目的は、20の質問形式のゲームをプレイするためのAIエージェントの構築と、最終的な提出物（`submission.tar.gz`）を生成することです。\n",
    "\n",
    "### 問題の概要\n",
    "「20の質問」ゲームでは、ある言葉を当てるために限られた質問への答え（「はい」または「いいえ」）に基づいて、情報を収集し、最終的にその言葉を推測することが求められます。ノートブックでは、質問者と回答者の役割を担うエージェントを開発することが核心です。\n",
    "\n",
    "### 使用している手法\n",
    "1. **エージェントの設計**: `Gemma`というライブラリを利用して、カジュアルな言語モデルを用いたエージェントを構築します。これには、予め定義されたプロンプトや例を用いて、エージェントがターンごとに自然な対話を行えるように設定されています。\n",
    "\n",
    "2. **フォーマッティング**: `GemmaFormatter`クラスを使用して、ユーザーとモデルの対話をフォーマットします。ユーザーの質問やモデルの応答を適切に管理し、トークンでの開始・終了マーカーも含まれます。\n",
    "\n",
    "3. **エージェントクラスの実装**: `GemmaQuestionerAgent`および`GemmaAnswererAgent`の二つのクラスを実装し、それぞれ質問を行うため・応答するためのメソッドが定義されています。\n",
    "\n",
    "4. **モデルの初期化**: PyTorchを利用して、Gemmaモデルの構成を取得し、モデルの重みをロードすることで、エージェントが適切に動作する準備を整えます。\n",
    "\n",
    "### 使用しているライブラリ\n",
    "- **PyTorch**: モデルの構築や学習を行うために使用。\n",
    "- **Gemma**: 特に言語モデル用に設計されたライブラリで、カジュアルな応答生成に用いられます。\n",
    "- **sentencepiece**: トークナイザーを用いて、テキストの前処理を支援します。\n",
    "- **immutabledict**: 不変の辞書を提供し、ハッシュ可能なデータ構造を支援します。\n",
    "\n",
    "最終的に、このノートブックは、必要なライブラリをインストールし、エージェントの実装を確立し、テスト後にファイルを圧縮してコンペティションに提出するための準備を整えます。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f61043",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容を基に、機械学習・深層学習の初心者がつまずきそうな専門用語の解説です。重要な用語や一般的な知識は除外しています。\n",
    "\n",
    "1. **Gemma**: 特定の NLP モデルのフレームワークで、質問応答タスク向けに設計されています。このノートブックでは、Gemma の PyTorch 実装を使用して、20の質問ゲームのエージェントを作成しています。\n",
    "\n",
    "2. **エージェント (Agent)**: 状態を持ち、特定のタスクを実行するプログラムやモデルのこと。この場合、質問者と回答者の役割を持つエージェントについてです。\n",
    "\n",
    "3. **few-shot examples**: モデルが学習するために使う、少数の具体的な例のこと。新しいタスクに対するモデルの適応を助けます。このケースでは、20の質問ゲームに関するいくつかの例を示しています。\n",
    "\n",
    "4. **トークナイザー**: 自然言語テキストを、モデルが扱える形式に変換するツールまたはプロセス。トークンとは、単語やサブワードの単位を指します。\n",
    "\n",
    "5. **量子化 (Quantization)**: モデルのサイズを減らすために、フロート型の重みを整数型に変換する技術。メモリ使用量を削減し、推論速度を向上させるために利用されます。\n",
    "\n",
    "6. **ケラフレーム (Checkpoint)**: モデルの学習中の状態を保存したもの。トレーニングの途中から再開できるようにするためのスナップショットです。\n",
    "\n",
    "7. **dyanmic programming**: 計算を分割し、部分問題の結果を記録して再利用するアルゴリズム技法。その性質により、パフォーマンスを向上させることができます。\n",
    "\n",
    "8. **リセット (Reset)**: 状態や条件を初期の状態に戻すプロセスで、ここではエージェントの状態をクリアして新たに開始することを指します。\n",
    "\n",
    "9. **教師なし学習 (Unsupervised Learning)**: 明示的なラベルなしでデータを利用して特徴を抽出する手法。特に、データの構造やパターンを見つける際に重要です。\n",
    "\n",
    "10. **プロンプト (Prompt)**: 言語モデルに対する問い合わせや命令文のこと。特に、このノートブックでは、モデルに対する指示の形式を整える役割を果たします。\n",
    "\n",
    "11. **セッション (Session)**: モデルが特定のタスクを実行するために必要な状態を保持するプロセス。ここでは、質問や回答のやり取りを状態として保持します。\n",
    "\n",
    "12. **擬似コード (Pseudocode)**: プログラミング言語の文法に依存せず、アルゴリズムを表現するための説明的なコードを書くこと。特にこのノートブックでのモデルの動作フローを明確にするために使用されます。\n",
    "\n",
    "これらの解説は、初心者が特に混乱しやすい用語や概念に焦点を当てています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b174dc38",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。このファイルは、右側の**コンペティションに提出**の見出しから直接提出できます。または、ノートブックビューアから*出力*タブをクリックし、`submission.tar.gz`を見つけてダウンロードします。競技のホームページの左上の**Agent Submit**をクリックして、ファイルをアップロードし、提出を完了してください。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6965927",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "# 必要なライブラリをインストールします。-qオプションは出力を抑制します。\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# GemmaのPyTorch実装をGitHubからクローンします。\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# Gemmaのライブラリ用のディレクトリを作成します。\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# Gemmaのクローンからライブラリを移動します。\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T01:03:48.270452Z",
     "iopub.status.busy": "2024-05-24T01:03:48.269116Z",
     "iopub.status.idle": "2024-05-24T01:04:05.548259Z",
     "shell.execute_reply": "2024-05-24T01:04:05.546438Z",
     "shell.execute_reply.started": "2024-05-24T01:03:48.270403Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "# 必要なライブラリをインストールします。-qオプションは出力を抑制します。\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# GemmaのPyTorch実装をGitHubからクローンします。\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# Gemmaのライブラリ用のディレクトリを作成します。\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# Gemmaのクローンからライブラリを移動します。\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995253bd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** このようにシステムパスを設定して、ノートブックとシミュレーション環境の両方でコードが動作するようにします。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルの重みパスを設定します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>' # ターン開始のトークン\n",
    "    _end_token = '<end_of_turn>' # ターン終了のトークン\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt) # ユーザープロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt) # モデルプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\" # ユーザーのターンを開始\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\" # モデルのターンを開始\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\" # ターンを終了\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\" # 状態をリセット\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt) # システムプロンプトを追加\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user') # few-shot例を適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters) # 交互に使用するためのサイクルを作成\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn) # フォーマッタを適用\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"与えられたdtypeをデフォルトのtorch dtypeとして設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float) # デフォルトのdtypeを元に戻す\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device) # デバイスの設定\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples) # フォーマッタのインスタンスを作成\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b() # モデル設定の取得\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\") # トークナイザーのパスを設定\n",
    "        model_config.quant = \"quant\" in variant # 量子化の設定\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config) # モデルのインスタンスを作成\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt') # チェックポイントのパスを設定\n",
    "            model.load_weights(ckpt_path) # 重みをロード\n",
    "            self.model = model.to(self._device).eval() # モデルをデバイスに移動し、評価モードにする\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs) # セッションの開始\n",
    "        prompt = str(self.formatter) # フォーマッタからプロンプトを取得\n",
    "        response = self._call_llm(prompt) # LLMに呼び出す\n",
    "        response = self._parse_response(response, obs) # レスポンスを解析\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device, # 使用するデバイス\n",
    "            output_len=max_new_tokens, # 生成するトークン数\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response) # キーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower() # 小文字に変換\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ] # 不等長のリストを交互に統合する関数\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセット\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を担います。\") # ユーザーの初期メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に統合\n",
    "        self.formatter.apply_turns(turns, start_agent='model') # 過去のターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\") # 質問時\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを推測してください。推測は二重アスタリスクで囲んでください。\") # 推測時\n",
    "        self.formatter.start_model_turn() # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', '')) # 質問を抽出\n",
    "            if match is None:\n",
    "                question = \"人ですか？\" # デフォルト質問\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response) # 推測を解析\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType) # エラー処理\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセット\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を担います。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\") # ユーザーの初期メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に統合\n",
    "        self.formatter.apply_turns(turns, start_agent='user') # 過去のターンを適用\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}のカテゴリ{obs.category}に関するものです。はいかいいえで答え、答案は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn() # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response) # 応答を解析\n",
    "        return 'yes' if 'yes' in answer else 'no' # yesまたはnoを返す\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを思い浮かべ、質問者のはいまたはいいえの質問に対して応答します。キーワードは特定の人、場所、物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を担います。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義します。そうすることで、必要なエージェントだけをロードできます。両方をロードすると、OOM（メモリ不足）が発生する可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs) # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs) # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs) # 回答者エージェントを呼び出す\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\" # デフォルトの応答\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T01:04:05.550627Z",
     "iopub.status.busy": "2024-05-24T01:04:05.550246Z",
     "iopub.status.idle": "2024-05-24T01:04:05.572569Z",
     "shell.execute_reply": "2024-05-24T01:04:05.571192Z",
     "shell.execute_reply.started": "2024-05-24T01:04:05.550594Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** このようにシステムパスを設定して、ノートブックとシミュレーション環境の両方でコードが動作するようにします。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルの重みパスを設定します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトのフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>' # ターン開始のトークン\n",
    "    _end_token = '<end_of_turn>' # ターン終了のトークン\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt) # ユーザープロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt) # モデルプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\" # ユーザーのターンを開始\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\" # モデルのターンを開始\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\" # ターンを終了\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\" # 状態をリセット\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt) # システムプロンプトを追加\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user') # few-shot例を適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters) # 交互に使用するためのサイクルを作成\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn) # フォーマッタを適用\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"与えられたdtypeをデフォルトのtorch dtypeとして設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float) # デフォルトのdtypeを元に戻す\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device) # デバイスの設定\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples) # フォーマッタのインスタンスを作成\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b() # モデル設定の取得\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\") # トークナイザーのパスを設定\n",
    "        model_config.quant = \"quant\" in variant # 量子化の設定\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config) # モデルのインスタンスを作成\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt') # チェックポイントのパスを設定\n",
    "            model.load_weights(ckpt_path) # 重みをロード\n",
    "            self.model = model.to(self._device).eval() # モデルをデバイスに移動し、評価モードにする\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs) # セッションの開始\n",
    "        prompt = str(self.formatter) # フォーマッタからプロンプトを取得\n",
    "        response = self._call_llm(prompt) # LLMに呼び出す\n",
    "        response = self._parse_response(response, obs) # レスポンスを解析\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device, # 使用するデバイス\n",
    "            output_len=max_new_tokens, # 生成するトークン数\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response) # キーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower() # 小文字に変換\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ] # 不等長のリストを交互に統合する関数\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセット\n",
    "        self.formatter.user(\"20の質問をプレイしましょう。あなたは質問者の役割を担います。\") # ユーザーの初期メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に統合\n",
    "        self.formatter.apply_turns(turns, start_agent='model') # 過去のターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいかいいえで答えられる質問をしてください。\") # 質問時\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを推測してください。推測は二重アスタリスクで囲んでください。\") # 推測時\n",
    "        self.formatter.start_model_turn() # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', '')) # 質問を抽出\n",
    "            if match is None:\n",
    "                question = \"人ですか？\" # デフォルト質問\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response) # 推測を解析\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType) # エラー処理\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs) # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset() # フォーマッタをリセット\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは回答者の役割を担います。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\") # ユーザーの初期メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers) # 質問と回答を交互に統合\n",
    "        self.formatter.apply_turns(turns, start_agent='user') # 過去のターンを適用\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}のカテゴリ{obs.category}に関するものです。はいかいいえで答え、答案は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn() # モデルのターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response) # 応答を解析\n",
    "        return 'yes' if 'yes' in answer else 'no' # yesまたはnoを返す\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを思い浮かべ、質問者のはいまたはいいえの質問に対して応答します。キーワードは特定の人、場所、物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をプレイしましょう。あなたは質問者の役割を担います。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解！\",\n",
    "]\n",
    "\n",
    "\n",
    "# **重要:** エージェントをグローバルに定義します。そうすることで、必要なエージェントだけをロードできます。両方をロードすると、OOM（メモリ不足）が発生する可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs) # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs) # 質問者エージェントを呼び出す\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs) # 回答者エージェントを呼び出す\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\" # デフォルトの応答\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03404e8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# pigzとpvをインストールします。出力を抑制します。\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T01:04:05.581818Z",
     "iopub.status.busy": "2024-05-24T01:04:05.580423Z",
     "iopub.status.idle": "2024-05-24T01:04:13.25562Z",
     "shell.execute_reply": "2024-05-24T01:04:13.254253Z",
     "shell.execute_reply.started": "2024-05-24T01:04:05.581766Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pigzとpvをインストールします。出力を抑制します。\n",
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d82402d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# submissionディレクトリ内のすべてのファイルをtar.gz圧縮します。\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-24T01:04:13.257696Z",
     "iopub.status.busy": "2024-05-24T01:04:13.257275Z",
     "iopub.status.idle": "2024-05-24T01:07:18.143328Z",
     "shell.execute_reply": "2024-05-24T01:07:18.140996Z",
     "shell.execute_reply.started": "2024-05-24T01:04:13.257654Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submissionディレクトリ内のすべてのファイルをtar.gz圧縮します。\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
