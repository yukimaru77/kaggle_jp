{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8bee9a6",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのスターターノートブックであり、特に「Rigging」というフレームワークを使用して、LLM（大規模言語モデル）が「20の質問」ゲームをプレイできるようにするために作成されています。\n",
    "\n",
    "### 問題の内容\n",
    "- コンペティションでは、参加者が言語モデルを使用して、限られた質問を通じて指定された単語を当てるゲームを行います。目指すべきは、最小限の質問で答えに到達することであり、そのためには効果的な質問を生成し、的確な回答を引き出すことが求められます。\n",
    "- Notebook内では、`llama3`モデルを`phi3`モデルに切り替えて性能を改善し、LLMと非LLMエージェントを組み合わせた戦略を取っています。Riggingフレームワークを使用することで、異なるモデル間のインタラクションを容易にし、期待される出力を確認しながらLLMにクエリを送信できます。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **Rigging**: Pydantic XMLに基づいた軽量のLLMインタラクションフレームワークで、LLMの生産パイプラインを管理します。質問の生成や回答の取得を柔軟に行うことができます。\n",
    "- **vLLM**: モデルをローカルサービスとしてホストするために使用され、データの取得やモデルのトレーニング環境を整える役割を果たします。\n",
    "- 追加のライブラリとして、`huggingface_hub`や`pandas`が使用され、モデルの重みのダウンロードやデータ管理を効率化しています。\n",
    "\n",
    "### ノートブックのワークフロー\n",
    "1. **初期設定**: Hugging FaceとKaggleのトークンの取得、依存パッケージのインストールが行われる。\n",
    "2. **モデルの準備**: `snapshot_download`を通じてHugging Faceからモデルの重みをダウンロードします。\n",
    "3. **vLLMサーバーの起動**: ローカル環境でモデルをホストし、LLMが質問と回答を行えるようにします。\n",
    "4. **質問者と回答者のエージェント作成**: Pythonクラスを利用して、質問を生成し、回答を処理するロジックが整備されています。\n",
    "5. **キーワードデータフレームの作成**: 公開テーマについてのキーワードが格納され、どの単語に対して質問するかを決定するためのベースになります。\n",
    "6. **最終提出物の作成**: 実行環境の準備が整い、`main.py`を含むZIPファイルが作成されます。\n",
    "\n",
    "このノートブックは、Kaggleのコンペティション用にLLMを効果的に活用できる基盤を提供しており、Riggingを利用することで、競技者が簡単にLLMのインタラクションを構築し、テストし、調整できるように設計されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc11079",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookに関連する専門用語の解説です。特に、初心者がつまずきやすいマイナーな用語や実務を経験していないと馴染みがない用語に焦点を当てました。\n",
    "\n",
    "1. **Rigging**:\n",
    "   - 軽量なLLMインタラクションフレームワークであり、Pydantic XMLに基づいて構築されています。特に、バックエンドのLLMモデルを簡単に入れ替えたり、LLMクエリパイプラインを設計するために使用されます。LLM（大規模言語モデル）とのインタラクションを効率化することを目的としています。\n",
    "\n",
    "2. **Pydantic**:\n",
    "   - Pythonのデータバリデーションと設定管理を行うためのライブラリです。特に、データモデルの定義を簡単に行える機能を提供し、型ヒントを用いたバリデーションが可能です。\n",
    "\n",
    "3. **vLLM**:\n",
    "   - モデルをローカルでホストするためのライブラリであり、特にLLMを効率的にサーバーとして運用するためのツールです。特に非同期的な処理をサポートしており、高速な応答が求められる環境に適しています。\n",
    "\n",
    "4. **Activation-aware Weight Quantization**:\n",
    "   - モデルの軽量化手法の一つで、モデルのパラメータを量子化（サイズを小さくする）する際に、活性化のビット幅も考慮する技術です。この手法は、モデルの推論時のパフォーマンスを改善するために使用されることがあります。\n",
    "\n",
    "5. **XML (eXtensible Markup Language)**:\n",
    "   - データの構造を記述するためのマークアップ言語で、特にデータの交換や保存に用いられます。このノートブックでは、LLMの入力や出力フォーマットとして使用されています。\n",
    "\n",
    "6. **ChatPipeline**:\n",
    "   - Riggingにおいて、LLMとの対話の流れを定義するためのモデルであり、特定のチャットの操作や応答管理を担います。LLMとのインタラクションを効率化するための一連のステップを含んでいます。\n",
    "\n",
    "7. **Subprocess**:\n",
    "   - Pythonで別のプログラムやスクリプトを実行するためのモジュールであり、特に外部コマンドを効率的に呼び出す際に使用されます。ここでは、vLLMサーバーをバックグラウンドで起動するために用いられています。\n",
    "\n",
    "8. **Parsing**:\n",
    "   - 入力されたデータを特定の形式や構造に従って解釈したり変換したりするプロセスです。LLMの出力を適切な形式で抽出するために使用されます。\n",
    "\n",
    "9. **Field Validator**:\n",
    "   - Pydanticでモデルのフィールド値が要件を満たしているかを確認するためのメソッドで、データの整合性を保証する役割を担います。特にそのモデル内で定義されます。\n",
    "\n",
    "10. **Observation**:\n",
    "    - ゲームの進行状況や状態を保持するためのデータ構造で、質問や回答、推測をトラックするのに使用されます。ゲームのロジックを管理するために重要な役割を果たします。\n",
    "\n",
    "これらの用語は、Notebookでの具体的な実例やコードに関連しているため、より理解を深めるための助けとなるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b96f70",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# LLM 20 Questions Starter with Rigging [Phi3]\n",
    "\n",
    "## This notebook is a revised version of an earlier notebook. We have replaced the `llama3` model with the `phi3` model as requested by @bhanupm. You can find the original notebook [here](https://www.kaggle.com/code/robikscube/phi3-intro-to-rigging-for-llm-20-questions/).\n",
    "\n",
    "In my early testing the llama3 version performs better, but this might be improved with better prompting.\n",
    "\n",
    "This starter notebook shows how the python package rigging can be used to create a baseline submission for the competition. This setup uses the `phi3` quantized model using vLLM.\n",
    "\n",
    "## Update **June 10, 2024**\n",
    "- Updated code to work with rigging 2.0\n",
    "- Including non-llm question asking agent that leverages the known keywords **note this won't work well on the private leaderboard**. Answer agent uses LLM via rigging.\n",
    "\n",
    "## What is Rigging?\n",
    "\n",
    "Rigging is a lightweight LLM interaction framework built on Pydantic XML. The goal is to make leveraging LLMs in production pipelines as simple and effictive as possible. Rigging is perfectly fit for the 20 questions tasks as it can:\n",
    "1. Easily handle swapping out different backend LLM models.\n",
    "2. Design LLM querying pipelines that check for expected outputs and retry until successful.\n",
    "3. Modern python with type hints, async support, pydantic validation, serialization, etc.\n",
    "\n",
    "Star the repo here: https://github.com/dreadnode/rigging\n",
    "Read the documentation here: https://rigging.dreadnode.io/\n",
    "\n",
    "Rigging is built and maintained by [dreadnode](https://www.dreadnode.io/) where we use it daily for our work.\n",
    "\n",
    "An example rigging pipeline might look like this:\n",
    "```{python}\n",
    "chat = rg.get_generator('gpt-4o') \\\n",
    "    .chat(f\"Provide me the names of all the countries in South America that start with the letter A {Answer.xml_tags()} tags.\") \\\n",
    "    .until_parsed_as(Answer) \\\n",
    "    .run() \n",
    "```\n",
    "\n",
    "Generators can be created seemlessly with most major LLM apis, so long as you have api keys saved as env variables.\n",
    "```\n",
    "export OPENAI_API_KEY=...\n",
    "export TOGETHER_API_KEY=...\n",
    "export TOGETHERAI_API_KEY=...\n",
    "export MISTRAL_API_KEY=...\n",
    "export ANTHROPIC_API_KEY=...\n",
    "```\n",
    "\n",
    "For this competition we must run our model locally, luckily rigging has support to run models using transformers on the back end.\n",
    "\n",
    "# Setup\n",
    "\n",
    "Below is some of the setup for this notebook. Where we will:\n",
    "- Load secret tokens for huggingface and kaggle (optional)\n",
    "- Install required packages\n",
    "- Create a helper utility script for testing our vLLM server\n",
    "\n",
    "This notebooks uses some hidden tokens using kaggle's secrets. This is optional and not required to run the code.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LLM 20 Questions スターター with Rigging [Phi3]\n",
    "\n",
    "## このノートブックは、以前のノートブックの改訂版です。`llama3`モデルを@bhanupmのリクエストに応じて`phi3`モデルに置き換えました。元のノートブックは[こちら](https://www.kaggle.com/code/robikscube/phi3-intro-to-rigging-for-llm-20-questions/)で見つけることができます。\n",
    "\n",
    "初期のテストでは、llama3バージョンがより良いパフォーマンスを発揮しましたが、より良いプロンプトによって改善される可能性があります。\n",
    "\n",
    "このスターターノートブックでは、コンペティションのためのベースライン提出を作成する方法を示します。このセットアップは、vLLMを使用した`phi3`量子化モデルを利用しています。\n",
    "\n",
    "## 更新 **2024年6月10日**\n",
    "- rigging 2.0に対応するようにコードを更新しました\n",
    "- Known keywordsを活用する非LLM質問エージェントを含めました **これはプライベートリーダーボードではうまく機能しません**。回答エージェントはriggingを介してLLMを使用します。\n",
    "\n",
    "## Riggingとは？\n",
    "\n",
    "Riggingは、Pydantic XMLに基づいて構築された軽量なLLMインタラクションフレームワークです。目的は、LLMを生産パイプラインでできるだけ簡単かつ効果的に活用できるようにすることです。Riggingは20の質問タスクに完全に適しており、以下のことが可能です：\n",
    "1. 異なるバックエンドLLMモデルを簡単に入れ替えることができます。\n",
    "2. 期待される出力をチェックし、成功するまで繰り返すLLMクエリパイプラインを設計できます。\n",
    "3. 現代のPythonで、型ヒント、非同期サポート、pydanticバリデーション、シリアル化などをサポートしています。\n",
    "\n",
    "リポジトリをこちらでスターしておいてください: https://github.com/dreadnode/rigging\n",
    "ドキュメントをこちらでお読みください: https://rigging.dreadnode.io/\n",
    "\n",
    "Riggingは[dreadnode](https://www.dreadnode.io/)によって構築され、維持されています。私たちは日常的にそれを使用しています。\n",
    "\n",
    "例として、以下のようなriggingパイプラインが考えられます：\n",
    "```{python}\n",
    "chat = rg.get_generator('gpt-4o') \\\n",
    "    .chat(f\"Provide me the names of all the countries in South America that start with the letter A {Answer.xml_tags()} tags.\") \\\n",
    "    .until_parsed_as(Answer) \\\n",
    "    .run() \n",
    "```\n",
    "\n",
    "生成器は、大多数の主要なLLM APIとシームレスに作成できます。APIキーが環境変数として保存されている限り、問題ありません。\n",
    "```\n",
    "export OPENAI_API_KEY=...\n",
    "export TOGETHER_API_KEY=...\n",
    "export TOGETHERAI_API_KEY=...\n",
    "export MISTRAL_API_KEY=...\n",
    "export ANTHROPIC_API_KEY=...\n",
    "```\n",
    "\n",
    "このコンペティションではモデルをローカルで実行する必要があるため、幸運にもriggingはトランスフォーマーをバックエンドに使用してモデルを実行するサポートがあります。\n",
    "\n",
    "# セットアップ\n",
    "\n",
    "以下は、このノートブックのいくつかのセットアップです。ここでは以下のことを行います：\n",
    "- Hugging FaceとKaggleのための秘密トークンをロードします（オプション）\n",
    "- 必要なパッケージをインストールします\n",
    "- vLLMサーバーをテストするためのヘルパーユーティリティスクリプトを作成します\n",
    "\n",
    "このノートブックは、Kaggleの秘密を使用していくつかの隠されたトークンを利用します。これはオプションであり、コードを実行するためには必要ありません。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f2d1a9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:54:57.226986Z",
     "iopub.status.busy": "2024-06-25T14:54:57.226154Z",
     "iopub.status.idle": "2024-06-25T14:54:57.336873Z",
     "shell.execute_reply": "2024-06-25T14:54:57.336172Z",
     "shell.execute_reply.started": "2024-06-25T14:54:57.22696Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b1939",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Pip install\n",
    "We will install:\n",
    "- [rigging](https://github.com/dreadnode/rigging) Used to created our LLM pipelines for the competition.\n",
    "- [vLLM](https://github.com/vllm-project/vllm) For hosting our model locally as an independent service.\n",
    "\n",
    "We also use [uv](https://github.com/astral-sh/uv) which allows us to install these packages much faster.\n",
    "\n",
    "**Note:** We are installing these packages to the `/kaggle/tmp/lib` directory. We only do this for the purposes of the competition setup, where we will later need to include the files from this path in our submission zip. We also install the vllm dependencies to `/kaggle/tmp/srvlib`.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## パッケージのインストール\n",
    "私たちは以下をインストールします：\n",
    "- [rigging](https://github.com/dreadnode/rigging) コンペティションのためのLLMパイプラインを作成するために使用します。\n",
    "- [vLLM](https://github.com/vllm-project/vllm) モデルをローカルで独立したサービスとしてホストするために使用します。\n",
    "\n",
    "また、[uv](https://github.com/astral-sh/uv)を使用しており、これによりこれらのパッケージをより迅速にインストールできます。\n",
    "\n",
    "**注意:** これらのパッケージは`/kaggle/tmp/lib`ディレクトリにインストールされています。これはコンペティションのセットアップ目的のために行い、このパス内のファイルを後で提出用のZIPに含める必要があります。また、vllmの依存パッケージも`/kaggle/tmp/srvlib`にインストールします。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e225eff",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Dependencies (uv for speed)\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 依存関係（速度向上のためのuv）\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T14:54:57.338592Z",
     "iopub.status.busy": "2024-06-25T14:54:57.338297Z",
     "iopub.status.idle": "2024-06-25T14:56:11.727967Z",
     "shell.execute_reply": "2024-06-25T14:56:11.726422Z",
     "shell.execute_reply.started": "2024-06-25T14:54:57.33855Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 依存関係（速度向上のためのuv）\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4822c49d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Download the LLM Locally\n",
    "\n",
    "Because this competition requires us to submit our code with model weights, we will first download the model weights using `snapshot_download` from huggingface.\n",
    "\n",
    "We are going to download the `solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ`. This is a Activation-aware Weight Quantization version of the model that is small enough to run in the competition requirements.\n",
    "\n",
    "**Note**: When using rigging in a normal situation this step would not be necessary, but we are downloading the weights seperately so that we can include them in our submission zip for the competition.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LLMをローカルにダウンロードする\n",
    "\n",
    "このコンペティションでは、モデルの重みを含むコードを提出する必要があるため、まず`snapshot_download`を使用してHugging Faceからモデルの重みをダウンロードします。\n",
    "\n",
    "`solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ`をダウンロードします。これは、コンペティション要件に十分に小さいActivation-aware Weight Quantizationバージョンのモデルです。\n",
    "\n",
    "**注意:** 通常の状況でriggingを使用する場合、このステップは必要ありませんが、コンペティション用に提出物に含めるために重みを別々にダウンロードしています。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d4da9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Download the model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"rhysjones/Phi-3-mini-mango-1-llamafied\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# モデルをダウンロードする\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"rhysjones/Phi-3-mini-mango-1-llamafied\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-25T14:56:40.058151Z",
     "iopub.status.busy": "2024-06-25T14:56:40.057745Z",
     "iopub.status.idle": "2024-06-25T14:57:26.232868Z",
     "shell.execute_reply": "2024-06-25T14:57:26.231829Z",
     "shell.execute_reply.started": "2024-06-25T14:56:40.05811Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルをダウンロードする\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"rhysjones/Phi-3-mini-mango-1-llamafied\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb74730c",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "We can see the model weights are stored in `/kaggle/tmp/model/`\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "モデルの重みが`/kaggle/tmp/model/`に保存されているのが確認できます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37355f45",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!ls -l /kaggle/tmp/model\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!ls -l /kaggle/tmp/model\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:43.927105Z",
     "iopub.status.busy": "2024-06-25T14:57:43.926241Z",
     "iopub.status.idle": "2024-06-25T14:57:44.886775Z",
     "shell.execute_reply": "2024-06-25T14:57:44.885813Z",
     "shell.execute_reply.started": "2024-06-25T14:57:43.927071Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /kaggle/tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8226f1ea",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Helper Utilities File\n",
    "\n",
    "These are helper functions we will use for starting our vLLM server.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ヘルパーユーティリティファイル\n",
    "\n",
    "これらは、vLLMサーバーを起動するために使用するヘルパー関数です。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130a557",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile util.py\n",
    "\n",
    "# Helpers for starting the vLLM server\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)\n",
    "            result = sock.connect_ex(('localhost', port))\n",
    "            if result == 0:\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60, debug: bool = False\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"Port {port} is already open\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},\n",
    "        stdout=subprocess.DEVNULL if not debug else None,\n",
    "        stderr=subprocess.DEVNULL if not debug else None,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen\n",
    "        time.sleep(1)\n",
    "    \n",
    "    popen.terminate()\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile util.py\n",
    "\n",
    "# vLLMサーバーを起動するためのヘルパー\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)\n",
    "            result = sock.connect_ex(('localhost', port))\n",
    "            if result == 0:\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60, debug: bool = False\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"ポート {port} はすでに開いています\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},\n",
    "        stdout=subprocess.DEVNULL if not debug else None,\n",
    "        stderr=subprocess.DEVNULL if not debug else None,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen\n",
    "        time.sleep(1)\n",
    "    \n",
    "    popen.terminate()\n",
    "    raise Exception(f\"プロセスが {port} を {timeout} 秒以内に開きませんでした。\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:49.332077Z",
     "iopub.status.busy": "2024-06-25T14:57:49.331727Z",
     "iopub.status.idle": "2024-06-25T14:57:49.339771Z",
     "shell.execute_reply": "2024-06-25T14:57:49.338936Z",
     "shell.execute_reply.started": "2024-06-25T14:57:49.332048Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# vLLMサーバーを起動するためのヘルパー\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)\n",
    "            result = sock.connect_ex(('localhost', port))\n",
    "            if result == 0:\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60, debug: bool = False\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"ポート {port} はすでに開いています\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},\n",
    "        stdout=subprocess.DEVNULL if not debug else None,\n",
    "        stderr=subprocess.DEVNULL if not debug else None,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen\n",
    "        time.sleep(1)\n",
    "    \n",
    "    popen.terminate()\n",
    "    raise Exception(f\"プロセスが {port} を {timeout} 秒以内に開きませんでした。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90208c82",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Starting up our vLLM server for testing\n",
    "\n",
    "Our model will be hosted using a vLLM server. Below we will start up the notebook so we can understand how it works in the kaggle environment.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# テストのためにvLLMサーバーを起動する\n",
    "\n",
    "私たちのモデルは、vLLMサーバーを使用してホストされます。以下では、Kaggle環境での動作を理解するためにノートブックを起動します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce037302",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# vLLM paths and settings.\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "util = importlib.reload(util)\n",
    "\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "assert g_srvlib_path.exists()\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "assert g_model_path.exists()\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# vLLMのパスと設定。\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "util = importlib.reload(util)\n",
    "\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "assert g_srvlib_path.exists()\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "assert g_model_path.exists()\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T14:57:51.596509Z",
     "iopub.status.busy": "2024-06-25T14:57:51.596145Z",
     "iopub.status.idle": "2024-06-25T14:57:51.603805Z",
     "shell.execute_reply": "2024-06-25T14:57:51.602922Z",
     "shell.execute_reply.started": "2024-06-25T14:57:51.596481Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLMのパスと設定。\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "util = importlib.reload(util)\n",
    "\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "assert g_srvlib_path.exists()\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "assert g_model_path.exists()\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f54c6ab",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Run the vLLM server using subprocess\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),\n",
    "    \"--port\", str(g_vllm_port),\n",
    "    \"--served-model-name\", g_vllm_model_name,\n",
    "    \"--dtype=half\"\n",
    "],\n",
    "    g_vllm_port,\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"vLLM Started\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# サブプロセスを使用してvLLMサーバーを実行する\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),\n",
    "    \"--port\", str(g_vllm_port),\n",
    "    \"--served-model-name\", g_vllm_model_name,\n",
    "    \"--dtype=half\"\n",
    "],\n",
    "    g_vllm_port,\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"vLLMが起動しました\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:31.209345Z",
     "iopub.status.busy": "2024-06-25T15:05:31.208642Z",
     "iopub.status.idle": "2024-06-25T15:05:49.239702Z",
     "shell.execute_reply": "2024-06-25T15:05:49.238752Z",
     "shell.execute_reply.started": "2024-06-25T15:05:31.209307Z"
    }
   },
   "outputs": [],
   "source": [
    "# サブプロセスを使用してvLLMサーバーを実行する\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),\n",
    "    \"--port\", str(g_vllm_port),\n",
    "    \"--served-model-name\", g_vllm_model_name,\n",
    "    \"--dtype=half\"\n",
    "],\n",
    "    g_vllm_port,\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"vLLMが起動しました\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36eadd7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "We can see that the llama3 model is loaded onto the 1st Tesla T4 GPU.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "llama3モデルが最初のTesla T4 GPUにロードされているのが確認できます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0072e7fa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!nvidia-smi\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:54.05533Z",
     "iopub.status.busy": "2024-06-25T15:05:54.05498Z",
     "iopub.status.idle": "2024-06-25T15:05:55.109773Z",
     "shell.execute_reply": "2024-06-25T15:05:55.108601Z",
     "shell.execute_reply.started": "2024-06-25T15:05:54.055302Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefc201d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Validating the Model\n",
    "\n",
    "Lets create our first rigging generator. In rigging the generators are the foundation for creating powerful LLM pipelines.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## モデルの検証\n",
    "\n",
    "最初のrigging生成器を作成します。riggingにおいて、生成器は強力なLLMパイプラインを作成する基本です。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc66bfa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Connect with Rigging\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    ")\n",
    "\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[Rigging Chat]')\n",
    "print(type(answer), answer)\n",
    "\n",
    "print()\n",
    "print('[LLM Response Only]')\n",
    "print(type(answer.last), answer.last)\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content\n",
    "print('[LLM Response as a String]')\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Riggingとの接続\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")\n",
    "\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[Rigging Chat]')\n",
    "print(type(answer), answer)\n",
    "\n",
    "print()\n",
    "print('[LLMのレスポンスのみ]')\n",
    "print(type(answer.last), answer.last)\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content\n",
    "print('[LLMレスポンスを文字列として]')\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:05:57.177132Z",
     "iopub.status.busy": "2024-06-25T15:05:57.176719Z",
     "iopub.status.idle": "2024-06-25T15:05:57.605963Z",
     "shell.execute_reply": "2024-06-25T15:05:57.604998Z",
     "shell.execute_reply.started": "2024-06-25T15:05:57.177094Z"
    }
   },
   "outputs": [],
   "source": [
    "# Riggingとの接続\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")\n",
    "\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[Rigging Chat]')\n",
    "print(type(answer), answer)\n",
    "\n",
    "print()\n",
    "print('[LLMのレスポンスのみ]')\n",
    "print(type(answer.last), answer.last)\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content\n",
    "print('[LLMレスポンスを文字列として]')\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bc5371",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Converting results to pandas dataframe\n",
    "\n",
    "Using the `to_df()` method we can easily convert the chat history to a pandas dataframe.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 結果をpandasデータフレームに変換する\n",
    "\n",
    "`to_df()`メソッドを使用することで、チャット履歴を簡単にpandasデータフレームに変換できます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95a5465",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "answer.to_df()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "answer.to_df()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:01.007133Z",
     "iopub.status.busy": "2024-06-25T15:06:01.00677Z",
     "iopub.status.idle": "2024-06-25T15:06:01.031837Z",
     "shell.execute_reply": "2024-06-25T15:06:01.030907Z",
     "shell.execute_reply.started": "2024-06-25T15:06:01.007105Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9bc921",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Changing Model Parameters\n",
    "\n",
    "Much like database connection strings, Rigging generators can be represented as strings which define what provider, model, API key, generation params, etc. should be used. They are formatted as follows:\n",
    "\n",
    "```\n",
    "<provider>!<model>,<**kwargs>\n",
    "```\n",
    "\n",
    "As an example, here we load the model with additional parameters:\n",
    "- temperature=0.9\n",
    "- max_tokens=512\n",
    "\n",
    "You can read more about these in the docs here: https://rigging.dreadnode.io/topics/generators/#overload-generation-params\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## モデルパラメーターの変更\n",
    "\n",
    "データベース接続文字列と同様に、Rigging生成器は使用するプロバイダー、モデル、APIキー、生成パラメータなどを定義する文字列として表現できます。それらは以下の形式にフォーマットされます：\n",
    "\n",
    "```\n",
    "<provider>!<model>,<**kwargs>\n",
    "```\n",
    "\n",
    "例えば、以下のように追加のパラメータを使用してモデルをロードします：\n",
    "- temperature=0.9\n",
    "- max_tokens=512\n",
    "\n",
    "これらについての詳細は、ドキュメントのこちらをお読みください: https://rigging.dreadnode.io/topics/generators/#overload-generation-params\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7517a60",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=0.9,max_tokens=512,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding,\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=0.9,max_tokens=512,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:01.71593Z",
     "iopub.status.busy": "2024-06-25T15:06:01.715601Z",
     "iopub.status.idle": "2024-06-25T15:06:01.72054Z",
     "shell.execute_reply": "2024-06-25T15:06:01.719689Z",
     "shell.execute_reply.started": "2024-06-25T15:06:01.715906Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=0.9,max_tokens=512,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1ceaaa",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "Alternatively we can set these parameters using the `rg.GenerateParams` class. This class allows you to set various model parameters:\n",
    "\n",
    "```\n",
    "rg.GenerateParams(\n",
    "    *,\n",
    "    temperature: float | None = None,\n",
    "    max_tokens: int | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    stop: list[str] | None = None,\n",
    "    presence_penalty: float | None = None,\n",
    "    frequency_penalty: float | None = None,\n",
    "    api_base: str | None = None,\n",
    "    timeout: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    extra: dict[str, typing.Any] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "https://rigging.dreadnode.io/api/generator/#rigging.generator.GenerateParams\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "また、`rg.GenerateParams`クラスを使用してこれらのパラメータを設定することもできます。このクラスを使用すると、さまざまなモデルパラメータを設定できます：\n",
    "\n",
    "```\n",
    "rg.GenerateParams(\n",
    "    *,\n",
    "    temperature: float | None = None,\n",
    "    max_tokens: int | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    stop: list[str] | None = None,\n",
    "    presence_penalty: float | None = None,\n",
    "    frequency_penalty: float | None = None,\n",
    "    api_base: str | None = None,\n",
    "    timeout: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    extra: dict[str, typing.Any] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "https://rigging.dreadnode.io/api/generator/#rigging.generator.GenerateParams\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277e86bd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "rg_params = rg.GenerateParams(\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 512,\n",
    ")\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "rg_params = rg.GenerateParams(\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 512,\n",
    ")\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:03.179319Z",
     "iopub.status.busy": "2024-06-25T15:06:03.178702Z",
     "iopub.status.idle": "2024-06-25T15:06:04.437702Z",
     "shell.execute_reply": "2024-06-25T15:06:04.43678Z",
     "shell.execute_reply.started": "2024-06-25T15:06:03.179289Z"
    }
   },
   "outputs": [],
   "source": [
    "rg_params = rg.GenerateParams(\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 512,\n",
    ")\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36e6fc",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "Or parameters can be set within the chain using params.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "また、パラメータはチェーン内でparamsを使用して設定することもできます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a56d0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "base_chat = generator.chat() # No params set\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature = 0.9, max_tokens = 512) \\\n",
    "    .run()\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "base_chat = generator.chat() # パラメータは設定されていません\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature = 0.9, max_tokens = 512) \\\n",
    "    .run()\n",
    "print(answer.last.content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:06:05.457476Z",
     "iopub.status.busy": "2024-06-25T15:06:05.456774Z",
     "iopub.status.idle": "2024-06-25T15:06:06.532498Z",
     "shell.execute_reply": "2024-06-25T15:06:06.531524Z",
     "shell.execute_reply.started": "2024-06-25T15:06:05.457446Z"
    }
   },
   "outputs": [],
   "source": [
    "base_chat = generator.chat() # パラメータは設定されていません\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature = 0.9, max_tokens = 512) \\\n",
    "    .run()\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b85d09",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Parsed outputs example\n",
    "\n",
    "Next we will create a pipeline where we:\n",
    "1. Create a rigging Model called `Answer`. This explains the expected output that we will parse from the model results.\n",
    "    - We will add some validators to this that will ensure the output is either `yes` or `no`\n",
    "    - This is fully customizable.\n",
    "    - Here `validate_content` is ensuring that our response conforms to the expected output (lowercase and starts with \"yes\" or \"no\")\n",
    "2. We can use the `Answer.xml_example()` in our prompt to let the LLM know how we expect the output to look.\n",
    "3. Later on we will use `.until_parsed_as(Answer)` to ensure the LLM output is extracted as defined here.\n",
    "\n",
    "**Note** `until_parsed_as()` can take a `max_rounds` parameter, which by default is 5.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# パースされた出力の例\n",
    "\n",
    "次に、以下の手順を実行するパイプラインを作成します：\n",
    "1. `Answer`というriggingモデルを作成します。これにより、モデルの結果からパースする期待出力が説明されます。\n",
    "    - ここで、出力が`yes`または`no`であることを確実にするために、いくつかのバリデーターを追加します。\n",
    "    - これは完全にカスタマイズ可能です。\n",
    "    - ここで`validate_content`は、私たちのレスポンスが期待される出力（小文字で始まり、\"yes\"または\"no\"で始まる）に準拠していることを確認します。\n",
    "2. 私たちは、プロンプトに`Answer.xml_example()`を使用して、LLMに出力がどのように見えるべきかを知らせます。\n",
    "3. 後で`.until_parsed_as(Answer)`を使用して、LLMの出力がここで定義されたように抽出されることを保証します。\n",
    "\n",
    "**注意** `until_parsed_as()`は、デフォルトで5の`max_rounds`パラメータを取ることができます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81df6b10",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です、'yes'または'no'である必要があります\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:21.347929Z",
     "iopub.status.busy": "2024-06-25T15:11:21.347101Z",
     "iopub.status.idle": "2024-06-25T15:11:21.356312Z",
     "shell.execute_reply": "2024-06-25T15:11:21.35534Z",
     "shell.execute_reply.started": "2024-06-25T15:11:21.347896Z"
    }
   },
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です、'yes'または'no'である必要があります\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1f81b2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Lets see what the xml example looks like for this we can use this in our prompt\n",
    "Answer.xml_example()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# このxmlの例がプロンプト内でどのように表示されるかを見てみましょう\n",
    "Answer.xml_example()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:21.731091Z",
     "iopub.status.busy": "2024-06-25T15:11:21.730735Z",
     "iopub.status.idle": "2024-06-25T15:11:21.736827Z",
     "shell.execute_reply": "2024-06-25T15:11:21.735955Z",
     "shell.execute_reply.started": "2024-06-25T15:11:21.731064Z"
    }
   },
   "outputs": [],
   "source": [
    "# このxmlの例がプロンプト内でどのように表示されるかを見てみましょう\n",
    "Answer.xml_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11ba64a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    "#     \"stop=<|eot_id|>\" # Llama requires some hand holding,\n",
    ")\n",
    "\n",
    "keyword='Tom Hanks'\n",
    "category='Famous Person'\n",
    "last_question='Is it a famous person?'\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "            The secret word for this game is \"{keyword}\" [{category}]\n",
    "\n",
    "            You are currently answering a question about the word above.\n",
    "\n",
    "            The next question is \"{last_question}\".\n",
    "\n",
    "            Answer the yes/no question above and place it in the following format:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - Your response should be accurate given the keyword above\n",
    "            - Always answer with \"yes\" or \"no\"\n",
    "\n",
    "            What is the answer?\n",
    "\"\"\"\n",
    "\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)\n",
    "    .until_parsed_as(Answer, max_rounds=50)\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print('=== Full Chat ===')\n",
    "print(chat)\n",
    "\n",
    "print()\n",
    "print('=== LLM Response Only ===')\n",
    "print(chat.last)\n",
    "\n",
    "print()\n",
    "print('=== Parsed Answer ===')\n",
    "print(chat.last.parse(Answer).content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    "#     \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")\n",
    "\n",
    "keyword='Tom Hanks'\n",
    "category='有名な人'\n",
    "last_question='それは有名な人ですか？'\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "            このゲームの秘密の言葉は \"{keyword}\" です [{category}]\n",
    "\n",
    "            現在、上記の言葉についての質問に答えています。\n",
    "\n",
    "            次の質問は \"{last_question}\" です。\n",
    "\n",
    "            上記のはい/いいえの質問に答え、以下のフォーマットにして下さい：\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - あなたのレスポンスは、上記のキーワードに対して正確であるべきです\n",
    "            - 常に \"yes\" または \"no\" で回答してください\n",
    "\n",
    "            答えは何ですか？\n",
    "\"\"\"\n",
    "\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)\n",
    "    .until_parsed_as(Answer, max_rounds=50)\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print('=== フルチャット ===')\n",
    "print(chat)\n",
    "\n",
    "print()\n",
    "print('=== LLMのレスポンスのみ ===')\n",
    "print(chat.last)\n",
    "\n",
    "print()\n",
    "print('=== パースされた回答 ===')\n",
    "print(chat.last.parse(Answer).content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:22.351891Z",
     "iopub.status.busy": "2024-06-25T15:11:22.351188Z",
     "iopub.status.idle": "2024-06-25T15:11:22.689862Z",
     "shell.execute_reply": "2024-06-25T15:11:22.688976Z",
     "shell.execute_reply.started": "2024-06-25T15:11:22.351859Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    "#     \"stop=<|eot_id|>\" # Llamaには少し手助けが必要です\n",
    ")\n",
    "\n",
    "keyword='Tom Hanks'\n",
    "category='有名な人'\n",
    "last_question='それは有名な人ですか？'\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "            このゲームの秘密の言葉は \"{keyword}\" です [{category}]\n",
    "\n",
    "            現在、上記の言葉についての質問に答えています。\n",
    "\n",
    "            次の質問は \"{last_question}\" です。\n",
    "\n",
    "            上記のはい/いいえの質問に答え、以下のフォーマットにして下さい：\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - あなたのレスポンスは、上記のキーワードに対して正確であるべきです\n",
    "            - 常に \"yes\" または \"no\" で回答してください\n",
    "\n",
    "            答えは何ですか？\n",
    "\"\"\"\n",
    "\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)\n",
    "    .until_parsed_as(Answer, max_rounds=50)\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print('=== フルチャット ===')\n",
    "print(chat)\n",
    "\n",
    "print()\n",
    "print('=== LLMのレスポンスのみ ===')\n",
    "print(chat.last)\n",
    "\n",
    "print()\n",
    "print('=== パースされた回答 ===')\n",
    "print(chat.last.parse(Answer).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f49a9d7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Create an example Questioner Chat Pipeline with Rigging\n",
    "\n",
    "Next lets create the questioner pipeline that will attempt to help determine what the keyword might be.\n",
    "\n",
    "First lets create a `Question` object which we will use to parse our output.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# Riggingで質問者チャットパイプラインの例を作成する\n",
    "\n",
    "次に、キーワードが何であるかを特定するために役立つ質問者パイプラインを作成します。\n",
    "\n",
    "まず、出力をパースするために使用する`Question`オブジェクトを作成します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3da22f7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**質問**\").to_pretty_xml()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:31.378829Z",
     "iopub.status.busy": "2024-06-25T15:11:31.378051Z",
     "iopub.status.idle": "2024-06-25T15:11:31.388132Z",
     "shell.execute_reply": "2024-06-25T15:11:31.387242Z",
     "shell.execute_reply.started": "2024-06-25T15:11:31.378787Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**質問**\").to_pretty_xml()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b238799",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "    You are currently asking the next question.\n",
    "\n",
    "    question and place it in the following format:\n",
    "    {Question.xml_example()}\n",
    "\n",
    "    - Your response should be a focused question which will gather the most information\n",
    "    - Start general with your questions\n",
    "    - Always try to bisect the remaining search space\n",
    "    - Pay attention to previous questions and answers\n",
    "\n",
    "    What is your next question?\n",
    "    \"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)\n",
    ".run()\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの優れたプレイヤーです。あなたは正確で、集中しており、構造的なアプローチをとります。あなたは有用な質問を作成し、質問に対する回答を行ったり、キーワードに関する推測を行ったりします。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "    あなたは現在次の質問をしています。\n",
    "\n",
    "    質問を以下のフォーマットにして下さい：\n",
    "    {Question.xml_example()}\n",
    "\n",
    "    - あなたのレスポンスは、最も多くの情報を集める焦点を絞った質問であるべきです\n",
    "    - 質問は一般的なものから始めてください\n",
    "    - 残りの探索空間を二分するようにして下さい\n",
    "    - 以前の質問と回答に注意してください\n",
    "\n",
    "    あなたの次の質問は何ですか？\n",
    "    \"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)\n",
    ".run()\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:32.156494Z",
     "iopub.status.busy": "2024-06-25T15:11:32.156098Z",
     "iopub.status.idle": "2024-06-25T15:11:36.412491Z",
     "shell.execute_reply": "2024-06-25T15:11:36.41159Z",
     "shell.execute_reply.started": "2024-06-25T15:11:32.156464Z"
    }
   },
   "outputs": [],
   "source": [
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの優れたプレイヤーです。あなたは正確で、集中しており、構造的なアプローチをとります。あなたは有用な質問を作成し、質問に対する回答を行ったり、キーワードに関する推測を行ったりします。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "    あなたは現在次の質問をしています。\n",
    "\n",
    "    質問を以下のフォーマットにして下さい：\n",
    "    {Question.xml_example()}\n",
    "\n",
    "    - あなたのレスポンスは、最も多くの情報を集める焦点を絞った質問であるべきです\n",
    "    - 質問は一般的なものから始めてください\n",
    "    - 残りの探索空間を二分するようにして下さい\n",
    "    - 以前の質問と回答に注意してください\n",
    "\n",
    "    あなたの次の質問は何ですか？\n",
    "    \"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)\n",
    ".run()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cae36f0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Dataframe representation of the conversation\n",
    "question_chat.to_df()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 会話のデータフレーム表現\n",
    "question_chat.to_df()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:38.428599Z",
     "iopub.status.busy": "2024-06-25T15:11:38.428005Z",
     "iopub.status.idle": "2024-06-25T15:11:38.450345Z",
     "shell.execute_reply": "2024-06-25T15:11:38.449351Z",
     "shell.execute_reply.started": "2024-06-25T15:11:38.428565Z"
    }
   },
   "outputs": [],
   "source": [
    "# 会話のデータフレーム表現\n",
    "question_chat.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89c4fd",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "We now are confident that the LLM response contains the quesion and case parse the question like:\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "私たちはLLMのレスポンスが質問を含んでいることを自信を持って確認し、以下のように質問を抽出することができます：\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c5731",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "question = question_chat.last.parse(Question).content\n",
    "print(question)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "question = question_chat.last.parse(Question).content\n",
    "print(question)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:41.697665Z",
     "iopub.status.busy": "2024-06-25T15:11:41.696998Z",
     "iopub.status.idle": "2024-06-25T15:11:41.702788Z",
     "shell.execute_reply": "2024-06-25T15:11:41.701924Z",
     "shell.execute_reply.started": "2024-06-25T15:11:41.697632Z"
    }
   },
   "outputs": [],
   "source": [
    "question = question_chat.last.parse(Question).content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96da5bb",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Create a keyword dataframe\n",
    "** Note this only works because we know the possible keywords in the public set. This will not work on the final leaderboard**\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# キーワードデータフレームを作成する\n",
    "** これは、公開されたセットの中での可能なキーワードを知っているためのみ機能します。最終的なリーダーボードでは機能しません**\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809655f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:42.899784Z",
     "iopub.status.busy": "2024-06-25T15:11:42.89879Z",
     "iopub.status.idle": "2024-06-25T15:11:44.237713Z",
     "shell.execute_reply": "2024-06-25T15:11:44.236602Z",
     "shell.execute_reply.started": "2024-06-25T15:11:42.899737Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e3e167",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!head keywords_local.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!head keywords_local.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:57.05108Z",
     "iopub.status.busy": "2024-06-25T15:11:57.050672Z",
     "iopub.status.idle": "2024-06-25T15:11:58.005916Z",
     "shell.execute_reply": "2024-06-25T15:11:58.004762Z",
     "shell.execute_reply.started": "2024-06-25T15:11:57.051046Z"
    }
   },
   "outputs": [],
   "source": [
    "!head keywords_local.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf06e68",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.append('./')\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return text[0].upper() + text[1:].lower()\n",
    "\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    for d in keywords_dict:\n",
    "        words = [w['keyword'] for w in d['words']]\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]\n",
    "        category_words_dict[d['category']] = words\n",
    "        all_words += words\n",
    "        all_cat_words += cat_word\n",
    "\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category','keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]\n",
    "    keyword_df.to_parquet('keywords.parquet')\n",
    "    \n",
    "create_keyword_df(KEYWORDS_JSON)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.append('./')\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return text[0].upper() + text[1:].lower()\n",
    "\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    for d in keywords_dict:\n",
    "        words = [w['keyword'] for w in d['words']]\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]\n",
    "        category_words_dict[d['category']] = words\n",
    "        all_words += words\n",
    "        all_cat_words += cat_word\n",
    "\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category','keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]\n",
    "    keyword_df.to_parquet('keywords.parquet')\n",
    "    \n",
    "create_keyword_df(KEYWORDS_JSON)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:11:59.747293Z",
     "iopub.status.busy": "2024-06-25T15:11:59.746348Z",
     "iopub.status.idle": "2024-06-25T15:11:59.904356Z",
     "shell.execute_reply": "2024-06-25T15:11:59.903389Z",
     "shell.execute_reply.started": "2024-06-25T15:11:59.747254Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.append('./')\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return text[0].upper() + text[1:].lower()\n",
    "\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    for d in keywords_dict:\n",
    "        words = [w['keyword'] for w in d['words']]\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]\n",
    "        category_words_dict[d['category']] = words\n",
    "        all_words += words\n",
    "        all_cat_words += cat_word\n",
    "\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category','keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]\n",
    "    keyword_df.to_parquet('keywords.parquet')\n",
    "    \n",
    "create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697cf14e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "keywords_df = pd.read_parquet('keywords.parquet')\n",
    "keywords_df.sample(10)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "keywords_df = pd.read_parquet('keywords.parquet')\n",
    "keywords_df.sample(10)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:12:00.416192Z",
     "iopub.status.busy": "2024-06-25T15:12:00.415824Z",
     "iopub.status.idle": "2024-06-25T15:12:00.564344Z",
     "shell.execute_reply": "2024-06-25T15:12:00.563422Z",
     "shell.execute_reply.started": "2024-06-25T15:12:00.416162Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df = pd.read_parquet('keywords.parquet')\n",
    "keywords_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e463ea2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "keywords_df['category'].value_counts()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "keywords_df['category'].value_counts()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T15:12:03.242015Z",
     "iopub.status.busy": "2024-06-25T15:12:03.241664Z",
     "iopub.status.idle": "2024-06-25T15:12:03.261012Z",
     "shell.execute_reply": "2024-06-25T15:12:03.260034Z",
     "shell.execute_reply.started": "2024-06-25T15:12:03.241989Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5102bb42",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Create `main.py` Script for Final Submission.\n",
    "\n",
    "Our final submission will be a zipped directory with a `main` file. This file is below.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 最終提出用の`main.py`スクリプトを作成する。\n",
    "\n",
    "私たちの最終提出物は、`main`ファイルを含むZIP圧縮ディレクトリになります。以下にそのファイルがあります。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1241bea",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile main.py\n",
    "\n",
    "# Main agent file\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path fixups\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# Logging noise\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# Fixed imports\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# Constants\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    ")\n",
    "\n",
    "# Types\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"question\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"thing/place\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # override first question until keyword bug is fixed.\n",
    "        return \"Are we playing 20 questions?\"\n",
    "    \n",
    "    \n",
    "    full_question = f\"\"\"\\\n",
    "                You are currently asking the next question.\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                Based on the history above, ask the next most useful yes/no\n",
    "                question and place it in the following format:\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - Your response should be a focused question which will gather the most information\n",
    "                - Start general with your questions\n",
    "                - Always try to bisect the remaining search space\n",
    "                - Pay attention to previous questions and answers\n",
    "\n",
    "                What is your next question?\n",
    "                \"\"\"\n",
    "    \n",
    "    print(' ======Asking Question ======')\n",
    "    print(full_question)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(full_question)\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'Is it a person?'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n",
    "        return \"yes\" # override until keyword bug is fixed.\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "\n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # Loop 5 times and take the most frequent response\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "#                     f\"\"\"\\\n",
    "#                         20 Questions game. Answer yes/no for this keyword: [{observation.keyword}]\n",
    "\n",
    "#                             Question: {last_question}\n",
    "\n",
    "#                             Rules:\n",
    "#                             1. Only consider [{observation.keyword}]\n",
    "#                             2. Check each letter for letter questions\n",
    "#                             3. Answer only yes or no\n",
    "\n",
    "#                             Format:\n",
    "#                             <answer>yes</answer>\n",
    "#                             OR\n",
    "#                             <answer>no</answer>\n",
    "\n",
    "#                             Your answer:\n",
    "#                         \"\"\"\n",
    "                    f\"\"\"\n",
    "                    Keyword: [{observation.keyword}]\n",
    "\n",
    "                    Q: {last_question}\n",
    "\n",
    "                    Answer yes or no in Format: <answer>yes</answer> OR <answer>no</answer>\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'Responses are {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% Error so answering yes %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                You are currently making an informed guess of the keyword.\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                Based on the history above, produce a single next best guess\n",
    "                for the keyword and place it in the following format:\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - Avoid repeat guesses based on the history above\n",
    "                - The guess should be a specific person, place, or thing\n",
    "\n",
    "                What is your guess?\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'france'\n",
    "    \n",
    "# vLLM and Generator\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name,\n",
    "        \"--dtype=half\"\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLM Started\")\n",
    "except ValueError:\n",
    "    print('vLLM Already Running')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Entrypoint\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"Does the keyword start with any letter?\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"Does the keyword start with the letter '{letters[0]}'\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" or '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"Does the keyword start with one of the letters {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# Simple question asker\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"Are we playing 20 questions?\",\n",
    "            \"Is the keyword a thing that is not a location?\",\n",
    "            \"Is the keyword a place?\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # Filter down keyword_df based on past answers\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"Is the keyword a thing that is not a location?\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"Is the keyword a place?\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('Does the keyword start '):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # Reset\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('sample letters', n_letters, sample_letters)\n",
    "        return sample_letters # ', '.join(sample_letters)\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # Reset\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"Is the keyword one of the following? {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "#                     response = f'Does the keyword start with one of the following letters : {sample_letters_str} ?'\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # Remove the guessed word\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # Reset the df\n",
    "#         print(f'Round {self.round}')\n",
    "#         print(f\"{response=}\")\n",
    "#         print(f'keyword_df size {self.keyword_df.shape}')\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "#                 return await ask(base, observation)\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "#                 return await guess(base, observation)\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # Async gate when executing in their framework\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile main.py\n",
    "\n",
    "# メインエージェントファイル\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# パスの修正\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# ロギングのノイズを無効にする\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# 固定されたインポート\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# 定数\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    ")\n",
    "\n",
    "# タイプ\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です、'yes'または'no'である必要があります\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"質問\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"物/場所\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# 関数\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # キーワードバグが修正されるまで、最初の質問をオーバーライド\n",
    "        return \"私たちは20の質問をプレイしていますか？\"\n",
    "    \n",
    "    \n",
    "    full_question = f\"\"\"\\\n",
    "                あなたは現在次の質問をしています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づき、次に最も有用なはい/いいえの質問をし、以下のフォーマットにしてください：\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - あなたのレスポンスは、最も多くの情報を集める焦点を絞った質問であるべきです\n",
    "                - 質問は一般的なものから始めてください\n",
    "                - 残りの探索空間を二分してください\n",
    "                - 以前の質問と回答に注意してください\n",
    "\n",
    "                あなたの次の質問は何ですか？\n",
    "                \"\"\"\n",
    "    \n",
    "    print(' ======質問中 ======')\n",
    "    print(full_question)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(full_question)\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return '人ですか？'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"回答者にキーワードが提供されていません\", file=sys.stderr)\n",
    "        return \"yes\" # キーワードバグが修正されるまでオーバーライド\n",
    "    \n",
    "    last_question = observation.questions[-1]\n",
    "\n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # 5回ループし、最も頻繁なレスポンスを取得\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "#                     f\"\"\"\\\n",
    "#                         20の質問ゲーム。次のキーワードに対してはい/いいえで回答してください：[ {observation.keyword} ]\n",
    "\n",
    "#                             質問: {last_question}\n",
    "\n",
    "#                             ルール:\n",
    "#                             1.  [{observation.keyword}] のみを考慮してください\n",
    "#                             2. 各文字の質問のために文字を確認してください\n",
    "#                             3. はいまたはいいえのみで答えてください\n",
    "\n",
    "#                             フォーマット:\n",
    "#                             <answer>yes</answer>\n",
    "#                             または\n",
    "#                             <answer>no</answer>\n",
    "\n",
    "#                             あなたの回答は：\n",
    "#                         \"\"\"\n",
    "                    f\"\"\"\n",
    "                    キーワード: [ {observation.keyword} ]\n",
    "\n",
    "                    Q: {last_question}\n",
    "\n",
    "                    フォーマット: <answer>yes</answer> または <answer>no</answer>ではいまたはいいえで答えてください\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'レスポンスは {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% エラーが発生したため、yesを回答します %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                あなたは現在キーワードの情報に基づく推測を行っています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                上記の歴史に基づき、キーワードの次に最適な推測を1つ生成し、以下のフォーマットにしてください：\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - 上記の歴史に基づいて重複した推測を避けてください\n",
    "                - 推測は特定の人物、場所、または物である必要があります\n",
    "\n",
    "                あなたの推測は何ですか？\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'フランス'\n",
    "    \n",
    "# vLLMとGenerator\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name,\n",
    "        \"--dtype=half\"\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLMが起動しました\")\n",
    "except ValueError:\n",
    "    print('vLLMはすでに実行中です')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの優れたプレイヤーです。あなたは正確で、集中しており、構造的なアプローチをとります。あなたは有用な質問を作成し、質問に対する回答を行ったり、キーワードに関する推測を行ったりします。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# エントリーポイント\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは文字 '{letters[0]}' で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次のいずれかの文字で始まりますか: {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# シンプルな質問者\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"私たちは20の質問をプレイしていますか？\",\n",
    "            \"キーワードは場所ではないものですか？\",\n",
    "            \"キーワードは場所ですか？\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # 以前の回答に基づいてkeyword_dfをフィルタリング\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"キーワードは場所ではないものですか？\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"キーワードは場所ですか？\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('キーワードは次の文字で始まりますか'):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('サンプル文字', n_letters, sample_letters)\n",
    "        return sample_letters # ', '.join(sample_letters)\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"キーワードは次のいずれかのものでしょうか？ {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # 推測された単語を取り除く\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # dfをリセット\n",
    "#         print(f'ラウンド {self.round}')\n",
    "#         print(f\"{response=}\")\n",
    "#         print(f'keyword_dfサイズ {self.keyword_df.shape}')\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"不明なターンタイプ\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # 彼らのフレームワークで実行するときの非同期ゲート\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:27:50.461711Z",
     "iopub.status.busy": "2024-06-25T21:27:50.461328Z",
     "iopub.status.idle": "2024-06-25T21:27:50.487868Z",
     "shell.execute_reply": "2024-06-25T21:27:50.486744Z",
     "shell.execute_reply.started": "2024-06-25T21:27:50.461679Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# メインエージェントファイル\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# パスの修正\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# ロギングのノイズを無効にする\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# 固定されたインポート\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# 定数\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=1.2\"\n",
    ")\n",
    "\n",
    "# タイプ\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"無効な回答です、'yes'または'no'である必要があります\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"質問\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"物/場所\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# 関数\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # キーワードバグが修正されるまで、最初の質問をオーバーライド\n",
    "        return \"私たちは20の質問をプレイしていますか？\"\n",
    "    \n",
    "    \n",
    "    full_question = f\"\"\"\\\n",
    "                あなたは現在次の質問をしています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                上記の履歴に基づき、次に最も有用なはい/いいえの質問をし、以下のフォーマットにしてください：\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - あなたのレスポンスは、最も多くの情報を集める焦点を絞った質問であるべきです\n",
    "                - 質問は一般的なものから始めてください\n",
    "                - 残りの探索空間を二分してください\n",
    "                - 以前の質問と回答に注意してください\n",
    "\n",
    "                あなたの次の質問は何ですか？\n",
    "                \"\"\"\n",
    "    \n",
    "    print(' ======質問中 ======')\n",
    "    print(full_question)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(full_question)\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return '人ですか？'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"回答者にキーワードが提供されていません\", file=sys.stderr)\n",
    "        return \"yes\" # キーワードバグが修正されるまでオーバーライド\n",
    "    \n",
    "    last_question = observation.questions[-1]\n",
    "\n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # 5回ループし、最も頻繁なレスポンスを取得\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "#                     f\"\"\"\\\n",
    "#                         20の質問ゲーム。次のキーワードに対してはい/いいえで回答してください：[ {observation.keyword} ]\n",
    "\n",
    "#                             質問: {last_question}\n",
    "\n",
    "#                             ルール:\n",
    "#                             1.  [{observation.keyword}] のみを考慮してください\n",
    "#                             2. 各文字の質問のために文字を確認してください\n",
    "#                             3. はいまたはいいえのみで答えてください\n",
    "\n",
    "#                             フォーマット:\n",
    "#                             <answer>yes</answer>\n",
    "#                             または\n",
    "#                             <answer>no</answer>\n",
    "\n",
    "#                             あなたの回答は：\n",
    "#                         \"\"\"\n",
    "                    f\"\"\"\n",
    "                    キーワード: [ {observation.keyword} ]\n",
    "\n",
    "                    Q: {last_question}\n",
    "\n",
    "                    フォーマット: <answer>yes</answer> または <answer>no</answer>ではいまたはいいえで答えてください\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'レスポンスは {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% エラーが発生したため、yesを回答します %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                あなたは現在キーワードの情報に基づく推測を行っています。\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                上記の歴史に基づき、キーワードの次に最適な推測を1つ生成し、以下のフォーマットにしてください：\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - 上記の歴史に基づいて重複した推測を避けてください\n",
    "                - 推測は特定の人物、場所、または物である必要があります\n",
    "\n",
    "                あなたの推測は何ですか？\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'フランス'\n",
    "    \n",
    "# vLLMとGenerator\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name,\n",
    "        \"--dtype=half\"\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLMが起動しました\")\n",
    "except ValueError:\n",
    "    print('vLLMはすでに実行中です')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "あなたは20の質問ゲームの優れたプレイヤーです。あなたは正確で、集中しており、構造的なアプローチをとります。あなたは有用な質問を作成し、質問に対する回答を行ったり、キーワードに関する推測を行ったりします。\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# エントリーポイント\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは文字 '{letters[0]}' で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次のいずれかの文字で始まりますか: {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# シンプルな質問者\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"私たちは20の質問をプレイしていますか？\",\n",
    "            \"キーワードは場所ではないものですか？\",\n",
    "            \"キーワードは場所ですか？\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # 以前の回答に基づいてkeyword_dfをフィルタリング\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"キーワードは場所ではないものですか？\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"キーワードは場所ですか？\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('キーワードは次の文字で始まりますか'):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('サンプル文字', n_letters, sample_letters)\n",
    "        return sample_letters # ', '.join(sample_letters)\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # リセット\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"キーワードは次のいずれかのものでしょうか？ {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # 推測された単語を取り除く\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # dfをリセット\n",
    "#         print(f'ラウンド {self.round}')\n",
    "#         print(f\"{response=}\")\n",
    "#         print(f'keyword_dfサイズ {self.keyword_df.shape}')\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"不明なターンタイプ\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # 彼らのフレームワークで実行するときの非同期ゲート\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df736b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Test the Agent Against Itself\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# エージェントを自己評価する\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7f9b9e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"Does the keyword start with any letter?\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"Does the keyword start with the letter '{letters[0]}'\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" or '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"Does the keyword start with one of the letters {formatted_letters}?\"\n",
    "\n",
    "format_first_letter_question(['a','b','c'])\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは文字 '{letters[0]}' で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次のいずれかの文字で始まりますか: {formatted_letters}?\"\n",
    "\n",
    "format_first_letter_question(['a','b','c'])\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T21:27:50.490153Z",
     "iopub.status.busy": "2024-06-25T21:27:50.489818Z",
     "iopub.status.idle": "2024-06-25T21:27:50.500109Z",
     "shell.execute_reply": "2024-06-25T21:27:50.499088Z",
     "shell.execute_reply.started": "2024-06-25T21:27:50.490126Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"キーワードはどの文字で始まりますか？\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"キーワードは文字 '{letters[0]}' で始まりますか？\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" または '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"キーワードは次のいずれかの文字で始まりますか: {formatted_letters}?\"\n",
    "\n",
    "format_first_letter_question(['a','b','c'])\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58384b4e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import Observation, agent_fn, observe\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import Observation, agent_fn, observe\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:53.891959Z",
     "iopub.status.busy": "2024-06-25T18:43:53.891614Z",
     "iopub.status.idle": "2024-06-25T18:43:53.944887Z",
     "shell.execute_reply": "2024-06-25T18:43:53.94407Z",
     "shell.execute_reply.started": "2024-06-25T18:43:53.891923Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import Observation, agent_fn, observe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3a681d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Check if vllm is running\n",
    "!ps -aef | grep vllm\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# vllmが実行中か確認\n",
    "!ps -aef | grep vllm\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:53.94651Z",
     "iopub.status.busy": "2024-06-25T18:43:53.946184Z",
     "iopub.status.idle": "2024-06-25T18:43:54.938834Z",
     "shell.execute_reply": "2024-06-25T18:43:54.937806Z",
     "shell.execute_reply.started": "2024-06-25T18:43:53.946485Z"
    }
   },
   "outputs": [],
   "source": [
    "# vllmが実行中か確認\n",
    "!ps -aef | grep vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87006bc6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "sample = keyword_df.sample(1)\n",
    "\n",
    "obs = Observation(step = 0,\n",
    "    role = 'guesser',\n",
    "    turnType= \"ask\",\n",
    "    keyword= sample['keyword'].values[0],\n",
    "    category= sample['category'].values[0],\n",
    "    questions = [],\n",
    "    answers= [],\n",
    "    guesses= [],\n",
    ")\n",
    "\n",
    "question_agent = None\n",
    "\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)\n",
    "    print(f'[{i} Question]: {question}')\n",
    "    obs.questions.append(question)\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    if obs.questions[-1].startswith('Are we playing 20 questions?'):\n",
    "        gt_answer = answer # whatever\n",
    "    elif obs.questions[-1].startswith('Is the keyword a thing that is not a location?'):\n",
    "        if sample['category'].values[0] == 'things':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword a place?'):\n",
    "        if sample['category'].values[0] == 'place':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('Does the keyword start'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])\n",
    "        gt_answer = obs.keyword[0] in letters_guess\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword one of the following?'):\n",
    "        possible_kw = obs.questions[-1].replace('Is the keyword one of the following? ','').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]\n",
    "        print(possible_kw)\n",
    "        gt_answer = obs.keyword in possible_kw\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "\n",
    "    print(f'[{i} Answer]: {answer} [True Answer]: {gt_answer}')\n",
    "    if answer != gt_answer:\n",
    "        break\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'\n",
    "    guess = await observe(obs)\n",
    "    print(f'[{i} Guess]: {guess} - [Keyword]: {obs.keyword}')\n",
    "    obs.guesses.append(guess)\n",
    "    if guess == obs.keyword:\n",
    "        print('GOT IT!')\n",
    "        break\n",
    "        \n",
    "    obs.step += 1\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "sample = keyword_df.sample(1)\n",
    "\n",
    "obs = Observation(step = 0,\n",
    "    role = 'guesser',\n",
    "    turnType= \"ask\",\n",
    "    keyword= sample['keyword'].values[0],\n",
    "    category= sample['category'].values[0],\n",
    "    questions = [],\n",
    "    answers= [],\n",
    "    guesses= [],\n",
    ")\n",
    "\n",
    "question_agent = None\n",
    "\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)\n",
    "    print(f'[{i} 質問]: {question}')\n",
    "    obs.questions.append(question)\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    if obs.questions[-1].startswith('私たちは20の質問をプレイしていますか？'):\n",
    "        gt_answer = answer # 何でも\n",
    "    elif obs.questions[-1].startswith('キーワードは場所ではないものですか？'):\n",
    "        if sample['category'].values[0] == 'things':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは場所ですか？'):\n",
    "        if sample['category'].values[0] == 'place':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは次の文字で始まりますか'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])\n",
    "        gt_answer = obs.keyword[0] in letters_guess\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは次のいずれかのものでしょうか？'):\n",
    "        possible_kw = obs.questions[-1].replace('キーワードは次のいずれかのものでしょうか？ ','').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]\n",
    "        print(possible_kw)\n",
    "        gt_answer = obs.keyword in possible_kw\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "\n",
    "    print(f'[{i} 回答]: {answer} [真の回答]: {gt_answer}')\n",
    "    if answer != gt_answer:\n",
    "        break\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'\n",
    "    guess = await observe(obs)\n",
    "    print(f'[{i} 推測]: {guess} - [キーワード]: {obs.keyword}')\n",
    "    obs.guesses.append(guess)\n",
    "    if guess == obs.keyword:\n",
    "        print('正解です！')\n",
    "        break\n",
    "        \n",
    "    obs.step += 1\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:43:54.940887Z",
     "iopub.status.busy": "2024-06-25T18:43:54.940586Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "sample = keyword_df.sample(1)\n",
    "\n",
    "obs = Observation(step = 0,\n",
    "    role = 'guesser',\n",
    "    turnType= \"ask\",\n",
    "    keyword= sample['keyword'].values[0],\n",
    "    category= sample['category'].values[0],\n",
    "    questions = [],\n",
    "    answers= [],\n",
    "    guesses= [],\n",
    ")\n",
    "\n",
    "question_agent = None\n",
    "\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)\n",
    "    print(f'[{i} 質問]: {question}')\n",
    "    obs.questions.append(question)\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    if obs.questions[-1].startswith('私たちは20の質問をプレイしていますか？'):\n",
    "        gt_answer = answer # 何でも\n",
    "    elif obs.questions[-1].startswith('キーワードは場所ではないものですか？'):\n",
    "        if sample['category'].values[0] == 'things':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは場所ですか？'):\n",
    "        if sample['category'].values[0] == 'place':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは次の文字で始まりますか'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])\n",
    "        gt_answer = obs.keyword[0] in letters_guess\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "    elif obs.questions[-1].startswith('キーワードは次のいずれかのものでしょうか？'):\n",
    "        possible_kw = obs.questions[-1].replace('キーワードは次のいずれかのものでしょうか？ ','').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]\n",
    "        print(possible_kw)\n",
    "        gt_answer = obs.keyword in possible_kw\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "\n",
    "    print(f'[{i} 回答]: {answer} [真の回答]: {gt_answer}')\n",
    "    if answer != gt_answer:\n",
    "        break\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'\n",
    "    guess = await observe(obs)\n",
    "    print(f'[{i} 推測]: {guess} - [キーワード]: {obs.keyword}')\n",
    "    obs.guesses.append(guess)\n",
    "    if guess == obs.keyword:\n",
    "        print('正解です！')\n",
    "        break\n",
    "        \n",
    "    obs.step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfea74b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# last_question = \"Does the keyword start with one of the letters 'a', 'n' or 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# You will percisely answer a yes/no question for the keyword [{obs.keyword}] in the category [{obs.category}]\n",
    "\n",
    "# [QUESTION] \"{last_question}\" [/QUESTION]\n",
    "\n",
    "# Remember they keyword is [{obs.keyword}] - The QUESTION should be answered percisely for this specific keyword.\n",
    "\n",
    "# Answer the yes/no question above and place it in the following format:\n",
    "# {Answer.xml_example()}\n",
    "# \"\"\").run()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# last_question = \"キーワードは次のいずれかの文字で始まりますか: 'a', 'n', または 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# あなたはキーワード[{obs.keyword}]に対してはい/いいえの質問に正確に答えます。カテゴリは[{obs.category}]です。\n",
    "\n",
    "# [質問] \"{last_question}\" [/質問]\n",
    "\n",
    "# キーワードは[{obs.keyword}]です - 質問にはこの特定のキーワードのために正確に答えてください。\n",
    "\n",
    "# 上記のはい/いいえの質問に答え、以下のフォーマットにしてください：\n",
    "# {Answer.xml_example()}\n",
    "# \"\"\").run()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T18:25:45.793242Z",
     "iopub.status.busy": "2024-06-25T18:25:45.79235Z",
     "iopub.status.idle": "2024-06-25T18:25:45.834748Z",
     "shell.execute_reply": "2024-06-25T18:25:45.833946Z",
     "shell.execute_reply.started": "2024-06-25T18:25:45.793208Z"
    }
   },
   "outputs": [],
   "source": [
    "# last_question = \"キーワードは次のいずれかの文字で始まりますか: 'a', 'n', または 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# あなたはキーワード[{obs.keyword}]に対してはい/いいえの質問に正確に答えます。カテゴリは[{obs.category}]です。\n",
    "\n",
    "# [質問] \"{last_question}\" [/質問]\n",
    "\n",
    "# キーワードは[{obs.keyword}]です - 質問にはこの特定のキーワードのために正確に答えてください。\n",
    "\n",
    "# 上記のはい/いいえの質問に答え、以下のフォーマットにしてください：\n",
    "# {Answer.xml_example()}\n",
    "# \"\"\").run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca97a648",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "# last_question = \"Does the keyword start with one of the letters 'a', 'n' or 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# You are playing a 20 questions game. Your task is to accurately answer a yes/no question about a specific keyword.\n",
    "\n",
    "# Keyword: [{obs.keyword}]\n",
    "# Category: [{obs.category}]\n",
    "\n",
    "# Question: {last_question}\n",
    "\n",
    "# Instructions:\n",
    "# 1. Consider ONLY the keyword [{obs.keyword}] when answering.\n",
    "# 2. Answer the question with either 'yes' or 'no'.\n",
    "# 3. Double-check your answer for accuracy before responding.\n",
    "# 4. Provide your answer in the XML format shown below.\n",
    "\n",
    "# Your response should be in this exact format:\n",
    "# <answer>yes</answer>\n",
    "# OR\n",
    "# <answer>no</answer>\n",
    "\n",
    "# Now, please answer the question accurately for the keyword [{obs.keyword}]:\n",
    "# \"\"\"\n",
    "#                           ).run()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# last_question = \"キーワードは次のいずれかの文字で始まりますか: 'a', 'n', または 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# あなたは20の質問ゲームをプレイしています。あなたの任務は特定のキーワードについてはっきりとしたはい/いいえの質問に答えることです。\n",
    "\n",
    "# キーワード: [{obs.keyword}]\n",
    "# カテゴリ: [{obs.category}]\n",
    "\n",
    "# 質問: {last_question}\n",
    "\n",
    "# 指示:\n",
    "# 1. 答える際にはキーワード[{obs.keyword}] のみを考慮してください。\n",
    "# 2. はいまたはいいえで答えてください。\n",
    "# 3. 答えを正確にするために再確認してください。\n",
    "# 4. 下記のXML形式で回答を提供してください。\n",
    "\n",
    "# あなたの応答はこの正確な形式であるべきです:\n",
    "# <answer>yes</answer>\n",
    "# または\n",
    "# <answer>no</answer>\n",
    "\n",
    "# さあ、キーワード[{obs.keyword}]に対して質問に正確に答えてください:\n",
    "# \"\"\"\n",
    "#                           ).run()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:27.467095Z",
     "iopub.status.busy": "2024-06-25T16:01:27.466766Z",
     "iopub.status.idle": "2024-06-25T16:01:29.430507Z",
     "shell.execute_reply": "2024-06-25T16:01:29.429725Z",
     "shell.execute_reply.started": "2024-06-25T16:01:27.467071Z"
    }
   },
   "outputs": [],
   "source": [
    "# last_question = \"キーワードは次のいずれかの文字で始まりますか: 'a', 'n', または 'g'?\"\n",
    "# out = await generator.chat(f\"\"\"\\\n",
    "# あなたは20の質問ゲームをプレイしています。あなたの任務は特定のキーワードについてはっきりとしたはい/いいえの質問に答えることです。\n",
    "\n",
    "# キーワード: [{obs.keyword}]\n",
    "# カテゴリ: [{obs.category}]\n",
    "\n",
    "# 質問: {last_question}\n",
    "\n",
    "# 指示:\n",
    "# 1. 答える際にはキーワード[{obs.keyword}] のみを考慮してください。\n",
    "# 2. はいまたはいいえで答えてください。\n",
    "# 3. 答えを正確にするために再確認してください。\n",
    "# 4. 下記のXML形式で回答を提供してください。\n",
    "\n",
    "# あなたの応答はこの正確な形式であるべきです:\n",
    "# <answer>yes</answer>\n",
    "# または\n",
    "# <answer>no</answer>\n",
    "\n",
    "# さあ、キーワード[{obs.keyword}]に対して質問に正確に答えてください:\n",
    "# \"\"\"\n",
    "#                           ).run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5bc31",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# print(out.prev[-1].content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# print(out.prev[-1].content)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:29.432014Z",
     "iopub.status.busy": "2024-06-25T16:01:29.431678Z",
     "iopub.status.idle": "2024-06-25T16:01:29.47228Z",
     "shell.execute_reply": "2024-06-25T16:01:29.471434Z",
     "shell.execute_reply.started": "2024-06-25T16:01:29.431977Z"
    }
   },
   "outputs": [],
   "source": [
    "# print(out.prev[-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb10ffd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# out.last\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# out.last\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-25T16:01:30.352451Z",
     "iopub.status.busy": "2024-06-25T16:01:30.351758Z",
     "iopub.status.idle": "2024-06-25T16:01:30.394972Z",
     "shell.execute_reply": "2024-06-25T16:01:30.393982Z",
     "shell.execute_reply.started": "2024-06-25T16:01:30.352412Z"
    }
   },
   "outputs": [],
   "source": [
    "# out.last"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37341080",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Zipping Model and Code for Submission\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# モデルとコードを提出用に圧縮\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e998bc5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.status.busy": "2024-06-25T15:37:49.840946Z",
     "iopub.status.idle": "2024-06-25T15:37:49.84127Z",
     "shell.execute_reply": "2024-06-25T15:37:49.841124Z",
     "shell.execute_reply.started": "2024-06-25T15:37:49.84111Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93b744c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py \\\n",
    "    -C /kaggle/working keywords.parquet\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py \\\n",
    "    -C /kaggle/working keywords.parquet\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:16:49.684667Z",
     "iopub.status.busy": "2024-06-18T19:16:49.684329Z",
     "iopub.status.idle": "2024-06-18T19:19:49.999133Z",
     "shell.execute_reply": "2024-06-18T19:19:49.997937Z",
     "shell.execute_reply.started": "2024-06-18T19:16:49.684638Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py \\\n",
    "    -C /kaggle/working keywords.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c065f6e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!ls -GFlash --color\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!ls -GFlash --color\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:50.002318Z",
     "iopub.status.busy": "2024-06-18T19:19:50.001899Z",
     "iopub.status.idle": "2024-06-18T19:19:51.037256Z",
     "shell.execute_reply": "2024-06-18T19:19:51.036303Z",
     "shell.execute_reply.started": "2024-06-18T19:19:50.002277Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -GFlash --color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7e7218",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Submitting using Kaggle CLI\n",
    "\n",
    "Optionally you can submit using the kaggle cli interface without needing to re-run commit the notebook.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# Kaggle CLIを使用して提出\n",
    "\n",
    "必要に応じて、ノートブックを再実行せずにKaggle CLIインターフェースを使用して提出できます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c536365f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"submit from notebook\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"ノートブックから提出\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:51.040706Z",
     "iopub.status.busy": "2024-06-18T19:19:51.040378Z",
     "iopub.status.idle": "2024-06-18T19:19:51.086269Z",
     "shell.execute_reply": "2024-06-18T19:19:51.085134Z",
     "shell.execute_reply.started": "2024-06-18T19:19:51.040676Z"
    }
   },
   "outputs": [],
   "source": [
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"ノートブックから提出\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
