{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f55ae43",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションに参加するための言語モデルを開発することに取り組んでいます。目標は、20の質問ゲームのゲームプレイにおいて、対象のキーワードを特定するために「はい」または「いいえ」で回答する質問を生成するAIエージェントを構築することです。\n",
    "\n",
    "### 問題の概要\n",
    "- **問題設定**: 20の質問ゲームにおいて、ユーザーが提案したキーワードを推定するための質問と回答のやり取りを行います。エージェントは、過去の質問と回答の履歴をもとに次の質問を生成し、正しいキーワードを推測します。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "1. **Gemmaライブラリ**: PyTorchベースのGemmaライブラリを使用して言語モデルを構築し、推論を行います。このライブラリは、大規模な因果関係の学習モデルをサポートし、予測生成に用います。ノートブックの中では、Gemmaモデルが初期化され、訓練された重みをロードしています。\n",
    "   \n",
    "2. **質問生成と回答解析**: \n",
    "   - **GemmaFormatterクラス**: 質問と応答のフォーマットを管理し、ユーザーおよびモデルのターンを適切に構築する機能を提供します。\n",
    "   - **GemmaAgentクラス**: AIエージェントの基本機能を実装し、質問の履歴、回答履歴、キーワードの確率を管理します。オブザベーションに基づいて応答を生成するためのメソッドも含まれています。\n",
    "   - **GemmaQuestionerAgent** と **GemmaAnswererAgent**: 質問者と回答者それぞれの役割を果たすためのエージェントクラスで、質問を生成し、キーワードに基づいて回答を提供します。\n",
    "\n",
    "### 評価とデバッグ\n",
    "- ノートブックでは、評価ログの解析やデバッグ機能も組み込まれています。エージェントのパフォーマンスを向上させるために、質問の洗練やモデルパラメータの調整を行っています。また、ユーザーのフィードバックを基にエージェントの性能を向上させるための試行が継続されています。\n",
    "\n",
    "### 結論\n",
    "このノートブックは、20の質問ゲームにおける効果的な推理を実現するためのAIエージェントの開発に注力しています。Gemmaライブラリを活用してモデルを訓練し、ユーザーとの対話の中で適切な質問を生成するためのアルゴリズムを実装しています。デバッグや評価に取り組み、AIエージェントの性能を向上させるための試行錯誤も行われています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d92d0d",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebook内で扱われている機械学習・深層学習に関連する主要な専門用語や概念の説明を示します。特に、初心者がつまずきやすいと思われる項目に焦点を当てて説明します。\n",
    "\n",
    "### 用語解説\n",
    "\n",
    "1. **プロンプト (prompt)**:\n",
    "   プロンプトとは、AIモデルに対して与えるインプットのことです。このノートブックでは、質問者LLMや回答者LLMが「20の質問」ゲームを進行するための質問や状況設定を行う際に使用されます。\n",
    "\n",
    "2. **インタリーブ (interleave)**:\n",
    "   異なるリスト（質問や回答など）の要素を交互に並べることを指します。ここでは、質問履歴と回答履歴を交互に組み合わせることによって、ゲームの進行を管理しています。\n",
    "\n",
    "3. **few-shot examples**:\n",
    "   「少数ショット学習」とは、限られた数の例を使って学習を行う手法です。このノートブックでは、初期設定としてモデルに与える例を指し、モデルがどのような質問をすればよいのかを学ぶのに役立ちます。\n",
    "\n",
    "4. **状態 (state)**:\n",
    "   エージェントがゲームの進行状況やコンテキストに関する情報を保持するための内部的な情報を指します。この状態は、質問やモデルの応答などの情報を管理するために使用されます。\n",
    "\n",
    "5. **トークン (token)**:\n",
    "   自然言語処理において、トークンは言葉や句を構成する最小単位を指します。モデルはこれらのトークンを入力として処理し、予測や生成を行います。このノートブックでは、特に「<start_of_turn>」や「<end_of_turn>」といった特殊トークンが使われています。\n",
    "\n",
    "6. **サンプリング (sampling)**:\n",
    "   モデルが生成する際に、次の単語やトークンを選択する方法のことです。サンプリングの手法には、「温度」や「top-k」「top-p」などのパラメータが含まれ、生成されるテキストの多様性や確定性に影響を与えます。\n",
    "\n",
    "7. **密推定 (density estimation)**:\n",
    "   特定のデータがどのように分布しているかを推定する手法です。特に、ここでは回答の確率を効率良く管理するために使用されています。\n",
    "\n",
    "8. **エクスポネンシャル平均 (exponential averaging)**:\n",
    "   新しいデータポイントに基づいて過去の情報の重要度を減衰させることで、最新の傾向をより重視する手法です。このコンテキストでは、確率を更新するための方法として役立っています。\n",
    "\n",
    "9. **エージェント (agent)**:\n",
    "   自立した者として振る舞うプログラムやシステムを指します。このノートブックでは、質問をするエージェントと回答するエージェントの2つが実装されています。\n",
    "\n",
    "10. **PyTorch (パイトーチ)**:\n",
    "    深層学習のフレームワークであり、テンソル計算や微分のためのライブラリです。モデルのトレーニングや予測に使用され、ここではGemmaモデルの実装に利用されています。\n",
    "\n",
    "11. **量子化 (quantization)**:\n",
    "    モデルのパラメータを低精度の形式に変換し、運用時のメモリ使用量や計算時間を削減する技術です。この手法により、モデルをコンパクトにし、リソース制約のある環境でも扱いやすくしています。\n",
    "\n",
    "これらの用語は、特に本ノートブック特有の文脈や実務経験のないユーザーにとって、理解するのが難しいかもしれません。それぞれの概念を把握することで、より良い理解が得られるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4d25c",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "Notes:\n",
    "\n",
    "This is a difficult challenge! I have attempted different prompts, added probability wieghts to yes/no answers, adjusted LLM parameters, and tried question refinement. It is hard to understand the performance of the LLM in the evaluation logs since it includes team pairing. I have added a debug to the current version. Trying something new everyday! Great learning experience. \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "ノート：\n",
    "\n",
    "これは難しい挑戦です！私はさまざまなプロンプトを試し、はい/いいえの回答に確率的重みを追加し、LLMのパラメータを調整し、質問を洗練しようとしました。評価ログではチームのペアリングが含まれているため、LLMのパフォーマンスを理解するのが難しいです。現在のバージョンにデバッグを追加しました。毎日新しいことに挑戦しています！素晴らしい学習体験です。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516765e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "# 必要なパッケージをインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorch リポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemmaライブラリのファイルを移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:02.393105Z",
     "iopub.status.busy": "2024-06-12T17:53:02.392519Z",
     "iopub.status.idle": "2024-06-12T17:53:15.438413Z",
     "shell.execute_reply": "2024-06-12T17:53:15.437599Z",
     "shell.execute_reply.started": "2024-06-12T17:53:02.393071Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "# 必要なパッケージをインストールします\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorch リポジトリをクローンします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma用のディレクトリを作成します\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemmaライブラリのファイルを移動します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e933aeb5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "#Agent Creation\n",
    "    \n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.question_history = []\n",
    "        self.answer_history = []\n",
    "        self.keyword_probabilities = defaultdict(float)\n",
    "        \n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model = self.model.to(self._device).eval()\n",
    "            \n",
    "        self.tokenizer = self._initialize_tokenizer(model_config.tokenizer)\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        self.formatter.reset()\n",
    "        if 'system_prompt' in obs:\n",
    "            self.formatter.user(obs['system_prompt'])\n",
    "        if 'few_shot_examples' in obs:\n",
    "            self.formatter.apply_turns(obs['few_shot_examples'], start_agent='user')\n",
    "        self.formatter.start_user_turn()\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=50, sampler_kwargs=None):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.6,\n",
    "                'top_p': 0.9,\n",
    "                'top_k': 50,\n",
    "                'repetition_penalty': 0.8,\n",
    "                'num_beams': 3,\n",
    "                'early_stopping': False,\n",
    "                'do_sample': True\n",
    "            }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def parse_response(self, response: str, obs: dict):\n",
    "        keyword = self._parse_keyword(response)\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            question = self._generate_question(keyword)\n",
    "            self.question_history.append(question)\n",
    "            return question\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{keyword}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            last_question = obs.get('questions', [])[-1]\n",
    "            answer = self._analyze_answer(last_question, keyword)\n",
    "            self.answer_history.append(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            return \"Invalid turn type\"\n",
    "    \n",
    "    def _analyze_answer(self, question: str, context: str):\n",
    "        question_lower = question.lower()\n",
    "        keyword = self._parse_keyword(context)\n",
    "        if \"what\" in question_lower:\n",
    "            return f\"Hint: It's a {keyword}\"\n",
    "        elif \"where\" in question_lower:\n",
    "            return f\"Hint: It's related to {keyword}\"\n",
    "        else:\n",
    "            answer = \"Yes\" if \"is\" in question_lower else \"No\"\n",
    "            self.answer_history.append(answer)\n",
    "            for keyword in self.keyword_probabilities:\n",
    "                self._update_probabilities(keyword, answer)\n",
    "            return answer\n",
    "        \n",
    "    def _update_probabilities(self, keyword: str, answer: str):\n",
    "        if keyword in self.keyword_probabilities:\n",
    "            self.keyword_probabilities[keyword] += 0.4 if answer == \"Yes\" else -0.25\n",
    "            self.keyword_probabilities[keyword] = max(0.0, min(1.0, self.keyword_probabilities[keyword]))\n",
    "        \n",
    "        total_prob = sum(self.keyword_probabilities.values())\n",
    "        if total_prob > 0:\n",
    "            for key in self.keyword_probabilities:\n",
    "                self.keyword_probabilities[key] /= total_prob\n",
    "    \n",
    "    def _suggest_keyword(self):\n",
    "        return max(self.keyword_probabilities, key=self.keyword_probabilities.get, default=\"\")\n",
    "\n",
    "    def _refine_question(self, question: str, answers: list):\n",
    "        if answers:\n",
    "            last_answer = answers[-1].lower()\n",
    "            if last_answer == \"no\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Could it be\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Might it\")\n",
    "            elif last_answer == \"yes\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Is it specifically\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Does it specifically\")\n",
    "        \n",
    "        return question\n",
    "\n",
    "    def _generate_question(self, context: str):\n",
    "        if self.question_history:\n",
    "            last_question = self.question_history[-1]\n",
    "            if last_question in context:\n",
    "                return \"Next logical or creative question based on the answer to the last question...\"\n",
    "        \n",
    "        input_ids = self.tokenizer.encode_plus(\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        output = self.model.generate(input_ids['input_ids'], attention_mask=input_ids['attention_mask'])\n",
    "        question = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        question = self._refine_question(question, self.answer_history)\n",
    "        self.question_history.append(question)\n",
    "        return question\n",
    "\n",
    "    def _make_decision(self, obs: dict, context: str):\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            return self._generate_question(context)\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{self._suggest_keyword()}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            question = obs.get('questions', [])[-1]\n",
    "            return self._analyze_answer(question, context)\n",
    "        return \"Invalid turn type\"\n",
    "\n",
    "# Questioner Agent\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play the game 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(f\"Please ask a yes-or-no question based on the context in {obs.questions} and {obs.answers} to discover the keyword. The yes-or-no question should not be something like \\\"Does it rain in the desert\\\" or \\\"Are you happy?\\\". It should be a question that helps you know the category and specifics about the keyword to help you narrow the possibilities of your guess. Surround your question with double asterisks.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword only. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a place?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "#Answerer Agent\n",
    "            \n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give only yes-or-no answers based on the context of the keyword and category.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return '**yes**' if '**yes**' in answer else '**no**'\n",
    "\n",
    "\n",
    "system_prompt = \"You are an AI assistant designed to ask and answers yes or no questions to discover a secret keyword. The keyword is a specific person, place, or thing. Try to guess the keyword in every round if you are the questioner. Try to narrow down to a catagory then use questions about the starting letter to further narrow your guess.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    (\"Is it a person?\", \"Yes\"),\n",
    "    (\"Is this person a male?\", \"Yes\"),\n",
    "    (\"Is this person an actor?\", \"No\"),\n",
    "    (\"Is this person a politician?\", \"Yes\"),\n",
    "    (\"Is this person currently alive?\", \"No\"),\n",
    "    (\"Is it a place?\", \"Yes\"),\n",
    "    (\"Is this place a city?\", \"Yes\"),\n",
    "    (\"Is this city in the United States?\", \"Yes\"),\n",
    "    (\"Is this city the capital of a state?\", \"No\"),\n",
    "    (\"Is this city on the east coast?\", \"Yes\"),\n",
    "    (\"Is this place a country?\", \"Yes\"),\n",
    "    (\"Is this country in Europe?\", \"Yes\"),\n",
    "    (\"Is this country a member of the European Union?\", \"Yes\"),\n",
    "    (\"Is it known for its historical landmarks?\", \"Yes\"),\n",
    "    (\"Is it a geographical feature?\", \"Yes\"),\n",
    "    (\"Is it a mountain?\", \"No\"),\n",
    "    (\"Is it a river?\", \"Yes\"),\n",
    "    (\"Is this river in Africa?\", \"Yes\"),\n",
    "    (\"Is this river the longest river in the world?\", \"Yes\"),\n",
    "    (\"Does this river begin with the letter n?\", \"Yes\"),\n",
    "    (\"Is the keyword the Nile River?\", \"Yes\"),\n",
    "    (\"Is it a thing?\", \"Yes\"),\n",
    "    (\"Is this thing man-made?\", \"Yes\"),\n",
    "    (\"Is it used for communication?\", \"Yes\"),\n",
    "    (\"Is it a type of technology?\", \"Yes\"),\n",
    "    (\"Is it smaller than a bread box?\", \"Yes\"),\n",
    "    (\"Does this thing begin with the letter s?\", \"Yes\"),\n",
    "    (\"Is it a type of vehicle?\", \"No\"),\n",
    "    (\"Is it found indoors?\", \"Yes\"),\n",
    "    (\"Is it used for entertainment?\", \"No\")\n",
    "]\n",
    "\n",
    "\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleエージェントのパスを設定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# Kaggleエージェントのパスが存在する場合、libを追加します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "# 存在しない場合は、ローカルのlibを追加します\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 重みのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "# 2つのリストをインタリーブします（サイズが異なる場合も考慮）\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 現在の状態を返します\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        # ユーザーからのプロンプトを状態に追加します\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        # モデルからのプロンプトを状態に追加します\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        # ユーザーのターンを開始するためのトークンを追加します\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        # モデルのターンを開始するためのトークンを追加します\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        # ターンの終了を示すトークンを追加します\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        # 状態をリセットし、初期プロンプトを設定します\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 提供されたターンを適用します\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"デフォルトのtorchデータ型を設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# エージェント作成\n",
    "    \n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.question_history = []\n",
    "        self.answer_history = []\n",
    "        self.keyword_probabilities = defaultdict(float)\n",
    "        \n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model = self.model.to(self._device).eval()\n",
    "            \n",
    "        self.tokenizer = self._initialize_tokenizer(model_config.tokenizer)\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        # セッションの初期化\n",
    "        self.formatter.reset()\n",
    "        if 'system_prompt' in obs:\n",
    "            self.formatter.user(obs['system_prompt'])\n",
    "        if 'few_shot_examples' in obs:\n",
    "            self.formatter.apply_turns(obs['few_shot_examples'], start_agent='user')\n",
    "        self.formatter.start_user_turn()\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=50, sampler_kwargs=None):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.6,\n",
    "                'top_p': 0.9,\n",
    "                'top_k': 50,\n",
    "                'repetition_penalty': 0.8,\n",
    "                'num_beams': 3,\n",
    "                'early_stopping': False,\n",
    "                'do_sample': True\n",
    "            }\n",
    "        # LLMを呼び出し、応答を生成します\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # レスポンスからキーワードを抽出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析します\n",
    "        keyword = self._parse_keyword(response)\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            question = self._generate_question(keyword)\n",
    "            self.question_history.append(question)\n",
    "            return question\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{keyword}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            last_question = obs.get('questions', [])[-1]\n",
    "            answer = self._analyze_answer(last_question, keyword)\n",
    "            self.answer_history.append(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            return \"無効なターンタイプ\"\n",
    "    \n",
    "    def _analyze_answer(self, question: str, context: str):\n",
    "        # 答えを分析します\n",
    "        question_lower = question.lower()\n",
    "        keyword = self._parse_keyword(context)\n",
    "        if \"what\" in question_lower:\n",
    "            return f\"ヒント: それは{keyword}です\"\n",
    "        elif \"where\" in question_lower:\n",
    "            return f\"ヒント: それは{keyword}に関連しています\"\n",
    "        else:\n",
    "            answer = \"はい\" if \"is\" in question_lower else \"いいえ\"\n",
    "            self.answer_history.append(answer)\n",
    "            for keyword in self.keyword_probabilities:\n",
    "                self._update_probabilities(keyword, answer)\n",
    "            return answer\n",
    "        \n",
    "    def _update_probabilities(self, keyword: str, answer: str):\n",
    "        # 確率を更新します\n",
    "        if keyword in self.keyword_probabilities:\n",
    "            self.keyword_probabilities[keyword] += 0.4 if answer == \"はい\" else -0.25\n",
    "            self.keyword_probabilities[keyword] = max(0.0, min(1.0, self.keyword_probabilities[keyword]))\n",
    "        \n",
    "        total_prob = sum(self.keyword_probabilities.values())\n",
    "        if total_prob > 0:\n",
    "            for key in self.keyword_probabilities:\n",
    "                self.keyword_probabilities[key] /= total_prob\n",
    "    \n",
    "    def _suggest_keyword(self):\n",
    "        # 最も高い確率を持つキーワードを提案します\n",
    "        return max(self.keyword_probabilities, key=self.keyword_probabilities.get, default=\"\")\n",
    "\n",
    "    def _refine_question(self, question: str, answers: list):\n",
    "        # 質問を洗練させます\n",
    "        if answers:\n",
    "            last_answer = answers[-1].lower()\n",
    "            if last_answer == \"いいえ\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Could it be\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Might it\")\n",
    "            elif last_answer == \"はい\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Is it specifically\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Does it specifically\")\n",
    "        \n",
    "        return question\n",
    "\n",
    "    def _generate_question(self, context: str):\n",
    "        # 次の質問を生成します\n",
    "        if self.question_history:\n",
    "            last_question = self.question_history[-1]\n",
    "            if last_question in context:\n",
    "                return \"前の質問の回答に基づいて次の論理的または創造的な質問...\"\n",
    "        \n",
    "        input_ids = self.tokenizer.encode_plus(\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        output = self.model.generate(input_ids['input_ids'], attention_mask=input_ids['attention_mask'])\n",
    "        question = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        question = self._refine_question(question, self.answer_history)\n",
    "        self.question_history.append(question)\n",
    "        return question\n",
    "\n",
    "    def _make_decision(self, obs: dict, context: str):\n",
    "        # 判断を行います\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            return self._generate_question(context)\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{self._suggest_keyword()}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            question = obs.get('questions', [])[-1]\n",
    "            return self._analyze_answer(question, context)\n",
    "        return \"無効なターンタイプ\"\n",
    "\n",
    "# 質問者エージェント\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 質問者としてのセッションを開始します\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20質問ゲームをプレイしましょう。あなたは質問者の役割を果たしています。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(f\"{obs.questions}および{obs.answers}のコンテキストに基づいて、キーワードを発見するためにyesまたはnoの質問をしてください。この質問は「砂漠では雨が降りますか？」や「あなたは幸せですか？」のようなものであってはなりません。キーワードの可能性を絞るためのカテゴリや詳細を知る手助けとなる質問である必要があります。あなたの質問は二重アスタリスクで囲んでください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを推測してください。推測は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析します\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは場所ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "# 答弁者エージェント\n",
    "            \n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 答弁者としてのセッションを開始します\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは答弁者の役割です。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。キーワードとカテゴリの文脈に基づいてyesまたはnoの回答のみを提供してください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析し、いいえかはいとのみの回答を返します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return '**はい**' if '**はい**' in answer else '**いいえ**'\n",
    "\n",
    "\n",
    "system_prompt = \"あなたは秘密のキーワードを発見するためにyesまたはnoの質問をするように設計されたAIアシスタントです。キーワードは特定の人、場所、または物です。質問者の場合は、毎ラウンド推測してください。カテゴリを絞り込んだら、最初の文字に関する質問をしてさらに推測を狭めてください。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    (\"それは人ですか？\", \"はい\"),\n",
    "    (\"この人は男性ですか？\", \"はい\"),\n",
    "    (\"この人は俳優ですか？\", \"いいえ\"),\n",
    "    (\"この人は政治家ですか？\", \"はい\"),\n",
    "    (\"この人は現在生きていますか？\", \"いいえ\"),\n",
    "    (\"それは場所ですか？\", \"はい\"),\n",
    "    (\"この場所は都市ですか？\", \"はい\"),\n",
    "    (\"この都市はアメリカにありますか？\", \"はい\"),\n",
    "    (\"この都市は州の首都ですか？\", \"いいえ\"),\n",
    "    (\"この都市は東海岸にありますか？\", \"はい\"),\n",
    "    (\"それは国ですか？\", \"はい\"),\n",
    "    (\"この国はヨーロッパにありますか？\", \"はい\"),\n",
    "    (\"この国は欧州連合のメンバーですか？\", \"はい\"),\n",
    "    (\"この国は歴史的なランドマークで知られていますか？\", \"はい\"),\n",
    "    (\"それは地理的特徴ですか？\", \"はい\"),\n",
    "    (\"それは山ですか？\", \"いいえ\"),\n",
    "    (\"それは川ですか？\", \"はい\"),\n",
    "    (\"この川はアフリカにありますか？\", \"はい\"),\n",
    "    (\"この川は世界で最も長い川ですか？\", \"はい\"),\n",
    "    (\"この川はnの文字で始まりますか？\", \"はい\"),\n",
    "    (\"キーワードはナイル川ですか？\", \"はい\"),\n",
    "    (\"それは物ですか？\", \"はい\"),\n",
    "    (\"この物は人間が作ったものですか？\", \"はい\"),\n",
    "    (\"それは通信に使用されますか？\", \"はい\"),\n",
    "    (\"それは技術の一種ですか？\", \"はい\"),\n",
    "    (\"それは食パンの箱より小さいですか？\", \"はい\"),\n",
    "    (\"それは車両の一種ですか？\", \"いいえ\"),\n",
    "    (\"それは屋内にありますか？\", \"はい\"),\n",
    "    (\"それは娯楽に使用されますか？\", \"いいえ\")\n",
    "]\n",
    "\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # 現在のエージェントが初期化されていない場合、エージェントを作成します\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # エージェントの動作を定義します\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:15.441153Z",
     "iopub.status.busy": "2024-06-12T17:53:15.44079Z",
     "iopub.status.idle": "2024-06-12T17:53:18.693805Z",
     "shell.execute_reply": "2024-06-12T17:53:18.693014Z",
     "shell.execute_reply.started": "2024-06-12T17:53:15.441121Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Kaggleエージェントのパスを設定します\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# Kaggleエージェントのパスが存在する場合、libを追加します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "# 存在しない場合は、ローカルのlibを追加します\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# 重みのパスを設定します\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "# 2つのリストをインタリーブします（サイズが異なる場合も考慮）\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        # 現在の状態を返します\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        # ユーザーからのプロンプトを状態に追加します\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        # モデルからのプロンプトを状態に追加します\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        # ユーザーのターンを開始するためのトークンを追加します\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        # モデルのターンを開始するためのトークンを追加します\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        # ターンの終了を示すトークンを追加します\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        # 状態をリセットし、初期プロンプトを設定します\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        # 提供されたターンを適用します\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"デフォルトのtorchデータ型を設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# エージェント作成\n",
    "    \n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.question_history = []\n",
    "        self.answer_history = []\n",
    "        self.keyword_probabilities = defaultdict(float)\n",
    "        \n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            self.model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            self.model.load_weights(ckpt_path)\n",
    "            self.model = self.model.to(self._device).eval()\n",
    "            \n",
    "        self.tokenizer = self._initialize_tokenizer(model_config.tokenizer)\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        # セッションの初期化\n",
    "        self.formatter.reset()\n",
    "        if 'system_prompt' in obs:\n",
    "            self.formatter.user(obs['system_prompt'])\n",
    "        if 'few_shot_examples' in obs:\n",
    "            self.formatter.apply_turns(obs['few_shot_examples'], start_agent='user')\n",
    "        self.formatter.start_user_turn()\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=50, sampler_kwargs=None):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.6,\n",
    "                'top_p': 0.9,\n",
    "                'top_k': 50,\n",
    "                'repetition_penalty': 0.8,\n",
    "                'num_beams': 3,\n",
    "                'early_stopping': False,\n",
    "                'do_sample': True\n",
    "            }\n",
    "        # LLMを呼び出し、応答を生成します\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # レスポンスからキーワードを抽出します\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        return match.group().lower() if match else ''\n",
    "\n",
    "    def parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析します\n",
    "        keyword = self._parse_keyword(response)\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            question = self._generate_question(keyword)\n",
    "            self.question_history.append(question)\n",
    "            return question\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{keyword}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            last_question = obs.get('questions', [])[-1]\n",
    "            answer = self._analyze_answer(last_question, keyword)\n",
    "            self.answer_history.append(answer)\n",
    "            return answer\n",
    "        else:\n",
    "            return \"無効なターンタイプ\"\n",
    "    \n",
    "    def _analyze_answer(self, question: str, context: str):\n",
    "        # 答えを分析します\n",
    "        question_lower = question.lower()\n",
    "        keyword = self._parse_keyword(context)\n",
    "        if \"what\" in question_lower:\n",
    "            return f\"ヒント: それは{keyword}です\"\n",
    "        elif \"where\" in question_lower:\n",
    "            return f\"ヒント: それは{keyword}に関連しています\"\n",
    "        else:\n",
    "            answer = \"はい\" if \"is\" in question_lower else \"いいえ\"\n",
    "            self.answer_history.append(answer)\n",
    "            for keyword in self.keyword_probabilities:\n",
    "                self._update_probabilities(keyword, answer)\n",
    "            return answer\n",
    "        \n",
    "    def _update_probabilities(self, keyword: str, answer: str):\n",
    "        # 確率を更新します\n",
    "        if keyword in self.keyword_probabilities:\n",
    "            self.keyword_probabilities[keyword] += 0.4 if answer == \"はい\" else -0.25\n",
    "            self.keyword_probabilities[keyword] = max(0.0, min(1.0, self.keyword_probabilities[keyword]))\n",
    "        \n",
    "        total_prob = sum(self.keyword_probabilities.values())\n",
    "        if total_prob > 0:\n",
    "            for key in self.keyword_probabilities:\n",
    "                self.keyword_probabilities[key] /= total_prob\n",
    "    \n",
    "    def _suggest_keyword(self):\n",
    "        # 最も高い確率を持つキーワードを提案します\n",
    "        return max(self.keyword_probabilities, key=self.keyword_probabilities.get, default=\"\")\n",
    "\n",
    "    def _refine_question(self, question: str, answers: list):\n",
    "        # 質問を洗練させます\n",
    "        if answers:\n",
    "            last_answer = answers[-1].lower()\n",
    "            if last_answer == \"いいえ\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Could it be\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Might it\")\n",
    "            elif last_answer == \"はい\":\n",
    "                if \"is it\" in question.lower():\n",
    "                    question = question.replace(\"Is it\", \"Is it specifically\")\n",
    "                elif \"does it\" in question.lower():\n",
    "                    question = question.replace(\"Does it\", \"Does it specifically\")\n",
    "        \n",
    "        return question\n",
    "\n",
    "    def _generate_question(self, context: str):\n",
    "        # 次の質問を生成します\n",
    "        if self.question_history:\n",
    "            last_question = self.question_history[-1]\n",
    "            if last_question in context:\n",
    "                return \"前の質問の回答に基づいて次の論理的または創造的な質問...\"\n",
    "        \n",
    "        input_ids = self.tokenizer.encode_plus(\n",
    "            context,\n",
    "            add_special_tokens=True,\n",
    "            max_length=512,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        output = self.model.generate(input_ids['input_ids'], attention_mask=input_ids['attention_mask'])\n",
    "        question = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        question = self._refine_question(question, self.answer_history)\n",
    "        self.question_history.append(question)\n",
    "        return question\n",
    "\n",
    "    def _make_decision(self, obs: dict, context: str):\n",
    "        # 判断を行います\n",
    "        turn_type = obs.get('turnType')\n",
    "        if turn_type == \"ask\":\n",
    "            return self._generate_question(context)\n",
    "        elif turn_type == \"guess\":\n",
    "            return f\"**{self._suggest_keyword()}**\"\n",
    "        elif turn_type == \"answer\":\n",
    "            question = obs.get('questions', [])[-1]\n",
    "            return self._analyze_answer(question, context)\n",
    "        return \"無効なターンタイプ\"\n",
    "\n",
    "# 質問者エージェント\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 質問者としてのセッションを開始します\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20質問ゲームをプレイしましょう。あなたは質問者の役割を果たしています。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(f\"{obs.questions}および{obs.answers}のコンテキストに基づいて、キーワードを発見するためにyesまたはnoの質問をしてください。この質問は「砂漠では雨が降りますか？」や「あなたは幸せですか？」のようなものであってはなりません。キーワードの可能性を絞るためのカテゴリや詳細を知る手助けとなる質問である必要があります。あなたの質問は二重アスタリスクで囲んでください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"キーワードを推測してください。推測は二重アスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析します\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは場所ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "# 答弁者エージェント\n",
    "            \n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        # 答弁者としてのセッションを開始します\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をプレイしましょう。あなたは答弁者の役割です。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"この質問はキーワード{obs.keyword}に関するもので、カテゴリは{obs.category}です。キーワードとカテゴリの文脈に基づいてyesまたはnoの回答のみを提供してください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        # レスポンスを解析し、いいえかはいとのみの回答を返します\n",
    "        answer = self._parse_keyword(response)\n",
    "        return '**はい**' if '**はい**' in answer else '**いいえ**'\n",
    "\n",
    "\n",
    "system_prompt = \"あなたは秘密のキーワードを発見するためにyesまたはnoの質問をするように設計されたAIアシスタントです。キーワードは特定の人、場所、または物です。質問者の場合は、毎ラウンド推測してください。カテゴリを絞り込んだら、最初の文字に関する質問をしてさらに推測を狭めてください。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    (\"それは人ですか？\", \"はい\"),\n",
    "    (\"この人は男性ですか？\", \"はい\"),\n",
    "    (\"この人は俳優ですか？\", \"いいえ\"),\n",
    "    (\"この人は政治家ですか？\", \"はい\"),\n",
    "    (\"この人は現在生きていますか？\", \"いいえ\"),\n",
    "    (\"それは場所ですか？\", \"はい\"),\n",
    "    (\"この場所は都市ですか？\", \"はい\"),\n",
    "    (\"この都市はアメリカにありますか？\", \"はい\"),\n",
    "    (\"この都市は州の首都ですか？\", \"いいえ\"),\n",
    "    (\"この都市は東海岸にありますか？\", \"はい\"),\n",
    "    (\"それは国ですか？\", \"はい\"),\n",
    "    (\"この国はヨーロッパにありますか？\", \"はい\"),\n",
    "    (\"この国は欧州連合のメンバーですか？\", \"はい\"),\n",
    "    (\"この国は歴史的なランドマークで知られていますか？\", \"はい\"),\n",
    "    (\"それは地理的特徴ですか？\", \"はい\"),\n",
    "    (\"それは山ですか？\", \"いいえ\"),\n",
    "    (\"それは川ですか？\", \"はい\"),\n",
    "    (\"この川はアフリカにありますか？\", \"はい\"),\n",
    "    (\"この川は世界で最も長い川ですか？\", \"はい\"),\n",
    "    (\"この川はnの文字で始まりますか？\", \"はい\"),\n",
    "    (\"キーワードはナイル川ですか？\", \"はい\"),\n",
    "    (\"それは物ですか？\", \"はい\"),\n",
    "    (\"この物は人間が作ったものですか？\", \"はい\"),\n",
    "    (\"それは通信に使用されますか？\", \"はい\"),\n",
    "    (\"それは技術の一種ですか？\", \"はい\"),\n",
    "    (\"それは食パンの箱より小さいですか？\", \"はい\"),\n",
    "    (\"それは車両の一種ですか？\", \"いいえ\"),\n",
    "    (\"それは屋内にありますか？\", \"はい\"),\n",
    "    (\"それは娯楽に使用されますか？\", \"いいえ\")\n",
    "]\n",
    "\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # 現在のエージェントが初期化されていない場合、エージェントを作成します\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    # エージェントの動作を定義します\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"はい\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f333363b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Submission\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 提出\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90309f9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:18.695002Z",
     "iopub.status.busy": "2024-06-12T17:53:18.694662Z",
     "iopub.status.idle": "2024-06-12T17:53:18.69901Z",
     "shell.execute_reply": "2024-06-12T17:53:18.698135Z",
     "shell.execute_reply.started": "2024-06-12T17:53:18.69498Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab7bafd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:18.701225Z",
     "iopub.status.busy": "2024-06-12T17:53:18.700938Z",
     "iopub.status.idle": "2024-06-12T17:53:18.708593Z",
     "shell.execute_reply": "2024-06-12T17:53:18.707722Z",
     "shell.execute_reply.started": "2024-06-12T17:53:18.701179Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7610ad97",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Debug\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# デバッグ\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b094e0d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# from kaggle_environments import make\n",
    "\n",
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\":\n",
    "#         response = \"Is it a duck?\"\n",
    "#     elif obs.turnType == \"guess\":\n",
    "#         response = \"duck\"\n",
    "#     elif obs.turnType == \"answer\":\n",
    "#         response = \"no\"\n",
    "#     return response\n",
    "\n",
    "\n",
    "# debug_config = {'episodeSteps': 61,\n",
    "#                 'actTimeout': 60,      \n",
    "#                 'runTimeout': 1200,    \n",
    "#                 'agentTimeout': 3600}  \n",
    "\n",
    "# env = make(environment=\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "\n",
    "# game_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\n",
    "\n",
    "# env.render(mode=\"ipython\", width=1080, height=700)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# from kaggle_environments import make\n",
    "\n",
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\":\n",
    "#         response = \"それはアヒルですか？\"\n",
    "#     elif obs.turnType == \"guess\":\n",
    "#         response = \"アヒル\"\n",
    "#     elif obs.turnType == \"answer\":\n",
    "#         response = \"いいえ\"\n",
    "#     return response\n",
    "\n",
    "\n",
    "# debug_config = {'episodeSteps': 61,\n",
    "#                 'actTimeout': 60,      \n",
    "#                 'runTimeout': 1200,    \n",
    "#                 'agentTimeout': 3600}  \n",
    "\n",
    "# 環境を作成します\n",
    "# env = make(environment=\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "\n",
    "# ゲームを実行します\n",
    "# game_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\n",
    "\n",
    "# 実行結果を表示します\n",
    "# env.render(mode=\"ipython\", width=1080, height=700)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:18.709657Z",
     "iopub.status.busy": "2024-06-12T17:53:18.709352Z",
     "iopub.status.idle": "2024-06-12T17:54:13.674169Z",
     "shell.execute_reply": "2024-06-12T17:54:13.673231Z",
     "shell.execute_reply.started": "2024-06-12T17:53:18.709635Z"
    }
   },
   "outputs": [],
   "source": [
    "# from kaggle_environments import make\n",
    "\n",
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\":\n",
    "#         response = \"それはアヒルですか？\"\n",
    "#     elif obs.turnType == \"guess\":\n",
    "#         response = \"アヒル\"\n",
    "#     elif obs.turnType == \"answer\":\n",
    "#         response = \"いいえ\"\n",
    "#     return response\n",
    "\n",
    "\n",
    "# debug_config = {'episodeSteps': 61,\n",
    "#                 'actTimeout': 60,      \n",
    "#                 'runTimeout': 1200,    \n",
    "#                 'agentTimeout': 3600}  \n",
    "\n",
    "# 環境を作成します\n",
    "# env = make(environment=\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "\n",
    "# ゲームを実行します\n",
    "# game_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\n",
    "\n",
    "# 実行結果を表示します\n",
    "# env.render(mode=\"ipython\", width=1080, height=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-12T17:53:18.709657Z",
     "iopub.status.busy": "2024-06-12T17:53:18.709352Z",
     "iopub.status.idle": "2024-06-12T17:54:13.674169Z",
     "shell.execute_reply": "2024-06-12T17:54:13.673231Z",
     "shell.execute_reply.started": "2024-06-12T17:53:18.709635Z"
    }
   },
   "outputs": [],
   "source": [
    "# from kaggle_environments import make\n",
    "\n",
    "# def simple_agent(obs, cfg):\n",
    "#     if obs.turnType == \"ask\":\n",
    "#         response = \"それはアヒルですか？\"\n",
    "#     elif obs.turnType == \"guess\":\n",
    "#         response = \"アヒル\"\n",
    "#     elif obs.turnType == \"answer\":\n",
    "#         response = \"いいえ\"\n",
    "#     return response\n",
    "\n",
    "\n",
    "# debug_config = {'episodeSteps': 61,\n",
    "#                 'actTimeout': 60,      \n",
    "#                 'runTimeout': 1200,    \n",
    "#                 'agentTimeout': 3600}  \n",
    "\n",
    "# 環境を作成します\n",
    "# env = make(environment=\"llm_20_questions\", configuration=debug_config, debug=True)\n",
    "\n",
    "\n",
    "# ゲームを実行します\n",
    "# game_output = env.run(agents=[agent_fn, simple_agent, simple_agent, simple_agent])\n",
    "\n",
    "# 実行結果を表示します\n",
    "# env.render(mode=\"ipython\", width=1080, height=700)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11220,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
