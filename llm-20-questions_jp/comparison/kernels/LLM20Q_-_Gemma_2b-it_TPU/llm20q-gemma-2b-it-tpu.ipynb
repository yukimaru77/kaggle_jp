{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba991e1a",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LLM 20 Questions」コンペティションに参加するためのエージェントを構築・実行することを目的としています。具体的には、質問者エージェントと回答者エージェントを用いて「20の質問」ゲームをプレイする際のロジックを実装しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "- ゲーム「20の質問」において、質問者エージェントが効果的に質問を行い、回答者エージェントが「はい」または「いいえ」という形で応答することで、ターゲットとなる単語をできるだけ少ない質問で推測するための処理を行います。\n",
    "\n",
    "### 手法とライブラリ\n",
    "1. **TPU/GPU環境の設定**: \n",
    "   - TPUを使用するための設定が行われており、TPUデバイスに合わせたテンソル設定が施されています。\n",
    "   - `pytorch-xla`に関連するセットアップが行われており、`torch`ライブラリが利用されています。\n",
    "\n",
    "2. **Gemma モデルの利用**:\n",
    "   - `gemma_pytorch`というリポジトリからGemmaモデルの重みや設定を取得し、使用します。このモデルは因果生成モデルとして動作します。\n",
    "   - `GemmaForCausalLM`クラスを用いて、特定のコンフィギュレーションに基づいた言語モデルを初期化します。\n",
    "\n",
    "3. **エージェントの実装**:\n",
    "   - 質問者エージェント (`GemmaQuestionerAgent`) と回答者エージェント (`GemmaAnswererAgent`) がそれぞれ独自のロジックを持って実装され、ユーザーのターンを管理します。\n",
    "   - 質問と回答をインタリーブする機能や、ユーザーの入力をテンプレートに基づいてフォーマットするための `GemmaFormatter` クラスが定義されています。\n",
    "\n",
    "4. **環境の構築とゲームプレイ**:\n",
    "   - Kaggleの環境 (`kaggle_environments`)を使用して、ゲームをシミュレーションし、実際にエージェントが相互にプレイすることが可能な設定となっています。\n",
    "\n",
    "5. **提出とパッケージング**:\n",
    "   - 最終的には、生成されたエージェントと関連データを圧縮ファイルとして提出する準備が整えられています。\n",
    "\n",
    "このノートブックは、Kaggleでの限られた質問数での推測問題に対する戦略的かつ効率的なアプローチを示しているとともに、高度な自然言語処理能力を活かしたAIエージェントの設計を反映しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6998f8",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に基づいて、機械学習や深層学習に関連して初心者がつまずきそうな専門用語の簡単な解説を列挙します。\n",
    "\n",
    "### 専門用語と解説\n",
    "\n",
    "1. **ImmutableDict**\n",
    "   - 変更できない辞書のデータ構造を提供するライブラリで、主に不変性を保証するために使用される。プログラム中で意図しない変更を防ぎ、データの信頼性を高める。\n",
    "\n",
    "2. **SentencePiece**\n",
    "   - 文を単語単位でなく、サブワード単位で分割するためのトークナイザー。これは特に自然言語処理において、未知の単語の処理や多言語対応に役立つ。言語モデルの訓練データを効率的に扱うために使われる。\n",
    "\n",
    "3. **TPU (Tensor Processing Unit)**\n",
    "   - Googleが設計したハードウェアアクセラレータで、特に機械学習に最適化されている。TPUは高い計算性能を持ち、特に深層学習モデルの訓練や推論の高速化に利用される。\n",
    "\n",
    "4. **xm.xla_device()**\n",
    "   - XLA（Accelerated Linear Algebra）は、TensorFlowの最適化コンパイラで、TPUやGPUで実行するためのデバイスを指定する際に用いる。`xm`はPyTorch XLAライブラリで、TPUをサポートするために使われる。\n",
    "\n",
    "5. **Causal Language Model (CLM)**\n",
    "   - 過去のトークン（単語やサブワード）を基に次のトークンを予測するモデル。このアプローチは、生成タスク（文章生成や対話生成など）において効果的。\n",
    "\n",
    "6. **Few-shot Learning (FSL)**\n",
    "   - 限られた訓練データ（例: 数個のサンプルのみ）から学習する手法。特に、モデルが新しいタスクに迅速に適応するために利用される。\n",
    "\n",
    "7. **Interleave**\n",
    "   - 2つのシーケンス（リストなど）の要素を交互に結びつける処理を指す。特に、質問と回答のペアや、異なるエピソードのデータを混ぜ合わせる際に用いられる。\n",
    "\n",
    "8. **Top-k Sampling**\n",
    "   - 次の単語を生成する際に、最も確率の高いk個の単語から選ぶ手法。これにより、生成される文章の多様性を向上させる。\n",
    "\n",
    "9. **Top-p (nucleus) Sampling**\n",
    "   - 確率の合計がpになるまでの最小のトークンの集合から単語を選ぶ手法。この手法は、生成されたテキストの品質を向上させるために使用される。\n",
    "\n",
    "10. **Observations (obs)**\n",
    "    - エージェントが受け取る情報や状態を指しており、質問、回答、ターンの種類などが含まれるデータ構造。\n",
    "\n",
    "11. **Context Manager**\n",
    "    - Pythonの構文で、特定のリソースを管理するための便利な機能。コードのブロックが終了するときにリソースを自動的に解放し、エラー処理を簡素化するために使用される。\n",
    "\n",
    "12. **Regex (Regular Expressions)**\n",
    "    - 文字列パターンを調べるために使用される強力なツール。特定の形式に合致する文字列の検索や抽出、置換を行う際に使われる。\n",
    "\n",
    "以上の専門用語の解説が、初心者がつまずく可能性のあるポイントを理解する助けになることを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70feac16",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "# 現在の作業ディレクトリに移動\n",
    "cd /kaggle/working\n",
    "# immutabledict と sentencepiece パッケージをインストール\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorch リポジトリをクローン\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma のディレクトリを作成\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemma_pytorch から必要なファイルを移動\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# 現在の作業ディレクトリに移動\n",
    "cd /kaggle/working\n",
    "# immutabledict と sentencepiece パッケージをインストール\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "# gemma_pytorch リポジトリをクローン\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "# gemma のディレクトリを作成\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "# gemma_pytorch から必要なファイルを移動\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02915e35",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# for TPU\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# TPU用のセットアップ\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# 必要なAPTパッケージをインストール\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU用のセットアップ\n",
    "!curl https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n",
    "# 必要なAPTパッケージをインストール\n",
    "!python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31006d47",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# for GPU\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# for TPU\n",
    "device = xm.xla_device()\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# GPU用のデバイス設定\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TPU用のデバイス設定\n",
    "device = xm.xla_device()\n",
    "# デフォルトのテンソルタイプを設定\n",
    "torch.set_default_tensor_type('torch.FloatTensor')\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU用のデバイス設定\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TPU用のデバイス設定\n",
    "device = xm.xla_device()\n",
    "# デフォルトのテンソルタイプを設定\n",
    "torch.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c72543",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "import torch, itertools, contextlib\n",
    "import os, sys, re\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "from gemma.config import get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    def __init__(self, sp: str = None, fse: Iterable = None):\n",
    "        self._system_prompt = sp\n",
    "        self._few_shot_examples = fse\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, sp=None, fse=None):\n",
    "        self._device = xm.xla_device()\n",
    "        self.formatter = GemmaFormatter(sp=sp, fse=fse)\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b()\n",
    "        model_config.tokenizer = WEIGHTS_PATH + '/tokenizer.model'\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            model.load_weights(WEIGHTS_PATH + '/gemma-2b-it.ckpt')\n",
    "            self.model = model.to(self._device).eval()\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    def _call_llm(self, prompt, max_nt=40, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {'temperature': 0.8, 'top_p': 0.9, 'top_k': 60,}\n",
    "        response = self.model.generate(\n",
    "            prompt, device=self._device, output_len=max_nt, **sampler_kwargs,)\n",
    "        return response\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None: keyword = ''\n",
    "        else: keyword = match.group().lower()\n",
    "        return keyword\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: question = \"Is it a person?\" #make random choice for person, place, thing\n",
    "            else: question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else: raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "sp = \"You are playing the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "fse = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(sp=sp, fse=fse,)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(sp=sp, fse=fse,)\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\": response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\": response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\": response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1: return \"yes\"\n",
    "    else: return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "\n",
    "import torch, itertools, contextlib\n",
    "import os, sys, re\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggleエージェントのパスを指定\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # エージェントのライブラリパスを追加\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    # ウェイトファイルのパスを指定\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    # ローカルパスを指定\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "# gemmaの設定をインポート\n",
    "from gemma.config import get_config_for_2b\n",
    "# gemmaのモデルをインポート\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    \n",
    "    def __init__(self, sp: str = None, fse: Iterable = None):\n",
    "        self._system_prompt = sp\n",
    "        self._few_shot_examples = fse\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    \n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    \n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    \n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    \n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, sp=None, fse=None):\n",
    "        self._device = xm.xla_device()\n",
    "        self.formatter = GemmaFormatter(sp=sp, fse=fse)\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b()\n",
    "        model_config.tokenizer = WEIGHTS_PATH + '/tokenizer.model'\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            model.load_weights(WEIGHTS_PATH + '/gemma-2b-it.ckpt')\n",
    "            self.model = model.to(self._device).eval()\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    \n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _call_llm(self, prompt, max_nt=40, **sampler_kwargs):\n",
    "        # サンプリングのパラメータを設定\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {'temperature': 0.8, 'top_p': 0.9, 'top_k': 60,}\n",
    "        # モデルを使ってレスポンスを生成\n",
    "        response = self.model.generate(\n",
    "            prompt, device=self._device, output_len=max_nt, **sampler_kwargs,)\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # キーワードをレスポンスから抽出\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None: \n",
    "            keyword = ''\n",
    "        else: \n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    # 2つのリストを交互に結合\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: \n",
    "                question = \"Is it a person?\"  # 人、場所、物のランダムな選択\n",
    "            else: \n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else: \n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# ゲームの設定\n",
    "sp = \"You are playing the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "fse = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "agent = None\n",
    "\n",
    "# エージェント取得関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(sp=sp, fse=fse,)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(sp=sp, fse=fse,)\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "# エージェントの関数\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\": \n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    if response is None or len(response) <= 1: \n",
    "        return \"yes\"\n",
    "    else: \n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "import torch, itertools, contextlib\n",
    "import os, sys, re\n",
    "from typing import Iterable\n",
    "from pathlib import Path\n",
    "\n",
    "# Kaggleエージェントのパスを指定\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # エージェントのライブラリパスを追加\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    # ウェイトファイルのパスを指定\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/2b-it/2\")\n",
    "else:\n",
    "    # ローカルパスを指定\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/2b-it/2\"\n",
    "\n",
    "# gemmaの設定をインポート\n",
    "from gemma.config import get_config_for_2b\n",
    "# gemmaのモデルをインポート\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "    \n",
    "    def __init__(self, sp: str = None, fse: Iterable = None):\n",
    "        self._system_prompt = sp\n",
    "        self._few_shot_examples = fse\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "    \n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "    \n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "    \n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "    \n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "    \n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, sp=None, fse=None):\n",
    "        self._device = xm.xla_device()\n",
    "        self.formatter = GemmaFormatter(sp=sp, fse=fse)\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b()\n",
    "        model_config.tokenizer = WEIGHTS_PATH + '/tokenizer.model'\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            model.load_weights(WEIGHTS_PATH + '/gemma-2b-it.ckpt')\n",
    "            self.model = model.to(self._device).eval()\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "    \n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _call_llm(self, prompt, max_nt=40, **sampler_kwargs):\n",
    "        # サンプリングのパラメータを設定\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {'temperature': 0.8, 'top_p': 0.9, 'top_k': 60,}\n",
    "        # モデルを使ってレスポンスを生成\n",
    "        response = self.model.generate(\n",
    "            prompt, device=self._device, output_len=max_nt, **sampler_kwargs,)\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        # キーワードをレスポンスから抽出\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None: \n",
    "            keyword = ''\n",
    "        else: \n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    # 2つのリストを交互に結合\n",
    "    return [item for pair in itertools.zip_longest(x, y) for item in pair if item is not None]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None: \n",
    "                question = \"Is it a person?\"  # 人、場所、物のランダムな選択\n",
    "            else: \n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else: \n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "    \n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "# ゲームの設定\n",
    "sp = \"You are playing the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "fse = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "agent = None\n",
    "\n",
    "# エージェント取得関数\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(sp=sp, fse=fse,)\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(sp=sp, fse=fse,)\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "    return agent\n",
    "\n",
    "# エージェントの関数\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\": \n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\": \n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    if response is None or len(response) <= 1: \n",
    "        return \"yes\"\n",
    "    else: \n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cad49f5",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Testing\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# テスト\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b846598f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q pygame\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install -q pygame\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756d378d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from kaggle_environments import make\n",
    "# Kaggle環境のインスタンスを作成\n",
    "env = make(\"llm_20_questions\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kaggle_environments import make\n",
    "# Kaggle環境のインスタンスを作成\n",
    "env = make(\"llm_20_questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339e5b0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "#Run Code\n",
    "%run submission/main.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# コードを実行\n",
    "%run submission/main.py\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# コードを実行\n",
    "%run submission/main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23455322",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "#*** Needs to be fixed ****\n",
    "#env.run([get_agent('questioner'), \"random\"])\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "#*** 修正が必要 ***\n",
    "#env.run([get_agent('questioner'), \"random\"])\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#*** 修正が必要 ***\n",
    "#env.run([get_agent('questioner'), \"random\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8f4b26",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "env.render(mode=\"ipython\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# Kaggle環境を表示\n",
    "env.render(mode=\"ipython\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kaggle環境を表示\n",
    "env.render(mode=\"ipython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5227b8b8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Package\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# パッケージ\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff4b1d8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325674ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/2b-it/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/2b-it/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/2b-it/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
