{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8442f6e1",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleコンペティション「LLM 20 Questions」におけるエージェント作成プロセスを示しています。具体的には、20の質問ゲームを効果的にプレイするための言語モデル（LLM）エージェントを生成し、最終的に `submission.tar.gz` ファイルを作成します。このファイルはコンペティションに提出するために使用されます。\n",
    "\n",
    "### 問題と目的\n",
    "「LLM 20 Questions」は、プレイヤーが質問を通じて特定のターゲットを推測するゲームです。このノートブックは、質問者と回答者の役割を持つAIエージェントを開発し、効率的にターゲットを推測する能力を高めることを目的としています。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **ライブラリと依存関係**: \n",
    "  - `immutabledict` や `sentencepiece` などのライブラリをインストールし、モデルの動作に必要なファイルを適切なディレクトリに配置します。\n",
    "  - GitHubから`gemma_pytorch`リポジトリをクローンし、モデル用のファイルを作成した作業ディレクトリに移動させます。\n",
    "\n",
    "- **モデルの設定と初期化**: \n",
    "  - モデルの初期化にはGemmaライブラリを使用し、特に`GemmaForCausalLM`クラスが利用されます。ここでは7Bおよび2Bのモデル設定が用意されており、指定したバリアントに応じて適切なモデルを動的に選択しています。\n",
    "\n",
    "- **プロンプトフォーマッタ**: \n",
    "  - `GemmaFormatter`クラスを定義して、ゲームのプロンプトを適切にフォーマットし、ユーザーのインプットをモデルに渡すための準備を行います。\n",
    "\n",
    "- **エージェントクラス**: \n",
    "  - `GemmaQuestionerAgent`と`GemmaAnswererAgent`という2つのエージェントクラスが実装され、質問者と回答者の役割をそれぞれ担当します。それぞれのクラスは内部でプロンプトを生成し、モデルからの応答を解析して正しい質問や答えを生成します。\n",
    "\n",
    "### 出力\n",
    "最終的に、ノートブックは`submission.tar.gz`ファイルを生成し、それをコンペティションに提出できる状態にします。この成果物には、すべての必要なモデルファイルと実行可能なコードが含まれています。\n",
    "\n",
    "このノートブックは、言語モデルが協調プレイに基づいた推理を行うための出発点として機能します。参加者はこの基盤を元に、自らのモデルや戦略を実装し、さらに改善することが期待されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4528407",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブックに関連した専門用語の簡単な解説です。初心者がつまずきそうなマイナーな概念や特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **LLM (Large Language Model)**: 大規模なデータセットを使って訓練された言語モデルであり、人間のように文章を理解し生成する能力を持つ。特定のタスク（この場合は20の質問ゲーム）に調整されることが多い。\n",
    "\n",
    "2. **Few-shot examples**: モデルに対して数例の入力を与え、その例を基にモデルがタスクを理解して実行できるようにする。例えば、質問と答えのペアを示すことで、モデルがその形式を学ぶ。\n",
    "\n",
    "3. **Gemma**: Googleが開発した特定のLLMアーキテクチャで、テキスト生成タスクのためにチューニングされている。gemma_pytorchはそのPyTorch実装を含むリポジトリ。\n",
    "\n",
    "4. **Causal LM (Causal Language Model)**: 文脈から次の単語を予測するために自動回帰モデルを使用した言語モデル。過去の情報を基に未来の情報を生成する能力がある。\n",
    "\n",
    "5. **Tokenization**: 文を構成する単語や記号を「トークン」と呼ばれる小さな単位に分割するプロセス。トークン化は、モデルがテキストを理解しやすくするための重要なステップです。\n",
    "\n",
    "6. **Weight files (重みファイル)**: ニューラルネットワークにおける学習結果（モデルがパターンを学ぶ過程で得られるパラメータ）のこと。これらのファイルは、訓練されたモデルを再利用するために必要です。\n",
    "\n",
    "7. **Sampling parameters**: 生成するテキストの多様性や一貫性を調整するための設定（例：temperature, top_k, top_p）。これらのパラメータを調整することで、モデルの応答のスタイルや創造性が変わる。\n",
    "\n",
    "8. **Device (デバイス)**: モデルを実行するためのハードウェア。多くの場合、GPU（グラフィックス処理ユニット）を意味します。深層学習のトレーニングや推論においては、GPUを利用することで計算速度が大幅に向上します。\n",
    "\n",
    "9. **Interleave**: 異なる2つのリストの要素を交互に組み合わせる操作。ここでは、質問と回答を交互に結合してゲームの進行状況を保持します。\n",
    "\n",
    "10. **Context manager**: Pythonの機能で、特定の処理が始まる前に状態を設定し、処理が終わった後に元の状態に戻すための構文。リソース管理に便利です。\n",
    "\n",
    "これらの用語は、このノートブックだけでなく、機械学習や深層学習における実務や研究でもよく使われる概念です。初心者はこれらを理解することで、学習を深めることができるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96055fe7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "このノートブックは、**LLM 20 Questions** のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz` ファイルが生成されます。このファイルは、右側の **Submit to competition** 見出しから直接提出することができます。あるいは、ノートブックビューアから *Output* タブをクリックし、`submission.tar.gz` を見つけてダウンロードします。競技のホームページの左上にある **Submit Agent** をクリックしてファイルをアップロードし、提出を行ってください。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093fe013",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working  # 作業ディレクトリに移動します\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece  # 必要なライブラリをインストールします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null  # gemma_pytorchリポジトリをクローンします\n",
    "mkdir /kaggle/working/submission/lib/gemma/  # gemma用のディレクトリを作成します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/  # gemmaのファイルを移動させます\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working  # 作業ディレクトリに移動します\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece  # 必要なライブラリをインストールします\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null  # gemma_pytorchリポジトリをクローンします\n",
    "mkdir /kaggle/working/submission/lib/gemma/  # gemma用のディレクトリを作成します\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/  # gemmaのファイルを移動させます"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0981044",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# 設定\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で動作するように、システムパスを次のように設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHが存在するか確認します\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))  # KAGGLE_AGENT_PATHのlibディレクトリをパスに追加します\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")  # 指定のパスが存在しない場合、別のパスを追加します\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b  # gemmaモデル設定の取得\n",
    "from gemma.model import GemmaForCausalLM  # gemmaモデルのインポート\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHが存在する場合\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")  # 重みファイルのパスを設定\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"  # 他のパスを設定\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'  # ターンの開始トークン\n",
    "    _end_token = '<end_of_turn>'  # ターンの終了トークン\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt  # システムプロンプトの保存\n",
    "        self._few_shot_examples = few_shot_examples  # Few-shot例の保存\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"  # ユーザーのターンをフォーマット\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"  # モデルのターンをフォーマット\n",
    "        self.reset()  # 状態をリセット\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state  # 現在の状態を返す\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)  # ユーザーのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)  # モデルのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"  # ユーザーターンの開始を追加\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"  # モデルターンの開始を追加\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"  # ターンの終了を追加\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"  # 状態を初期化\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  # システムプロンプトを追加\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # Few-shot例のターンを適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]  # ターンの実行順序を決定\n",
    "        formatters = itertools.cycle(formatters)  # 順序を循環させる\n",
    "        for fmt, turn in zip(formatters, turns):  # フォーマッタとターンを結合\n",
    "            fmt(turn)  # フォーマッタでターンを処理\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)  # デフォルトのdtypeを変更\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)  # 元のdtypeに戻す\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant  # エージェントのバリアントを保存\n",
    "        self._device = torch.device(device)  # 使用するデバイスを指定\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)  # フォーマッタを初期化\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()  # モデル設定を取得\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザーのパスを設定\n",
    "        model_config.quant = \"quant\" in variant  # 量子化設定\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):  # データ型を設定しモデルを初期化\n",
    "            model = GemmaForCausalLM(model_config)  # モデルを初期化\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')  # 重みのチェックポイントパスを設定\n",
    "            model.load_weights(ckpt_path)  # 重みをロード\n",
    "            self.model = model.to(self._device).eval()  # モデルを指定デバイスに移動し評価モードにする\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = str(self.formatter)  # フォーマッタの内容を文字列に変換\n",
    "        response = self._call_llm(prompt)  # LLMにプロンプトを渡して応答を得る\n",
    "        response = self._parse_response(response, obs)  # 応答を解析\n",
    "        print(f\"{response=}\")  # 応答をコンソールに出力\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # メソッドを抽象化\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {  # サンプリングのパラメータを設定\n",
    "                'temperature': 0.01,  # 温度パラメータ\n",
    "                'top_p': 0.1,  # トップ確率\n",
    "                'top_k': 1,  # トップKの設定\n",
    "        }\n",
    "        response = self.model.generate(  # モデルから新しいトークンを生成\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,  # 最大トークン数\n",
    "            **sampler_kwargs,  # その他のサンプリングパラメータ\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''  # キーワードが見つからない場合\n",
    "        else:\n",
    "            keyword = match.group().lower()  # キーワードを小文字に変換\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # メソッドを抽象化\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [  # 不均一なリストを交互に結合\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に結合\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")  # 質問するよう指示\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")  # キーワード予想の指示\n",
    "        self.formatter.start_model_turn()  # モデルターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # 質問を抽出\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"  # デフォルトの質問\n",
    "            else:\n",
    "                question = match.group()  # 抽出した質問\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)  # キーワードの予想を解析\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)  # エラーハンドリング\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に結合\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ターンを適用\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")  # 回答指示メッセージ\n",
    "        self.formatter.start_model_turn()  # モデルターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # 回答を解析\n",
    "        return 'yes' if 'yes' in answer else 'no'  # \"yes\"であればyes、その他はnoを返す\n",
    "\n",
    "\n",
    "# エージェント作成\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"  # システムプロンプトの設定\n",
    "\n",
    "few_shot_examples = [  # Few-shot例の設定\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントはグローバルに定義します。必要なエージェントだけをロードするため。\n",
    "# 両方をロードすると、OOM（Out of Memory）に繋がる可能性があります。\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # 質問者エージェントの初期化\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # 回答者エージェントの初期化\n",
    "    assert agent is not None, \"Agent not initialized.\"  # エージェントが初期化されているか確認\n",
    "\n",
    "    return agent  # エージェントを返す\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントの応答を取得\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントの応答を取得\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントの応答を取得\n",
    "    if response is None or len(response) <= 1:  # 応答が空か長さが1以下の場合\n",
    "        return \"yes\"  # デフォルトの応答\n",
    "    else:\n",
    "        return response  # 通常の応答を返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# 設定\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で動作するように、システムパスを次のように設定します。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHが存在するか確認します\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))  # KAGGLE_AGENT_PATHのlibディレクトリをパスに追加します\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")  # 指定のパスが存在しない場合、別のパスを追加します\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b  # gemmaモデル設定の取得\n",
    "from gemma.model import GemmaForCausalLM  # gemmaモデルのインポート\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):  # KAGGLE_AGENT_PATHが存在する場合\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")  # 重みファイルのパスを設定\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"  # 他のパスを設定\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'  # ターンの開始トークン\n",
    "    _end_token = '<end_of_turn>'  # ターンの終了トークン\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt  # システムプロンプトの保存\n",
    "        self._few_shot_examples = few_shot_examples  # Few-shot例の保存\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"  # ユーザーのターンをフォーマット\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"  # モデルのターンをフォーマット\n",
    "        self.reset()  # 状態をリセット\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state  # 現在の状態を返す\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)  # ユーザーのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)  # モデルのプロンプトを追加\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"  # ユーザーターンの開始を追加\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"  # モデルターンの開始を追加\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"  # ターンの終了を追加\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"  # 状態を初期化\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  # システムプロンプトを追加\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')  # Few-shot例のターンを適用\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]  # ターンの実行順序を決定\n",
    "        formatters = itertools.cycle(formatters)  # 順序を循環させる\n",
    "        for fmt, turn in zip(formatters, turns):  # フォーマッタとターンを結合\n",
    "            fmt(turn)  # フォーマッタでターンを処理\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェント定義\n",
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"指定されたdtypeにデフォルトのtorch dtypeを設定します。\"\"\"\n",
    "    torch.set_default_dtype(dtype)  # デフォルトのdtypeを変更\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)  # 元のdtypeに戻す\n",
    "\n",
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant  # エージェントのバリアントを保存\n",
    "        self._device = torch.device(device)  # 使用するデバイスを指定\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)  # フォーマッタを初期化\n",
    "\n",
    "        print(\"モデルの初期化中\")\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()  # モデル設定を取得\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")  # トークナイザーのパスを設定\n",
    "        model_config.quant = \"quant\" in variant  # 量子化設定\n",
    "\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):  # データ型を設定しモデルを初期化\n",
    "            model = GemmaForCausalLM(model_config)  # モデルを初期化\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')  # 重みのチェックポイントパスを設定\n",
    "            model.load_weights(ckpt_path)  # 重みをロード\n",
    "            self.model = model.to(self._device).eval()  # モデルを指定デバイスに移動し評価モードにする\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)  # セッションを開始\n",
    "        prompt = str(self.formatter)  # フォーマッタの内容を文字列に変換\n",
    "        response = self._call_llm(prompt)  # LLMにプロンプトを渡して応答を得る\n",
    "        response = self._parse_response(response, obs)  # 応答を解析\n",
    "        print(f\"{response=}\")  # 応答をコンソールに出力\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError  # メソッドを抽象化\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {  # サンプリングのパラメータを設定\n",
    "                'temperature': 0.01,  # 温度パラメータ\n",
    "                'top_p': 0.1,  # トップ確率\n",
    "                'top_k': 1,  # トップKの設定\n",
    "        }\n",
    "        response = self.model.generate(  # モデルから新しいトークンを生成\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,  # 最大トークン数\n",
    "            **sampler_kwargs,  # その他のサンプリングパラメータ\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)  # キーワードを抽出\n",
    "        if match is None:\n",
    "            keyword = ''  # キーワードが見つからない場合\n",
    "        else:\n",
    "            keyword = match.group().lower()  # キーワードを小文字に変換\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError  # メソッドを抽象化\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [  # 不均一なリストを交互に結合\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に結合\n",
    "        self.formatter.apply_turns(turns, start_agent='model')  # ターンを適用\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")  # 質問するよう指示\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")  # キーワード予想の指示\n",
    "        self.formatter.start_model_turn()  # モデルターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))  # 質問を抽出\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"  # デフォルトの質問\n",
    "            else:\n",
    "                question = match.group()  # 抽出した質問\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)  # キーワードの予想を解析\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)  # エラーハンドリング\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)  # 親クラスの初期化\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()  # フォーマッタをリセット\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")  # ゲーム開始メッセージ\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)  # 質問と回答を交互に結合\n",
    "        self.formatter.apply_turns(turns, start_agent='user')  # ターンを適用\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")  # 回答指示メッセージ\n",
    "        self.formatter.start_model_turn()  # モデルターンを開始\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)  # 回答を解析\n",
    "        return 'yes' if 'yes' in answer else 'no'  # \"yes\"であればyes、その他はnoを返す\n",
    "\n",
    "\n",
    "# エージェント作成\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"  # システムプロンプトの設定\n",
    "\n",
    "few_shot_examples = [  # Few-shot例の設定\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is it a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "# **重要:** エージェントはグローバルに定義します。必要なエージェントだけをロードするため。\n",
    "# 両方をロードすると、OOM（Out of Memory）に繋がる可能性があります。\n",
    "agent = None\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # 質問者エージェントの初期化\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )  # 回答者エージェントの初期化\n",
    "    assert agent is not None, \"Agent not initialized.\"  # エージェントが初期化されているか確認\n",
    "\n",
    "    return agent  # エージェントを返す\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントの応答を取得\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)  # 質問者エージェントの応答を取得\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)  # 回答者エージェントの応答を取得\n",
    "    if response is None or len(response) <= 1:  # 応答が空か長さが1以下の場合\n",
    "        return \"yes\"  # デフォルトの応答\n",
    "    else:\n",
    "        return response  # 通常の応答を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c0b6fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null  # pigzとpvをインストールします\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null  # pigzとpvをインストールします"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801d1b64",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2  # submission.tar.gzファイルを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2  # submission.tar.gzファイルを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db73b66",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Samar Elhissi\n",
    "> \n",
    "> 例をありがとう、ローカルでテストするにはどうすれば良いですか？\n",
    "> \n",
    "> \n",
    "> \n",
    "> > ## Valentin Baltazar\n",
    "> > \n",
    "> > ハードウェアがあるか確認してください…このLLMは多くの計算を必要とし、トレーニングとファインチューニングには強力なGPUが必要です。クラウドを利用するのが、はるかに簡単です。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Michael Kamal 92\n",
    "> \n",
    "> ありがとうございます、few_shot_examplesについて質問したいのですが、どのように作成すれば良いですか？\n",
    "> > 例えば、( 'is it place?', 'yes-or-no' )のようにする必要がありますか、それとも( 'is it place?', 'yes', ) のようにすれば良いですか？それとも( 'is it place?', 'yes', 'Now guess the keyword' )でしょうか？それとも( 'is it place?', 'no', 'Now guess the keyword', 'France' )でしょうか？それとも( 'is it place?', 'yes', 'France' )でしょうか？どれが正しい質問、回答、予測の作り方ですか？\n",
    "> \n",
    "> もう一つの質問ですが、Gemmaはfew_shot_examplesでトレーニングしますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## Yukky_2801\n",
    "> \n",
    "> こんにちは、私はKaggleの初心者です。あなたのノートブックを実行すると、以下のエラーが出ました：\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: 前のエラーのために失敗したと終了します。\n",
    "> \n",
    "> submission.tar.gzをエラーと共に提出できません。どういうことかわからないのですが、解決策を提供していただけますか？\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> > もちろん「gemma/pytorch/7b-it-quant/2」このパスにアクセスしようとしています。ファイルがそのパスにあることを確認してください（ノートブックの右側を見て、gemmaモデルがそこのパスと一致しているか確認してください）。\n",
    ">   \n",
    "> > ## Aryan Singh\n",
    "> > > Gemma 7b-it-quant V2を追加するには、Add Input機能を使用します。\n",
    "> > > \n",
    "> > > まず、こちらでライセンスを受け入れることを確認してください：[https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "> > \n",
    "> > \n",
    "> > > ## Talal Mufti\n",
    "> > > > すべてのファイルがそのパスにあることを確認した後、依然として問題が発生したため、bashコマンドを少し修正しました。個人的には、これが私にとってうまくいきました：\n",
    "> > > > !tar --use-compress-program='pigz --fast --recursive | pv' -f submission.tar.gz -c /kaggle/working/submission . -c /kaggle/input/gemma/pytorch/7b-it-quant/2\n",
    "\n",
    "---\n",
    "\n",
    "> ## Muhammad Hadi13\n",
    "> \n",
    "> なぜファイルをコピーして実行しているのに、常に1.35MB以上の出力が生成されず、バリデーションエピソードで失敗するのかわかりません。Ryanの出力は約7GBでした。この件について助けが必要です！！\n",
    "> \n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> > tar: gemma/pytorch: Cannot stat: No such file or directory\n",
    "> \n",
    "> tar: 前のエラーのために終了しました。\n",
    "\n",
    "> ## Aryan Singh\n",
    "> > > Gemma 7b-it-quant V2を事前に追加する必要があります。\n",
    "> > > \n",
    "> > > ノートブック内でモデルを追加するには、Add input機能を使用します。\n",
    "> > > \n",
    "> > > まず、こちらでライセンスを受け入れることを確認してください：[https://www.kaggle.com/models/google/gemma](https://www.kaggle.com/models/google/gemma)\n",
    "\n",
    "---\n",
    "\n",
    "> ## Ship of Theseus\n",
    "> \n",
    "> Thank Ryan, greate work! Nice code to run on localhost and sharing to Kaggle Community\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## shiv_314\n",
    "> \n",
    "> 皆さん！一つ助けが必要です。gemmaパッケージでインポートエラーが発生しています。\n",
    "> \n",
    "> Pythonのためにすでに正しいシステムパスを追加しましたが、それでも同じ問題が発生しています。助けてください！\n",
    "\n",
    "---\n",
    "\n",
    "> ## dedq\n",
    "> \n",
    "> Thank Ryan, greate work! Nice code to run on localhost and sharing to Kaggle Community\n",
    "\n",
    "---\n",
    "\n",
    "> ## Code Hacker\n",
    "> \n",
    "> このノートブックの出力ファイルtar.gzを提出しようとしたが、失敗しました…\n",
    "\n",
    "> ## Code Hacker\n",
    "> > > このモデルに同意しなかった。下の赤いボタンをクリック…\n",
    "\n",
    "---\n",
    "\n",
    "> ## JAPerez\n",
    "> \n",
    "> Great work Ryan!\n",
    "\n",
    "---\n",
    "\n",
    "> ## philipha2\n",
    "> \n",
    "> こんにちは、私はこのコンペの初心者です。あなたのノートブックを実行して提出しようとしました。\n",
    "> 提出物のファイルであるsubmission.tar.gzをどこに置けばよいのでしょうか？ \n",
    "> Submit agentsボタンをクリックした後、このファイルをそのまま提出すればよいですか？ \n",
    "> 時間がかかります\n",
    "> 基本的な質問かもしれませんが、返信ありがとうございます！\n",
    "\n",
    "> ## Kanchan Maurya\n",
    "> > > Submit agentsをクリックした後、このファイルを提出しています。それに時間がかかるのは、初期シミュレーションが機能しているためです。\n",
    "\n",
    "---\n",
    "\n",
    "> ## vj4science\n",
    "> \n",
    "> Thanks Ryan - this is a good head start to the competition! much appreciated!\n",
    "\n",
    "---\n",
    "\n",
    "> ## gb_kwon\n",
    "> \n",
    "> Thank you so much for your COOL guidelines!\n",
    "\n",
    "---\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> \n",
    "> main.pyで、gemma_pytorchライブラリを次のようにインポートしています：from gemma.config。\n",
    "> \n",
    "> これは私には機能しませんが、gemmaとインポートするとエラーは出ません。\n",
    "> \n",
    "> 自分のローカルのgemmaモジュールのパスを手動で指定するのと、Pythonライブラリ名でインポートするのを試みましたが。何かアイデアはありますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## Duy Thai\n",
    "> \n",
    "> こんにちは[@ryanholbrook](https://www.kaggle.com/ryanholbrook)、あなたのノートブックを試したところ、「添付されたモデルはアクセスするために追加の手順が必要です。詳細はModelパネルを参照してください。」というメッセージが表示されました。それについてはどうすればよいでしょうか？\n",
    "> \n",
    "> パネルを開いたとき、私はこれだけを見ています：\n",
    "\n",
    "> ## Andres H. Zapke\n",
    "> > > \"Models\"へ行き、Gemmaを検索し、モデルのライセンスを受け入れます。\n",
    "\n",
    "> ## Duy Thai\n",
    "> > > ありがとうございます！\n",
    "\n",
    "---\n",
    "\n",
    "> ## Kai_Huang\n",
    "> \n",
    "> こんにちは、私はKaggleの初心者です。あなたの !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2 のコードブロックを実行したとき、次のエラーが出ました：\n",
    "> \n",
    "> tar: gemma/pytorch/7b-it-quant/2: Cannot stat: No such file or directory\n",
    "> \n",
    "> 1.37MiB 0:00:00 [36.4MiB/s] [<=> ]\n",
    "> \n",
    "> tar: 前のエラーのために終了しました\n",
    "> \n",
    "> どういうことかわからないのですが、教えていただけますか？ありがとうございます！\n",
    "\n",
    "> ## Kai_Huang\n",
    "> > > ああ、わかりました。モデルをノートブックに入力していなかったのですね😱\n",
    "\n",
    "> ## D Prince Armand KOUMI\n",
    "> > > モデルがない場合は、追加してみてください。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Qusay AL-Btoush\n",
    "> \n",
    "> とても良い、Ryan \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
