{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58ee3535",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleのコンペティション「LLM 20 Questions」に参加するためのエージェントを作成するプロセスを示しています。具体的には、質問者と回答者の役割を果たすエージェントを実装し、言語モデル（LLM）を使用して質問-回答ゲームを進行することを目的としています。\n",
    "\n",
    "### 問題の概要\n",
    "このノートブックは、言語モデルを活用して「20の質問」ゲームを自動化するエージェントを構築するためのフレームワークを提供しています。エージェントは、与えられたゲームの状況に基づき、質問を生成し、またその質問に対する「はい」または「いいえ」での応答を提供する必要があります。最終的には、ターゲット単語を最速で当てることが求められます。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "- **Gemma**: このノートブックではGoogleの`gemma_pytorch`ライブラリを使用しています。このライブラリは、因果言語モデルの訓練と推論に特化しています。\n",
    "- **ImmutableDict**と**SentencePiece**: 質問と回答を効率的に処理するために`immutabledict`と`sentencepiece`パッケージをインストールしています。`immutabledict`は変更不可の辞書を提供し、`sentencepiece`はトークナイザとして使用されます。\n",
    "- **PyTorch**: 言語モデルの実行にはPyTorchが利用されており、GPU上での処理が行われています。\n",
    "\n",
    "### 主なセクション\n",
    "1. **環境のセットアップ**: 必要なパッケージをインストールし、`gemma_pytorch`ライブラリを含むディレクトリを作成します。\n",
    "   \n",
    "2. **エージェント定義**:\n",
    "   - `GemmaAgent`クラスは、言語モデルとやり取りし、プロンプトを生成して応答を得るための基本機能を持ちます。\n",
    "   - `GemmaQuestionerAgent`と`GemmaAnswererAgent`クラスは、それぞれ質問者と回答者のエージェントを実装しています。これらのクラスは各ターンの処理、プロンプトのフォーマッティング、モデルからの応答解析を行います。\n",
    "\n",
    "3. **エージェントの動作**: \n",
    "   - ゲームの状態を表すデータをもとに、エージェントのメソッドを呼び出し、適切な応答を生成します。\n",
    "   - 各エージェントは、ターンタイプ（質問、推測、回答）に応じて異なるロジックを実行します。\n",
    "\n",
    "4. **ファイルの圧縮と提出準備**: 最後に、生成されたファイルと必要なモデルファイルを一つのtarボールに圧縮し、Kaggleへの提出を容易にします。\n",
    "\n",
    "このノートブックは、言語モデルを使用した「20の質問」ゲームの実行に必要なエージェントの構築方法を詳細に説明しており、コメントが豊富に含まれているため、実際の実装の概念を理解しやすくなっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04047a12",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、あなたが示したJupyterノートブックの内容から、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説です。特にマイナーなものや実務を経験していないと馴染みのないもの、ノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **Gemma**:\n",
    "   - このノートブック内で使用されている言語モデルの名称。本コンペティションにおける20の質問ゲームに特化したモデル。\n",
    "\n",
    "2. **Causal LM (因果言語モデル)**:\n",
    "   - 文章を生成する際に、過去の単語やトークンのみを考慮して次に来る単語を予測するタイプのモデル。自然言語生成において広く使われる。\n",
    "\n",
    "3. **量子化 (Quantization)**:\n",
    "   - モデルの重みや活性化値を低ビットの整数に変換する技術。これにより、モデルのサイズを縮小し、計算を効率化することができる。`get_config_for_2b`や`get_config_for_7b`はそれぞれ異なるモデルの設定を取得するための関数。\n",
    "\n",
    "4. **Interleave (インターリーブ)**:\n",
    "   - 2つのリストの要素を交互に並べる操作。リストの長さが異なる場合は、短い方のリストが終わるまで要素を交互に並べ、残りはNoneで埋める。\n",
    "\n",
    "5. **サンプリング (Sampling)**:\n",
    "   - 言語モデルからの出力生成の際に、次に生成するトークンを決定する方法。一般的には、確率的に選択されたトークンが使用され、様々なサンプリング手法（例：温度設定、top-k、top-p）がある。\n",
    "\n",
    "6. **Few-shot examples**:\n",
    "   - モデルに特定のタスクを教えるために提供される少数の例。この場合、プレイするゲームの例が含まれている。\n",
    "\n",
    "7. **デフォルトのテンソル型 (Default Tensor Type)**:\n",
    "   - PyTorchにおいて、テンソルを作成する際に使用されるデフォルトのデータ型。特定のデータ型を指定することで、メモリの使用量や計算速度を最適化することができる。\n",
    "\n",
    "8. **contextlib.contextmanager**:\n",
    "   - Pythonの標準ライブラリに含まれるモジュールで、資源の管理を簡易化するための文脈管理を提供する。特にリソースが必要な時、使い終わった時に自動的に解放されるべき場合に役立つ。\n",
    "\n",
    "9. **tar (Tape ARchiver)**:\n",
    "   - Unix系のオペレーティングシステムで、ファイルをアーカイブするためのコマンド。複数のファイルを一つのファイルにまとめる際に使用される。\n",
    "\n",
    "10. **pigz**:\n",
    "    - gzip（GNU zip）の並列実装であり、マルチコアCPUを使ってフィアルを圧縮するためのツール。大容量のファイルを迅速に圧縮できる。\n",
    "\n",
    "11. **pv (Pipe Viewer)**:\n",
    "    - パイプのデータを監視し、その進捗や統計情報を表示するためのツール。これにより、長時間かかる処理の進行状況を確認できる。\n",
    "\n",
    "これらの用語は、ノートブック内で特有のコンテキストや技術的な背景を持つため、初心者にとっては理解するのが難しいかもしれません。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370deb3",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "While working through the [Starter Notebook](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook), I created this fully documented version jointly with my buddy ChatGPT. The code is 100% the same, no changes, but I hope you find the comments useful.\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "[スタートノートブック](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook)を進める中で、私は友人のChatGPTと共同でこの完全に文書化されたバージョンを作成しました。コードは100％同じで、変更はありませんが、コメントが役に立つことを期待しています。\n",
    "\n",
    "このノートブックは、**LLM 20 Questions**のエージェント作成プロセスを示しています。このノートブックを実行すると、`submission.tar.gz`ファイルが生成されます。このファイルは右側の**コンペティションに提出**という見出しから直接提出できます。あるいは、ノートブックビューワーから*Output*タブをクリックして`submission.tar.gz`を見つけ、ダウンロードします。競技ホームページの左上にある**エージェントを提出**をクリックして、ファイルをアップロードし、提出を完了させます。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03083edc",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:06:48.488574Z",
     "iopub.status.busy": "2024-06-06T05:06:48.488156Z",
     "iopub.status.idle": "2024-06-06T05:07:03.276374Z",
     "shell.execute_reply": "2024-06-06T05:07:03.275345Z",
     "shell.execute_reply.started": "2024-06-06T05:06:48.488539Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b6af3",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "The above cell sets up the environment by installing required Python packages and preparing the `gemma_pytorch` library for use in the agent. This ensures that all necessary dependencies are bundled together in the submission file:\n",
    "\n",
    "- `%%bash` is a cell magic command in Jupyter notebooks that indicates that the entire cell should be executed as a bash script. This means that all the lines following the `%%bash` will be interpreted and run as bash commands in the shell, rather than as Python code.\n",
    "\n",
    "- `cd /kaggle/working` changes the current directory to `/kaggle/working`, which is a working directory in the Kaggle environment where you can store files and set up your workspace.\n",
    "\n",
    "- `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece` installs the `immutabledict` and `sentencepiece` Python packages.\n",
    "   - `-q` suppresses the output from the installation process (quiet mode).\n",
    "   - `-U` upgrades the packages if they are already installed.\n",
    "   - `-t /kaggle/working/submission/lib` specifies the target directory where the packages should be installed. This ensures that the dependencies are included in the submission package.\n",
    "\n",
    "- `git clone https://github.com/google/gemma_pytorch.git > /dev/null` clones the [`gemma_pytorch` repository](https://github.com/google/gemma_pytorch) from GitHub into the current directory (`/kaggle/working`).\n",
    "   - `> /dev/null` redirects the output of the `git clone` command to `/dev/null`, effectively hiding it from the notebook's output. The `/dev/null` is a special file in Unix-like operating systems, including the environment used by Kaggle. It acts as a black hole for data: any data written to `/dev/null` is discarded and cannot be retrieved. It is not a directory or a space where you can find files; rather, it is a way to suppress output, keeping the notebook output clean and free of unnecessary details.\n",
    "\n",
    "- `mkdir /kaggle/working/submission/lib/gemma/` creates a new directory named `gemma` inside the `submission/lib` directory. This directory will hold the files from the `gemma_pytorch` repository.\n",
    "\n",
    "- `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/` moves all the files from the `gemma` directory inside the cloned `gemma_pytorch` repository to the newly created `gemma` directory in `submission/lib`. This ensures that the necessary files from the `gemma_pytorch` repository are included in the submission package.\n",
    "\n",
    "These are the packeges which are installed:\n",
    "\n",
    "**immutabledict** is a package that provides an immutable dictionary. Immutable dictionaries are useful when you need to ensure that the dictionary cannot be modified after it has been created. This can help prevent accidental changes to the data structure, which is particularly important in certain applications like configurations, constants, or when working with concurrent programming.\n",
    "\n",
    "**sentencepiece** is the tokenizer of gemma\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "上記のセルは、必要なPythonパッケージをインストールし、エージェントで使用するために`gemma_pytorch`ライブラリを準備することで、環境をセットアップします。これにより、すべての必要な依存関係が提出ファイルにバンドルされます：\n",
    "\n",
    "- `%%bash`はJupyterノートブックで使用されるセルマジックコマンドであり、セル全体がbashスクリプトとして実行されるべきであることを示します。これにより、`%%bash`以降のすべての行が、Pythonコードではなく、シェル内でbashコマンドとして解釈されて実行されます。\n",
    "\n",
    "- `cd /kaggle/working`は、現在の作業ディレクトリを`/kaggle/working`に変更します。このディレクトリはKaggle環境内の作業ディレクトリであり、ファイルを保存したり、ワークスペースを設定したりできます。\n",
    "\n",
    "- `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece`は、`immutabledict`および`sentencepiece` Pythonパッケージをインストールします。\n",
    "   - `-q`はインストールプロセスからの出力を抑制します（クワイエットモード）。\n",
    "   - `-U`は、すでにインストールされているパッケージをアップグレードします。\n",
    "   - `-t /kaggle/working/submission/lib`は、パッケージをインストールする対象ディレクトリを指定します。これにより、依存関係が提出パッケージに含まれるようになります。\n",
    "\n",
    "- `git clone https://github.com/google/gemma_pytorch.git > /dev/null`は、GitHubから現在のディレクトリ（`/kaggle/working`）に[`gemma_pytorch`リポジトリ](https://github.com/google/gemma_pytorch)をクローンします。\n",
    "   - `> /dev/null`は`git clone`コマンドの出力を`/dev/null`にリダイレクトし、実質的にその出力をノートブックの出力から隠します。`/dev/null`は、Unix系オペレーティングシステムにおける特別なファイルで、データを記録すると捨てられ、取り出すことができません。これは、出力を抑制し、ノートブックの出力を整理して不必要な詳細を避ける方法です。\n",
    "\n",
    "- `mkdir /kaggle/working/submission/lib/gemma/`は、`submission/lib`ディレクトリ内に`gemma`という名前の新しいディレクトリを作成します。このディレクトリには、`gemma_pytorch`リポジトリからのファイルが格納されます。\n",
    "\n",
    "- `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/`は、クローンした`gemma_pytorch`リポジトリの`gemma`ディレクトリ内のすべてのファイルを、新しく作成した`submission/lib/gemma/`ディレクトリに移動します。これにより、`gemma_pytorch`リポジトリから必要なファイルが提出パッケージに含まれるようになります。\n",
    "\n",
    "これらはインストールされるパッケージです：\n",
    "\n",
    "**immutabledict**は、変更不可の辞書を提供するパッケージです。変更不可の辞書は、作成後に辞書が変更されないことを保証する必要があるときに便利です。これは、特に構成、定数、または並行プログラミングで作業する際に重要です。\n",
    "\n",
    "**sentencepiece**は、gemmaのトークナイザーです。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c162f95c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Import necessary modules for handling tensor operations and model configurations\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# Define the path for model weights.\n",
    "# Check if the script is running in the Kaggle simulation environment.\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # If running in the Kaggle simulation environment, set the weights path accordingly\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # If running locally or in a different environment, use the local input directory for weights\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    Class for formatting prompts and responses for the 20 Questions game.\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaFormatter with an optional system prompt and few-shot examples.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt (str): Initial prompt for the system.\n",
    "            few_shot_examples (Iterable): Examples to initialize the prompt.\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return the current state of the prompt.\n",
    "        \n",
    "        Returns:\n",
    "            str: The current formatted state.\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        Add a user's prompt to the current state.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The user's prompt.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        Add a model's prompt to the current state.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The model's prompt.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        Start a new user turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        Start a new model turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        End the current turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the formatter to its initial state,\n",
    "        including system prompt and few-shot examples if provided.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        Apply a sequence of turns to the formatter.\n",
    "        \n",
    "        Args:\n",
    "            turns (Iterable): The sequence of turns to apply.\n",
    "            start_agent (str): Specifies whether 'user' or 'model' starts first.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    Context manager for setting the default tensor type in PyTorch.\n",
    "\n",
    "    This context manager temporarily sets the default tensor type to the specified dtype\n",
    "    and restores the original default tensor type upon exiting the context.\n",
    "\n",
    "    Args:\n",
    "        dtype (torch.dtype): The desired default tensor type to set.\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #Set the default torch dtype to the given dtype.\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # Restore the default tensor type to float\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Base class for an agent that interacts with the Gemma language model.\n",
    "    This class handles model initialization, prompt formatting, and response generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaAgent with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            variant (str): The model variant to use (e.g., '7b-it-quant').\n",
    "            device (str): The device to run the model on (e.g., 'cuda:0' for GPU).\n",
    "            system_prompt (str): Initial prompt for the system.\n",
    "            few_shot_examples (Iterable): Examples to initialize the prompt.\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        # Get the model configuration based on the variant\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # Set the default tensor type and initialize the model\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        Handle a new observation by starting a session, generating a prompt, and parsing the response.\n",
    "\n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "            *args: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        Start a new session for the agent.\n",
    "\n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        Generate a response from the language model using the provided prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt for the language model.\n",
    "            max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "            **sampler_kwargs: Additional keyword arguments for the sampling process.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        Extract the keyword from the model's response.\n",
    "\n",
    "        Args:\n",
    "            response (str): The model's response.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted keyword, or an empty string if no keyword is found.\n",
    "        \"\"\"\n",
    "        # find a substring that is enclosed between double asterisks (**).\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response based on the observation.\n",
    "\n",
    "        Args:\n",
    "            response (str): The model's response.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    Interleave two lists, x and y, handling unequal lengths by filling in None for missing values.\n",
    "    \n",
    "    This function takes two lists and interleaves their elements. If the lists have unequal lengths,\n",
    "    it uses None to fill in the missing values and excludes these None values from the final result.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to interleave.\n",
    "        y (list): The second list to interleave.\n",
    "\n",
    "    Returns:\n",
    "        list: A list with elements from x and y interleaved, excluding None values.\n",
    "    \n",
    "    Example:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    Agent for playing the role of the Questioner in the 20 Questions game.\n",
    "    \n",
    "    This agent is responsible for generating questions based on the game state and \n",
    "    parsing responses from the language model to continue the questioning process.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaQuestionerAgent with the provided arguments.\n",
    "        \n",
    "        Args:\n",
    "            *args: Positional arguments to pass to the base class initializer.\n",
    "            **kwargs: Keyword arguments to pass to the base class initializer.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        Start a new session for the Questioner agent by resetting the formatter \n",
    "        and applying the initial prompt and previous turns.\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.questions (list): List of previous questions asked.\n",
    "                - obs.answers (list): List of corresponding answers received.\n",
    "                - obs.turnType (str): Type of the current turn ('ask' or 'guess').\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response based on the type of turn in the game.\n",
    "        \n",
    "        Args:\n",
    "            response (str): The response generated by the language model.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.turnType (str): Type of the current turn ('ask' or 'guess').\n",
    "        \n",
    "        Returns:\n",
    "            str: The parsed question or guess based on the turn type.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the turn type in the observation is unknown.\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    Agent for playing the role of the Answerer in the 20 Questions game.\n",
    "    \n",
    "    This agent is responsible for providing yes-or-no answers based on the game state and \n",
    "    parsing responses from the language model to continue the answering process.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaAnswererAgent with the provided arguments.\n",
    "        \n",
    "        Args:\n",
    "            *args: Positional arguments to pass to the base class initializer.\n",
    "            **kwargs: Keyword arguments to pass to the base class initializer.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        Start a new session for the Answerer agent by resetting the formatter \n",
    "        and applying the initial prompt and previous turns.\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.questions (list): List of previous questions asked.\n",
    "                - obs.answers (list): List of corresponding answers given.\n",
    "                - obs.keyword (str): The keyword that the questioner is trying to guess.\n",
    "                - obs.category (str): The category of the keyword.\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response to extract the yes-or-no answer.\n",
    "        \n",
    "        Args:\n",
    "            response (str): The response generated by the language model.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "        \n",
    "        Returns:\n",
    "            str: 'yes' if the answer contains 'yes', otherwise 'no'.\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    Initialize and return the appropriate agent (questioner or answerer) based on the provided name.\n",
    "    \n",
    "    This function uses a global variable to ensure that only one instance of the agent is created.\n",
    "    If the agent is not already initialized, it creates a new instance of either \n",
    "    GemmaQuestionerAgent or GemmaAnswererAgent based on the provided name.\n",
    "    \n",
    "    Args:\n",
    "        name (str): The name of the agent to initialize ('questioner' or 'answerer').\n",
    "\n",
    "    Returns:\n",
    "        GemmaAgent: An instance of either GemmaQuestionerAgent or GemmaAnswererAgent.\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If the agent name is not recognized or the agent fails to initialize.\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "    Determine the type of turn and invoke the appropriate agent to generate a response.\n",
    "    \n",
    "    This function examines the turn type in the observation and uses the corresponding agent \n",
    "    (questioner or answerer) to generate a response. If the response is None or empty, it returns \"yes\".\n",
    "    \n",
    "    Args:\n",
    "        obs (dict): The observation dictionary containing game state information.\n",
    "            - obs.turnType (str): The type of the current turn ('ask', 'guess', or 'answer').\n",
    "        cfg (dict): Configuration settings for the agent (not used in the current implementation).\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the agent or \"yes\" if the response is None or empty.\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で機能するように、システムパスをこのように設定してください。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# テンソル操作とモデル設定のハンドリングに必要なモジュールをインポートします。\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルウェイトのパスを定義します。\n",
    "# スクリプトがKaggleシミュレーション環境で実行されているか確認します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # Kaggleシミュレーション環境で実行されている場合、ウェイトパスを適切に設定します。\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # ローカルまたは別の環境で実行されている場合、ローカル入力ディレクトリを使用します。\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    20の質問ゲームのプロンプトと応答をフォーマットするためのクラス。\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        オプションのシステムプロンプトと少数のサンプルを使用してGemmaFormatterを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        プロンプトの現在の状態を返します。\n",
    "        \n",
    "        Returns:\n",
    "            str: 現在のフォーマットされた状態。\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にユーザーのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): ユーザーのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にモデルのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): モデルのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        新しいユーザーターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        新しいモデルターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        現在のターンを終了します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        フォーマッタを初期状態にリセットします。\n",
    "        提供された場合はシステムプロンプトと少数のサンプルも含みます。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        フォーマッタに一連のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            turns (Iterable): 適用するターンのシーケンス。\n",
    "            start_agent (str): 'user'または'model'のどちらが最初に開始されるかを指定します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    PyTorchにおけるデフォルトのテンソル型を設定するためのコンテキストマネージャ。\n",
    "\n",
    "    このコンテキストマネージャは、一時的に指定されたdtypeにデフォルトのテンソル型を設定し、\n",
    "    コンテキストを抜けると元のデフォルトのテンソル型に戻します。\n",
    "\n",
    "    Args:\n",
    "        dtype (torch.dtype): 設定するデフォルトのテンソル型。\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 指定されたdtypeにデフォルトのtorch dtypeを設定します。\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # デフォルトのテンソル型をfloatに戻します\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Gemma言語モデルとインタラクションするエージェントの基本クラス。\n",
    "    このクラスは、モデルの初期化、プロンプトのフォーマット、応答の生成を処理します。\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        指定された設定でGemmaAgentを初期化します。\n",
    "\n",
    "        Args:\n",
    "            variant (str): 使用するモデルバリアント（例：'7b-it-quant'）。\n",
    "            device (str): モデルを実行するデバイス（例：'cuda:0'はGPU）。\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        # バリアントに基づいてモデルの設定を取得\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # デフォルトのテンソル型を設定し、モデルを初期化\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        新しい観察を扱い、セッションを開始し、プロンプトを生成し、応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            *args: 追加の引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        エージェントの新しいセッションを開始します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        提供されたプロンプトを使用して言語モデルから応答を生成します。\n",
    "\n",
    "        Args:\n",
    "            prompt (str): 言語モデルに対する入力プロンプト。\n",
    "            max_new_tokens (int): 生成する最大新トークン数。\n",
    "            **sampler_kwargs: サンプリングプロセスに対する追加のキーワード引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        モデルの応答からキーワードを抽出します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "\n",
    "        Returns:\n",
    "            str: 抽出されたキーワード、見つからない場合は空の文字列。\n",
    "        \"\"\"\n",
    "        # ダブルアスタリスク（**）で囲まれた部分文字列を見つけます。\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        観察に基づいてモデルの応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    2つのリストxとyをインターリーブし、欠損値のためにNoneで埋めます。\n",
    "    \n",
    "    この関数は、2つのリストの要素をインターリーブします。リストの長さが異なる場合は、\n",
    "    欠損値のためにNoneを使用し、最終結果からこれらのNone値を除外します。\n",
    "\n",
    "    Args:\n",
    "        x (list): インターリーブする最初のリスト。\n",
    "        y (list): インターリーブする2番目のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: None値を除外してインターリーブされたxとyの要素を持つリスト。\n",
    "    \n",
    "    例:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの質問者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて質問を生成し、言語モデルからの応答を解析して\n",
    "    質問プロセスを進めます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaQuestionerAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        質問者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 受け取った対応する回答のリスト。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいまたはいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測はダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        ゲームのターンのタイプに基づいてモデルの応答を解析します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \n",
    "        Returns:\n",
    "            str: ターンタイプに基づいて解析された質問または推測。\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: 観察内のターンタイプが不明である場合。\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの回答者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて「はい」または「いいえ」の回答を提供し、\n",
    "    言語モデルからの応答を解析して回答プロセスを続けます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaAnswererAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        回答者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 与えられた対応する回答のリスト。\n",
    "                - obs.keyword (str): 質問者が推測しているキーワード。\n",
    "                - obs.category (str): キーワードのカテゴリ。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"その質問はキーワード{obs.keyword}についてで、カテゴリは{obs.category}です。「はい」または「いいえ」で答えて、回答をダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        モデルの応答を解析して「はい」または「いいえ」の回答を抽出します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "        \n",
    "        Returns:\n",
    "            str: 'yes'（回答に「yes」が含まれている場合）、それ以外は「no」。\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者の質問に「はい」または「いいえ」で答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "# **重要:** 必要なエージェントを一つだけロードできるようにエージェントをグローバルとして定義します。\n",
    "# 両方のエージェントをロードすると、メモリ不足になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    提供された名前に基づいて、適切なエージェント（質問者または回答者）を初期化して返します。\n",
    "    \n",
    "    この関数はグローバル変数を使用して、エージェントのインスタンスが一つだけ作成されることを保証します。\n",
    "    エージェントがまだ初期化されていない場合、提供された名前に基づいてGemmaQuestionerAgentまたはGemmaAnswererAgentの新しいインスタンスを作成します。\n",
    "    \n",
    "    Args:\n",
    "        name (str): 初期化するエージェントの名前（'questioner'または'answerer'）。\n",
    "\n",
    "    Returns:\n",
    "        GemmaAgent: GemmaQuestionerAgentまたはGemmaAnswererAgentのインスタンス。\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: エージェント名が認識されない場合、またはエージェントの初期化に失敗した場合。\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "   ターンのタイプを判定し、適切なエージェントを呼び出して応答を生成します。\n",
    "    \n",
    "    この関数は観察内のターンタイプを調べ、その対応するエージェント（質問者または回答者）を使用して応答を生成します。\n",
    "    応答がNoneまたは空である場合、\"yes\"を返します。\n",
    "    \n",
    "    Args:\n",
    "        obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            - obs.turnType (str): 現在のターンのタイプ（'ask'、'guess'、または'answer'）。\n",
    "        cfg (dict): エージェントの設定（現在の実装では使用されていません）。\n",
    "\n",
    "    Returns:\n",
    "        str: エージェントによって生成された応答、または応答がNoneまたは空である場合は\"yes\"。\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:08.002368Z",
     "iopub.status.busy": "2024-06-06T05:07:08.001976Z",
     "iopub.status.idle": "2024-06-06T05:07:08.023358Z",
     "shell.execute_reply": "2024-06-06T05:07:08.02231Z",
     "shell.execute_reply.started": "2024-06-06T05:07:08.002336Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# セットアップ\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **重要:** コードがノートブックとシミュレーション環境の両方で機能するように、システムパスをこのように設定してください。\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# テンソル操作とモデル設定のハンドリングに必要なモジュールをインポートします。\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# モデルウェイトのパスを定義します。\n",
    "# スクリプトがKaggleシミュレーション環境で実行されているか確認します。\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # Kaggleシミュレーション環境で実行されている場合、ウェイトパスを適切に設定します。\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # ローカルまたは別の環境で実行されている場合、ローカル入力ディレクトリを使用します。\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# プロンプトフォーマット\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    20の質問ゲームのプロンプトと応答をフォーマットするためのクラス。\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        オプションのシステムプロンプトと少数のサンプルを使用してGemmaFormatterを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        プロンプトの現在の状態を返します。\n",
    "        \n",
    "        Returns:\n",
    "            str: 現在のフォーマットされた状態。\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にユーザーのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): ユーザーのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        現在の状態にモデルのプロンプトを追加します。\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): モデルのプロンプト。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        新しいユーザーターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        新しいモデルターンを開始します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        現在のターンを終了します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        フォーマッタを初期状態にリセットします。\n",
    "        提供された場合はシステムプロンプトと少数のサンプルも含みます。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        フォーマッタに一連のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            turns (Iterable): 適用するターンのシーケンス。\n",
    "            start_agent (str): 'user'または'model'のどちらが最初に開始されるかを指定します。\n",
    "        \n",
    "        Returns:\n",
    "            self: GemmaFormatterインスタンス。\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# エージェントの定義\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    PyTorchにおけるデフォルトのテンソル型を設定するためのコンテキストマネージャ。\n",
    "\n",
    "    このコンテキストマネージャは、一時的に指定されたdtypeにデフォルトのテンソル型を設定し、\n",
    "    コンテキストを抜けると元のデフォルトのテンソル型に戻します。\n",
    "\n",
    "    Args:\n",
    "        dtype (torch.dtype): 設定するデフォルトのテンソル型。\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    # 指定されたdtypeにデフォルトのtorch dtypeを設定します。\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # デフォルトのテンソル型をfloatに戻します\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Gemma言語モデルとインタラクションするエージェントの基本クラス。\n",
    "    このクラスは、モデルの初期化、プロンプトのフォーマット、応答の生成を処理します。\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        指定された設定でGemmaAgentを初期化します。\n",
    "\n",
    "        Args:\n",
    "            variant (str): 使用するモデルバリアント（例：'7b-it-quant'）。\n",
    "            device (str): モデルを実行するデバイス（例：'cuda:0'はGPU）。\n",
    "            system_prompt (str): システムの初期プロンプト。\n",
    "            few_shot_examples (Iterable): プロンプトを初期化するための例。\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"モデルを初期化しています\")\n",
    "        # バリアントに基づいてモデルの設定を取得\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # デフォルトのテンソル型を設定し、モデルを初期化\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        新しい観察を扱い、セッションを開始し、プロンプトを生成し、応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            *args: 追加の引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        エージェントの新しいセッションを開始します。\n",
    "\n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        提供されたプロンプトを使用して言語モデルから応答を生成します。\n",
    "\n",
    "        Args:\n",
    "            prompt (str): 言語モデルに対する入力プロンプト。\n",
    "            max_new_tokens (int): 生成する最大新トークン数。\n",
    "            **sampler_kwargs: サンプリングプロセスに対する追加のキーワード引数。\n",
    "\n",
    "        Returns:\n",
    "            str: モデルから生成された応答。\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        モデルの応答からキーワードを抽出します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "\n",
    "        Returns:\n",
    "            str: 抽出されたキーワード、見つからない場合は空の文字列。\n",
    "        \"\"\"\n",
    "        # ダブルアスタリスク（**）で囲まれた部分文字列を見つけます。\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        観察に基づいてモデルの応答を解析します。\n",
    "\n",
    "        Args:\n",
    "            response (str): モデルの応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: このメソッドはサブクラスによって実装される必要があります。\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    2つのリストxとyをインターリーブし、欠損値のためにNoneで埋めます。\n",
    "    \n",
    "    この関数は、2つのリストの要素をインターリーブします。リストの長さが異なる場合は、\n",
    "    欠損値のためにNoneを使用し、最終結果からこれらのNone値を除外します。\n",
    "\n",
    "    Args:\n",
    "        x (list): インターリーブする最初のリスト。\n",
    "        y (list): インターリーブする2番目のリスト。\n",
    "\n",
    "    Returns:\n",
    "        list: None値を除外してインターリーブされたxとyの要素を持つリスト。\n",
    "    \n",
    "    例:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの質問者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて質問を生成し、言語モデルからの応答を解析して\n",
    "    質問プロセスを進めます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaQuestionerAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        質問者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 受け取った対応する回答のリスト。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"20の質問をしましょう。あなたは質問者の役割を果たします。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"はいまたはいいえで答えられる質問をしてください。\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"今、キーワードを推測してください。推測はダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        ゲームのターンのタイプに基づいてモデルの応答を解析します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.turnType (str): 現在のターンのタイプ（'ask'または'guess'）。\n",
    "        \n",
    "        Returns:\n",
    "            str: ターンタイプに基づいて解析された質問または推測。\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: 観察内のターンタイプが不明である場合。\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"それは人ですか？\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"不明なターンタイプ:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    20の質問ゲームの回答者の役割を果たすエージェント。\n",
    "    \n",
    "    このエージェントは、ゲームのステートに基づいて「はい」または「いいえ」の回答を提供し、\n",
    "    言語モデルからの応答を解析して回答プロセスを続けます。\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        提供された引数でGemmaAnswererAgentを初期化します。\n",
    "        \n",
    "        Args:\n",
    "            *args: 基本クラスの初期化子に渡す位置引数。\n",
    "            **kwargs: 基本クラスの初期化子に渡すキーワード引数。\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        回答者エージェントの新しいセッションを開始し、フォーマッタをリセットし、\n",
    "        初期プロンプトと以前のターンを適用します。\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "                - obs.questions (list): 以前にした質問のリスト。\n",
    "                - obs.answers (list): 与えられた対応する回答のリスト。\n",
    "                - obs.keyword (str): 質問者が推測しているキーワード。\n",
    "                - obs.category (str): キーワードのカテゴリ。\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"20の質問をしましょう。あなたは回答者の役割を果たします。キーワードは{obs.keyword}で、カテゴリは{obs.category}です。\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"その質問はキーワード{obs.keyword}についてで、カテゴリは{obs.category}です。「はい」または「いいえ」で答えて、回答をダブルアスタリスクで囲んでください。\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        モデルの応答を解析して「はい」または「いいえ」の回答を抽出します。\n",
    "        \n",
    "        Args:\n",
    "            response (str): 言語モデルによって生成された応答。\n",
    "            obs (dict): ゲームステート情報を含む観察辞書。\n",
    "        \n",
    "        Returns:\n",
    "            str: 'yes'（回答に「yes」が含まれている場合）、それ以外は「no」。\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# エージェントの作成\n",
    "system_prompt = \"あなたは20の質問ゲームをプレイするために設計されたAIアシスタントです。このゲームでは、回答者がキーワードを考え、質問者の質問に「はい」または「いいえ」で答えます。キーワードは特定の人、場所、または物です。\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"20の質問をしましょう。あなたは質問者の役割を果たします。最初の質問をしてください。\",\n",
    "    \"それは人ですか？\", \"**いいえ**\",\n",
    "    \"それは場所ですか？\", \"**はい**\",\n",
    "    \"それは国ですか？\", \"**はい** ではキーワードを推測してください。\",\n",
    "    \"**フランス**\", \"正解です！\",\n",
    "]\n",
    "\n",
    "# **重要:** 必要なエージェントを一つだけロードできるようにエージェントをグローバルとして定義します。\n",
    "# 両方のエージェントをロードすると、メモリ不足になる可能性があります。\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    提供された名前に基づいて、適切なエージェント（質問者または回答者）を初期化して返します。\n",
    "    \n",
    "    この関数はグローバル変数を使用して、エージェントのインスタンスが一つだけ作成されることを保証します。\n",
    "    エージェントがまだ初期化されていない場合、提供された名前に基づいてGemmaQuestionerAgentまたはGemmaAnswererAgentの新しいインスタンスを作成します。\n",
    "    \n",
    "    Args:\n",
    "        name (str): 初期化するエージェントの名前（'questioner'または'answerer'）。\n",
    "\n",
    "    Returns:\n",
    "        GemmaAgent: GemmaQuestionerAgentまたはGemmaAnswererAgentのインスタンス。\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: エージェント名が認識されない場合、またはエージェントの初期化に失敗した場合。\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"エージェントが初期化されていません。\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "   ターンのタイプを判定し、適切なエージェントを呼び出して応答を生成します。\n",
    "    \n",
    "    この関数は観察内のターンタイプを調べ、その対応するエージェント（質問者または回答者）を使用して応答を生成します。\n",
    "    応答がNoneまたは空である場合、\"yes\"を返します。\n",
    "    \n",
    "    Args:\n",
    "        obs (dict): ゲームステート情報を含む観察辞書。\n",
    "            - obs.turnType (str): 現在のターンのタイプ（'ask'、'guess'、または'answer'）。\n",
    "        cfg (dict): エージェントの設定（現在の実装では使用されていません）。\n",
    "\n",
    "    Returns:\n",
    "        str: エージェントによって生成された応答、または応答がNoneまたは空である場合は\"yes\"。\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbc1e7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!apt install pigz pv > /dev/null\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:10.340786Z",
     "iopub.status.busy": "2024-06-06T05:07:10.340376Z",
     "iopub.status.idle": "2024-06-06T05:07:18.728324Z",
     "shell.execute_reply": "2024-06-06T05:07:18.726981Z",
     "shell.execute_reply.started": "2024-06-06T05:07:10.340755Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f59b3fa",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "The above cell installs `pigz` for fast compression and `pv` for progress monitoring:\n",
    "\n",
    "- **apt install pigz pv**: Installs `pigz` (a parallel implementation of gzip) and `pv` (Pipe Viewer, which allows monitoring the progress of data through a pipeline).\n",
    "- **> /dev/null**: Redirects the command's output to `/dev/null`, suppressing the output to keep the notebook clean.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "上記のセルは、高速圧縮のための`pigz`と、進捗監視のための`pv`をインストールします：\n",
    "\n",
    "- **apt install pigz pv**: `pigz`（gzipの並列実装）と`pv`（パイプビューア、データのパイプラインを通る進捗を監視可能）をインストールします。\n",
    "- **> /dev/null**: コマンドの出力を`/dev/null`にリダイレクトし、出力を抑制してノートブックをクリーンに保ちます。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7fd5c1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:08:42.811284Z",
     "iopub.status.busy": "2024-06-06T05:08:42.810305Z",
     "iopub.status.idle": "2024-06-06T05:11:31.295042Z",
     "shell.execute_reply": "2024-06-06T05:11:31.29284Z",
     "shell.execute_reply.started": "2024-06-06T05:08:42.811242Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8116e3c2",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "The above cell packages the submission directory and necessary model files into a compressed tarball (`submission.tar.gz`). This tarball includes:\n",
    "   - All files in `/kaggle/working/submission`.\n",
    "   - The directory `gemma/pytorch/7b-it-quant/2` from `/kaggle/input`.\n",
    "   \n",
    "Detailed breakdown:\n",
    "- **--use-compress-program='pigz --fast --recursive | pv'**: Specifies to use `pigz` for compression, which will use multiple CPU cores to speed up the process. The `--fast` option ensures quick compression, and `--recursive` processes directories recursively. The output is piped through `pv` to monitor the progress.\n",
    "- **-cf submission.tar.gz**: Creates a file named `submission.tar.gz`.\n",
    "- **-C /kaggle/working/submission**: Changes to the directory `/kaggle/working/submission` before adding files to the archive.\n",
    "- **.**: Adds all files from the current directory (which is `/kaggle/working/submission` due to the `-C` option) to the archive.\n",
    "- **-C /kaggle/input/**: Changes to the `/kaggle/input/` directory before adding more files.\n",
    "- **gemma/pytorch/7b-it-quant/2**: Adds the `gemma/pytorch/7b-it-quant/2` directory and its contents from `/kaggle/input/` to the archive.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "上記のセルは、提出ディレクトリと必要なモデルファイルを圧縮tarボール（`submission.tar.gz`）内にパッケージ化します。このtarボールには以下が含まれます：\n",
    "   - `/kaggle/working/submission`内のすべてのファイル。\n",
    "   - `/kaggle/input`からの`gemma/pytorch/7b-it-quant/2`ディレクトリ。\n",
    "   \n",
    "詳細な内訳：\n",
    "- **--use-compress-program='pigz --fast --recursive | pv'**: 圧縮に`pigz`を使用するように指定し、複数のCPUコアを利用してプロセスを高速化します。`--fast`オプションは迅速な圧縮を保証し、`--recursive`はディレクトリを再帰的に処理します。出力は`pv`を通してパイプされ、進捗を監視します。\n",
    "- **-cf submission.tar.gz**: `submission.tar.gz`という名前のファイルを作成します。\n",
    "- **-C /kaggle/working/submission**: アーカイブにファイルを追加する前に、ディレクトリを`/kaggle/working/submission`に変更します。\n",
    "- **.**: 現在のディレクトリ（オプションの`-C`によって`/kaggle/working/submission`になっています）からすべてのファイルをアーカイブに追加します。\n",
    "- **-C /kaggle/input/**: より多くのファイルを追加する前に、`/kaggle/input/`ディレクトリに変更します。\n",
    "- **gemma/pytorch/7b-it-quant/2**: `/kaggle/input/`から`gemma/pytorch/7b-it-quant/2`ディレクトリとその内容をアーカイブに追加します。\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11220,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
