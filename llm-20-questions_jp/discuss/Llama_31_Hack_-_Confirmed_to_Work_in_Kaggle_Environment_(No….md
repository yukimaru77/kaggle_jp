# Llama 3.1 ハック - Kaggle 環境（ノートブックおよびコンペティション）で動作確認済み
**Matthew S Farmer** *2024年8月2日金曜日 04:34:42 GMT+0900（日本標準時）* (21票)

# Llama 3.1
最新のLlama 3.1のリリースを見て、「これがLLM 20 Questionsコンペティションで使えそう」と思ったことでしょう。ノートブックを立ち上げてモデルをインポートし、読み込もうとすると、RoPEスケーリングに関するエラーが発生しました…。ディスカッションボードにアクセスしても助けを見つけられず、オンラインでは「transformersを更新しろ」としか書いていません。それを実行するとノートブックは動くようになりますが、今度は厄介なバリデーションエラーに直面します！どうすればいいのでしょう？いじくり回すことが好きな人なら、どこかにワークアラウンドがあることを知っています…

## 私たちにはワークアラウンドがあります！
私は、transformersを更新することなく、ノートブックおよびゲーム環境で動作することを確認しました。 
```
import json
with open("YOUR_LOCAL_MODEL_PATH/config.json", "r") as file:
    config = json.load(file)
config["rope_scaling"] = {"factor":8.0,"type":"dynamic"}
with open("YOUR_LOCAL_MODEL_PATH/config.json", "w") as file:
    json.dump(config, file)
```
## 実装手順
1. 試していたtransformersの更新をすべて取り消します。
2. 使用したいLlama 3.1モデルを作業フォルダにインポートします。
3. そのフォルダ内のconfig.jsonのパスを確認し、上記のコード内の全大文字のパスに置き換えます。
4. 提出用の.pyスクリプトおよびtarball提出の前に、コードブロック内にこのコードを追加します。
5. 通常通りモデルとスクリプトを読み込みます。
6. 必要に応じてノートブック内でテキスト生成を確認します。
7. 更新された設定ファイルをモデルと一緒に圧縮して提出用に準備します。
8. バリデーション後の緑のチェックマークを楽しんでください。

最終評価が迫る中、皆さんのコンペティションのレベルが上がることを願っています。最良のエージェントが勝ちますように！

## 要約
config.jsonを現在のtransformersバージョンが期待する辞書（2つのフィールド）に変更します。 
では、ハッピーカグリングを！

RoPEスケーリングについて質問がある場合は、[ドキュメントをチェックしてみてください！ ](https://huggingface.co/docs/text-generation-inference/en/basic_tutorials/preparing_model)

---

# 他のユーザーからのコメント
> ## VolodymyrBilyachat
> 
> 伝説ですね！このシンプルなハックに感謝します :)
