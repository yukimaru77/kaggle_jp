# LLMの概要とLlama 3の設定
**Raki** *2024年5月18日 21:29:56 (日本標準時)* (6票)
## Gemmaとその他のLLMの評価
### Gemmaの問題点
Gemma 7bの量子化版は、スタートノートブックで使用されているものですが、指示に従うのが苦手で、サイズに対して最先端のパフォーマンスを達成できていません。
### 推奨するLLM評価方法: LMSYS Arenaリーダーボード
強力な汎用LLMを特定するために、[LMSYS Arenaリーダーボード](https://chat.lmsys.org/)をお勧めします。このEloレーティングシステムは、ユーザーが質問を投げかけ、異なる2つのモデルからの回答を比較することで、どちらが好ましいかを判断します（ブラインド方式）。この評価方法は、さまざまなトピックにわたってユーザーの要求を一貫して満たすことに依存するため、「ゲーム化」するのが難しいです。他の指標は、ベンチマークがトレーニングセットに漏れる可能性があり、特定のパフォーマンスを最適化しやすい狭いタスクに焦点を当てることが多いです。
### 現在のトップモデル
- GPT-4: 1287 Elo
- Gemini 1.5 Pro（Google）: 1248 Elo
- 最良のAnthropicモデル: 1246 Elo
- LLaMA 3 70B Instruct（Meta）: 1203 Elo（最良のオープンソースモデルですが、大きすぎます）
### Gemmaのパフォーマンス（スタートと同様）
- Gemma 7B-IT: 1043 Elo
- 量子化版: わずかに劣ります（量子化はFP32などの形式からINT8に重みを減少させ、VRAMと計算要件を大幅に削減しますが、品質に対しては妥協が生じます）
### 代替案: LLaMA 3 8B Instruct
- LLaMA 3 8B Instruct: 1155 Elo（全体的にかなり強力なモデルです）
T4での実行に必要な量子化をまだ行う必要がありますが、T4には16 GiBのVRAMがあります。
### 推論を実行する最良の方法: llama.cpp
私が知る限り、非独自の量子化されたLLMで推論を行う最良の方法は、llama.cppです。このツールは、量子化やKVキャッシングなど、推論を高速化するためのさまざまな技術を採用しています。
### LLaMA 3 8B Instruct用のリソース
以前にいくつかの量子化されたLLaMA 3 8B Instructモデルのデータセットを設定しました：
- [データセット](https://www.kaggle.com/datasets/raki21/meta-llama-3-8b-gguf)
- [Notebook](https://www.kaggle.com/code/raki21/llama-3-gguf-with-llama-cpp) 使用法を示したものです。チャット全体で継続性を持つQ20の例を追加するプロセスにあります。
8ビット量子化バリアントの使用をお勧めします。現在のエージェント設定に統合する時間はありませんが、この書き込みが何らかの形でスタートする手助けになることを願っています！
AIノート: テキストは自分で書きましたが、より良いマークダウン構造に整形し、誤字を修正するためにGPT4oを通しました。
---
# 他のユーザーからのコメント
> ## Melinda
> 
> こんにちは、このノートブックを投稿してくれてありがとう！ 私は自分の提出物に対してllama-cpp-pythonを動作させようとしていますが、あなたのノートブックをコピーすると、pip installコマンドを実行できます。しかし、「-t /kaggle/working/submission/lib」オプションを指定してフォルダーにpip installを試みると、さまざまな依存関係解決エラーが発生します。 （「ERROR: pipの依存関係解決機能は、現在すべてのインストールパッケージを考慮に入れていません。この動作が原因で、次の依存関係の衝突が発生しています。（etc - 大量のリスト）」）
> 
> あなたはllama-cpp-pythonとllama-3-8b-instructを提出用にパッケージ化する方法について何かアドバイスがありますか？ 
> 
> ---
