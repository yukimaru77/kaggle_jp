# モデル選択
**Matthew S Farmer** *2024年6月29日土曜日 00:51:01 GMT+0900 (日本標準時)* (6票)

特定の20問ゲームのデータセットでモデルを微調整する以外に、このコンペティションに最適なモデルを選択する方法について考えてきました。そのため、[HF Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) を確認し、さまざまなベンチマークを調べてみました。

パフォーマンスの鍵は、演繹的推論と、モデルが明示的な指示に従う能力（解析を支援するため）にあるはずです。そのため、次の2つのベンチマークを優先します。

MUSRとIFEval

MuSR（マルチステップソフト推論）（https://arxiv.org/abs/2310.16049） - MuSRは、アルゴリズム的に生成された複雑な問題からなる新しいデータセットで、それぞれ約1,000語の長さです。問題には、殺人ミステリー、オブジェクト配置問題、チーム割り当ての最適化などが含まれます。これらの問題を解決するには、モデルが長距離コンテキスト解析と推論を統合する必要があります。このデータセットでは、ランダムなパフォーマンスを上回るモデルはほとんどありません。

IFEval（https://arxiv.org/abs/2311.07911） - IFEvalは、モデルが「キーワードxを含める」や「フォーマットyを使用する」などの明示的な指示に従う能力をテストするために設計されたデータセットです。焦点は、生成されたコンテンツではなく、モデルのフォーマット指示への準拠であり、厳格で厳密な指標の使用を可能にします。

- Phi 3、Qwen 2、Openchat 3.5、Yi、Hermes 2… 上記のベンチマークを考慮すると、すべてがトップに位置しています。
- 対照的に：Gemma 2 7b it（スターターノートブックモデル）のMUSRは12.53であるのに対し、IntelのNeural ChatのMUSRは23.02です…

いくつかの考えられる点です。楽しいカグリングを！
---
# 他のユーザーからのコメント
> ## Azim Sonawalla
> 
> 
> パフォーマンスの鍵は、演繹的推論と、モデルが明示的な指示に従う能力（解析を支援するため）にあるはずです。
> 
> この仮定については、よくわかりません。ボットは、質問者/推測者として最大20個の同時条件を満たすキーワードを見つけることができる程度に推論する必要がありますが、最後の候補を絞り込むために、後半のゲームで質問を思いつくために、実際の事実の知識が必要です。
> 
> 私は、より高いトークン数でトレーニングされたモデル（例：llama3）がこの理由でうまくいくのではないかと考えてきましたが、そのように実験する時間はありませんでした。
> 
> 
> 
> > ## Matthew S Farmerトピック作成者
> > 
> > 面白い反論ですね！私の仮定に挑戦してくれてありがとうございます。あなたの考えに従うと、多言語モデルは、英語のみのモデルよりも多くの語彙トレーニングが必要なため、このコンペティションに最適かもしれませんか？
> > 
> > 
> > 
---
