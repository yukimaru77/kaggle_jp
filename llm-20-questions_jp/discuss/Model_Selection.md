# モデル選択
**マシュー・S・ファーマー** *2024年6月29日 00:51:01 (日本標準時)* (6票)
特定の20 Qデータセットに対してモデルをファインチューニングすることを除けば、このコンペティションに最適なモデルを選定する方法について考えています。そのため、[HF Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)をチェックし、さまざまなベンチマークに目を通してきました。  
パフォーマンスの鍵は、演繹的推論と明示的な指示に従うモデルの能力にあると思います（解析を助けるために）。これにより、以下の2つのベンチマークを優先することにしました：  
- MUSRおよびIFEval
  - MuSR (Multistep Soft Reasoning) (https://arxiv.org/abs/2310.16049) – MuSRは、アルゴリズムで生成された複雑な問題から成る新しいデータセットで、各問題は約1,000ワードの長さです。問題には、殺人ミステリー、オブジェクト配置の質問、およびチーム割り当ての最適化が含まれます。これらの問題を解決するには、モデルが推論と長期的な文脈解析を統合する必要があります。このデータセットでは、ほとんどのモデルがランダムなパフォーマンスを上回ることはありません。
  - IFEval (https://arxiv.org/abs/2311.07911) – IFEvalは、明示的な指示に従うモデルの能力をテストするために設計されたデータセットで、「キーワードxを含める」や「フォーマットyを使用する」といった指示が含まれます。焦点は、生成されたコンテンツよりもフォーマット指示へのモデルの遵守にあり、厳格な指標の使用が可能です。
- Phi 3、Qwen 2、Openchat 3.5、Yi、Hermes 2... これらは上記のベンチマークに基づいてボードのトップに位置しています。  
- 対照的に、Gemma 2 7b（スターターノートブックモデル）はMUSRが12.53であるのに対し、IntelのNeural ChatはMUSRが23.02です…。  
いくつかのことを考慮してみてください。楽しいKaggleライフを！

---

# 他のユーザーからのコメント
> ## アジム・ソナワラ
>
> パフォーマンスの鍵は演繹的推論と、解析を助けるための明示的な指示に従うモデルの能力にあると思います。
>
> この仮定には疑問があります。ボットは、質問者/推測者として同時に20の条件を満たすキーワードを見つけるために推論が必要ですが、実際には遅い段階で最後の数候補を絞り込むために事実の知識も必要です。
>
> より高いトークンカウントで訓練されたモデル（例：Llama3）が、この理由でうまく機能するのではないかと考えていますが、その方向での実験はまだ行っていません。
>
> > ## マシュー・S・ファーマー トピック作成者
> > 
> > 興味深い反論ですね！私の仮定に挑んでくれてありがとう。あなたの考えに従うと、多言語モデルがこのコンペティションには最適かもしれませんね。英語モデルよりもはるかに大きな語彙の訓練が必要だからでしょうか？ 
> > 
> > 
