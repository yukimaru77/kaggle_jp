# 質問と回答に適したLLMモデルを共有しましょう！
**c-number** *2024年7月8日 月曜日 10:46:38 JST* (4 votes)
どんなモデルを使っていますか？
私はgoogle/gemma-7b-itとmeta-llama/Meta-Llama-3-8B-Instructの両方を使用しています。どちらも8ビット量子化されています。
---
# 他のユーザーからのコメント
> ## Chris Deotte
> 
> 基本的な5つのモデルは、Llama3、Mistral、Gemma2、Phi3、Qwen2です。そして、SmaugとBagelという2つの一般的なアップグレードがあります。すべて、このコンペティションでうまく機能する約70億パラメータのサイズを持っています。
> 
> 
> 
---
> ## Iqbal Singh
> 
> Phi3 Miniです。ファインチューニングはしていません！
> 
> 
> 
---
> ## TuMinhDang
> 
> gemma-9b-itをファインチューニングして使用しています。
> 
> 
> 
---
> ## Kasahara
> 
> llama3-8b-it、gemma2-9b-it、gemma-7b-it、mistral-7bを試してみました。私の実験では、llama3-8b-itが最も良い結果を出しました。
> 
> 
> 
> > ## c-numberトピック作成者
> > 
> > 私も同じ印象です。
> > 
> > 
> > 
---
> ## OminousDude
> 
> 私もllama meta-llama/Meta-Llama-3-8B-Instructを使用しています。IF-Evalスコアがとても高いからです。しかし、4ビット量子化を選択しました。これは、より高速に動作し、エージェントがタイムアウトになることを心配することなく、プロンプトと戦略をより長くすることができます。また、秘密にするつもりがないなら、どのように両方のモデルを使用しているのですか？キーワードのカテゴリに基づいて選択しているのですか？
> 
> 
> 
> > ## c-numberトピック作成者
> > 
> > 現時点では、2つのモデルのうち1つのみを提出していますが、両方ともテストしています。
> > 
> > 
> > 
> > > ## OminousDude
> > > 
> > > なるほど！Gemma 2は非常に有望で、場所に関する非常に良い戦略を持っています。実際のベンチマークと動作するAWQバージョンがリリースされたら、後で使用するかもしれません。
> > > 
> > > 
> > > 
---
> ## Matthew S Farmer
> 
> Phi3 miniを使用しています。
> 
> 
> 
---
