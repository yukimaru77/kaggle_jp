# 質問を作成し、回答するためのLLMモデルを共有しましょう！
**c-number** *2024年7月8日 10:46:38 (日本標準時)* (4票)
どのモデルを使用していますか？
私はgoogle/gemma-7b-itとmeta-llama/Meta-Llama-3-8B-Instructの両方を8ビット量子化で使用しています。
---
 # 他のユーザーからのコメント
> ## Chris Deotte
> 
> 基本の5つのモデルは、Llama3、Mistral、Gemma2、Phi3、Qwen2です。そして、人気のある2つのアップグレードモデルはSmaugとBagelです。これらはすべて、このコンペティションでうまく機能する7Bパラメータサイズのバージョンを揃えています。
> 
> ---
> 
> ## Iqbal Singh
> 
> Phi3 Miniを使用しています。ファインチューニングは行っていません！
> 
> ---
> 
> ## TuMinhDang
> 
> 私はgemma-9b-itをファインチューニングして使っています。
> 
> ---
> ## Kasahara
> 
> llama3-8b-it、gemma2-9b-it、gemma-7b-it、mistral-7bを試しましたが、実験の結果、llama3-8b-itが最も良い結果を出しました。
> 
> > ## c-number (トピック作成者)
> > 
> > 私も同じ印象です。
> > 
> > ---
> 
> ## OminousDude
> 
> 私もllama meta-llama/Meta-Llama-3-8B-Instructを使用しています。非常に高いIF-Evalスコアを持っていますからね。しかし、4ビットの量子化を選んだのは、処理が速くなり、エージェントのタイムアウトを心配せずにプロンプトや戦略を長くできるからです。また、どちらのモデルをどのように使い分けているのか秘密にしないのであれば、キーワードのカテゴリーに基づいて選んでいるのですか？
> 
> > ## c-number (トピック作成者)
> > 
> > 今のところ、2つのモデルのうち一つだけを提出していますが、両方をテストしています。
> > 
> > > ## OminousDude
> > > 
> > > なるほど！Gemma 2はかなり期待が持てそうで、位置情報に対する非常に優れた戦略を持っています。実際のベンチマークと動作するAWQバージョンが出てきたら使うかもしれませんね。
> > > 
> > > ---
> 
> ## Matthew S Farmer
> 
> 私はPhi3 miniを使っています。
> 
> ---
