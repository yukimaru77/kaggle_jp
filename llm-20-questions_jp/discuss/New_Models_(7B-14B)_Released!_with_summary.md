# 要約 
このディスカッションは、Kaggleの「LLM 20 Questions」コンペティションで、新しくリリースされた7B〜14Bのパラメータを持つ言語モデルについて議論しています。

参加者は、Gemma2-9B-IT、Mistral-Nemo-Instruct-2407、Minitron-8B-base、Apple-DCLM-7B、Llama-3.1-8B-Instruct、Qwen2-7B-Instruct、Phi-3-Mini-4k-Instruct、Phi-3-Medium-4k-Instructといったモデルのパフォーマンスについて意見交換しています。

**主なポイント:**

* **Gemma2-9B-IT:** マークダウンで回答するのが得意で、一般的な回答をいくつか提供します。指示に従うのが得意ですが、物事のカテゴリの語彙を少し増やせれば、このコンペティションの有力候補になりそうです。
* **Mistralのバリアント (Nemo int と Minitron):** 制約と指示に従うのが難しいです。
* **Llama 3.1:** Kaggle環境でROPEエラーが発生します。
* **Qwen2 7b:** 指示に従うのが得意ですが、キーワードの回答を具体的にできません。
* **Phi3 mini:** 3つの役割すべてにおいて全体的に優れていますが、物事のカテゴリの語彙が限られています。
* **Phi3 medium:** Phi3 miniよりも性能が劣る可能性があります。このモデルは、質問者と推測者として、哲学的な話をするのを止めることができませんでした。
* **Llama 3のコミュニティによるファインチューニング:** 参加者の中には、Llama 3のファインチューニングされたバリアントで最高の結果を得ている人がいます。
* **アルファベットによる二分法:** パブリックリーダーボードで非常に高いスコアを記録しているため、最良の戦略である可能性があります。
* **Llama 3.1のROPEエラー:** Matthew S Farmerは、ROPEエラーの解決策を開発しました。
* **モデルのロードエラー:** Llama 3.1は、ロード時にエラーが発生することがあります。

**結論:**

参加者は、新しいモデルのパフォーマンスについてさまざまな意見を持っています。Gemma2-9B-IT、Llama 3.1、Phi3 miniは、有望なモデルとして挙げられていますが、それぞれに課題があります。参加者は、これらのモデルをさらに調査し、コンペティションに最適なモデルを見つけるために努力しています。


---
# 新しいモデル (7B-14B) がリリースされました！
**Chris Deotte** *2024年7月29日(月) 06:07:11 JST* (17 votes)

ここ1、2ヶ月で多くの新しいモデルがリリースされました。これらの新しいモデルを試した人はいますか？パフォーマンスはどうですか？

- Gemma2-9B-IT [こちら](https://huggingface.co/google/gemma-2-9b-it)
- (Nvidia) Mistral-Nemo-Instruct-2407 (12B) [こちら](https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407)
- (Nvidia) Minitron-8B-base [こちら](https://huggingface.co/nvidia/Minitron-8B-Base)
- Apple-DCLM-7B [こちら](https://huggingface.co/apple/DCLM-7B)
- Llama-3.1-8B-Instruct [こちら](https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct)
- Qwen2-7B-Instruct [こちら](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
- Phi-3-Mini-4k-Instruct (4B) [こちら](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)
- Phi-3-Medium-4k-Instruct (14B) [こちら](https://huggingface.co/microsoft/Phi-3-medium-4k-instruct)
---
 # 他のユーザーからのコメント
> ## Matthew S Farmer
> 
> Gemma 2 - マークダウンで回答するのが好きで、一般的な回答をいくつか提供します。指示に従うのが得意で、物事のカテゴリの語彙を少し増やせれば、このコンペティションの有力候補になりそうです。
> 
> Mistral のバリアント (Nemo int と Minitron) - 制約と指示に従うのが難しいです。
> 
> Llama 3.1 - Kaggle 環境で ROPE エラーが発生します。
> 
> Qwen2 7b - 指示に従うのが得意ですが、キーワードの回答を具体的にできません。
> 
> Phi3 mini - 3 つの役割すべてにおいて全体的に優れていますが、物事のカテゴリの語彙が限られています。
> 
> Phi3 medium - 興味深いことに、Phi3 mini よりも性能が劣りますか？このモデルは、質問者と推測者として、哲学的な話をするのを止めることができませんでした。AWQ として実装しましたが、量子化が指示のトレーニングに影響を与えているのでしょうか？
> 
> 私は、LLaMa 3 のコミュニティによるファインチューニングに戻り続けています…そこで最高の結果を得ています。
> 
> MaziyarPanahi/Llama-3-8B-Instruct-v0.10
> 
> mlabonne/Daredevil-8B
> 
> openchat/openchat-3.6-8b-20240522
> 
> 
> 
> > ## Chris Deotteトピック作成者
> > 
> > 包括的な要約をありがとうございます。素晴らしい実験ですね。
> > 
> > 
> > 
> > > ## OminousDude
> > > 
> > > 上記のほとんどのモデルを試しましたが、どのモデルを使用すべきか、その理由をより正確に説明できます。
> > > 
> > > Gemma 2: Huggingface をアップグレードしないとエラーが発生します (Kaggle 環境ではまだ古いバージョンが使用されているため、"Gemma2ForCasualLM" がサポートされていません)。さらに、これは非常に優れたモデルであり、パラメータ数の点で LLM リーダーボードで現在最高のスコアを記録しています。ただし、このモデルは最近リリースされたばかりなので、ファインチューニングが不十分です。つまり、ほとんどのモデル (たとえば Llama 3) には、それぞれが何らかの点で役立ち、他の点では劣る、多くのファインチューニングされたバリアント (Smaug など) があります。私にとって、完璧なモデルは Gemma 2 ではありません。なぜなら、Gemma 2 にはまだこれらのバリアントがないからです。Gemma 2 は LLM 20Q 用ではないため、同様のタスクでファインチューニングされたモデルが Gemma 2 を上回る可能性があると思います。
> > > 
> > > Mistral + バリアント: Matthew が上記で述べたように、これらのモデルは指示に従うのが難しく、洗練されたプロンプトを使用している人は運が悪いです。ただし、Nemo は他のモデルとは異なる可能性があります。なぜなら、Nemo は現在最高のトークナイザー (小型モデルの中で) である Tekken を搭載しているからです。[こちら](https://mistral.ai/news/mistral-nemo/) で説明されています。
> > > 
> > > Llama 3.1: 非常に有望ですが、ロード時にエラーが発生します。ロードエラーが修正されれば、最終的には Llama 3.1 だけが上位にランクインする可能性があります。ただし、それは時間の問題であり、誰かがこのモデルを動作させることができれば、このコンペティションは Llama 3.1 によって支配される可能性があります。
> > > 
> > > Qwen2: Matthew には同意できません。統計も私を支持しています。私のテストと [LLM リーダーボード](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard) から、このモデルは堅牢な指示に従うのが得意だとは思いません。IFEval スコア (これは指示に従う能力を示しています) は 31.49 です (指示バリアントのスコアは 56.79 です)。ただし、Llama 3 と比較すると、Llama 3 は 74.08 を取得するため、かなり劣っています (これは指示スコアです)。さらに、Llama 3.1 は 77.40 を取得しており、20 ポイント以上もリードしています。ただし、Qwen は優れた回答者であり、良い質問をします (ただし、指示に従うのが苦手です)。
> > > 
> > > Phi 3 mini & medium: Mini の方が性能が良いですが、トレーニングに使用されたデータ量がはるかに少ないため、多くのオブジェクト/物事を認識していません。Medium は、Matt が述べたように、質問者であるときに質問をすることがあるため、[これら](https://www.kaggle.com/competitions/llm-20-questions/discussion/519297) のフラットアースに関する質問をしたボットである可能性があります。
> > > 
> > > Matt が上記のステートメントをした理由を理解するのに役立つことを願っています。
> > > 
> > > PS: 最良の戦略は、明らかにアルファベットによる二分法です。なぜなら、パブリックリーダーボードで非常に高いスコアを記録しているからです。
> > > 
> > > 
> > > 
> > ## G R Shanker Sai
> > 
> > [@matthewsfarmer](https://www.kaggle.com/matthewsfarmer) さん、こんにちは。
> > 
> > この洞察を共有していただきありがとうございます。ただ、"LLaMa 3 のコミュニティによるファインチューニング" とは、Huggingface に存在する Llama 3 のさまざまなフレーバーを指しているのでしょうか、それとも独自のデータでファインチューニングしているのでしょうか？
> > 
> > 
> > 
> > > ## Matthew S Farmer
> > > 
> > > はい、Huggingface のことです。私のコメントの最後にいくつか挙げました。私もモデルをファインチューニングしましたが、HF のモデルの方が優れています！
> > > 
> > > 
> > > 
> > ## Matthew S Farmer
> > 
> > RoPE エラーが解決されました: [https://www.kaggle.com/competitions/llm-20-questions/discussion/523619](https://www.kaggle.com/competitions/llm-20-questions/discussion/523619)
> > 
> > 
> > 
---
> ## Muhammad Ehsan
> 
> (ChatGPT-4o によって書かれました)
> 
> 以下に、各モデルの詳細を説明します。
> 
> - Gemma2-9B-IT:
> 
> このモデルは、90 億のパラメータを持ち、詳細な理解と複雑なタスクのために設計されています。コンテキストとニュアンスを深く理解する必要があるアプリケーションに役立ちます。
> 
> - Mistral-Nemo-Instruct-2407:
> 
> 120 億のパラメータを持つこのモデルは、指示タスク向けに最適化されており、つまり、与えられた特定の指示に従い、実行するのが得意です。
> 
> - Minitron-8B-base:
> 
> 80 億のパラメータを持つこのモデルは、汎用的なベースモデルとして機能します。汎用性が高く、さまざまなタスクに使用できますが、他のモデルと比較して特殊な機能がない可能性があります。
> 
> - Apple-DCLM-7B:
> 
> Apple によって開発されたこのモデルは、70 億のパラメータを持ちます。さまざまなアプリケーションを対象としており、Apple のエコシステムに固有の機能や最適化が含まれている可能性があります。
> 
> - Llama-3.1-8B-Instruct:
> 
> 80 億のパラメータを持つ Llama のこのバージョンは、指示やガイドラインに従うタスク向けに調整されています。特定のコマンドを理解し、それに応じて行動するように構築されています。
> 
> - Qwen2-7B-Instruct:
> 
> 70 億のパラメータを持つ、もう 1 つの指示に焦点を当てたモデルです。詳細な指示を効果的に解釈し、それに応答するように設計されています。
> 
> - Phi-3-Mini-4k-Instruct:
> 
> この小型モデルは、40 億のパラメータを持ち、指示に従うように最適化されており、広範な処理能力を必要としないが、優れたコマンドに従う能力を必要とするタスクに適しています。
> 
> - Phi-3-Medium-4k-Instruct:
> 
> 140 億のパラメータを持つ中型モデルであり、このモデルも指示に従うタスク向けに調整されており、小型バージョンと比較して、より多くの処理能力と複雑さを提供します。
> 
> 
> 
> > ## OminousDude
> > 
> > この回答を書くためにどのモデルを使用しましたか？Llama 3 でしたか？AI によって生成されたように見えます…
> > 
> > 
> > 
> > ## fufu2022
> > 
> > ありがとうございます！Gemma2-9B-IT と Llama-3.1-8B が私にとって最適です。
> > 
> > 
> > 
> > > ## torino
> > > 
> > > [@fufu2022](https://www.kaggle.com/fufu2022) さん、こんにちは。
> > > 
> > > Llama3.1 を提出環境にどのようにロードしますか？秘密でなければ、共有していただけますか？
> > > 
> > > 
> > > 
> > > ## OminousDude
> > > 
> > > 彼はまだ成功していないと思います。誰も成功していないと思います。彼は単にうまくいくと思っているだけだと思います。
> > > 
> > > 
> > > 
> > > ## Matthew S Farmer
> > > 
> > > 私は今日 [解決策を開発しました。](https://www.kaggle.com/competitions/llm-20-questions/discussion/523619)
> > > 
> > > 
> > > 
---
> ## francesco fiamingo
> 
> わあ！ありがとうございます。その中のいくつか (Mistral、Llama、Qwen) は試しましたが、他のいくつかは聞いたことがありませんでした！本当にありがとうございます！ところで、あなたの意見では、このゲームに最適なモデルはどれですか？
> 
> 
> 
---
> ## Aadit Shukla
> 
> まだこれらの新しいモデルを試す機会はありませんでしたが、パフォーマンスが本当に気になっています。聞いたところによると、印象的な機能を提供しているようです。ここで、これらのモデルを経験した人はいますか？あなたの考えを聞きたいです！
> 
>  更新をありがとうございます [@cdeotte](https://www.kaggle.com/cdeotte)。
> 
> 
> 
---


