{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4de1bcee",
   "metadata": {},
   "source": [
    "# LLM 20 Questions Baseline\n",
    "\n",
    "You can change models and prompts conveniently since I used `tokenizer.apply_chat_template` to apply special tokens automatically. \n",
    "\n",
    "\n",
    "Supported models:\n",
    "- `LLAMA3 variants`\n",
    "- `Phi-3 variants`\n",
    "- `Qwen-2 variants`\n",
    "\n",
    "## Prerequisites\n",
    "Set accelerator to GPU T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-01T02:42:36.432851Z",
     "iopub.status.busy": "2024-08-01T02:42:36.432507Z",
     "iopub.status.idle": "2024-08-01T02:43:16.74253Z",
     "shell.execute_reply": "2024-08-01T02:43:16.741715Z",
     "shell.execute_reply.started": "2024-08-01T02:42:36.432822Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "mkdir -p /tmp/model\n",
    "pip install -q bitsandbytes accelerate\n",
    "pip install -qU transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8a878b",
   "metadata": {},
   "source": [
    "## Download model\n",
    "\n",
    "### HuggingFace Login\n",
    "\n",
    "\n",
    "1. Issue HuggingFace access token (https://huggingface.co/settings/tokens)\n",
    "\n",
    "\n",
    "2. Add HuggingFace access token to secrets. You can find it in `Add-ons -> secrets`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![Screenshot 2024-08-01 at 11.40.17â€¯AM.png](attachment:fb5805e5-566e-41f1-ba50-0d9f9fade571.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.744239Z",
     "iopub.status.busy": "2024-08-01T02:43:16.743959Z",
     "iopub.status.idle": "2024-08-01T02:43:16.882488Z",
     "shell.execute_reply": "2024-08-01T02:43:16.88177Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.744215Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "\n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64804d00",
   "metadata": {},
   "source": [
    "### Select Model\n",
    "\n",
    "Find your desired model from [HuggingFace Model Hub](https://huggingface.co/models) and use the model name in the next command.\n",
    "\n",
    "Supported models:\n",
    "- `LLAMA3 variants`\n",
    "- `Phi-3 variants`\n",
    "- `Qwen-2 variants`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.883834Z",
     "iopub.status.busy": "2024-08-01T02:43:16.883562Z",
     "iopub.status.idle": "2024-08-01T02:43:16.887672Z",
     "shell.execute_reply": "2024-08-01T02:43:16.886858Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.883809Z"
    }
   },
   "outputs": [],
   "source": [
    "repo_id = \"meta-llama/Meta-Llama-3-8B-Instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5c6343",
   "metadata": {},
   "source": [
    "### Download Model via HuggingFace\n",
    "To reduce disk usage, download model in `/tmp/model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:43:16.890807Z",
     "iopub.status.busy": "2024-08-01T02:43:16.890312Z",
     "iopub.status.idle": "2024-08-01T02:46:44.781238Z",
     "shell.execute_reply": "2024-08-01T02:46:44.78017Z",
     "shell.execute_reply.started": "2024-08-01T02:43:16.890775Z"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=repo_id,\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:44.783114Z",
     "iopub.status.busy": "2024-08-01T02:46:44.78267Z",
     "iopub.status.idle": "2024-08-01T02:46:45.823393Z",
     "shell.execute_reply": "2024-08-01T02:46:45.82208Z",
     "shell.execute_reply.started": "2024-08-01T02:46:44.783078Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66994dc7",
   "metadata": {},
   "source": [
    "### Save quantized model\n",
    "Now, load downloaded model on memory with quantization.  \n",
    "This will save storage.\n",
    "\n",
    "\n",
    "Moreover, since the saved model has already been quantized, we do not need `bitsandbytes` package in `main.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:46:45.825488Z",
     "iopub.status.busy": "2024-08-01T02:46:45.825093Z",
     "iopub.status.idle": "2024-08-01T02:47:26.827498Z",
     "shell.execute_reply": "2024-08-01T02:47:26.826601Z",
     "shell.execute_reply.started": "2024-08-01T02:46:45.825451Z"
    }
   },
   "outputs": [],
   "source": [
    "# load model on memory\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "downloaded_model = \"/tmp/model\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    downloaded_model,\n",
    "    quantization_config = bnb_config,\n",
    "    torch_dtype = torch.float16,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(downloaded_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:26.829232Z",
     "iopub.status.busy": "2024-08-01T02:47:26.82879Z",
     "iopub.status.idle": "2024-08-01T02:47:44.162649Z",
     "shell.execute_reply": "2024-08-01T02:47:44.161658Z",
     "shell.execute_reply.started": "2024-08-01T02:47:26.829206Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model in submission directory\n",
    "model.save_pretrained(\"/kaggle/working/submission/model\")\n",
    "tokenizer.save_pretrained(\"/kaggle/working/submission/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.164433Z",
     "iopub.status.busy": "2024-08-01T02:47:44.164008Z",
     "iopub.status.idle": "2024-08-01T02:47:44.529886Z",
     "shell.execute_reply": "2024-08-01T02:47:44.529036Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.164407Z"
    }
   },
   "outputs": [],
   "source": [
    "# unload model from memory\n",
    "import gc, torch\n",
    "del model, tokenizer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653773a",
   "metadata": {},
   "source": [
    "## Agent\n",
    "\n",
    "### Prompts\n",
    "\n",
    "Prompts are reffered from the [Anthropic Prompt Library](https://docs.anthropic.com/en/prompt-library/library)\n",
    "\n",
    "Prompts are consisted of 2 parts:\n",
    "- `system_prompt`: This is the first question to determine category.\n",
    "- `chat_history`: This is the chat history to provide context for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:44.531437Z",
     "iopub.status.busy": "2024-08-01T02:47:44.531097Z",
     "iopub.status.idle": "2024-08-01T02:47:45.526013Z",
     "shell.execute_reply": "2024-08-01T02:47:45.524961Z",
     "shell.execute_reply.started": "2024-08-01T02:47:44.531412Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/prompts.py\n",
    "\n",
    "def asker_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    ask_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to ask questions to the user to guess the word the user is thinking of.\n",
    "Narrow down the possibilities by asking yes/no questions.\n",
    "Think step by step and try to ask the most informative questions.\n",
    "\\n\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": ask_prompt})\n",
    "\n",
    "    # Chat history\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        message.append({\"role\": \"assistant\", \"content\": q})\n",
    "        message.append({\"role\": \"user\", \"content\": a})\n",
    "\n",
    "    return message\n",
    "\n",
    "\n",
    "def guesser_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    guess_prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to guess the word the user is thinking of.\n",
    "Think step by step.\n",
    "\\n\"\"\"\n",
    "\n",
    "    # Chat history\n",
    "    chat_history = \"\"\n",
    "    for q, a in zip(obs.questions, obs.answers):\n",
    "        chat_history += f\"\"\"Question: {q}\\nAnswer: {a}\\n\"\"\"\n",
    "    prompt = (\n",
    "            guess_prompt + f\"\"\"so far, the current state of the game is as following:\\n{chat_history}\n",
    "        based on the conversation, can you guess the word, please give only the word, no verbosity around\"\"\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    return message\n",
    "\n",
    "\n",
    "def answerer_prompt(obs):\n",
    "    message = []\n",
    "    \n",
    "    # System prompt\n",
    "    prompt = f\"\"\"You are a helpful AI assistant with expertise in playing 20 questions game.\n",
    "Your task is to answer the questions of the user to help him guess the word you're thinking of.\n",
    "Your answers must be 'yes' or 'no'.\n",
    "The keyword is: \"{obs.keyword}\", it is of category: \"{obs.category}\"\n",
    "Provide accurate answers to help the user to guess the keyword.\n",
    "\"\"\"\n",
    "\n",
    "    message.append({\"role\": \"system\", \"content\": prompt})\n",
    "    \n",
    "    # Chat history\n",
    "    message.append({\"role\": \"user\", \"content\": obs.questions[0]})\n",
    "    \n",
    "    if len(obs.answers)>=1:\n",
    "        for q, a in zip(obs.questions[1:], obs.answers):\n",
    "            message.append({\"role\": \"assistant\", \"content\": a})\n",
    "            message.append({\"role\": \"user\", \"content\": q})\n",
    "    \n",
    "    return message\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c983e1",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "To add more LLM models, add end-of-turn token to terminators list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.530407Z",
     "iopub.status.busy": "2024-08-01T02:47:45.530085Z",
     "iopub.status.idle": "2024-08-01T02:47:45.539942Z",
     "shell.execute_reply": "2024-08-01T02:47:45.538701Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.53038Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from prompts import *\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    MODEL_PATH = os.path.join(KAGGLE_AGENT_PATH, \"model\")\n",
    "else:\n",
    "    MODEL_PATH = \"/kaggle/working/submission/model\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_PATH,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# specify end-of-turn tokens for your desired model\n",
    "terminators = [tokenizer.eos_token_id]\n",
    "\n",
    "# Additional potential end-of-turn token\n",
    "# llama3, phi3, gwen2 by order\n",
    "potential_terminators = [\"<|eot_id|>\", \"<|end|>\", \"<end_of_turn>\"]\n",
    "\n",
    "for token in potential_terminators:\n",
    "    token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "    if token_id is not None:\n",
    "        terminators.append(token_id)\n",
    "\n",
    "def generate_response(chat):\n",
    "    inputs = tokenizer.apply_chat_template(chat, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    outputs = model.generate(inputs, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, eos_token_id=terminators)\n",
    "    response = outputs[0][inputs.shape[-1]:]\n",
    "    out = tokenizer.decode(response, skip_special_tokens=True)\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\n",
    "            \"asking\", \"guessing\", \"answering\",\n",
    "        ], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            # launch the asker role\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            # launch the answerer role\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower():\n",
    "                output = \"no\"\n",
    "            if \"yes\" not in output.lower() and \"no\" not in output.lower():\n",
    "                output = \"yes\"\n",
    "        if mode == \"guessing\":\n",
    "            # launch the guesser role\n",
    "            output = self.guesser(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        \n",
    "        input = asker_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def guesser(self, obs):\n",
    "        input = guesser_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "    def answerer(self, obs):\n",
    "        input = answerer_prompt(obs)\n",
    "        output = generate_response(input)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "\n",
    "def agent(obs, cfg):\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = robot.on(mode=\"asking\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = robot.on(mode=\"guessing\", obs=obs)\n",
    "\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = robot.on(mode=\"answering\", obs=obs)\n",
    "\n",
    "    if response == None or len(response) <= 1:\n",
    "        response = \"yes\"\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf90d1",
   "metadata": {},
   "source": [
    "## Simulation\n",
    "\n",
    "### Install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:47:45.541971Z",
     "iopub.status.busy": "2024-08-01T02:47:45.541077Z",
     "iopub.status.idle": "2024-08-01T02:48:02.91265Z",
     "shell.execute_reply": "2024-08-01T02:48:02.911458Z",
     "shell.execute_reply.started": "2024-08-01T02:47:45.541938Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.914553Z",
     "iopub.status.busy": "2024-08-01T02:48:02.914201Z",
     "iopub.status.idle": "2024-08-01T02:48:02.921087Z",
     "shell.execute_reply": "2024-08-01T02:48:02.920176Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.914522Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile dumb.py\n",
    "\n",
    "def dumb_agent(obs, cfg):\n",
    "    \n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"Is it a duck?\"\n",
    "    # if agent is guesser and turnType is \"guess\"\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"duck\"\n",
    "    # if agent is the answerer\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"no\"\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:48:02.922495Z",
     "iopub.status.busy": "2024-08-01T02:48:02.922131Z",
     "iopub.status.idle": "2024-08-01T02:49:45.023058Z",
     "shell.execute_reply": "2024-08-01T02:49:45.022131Z",
     "shell.execute_reply.started": "2024-08-01T02:48:02.922463Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"llm_20_questions\", debug=True)\n",
    "game_output = env.run(agents=[\"submission/main.py\", \"submission/main.py\", \"dumb.py\", \"dumb.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:51:55.095662Z",
     "iopub.status.busy": "2024-08-01T02:51:55.095291Z",
     "iopub.status.idle": "2024-08-01T02:51:55.156977Z",
     "shell.execute_reply": "2024-08-01T02:51:55.155859Z",
     "shell.execute_reply.started": "2024-08-01T02:51:55.095636Z"
    }
   },
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\", width=600, height=700)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64082156",
   "metadata": {},
   "source": [
    "## Submit Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:45.091747Z",
     "iopub.status.busy": "2024-08-01T02:49:45.091472Z",
     "iopub.status.idle": "2024-08-01T02:49:51.432444Z",
     "shell.execute_reply": "2024-08-01T02:49:51.431242Z",
     "shell.execute_reply.started": "2024-08-01T02:49:45.091723Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T02:49:51.435019Z",
     "iopub.status.busy": "2024-08-01T02:49:51.434137Z",
     "iopub.status.idle": "2024-08-01T02:51:27.838366Z",
     "shell.execute_reply": "2024-08-01T02:51:27.836645Z",
     "shell.execute_reply.started": "2024-08-01T02:49:51.434979Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
