{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6887800e",
   "metadata": {},
   "source": [
    "Hi all, I split the code into several parts and added some comments and explanation generated from chatGPT. \n",
    "\n",
    "The Gemma Example in this link(https://www.kaggle.com/models/google/gemma/PyTorch/7b-it-quant/2) might useful for the beginner.\n",
    "\n",
    "Hope this notebook can help. \n",
    "\n",
    "---\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:25.021619Z",
     "iopub.status.busy": "2024-05-25T15:58:25.020891Z",
     "iopub.status.idle": "2024-05-25T15:58:38.22655Z",
     "shell.execute_reply": "2024-05-25T15:58:38.225563Z",
     "shell.execute_reply.started": "2024-05-25T15:58:25.021585Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60da11c1",
   "metadata": {},
   "source": [
    "# Part 1: File Creation\n",
    "\n",
    "- `%%writefile` tells the notebook to write everything in the cell below this line into a file.\n",
    "- `submission/main.py` is the path where the file will be written. If the directory `submission` doesn't exist, it will be created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:41.461435Z",
     "iopub.status.busy": "2024-05-25T15:58:41.46109Z",
     "iopub.status.idle": "2024-05-25T15:58:41.465484Z",
     "shell.execute_reply": "2024-05-25T15:58:41.46457Z",
     "shell.execute_reply.started": "2024-05-25T15:58:41.461408Z"
    }
   },
   "outputs": [],
   "source": [
    "# #%%writefile submission/main.py\n",
    "# # Setup\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80be6085",
   "metadata": {},
   "source": [
    "# Part 2: Importing Required Libraries and Setting Up Weights Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:42.15402Z",
     "iopub.status.busy": "2024-05-25T15:58:42.153406Z",
     "iopub.status.idle": "2024-05-25T15:58:45.700446Z",
     "shell.execute_reply": "2024-05-25T15:58:45.699681Z",
     "shell.execute_reply.started": "2024-05-25T15:58:42.153989Z"
    }
   },
   "outputs": [],
   "source": [
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c81148b",
   "metadata": {},
   "source": [
    "# Part 3: Prompt Formatting with GemmaFormatter\n",
    "\n",
    "- `GemmaFormatter`: formats the conversation between the user and the model using predefined tokens to mark the start and end of turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.702729Z",
     "iopub.status.busy": "2024-05-25T15:58:45.702182Z",
     "iopub.status.idle": "2024-05-25T15:58:45.714485Z",
     "shell.execute_reply": "2024-05-25T15:58:45.713694Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.702695Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    # Initilatization\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    # Returns the current state of the conversation as a string.\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    # Methods of Adding Turns - Adds a user prompt to the conversation\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "        \n",
    "    # Methods of Adding Turns - Adds a model response to the conversation.\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    # Methods of Adding Turns -  Marks the start of a user turn\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    # Methods of Adding Turns -  Marks the start of a model turn.\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    # Methods of Adding Turns -  Marks the end of the current turn\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    # Reset Method\n",
    "    def reset(self):\n",
    "        # Initializes `_state` to an empty string\n",
    "        self._state = \"\"  \n",
    "\n",
    "        # Adds the system prompt if provided.\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)  \n",
    "            \n",
    "        # Applies few-shot examples if provided\n",
    "        if self._few_shot_examples is not None: \n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d81f7b",
   "metadata": {},
   "source": [
    "###  Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.715692Z",
     "iopub.status.busy": "2024-05-25T15:58:45.715435Z",
     "iopub.status.idle": "2024-05-25T15:58:45.730863Z",
     "shell.execute_reply": "2024-05-25T15:58:45.730057Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.715671Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize formatter with a system prompt and few-shot examples\n",
    "formatter = GemmaFormatter(\n",
    "    system_prompt=\"This is a system prompt.\",\n",
    "    few_shot_examples=[\"Example question?\", \"Example answer.\"]\n",
    ")\n",
    "\n",
    "# Add a user turn\n",
    "formatter.user(\"What is the capital of France?\")\n",
    "\n",
    "# Add a model turn\n",
    "formatter.model(\"The capital of France is Paris.\")\n",
    "\n",
    "# Print the formatted conversation\n",
    "print(formatter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece06458",
   "metadata": {},
   "source": [
    "The `GemmaFormatter` class thus helps in structuring and formatting conversations in a consistent manner, ensuring that turns are properly marked and organized.\n",
    "\n",
    "# Part 4: Agent Definitions and Utilities\n",
    "\n",
    "- The `_set_default_tensor_type` context manager temporarily sets the default data type for PyTorch tensors to a specified type and then resets it back to torch.float after the block of code using the context manager is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.7338Z",
     "iopub.status.busy": "2024-05-25T15:58:45.733075Z",
     "iopub.status.idle": "2024-05-25T15:58:45.741278Z",
     "shell.execute_reply": "2024-05-25T15:58:45.740545Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.733776Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"Set the default torch dtype to the given dtype.\"\"\"\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    torch.set_default_dtype(torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066072f",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.742569Z",
     "iopub.status.busy": "2024-05-25T15:58:45.742251Z",
     "iopub.status.idle": "2024-05-25T15:58:45.76648Z",
     "shell.execute_reply": "2024-05-25T15:58:45.765652Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.742535Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.tensor([1.0, 2.0]).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.767743Z",
     "iopub.status.busy": "2024-05-25T15:58:45.767473Z",
     "iopub.status.idle": "2024-05-25T15:58:45.772646Z",
     "shell.execute_reply": "2024-05-25T15:58:45.771803Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.767721Z"
    }
   },
   "outputs": [],
   "source": [
    "with _set_default_tensor_type(torch.float64):\n",
    "    print(torch.tensor([1.0, 2.0]).dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6997e5",
   "metadata": {},
   "source": [
    "# Part 5: Base GemmaAgent Class\n",
    "\n",
    "The GemmaAgent class is designed to:\n",
    "\n",
    "- Initialize and configure a language model.\n",
    "- Format and handle prompts and responses.\n",
    "- Use context managers to temporarily set tensor data types.\n",
    "- Interact with the model to generate responses based on formatted prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:45.949245Z",
     "iopub.status.busy": "2024-05-25T15:58:45.948631Z",
     "iopub.status.idle": "2024-05-25T15:58:45.960292Z",
     "shell.execute_reply": "2024-05-25T15:58:45.959431Z",
     "shell.execute_reply.started": "2024-05-25T15:58:45.949221Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAgent:\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        \n",
    "        # Model Config.\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "        \n",
    "        # Model.\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a28e9",
   "metadata": {},
   "source": [
    "# Part 6: GemmaQuestionerAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:46.723064Z",
     "iopub.status.busy": "2024-05-25T15:58:46.722291Z",
     "iopub.status.idle": "2024-05-25T15:58:46.732867Z",
     "shell.execute_reply": "2024-05-25T15:58:46.731969Z",
     "shell.execute_reply.started": "2024-05-25T15:58:46.723034Z"
    }
   },
   "outputs": [],
   "source": [
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2542f4",
   "metadata": {},
   "source": [
    "GemmaQuestionerAgent:\n",
    "- `__init__` ： Sets up the agent by calling the parent class's constructor.\n",
    "-`_start_session` : Interleaving questions and answers and setting up the conversation format.\n",
    "- `_parse_response` : Interprets the model's responses differently depending on whether the agent is asking a question or making a guess.\n",
    "\n",
    "# Part 7: GemmaAnswererAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:47.844243Z",
     "iopub.status.busy": "2024-05-25T15:58:47.843475Z",
     "iopub.status.idle": "2024-05-25T15:58:47.850945Z",
     "shell.execute_reply": "2024-05-25T15:58:47.850005Z",
     "shell.execute_reply.started": "2024-05-25T15:58:47.844215Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b00ab49",
   "metadata": {},
   "source": [
    "# Part 8: Agent Creation and Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T15:58:49.041213Z",
     "iopub.status.busy": "2024-05-25T15:58:49.040862Z",
     "iopub.status.idle": "2024-05-25T15:58:49.051556Z",
     "shell.execute_reply": "2024-05-25T15:58:49.05063Z",
     "shell.execute_reply.started": "2024-05-25T15:58:49.041185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "\n",
    "# Initialize agent variable\n",
    "agent = None\n",
    "\n",
    "# Function to get the appropriate agent based on the name\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    # If agent is not initialized and the requested agent is a \"questioner\"\n",
    "    if agent is None and name == 'questioner':\n",
    "        # Initialize GemmaQuestionerAgent with specific parameters\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',  # Device for computation\n",
    "            system_prompt=system_prompt,  # System prompt for the agent\n",
    "            few_shot_examples=few_shot_examples,  # Examples to guide the agent's behavior\n",
    "        )\n",
    "    # If agent is not initialized and the requested agent is an \"answerer\"\n",
    "    elif agent is None and name == 'answerer':\n",
    "        # Initialize GemmaAnswererAgent with the same parameters\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    \n",
    "    # Ensure that the agent is initialized\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    # Return the initialized agent\n",
    "    return agent\n",
    "\n",
    "# Function to handle interactions based on observations\n",
    "def agent_fn(obs, cfg):\n",
    "    # If observation is for asking a question\n",
    "    if obs.turnType == \"ask\":\n",
    "        # Get the \"questioner\" agent to respond to the observation\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # If observation is for making a guess\n",
    "    elif obs.turnType == \"guess\":\n",
    "        # Get the \"questioner\" agent to respond to the observation\n",
    "        response = get_agent('questioner')(obs)\n",
    "    # If observation is for providing an answer\n",
    "    elif obs.turnType == \"answer\":\n",
    "        # Get the \"answerer\" agent to respond to the observation\n",
    "        response = get_agent('answerer')(obs)\n",
    "    \n",
    "    # If the response from the agent is either None or very short\n",
    "    if response is None or len(response) <= 1:\n",
    "        # Assume a positive response (\"yes\")\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        # Return the response received from the agent\n",
    "        return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca2095b",
   "metadata": {},
   "source": [
    "1. **GemmaFormatter Class**: This class handles formatting prompts for the game. It has methods to construct user and model turns, start user and model turns, end turns, reset the state, and apply turns. It ensures consistent formatting of prompts for the agents.\n",
    "\n",
    "2. **GemmaAgent Class**: This is an abstract class representing a generic agent in the game. It defines common methods and attributes such as initialization, call, starting a session, calling the language model (LLM), parsing responses, and setting default tensor type.\n",
    "\n",
    "3. **GemmaQuestionerAgent Class** and **GemmaAnswererAgent Class**: These classes inherit from GemmaAgent and implement specific behaviors for the Questioner and Answerer agents, respectively. They override the `_start_session` and `_parse_response` methods to customize the behavior of the agents.\n",
    "\n",
    "4. **interleave_unequal Function**: This function interleaves two lists of unequal lengths. It's used to interleave questions and answers in the game.\n",
    "\n",
    "5. **get_agent Function**: This function initializes and returns the appropriate agent based on the input name ('questioner' or 'answerer'). It ensures that only one instance of the agent is created and reused.\n",
    "\n",
    "6. **agent_fn Function**: This function acts as the entry point for the game. It determines the type of agent to use based on the observation's turn type ('ask', 'guess', or 'answer') and calls the respective agent's `__call__` method to generate a response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:06.686412Z",
     "iopub.status.busy": "2024-05-25T11:17:06.685976Z",
     "iopub.status.idle": "2024-05-25T11:17:06.693459Z",
     "shell.execute_reply": "2024-05-25T11:17:06.691475Z",
     "shell.execute_reply.started": "2024-05-25T11:17:06.686368Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-25T11:17:11.751057Z",
     "iopub.status.busy": "2024-05-25T11:17:11.750579Z",
     "iopub.status.idle": "2024-05-25T11:17:11.756935Z",
     "shell.execute_reply": "2024-05-25T11:17:11.755388Z",
     "shell.execute_reply.started": "2024-05-25T11:17:11.751024Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
