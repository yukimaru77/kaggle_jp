{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:46:55.794817Z",
     "iopub.status.busy": "2024-06-28T13:46:55.794466Z",
     "iopub.status.idle": "2024-06-28T13:47:25.200385Z",
     "shell.execute_reply": "2024-06-28T13:47:25.199204Z",
     "shell.execute_reply.started": "2024-06-28T13:46:55.794784Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup the environment\n",
    "!pip install -q -U immutabledict sentencepiece \n",
    "!pip install -q 'kaggle_environments>=1.14.8'\n",
    "!git clone https://github.com/google/gemma_pytorch.git\n",
    "!mkdir /kaggle/working/gemma/\n",
    "!mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/gemma/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:25.203258Z",
     "iopub.status.busy": "2024-06-28T13:47:25.202653Z",
     "iopub.status.idle": "2024-06-28T13:47:31.779874Z",
     "shell.execute_reply": "2024-06-28T13:47:31.779106Z",
     "shell.execute_reply.started": "2024-06-28T13:47:25.203217Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"/kaggle/working/gemma_pytorch/\") \n",
    "import contextlib\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "import kaggle_environments\n",
    "import itertools\n",
    "from gemma.config import GemmaConfig, get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "from gemma.tokenizer import Tokenizer\n",
    "from typing import Iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.781426Z",
     "iopub.status.busy": "2024-06-28T13:47:31.781003Z",
     "iopub.status.idle": "2024-06-28T13:47:31.791789Z",
     "shell.execute_reply": "2024-06-28T13:47:31.790918Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.7814Z"
    }
   },
   "outputs": [],
   "source": [
    "class GemmaFormatter:\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.794995Z",
     "iopub.status.busy": "2024-06-28T13:47:31.794426Z",
     "iopub.status.idle": "2024-06-28T13:47:31.808707Z",
     "shell.execute_reply": "2024-06-28T13:47:31.80787Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.794963Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(VARIANT, device):\n",
    "    WEIGHTS_PATH = f'/kaggle/input/gemma/pytorch/{VARIANT}/2' \n",
    "\n",
    "    @contextlib.contextmanager\n",
    "    def _set_default_tensor_type(dtype: torch.dtype):\n",
    "        \"\"\"Sets the default torch dtype to the given dtype.\"\"\"\n",
    "        torch.set_default_dtype(dtype)\n",
    "        yield\n",
    "        torch.set_default_dtype(torch.float)\n",
    "\n",
    "    # Model Config.\n",
    "    model_config = get_config_for_2b() if \"2b\" in VARIANT else get_config_for_7b()\n",
    "    model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "    model_config.quant = \"quant\" in VARIANT\n",
    "\n",
    "    # Model.\n",
    "    with _set_default_tensor_type(model_config.get_dtype()):\n",
    "        model = GemmaForCausalLM(model_config)\n",
    "        ckpt_path = os.path.join(WEIGHTS_PATH, f'gemma-{VARIANT}.ckpt')\n",
    "        model.load_weights(ckpt_path)\n",
    "        model = model.to(device).eval()\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.809865Z",
     "iopub.status.busy": "2024-06-28T13:47:31.809558Z",
     "iopub.status.idle": "2024-06-28T13:47:31.821925Z",
     "shell.execute_reply": "2024-06-28T13:47:31.821138Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.809814Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_keyword(response: str):\n",
    "    match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "    if match is None:\n",
    "        keyword = ''\n",
    "    else:\n",
    "        keyword = match.group().lower()\n",
    "    return keyword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.823345Z",
     "iopub.status.busy": "2024-06-28T13:47:31.823021Z",
     "iopub.status.idle": "2024-06-28T13:47:31.832614Z",
     "shell.execute_reply": "2024-06-28T13:47:31.831874Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.823315Z"
    }
   },
   "outputs": [],
   "source": [
    "def _parse_response(response: str, obs: dict):\n",
    "    if obs['turnType'] == 'ask':\n",
    "        match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "        if match is None:\n",
    "            question = \"Is it a person?\"\n",
    "        else:\n",
    "            question = match.group()\n",
    "        return question\n",
    "    elif obs['turnType'] == 'guess':\n",
    "        guess = _parse_keyword(response)\n",
    "        return guess\n",
    "    else:\n",
    "        raise ValueError(\"Unknown turn type:\", obs['turnType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.834217Z",
     "iopub.status.busy": "2024-06-28T13:47:31.83377Z",
     "iopub.status.idle": "2024-06-28T13:47:31.842001Z",
     "shell.execute_reply": "2024-06-28T13:47:31.841138Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.834187Z"
    }
   },
   "outputs": [],
   "source": [
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.843806Z",
     "iopub.status.busy": "2024-06-28T13:47:31.843109Z",
     "iopub.status.idle": "2024-06-28T13:47:31.851451Z",
     "shell.execute_reply": "2024-06-28T13:47:31.850649Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.843774Z"
    }
   },
   "outputs": [],
   "source": [
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a thing?\", \"**no**\",\n",
    "    \"Is is a country?\", \"**yes**\",\n",
    "    \"Is it Europe?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.852794Z",
     "iopub.status.busy": "2024-06-28T13:47:31.85249Z",
     "iopub.status.idle": "2024-06-28T13:47:31.864509Z",
     "shell.execute_reply": "2024-06-28T13:47:31.863739Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.852772Z"
    }
   },
   "outputs": [],
   "source": [
    "obs = {\n",
    "    'turnType': 'ask',\n",
    "    'questions': [\n",
    "        'Is it a living entity?',\n",
    "        'Is it man-made?',\n",
    "        'Can it be held in a single hand?'\n",
    "    ],\n",
    "    'answers': [\n",
    "        'no',\n",
    "        'yes',\n",
    "        'yes'\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.868247Z",
     "iopub.status.busy": "2024-06-28T13:47:31.867968Z",
     "iopub.status.idle": "2024-06-28T13:47:31.877294Z",
     "shell.execute_reply": "2024-06-28T13:47:31.876427Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.868204Z"
    }
   },
   "outputs": [],
   "source": [
    "formatter.reset()\n",
    "formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "turns = interleave_unequal(obs['questions'], obs['answers'])\n",
    "formatter.apply_turns(turns, start_agent='model')\n",
    "if obs['turnType'] == 'ask':\n",
    "    formatter.user(\"Please ask a yes-or-no question.\")\n",
    "elif obs['turnType'] == 'guess':\n",
    "    formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "formatter.start_model_turn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.87862Z",
     "iopub.status.busy": "2024-06-28T13:47:31.878349Z",
     "iopub.status.idle": "2024-06-28T13:47:31.887707Z",
     "shell.execute_reply": "2024-06-28T13:47:31.886764Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.878596Z"
    }
   },
   "outputs": [],
   "source": [
    "prompt = str(formatter)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.890731Z",
     "iopub.status.busy": "2024-06-28T13:47:31.888869Z",
     "iopub.status.idle": "2024-06-28T13:47:31.895711Z",
     "shell.execute_reply": "2024-06-28T13:47:31.895002Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.890706Z"
    }
   },
   "outputs": [],
   "source": [
    "MACHINE_TYPE = \"cuda\" \n",
    "device = torch.device(MACHINE_TYPE)\n",
    "max_new_tokens = 32\n",
    "sampler_kwargs = {\n",
    "'temperature': 0.01,\n",
    "'top_p': 0.1,\n",
    "'top_k': 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95167767",
   "metadata": {},
   "source": [
    "# Gemma 2b V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:47:31.897079Z",
     "iopub.status.busy": "2024-06-28T13:47:31.896743Z",
     "iopub.status.idle": "2024-06-28T13:48:09.346136Z",
     "shell.execute_reply": "2024-06-28T13:48:09.345216Z",
     "shell.execute_reply.started": "2024-06-28T13:47:31.897046Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(\"2b\", device)\n",
    "\n",
    "response = model.generate(\n",
    "    prompt,\n",
    "    device=device,\n",
    "    output_len=max_new_tokens,\n",
    "    **sampler_kwargs\n",
    ")\n",
    "response = _parse_response(response, obs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3593bd",
   "metadata": {},
   "source": [
    "# Gemma 2b-it V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:48:09.347623Z",
     "iopub.status.busy": "2024-06-28T13:48:09.34733Z",
     "iopub.status.idle": "2024-06-28T13:48:44.807608Z",
     "shell.execute_reply": "2024-06-28T13:48:44.806732Z",
     "shell.execute_reply.started": "2024-06-28T13:48:09.347597Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(\"2b-it\", device)\n",
    "\n",
    "response = model.generate(\n",
    "    prompt,\n",
    "    device=device,\n",
    "    output_len=max_new_tokens,\n",
    "    **sampler_kwargs\n",
    ")\n",
    "response = _parse_response(response, obs)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e930c666",
   "metadata": {},
   "source": [
    "# Gemma 7b-it-quant V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-28T13:48:44.809002Z",
     "iopub.status.busy": "2024-06-28T13:48:44.808712Z",
     "iopub.status.idle": "2024-06-28T13:49:57.08955Z",
     "shell.execute_reply": "2024-06-28T13:49:57.08861Z",
     "shell.execute_reply.started": "2024-06-28T13:48:44.808978Z"
    }
   },
   "outputs": [],
   "source": [
    "model = load_model(\"7b-it-quant\", device)\n",
    "\n",
    "response = model.generate(\n",
    "    prompt,\n",
    "    device=device,\n",
    "    output_len=max_new_tokens,\n",
    "    **sampler_kwargs\n",
    ")\n",
    "response = _parse_response(response, obs)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5305,
     "sourceId": 11357,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 5383,
     "sourceId": 11358,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
