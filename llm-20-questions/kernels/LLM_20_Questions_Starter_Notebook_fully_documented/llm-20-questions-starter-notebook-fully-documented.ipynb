{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f740f4ee",
   "metadata": {},
   "source": [
    "While working through the [Starter Notebook](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook), I created this fully documented version jointly with my buddy ChatGPT. The code is 100% the same, no changes, but I hope you find the comments useful.\n",
    "\n",
    "This notebook illustrates the agent creation process for the **LLM 20 Questions**. Running this notebook produces a `submission.tar.gz` file. You may submit this file directly from the **Submit to competition** heading to the right. Alternatively, from the notebook viewer, click the *Output* tab then find and download `submission.tar.gz`. Click **Submit Agent** at the upper-left of the competition homepage to upload your file and make your submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:06:48.488574Z",
     "iopub.status.busy": "2024-06-06T05:06:48.488156Z",
     "iopub.status.idle": "2024-06-06T05:07:03.276374Z",
     "shell.execute_reply": "2024-06-06T05:07:03.275345Z",
     "shell.execute_reply.started": "2024-06-06T05:06:48.488539Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working\n",
    "pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece\n",
    "git clone https://github.com/google/gemma_pytorch.git > /dev/null\n",
    "mkdir /kaggle/working/submission/lib/gemma/\n",
    "mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ff1d3e",
   "metadata": {},
   "source": [
    "The above cell sets up the environment by installing required Python packages and preparing the `gemma_pytorch` library for use in the agent. This ensures that all necessary dependencies are bundled together in the submission file:\n",
    "\n",
    "- `%%bash` is a cell magic command in Jupyter notebooks that indicates that the entire cell should be executed as a bash script. This means that all the lines following the `%%bash` will be interpreted and run as bash commands in the shell, rather than as Python code.\n",
    "\n",
    "- `cd /kaggle/working` changes the current directory to `/kaggle/working`, which is a working directory in the Kaggle environment where you can store files and set up your workspace.\n",
    "\n",
    "- `pip install -q -U -t /kaggle/working/submission/lib immutabledict sentencepiece` installs the `immutabledict` and `sentencepiece` Python packages.\n",
    "   - `-q` suppresses the output from the installation process (quiet mode).\n",
    "   - `-U` upgrades the packages if they are already installed.\n",
    "   - `-t /kaggle/working/submission/lib` specifies the target directory where the packages should be installed. This ensures that the dependencies are included in the submission package.\n",
    "\n",
    "- `git clone https://github.com/google/gemma_pytorch.git > /dev/null` clones the [`gemma_pytorch` repository](https://github.com/google/gemma_pytorch) from GitHub into the current directory (`/kaggle/working`).\n",
    "   - `> /dev/null` redirects the output of the `git clone` command to `/dev/null`, effectively hiding it from the notebook's output. The `/dev/null` is a special file in Unix-like operating systems, including the environment used by Kaggle. It acts as a black hole for data: any data written to `/dev/null` is discarded and cannot be retrieved. It is not a directory or a space where you can find files; rather, it is a way to suppress output, keeping the notebook output clean and free of unnecessary details.\n",
    "\n",
    "- `mkdir /kaggle/working/submission/lib/gemma/` creates a new directory named `gemma` inside the `submission/lib` directory. This directory will hold the files from the `gemma_pytorch` repository.\n",
    "\n",
    "- `mv /kaggle/working/gemma_pytorch/gemma/* /kaggle/working/submission/lib/gemma/` moves all the files from the `gemma` directory inside the cloned `gemma_pytorch` repository to the newly created `gemma` directory in `submission/lib`. This ensures that the necessary files from the `gemma_pytorch` repository are included in the submission package.\n",
    "\n",
    "These are the packeges which are installed:\n",
    "\n",
    "**immutabledict** is a package that provides an immutable dictionary. Immutable dictionaries are useful when you need to ensure that the dictionary cannot be modified after it has been created. This can help prevent accidental changes to the data structure, which is particularly important in certain applications like configurations, constants, or when working with concurrent programming.\n",
    "\n",
    "**sentencepiece** is the tokenizer of gemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:08.002368Z",
     "iopub.status.busy": "2024-06-06T05:07:08.001976Z",
     "iopub.status.idle": "2024-06-06T05:07:08.023358Z",
     "shell.execute_reply": "2024-06-06T05:07:08.02231Z",
     "shell.execute_reply.started": "2024-06-06T05:07:08.002336Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Import necessary modules for handling tensor operations and model configurations\n",
    "import torch\n",
    "from gemma.config import get_config_for_7b, get_config_for_2b\n",
    "from gemma.model import GemmaForCausalLM\n",
    "\n",
    "# Define the path for model weights.\n",
    "# Check if the script is running in the Kaggle simulation environment.\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    # If running in the Kaggle simulation environment, set the weights path accordingly\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"gemma/pytorch/7b-it-quant/2\")\n",
    "else:\n",
    "    # If running locally or in a different environment, use the local input directory for weights\n",
    "    WEIGHTS_PATH = \"/kaggle/input/gemma/pytorch/7b-it-quant/2\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class GemmaFormatter:\n",
    "    \"\"\"\n",
    "    Class for formatting prompts and responses for the 20 Questions game.\n",
    "    \"\"\"\n",
    "    _start_token = '<start_of_turn>'\n",
    "    _end_token = '<end_of_turn>'\n",
    "\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaFormatter with an optional system prompt and few-shot examples.\n",
    "        \n",
    "        Args:\n",
    "            system_prompt (str): Initial prompt for the system.\n",
    "            few_shot_examples (Iterable): Examples to initialize the prompt.\n",
    "        \"\"\"\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self._turn_user = f\"{self._start_token}user\\n{{}}{self._end_token}\\n\"\n",
    "        self._turn_model = f\"{self._start_token}model\\n{{}}{self._end_token}\\n\"\n",
    "        self.reset()\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        Return the current state of the prompt.\n",
    "        \n",
    "        Returns:\n",
    "            str: The current formatted state.\n",
    "        \"\"\"\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        \"\"\"\n",
    "        Add a user's prompt to the current state.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The user's prompt.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += self._turn_user.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        \"\"\"\n",
    "        Add a model's prompt to the current state.\n",
    "        \n",
    "        Args:\n",
    "            prompt (str): The model's prompt.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += self._turn_model.format(prompt)\n",
    "        return self\n",
    "\n",
    "    def start_user_turn(self):\n",
    "        \"\"\"\n",
    "        Start a new user turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}user\\n\"\n",
    "        return self\n",
    "\n",
    "    def start_model_turn(self):\n",
    "        \"\"\"\n",
    "        Start a new model turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._start_token}model\\n\"\n",
    "        return self\n",
    "\n",
    "    def end_turn(self):\n",
    "        \"\"\"\n",
    "        End the current turn.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state += f\"{self._end_token}\\n\"\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the formatter to its initial state,\n",
    "        including system prompt and few-shot examples if provided.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        self._state = \"\"\n",
    "        if self._system_prompt is not None:\n",
    "            self.user(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        \"\"\"\n",
    "        Apply a sequence of turns to the formatter.\n",
    "        \n",
    "        Args:\n",
    "            turns (Iterable): The sequence of turns to apply.\n",
    "            start_agent (str): Specifies whether 'user' or 'model' starts first.\n",
    "        \n",
    "        Returns:\n",
    "            self: The GemmaFormatter instance.\n",
    "        \"\"\"\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def _set_default_tensor_type(dtype: torch.dtype):\n",
    "    \"\"\"\n",
    "    Context manager for setting the default tensor type in PyTorch.\n",
    "\n",
    "    This context manager temporarily sets the default tensor type to the specified dtype\n",
    "    and restores the original default tensor type upon exiting the context.\n",
    "\n",
    "    Args:\n",
    "        dtype (torch.dtype): The desired default tensor type to set.\n",
    "\n",
    "    Yields:\n",
    "        None\n",
    "    \"\"\"\n",
    "    #Set the default torch dtype to the given dtype.\n",
    "    torch.set_default_dtype(dtype)\n",
    "    yield\n",
    "    # Restore the default tensor type to float\n",
    "    torch.set_default_dtype(torch.float)\n",
    "\n",
    "\n",
    "class GemmaAgent:\n",
    "    \"\"\"\n",
    "    Base class for an agent that interacts with the Gemma language model.\n",
    "    This class handles model initialization, prompt formatting, and response generation.\n",
    "    \"\"\"\n",
    "    def __init__(self, variant='7b-it-quant', device='cuda:0', system_prompt=None, few_shot_examples=None):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaAgent with the specified configuration.\n",
    "\n",
    "        Args:\n",
    "            variant (str): The model variant to use (e.g., '7b-it-quant').\n",
    "            device (str): The device to run the model on (e.g., 'cuda:0' for GPU).\n",
    "            system_prompt (str): Initial prompt for the system.\n",
    "            few_shot_examples (Iterable): Examples to initialize the prompt.\n",
    "        \"\"\"\n",
    "        self._variant = variant\n",
    "        self._device = torch.device(device)\n",
    "        self.formatter = GemmaFormatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "\n",
    "        print(\"Initializing model\")\n",
    "        # Get the model configuration based on the variant\n",
    "        model_config = get_config_for_2b() if \"2b\" in variant else get_config_for_7b()\n",
    "        model_config.tokenizer = os.path.join(WEIGHTS_PATH, \"tokenizer.model\")\n",
    "        model_config.quant = \"quant\" in variant\n",
    "\n",
    "        # Set the default tensor type and initialize the model\n",
    "        with _set_default_tensor_type(model_config.get_dtype()):\n",
    "            model = GemmaForCausalLM(model_config)\n",
    "            ckpt_path = os.path.join(WEIGHTS_PATH , f'gemma-{variant}.ckpt')\n",
    "            model.load_weights(ckpt_path)\n",
    "            self.model = model.to(self._device).eval()\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        \"\"\"\n",
    "        Handle a new observation by starting a session, generating a prompt, and parsing the response.\n",
    "\n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "            *args: Additional arguments.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        self._start_session(obs)\n",
    "        prompt = str(self.formatter)\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        print(f\"{response=}\")\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        \"\"\"\n",
    "        Start a new session for the agent.\n",
    "\n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32, **sampler_kwargs):\n",
    "        \"\"\"\n",
    "        Generate a response from the language model using the provided prompt.\n",
    "\n",
    "        Args:\n",
    "            prompt (str): The input prompt for the language model.\n",
    "            max_new_tokens (int): The maximum number of new tokens to generate.\n",
    "            **sampler_kwargs: Additional keyword arguments for the sampling process.\n",
    "\n",
    "        Returns:\n",
    "            str: The generated response from the model.\n",
    "        \"\"\"\n",
    "        if sampler_kwargs is None:\n",
    "            sampler_kwargs = {\n",
    "                'temperature': 0.01,\n",
    "                'top_p': 0.1,\n",
    "                'top_k': 1,\n",
    "        }\n",
    "        response = self.model.generate(\n",
    "            prompt,\n",
    "            device=self._device,\n",
    "            output_len=max_new_tokens,\n",
    "            **sampler_kwargs,\n",
    "        )\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        \"\"\"\n",
    "        Extract the keyword from the model's response.\n",
    "\n",
    "        Args:\n",
    "            response (str): The model's response.\n",
    "\n",
    "        Returns:\n",
    "            str: The extracted keyword, or an empty string if no keyword is found.\n",
    "        \"\"\"\n",
    "        # find a substring that is enclosed between double asterisks (**).\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response based on the observation.\n",
    "\n",
    "        Args:\n",
    "            response (str): The model's response.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "\n",
    "        Raises:\n",
    "            NotImplementedError: This method should be implemented by subclasses.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    \"\"\"\n",
    "    Interleave two lists, x and y, handling unequal lengths by filling in None for missing values.\n",
    "    \n",
    "    This function takes two lists and interleaves their elements. If the lists have unequal lengths,\n",
    "    it uses None to fill in the missing values and excludes these None values from the final result.\n",
    "\n",
    "    Args:\n",
    "        x (list): The first list to interleave.\n",
    "        y (list): The second list to interleave.\n",
    "\n",
    "    Returns:\n",
    "        list: A list with elements from x and y interleaved, excluding None values.\n",
    "    \n",
    "    Example:\n",
    "        >>> interleave_unequal([1, 2, 3], ['a', 'b'])\n",
    "        [1, 'a', 2, 'b', 3]\n",
    "    \"\"\"\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class GemmaQuestionerAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    Agent for playing the role of the Questioner in the 20 Questions game.\n",
    "    \n",
    "    This agent is responsible for generating questions based on the game state and \n",
    "    parsing responses from the language model to continue the questioning process.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaQuestionerAgent with the provided arguments.\n",
    "        \n",
    "        Args:\n",
    "            *args: Positional arguments to pass to the base class initializer.\n",
    "            **kwargs: Keyword arguments to pass to the base class initializer.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        Start a new session for the Questioner agent by resetting the formatter \n",
    "        and applying the initial prompt and previous turns.\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.questions (list): List of previous questions asked.\n",
    "                - obs.answers (list): List of corresponding answers received.\n",
    "                - obs.turnType (str): Type of the current turn ('ask' or 'guess').\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "        if obs.turnType == 'ask':\n",
    "            self.formatter.user(\"Please ask a yes-or-no question.\")\n",
    "        elif obs.turnType == 'guess':\n",
    "            self.formatter.user(\"Now guess the keyword. Surround your guess with double asterisks.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response based on the type of turn in the game.\n",
    "        \n",
    "        Args:\n",
    "            response (str): The response generated by the language model.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.turnType (str): Type of the current turn ('ask' or 'guess').\n",
    "        \n",
    "        Returns:\n",
    "            str: The parsed question or guess based on the turn type.\n",
    "        \n",
    "        Raises:\n",
    "            ValueError: If the turn type in the observation is unknown.\n",
    "        \"\"\"\n",
    "        if obs.turnType == 'ask':\n",
    "            match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "            if match is None:\n",
    "                question = \"Is it a person?\"\n",
    "            else:\n",
    "                question = match.group()\n",
    "            return question\n",
    "        elif obs.turnType == 'guess':\n",
    "            guess = self._parse_keyword(response)\n",
    "            return guess\n",
    "        else:\n",
    "            raise ValueError(\"Unknown turn type:\", obs.turnType)\n",
    "\n",
    "\n",
    "class GemmaAnswererAgent(GemmaAgent):\n",
    "    \"\"\"\n",
    "    Agent for playing the role of the Answerer in the 20 Questions game.\n",
    "    \n",
    "    This agent is responsible for providing yes-or-no answers based on the game state and \n",
    "    parsing responses from the language model to continue the answering process.\n",
    "    \"\"\"\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the GemmaAnswererAgent with the provided arguments.\n",
    "        \n",
    "        Args:\n",
    "            *args: Positional arguments to pass to the base class initializer.\n",
    "            **kwargs: Keyword arguments to pass to the base class initializer.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        \"\"\"\n",
    "        Start a new session for the Answerer agent by resetting the formatter \n",
    "        and applying the initial prompt and previous turns.\n",
    "        \n",
    "        Args:\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "                - obs.questions (list): List of previous questions asked.\n",
    "                - obs.answers (list): List of corresponding answers given.\n",
    "                - obs.keyword (str): The keyword that the questioner is trying to guess.\n",
    "                - obs.category (str): The category of the keyword.\n",
    "        \"\"\"\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "        self.formatter.start_model_turn()\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        \"\"\"\n",
    "        Parse the model's response to extract the yes-or-no answer.\n",
    "        \n",
    "        Args:\n",
    "            response (str): The response generated by the language model.\n",
    "            obs (dict): The observation dictionary containing game state information.\n",
    "        \n",
    "        Returns:\n",
    "            str: 'yes' if the answer contains 'yes', otherwise 'no'.\n",
    "        \"\"\"\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are an AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific person, place, or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is it a person?\", \"**no**\",\n",
    "    \"Is is a place?\", \"**yes**\",\n",
    "    \"Is it a country?\", \"**yes** Now guess the keyword.\",\n",
    "    \"**France**\", \"Correct!\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    \"\"\"\n",
    "    Initialize and return the appropriate agent (questioner or answerer) based on the provided name.\n",
    "    \n",
    "    This function uses a global variable to ensure that only one instance of the agent is created.\n",
    "    If the agent is not already initialized, it creates a new instance of either \n",
    "    GemmaQuestionerAgent or GemmaAnswererAgent based on the provided name.\n",
    "    \n",
    "    Args:\n",
    "        name (str): The name of the agent to initialize ('questioner' or 'answerer').\n",
    "\n",
    "    Returns:\n",
    "        GemmaAgent: An instance of either GemmaQuestionerAgent or GemmaAnswererAgent.\n",
    "    \n",
    "    Raises:\n",
    "        AssertionError: If the agent name is not recognized or the agent fails to initialize.\n",
    "    \"\"\"\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = GemmaQuestionerAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = GemmaAnswererAgent(\n",
    "            device='cuda:0',\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    \"\"\"\n",
    "    Determine the type of turn and invoke the appropriate agent to generate a response.\n",
    "    \n",
    "    This function examines the turn type in the observation and uses the corresponding agent \n",
    "    (questioner or answerer) to generate a response. If the response is None or empty, it returns \"yes\".\n",
    "    \n",
    "    Args:\n",
    "        obs (dict): The observation dictionary containing game state information.\n",
    "            - obs.turnType (str): The type of the current turn ('ask', 'guess', or 'answer').\n",
    "        cfg (dict): Configuration settings for the agent (not used in the current implementation).\n",
    "\n",
    "    Returns:\n",
    "        str: The response generated by the agent or \"yes\" if the response is None or empty.\n",
    "    \"\"\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:07:10.340786Z",
     "iopub.status.busy": "2024-06-06T05:07:10.340376Z",
     "iopub.status.idle": "2024-06-06T05:07:18.728324Z",
     "shell.execute_reply": "2024-06-06T05:07:18.726981Z",
     "shell.execute_reply.started": "2024-06-06T05:07:10.340755Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450c798",
   "metadata": {},
   "source": [
    "The above cell installs `pigz` for fast compression and `pv` for progress monitoring:\n",
    "\n",
    "- **apt install pigz pv**: Installs `pigz` (a parallel implementation of gzip) and `pv` (Pipe Viewer, which allows monitoring the progress of data through a pipeline).\n",
    "- **> /dev/null**: Redirects the command's output to `/dev/null`, suppressing the output to keep the notebook clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-06T05:08:42.811284Z",
     "iopub.status.busy": "2024-06-06T05:08:42.810305Z",
     "iopub.status.idle": "2024-06-06T05:11:31.295042Z",
     "shell.execute_reply": "2024-06-06T05:11:31.29284Z",
     "shell.execute_reply.started": "2024-06-06T05:08:42.811242Z"
    },
    "papermill": {
     "duration": 148.240766,
     "end_time": "2024-04-17T13:50:23.136669",
     "exception": false,
     "start_time": "2024-04-17T13:47:54.895903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ gemma/pytorch/7b-it-quant/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044aad92",
   "metadata": {},
   "source": [
    "The above cell packages the submission directory and necessary model files into a compressed tarball (`submission.tar.gz`). This tarball includes:\n",
    "   - All files in `/kaggle/working/submission`.\n",
    "   - The directory `gemma/pytorch/7b-it-quant/2` from `/kaggle/input`.\n",
    "   \n",
    "Detailed breakdown:\n",
    "- **--use-compress-program='pigz --fast --recursive | pv'**: Specifies to use `pigz` for compression, which will use multiple CPU cores to speed up the process. The `--fast` option ensures quick compression, and `--recursive` processes directories recursively. The output is piped through `pv` to monitor the progress.\n",
    "- **-cf submission.tar.gz**: Creates a file named `submission.tar.gz`.\n",
    "- **-C /kaggle/working/submission**: Changes to the directory `/kaggle/working/submission` before adding files to the archive.\n",
    "- **.**: Adds all files from the current directory (which is `/kaggle/working/submission` due to the `-C` option) to the archive.\n",
    "- **-C /kaggle/input/**: Changes to the `/kaggle/input/` directory before adding more files.\n",
    "- **gemma/pytorch/7b-it-quant/2**: Adds the `gemma/pytorch/7b-it-quant/2` directory and its contents from `/kaggle/input/` to the archive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 8749,
     "sourceId": 11359,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 8749,
     "sourceId": 11220,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
