{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f368c6f",
   "metadata": {},
   "source": [
    "# Twenty Questions with Phi-3\n",
    "\n",
    "## Install packages to the submission folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U -t /kaggle/working/submission/lib tqdm pydantic transformers -qq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31b0131",
   "metadata": {},
   "source": [
    "## Llama.cpp setup (unused as of now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" pip install -t /kaggle/working/submission/lib llama-cpp-python \\\n",
    "#   --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mkdir -p /kaggle/working/submission/lib/phi3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !curl -L \"https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf?download=true\" > \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os, sys\n",
    "# KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "# if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "#     sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "#     WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "# else:\n",
    "#     sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "#     WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d93c43",
   "metadata": {},
   "source": [
    "## Write the submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:16.018556Z",
     "iopub.status.busy": "2024-05-19T03:34:16.018251Z",
     "iopub.status.idle": "2024-05-19T03:34:16.035456Z",
     "shell.execute_reply": "2024-05-19T03:34:16.034339Z",
     "shell.execute_reply.started": "2024-05-19T03:34:16.018528Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "from pydantic.dataclasses import dataclass\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from typing import Literal, List\n",
    "import os, sys, json\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    os.chdir(os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "    #WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"lib/phi3/model.gguf\")\n",
    "else:\n",
    "    os.chdir(\"/kaggle/working/submission/lib\")\n",
    "    #WEIGHTS_PATH = \"/kaggle/working/submission/lib/phi3/model.gguf\"\n",
    "    \n",
    "print(f\"Current Directory is {os.getcwd()}. \\nFiles in here: {', '.join(os.listdir())}\")\n",
    "\n",
    "#Import model\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\",  \n",
    "    device_map=\"cuda\", torch_dtype=\"auto\", trust_remote_code=True,\n",
    "    cache_dir=\"./huggingface\"\n",
    "    \n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-4k-instruct\", cache_dir=\"./huggingface\")\n",
    "hf_llm = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def ask(prompt: str, max_new_tokens=100) -> str:\n",
    "    result = hf_llm(text_inputs=prompt, return_full_text=False, temperature=0.2, do_sample=False, max_new_tokens=max_new_tokens)\n",
    "    return result[0]['generated_text']\n",
    "\n",
    "assert ask(\"<|user|>\\nHello!<|end|>\\n<|assistant|>\")\n",
    "\n",
    "@dataclass\n",
    "class KaggleObservation:\n",
    "  remainingOverageTime: int | float\n",
    "  step: int\n",
    "\n",
    "  questions: list[str]\n",
    "  answers: list[str]\n",
    "  guesses: list[str]\n",
    "\n",
    "  role: Literal[\"guesser\", \"answerer\"]\n",
    "  turnType: Literal[\"ask\", \"guess\", \"answer\"]\n",
    "\n",
    "  keyword: str\n",
    "  category: str\n",
    "\n",
    "@dataclass\n",
    "class KaggleConfig:\n",
    "  episodeSteps: int\n",
    "  actTimeout: int | float\n",
    "  runTimeout: int | float\n",
    "  agentTimeout: int | float\n",
    "  __raw_path__: str\n",
    "\n",
    "# llm = Llama(\n",
    "#   model_path=WEIGHTS_PATH,  # path to GGUF file\n",
    "#   n_ctx=2048,  # The max sequence length to use - note that longer sequence lengths require much more resources\n",
    "#   n_threads=4, # The number of CPU threads to use, tailor to your system and the resulting performance\n",
    "#   n_gpu_layers=35, # The number of layers to offload to GPU, if you have GPU acceleration available. Set to 0 if no GPU acceleration is available on your system.\n",
    "#   use_mlock=True, # Whether to use mlock to lock the memory in RAM, preventing it from being swapped to disk. This is useful for large models that don't fit in RAM.\n",
    "#   use_mmap=False, #\n",
    "# )\n",
    "\n",
    "def get_context_prompt(observation: KaggleObservation) -> str:\n",
    "  questions = observation.questions\n",
    "  answers = observation.answers\n",
    "\n",
    "  history_prompt = \"\"\n",
    "  for index in range(len(max(questions, answers))):\n",
    "    history_prompt += f\"<|user|>\\n{questions[index]}<|end|>\\n\" if index < len(questions) else \"\"\n",
    "    history_prompt += f\"<|assistant|>\\n{answers[index]}<|end|>\\n\" if index < len(answers) else \"\"\n",
    "  #history_prompt += \"<|assistant|>\\n\"\n",
    "  \n",
    "  return history_prompt\n",
    "\n",
    "def get_guesser_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nLet's play 20 Questions. You are playing the role of the {observation.role.title()}.<|end|>\\n\"\n",
    "  prompt += get_context_prompt(observation)\n",
    "\n",
    "  if observation.turnType == \"ask\":\n",
    "    prompt += f\"<|user|>\\nTake a break, and ask a short yes-or-no question that would be useful to determine what the city I'm thinking about. Previous questions have been listed above. KEEP YOUR QUESTION ONE SENTENCE ONLY! Do not add any explaination to why you chose the question.<|end|>\\n\"\n",
    "  elif observation.turnType == \"guess\":\n",
    "    prompt += f\"<|user|>\\nNow, based on the information above, guess what city I'm thinking about, \" \n",
    "    prompt += f\"which aren't these: {', '.join(observation.guesses)}.\"\n",
    "    prompt += f\"Now, Make an informed guess, and only provide one word!<|end|>\\n\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid turnType: {observation.turnType}\\n\\n{observation}\")\n",
    "  \n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "\n",
    "  return prompt\n",
    "\n",
    "def get_answerer_prompt(observation: KaggleObservation) -> str:\n",
    "  prompt = f\"<|user|>\\nYou are a highly experienced tour guide specialized in the city {', '.join(observation.keyword.split(' '))}.\\n\"\n",
    "  prompt += \"You must answer a question about this city accurately, but only using the word **yes** or **no**.<|end|>\\n\"\n",
    "\n",
    "  prompt += f\"<|user|>{observation.questions[-1]}<|end|>\\n\"\n",
    "  prompt += \"<|assistant|>\\n\"\n",
    "  return prompt\n",
    "\n",
    "\n",
    "def play(obs, conf):\n",
    "  print(\"Observation: \" + json.dumps(obs, indent=2, ensure_ascii=False))\n",
    "  print(\"Confing: \" + json.dumps(conf, indent=2, ensure_ascii=False))\n",
    "  observation = KaggleObservation(**obs)\n",
    "  config = KaggleConfig(**conf)\n",
    "  if observation.role == \"guesser\":\n",
    "    prompt = get_guesser_prompt(observation)\n",
    "    result = ask(prompt, max_new_tokens=40).split(\"\\n\")[0].strip()#, stop=[\"<|end|>\"], max_tokens=256, temperature=0.5, echo=False)\n",
    "  elif observation.role == \"answerer\":\n",
    "    prompt = get_answerer_prompt(observation)\n",
    "    answer = ask(prompt, max_new_tokens=20)#, stop=[\"<|end|>\"], max_tokens=20, temperature=0.5, echo=False)\n",
    "    result = \"no\" if \"no\" in answer else \"yes\"\n",
    "  else:\n",
    "    raise ValueError(f\"Invalid role: {observation.role}\\n\\n{observation}\")\n",
    "  print(f\"Result: {result}\")\n",
    "  return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574e7c28",
   "metadata": {},
   "source": [
    "## Just checkin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:19.709658Z",
     "iopub.status.busy": "2024-05-19T03:34:19.708926Z",
     "iopub.status.idle": "2024-05-19T03:34:35.618916Z",
     "shell.execute_reply": "2024-05-19T03:34:35.617816Z",
     "shell.execute_reply.started": "2024-05-19T03:34:19.709623Z"
    }
   },
   "outputs": [],
   "source": [
    "from submission.main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:35.621549Z",
     "iopub.status.busy": "2024-05-19T03:34:35.620903Z",
     "iopub.status.idle": "2024-05-19T03:34:37.664916Z",
     "shell.execute_reply": "2024-05-19T03:34:37.663875Z",
     "shell.execute_reply.started": "2024-05-19T03:34:35.621517Z"
    }
   },
   "outputs": [],
   "source": [
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'guesser', \n",
    "  'turnType': 'ask', \n",
    "  'keyword': '', #eg. bangkok\n",
    "  'category': '', #eg. city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-19T03:34:37.666437Z",
     "iopub.status.busy": "2024-05-19T03:34:37.666159Z",
     "iopub.status.idle": "2024-05-19T03:34:38.716761Z",
     "shell.execute_reply": "2024-05-19T03:34:38.71582Z",
     "shell.execute_reply.started": "2024-05-19T03:34:37.666414Z"
    }
   },
   "outputs": [],
   "source": [
    "assert play({\n",
    "  'remainingOverageTime': 300, \n",
    "  'step': 0, \n",
    "  'questions': [\"Is the city you're thinking of located in North America?\"], \n",
    "  'guesses': [], \n",
    "  'answers': [], \n",
    "  'role': 'answerer', \n",
    "  'turnType': 'answer', \n",
    "  'keyword': '', #eg. bangkok\n",
    "  'category': '', #eg. city\n",
    "}, {\n",
    "  'episodeSteps': 61, \n",
    "  'actTimeout': 60, \n",
    "  'runTimeout': 9600, \n",
    "  'agentTimeout': 3600, \n",
    "  '__raw_path__': '/kaggle_simulations/agent/main.py'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df8b8c5",
   "metadata": {},
   "source": [
    "## Archiving the directory into a tar.gz to submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Success.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
