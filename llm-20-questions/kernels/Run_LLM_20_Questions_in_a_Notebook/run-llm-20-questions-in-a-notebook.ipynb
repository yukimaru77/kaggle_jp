{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464640fb",
   "metadata": {},
   "source": [
    "# Environment Tips: Run LLM 20 Questions in a Notebook\n",
    "\n",
    "\n",
    "To test and debug your agent in LLM 20 Questions or any other competition using Kaggle environments, it helps to be able to run the environment in a notebook. Here are some tips and explanations that may be helpful.\n",
    "\n",
    "# Create a Simple Agent\n",
    "\n",
    "If you just want to experiment, an agent can be as simple as a Python function. Your agent is a function with two inputs, obs and cfg, and it provides a text response as output. \n",
    "\n",
    "The agent needs to be able to handle three turnTypes (\"ask\", \"guess\" and \"answer\"). The response for answer has to be \"yes\" or \"no\".\n",
    "\n",
    "Here are four simple agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:38.614112Z",
     "iopub.status.busy": "2024-08-04T11:44:38.613368Z",
     "iopub.status.idle": "2024-08-04T11:44:38.622701Z",
     "shell.execute_reply": "2024-08-04T11:44:38.621666Z",
     "shell.execute_reply.started": "2024-08-04T11:44:38.614067Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_agent1(obs, cfg):\n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\": response = \"Is it a duck?\"\n",
    "    elif obs.turnType == \"guess\": response = \"duck\"\n",
    "    elif obs.turnType == \"answer\": response = \"no\"\n",
    "    return response\n",
    "\n",
    "def simple_agent2(obs, cfg):\n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\": response = \"Is it a bird?\"\n",
    "    elif obs.turnType == \"guess\": response = \"bird\"\n",
    "    elif obs.turnType == \"answer\": response = \"no\"\n",
    "    return response\n",
    "\n",
    "def simple_agent3(obs, cfg):\n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\": response = \"Is it a pig?\"\n",
    "    elif obs.turnType == \"guess\": response = \"pig\"\n",
    "    elif obs.turnType == \"answer\": response = \"no\"\n",
    "    return response\n",
    "\n",
    "def simple_agent4(obs, cfg):\n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\": response = \"Is it a cow?\"\n",
    "    elif obs.turnType == \"guess\": response = \"cow\"\n",
    "    elif obs.turnType == \"answer\": response = \"no\"\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ae17da",
   "metadata": {},
   "source": [
    "# Make and Configure Game Environment\n",
    "\n",
    "Kaggle environments are created with the `make()` function with the *environment* name (`\"llm_20_questions\"`) and some optional defaults, like *configuration* and *info*. If you want to run a game just like the competition, you can just use the defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:38.624485Z",
     "iopub.status.busy": "2024-08-04T11:44:38.624117Z",
     "iopub.status.idle": "2024-08-04T11:44:38.749991Z",
     "shell.execute_reply": "2024-08-04T11:44:38.749198Z",
     "shell.execute_reply.started": "2024-08-04T11:44:38.624461Z"
    }
   },
   "outputs": [],
   "source": [
    "import kaggle_environments\n",
    "env = kaggle_environments.make(environment=\"llm_20_questions\")\n",
    "# (you can ignore the error \"No pygame installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf8a146",
   "metadata": {},
   "source": [
    "When you initialize the environment, it sets the keyword to be guessed. You can inspect or change this in `kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:38.751847Z",
     "iopub.status.busy": "2024-08-04T11:44:38.75156Z",
     "iopub.status.idle": "2024-08-04T11:44:38.757664Z",
     "shell.execute_reply": "2024-08-04T11:44:38.756676Z",
     "shell.execute_reply.started": "2024-08-04T11:44:38.751823Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"The keyword for this session is: \")\n",
    "print(kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword)\n",
    "print(\" \")\n",
    "print(\"Some keywords have a list of alternative guesses (alts) that are also accepted.\")\n",
    "print(\"For this session, the list of alts is:\")\n",
    "print(kaggle_environments.envs.llm_20_questions.llm_20_questions.alts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420a075f",
   "metadata": {},
   "source": [
    "# Running LLM 20 Questions (default)\n",
    "\n",
    "With the environment you created (`env`), you run the game in this environment. When you run the game, you must submit a list of four agents: \n",
    "* \"Agent1\" (guesser for Team 1), \n",
    "* \"Agent2\" (answerer for Team 1), \n",
    "* \"Agent3\" (guesser for Team 2), \n",
    "* \"Agent4\" (answerer for Team 2). \n",
    "\n",
    "In the competition, you are randomly paired with a teammate to either be the guesser or the answerer.\n",
    "\n",
    "(When I first started this competition, I mistakenly thought your agent plays both the guesser and answerer role for the team. But you are paired with someone else in the competition. You do well or poorly depending on your ability to cooperate with a random partner.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:38.758939Z",
     "iopub.status.busy": "2024-08-04T11:44:38.758684Z",
     "iopub.status.idle": "2024-08-04T11:44:39.185557Z",
     "shell.execute_reply": "2024-08-04T11:44:39.184598Z",
     "shell.execute_reply.started": "2024-08-04T11:44:38.758917Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "game_output = env.run(agents=[simple_agent1, simple_agent2, simple_agent3, simple_agent4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1371a277",
   "metadata": {},
   "source": [
    "The game in this example completes quickly since the simple agents respond immediately. A real game with large LLM's as agents could take a minute for each step, so the total game could take an hour!\n",
    "\n",
    "You can look at the data from each step of the game in `game_output`.\n",
    "\n",
    "If want to watch the game visually, you can render it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:39.188108Z",
     "iopub.status.busy": "2024-08-04T11:44:39.187797Z",
     "iopub.status.idle": "2024-08-04T11:44:39.25925Z",
     "shell.execute_reply": "2024-08-04T11:44:39.258232Z",
     "shell.execute_reply.started": "2024-08-04T11:44:39.18807Z"
    }
   },
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0bfba3",
   "metadata": {},
   "source": [
    "# Create an Agent that Could be Submitted\n",
    "\n",
    "To submit an agent to the competition, you need to write the Python code for the agent in a file titled `main.py` and put it along with any supporting files in `submission.tar.gz`\n",
    "\n",
    "A simple example is below. Of course, in the actual competition, you'll probably want to use a real LLM like in the official starter notebook (https://www.kaggle.com/code/muhammadehsan000/llm-20-questions-starter). Running LLM agents in a notebook will take more time and memory, so if you're testing your LLM agent as player 1, you might want to put a simple agent as player 2.\n",
    "\n",
    "* Create a directory `/kaggle/working/submission` with a subdirectory `lib` where you would put any supporting files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:39.260591Z",
     "iopub.status.busy": "2024-08-04T11:44:39.260314Z",
     "iopub.status.idle": "2024-08-04T11:44:39.266042Z",
     "shell.execute_reply": "2024-08-04T11:44:39.265141Z",
     "shell.execute_reply.started": "2024-08-04T11:44:39.260567Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "submission_directory = \"/kaggle/working/submission\"\n",
    "submission_subdirectory = \"lib\"\n",
    "\n",
    "# Create the main directory if it doesn't exist\n",
    "if not os.path.exists(submission_directory):\n",
    "    os.mkdir(submission_directory)\n",
    "    subdirectory_path = os.path.join(submission_directory, submission_subdirectory)\n",
    "    os.mkdir(subdirectory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:39.267309Z",
     "iopub.status.busy": "2024-08-04T11:44:39.26701Z",
     "iopub.status.idle": "2024-08-04T11:44:39.275727Z",
     "shell.execute_reply": "2024-08-04T11:44:39.274693Z",
     "shell.execute_reply.started": "2024-08-04T11:44:39.267285Z"
    }
   },
   "outputs": [],
   "source": [
    "# create an example file to save in the lib directory\n",
    "import csv\n",
    "with open(os.path.join(subdirectory_path, \"example.csv\"),mode='w') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"cow\", \"horse\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95b68b",
   "metadata": {},
   "source": [
    "* Write the main.py Python code for your agent\n",
    "* The environment will use the last function in main.py for your agent, in this case `agent_fun()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:39.277407Z",
     "iopub.status.busy": "2024-08-04T11:44:39.277012Z",
     "iopub.status.idle": "2024-08-04T11:44:39.290427Z",
     "shell.execute_reply": "2024-08-04T11:44:39.289563Z",
     "shell.execute_reply.started": "2024-08-04T11:44:39.277376Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /kaggle/working/submission/main.py\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import random\n",
    "\n",
    "\n",
    "# If you put other files (e.g. model weights) in your submission/lib directory, you need to set the path\n",
    "KAGGLE_COMPETITION_PATH = \"/kaggle_simulations/agent/\" # competition path\n",
    "if os.path.exists(KAGGLE_COMPETITION_PATH):  # if running in the competition\n",
    "    subdirectory_path = os.path.join(KAGGLE_COMPETITION_PATH, \"lib\")\n",
    "else: # if running in notebook\n",
    "    subdirectory_path = os.path.join(\"/kaggle/working/submission/\", \"lib\")\n",
    "sys.path.insert(0, subdirectory_path)\n",
    "\n",
    "\n",
    "# Loading our example file\n",
    "with open(os.path.join(subdirectory_path,\"example.csv\"), mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    guess_list = list(reader)\n",
    "    guess_list = guess_list[0]\n",
    "\n",
    "# Setting a random \"animal\" from example file as a global variable\n",
    "animal = random.choice(guess_list)\n",
    "    \n",
    "# Last function in the main.py will be the agent function\n",
    "def agent_fn(obs, cfg):\n",
    "    \n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = f'Does it look like a {animal}?'\n",
    "    # if agent is guesser and turnType is \"guess\"\n",
    "    elif obs.turnType == \"guess\":\n",
    "        if obs.answers[-1]==\"yes\":\n",
    "            response = animal\n",
    "        else:\n",
    "            response = \"penguin\"\n",
    "    # if agent is the answerer\n",
    "    elif obs.turnType == \"answer\":\n",
    "        if obs.keyword in obs.questions[-1]:\n",
    "            response = \"yes\"\n",
    "        else:\n",
    "            response = \"no\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8a678a",
   "metadata": {},
   "source": [
    "This `main.py` file with the agent is ready to submit along with the `/lib/example.csv` supporting file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:39.292181Z",
     "iopub.status.busy": "2024-08-04T11:44:39.291728Z",
     "iopub.status.idle": "2024-08-04T11:44:46.946371Z",
     "shell.execute_reply": "2024-08-04T11:44:46.945156Z",
     "shell.execute_reply.started": "2024-08-04T11:44:39.29215Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null\n",
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085cebcd",
   "metadata": {},
   "source": [
    "You can run the agent in `main.py` from this Jupyter notebook as both players on Team 1, and we'll use simple_agent3 and simple_agent4 for Team 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:46.948394Z",
     "iopub.status.busy": "2024-08-04T11:44:46.948026Z",
     "iopub.status.idle": "2024-08-04T11:44:47.421039Z",
     "shell.execute_reply": "2024-08-04T11:44:47.420117Z",
     "shell.execute_reply.started": "2024-08-04T11:44:46.948366Z"
    }
   },
   "outputs": [],
   "source": [
    "game_output = env.run(agents=[\"/kaggle/working/submission/main.py\", \"/kaggle/working/submission/main.py\", simple_agent3, simple_agent4])\n",
    "env.render(mode=\"ipython\", width=600, height=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b25868",
   "metadata": {},
   "source": [
    "# Debugging Tips\n",
    "\n",
    "When you're designing and debugging, you normally want to change some of the optional arguments in creating the environment. These include:\n",
    "\n",
    "`env = make(environment, configuration=None, info=None, steps=None, logs=None, debug=False, state=None)`\n",
    "\n",
    "You can look at the specifications in `env.specification` to learn about how `configuration` and other objects defined in the environment. It has explanations and shows the default values.\n",
    "\n",
    "When working on new agents, I'd suggest changing the configuration to run a shorter episode with only a few steps and setting `debug=True` so you can see any verbose output printed by your agents.\n",
    "\n",
    "Here is a new environment that is better for debugging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.42522Z",
     "iopub.status.busy": "2024-08-04T11:44:47.424544Z",
     "iopub.status.idle": "2024-08-04T11:44:47.54417Z",
     "shell.execute_reply": "2024-08-04T11:44:47.543462Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.425186Z"
    }
   },
   "outputs": [],
   "source": [
    "# For debugging, play game with only two rounds\n",
    "debug_config = {'episodeSteps': 7,     # initial step plus 3 steps per round (ask/answer/guess)\n",
    "                'actTimeout': 5,       # agent time per round in seconds; default is 60\n",
    "                'runTimeout': 60,      # max time for the episode in seconds; default is 1200\n",
    "                'agentTimeout': 3600}  # obsolete field; default is 3600\n",
    "\n",
    "env = kaggle_environments.make(\"llm_20_questions\", configuration=debug_config, debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1353f",
   "metadata": {},
   "source": [
    "Note that are session already set the keyword to be guessed. So if you run another game, you'll be guessing the same keyword!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.545562Z",
     "iopub.status.busy": "2024-08-04T11:44:47.54521Z",
     "iopub.status.idle": "2024-08-04T11:44:47.550129Z",
     "shell.execute_reply": "2024-08-04T11:44:47.549275Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.54553Z"
    }
   },
   "outputs": [],
   "source": [
    "print(kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2184e668",
   "metadata": {},
   "source": [
    "When debugging, you might want to set the keyword manually (or randomly from the keywords list)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.551592Z",
     "iopub.status.busy": "2024-08-04T11:44:47.551287Z",
     "iopub.status.idle": "2024-08-04T11:44:47.559525Z",
     "shell.execute_reply": "2024-08-04T11:44:47.558731Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.551558Z"
    }
   },
   "outputs": [],
   "source": [
    "keyword = \"Duck\"\n",
    "alts = [\"The Duck\",\"A Duck\"]\n",
    "kaggle_environments.envs.llm_20_questions.llm_20_questions.category = \"Example\"\n",
    "kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword_obj = {'keyword':keyword,'alts':alts}\n",
    "kaggle_environments.envs.llm_20_questions.llm_20_questions.keyword = keyword\n",
    "kaggle_environments.envs.llm_20_questions.llm_20_questions.alts = alts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51830a13",
   "metadata": {},
   "source": [
    "And we can have our agent print some information for debugging. I added print statements to simple agent 1 to show what information is available in `obs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.560853Z",
     "iopub.status.busy": "2024-08-04T11:44:47.560596Z",
     "iopub.status.idle": "2024-08-04T11:44:47.57073Z",
     "shell.execute_reply": "2024-08-04T11:44:47.569893Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.560831Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_verbose_agent1(obs, cfg):\n",
    "    \n",
    "    # if agent is guesser and turnType is \"ask\"\n",
    "    if obs.turnType == \"ask\":\n",
    "        response = \"Is it a duck?\"\n",
    "    # if agent is guesser and turnType is \"guess\"\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = \"duck\"\n",
    "    # if agent is the answerer\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = \"no\"\n",
    "    \n",
    "    # Print debugging information\n",
    "    print(\"====================\")\n",
    "    print(f\"step = {obs.step}\")\n",
    "    print(f\"turnType = {obs.turnType}\")\n",
    "    print(\"obs =\")\n",
    "    print(obs)\n",
    "    print(\" \")\n",
    "    print(f'response = \"{response}\"')\n",
    "    \n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffd415",
   "metadata": {},
   "source": [
    "Putting this simple_verbose_agent1 as both players on Team 1 allows us to observe each of the three turn types (ask/guess/answer). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.571981Z",
     "iopub.status.busy": "2024-08-04T11:44:47.571748Z",
     "iopub.status.idle": "2024-08-04T11:44:47.691997Z",
     "shell.execute_reply": "2024-08-04T11:44:47.691127Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.571962Z"
    }
   },
   "outputs": [],
   "source": [
    "game_output = env.run(agents=[simple_verbose_agent1,simple_verbose_agent1, simple_agent3, \"/kaggle/working/submission/main.py\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T11:44:47.693257Z",
     "iopub.status.busy": "2024-08-04T11:44:47.69298Z",
     "iopub.status.idle": "2024-08-04T11:44:47.703361Z",
     "shell.execute_reply": "2024-08-04T11:44:47.702405Z",
     "shell.execute_reply.started": "2024-08-04T11:44:47.693234Z"
    }
   },
   "outputs": [],
   "source": [
    "env.render(mode=\"ipython\", width=600, height=500)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
