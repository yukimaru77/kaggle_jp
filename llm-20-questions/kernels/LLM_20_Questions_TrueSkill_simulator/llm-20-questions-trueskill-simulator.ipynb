{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "436814be",
   "metadata": {},
   "source": [
    "The fairness of the ranking system of the [LLM 20 Questions competition](https://www.kaggle.com/competitions/llm-20-questions/overview) has been questioned multiple times in discussions, so I decided to write a small toying notebook to evaluate it.  We're not going to evaluate the maths, just play with data.\n",
    "\n",
    "# Official description of the ranking system\n",
    "\n",
    "At the time of writing (July 17th), the competition's overview states :\n",
    "> ### Evaluation\n",
    ">\n",
    "> Each day your team is able to submit up to 5 agents (bots) to the competition. Each submission will play episodes (games) against other bots on the leaderboard that have a similar skill rating. Over time, skill ratings will go up with wins, down with losses, or evened out with ties.\n",
    "> \n",
    "> This competition is configured to run in a cooperative, 2 vs. 2 format. Your bot will be randomly paired with a bot of similar skill in order to face off against another random pairing. On each pair, one bot will be randomly assigned as questioner and the other as answerer. Since you win/lose/tie as a pair, you are incentivized to work together!\n",
    "> \n",
    "> Every bot submitted will continue to play episodes until the end of the competition, with newer bots selected to play more frequently. Once you have reached three active submissions, older ones will be deactivated. On the leaderboard, only your best scoring bot will be shown, but you can track the progress of all of your submissions on your Submissions page.\n",
    "> \n",
    "> Each submission has an estimated skill rating which is modeled by a Gaussian N(μ,σ2) where μ is the estimated skill and σ represents the uncertainty of that estimate which will decrease over time.\n",
    "> \n",
    "> When you upload a submission, we first play a validation episode where that submission plays against copies of itself to make sure it works properly. If the episode fails, the submission is marked as error and you can download the agent logs to help figure out why. Otherwise, we initialize the submission with μ0=600 and it joins the pool of for ongoing evaluation. At this time we also deactivate older agents if the total number of active agents is greater than three.\n",
    "> \n",
    "> ### Ranking System\n",
    "> \n",
    "> After an episode finishes, we'll update the rating estimate for all bots in the episode. If one bot pair won, we'll increase their μ and decrease the opponent's μ -- if the result was a tie, then we'll move the μ values closer towards their mean. The updates will have magnitude relative to the deviation from the expected result based on the previous μ values, and also relative to each bot’s uncertainty σ. We also reduce the σ terms relative to the amount of information gained by the result. The score by which your bot wins or loses an episode does not affect the skill rating updates.\n",
    "> \n",
    "> ### Final Evaluation\n",
    "> \n",
    "> At the submission deadline on August 13, 2024, submissions will be locked. From August 13, 2024 to August 27th, 2024 we will continue to run episodes against a new set of unpublished, secret words. During this period only your three active submissions will be eligible for the leaderboard. At the conclusion of this period, the leaderboard is final.\n",
    "\n",
    "# TrueSkill\n",
    "\n",
    "I don't know much about rating systems.  A quick search shows that a candidate for a system with a $\\mu$ **and** $\\sigma$ parameters may be the [Glicko rating system](https://en.wikipedia.org/wiki/Glicko_rating_system).  I can't remember how I came to this conclusion (maybe it had to do with team play ?), but another more likely candidate is [TrueSkill](https://en.wikipedia.org/wiki/TrueSkill).  I won't go into details, follow the Wikipedia link if you're interested.  It seems quite popular in the world of video games.  There is a [TrueSkill 2](https://www.microsoft.com/en-us/research/uploads/prod/2018/03/trueskill2.pdf), but at first glance its improvements don't seem relevant for our kind of competition.\n",
    "\n",
    "## Installation\n",
    "\n",
    "Fortunately, there is a [Python library](https://github.com/sublee/trueskill) of TrueSkill available on [Pypi](https://pypi.org/project/trueskill/), let's install it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:40.937504Z",
     "iopub.status.busy": "2024-07-20T13:33:40.937077Z",
     "iopub.status.idle": "2024-07-20T13:33:53.25384Z",
     "shell.execute_reply": "2024-07-20T13:33:53.252406Z",
     "shell.execute_reply.started": "2024-07-20T13:33:40.937471Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install trueskill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f333cb",
   "metadata": {},
   "source": [
    "Players ratings are represented by a `Rating` class, and a game is played by `rate()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.257394Z",
     "iopub.status.busy": "2024-07-20T13:33:53.25688Z",
     "iopub.status.idle": "2024-07-20T13:33:53.264373Z",
     "shell.execute_reply": "2024-07-20T13:33:53.262782Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.257352Z"
    }
   },
   "outputs": [],
   "source": [
    "from trueskill import Rating, rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d74b2f",
   "metadata": {},
   "source": [
    "## Let's play\n",
    "\n",
    "Let's define four players with $\\mu=600$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.26666Z",
     "iopub.status.busy": "2024-07-20T13:33:53.26616Z",
     "iopub.status.idle": "2024-07-20T13:33:53.280295Z",
     "shell.execute_reply": "2024-07-20T13:33:53.278956Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.266618Z"
    }
   },
   "outputs": [],
   "source": [
    "player1 = Rating(mu=600)\n",
    "player2 = Rating(mu=600)\n",
    "player3 = Rating(mu=600)\n",
    "player4 = Rating(mu=600)\n",
    "\n",
    "teamA = [ player1, player2 ]\n",
    "teamB = [ player3, player4 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fb6b7e",
   "metadata": {},
   "source": [
    "Now let them play a game :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.282315Z",
     "iopub.status.busy": "2024-07-20T13:33:53.281865Z",
     "iopub.status.idle": "2024-07-20T13:33:53.299601Z",
     "shell.execute_reply": "2024-07-20T13:33:53.29792Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.282277Z"
    }
   },
   "outputs": [],
   "source": [
    "rate(\n",
    "    [ teamA, teamB ],\n",
    "    [ 0, 1 ]  # teamA is ranked lower than teamB, so teamA won\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c71580db",
   "metadata": {},
   "source": [
    "The `rate()` function returns the new ratings for each players, by teams.\n",
    "\n",
    "## Simple display function\n",
    "\n",
    "It will be convenient to define a function to help reading the consequences of a game :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.303163Z",
     "iopub.status.busy": "2024-07-20T13:33:53.302772Z",
     "iopub.status.idle": "2024-07-20T13:33:53.319621Z",
     "shell.execute_reply": "2024-07-20T13:33:53.318188Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.303131Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "def play_game(\n",
    "    teamA: Tuple[Rating, Rating],\n",
    "    teamB: Tuple[Rating, Rating],\n",
    "    outcome: Tuple[int, int]\n",
    "):\n",
    "    \"\"\"\n",
    "    Play a game between two teams and display the effect on the players' ratings.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        teamA: Tuple[Rating, Rating]\n",
    "            A team of two players.\n",
    "        teamB: Tuple[Rating, Rating]\n",
    "            A team of two players.\n",
    "        outcome: Tuple[int, int]\n",
    "            The ranking of the players in the game (0: first, 1: second)\n",
    "    \"\"\"\n",
    "    new_ratings = rate(\n",
    "        [ teamA, teamB ],\n",
    "        outcome\n",
    "    )\n",
    "    for old,new in zip(teamA, new_ratings[0]):\n",
    "        delta = int(new.mu - old.mu)\n",
    "        print(f\"{old.mu:.0f} -> {new.mu:.0f} ({'+' if delta >=0 else ''}{delta})\")\n",
    "    print(\"vs\")\n",
    "    for old,new in zip(teamB, new_ratings[1]):\n",
    "        delta = int(new.mu - old.mu)\n",
    "        print(f\"{old.mu:.0f} -> {new.mu:.0f} ({'+' if delta >=0 else ''}{delta})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765102b5",
   "metadata": {},
   "source": [
    "Applying it on our previous players :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.322078Z",
     "iopub.status.busy": "2024-07-20T13:33:53.321593Z",
     "iopub.status.idle": "2024-07-20T13:33:53.334908Z",
     "shell.execute_reply": "2024-07-20T13:33:53.333636Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.322038Z"
    }
   },
   "outputs": [],
   "source": [
    "play_game(teamA, teamB, [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21536b3a",
   "metadata": {},
   "source": [
    "## Finding sigma\n",
    "\n",
    "Now it will quick become obvious that the competition doesn't use the $\\sigma$ default value.  The TrueSkill recommandation is to set $\\sigma = \\frac{\\mu}{3}$, but after playing a bit with these parameters I believe the relation for this competition has rather been set to $\\sigma = \\frac{\\mu}{2} = 300$.\n",
    "\n",
    "Indeed, let's try to reproduce the outcome of an actual episode.  Here is the first win of my first submission :\n",
    "\n",
    "> [1st] gguillard 600 (+123) vs [1st] JavaZero 570 (+34) vs\n",
    ">\n",
    "> [3rd] atom1231 577 (-72) vs [3rd] KKY 464 (-44)\n",
    "\n",
    "With a little trial and error, and assuming my $\\sigma$ would not be very far than its initialization at 300, it is quick and easy to find a matching combination of parameters :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.33718Z",
     "iopub.status.busy": "2024-07-20T13:33:53.336766Z",
     "iopub.status.idle": "2024-07-20T13:33:53.354564Z",
     "shell.execute_reply": "2024-07-20T13:33:53.35308Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.337142Z"
    }
   },
   "outputs": [],
   "source": [
    "play_game(\n",
    "    [\n",
    "        Rating(mu=600, sigma=295),\n",
    "        Rating(mu=570, sigma=156)\n",
    "    ],\n",
    "    [\n",
    "        Rating(mu=577, sigma=225),\n",
    "        Rating(mu=464, sigma=176)\n",
    "    ],\n",
    "    [ 0, 1 ]  # first team won\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7251948",
   "metadata": {},
   "source": [
    "## The draw probability\n",
    "\n",
    "So is the scoring system TrueSkill with $\\mu=600$ and $\\sigma=300$ ?  Well, not quite.  These settings don't work well in case of draws, the variations are much larger than what can be observed in the competition (typically 0 to ± a few units) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.356954Z",
     "iopub.status.busy": "2024-07-20T13:33:53.356537Z",
     "iopub.status.idle": "2024-07-20T13:33:53.370446Z",
     "shell.execute_reply": "2024-07-20T13:33:53.368912Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.356917Z"
    }
   },
   "outputs": [],
   "source": [
    "play_game(\n",
    "    [\n",
    "        Rating(mu=600, sigma=295),\n",
    "        Rating(mu=570, sigma=156)\n",
    "    ],\n",
    "    [\n",
    "        Rating(mu=577, sigma=225),\n",
    "        Rating(mu=464, sigma=176)\n",
    "    ],\n",
    "    [ 1, 1 ]  # no winner\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8e4252",
   "metadata": {},
   "source": [
    "Reading again the rules, the keypoint is here :\n",
    "\n",
    "> We also reduce the σ terms relative to the amount of information gained by the result.\n",
    "\n",
    "At first I played a bit with parameters and thought $\\sigma$ was manually reduced by a factor 100 for draws, it seemed to work fairly well.  Then reading `help(trueskill)` I figured it may already be taken into account within TrueSkill, since the V and W functions, responsible for the $\\mu$ and $\\sigma$ update, respectively, have a \"win\" **and** a \"draw\" version.\n",
    "\n",
    "Actually, one can define a TrueSkill environment with a few more parameters, including `draw_probability`.  That's interesting, because we all know the draw probability for a game in this competition is… *fairly high*.\n",
    "\n",
    "Let's compute it, we'll need it for our simulation.  We'll use @waechter's [LLM 20 Questions - Games dataset](https://www.kaggle.com/code/waechter/llm-20-questions-games-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:33:53.372436Z",
     "iopub.status.busy": "2024-07-20T13:33:53.37207Z",
     "iopub.status.idle": "2024-07-20T13:34:54.361897Z",
     "shell.execute_reply": "2024-07-20T13:34:54.360753Z",
     "shell.execute_reply.started": "2024-07-20T13:33:53.372408Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "games_df = pd.read_csv(\n",
    "    \"/kaggle/input/llm-20-questions-games-dataset/LLM-20Questions-games.csv\",\n",
    "    converters = {\n",
    "        col_list: literal_eval\n",
    "        for col_list in [\"answers\", \"questions\", \"guesses\"]\n",
    "    }\n",
    ")\n",
    "games_df = games_df[games_df[\"guesser_SubmissionId\"]!=0]  # it seems that submissions of the day are not yet labelled\n",
    "games_df[\"CreateTime\"] = pd.to_datetime(games_df[\"CreateTime\"], format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1090132",
   "metadata": {},
   "source": [
    "This dataframe contains two rows per game, one for each team, and a boolean \"guessed\" column.  In case of a draw, \"guessed\" is the same for both teams (although we could safely ignore the 3 games where both teams won at the same round…)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.363788Z",
     "iopub.status.busy": "2024-07-20T13:34:54.363362Z",
     "iopub.status.idle": "2024-07-20T13:34:54.414383Z",
     "shell.execute_reply": "2024-07-20T13:34:54.4131Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.363749Z"
    }
   },
   "outputs": [],
   "source": [
    "games_df.groupby(\"game_num\")[\"guessed\"].sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.416008Z",
     "iopub.status.busy": "2024-07-20T13:34:54.415666Z",
     "iopub.status.idle": "2024-07-20T13:34:54.423592Z",
     "shell.execute_reply": "2024-07-20T13:34:54.422411Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.41598Z"
    }
   },
   "outputs": [],
   "source": [
    "(144607 + 3) / (144607 + 1208 + 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6992c85f",
   "metadata": {},
   "source": [
    "Therefore, the probability of a draw is : $p \\simeq 0.9917$.  Fairly high indeed…  We'll update it later with more accurate data (spoiler : it'll only get worse).\n",
    "\n",
    "## Beta and tau\n",
    "\n",
    "The documentation also mentions two other parameters that may be of interest :\n",
    "\n",
    ">  :param beta: the distance which guarantees about 76% chance of winning.\n",
    ">               The recommended value is a half of ``sigma``.\n",
    ">\n",
    ">  :param tau: the dynamic factor which restrains a fixation of rating.  The\n",
    ">              recommended value is ``sigma`` per cent.\n",
    "\n",
    "Let's use the recommended values, namely $\\beta = \\frac{\\sigma}{2} = 150$ and $\\tau = \\frac{\\sigma}{100} = 3$.\n",
    "\n",
    "Now let's define our new TrueSkill environment :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.426179Z",
     "iopub.status.busy": "2024-07-20T13:34:54.425846Z",
     "iopub.status.idle": "2024-07-20T13:34:54.439415Z",
     "shell.execute_reply": "2024-07-20T13:34:54.43828Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.426154Z"
    }
   },
   "outputs": [],
   "source": [
    "from trueskill import setup\n",
    "\n",
    "setup(\n",
    "    mu = 600,\n",
    "    sigma = 300,\n",
    "    beta = 150,\n",
    "    tau = 3,\n",
    "    draw_probability = 0.9917\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4d3c99",
   "metadata": {},
   "source": [
    "It takes a little bit of adjustment again to get the right $\\sigma$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.44138Z",
     "iopub.status.busy": "2024-07-20T13:34:54.440906Z",
     "iopub.status.idle": "2024-07-20T13:34:54.453125Z",
     "shell.execute_reply": "2024-07-20T13:34:54.451802Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.441337Z"
    }
   },
   "outputs": [],
   "source": [
    "play_game(\n",
    "    [\n",
    "        Rating(mu=600, sigma=144),\n",
    "        Rating(mu=570, sigma=76)\n",
    "    ],\n",
    "    [\n",
    "        Rating(mu=577, sigma=109),\n",
    "        Rating(mu=464, sigma=85)\n",
    "    ],\n",
    "    [ 0, 1 ]  # first team won\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f316d86a",
   "metadata": {},
   "source": [
    "But then, ratings update after a draw are much more consistent with what we actually observe in the competition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.460725Z",
     "iopub.status.busy": "2024-07-20T13:34:54.459243Z",
     "iopub.status.idle": "2024-07-20T13:34:54.46884Z",
     "shell.execute_reply": "2024-07-20T13:34:54.467572Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.460676Z"
    }
   },
   "outputs": [],
   "source": [
    "play_game(\n",
    "    [\n",
    "        Rating(mu=600, sigma=144),\n",
    "        Rating(mu=570, sigma=76)\n",
    "    ],\n",
    "    [\n",
    "        Rating(mu=577, sigma=109),\n",
    "        Rating(mu=464, sigma=85)\n",
    "    ],\n",
    "    [ 1, 1 ]  # no winner\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f93dc0b",
   "metadata": {},
   "source": [
    "It looks to me that we managed to construct a scoring system quite similar to the one used by the LLM 20 Questions competition.  It is possible that the parameters are not *exactly* those ones, and anyway the draw probability couldn't have been deduced before a certain number of games (maybe it's updated continuously ? – it also accepts a function, BTW), but I would be surprised if the ability of these settings to reproduce the results of a game was a coincidence.\n",
    "\n",
    "# Modeling the competition\n",
    "\n",
    "## Winning probabilities\n",
    "\n",
    "Now what about generating a full board of players, teaming them up randomly and having them play ? :D\n",
    "\n",
    "Let's first extract some realistic probabilities of winning.  Using the *LLM 20 Questions - Games dataset* again, we'll compute the `nb_guessed / nb_played` ratio for each submission.\n",
    "\n",
    "But in order to get a realistic distribution of skills, we will first filter out submissions with a small number of games played (say 10) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.470566Z",
     "iopub.status.busy": "2024-07-20T13:34:54.470167Z",
     "iopub.status.idle": "2024-07-20T13:34:54.490357Z",
     "shell.execute_reply": "2024-07-20T13:34:54.489151Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.470535Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_games_played = games_df.groupby(\"guesser_SubmissionId\").size()\n",
    "nb_games_played = nb_games_played[nb_games_played>=10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.492146Z",
     "iopub.status.busy": "2024-07-20T13:34:54.491807Z",
     "iopub.status.idle": "2024-07-20T13:34:54.590065Z",
     "shell.execute_reply": "2024-07-20T13:34:54.589034Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.492118Z"
    }
   },
   "outputs": [],
   "source": [
    "games_df = games_df[games_df[\"guesser_SubmissionId\"].isin(nb_games_played.index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbead2e",
   "metadata": {},
   "source": [
    "Since the competition only allows three submissions per competitor, we also have to remove all bots but the last three — some people have more than 100 submissions !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.592534Z",
     "iopub.status.busy": "2024-07-20T13:34:54.591788Z",
     "iopub.status.idle": "2024-07-20T13:34:54.638829Z",
     "shell.execute_reply": "2024-07-20T13:34:54.63772Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.592495Z"
    }
   },
   "outputs": [],
   "source": [
    "games_df.groupby(\"guesser\")[\"guesser_SubmissionId\"].nunique().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.640503Z",
     "iopub.status.busy": "2024-07-20T13:34:54.640135Z",
     "iopub.status.idle": "2024-07-20T13:34:54.748272Z",
     "shell.execute_reply": "2024-07-20T13:34:54.747023Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.640473Z"
    }
   },
   "outputs": [],
   "source": [
    "last_bots = games_df.sort_values(\n",
    "    by = \"CreateTime\"  # don't sort by \"guesser\" here because some submissions have several names\n",
    ").drop_duplicates(\n",
    "    subset = \"guesser_SubmissionId\",\n",
    "    keep = \"last\"\n",
    ").groupby(\n",
    "    [ \"guesser\" ]\n",
    ")[[\n",
    "    \"guesser\",\n",
    "    \"guesser_SubmissionId\",\n",
    "    \"CreateTime\"\n",
    "]].tail(3).sort_values(\n",
    "    by = \"guesser\"\n",
    ")[\"guesser_SubmissionId\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.750282Z",
     "iopub.status.busy": "2024-07-20T13:34:54.749833Z",
     "iopub.status.idle": "2024-07-20T13:34:54.797709Z",
     "shell.execute_reply": "2024-07-20T13:34:54.796632Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.750241Z"
    }
   },
   "outputs": [],
   "source": [
    "last_games_df = games_df[games_df[\"guesser_SubmissionId\"].isin(last_bots)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b23ff60",
   "metadata": {},
   "source": [
    "Time to update our draw probability :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.799555Z",
     "iopub.status.busy": "2024-07-20T13:34:54.79912Z",
     "iopub.status.idle": "2024-07-20T13:34:54.830249Z",
     "shell.execute_reply": "2024-07-20T13:34:54.829136Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.799517Z"
    }
   },
   "outputs": [],
   "source": [
    "last_games_df.groupby(\"game_num\")[\"guessed\"].sum().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.831778Z",
     "iopub.status.busy": "2024-07-20T13:34:54.831459Z",
     "iopub.status.idle": "2024-07-20T13:34:54.839422Z",
     "shell.execute_reply": "2024-07-20T13:34:54.838194Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.831752Z"
    }
   },
   "outputs": [],
   "source": [
    "(123874 + 1) / (123874 + 546 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c9ee306",
   "metadata": {},
   "source": [
    "As mentioned earlier, it's even worse than before, with the probability of a draw being $p \\simeq 0.9956$…\n",
    "\n",
    "We'll need to distinguish between \"good bots\" and \"dummy bots\" to compute the actual skills distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.841505Z",
     "iopub.status.busy": "2024-07-20T13:34:54.841065Z",
     "iopub.status.idle": "2024-07-20T13:34:54.874546Z",
     "shell.execute_reply": "2024-07-20T13:34:54.873313Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.841469Z"
    }
   },
   "outputs": [],
   "source": [
    "goodbots = set(\n",
    "    last_games_df[\n",
    "        last_games_df.groupby(\"guesser_SubmissionId\")[\"guessed\"].transform(\"any\")\n",
    "    ][\"guesser_SubmissionId\"].unique()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.876144Z",
     "iopub.status.busy": "2024-07-20T13:34:54.875823Z",
     "iopub.status.idle": "2024-07-20T13:34:54.883658Z",
     "shell.execute_reply": "2024-07-20T13:34:54.882398Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.87612Z"
    }
   },
   "outputs": [],
   "source": [
    "badbots = set(last_games_df[\"guesser_SubmissionId\"].unique()) - goodbots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.886244Z",
     "iopub.status.busy": "2024-07-20T13:34:54.885051Z",
     "iopub.status.idle": "2024-07-20T13:34:54.897491Z",
     "shell.execute_reply": "2024-07-20T13:34:54.896387Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.886127Z"
    }
   },
   "outputs": [],
   "source": [
    "len(goodbots), len(badbots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577a4a8",
   "metadata": {},
   "source": [
    "Now let's consider every team of good bots.  We don't care about the opponents being dumb, because it only takes a reliable teammate to find the keyword.  We will also discard sessions in which the team didn't have time to find the keyword because the other team did before, as this would only bias our simulation.  On the other hand, we keep instances in which the keyword was guessed although the teammate is in the badbots category, because it may just be that the teammate didn't have a chance to win yet.  Incidentally, removing these cases would drastically reduce the statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.899865Z",
     "iopub.status.busy": "2024-07-20T13:34:54.899416Z",
     "iopub.status.idle": "2024-07-20T13:34:54.927197Z",
     "shell.execute_reply": "2024-07-20T13:34:54.925925Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.899826Z"
    }
   },
   "outputs": [],
   "source": [
    "fair_games = last_games_df[\"guesser_SubmissionId\"].isin(goodbots) * (\n",
    "    last_games_df[\"answerer_SubmissionId\"].isin(goodbots) + last_games_df[\"guessed\"]\n",
    ")\n",
    "fair_games_df = last_games_df.loc[fair_games]\n",
    "fair_games_df = fair_games_df[(fair_games_df[\"guessed\"]) | (fair_games_df[\"nb_round\"]==20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.928963Z",
     "iopub.status.busy": "2024-07-20T13:34:54.928626Z",
     "iopub.status.idle": "2024-07-20T13:34:54.955558Z",
     "shell.execute_reply": "2024-07-20T13:34:54.954386Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.928934Z"
    }
   },
   "outputs": [],
   "source": [
    "skills_df = fair_games_df.groupby(\"guesser_SubmissionId\").agg(\n",
    "    guessed = (\"guessed\", \"sum\"),\n",
    "    played = (\"guessed\", \"size\")\n",
    ").sort_index()\n",
    "skills_df[\"skill\"] = skills_df[\"guessed\"] / skills_df[\"played\"]\n",
    "skills_df.sort_values(by = \"skill\", ascending = False, inplace = True)\n",
    "skills_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc458a61",
   "metadata": {},
   "source": [
    "Let's remove bots with less than $N = 10$ fair games in order to avoid unrealistic probabilities (we already did it for *all* games, but *fair* games is an other story) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.957187Z",
     "iopub.status.busy": "2024-07-20T13:34:54.956863Z",
     "iopub.status.idle": "2024-07-20T13:34:54.969262Z",
     "shell.execute_reply": "2024-07-20T13:34:54.968259Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.957159Z"
    }
   },
   "outputs": [],
   "source": [
    "skills_df = skills_df[skills_df[\"played\"]>=10]\n",
    "skills_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bae8dc7",
   "metadata": {},
   "source": [
    "Finally, let's increase the dynamic range of skills by allowing the best guesser to be a superguesser with 98 % of chances to find the keyword.  Thats very optimistic !  But this will also be representative of the evolution towards better bots as we approach the end of the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.971278Z",
     "iopub.status.busy": "2024-07-20T13:34:54.970946Z",
     "iopub.status.idle": "2024-07-20T13:34:54.981237Z",
     "shell.execute_reply": "2024-07-20T13:34:54.980089Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.97125Z"
    }
   },
   "outputs": [],
   "source": [
    "BEST_SKILL = .98\n",
    "skills_df[\"skill\"] *= BEST_SKILL / skills_df[\"skill\"].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973bfdb",
   "metadata": {},
   "source": [
    "Now we have a somehow realistic representation of the global skill distribution of the current (legit) bots.  Let's have a look at it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.982895Z",
     "iopub.status.busy": "2024-07-20T13:34:54.982566Z",
     "iopub.status.idle": "2024-07-20T13:34:54.994731Z",
     "shell.execute_reply": "2024-07-20T13:34:54.993518Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.982868Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:54.996483Z",
     "iopub.status.busy": "2024-07-20T13:34:54.996104Z",
     "iopub.status.idle": "2024-07-20T13:34:55.336454Z",
     "shell.execute_reply": "2024-07-20T13:34:55.335017Z",
     "shell.execute_reply.started": "2024-07-20T13:34:54.996445Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "_ = plt.hist(skills_df[\"skill\"], bins=15)\n",
    "_ = plt.xlabel(\"Probability to find the keyword against a fair teammate\")\n",
    "_ = plt.ylabel(\"Number of bots\")\n",
    "_ = plt.title(\"Good bots skill distribution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b29dea7",
   "metadata": {},
   "source": [
    "That should lead to a clear leaderboard !\n",
    "\n",
    "## A Player class\n",
    "\n",
    "Now let's create a Player class in order to keep track of the ratings history :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.338487Z",
     "iopub.status.busy": "2024-07-20T13:34:55.338001Z",
     "iopub.status.idle": "2024-07-20T13:34:55.346266Z",
     "shell.execute_reply": "2024-07-20T13:34:55.344973Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.338444Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from trueskill import TrueSkill\n",
    "\n",
    "class Player:\n",
    "    def __init__(\n",
    "        self,\n",
    "        skill: float,\n",
    "        env: Optional[TrueSkill] = None  # ignore this for now, this will be useful later\n",
    "    ):\n",
    "        self.skill = skill\n",
    "        self.rating = Rating() if env is None else env.Rating()  # start with environment default values, i.e. (600,300)\n",
    "        self.history = []\n",
    "        \n",
    "    def update(self, new_rating):\n",
    "        self.history.append(self.rating)\n",
    "        self.rating = new_rating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce43dbd3",
   "metadata": {},
   "source": [
    "## A pool of players\n",
    "\n",
    "We can then build a pool of players, some with a range of realistic skills, along with a number of dummy bots.  Since there was a significant reduction of the number of good bots considered for the skills evaluation, it seems fair to apply the same reduction ratio to the number of bad bots as well, but anyway it shouldn't change much the outcome of the simulation (actually it may get worse…).  Play with it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.348153Z",
     "iopub.status.busy": "2024-07-20T13:34:55.347775Z",
     "iopub.status.idle": "2024-07-20T13:34:55.368358Z",
     "shell.execute_reply": "2024-07-20T13:34:55.367266Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.348122Z"
    }
   },
   "outputs": [],
   "source": [
    "reduction_ratio = len(skills_df) / len(goodbots)\n",
    "\n",
    "players = { ii: Player(s) for ii,s in enumerate(skills_df[\"skill\"].values) }\n",
    "players.update({ len(players)+ii: Player(0) for ii in range(int(len(badbots)*reduction_ratio)) })\n",
    "\n",
    "len(players)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44609da",
   "metadata": {},
   "source": [
    "We still have more players than in the actual leaderboard, but that's the number of bots, remember each competitor on the leaderboard may have up to three active submissions.\n",
    "\n",
    "## A game function\n",
    "\n",
    "Finally, we need to define a game function taking into account the winning probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.369869Z",
     "iopub.status.busy": "2024-07-20T13:34:55.369569Z",
     "iopub.status.idle": "2024-07-20T13:34:55.38036Z",
     "shell.execute_reply": "2024-07-20T13:34:55.378824Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.369844Z"
    }
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.383034Z",
     "iopub.status.busy": "2024-07-20T13:34:55.382581Z",
     "iopub.status.idle": "2024-07-20T13:34:55.394885Z",
     "shell.execute_reply": "2024-07-20T13:34:55.393905Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.382994Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_match(\n",
    "    teamA: Tuple[Player, Player],\n",
    "    teamB: Tuple[Player, Player],\n",
    "    env: Optional[TrueSkill] = None,\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plays a match between two teams.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        teamA: Tuple[Player, Player]\n",
    "            A team of two players.\n",
    "        teamB: Tuple[Player, Player]\n",
    "            Another team of two players.\n",
    "        debug: boolean\n",
    "            A flag for debugging.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Players skills :\", [p.skill for p in teamA+teamB])\n",
    "        print(\"Players ratings :\", [tuple(p.rating for p in teamA), tuple(p.rating for p in teamB)])\n",
    "    # for each team, if any player is a dummy bot, the team fails to find the keyword\n",
    "    # else the team finds the keyword if its guesser's skill is higher than a random number\n",
    "    # (beware of the reversed logic in the code)\n",
    "    ranks = [\n",
    "        1 if teamA[0].skill * teamA[1].skill == 0 or random.random() > teamA[0].skill else 0,\n",
    "        1 if teamB[0].skill * teamB[1].skill == 0 or random.random() > teamB[0].skill else 0\n",
    "    ]\n",
    "    \n",
    "    # new ratings are the results of the game\n",
    "    rating_function = rate if env is None else env.rate\n",
    "    new_ratings = rating_function(\n",
    "        [\n",
    "            [ p.rating for p in teamA ],\n",
    "            [ p.rating for p in teamB ]\n",
    "        ],\n",
    "        ranks\n",
    "    )\n",
    "    \n",
    "    # update all players' ratings\n",
    "    for p,r in zip(teamA, new_ratings[0]):\n",
    "        p.update(r)\n",
    "    for p,r in zip(teamB, new_ratings[1]):\n",
    "        p.update(r)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"Game outcome :\", ranks)\n",
    "        print(\"New ratings :\", new_ratings)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d01de1f",
   "metadata": {},
   "source": [
    "And voilà !  We have everything we need to start simulating the competition.  Here is a first game :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.396542Z",
     "iopub.status.busy": "2024-07-20T13:34:55.396091Z",
     "iopub.status.idle": "2024-07-20T13:34:55.414912Z",
     "shell.execute_reply": "2024-07-20T13:34:55.413579Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.396513Z"
    }
   },
   "outputs": [],
   "source": [
    "play_match(\n",
    "    [players[0], players[1]],\n",
    "    [players[2], players[3]],\n",
    "    debug = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2af308d",
   "metadata": {},
   "source": [
    "# Make your own competition simulation\n",
    "\n",
    "For convenience and the ability to test different settings easily, we will define a competition class where we can parametrize the number and skills of players, the number of games and the TrueSkill settings, and which provides a dataframe with the ratings history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.416911Z",
     "iopub.status.busy": "2024-07-20T13:34:55.416425Z",
     "iopub.status.idle": "2024-07-20T13:34:55.429916Z",
     "shell.execute_reply": "2024-07-20T13:34:55.428701Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.416872Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.43225Z",
     "iopub.status.busy": "2024-07-20T13:34:55.431818Z",
     "iopub.status.idle": "2024-07-20T13:34:55.446402Z",
     "shell.execute_reply": "2024-07-20T13:34:55.445037Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.432152Z"
    }
   },
   "outputs": [],
   "source": [
    "class Competition:\n",
    "    \"\"\"\n",
    "    A configurable TrueSkill competition environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        skill_dist: List[float],\n",
    "        nb_dummy: int,\n",
    "        mu: float = 600,\n",
    "        sigma: float = 300,\n",
    "        beta: float = 150,\n",
    "        tau: float = 3,\n",
    "        draw_probability: float = 0.9956,\n",
    "        seed: float = 42\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "            skill_dist: List[float]\n",
    "                The probability to find the keyword, for each \"good\" submission.\n",
    "            nb_dummy: int\n",
    "                The number of dummy bots\n",
    "            mu: float\n",
    "                The TrueSkill mu parameter.\n",
    "            sigma: float\n",
    "                The TrueSkill sigma parameter.\n",
    "            beta: float\n",
    "                The TrueSkill beta parameter.\n",
    "            tau: float\n",
    "                The TrueSkill tau parameter.\n",
    "            draw_probability: float\n",
    "                The TrueSkill draw_probability parameter.\n",
    "            seed: float\n",
    "                A random seed.\n",
    "        \"\"\"\n",
    "        # our competition environment\n",
    "        self._trueskill = TrueSkill(\n",
    "            mu = mu,\n",
    "            sigma = sigma,\n",
    "            beta = beta,\n",
    "            tau = tau,\n",
    "            draw_probability = draw_probability\n",
    "        )\n",
    "        \n",
    "        # our pool of players — and that's why we needed an \"env\" parameters for the Player constructor\n",
    "        self.players = { ii: Player(s, self._trueskill) for ii,s in enumerate(skill_dist) }\n",
    "        self.players.update({ len(self.players)+ii: Player(0, self._trueskill) for ii in range(nb_dummy) })\n",
    "        \n",
    "        # the random seed\n",
    "        self._seed = seed\n",
    "        random.seed(seed)\n",
    "        \n",
    "        \n",
    "    def play_games(self, nb_games: int = 10000):\n",
    "        \"\"\"\n",
    "        Simulate games and update the players history.\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "            nb_games: int\n",
    "                The number of games to play.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in tqdm(range(nb_games)):\n",
    "            player_ids = random.sample(range(len(self.players)), 4)  # let's pick 4 players randomly\n",
    "            try:\n",
    "                play_match(\n",
    "                    [\n",
    "                        self.players[player_ids[0]],\n",
    "                        self.players[player_ids[1]]\n",
    "                    ],\n",
    "                    [\n",
    "                        self.players[player_ids[2]],\n",
    "                        self.players[player_ids[3]]\n",
    "                    ],\n",
    "                    env = self._trueskill\n",
    "                )\n",
    "            except:\n",
    "                continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333a5939",
   "metadata": {},
   "source": [
    "Let's recover the skill distribution we already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:34:55.448291Z",
     "iopub.status.busy": "2024-07-20T13:34:55.447811Z",
     "iopub.status.idle": "2024-07-20T13:34:55.467925Z",
     "shell.execute_reply": "2024-07-20T13:34:55.466582Z",
     "shell.execute_reply.started": "2024-07-20T13:34:55.448256Z"
    }
   },
   "outputs": [],
   "source": [
    "skill_dist = skills_df[\"skill\"].values\n",
    "nb_dummy = len(players) - len(skill_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:00:12.375277Z",
     "iopub.status.busy": "2024-07-20T11:00:12.374543Z",
     "iopub.status.idle": "2024-07-20T11:00:12.383547Z",
     "shell.execute_reply": "2024-07-20T11:00:12.382319Z",
     "shell.execute_reply.started": "2024-07-20T11:00:12.37524Z"
    }
   },
   "outputs": [],
   "source": [
    "# We'll stick to the LLM20 guessed settings, but you can define several competitions to compare them\n",
    "LLM20 = Competition(skill_dist, nb_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:00:12.634531Z",
     "iopub.status.busy": "2024-07-20T11:00:12.634111Z",
     "iopub.status.idle": "2024-07-20T11:01:02.358646Z",
     "shell.execute_reply": "2024-07-20T11:01:02.357216Z",
     "shell.execute_reply.started": "2024-07-20T11:00:12.634496Z"
    }
   },
   "outputs": [],
   "source": [
    "# Expect ~1700 games per second on Kaggle's systems\n",
    "# Note that the history is not reset, the games add to the previous ones\n",
    "LLM20.play_games(100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35195405",
   "metadata": {},
   "source": [
    "# Plots\n",
    "\n",
    "Before diving into the results of this simulation, let's keep in mind the distribution of the actual number of games played by each submission in the real world, after two months of competition :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:02.360863Z",
     "iopub.status.busy": "2024-07-20T11:01:02.360503Z",
     "iopub.status.idle": "2024-07-20T11:01:02.735353Z",
     "shell.execute_reply": "2024-07-20T11:01:02.734131Z",
     "shell.execute_reply.started": "2024-07-20T11:01:02.360832Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "last_games_df.groupby(\"guesser_SubmissionId\").size().hist(bins=15)\n",
    "_ = plt.title(\"Actual distribution of games played\")\n",
    "_ = plt.xlabel(\"Number of games played\")\n",
    "_ = plt.ylabel(\"Number of bots\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:02.737202Z",
     "iopub.status.busy": "2024-07-20T11:01:02.736784Z",
     "iopub.status.idle": "2024-07-20T11:01:02.753105Z",
     "shell.execute_reply": "2024-07-20T11:01:02.751637Z",
     "shell.execute_reply.started": "2024-07-20T11:01:02.737169Z"
    }
   },
   "outputs": [],
   "source": [
    "last_games_df[\"guesser_SubmissionId\"].nunique(), (last_games_df.groupby(\"guesser_SubmissionId\").size()>100).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c92bac4",
   "metadata": {},
   "source": [
    "Some bots played much more games as others because they're much older.  Actually, about half bots played less than 100 games, while some others played more than 500 !  This is an important point that we will just ignore further on, but it's good to have it in mind when comparing to the actual competition.\n",
    "\n",
    "Now let's make some plots of our simulation to see how this ranking system performs in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T10:26:16.930027Z",
     "iopub.status.busy": "2024-07-20T10:26:16.929703Z",
     "iopub.status.idle": "2024-07-20T10:26:19.294599Z",
     "shell.execute_reply": "2024-07-20T10:26:19.29345Z",
     "shell.execute_reply.started": "2024-07-20T10:26:16.929999Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,8))\n",
    "cmap = plt.get_cmap('viridis')\n",
    "for i in range(len(LLM20.players))[::-1]:  # players sorted by increasing skill, to get the best on top\n",
    "    pskill = LLM20.players[i].skill\n",
    "    plt.plot(\n",
    "        [r.mu for r in LLM20.players[i].history],\n",
    "        c = cmap(pskill) if pskill>0 else [.6]*4  # bad bots in grey\n",
    "    )\n",
    "_ = plt.title(\"Mu evolution\")\n",
    "_ = plt.xlabel(\"Number of games played\")\n",
    "_ = plt.ylabel(\"TrueSkill's mu value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f96f5e",
   "metadata": {},
   "source": [
    "The above plot shows that although there is *a trend*, the system fails to distinguish players according to their skill, even after a large number of games.  The gray lines correspond to dummy bots.\n",
    "\n",
    "Let's have a closer look at the behaviour of the top 10 simulated players :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:40.674394Z",
     "iopub.status.busy": "2024-07-20T11:01:40.673982Z",
     "iopub.status.idle": "2024-07-20T11:01:41.190297Z",
     "shell.execute_reply": "2024-07-20T11:01:41.189228Z",
     "shell.execute_reply.started": "2024-07-20T11:01:40.674364Z"
    }
   },
   "outputs": [],
   "source": [
    "NFIRSTS = 10\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "cmap = plt.get_cmap('tab10')\n",
    "for i in range(NFIRSTS):\n",
    "    pskill = LLM20.players[i].skill\n",
    "    plt.plot(\n",
    "        [r.mu for r in LLM20.players[i].history],\n",
    "        c = cmap(i),\n",
    "        label = f\"skill = {pskill:.2f}\"\n",
    "    )\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Mu evolution of top 10 (simulated) players\")\n",
    "_ = plt.xlabel(\"Number of games played\")\n",
    "_ = plt.ylabel(\"TrueSkill's mu value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f948c60",
   "metadata": {},
   "source": [
    "The best players are struggling to keep up with poorer ones…\n",
    "\n",
    "It may be interesting to compare the evolution of players with the same skill.  We could set a specific competition environment to test that, but I'm more interesting in keeping the most realistic scenario, so let's just check among our current pool of players :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:55.170965Z",
     "iopub.status.busy": "2024-07-20T11:01:55.170558Z",
     "iopub.status.idle": "2024-07-20T11:01:55.176009Z",
     "shell.execute_reply": "2024-07-20T11:01:55.17492Z",
     "shell.execute_reply.started": "2024-07-20T11:01:55.170931Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:55.360164Z",
     "iopub.status.busy": "2024-07-20T11:01:55.359355Z",
     "iopub.status.idle": "2024-07-20T11:01:55.367155Z",
     "shell.execute_reply": "2024-07-20T11:01:55.365919Z",
     "shell.execute_reply.started": "2024-07-20T11:01:55.360118Z"
    }
   },
   "outputs": [],
   "source": [
    "duplicated_skills = Counter(skill_dist[:15]).most_common(2)\n",
    "duplicated_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0356c82e",
   "metadata": {},
   "source": [
    "We have two couples of interesting candidates among the 15 best players.  Let's check how they performed over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:55.729569Z",
     "iopub.status.busy": "2024-07-20T11:01:55.729165Z",
     "iopub.status.idle": "2024-07-20T11:01:56.143681Z",
     "shell.execute_reply": "2024-07-20T11:01:56.142498Z",
     "shell.execute_reply.started": "2024-07-20T11:01:55.729538Z"
    }
   },
   "outputs": [],
   "source": [
    "SHOW_ALL = True\n",
    "fig = plt.figure(figsize=(16,8))\n",
    "cmap = plt.get_cmap('tab10')\n",
    "gray = [.05] * 4\n",
    "if SHOW_ALL:\n",
    "    for i in range(len(LLM20.players)):\n",
    "        plt.plot(\n",
    "            [r.mu for r in LLM20.players[i].history],\n",
    "            c = gray\n",
    "        )\n",
    "for i in range(20):\n",
    "    pskill = LLM20.players[i].skill\n",
    "    if pskill == duplicated_skills[0][0]:\n",
    "        color = cmap(0)\n",
    "    elif pskill == duplicated_skills[1][0]:\n",
    "        color = cmap(1)\n",
    "    else:\n",
    "        continue\n",
    "    plt.plot(\n",
    "        [r.mu for r in LLM20.players[i].history],\n",
    "        c = color,\n",
    "        label = f\"skill = {pskill:.2f}\"\n",
    "    )\n",
    "_ = plt.legend()\n",
    "_ = plt.title(\"Mu evolution of players with identical skills\")\n",
    "_ = plt.xlabel(\"Number of games played\")\n",
    "_ = plt.ylabel(\"TrueSkill's mu value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f1bf23",
   "metadata": {},
   "source": [
    "Although there are some less/more skilled competitors inbetween, hat's not that bad, considering that @c-number, who owns currently the first place of the actual leaderboard, reported that their three identical submissions [were spread on 60 positions](https://www.kaggle.com/competitions/llm-20-questions/discussion/520928), with a score varying from $\\mu=1143$ to… $\\mu=767$ !!!\n",
    "\n",
    "# Comparing with the leaderboard\n",
    "\n",
    "Let's check how our simulated leaderboard compares with the actual one :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:01:02.756138Z",
     "iopub.status.busy": "2024-07-20T11:01:02.75566Z",
     "iopub.status.idle": "2024-07-20T11:01:02.781335Z",
     "shell.execute_reply": "2024-07-20T11:01:02.780094Z",
     "shell.execute_reply.started": "2024-07-20T11:01:02.756092Z"
    }
   },
   "outputs": [],
   "source": [
    "lb = pd.read_csv(\"/kaggle/input/llm-20-lb-2024-07-14/llm-20-questions-publicleaderboard-2024-07-14.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:17:08.830993Z",
     "iopub.status.busy": "2024-07-20T11:17:08.830619Z",
     "iopub.status.idle": "2024-07-20T11:17:09.887207Z",
     "shell.execute_reply": "2024-07-20T11:17:09.886084Z",
     "shell.execute_reply.started": "2024-07-20T11:17:08.830964Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "_ = plt.hist(\n",
    "    lb.Score,\n",
    "    bins = 100,\n",
    "    range = (300, 1200),\n",
    "    log = True,\n",
    "    label = \"Leaderboard\"\n",
    ")\n",
    "_ = plt.hist(\n",
    "    [ p.history[-1].mu for p in LLM20.players.values() ],\n",
    "    bins = 100,\n",
    "    range = (300, 1200),\n",
    "    alpha = .5,\n",
    "    log = True,\n",
    "    label = \"Simulation\"\n",
    ")\n",
    "_ = plt.title(\"Played games corrected leaderboard distribution\")\n",
    "_ = plt.xlabel(\"TrueSkill mu\")\n",
    "_ = plt.ylabel(\"Number of bots\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811bf9ce",
   "metadata": {},
   "source": [
    "That's far from perfect, but not too bad either.  Part of the differences can be explained by the different number of games played, but it's not enough, see the corrected comparison :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:17:17.975148Z",
     "iopub.status.busy": "2024-07-20T11:17:17.974713Z",
     "iopub.status.idle": "2024-07-20T11:17:19.036353Z",
     "shell.execute_reply": "2024-07-20T11:17:19.035282Z",
     "shell.execute_reply.started": "2024-07-20T11:17:17.975114Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "played_dist = last_games_df.groupby(\"guesser_SubmissionId\").size().values\n",
    "_ = plt.hist(\n",
    "    lb.Score,\n",
    "    bins = 100,\n",
    "    range = (300, 1200),\n",
    "    log = True,\n",
    "    label = \"Leaderboard\"\n",
    ")\n",
    "_ = plt.hist(\n",
    "    [\n",
    "        p.history[min(len(p.history)-1, random.choice(played_dist))].mu  # we pick the \"last game\" number from the played games distribution\n",
    "        for p in LLM20.players.values()\n",
    "    ],\n",
    "    bins = 100,\n",
    "    range = (300, 1200),\n",
    "    alpha = .5,\n",
    "    log = True,\n",
    "    label = \"Simulation\"\n",
    ")\n",
    "_ = plt.title(\"Played games corrected leaderboard distribution\")\n",
    "_ = plt.xlabel(\"TrueSkill mu\")\n",
    "_ = plt.ylabel(\"Number of bots\")\n",
    "_ =  plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080015a3",
   "metadata": {},
   "source": [
    "Another point is the skill distribution being shifted towards higher values, but again it doesn't suffice to explain the difference, so I think maybe the value we set for $\\beta$ or $\\tau$ may be off somehow.  There is also the possibility that the parameters were adjusted along the competition.\n",
    "\n",
    "Anyway, from my tests I strongly doubt it would change the outcome of this simulation.\n",
    "\n",
    "# Comparing with the top10 games history\n",
    "\n",
    "I manually scrapped the game history of the leaderboard's top 10 positions and processed the data into a dictionary which can be found as a pickle in the [LLM-20-top10-LB-matches](https://www.kaggle.com/datasets/gguillard/llm-20-top10-lb-matches) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:39:56.944864Z",
     "iopub.status.busy": "2024-07-20T11:39:56.943904Z",
     "iopub.status.idle": "2024-07-20T11:39:56.956323Z",
     "shell.execute_reply": "2024-07-20T11:39:56.955114Z",
     "shell.execute_reply.started": "2024-07-20T11:39:56.944823Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"/kaggle/input/llm-20-top10-lb-matches/matches.pickle\", \"rb\") as f:\n",
    "    top10_games_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d83b7e",
   "metadata": {},
   "source": [
    "How does their evolution compares with our simulation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:43:42.024192Z",
     "iopub.status.busy": "2024-07-20T11:43:42.023771Z",
     "iopub.status.idle": "2024-07-20T11:43:46.760287Z",
     "shell.execute_reply": "2024-07-20T11:43:46.759072Z",
     "shell.execute_reply.started": "2024-07-20T11:43:42.02416Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "gray = [.05] * 4\n",
    "if SHOW_ALL:\n",
    "    for i in range(len(LLM20.players)):\n",
    "        plt.plot(\n",
    "            [r.mu for r in LLM20.players[i].history[:200]],\n",
    "            c = gray\n",
    "        )\n",
    "for kaggler, df in top10_games_dict.items():\n",
    "    _ = plt.plot(df[kaggler], label = kaggler)\n",
    "_ = plt.title(\"Top 10 leaderboard mu evolution (background = simulation)\")\n",
    "_ = plt.xlabel(\"Number of games played\")\n",
    "_ = plt.ylabel(\"TrueSkill's mu value\")\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96205ee8",
   "metadata": {},
   "source": [
    "The point of this plot is just to show that the leaderboard behaviour is on par with our simulation.  Note the impressive remontada from SpiralTip…\n",
    "\n",
    "# Team matching\n",
    "\n",
    "There is an important point that we didn't take into account in all this process.  Quoting the competition overview : \n",
    "\n",
    "> Each submission will play episodes (games) against other bots on the leaderboard that have a similar skill rating.\n",
    "\n",
    "AFAICS, the TrueSkill library doesn't provide a convenient function to draw players according to their skill rating.  Is it worth bothering about this point ?  Since we have the top 10 players games history, we can check how they were paired along their $\\mu$ evolution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T11:52:33.744718Z",
     "iopub.status.busy": "2024-07-20T11:52:33.744306Z",
     "iopub.status.idle": "2024-07-20T11:52:34.72913Z",
     "shell.execute_reply": "2024-07-20T11:52:34.727955Z",
     "shell.execute_reply.started": "2024-07-20T11:52:33.744682Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "for kaggler, df in top10_games_dict.items():\n",
    "    _ = plt.scatter(df[kaggler], df[\"Op1\"], s = 1, label = kaggler)\n",
    "    _ = plt.scatter(df[kaggler], df[\"Op2\"], s = 1)\n",
    "    _ = plt.scatter(df[kaggler], df[\"Op3\"], s = 1)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1528eedb",
   "metadata": {},
   "source": [
    "Although there is some trend, it is not obvious how to infer a simulation function from that.  Furthermore, since the high scoring bots can still be paired with dummy bots, their ratings can still be degraded (although it's somehow mitigated by their smaller $\\sigma$).\n",
    "\n",
    "But most of all, this doesn't address (and on the contrary enforce) the \"[pit of dumbness](https://www.kaggle.com/competitions/llm-20-questions/discussion/514628#2889458)\" issue, as @loh-maa named it very acurately.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "Many things are probably missing in this simulation.  In particular, the fact that there are new submissions over time, the fact that the game frequency decreases over time for each player (maybe in relation to sigma ?), or the fact that to a certain extent, players are matched against players of a similar rank.  I don't think any of these \"feature\" would change my conclusions, though.  It is clear to me that in its current state, the ranking system of the LLM 20 Questions competition gives more room to chance than to performance.\n",
    "\n",
    "# Playground\n",
    "\n",
    "If you wanna just simulate a TrueSkill environment without bothering (re)reading everything above, here is everything you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:07:21.162116Z",
     "iopub.status.busy": "2024-07-20T12:07:21.161724Z",
     "iopub.status.idle": "2024-07-20T12:07:38.260507Z",
     "shell.execute_reply": "2024-07-20T12:07:38.259054Z",
     "shell.execute_reply.started": "2024-07-20T12:07:21.162083Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install trueskill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:14:02.066241Z",
     "iopub.status.busy": "2024-07-20T12:14:02.065836Z",
     "iopub.status.idle": "2024-07-20T12:14:02.071945Z",
     "shell.execute_reply": "2024-07-20T12:14:02.070471Z",
     "shell.execute_reply.started": "2024-07-20T12:14:02.066209Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import List, Optional, Tuple\n",
    "import warnings\n",
    "\n",
    "from tqdm import tqdm\n",
    "from trueskill import TrueSkill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:08:22.91245Z",
     "iopub.status.busy": "2024-07-20T12:08:22.912031Z",
     "iopub.status.idle": "2024-07-20T12:08:22.919896Z",
     "shell.execute_reply": "2024-07-20T12:08:22.918457Z",
     "shell.execute_reply.started": "2024-07-20T12:08:22.912417Z"
    }
   },
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(\n",
    "        self,\n",
    "        skill: float,\n",
    "        env: Optional[TrueSkill] = None  # ignore this for now, this will be useful later\n",
    "    ):\n",
    "        self.skill = skill\n",
    "        self.rating = Rating() if env is None else env.Rating()  # start with environment default values, i.e. (600,300)\n",
    "        self.history = []\n",
    "        \n",
    "    def update(self, new_rating):\n",
    "        self.history.append(self.rating)\n",
    "        self.rating = new_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:15:42.456648Z",
     "iopub.status.busy": "2024-07-20T12:15:42.456207Z",
     "iopub.status.idle": "2024-07-20T12:15:42.467776Z",
     "shell.execute_reply": "2024-07-20T12:15:42.466419Z",
     "shell.execute_reply.started": "2024-07-20T12:15:42.456615Z"
    }
   },
   "outputs": [],
   "source": [
    "def play_match(\n",
    "    teamA: Tuple[Player, Player],\n",
    "    teamB: Tuple[Player, Player],\n",
    "    env: Optional[TrueSkill],\n",
    "    debug: bool = False\n",
    "):\n",
    "    \"\"\"\n",
    "    Plays a match between two teams.\n",
    "    \n",
    "    Params\n",
    "    ------\n",
    "        teamA: Tuple[Player, Player]\n",
    "            A team of two players.\n",
    "        teamB: Tuple[Player, Player]\n",
    "            Another team of two players.\n",
    "        debug: boolean\n",
    "            A flag for debugging.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(\"Players skills :\", [p.skill for p in teamA+teamB])\n",
    "        print(\"Players ratings :\", [tuple(p.rating for p in teamA), tuple(p.rating for p in teamB)])\n",
    "    # for each team, if any player is a dummy bot, the team fails to find the keyword\n",
    "    # else the team finds the keyword if its guesser's skill is higher than a random number\n",
    "    # (beware of the reversed logic in the code)\n",
    "    ranks = [\n",
    "        1 if teamA[0].skill * teamA[1].skill == 0 or random.random() > teamA[0].skill else 0,\n",
    "        1 if teamB[0].skill * teamB[1].skill == 0 or random.random() > teamB[0].skill else 0\n",
    "    ]\n",
    "    \n",
    "    # new ratings are the results of the game\n",
    "    new_ratings = env.rate(\n",
    "        [\n",
    "            [ p.rating for p in teamA ],\n",
    "            [ p.rating for p in teamB ]\n",
    "        ],\n",
    "        ranks\n",
    "    )\n",
    "    \n",
    "    # update all players' ratings\n",
    "    for p,r in zip(teamA, new_ratings[0]):\n",
    "        p.update(r)\n",
    "    for p,r in zip(teamB, new_ratings[1]):\n",
    "        p.update(r)\n",
    "        \n",
    "    if debug:\n",
    "        print(\"Game outcome :\", ranks)\n",
    "        print(\"New ratings :\", new_ratings)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:16:34.352375Z",
     "iopub.status.busy": "2024-07-20T12:16:34.351231Z",
     "iopub.status.idle": "2024-07-20T12:16:34.364019Z",
     "shell.execute_reply": "2024-07-20T12:16:34.362645Z",
     "shell.execute_reply.started": "2024-07-20T12:16:34.352308Z"
    }
   },
   "outputs": [],
   "source": [
    "class Competition:\n",
    "    \"\"\"\n",
    "    A configurable TrueSkill competition environment.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        skill_dist: List[float],\n",
    "        nb_dummy: int,\n",
    "        mu: float = 600,\n",
    "        sigma: float = 300,\n",
    "        beta: float = 150,\n",
    "        tau: float = 3,\n",
    "        draw_probability: float = 0.9956,\n",
    "        seed: float = 42,\n",
    "        debug: bool = False\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "            skill_dist: List[float]\n",
    "                The probability to find the keyword, for each \"good\" submission.\n",
    "            nb_dummy: int\n",
    "                The number of dummy bots\n",
    "            mu: float\n",
    "                The TrueSkill mu parameter.\n",
    "            sigma: float\n",
    "                The TrueSkill sigma parameter.\n",
    "            beta: float\n",
    "                The TrueSkill beta parameter.\n",
    "            tau: float\n",
    "                The TrueSkill tau parameter.\n",
    "            draw_probability: float\n",
    "                The TrueSkill draw_probability parameter.\n",
    "            seed: float\n",
    "                A random seed.\n",
    "            debug: bool\n",
    "                A debugging flag.\n",
    "        \"\"\"\n",
    "        # our competition environment\n",
    "        self._trueskill = TrueSkill(\n",
    "            mu = mu,\n",
    "            sigma = sigma,\n",
    "            beta = beta,\n",
    "            tau = tau,\n",
    "            draw_probability = draw_probability\n",
    "        )\n",
    "        \n",
    "        # our pool of players — and that's why we needed an \"env\" parameters for the Player constructor\n",
    "        self.players = { ii: Player(s, self._trueskill) for ii,s in enumerate(skill_dist) }\n",
    "        self.players.update({ len(self.players)+ii: Player(0, self._trueskill) for ii in range(nb_dummy) })\n",
    "        \n",
    "        # the random seed\n",
    "        self._seed = seed\n",
    "        random.seed(seed)\n",
    "        \n",
    "        self.debug = debug\n",
    "        \n",
    "        \n",
    "    def play_games(self, nb_games: int = 10000):\n",
    "        \"\"\"\n",
    "        Simulate games and update the players history.\n",
    "        \n",
    "        Params\n",
    "        ------\n",
    "            nb_games: int\n",
    "                The number of games to play.\n",
    "        \"\"\"\n",
    "        \n",
    "        for i in tqdm(range(nb_games)):\n",
    "            player_ids = random.sample(range(len(self.players)), 4)  # let's pick 4 players randomly\n",
    "            try:\n",
    "                play_match(\n",
    "                    [\n",
    "                        self.players[player_ids[0]],\n",
    "                        self.players[player_ids[1]]\n",
    "                    ],\n",
    "                    [\n",
    "                        self.players[player_ids[2]],\n",
    "                        self.players[player_ids[3]]\n",
    "                    ],\n",
    "                    env = self._trueskill,\n",
    "                    debug = self.debug\n",
    "                )\n",
    "            except Exception as e:\n",
    "                warnings.warn(f\"Caught exception {e}\")\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:16:36.62176Z",
     "iopub.status.busy": "2024-07-20T12:16:36.62138Z",
     "iopub.status.idle": "2024-07-20T12:16:36.627307Z",
     "shell.execute_reply": "2024-07-20T12:16:36.626122Z",
     "shell.execute_reply.started": "2024-07-20T12:16:36.621731Z"
    }
   },
   "outputs": [],
   "source": [
    "nb_players = 10  # number of actual players\n",
    "nb_dummy_players = 10  # number of dummy (\"bad\") bots\n",
    "skill_distribution = [ random.random() for i in range(nb_players)]  # skill distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:16:37.677152Z",
     "iopub.status.busy": "2024-07-20T12:16:37.676795Z",
     "iopub.status.idle": "2024-07-20T12:16:37.683028Z",
     "shell.execute_reply": "2024-07-20T12:16:37.68168Z",
     "shell.execute_reply.started": "2024-07-20T12:16:37.677126Z"
    }
   },
   "outputs": [],
   "source": [
    "# you can define several competitions to compare them\n",
    "C1 = Competition(skill_distribution, nb_dummy_players)\n",
    "C2 = Competition(skill_distribution, nb_dummy_players, mu = 500, sigma = 100, beta = 200, tau = 10, draw_probability = .8, seed = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:16:40.16207Z",
     "iopub.status.busy": "2024-07-20T12:16:40.161637Z",
     "iopub.status.idle": "2024-07-20T12:16:49.676808Z",
     "shell.execute_reply": "2024-07-20T12:16:49.675525Z",
     "shell.execute_reply.started": "2024-07-20T12:16:40.162037Z"
    }
   },
   "outputs": [],
   "source": [
    "# Expect ~1700 games per second on Kaggle's systems\n",
    "# Note that the history is not reset, the games add to the previous ones\n",
    "C1.play_games(10000)\n",
    "C2.play_games(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:17:59.132609Z",
     "iopub.status.busy": "2024-07-20T12:17:59.132182Z",
     "iopub.status.idle": "2024-07-20T12:17:59.138016Z",
     "shell.execute_reply": "2024-07-20T12:17:59.136685Z",
     "shell.execute_reply.started": "2024-07-20T12:17:59.132576Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T12:17:59.246061Z",
     "iopub.status.busy": "2024-07-20T12:17:59.24568Z",
     "iopub.status.idle": "2024-07-20T12:17:59.548567Z",
     "shell.execute_reply": "2024-07-20T12:17:59.547442Z",
     "shell.execute_reply.started": "2024-07-20T12:17:59.246033Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot([ r.mu for r in C1.players[0].history])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5e9df671",
   "metadata": {},
   "source": [
    "# Variability over seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:35:24.052666Z",
     "iopub.status.busy": "2024-07-20T13:35:24.052141Z",
     "iopub.status.idle": "2024-07-20T13:35:24.081765Z",
     "shell.execute_reply": "2024-07-20T13:35:24.080103Z",
     "shell.execute_reply.started": "2024-07-20T13:35:24.052626Z"
    }
   },
   "outputs": [],
   "source": [
    "C = [Competition(skill_dist, nb_dummy, seed=i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:45:54.361811Z",
     "iopub.status.busy": "2024-07-20T13:45:54.361392Z",
     "iopub.status.idle": "2024-07-20T13:47:11.105403Z",
     "shell.execute_reply": "2024-07-20T13:47:11.10421Z",
     "shell.execute_reply.started": "2024-07-20T13:45:54.361781Z"
    }
   },
   "outputs": [],
   "source": [
    "for comp in C:\n",
    "    comp.play_games(50000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:50:04.336765Z",
     "iopub.status.busy": "2024-07-20T13:50:04.33635Z",
     "iopub.status.idle": "2024-07-20T13:50:04.347264Z",
     "shell.execute_reply": "2024-07-20T13:50:04.345891Z",
     "shell.execute_reply.started": "2024-07-20T13:50:04.336735Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_first_n(comp, nfirsts, a):\n",
    "    #fig = plt.figure(figsize=(16,8))\n",
    "    cmap = plt.get_cmap('tab10')\n",
    "    for i in range(nfirsts):\n",
    "        pskill = comp.players[i].skill\n",
    "        a.plot(\n",
    "            [r.mu for r in comp.players[i].history],\n",
    "            c = cmap(i),\n",
    "            label = f\"skill = {pskill:.2f}\"\n",
    "        )\n",
    "    _ = a.legend()\n",
    "    _ = a.set_title(\"Mu evolution of top 10 (simulated) players\")\n",
    "    _ = a.set_xlabel(\"Number of games played\")\n",
    "    _ = a.set_ylabel(\"TrueSkill's mu value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T13:50:17.266956Z",
     "iopub.status.busy": "2024-07-20T13:50:17.266568Z",
     "iopub.status.idle": "2024-07-20T13:50:19.507685Z",
     "shell.execute_reply": "2024-07-20T13:50:19.506404Z",
     "shell.execute_reply.started": "2024-07-20T13:50:17.266928Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=4, ncols=1, figsize=(16,16), constrained_layout=True)\n",
    "for i,comp in enumerate(C):\n",
    "    #plt.axes(ax[i])\n",
    "    plot_first_n(comp, 5, ax[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T14:05:05.14683Z",
     "iopub.status.busy": "2024-07-20T14:05:05.1464Z",
     "iopub.status.idle": "2024-07-20T14:05:05.159688Z",
     "shell.execute_reply": "2024-07-20T14:05:05.158535Z",
     "shell.execute_reply.started": "2024-07-20T14:05:05.146797Z"
    }
   },
   "outputs": [],
   "source": [
    "for comp in C:\n",
    "    scores = {p: v.rating.mu for p,v in comp.players.items()}\n",
    "    leaderboard = {k:v for k,v in sorted(scores.items(), key=lambda item: item[1], reverse = True)}\n",
    "    print(\"Leaderboard\")\n",
    "    print(\"rank\\tplayer\\tskill\\tmu\")\n",
    "    for i in range(5):\n",
    "        pnum = list(leaderboard.keys())[i]\n",
    "        print(f\"{i+1}\\t{pnum}\\t{comp.players[pnum].skill:.2f}\\t{leaderboard[i]:.0f}\")\n",
    "    print(\"-------------\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "datasetId": 5400047,
     "sourceId": 8969887,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5394349,
     "sourceId": 8997086,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 188996916,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
