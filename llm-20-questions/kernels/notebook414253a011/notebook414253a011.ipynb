{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:45.176925Z",
     "iopub.status.busy": "2024-07-22T18:14:45.176479Z",
     "iopub.status.idle": "2024-07-22T18:14:45.188448Z",
     "shell.execute_reply": "2024-07-22T18:14:45.187358Z",
     "shell.execute_reply.started": "2024-07-22T18:14:45.176887Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:14:54.171348Z",
     "iopub.status.busy": "2024-07-22T18:14:54.170957Z",
     "iopub.status.idle": "2024-07-22T18:14:54.187649Z",
     "shell.execute_reply": "2024-07-22T18:14:54.186496Z",
     "shell.execute_reply.started": "2024-07-22T18:14:54.171301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile submission/main.py\n",
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# **IMPORTANT:** Set up your system path like this to make your code work\n",
    "# both in notebooks and in the simulations environment.\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    sys.path.insert(0, os.path.join(KAGGLE_AGENT_PATH, 'lib'))\n",
    "else:\n",
    "    sys.path.insert(0, \"/kaggle/working/submission/lib\")\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# https://github.com/Lightning-AI/litgpt/issues/327\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    WEIGHTS_PATH = os.path.join(KAGGLE_AGENT_PATH, \"llama-3/transformers/8b-chat-hf/1\")\n",
    "else:\n",
    "    WEIGHTS_PATH = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n",
    "\n",
    "# Prompt Formatting\n",
    "import itertools\n",
    "from typing import Iterable\n",
    "\n",
    "\n",
    "class Llama3Formatter:\n",
    "    def __init__(self, system_prompt: str = None, few_shot_examples: Iterable = None):\n",
    "        self._system_prompt = system_prompt\n",
    "        self._few_shot_examples = few_shot_examples\n",
    "        self.reset()\n",
    "\n",
    "    def get_dict(self):\n",
    "        return self._state\n",
    "\n",
    "    def user(self, prompt):\n",
    "        self._state.append({'role': 'user', 'content': prompt})\n",
    "        return self\n",
    "\n",
    "    def model(self, prompt):\n",
    "        self._state.append({'role': 'assistant', 'content': prompt})\n",
    "        return self\n",
    "    \n",
    "    def system(self, prompt):\n",
    "        self._state.append({'role': 'system', 'content': prompt})\n",
    "        return self\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = []\n",
    "        if self._system_prompt is not None:\n",
    "            self.system(self._system_prompt)\n",
    "        if self._few_shot_examples is not None:\n",
    "            self.apply_turns(self._few_shot_examples, start_agent='user')\n",
    "        return self\n",
    "\n",
    "    def apply_turns(self, turns: Iterable, start_agent: str):\n",
    "        formatters = [self.model, self.user] if start_agent == 'model' else [self.user, self.model]\n",
    "        formatters = itertools.cycle(formatters)\n",
    "        for fmt, turn in zip(formatters, turns):\n",
    "            fmt(turn)\n",
    "        return self\n",
    "\n",
    "\n",
    "# Agent Definitions\n",
    "import re\n",
    "\n",
    "class Llama3Agent:\n",
    "    def __init__(self, system_prompt=None, few_shot_examples=None):\n",
    "        self.formatter = Llama3Formatter(system_prompt=system_prompt, few_shot_examples=few_shot_examples)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)\n",
    "        self.terminators = [self.tokenizer.eos_token_id, self.tokenizer.convert_tokens_to_ids(\"\")]\n",
    "        \n",
    "        ### Load original model\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            WEIGHTS_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.bfloat16\n",
    "        )\n",
    "\n",
    "    def __call__(self, obs, *args):\n",
    "        self._start_session(obs)\n",
    "        prompt = self.formatter.get_dict()\n",
    "        response = self._call_llm(prompt)\n",
    "        response = self._parse_response(response, obs)\n",
    "        return response\n",
    "\n",
    "    def _start_session(self, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _call_llm(self, prompt, max_new_tokens=32):\n",
    "        input_ids = self.tokenizer.apply_chat_template(\n",
    "            prompt,\n",
    "            add_generation_prompt=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.model.device)\n",
    "        outputs = self.model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            eos_token_id=self.terminators,\n",
    "            do_sample=True,\n",
    "            temperature=0.6,\n",
    "            top_p=0.9\n",
    "        )\n",
    "        response = self.tokenizer.decode(outputs[0][input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _parse_keyword(self, response: str):\n",
    "        match = re.search(r\"(?<=\\*\\*)([^*]+)(?=\\*\\*)\", response)\n",
    "        if match is None:\n",
    "            keyword = ''\n",
    "        else:\n",
    "            keyword = match.group().lower()\n",
    "        return keyword\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "def interleave_unequal(x, y):\n",
    "    return [\n",
    "        item for pair in itertools.zip_longest(x, y) for item in pair if item is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "class Llama3QuestionerAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.category_determined = False\n",
    "        self.is_place = False\n",
    "        self.first_char_range = (0, 25)  # A to Z in terms of index\n",
    "        self.second_char_range = None\n",
    "        self.final_guess = None\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        global guesses\n",
    "\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(\"Let's play 20 Questions. You are playing the role of the Questioner.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='model')\n",
    "\n",
    "        if not self.category_determined:\n",
    "            self.formatter.user(\"Is it a place?\")\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)  # Convert index to alphabet (0 -> A, 1 -> B, ..., 25 -> Z)\n",
    "                self.formatter.user(f\"Does the keyword start with a letter before {mid_char}?\")\n",
    "            elif self.final_guess is None:\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                mid_char = chr(65 + mid_index)\n",
    "                self.formatter.user(f\"Does the second letter of the keyword come before {mid_char}?\")\n",
    "            else:\n",
    "                self.formatter.user(f\"Is the keyword **{self.final_guess}**?\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        if not self.category_determined:\n",
    "            answer = self._parse_keyword(response)\n",
    "            self.is_place = (answer == 'yes')\n",
    "            self.category_determined = True\n",
    "            return \"Is it a place?\"\n",
    "        else:\n",
    "            if self.second_char_range is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.first_char_range = (self.first_char_range[0], mid_index)\n",
    "                else:\n",
    "                    self.first_char_range = (mid_index + 1, self.first_char_range[1])\n",
    "\n",
    "                if self.first_char_range[0] == self.first_char_range[1]:\n",
    "                    self.second_char_range = (0, 25)  # Reset for second character\n",
    "                    return f\"Does the keyword start with {chr(65 + self.first_char_range[0])}?\"\n",
    "                else:\n",
    "                    mid_index = (self.first_char_range[0] + self.first_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the keyword start with a letter before {mid_char}?\"\n",
    "            elif self.final_guess is None:\n",
    "                answer = self._parse_keyword(response)\n",
    "                mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                if answer == 'yes':\n",
    "                    self.second_char_range = (self.second_char_range[0], mid_index)\n",
    "                else:\n",
    "                    self.second_char_range = (mid_index + 1, self.second_char_range[1])\n",
    "\n",
    "                if self.second_char_range[0] == self.second_char_range[1]:\n",
    "                    first_char = chr(65 + self.first_char_range[0])\n",
    "                    second_char = chr(65 + self.second_char_range[0])\n",
    "                    self.final_guess = first_char + second_char\n",
    "                    return f\"Does the keyword start with {first_char}{second_char}?\"\n",
    "                else:\n",
    "                    mid_index = (self.second_char_range[0] + self.second_char_range[1]) // 2\n",
    "                    mid_char = chr(65 + mid_index)\n",
    "                    return f\"Does the second letter of the keyword come before {mid_char}?\"\n",
    "            else:\n",
    "                answer = self._parse_keyword(response)\n",
    "                if answer == 'yes':\n",
    "                    return f\"The keyword is **{self.final_guess}**.\"\n",
    "                else:\n",
    "                    self.final_guess = None\n",
    "                    return \"Let's continue guessing.\"\n",
    "\n",
    "\n",
    "class Llama3AnswererAgent(Llama3Agent):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def _start_session(self, obs):\n",
    "        self.formatter.reset()\n",
    "        self.formatter.user(f\"Let's play 20 Questions. You are playing the role of the Answerer. The keyword is {obs.keyword} in the category {obs.category}.\")\n",
    "        turns = interleave_unequal(obs.questions, obs.answers)\n",
    "        self.formatter.apply_turns(turns, start_agent='user')\n",
    "        self.formatter.user(f\"The question is about the keyword {obs.keyword} in the category {obs.category}. Give yes-or-no answer and surround your answer with double asterisks, like **yes** or **no**.\")\n",
    "\n",
    "    def _parse_response(self, response: str, obs: dict):\n",
    "        answer = self._parse_keyword(response)\n",
    "        return 'yes' if 'yes' in answer else 'no'\n",
    "\n",
    "\n",
    "# Agent Creation\n",
    "system_prompt = \"You are a very smart AI assistant designed to play the 20 Questions game. In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner. The keyword is a specific place or thing.\"\n",
    "\n",
    "few_shot_examples = [\n",
    "    \"Let's play 20 Questions. You are playing the role of the Questioner. Please ask your first question.\",\n",
    "    \"Is the category of the keyword place?\", \"**no**\",\n",
    "    \"Is it a food?\", \"**yes** Now guess the keyword in the category things.\",\n",
    "    \"**Veggie Burger**\", \"Correct.\",\n",
    "]\n",
    "\n",
    "\n",
    "# **IMPORTANT:** Define agent as a global so you only have to load\n",
    "# the agent you need. Loading both will likely lead to OOM.\n",
    "agent = None\n",
    "\n",
    "\n",
    "def get_agent(name: str):\n",
    "    global agent\n",
    "    \n",
    "    if agent is None and name == 'questioner':\n",
    "        agent = Llama3QuestionerAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    elif agent is None and name == 'answerer':\n",
    "        agent = Llama3AnswererAgent(\n",
    "            system_prompt=system_prompt,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "        )\n",
    "    assert agent is not None, \"Agent not initialized.\"\n",
    "\n",
    "    return agent\n",
    "\n",
    "\n",
    "turnRound = 1\n",
    "guesses = []\n",
    "\n",
    "def agent_fn(obs, cfg):\n",
    "    global turnRound\n",
    "    global guesses\n",
    "\n",
    "    if obs.turnType == \"ask\":\n",
    "        if turnRound == 1:\n",
    "            response = \"Is it a place?\" # First question\n",
    "        else:\n",
    "            response = get_agent('questioner')(obs)\n",
    "    elif obs.turnType == \"guess\":\n",
    "        response = get_agent('questioner')(obs)\n",
    "        turnRound += 1\n",
    "        guesses.append(response)\n",
    "    elif obs.turnType == \"answer\":\n",
    "        response = get_agent('answerer')(obs)\n",
    "        turnRound += 1\n",
    "    if response is None or len(response) <= 1:\n",
    "        return \"yes\"\n",
    "    else:\n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:15.133496Z",
     "iopub.status.busy": "2024-07-22T18:15:15.132612Z",
     "iopub.status.idle": "2024-07-22T18:15:23.195997Z",
     "shell.execute_reply": "2024-07-22T18:15:23.194844Z",
     "shell.execute_reply.started": "2024-07-22T18:15:15.133456Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T18:15:24.886486Z",
     "iopub.status.busy": "2024-07-22T18:15:24.88602Z",
     "iopub.status.idle": "2024-07-22T18:21:46.738456Z",
     "shell.execute_reply": "2024-07-22T18:21:46.736412Z",
     "shell.execute_reply.started": "2024-07-22T18:15:24.886445Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/working/submission . -C /kaggle/input/ llama-3/transformers/8b-chat-hf/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T03:23:33.154199Z",
     "iopub.status.busy": "2024-07-22T03:23:33.153786Z",
     "iopub.status.idle": "2024-07-22T03:23:33.166077Z",
     "shell.execute_reply": "2024-07-22T03:23:33.164667Z",
     "shell.execute_reply.started": "2024-07-22T03:23:33.154165Z"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
