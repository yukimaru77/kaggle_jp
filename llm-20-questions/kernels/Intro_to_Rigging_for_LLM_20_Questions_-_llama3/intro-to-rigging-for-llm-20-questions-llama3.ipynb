{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d944a97",
   "metadata": {},
   "source": [
    "# LLM 20 Questions Starter with Rigging\n",
    "\n",
    "This starter notebook shows how the python package rigging can be used to create a baseline submission for the competition. This setup uses the `llama3` quantized model using vLLM.\n",
    "\n",
    "## Update **June 10, 2024**\n",
    "- Updated code to work with rigging 2.0\n",
    "- Including non-llm question asking agent that leverages the known keywords **note this won't work well on the private leaderboard**. Answer agent uses LLM via rigging.\n",
    "\n",
    "## What is Rigging?\n",
    "\n",
    "Rigging is a lightweight LLM interaction framework built on Pydantic XML. The goal is to make leveraging LLMs in production pipelines as simple and effictive as possible. Rigging is perfectly fit for the 20 questions tasks as it can:\n",
    "1. Easily handle swapping out different backend LLM models.\n",
    "2. Design LLM querying pipelines that check for expected outputs and retry until successful.\n",
    "3. Modern python with type hints, async support, pydantic validation, serialization, etc.\n",
    "\n",
    "Star the repo here: https://github.com/dreadnode/rigging\n",
    "Read the documentation here: https://rigging.dreadnode.io/\n",
    "\n",
    "Rigging is built and maintained by [dreadnode](https://www.dreadnode.io/) where we use it daily for our work.\n",
    "\n",
    "An example rigging pipeline might look like this:\n",
    "```{python}\n",
    "chat = rg.get_generator('gpt-4o') \\\n",
    "    .chat(f\"Provide me the names of all the countries in South America that start with the letter A {Answer.xml_tags()} tags.\") \\\n",
    "    .until_parsed_as(Answer) \\\n",
    "    .run() \n",
    "```\n",
    "\n",
    "Generators can be created seemlessly with most major LLM apis, so long as you have api keys saved as env variables.\n",
    "```\n",
    "export OPENAI_API_KEY=...\n",
    "export TOGETHER_API_KEY=...\n",
    "export TOGETHERAI_API_KEY=...\n",
    "export MISTRAL_API_KEY=...\n",
    "export ANTHROPIC_API_KEY=...\n",
    "```\n",
    "\n",
    "For this competition we must run our model locally, luckily rigging has support to run models using transformers on the back end.\n",
    "\n",
    "# Setup\n",
    "\n",
    "Below is some of the setup for this notebook. Where we will:\n",
    "- Load secret tokens for huggingface and kaggle (optional)\n",
    "- Install required packages\n",
    "- Create a helper utility script for testing our vLLM server\n",
    "\n",
    "This notebooks uses some hidden tokens using kaggle's secrets. This is optional and not required to run the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:44:41.417183Z",
     "iopub.status.busy": "2024-06-18T18:44:41.416506Z",
     "iopub.status.idle": "2024-06-18T18:44:41.516324Z",
     "shell.execute_reply": "2024-06-18T18:44:41.51541Z",
     "shell.execute_reply.started": "2024-06-18T18:44:41.417155Z"
    }
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "secrets = UserSecretsClient()\n",
    "\n",
    "HF_TOKEN: str | None  = None\n",
    "KAGGLE_KEY: str | None = None\n",
    "KAGGLE_USERNAME: str | None = None\n",
    "    \n",
    "try:\n",
    "    HF_TOKEN = secrets.get_secret(\"HF_TOKEN\")\n",
    "    KAGGLE_KEY = secrets.get_secret(\"KAGGLE_KEY\")\n",
    "    KAGGLE_USERNAME = secrets.get_secret(\"KAGGLE_USERNAME\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7230f5",
   "metadata": {},
   "source": [
    "## Pip install\n",
    "We will install:\n",
    "- [rigging](https://github.com/dreadnode/rigging) Used to created our LLM pipelines for the competition.\n",
    "- [vLLM](https://github.com/vllm-project/vllm) For hosting our model locally as an independent service.\n",
    "\n",
    "We also use [uv](https://github.com/astral-sh/uv) which allows us to install these packages much faster.\n",
    "\n",
    "**Note:** We are installing these packages to the `/kaggle/tmp/lib` directory. We only do this for the purposes of the competition setup, where we will later need to include the files from this path in our submission zip. We also install the vllm dependencies to `/kaggle/tmp/srvlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:44:41.518147Z",
     "iopub.status.busy": "2024-06-18T18:44:41.517895Z",
     "iopub.status.idle": "2024-06-18T18:45:50.112765Z",
     "shell.execute_reply": "2024-06-18T18:45:50.111214Z",
     "shell.execute_reply.started": "2024-06-18T18:44:41.518124Z"
    },
    "papermill": {
     "duration": 13.257308,
     "end_time": "2024-04-17T13:47:49.310664",
     "exception": false,
     "start_time": "2024-04-17T13:47:36.053356",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dependencies (uv for speed)\n",
    "!pip install uv==0.1.45\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/lib \\\n",
    "    rigging==2.0.0 \\\n",
    "    kaggle\n",
    "\n",
    "!uv pip install -U \\\n",
    "    --python $(which python) \\\n",
    "    --target /kaggle/tmp/srvlib \\\n",
    "    vllm==0.4.2 \\\n",
    "    numpy==1.26.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1ea1",
   "metadata": {},
   "source": [
    "# Download the LLM Locally\n",
    "\n",
    "Because this competition requires us to submit our code with model weights, we will first download the model weights using `snapshot_download` from huggingface.\n",
    "\n",
    "We are going to download the `solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ`. This is a Activation-aware Weight Quantization version of the model that is small enough to run in the competition requirements.\n",
    "\n",
    "**Note**: When using rigging in a normal situation this step would not be necessary, but we are downloading the weights seperately so that we can include them in our submission zip for the competition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-06-18T18:45:50.119389Z",
     "iopub.status.busy": "2024-06-18T18:45:50.119071Z",
     "iopub.status.idle": "2024-06-18T18:46:23.55105Z",
     "shell.execute_reply": "2024-06-18T18:46:23.550098Z",
     "shell.execute_reply.started": "2024-06-18T18:45:50.11936Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download the model\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "if g_model_path.exists():\n",
    "    shutil.rmtree(g_model_path)\n",
    "g_model_path.mkdir(parents=True)\n",
    "\n",
    "snapshot_download(\n",
    "    repo_id=\"solidrust/Meta-Llama-3-8B-Instruct-hf-AWQ\",\n",
    "    ignore_patterns=\"original*\",\n",
    "    local_dir=g_model_path,\n",
    "    local_dir_use_symlinks=False,\n",
    "    token=globals().get(\"HF_TOKEN\", None)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da84620b",
   "metadata": {},
   "source": [
    "We can see the model weights are stored in `/kaggle/tmp/model/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:26.312229Z",
     "iopub.status.busy": "2024-06-18T18:46:26.311603Z",
     "iopub.status.idle": "2024-06-18T18:46:27.343572Z",
     "shell.execute_reply": "2024-06-18T18:46:27.342399Z",
     "shell.execute_reply.started": "2024-06-18T18:46:26.312197Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -l /kaggle/tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7218b37",
   "metadata": {},
   "source": [
    "# Helper Utilities File\n",
    "\n",
    "These are helper functions we will use for starting our vLLM server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:28.861984Z",
     "iopub.status.busy": "2024-06-18T18:46:28.861242Z",
     "iopub.status.idle": "2024-06-18T18:46:34.72919Z",
     "shell.execute_reply": "2024-06-18T18:46:34.728213Z",
     "shell.execute_reply.started": "2024-06-18T18:46:28.861946Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile util.py\n",
    "\n",
    "# Helpers for starting the vLLM server\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "import socket\n",
    "import time\n",
    "\n",
    "def check_port(port: int) -> bool:\n",
    "    try:\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            sock.settimeout(1)\n",
    "            result = sock.connect_ex(('localhost', port))\n",
    "            if result == 0:\n",
    "                return True\n",
    "    except socket.error:\n",
    "        pass\n",
    "    \n",
    "    return False\n",
    "\n",
    "def run_and_wait_for_port(\n",
    "    cmd: list[str], port: int, env: dict[str, str] | None, timeout: int = 60, debug: bool = False\n",
    ") -> subprocess.Popen:\n",
    "    \n",
    "    if check_port(port):\n",
    "        raise ValueError(f\"Port {port} is already open\")\n",
    "        \n",
    "    popen = subprocess.Popen(\n",
    "        cmd,\n",
    "        env={**os.environ, **(env or {})},\n",
    "        stdout=subprocess.DEVNULL if not debug else None,\n",
    "        stderr=subprocess.DEVNULL if not debug else None,\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    while time.time() - start_time < timeout:\n",
    "        if check_port(port):\n",
    "            return popen\n",
    "        time.sleep(1)\n",
    "    \n",
    "    popen.terminate()\n",
    "    raise Exception(f\"Process did not open port {port} within {timeout} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2cf846",
   "metadata": {},
   "source": [
    "# Starting up our vLLM server for testing\n",
    "\n",
    "Our model will be hosted using a vLLM server. Below we will start up the notebook so we can understand how it works in the kaggle environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:34.731502Z",
     "iopub.status.busy": "2024-06-18T18:46:34.731125Z",
     "iopub.status.idle": "2024-06-18T18:46:34.740817Z",
     "shell.execute_reply": "2024-06-18T18:46:34.739992Z",
     "shell.execute_reply.started": "2024-06-18T18:46:34.73147Z"
    }
   },
   "outputs": [],
   "source": [
    "# vLLM paths and settings.\n",
    "\n",
    "import importlib\n",
    "from pathlib import Path\n",
    "import util\n",
    "\n",
    "util = importlib.reload(util)\n",
    "\n",
    "g_srvlib_path = Path(\"/kaggle/tmp/srvlib\")\n",
    "assert g_srvlib_path.exists()\n",
    "\n",
    "g_model_path = Path(\"/kaggle/tmp/model\")\n",
    "assert g_model_path.exists()\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:46:36.925703Z",
     "iopub.status.busy": "2024-06-18T18:46:36.925325Z",
     "iopub.status.idle": "2024-06-18T18:47:02.966755Z",
     "shell.execute_reply": "2024-06-18T18:47:02.965667Z",
     "shell.execute_reply.started": "2024-06-18T18:46:36.925673Z"
    }
   },
   "outputs": [],
   "source": [
    "# Run the vLLM server using subprocess\n",
    "vllm = util.run_and_wait_for_port([\n",
    "    \"python\", \"-m\",\n",
    "    \"vllm.entrypoints.openai.api_server\",\n",
    "    \"--enforce-eager\",\n",
    "    \"--model\", str(g_model_path),\n",
    "    \"--port\", str(g_vllm_port),\n",
    "    \"--served-model-name\", g_vllm_model_name\n",
    "],\n",
    "    g_vllm_port,\n",
    "    {\"PYTHONPATH\": str(g_srvlib_path)},\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "print(\"vLLM Started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39d4fde",
   "metadata": {},
   "source": [
    "We can see that the llama3 model is loaded onto the 1st Tesla T4 GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:02.968876Z",
     "iopub.status.busy": "2024-06-18T18:47:02.968472Z",
     "iopub.status.idle": "2024-06-18T18:47:04.01652Z",
     "shell.execute_reply": "2024-06-18T18:47:04.01532Z",
     "shell.execute_reply.started": "2024-06-18T18:47:02.968842Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68c89a",
   "metadata": {},
   "source": [
    "## Validating the Model\n",
    "\n",
    "Lets create our first rigging generator. In rigging the generators are the foundation for creating powerful LLM pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:07.297877Z",
     "iopub.status.busy": "2024-06-18T18:47:07.297084Z",
     "iopub.status.idle": "2024-06-18T18:47:15.064385Z",
     "shell.execute_reply": "2024-06-18T18:47:15.06343Z",
     "shell.execute_reply.started": "2024-06-18T18:47:07.297837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Connect with Rigging\n",
    "\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "sys.path.insert(0, \"/kaggle/tmp/lib\")\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "import rigging as rg\n",
    "\n",
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    ")\n",
    "\n",
    "answer = await generator.chat(\"Say Hello!\").run()\n",
    "\n",
    "print()\n",
    "print('[Rigging Chat]')\n",
    "print(type(answer), answer)\n",
    "\n",
    "print()\n",
    "print('[LLM Response Only]')\n",
    "print(type(answer.last), answer.last)\n",
    "\n",
    "print()\n",
    "answer_string = answer.last.content\n",
    "print('[LLM Response as a String]')\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8569ec",
   "metadata": {},
   "source": [
    "## Converting results to pandas dataframe\n",
    "\n",
    "Using the `to_df()` method we can easily convert the chat history to a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.065705Z",
     "iopub.status.busy": "2024-06-18T18:47:15.065434Z",
     "iopub.status.idle": "2024-06-18T18:47:15.110363Z",
     "shell.execute_reply": "2024-06-18T18:47:15.109471Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.065681Z"
    }
   },
   "outputs": [],
   "source": [
    "answer.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bbc35a",
   "metadata": {},
   "source": [
    "## Changing Model Parameters\n",
    "\n",
    "Much like database connection strings, Rigging generators can be represented as strings which define what provider, model, API key, generation params, etc. should be used. They are formatted as follows:\n",
    "\n",
    "```\n",
    "<provider>!<model>,<**kwargs>\n",
    "```\n",
    "\n",
    "As an example, here we load the model with additional parameters:\n",
    "- temperature=0.9\n",
    "- max_tokens=512\n",
    "\n",
    "You can read more about these in the docs here: https://rigging.dreadnode.io/topics/generators/#overload-generation-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.111849Z",
     "iopub.status.busy": "2024-06-18T18:47:15.111525Z",
     "iopub.status.idle": "2024-06-18T18:47:15.221209Z",
     "shell.execute_reply": "2024-06-18T18:47:15.220415Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.111825Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"temperature=0.9,max_tokens=512,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c77e58",
   "metadata": {},
   "source": [
    "Alternatively we can set these parameters using the `rg.GenerateParams` class. This class allows you to set various model parameters:\n",
    "\n",
    "```\n",
    "rg.GenerateParams(\n",
    "    *,\n",
    "    temperature: float | None = None,\n",
    "    max_tokens: int | None = None,\n",
    "    top_k: int | None = None,\n",
    "    top_p: float | None = None,\n",
    "    stop: list[str] | None = None,\n",
    "    presence_penalty: float | None = None,\n",
    "    frequency_penalty: float | None = None,\n",
    "    api_base: str | None = None,\n",
    "    timeout: int | None = None,\n",
    "    seed: int | None = None,\n",
    "    extra: dict[str, typing.Any] = None,\n",
    ")\n",
    "```\n",
    "\n",
    "https://rigging.dreadnode.io/api/generator/#rigging.generator.GenerateParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:15.223081Z",
     "iopub.status.busy": "2024-06-18T18:47:15.222833Z",
     "iopub.status.idle": "2024-06-18T18:47:17.479867Z",
     "shell.execute_reply": "2024-06-18T18:47:17.478938Z",
     "shell.execute_reply.started": "2024-06-18T18:47:15.223059Z"
    }
   },
   "outputs": [],
   "source": [
    "rg_params = rg.GenerateParams(\n",
    "    temperature = 0.9,\n",
    "    max_tokens = 512,\n",
    ")\n",
    "base_chat = generator.chat(params=rg_params)\n",
    "answer = await base_chat.fork('How is it going?').run()\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc12519",
   "metadata": {},
   "source": [
    "Or parameters can be set within the chain using params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:17.481882Z",
     "iopub.status.busy": "2024-06-18T18:47:17.481227Z",
     "iopub.status.idle": "2024-06-18T18:47:19.65719Z",
     "shell.execute_reply": "2024-06-18T18:47:19.656339Z",
     "shell.execute_reply.started": "2024-06-18T18:47:17.481845Z"
    }
   },
   "outputs": [],
   "source": [
    "base_chat = generator.chat() # No params set\n",
    "answer = await base_chat.fork('How is it going?') \\\n",
    "    .with_(temperature = 0.9, max_tokens = 512) \\\n",
    "    .run()\n",
    "print(answer.last.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e30ea5",
   "metadata": {},
   "source": [
    "# Parsed outputs example\n",
    "\n",
    "Next we will create a pipeline where we:\n",
    "1. Create a rigging Model called `Answer`. This explains the expected output that we will parse from the model results.\n",
    "    - We will add some validators to this that will ensure the output is either `yes` or `no`\n",
    "    - This is fully customizable.\n",
    "    - Here `validate_content` is ensuring that our response conforms to the expected output (lowercase and starts with \"yes\" or \"no\")\n",
    "2. We can use the `Answer.xml_example()` in our prompt to let the LLM know how we expect the output to look.\n",
    "3. Later on we will use `.until_parsed_as(Answer)` to ensure the LLM output is extracted as defined here.\n",
    "\n",
    "**Note** `until_parsed_as()` can take a `max_rounds` parameter, which by default is 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:19.659101Z",
     "iopub.status.busy": "2024-06-18T18:47:19.658668Z",
     "iopub.status.idle": "2024-06-18T18:47:19.668457Z",
     "shell.execute_reply": "2024-06-18T18:47:19.667558Z",
     "shell.execute_reply.started": "2024-06-18T18:47:19.659066Z"
    }
   },
   "outputs": [],
   "source": [
    "import typing as t\n",
    "from pydantic import field_validator\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}**yes/no**{Answer.xml_end_tag()}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:19.669852Z",
     "iopub.status.busy": "2024-06-18T18:47:19.669536Z",
     "iopub.status.idle": "2024-06-18T18:47:19.752363Z",
     "shell.execute_reply": "2024-06-18T18:47:19.751366Z",
     "shell.execute_reply.started": "2024-06-18T18:47:19.669814Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lets see what the xml example looks like for this we can use this in our prompt\n",
    "Answer.xml_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:21.724128Z",
     "iopub.status.busy": "2024-06-18T18:47:21.723509Z",
     "iopub.status.idle": "2024-06-18T18:47:33.30668Z",
     "shell.execute_reply": "2024-06-18T18:47:33.305798Z",
     "shell.execute_reply.started": "2024-06-18T18:47:21.724097Z"
    }
   },
   "outputs": [],
   "source": [
    "generator = rg.get_generator(\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding,\n",
    ")\n",
    "\n",
    "keyword='Tom Hanks'\n",
    "category='Famous Person'\n",
    "last_question='Is it a famous person?'\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "            The secret word for this game is \"{keyword}\" [{category}]\n",
    "\n",
    "            You are currently answering a question about the word above.\n",
    "\n",
    "            The next question is \"{last_question}\".\n",
    "\n",
    "            Answer the yes/no question above and place it in the following format:\n",
    "            {Answer.xml_example()}\n",
    "\n",
    "            - Your response should be accurate given the keyword above\n",
    "            - Always answer with \"yes\" or \"no\"\n",
    "\n",
    "            What is the answer?\n",
    "\"\"\"\n",
    "\n",
    "chat = await (\n",
    "    generator\n",
    "    .chat(prompt)\n",
    "    .until_parsed_as(Answer, max_rounds=50)\n",
    "    .run()\n",
    ")\n",
    "\n",
    "print('=== Full Chat ===')\n",
    "print(chat)\n",
    "\n",
    "print()\n",
    "print('=== LLM Response Only ===')\n",
    "print(chat.last)\n",
    "\n",
    "print()\n",
    "print('=== Parsed Answer ===')\n",
    "print(chat.last.parse(Answer).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122ff8fc",
   "metadata": {},
   "source": [
    "# Create an example Questioner Chat Pipeline with Rigging\n",
    "\n",
    "Next lets create the questioner pipeline that will attempt to help determine what the keyword might be.\n",
    "\n",
    "First lets create a `Question` object which we will use to parse our output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:33.308069Z",
     "iopub.status.busy": "2024-06-18T18:47:33.307802Z",
     "iopub.status.idle": "2024-06-18T18:47:33.315461Z",
     "shell.execute_reply": "2024-06-18T18:47:33.314429Z",
     "shell.execute_reply.started": "2024-06-18T18:47:33.308044Z"
    }
   },
   "outputs": [],
   "source": [
    "from pydantic import StringConstraints  # noqa\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"**question**\").to_pretty_xml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:39.082592Z",
     "iopub.status.busy": "2024-06-18T18:47:39.082181Z",
     "iopub.status.idle": "2024-06-18T18:47:39.859978Z",
     "shell.execute_reply": "2024-06-18T18:47:39.859196Z",
     "shell.execute_reply.started": "2024-06-18T18:47:39.082562Z"
    }
   },
   "outputs": [],
   "source": [
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "question_chat = await (base.fork(\n",
    "    f\"\"\"\\\n",
    "    You are currently asking the next question.\n",
    "\n",
    "    question and place it in the following format:\n",
    "    {Question.xml_example()}\n",
    "\n",
    "    - Your response should be a focused question which will gather the most information\n",
    "    - Start general with your questions\n",
    "    - Always try to bisect the remaining search space\n",
    "    - Pay attention to previous questions and answers\n",
    "\n",
    "    What is your next question?\n",
    "    \"\"\"\n",
    ")\n",
    ".until_parsed_as(Question, attempt_recovery=True)\n",
    ".run()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:39.861319Z",
     "iopub.status.busy": "2024-06-18T18:47:39.861048Z",
     "iopub.status.idle": "2024-06-18T18:47:39.884675Z",
     "shell.execute_reply": "2024-06-18T18:47:39.883442Z",
     "shell.execute_reply.started": "2024-06-18T18:47:39.861295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dataframe representation of the conversation\n",
    "question_chat.to_df()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f052539d",
   "metadata": {},
   "source": [
    "We now are confident that the LLM response contains the quesion and case parse the question like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T18:47:46.02397Z",
     "iopub.status.busy": "2024-06-18T18:47:46.023601Z",
     "iopub.status.idle": "2024-06-18T18:47:46.029781Z",
     "shell.execute_reply": "2024-06-18T18:47:46.028875Z",
     "shell.execute_reply.started": "2024-06-18T18:47:46.02394Z"
    }
   },
   "outputs": [],
   "source": [
    "question = question_chat.last.parse(Question).content\n",
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea447dc",
   "metadata": {},
   "source": [
    "# Create a keyword dataframe\n",
    "** Note this only works because we know the possible keywords in the public set. This will not work on the final leaderboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:13:09.05161Z",
     "iopub.status.busy": "2024-06-18T19:13:09.050676Z",
     "iopub.status.idle": "2024-06-18T19:13:11.606383Z",
     "shell.execute_reply": "2024-06-18T19:13:11.605418Z",
     "shell.execute_reply.started": "2024-06-18T19:13:09.051574Z"
    }
   },
   "outputs": [],
   "source": [
    "!wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:13:13.191677Z",
     "iopub.status.busy": "2024-06-18T19:13:13.190866Z",
     "iopub.status.idle": "2024-06-18T19:13:14.141544Z",
     "shell.execute_reply": "2024-06-18T19:13:14.140544Z",
     "shell.execute_reply.started": "2024-06-18T19:13:13.19164Z"
    }
   },
   "outputs": [],
   "source": [
    "!head keywords_local.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:08.09623Z",
     "iopub.status.busy": "2024-06-18T19:39:08.095864Z",
     "iopub.status.idle": "2024-06-18T19:39:08.157802Z",
     "shell.execute_reply": "2024-06-18T19:39:08.156896Z",
     "shell.execute_reply.started": "2024-06-18T19:39:08.096204Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import pandas as pd\n",
    "sys.path.append('./')\n",
    "from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "def capitalize_first_word(text):\n",
    "    if not text:\n",
    "        return text\n",
    "    return text[0].upper() + text[1:].lower()\n",
    "\n",
    "def create_keyword_df(KEYWORDS_JSON):\n",
    "    keywords_dict = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "    category_words_dict = {}\n",
    "    all_words = []\n",
    "    all_cat_words = []\n",
    "    for d in keywords_dict:\n",
    "        words = [w['keyword'] for w in d['words']]\n",
    "        cat_word = [(d['category'], w['keyword']) for w in d['words']]\n",
    "        category_words_dict[d['category']] = words\n",
    "        all_words += words\n",
    "        all_cat_words += cat_word\n",
    "\n",
    "    keyword_df = pd.DataFrame(all_cat_words, columns=['category','keyword'])\n",
    "    keyword_df['first_letter'] = keyword_df['keyword'].str[0]\n",
    "    keyword_df['second_letter'] = keyword_df['keyword'].str[1]\n",
    "    keyword_df.to_parquet('keywords.parquet')\n",
    "    \n",
    "create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:08.568525Z",
     "iopub.status.busy": "2024-06-18T19:39:08.56792Z",
     "iopub.status.idle": "2024-06-18T19:39:08.621277Z",
     "shell.execute_reply": "2024-06-18T19:39:08.620385Z",
     "shell.execute_reply.started": "2024-06-18T19:39:08.568491Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df = pd.read_parquet('keywords.parquet')\n",
    "keywords_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:39:09.267519Z",
     "iopub.status.busy": "2024-06-18T19:39:09.267113Z",
     "iopub.status.idle": "2024-06-18T19:39:09.314644Z",
     "shell.execute_reply": "2024-06-18T19:39:09.313676Z",
     "shell.execute_reply.started": "2024-06-18T19:39:09.267487Z"
    }
   },
   "outputs": [],
   "source": [
    "keywords_df['category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c227376b",
   "metadata": {},
   "source": [
    "# Create `main.py` Script for Final Submission.\n",
    "\n",
    "Our final submission will be a zipped directory with a `main` file. This file is below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:42.817674Z",
     "iopub.status.busy": "2024-06-18T20:07:42.817302Z",
     "iopub.status.idle": "2024-06-18T20:07:42.871384Z",
     "shell.execute_reply": "2024-06-18T20:07:42.870434Z",
     "shell.execute_reply.started": "2024-06-18T20:07:42.817645Z"
    },
    "papermill": {
     "duration": 0.016612,
     "end_time": "2024-04-17T13:47:49.33012",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.313508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "# Main agent file\n",
    "\n",
    "import itertools\n",
    "import os\n",
    "import sys\n",
    "import typing as t\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Path fixups\n",
    "\n",
    "g_working_path = Path('/kaggle/working')\n",
    "g_input_path = Path('/kaggle/input')\n",
    "g_temp_path = Path(\"/kaggle/tmp\")\n",
    "g_agent_path = Path(\"/kaggle_simulations/agent/\")\n",
    "\n",
    "g_model_path = g_temp_path / \"model\"\n",
    "g_srvlib_path = g_temp_path / \"srvlib\"\n",
    "g_lib_path = g_temp_path / \"lib\"\n",
    "\n",
    "if g_agent_path.exists():\n",
    "    g_lib_path = g_agent_path / \"lib\"\n",
    "    g_model_path = g_agent_path / \"model\"\n",
    "    g_srvlib_path = g_agent_path / \"srvlib\"\n",
    "else:\n",
    "    g_agent_path = Path('/kaggle/working')\n",
    "    \n",
    "sys.path.insert(0, str(g_lib_path))\n",
    "\n",
    "# Logging noise\n",
    "\n",
    "logging.getLogger(\"LiteLLM\").setLevel(logging.WARNING)\n",
    "\n",
    "# Fixed imports\n",
    "\n",
    "import util # noqa\n",
    "import rigging as rg  # noqa\n",
    "from pydantic import BaseModel, field_validator, StringConstraints  # noqa\n",
    "\n",
    "# Constants\n",
    "\n",
    "g_vllm_port = 9999\n",
    "g_vllm_model_name = \"custom\"\n",
    "\n",
    "g_generator_id = (\n",
    "    f\"openai/{g_vllm_model_name},\" \\\n",
    "    f\"api_base=http://localhost:{g_vllm_port}/v1,\" \\\n",
    "    \"api_key=sk-1234,\" \\\n",
    "    \"stop=<|eot_id|>\" # Llama requires some hand holding\n",
    ")\n",
    "\n",
    "# Types\n",
    "\n",
    "str_strip = t.Annotated[str, StringConstraints(strip_whitespace=True)]\n",
    "\n",
    "class Observation(BaseModel):\n",
    "    step: int\n",
    "    role: t.Literal[\"guesser\", \"answerer\"]\n",
    "    turnType: t.Literal[\"ask\", \"answer\", \"guess\"]\n",
    "    keyword: str\n",
    "    category: str\n",
    "    questions: list[str]\n",
    "    answers: list[str]\n",
    "    guesses: list[str]\n",
    "    \n",
    "    @property\n",
    "    def empty(self) -> bool:\n",
    "        return all(len(t) == 0 for t in [self.questions, self.answers, self.guesses])\n",
    "    \n",
    "    def get_history(self) -> t.Iterator[tuple[str, str, str]]:\n",
    "        return itertools.zip_longest(self.questions, self.answers, self.guesses, fillvalue=\"[none]\")\n",
    "\n",
    "    def get_history_as_xml(self, *, skip_guesses: bool = False) -> str:\n",
    "        if not self.empty:\n",
    "            history = \"\\n\".join(\n",
    "            f\"\"\"\\\n",
    "            <turn-{i}>\n",
    "            Question: {question}\n",
    "            Answer: {answer}\n",
    "            {'Guess: ' + guess if not skip_guesses else ''}\n",
    "            </turn-{i}>\n",
    "            \"\"\"\n",
    "            for i, (question, answer, guess) in enumerate(self.get_history())\n",
    "            )\n",
    "            return history\n",
    "        return \"none yet.\"\n",
    "\n",
    "\n",
    "class Answer(rg.Model):\n",
    "    content: t.Literal[\"yes\", \"no\"]\n",
    "\n",
    "    @field_validator(\"content\", mode=\"before\")\n",
    "    def validate_content(cls, v: str) -> str:\n",
    "        for valid in [\"yes\", \"no\"]:\n",
    "            if v.lower().startswith(valid):\n",
    "                return valid\n",
    "        raise ValueError(\"Invalid answer, must be 'yes' or 'no'\")\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return f\"{Answer.xml_start_tag()}yes/no{Answer.xml_end_tag()}\"\n",
    "\n",
    "\n",
    "class Question(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Question(content=\"question\").to_pretty_xml()\n",
    "\n",
    "\n",
    "class Guess(rg.Model):\n",
    "    content: str_strip\n",
    "\n",
    "    @classmethod\n",
    "    def xml_example(cls) -> str:\n",
    "        return Guess(content=\"thing/place\").to_pretty_xml()\n",
    "\n",
    "\n",
    "# Functions\n",
    "\n",
    "async def ask(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    if observation.step == 0:\n",
    "        # override first question until keyword bug is fixed.\n",
    "        return \"Are we playing 20 questions?\"\n",
    "    \n",
    "    try:\n",
    "        chat = await (\n",
    "             base.fork(\n",
    "                f\"\"\"\\\n",
    "                You are currently asking the next question.\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml(skip_guesses=True)}\n",
    "                </game-history>\n",
    "\n",
    "                Based on the history above, ask the next most useful yes/no\n",
    "                question and place it in the following format:\n",
    "                {Question.xml_example()}\n",
    "\n",
    "                - Your response should be a focused question which will gather the most information\n",
    "                - Start general with your questions\n",
    "                - Always try to bisect the remaining search space\n",
    "                - Pay attention to previous questions and answers\n",
    "\n",
    "                What is your next question?\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Question, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "        return chat.last.parse(Question).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'Is it a person?'\n",
    "\n",
    "async def answer(base: rg.ChatPipeline, observation: Observation) -> t.Literal[\"yes\", \"no\"]:\n",
    "    if not observation.keyword:\n",
    "        print(\"Keyword wasn't provided to answerer\", file=sys.stderr)\n",
    "        return \"yes\" # override until keyword bug is fixed.\n",
    "            \n",
    "    last_question = observation.questions[-1]\n",
    "    \n",
    "#     print('=' * 10)\n",
    "#     print(f\"\"\"\\\n",
    "#                 Provide the best yes/no answer to the question about the keyword [{observation.keyword}] in the category [{observation.category}]\n",
    "\n",
    "#                 [QUESTION] \"{last_question}\" [/QUESTION]\n",
    "                \n",
    "#                 Remember they keyword is [{observation.keyword}]\n",
    "                \n",
    "#                 Answer the yes/no question above and place it in the following format:\n",
    "#                 {Answer.xml_example()}\n",
    "#                 \"\"\"\n",
    "#          )\n",
    "#     print('=' * 10)\n",
    "    try:\n",
    "        responses = []\n",
    "        for i in range(5):\n",
    "            # Loop 5 times and take the most frequent response\n",
    "            chat = await (\n",
    "                base.fork(\n",
    "                    f\"\"\"\\\n",
    "                    Provide the best yes/no answer to the question about the keyword [{observation.keyword}] in the category [{observation.category}]\n",
    "\n",
    "                    [QUESTION] \"{last_question}\" [/QUESTION]\n",
    "\n",
    "                    Remember they keyword is [{observation.keyword}]\n",
    "\n",
    "                    Answer the yes/no question above and place it in the following format:\n",
    "                    {Answer.xml_example()}\n",
    "                    \"\"\"\n",
    "                )\n",
    "                .until_parsed_as(Answer, attempt_recovery=True, max_rounds=20)\n",
    "                .run()\n",
    "            )\n",
    "            responses.append(chat.last.parse(Answer).content.strip('*'))\n",
    "            \n",
    "        print(f'Responses are {responses}')\n",
    "        return pd.Series(responses).value_counts().index[0]\n",
    "#         print('=' * 10)\n",
    "#         print('Response.....')\n",
    "#         print(chat.last)\n",
    "#         print('=' * 10)\n",
    "#         return chat.last.parse(Answer).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        print('%%%%%%%%%%%% Error so answering yes %%%%%%%%%%%% ')\n",
    "        return 'yes'\n",
    "\n",
    "async def guess(base: rg.ChatPipeline, observation: Observation) -> str:\n",
    "    try:\n",
    "\n",
    "        chat = await (\n",
    "            base.fork(\n",
    "                f\"\"\"\\\n",
    "                You are currently making an informed guess of the keyword.\n",
    "\n",
    "                <game-history>\n",
    "                {observation.get_history_as_xml()}\n",
    "                </game-history>\n",
    "\n",
    "                Based on the history above, produce a single next best guess\n",
    "                for the keyword and place it in the following format:\n",
    "                {Guess.xml_example()}\n",
    "\n",
    "                - Avoid repeat guesses based on the history above\n",
    "                - The guess should be a specific person, place, or thing\n",
    "\n",
    "                What is your guess?\n",
    "                \"\"\"\n",
    "            )\n",
    "            .until_parsed_as(Guess, attempt_recovery=True, max_rounds=20)\n",
    "            .run()\n",
    "        )\n",
    "\n",
    "        return chat.last.parse(Guess).content.strip('*')\n",
    "    except rg.error.MessagesExhaustedMaxRoundsError:\n",
    "        return 'france'\n",
    "    \n",
    "# vLLM and Generator\n",
    "\n",
    "try:\n",
    "    vllm = util.run_and_wait_for_port([\n",
    "        \"python\", \"-m\",\n",
    "        \"vllm.entrypoints.openai.api_server\",\n",
    "        \"--enforce-eager\",\n",
    "        \"--model\", str(g_model_path),\n",
    "        \"--port\", str(g_vllm_port),\n",
    "        \"--served-model-name\", g_vllm_model_name\n",
    "    ], g_vllm_port, {\"PYTHONPATH\": str(g_srvlib_path)})\n",
    "\n",
    "    print(\"vLLM Started\")\n",
    "except ValueError:\n",
    "    print('vLLM Already Running')\n",
    "    \n",
    "    \n",
    "generator = rg.get_generator(g_generator_id)\n",
    "\n",
    "base =  generator.chat(\"\"\"\\\n",
    "You are a talented player of the 20 questions game. You are accurate, focused, and\n",
    "structured in your approach. You will create useful questions, make guesses, or answer\n",
    "questions about a keyword.\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Entrypoint\n",
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"Does the keyword start with any letter?\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"Does the keyword start with the letter '{letters[0]}'\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" or '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"Does the keyword start with one of the letters {formatted_letters}?\"\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches\n",
    "\n",
    "# Simple question asker\n",
    "class SimpleQuestionerAgent():\n",
    "    def __init__(self, keyword_df: pd.DataFrame):\n",
    "        self.keyword_df = keyword_df\n",
    "        self.keyword_df_init = keyword_df.copy()\n",
    "        self.round = 0\n",
    "        self.category_questions = [\n",
    "            \"Are we playing 20 questions?\",\n",
    "            \"Is the keyword a thing that is not a place?\",\n",
    "            \"Is the keyword a place?\",\n",
    "        ]\n",
    "        self.found_category = False\n",
    "        \n",
    "    def filter_keywords(self, obs):\n",
    "        print(self.keyword_df.shape)\n",
    "        # Filter down keyword_df based on past answers\n",
    "        for i, answer in enumerate(obs.answers):\n",
    "            if obs.questions[i] in self.category_questions:\n",
    "                if answer == 'yes':\n",
    "                    if obs.questions[i] == \"Is the keyword a thing that is not a place?\":\n",
    "                        self.found_category = 'things'\n",
    "                    if obs.questions[i] == \"Is the keyword a place?\":\n",
    "                        self.found_category = 'place'\n",
    "                    fc = self.found_category\n",
    "                    self.keyword_df = self.keyword_df.query('category == @fc').reset_index(drop=True)\n",
    "    \n",
    "            if obs.questions[i].startswith('Does the keyword start '):\n",
    "                if self.keyword_df['first_letter'].nunique() <= 1:\n",
    "                    break\n",
    "                letter_question = obs.questions[i]\n",
    "#                 letters = letter_question.replace('Precisely evaluate the very first letter in the keyword. If the keyword is multiple words only evaluate the first word. Answer Yes/No if ANY of these letters are the first letter in the keyword: ','')\n",
    "#                 letters = letter_question.split(' (say yes if it does start with one of them, no if it doesnt) ')[-1]\n",
    "#                 letters = letters.replace('?','').replace(' ','').replace(':','').replace('[','').replace(']','').strip().split(',')\n",
    "                letters = extract_letters_from_question(letter_question)\n",
    "                self.keyword_df = self.keyword_df.reset_index(drop=True).copy()\n",
    "                if obs.answers[i] == 'yes':\n",
    "#                     print(f'Filtering down to letters {letters}')\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "                elif obs.answers[i] == 'no':\n",
    "#                     print(f'Excluding letters {letters}')\n",
    "                    self.keyword_df = self.keyword_df.loc[\n",
    "                        ~self.keyword_df['first_letter'].isin(letters)].reset_index(drop=True).copy()\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # Reset\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "            \n",
    "    def get_letters(self, obs, max_letters=20):\n",
    "        n_letters = self.keyword_df['first_letter'].nunique()\n",
    "        sample_letters = self.keyword_df['first_letter'].drop_duplicates().sample(n_letters // 2).values.tolist()\n",
    "        sample_letters = sample_letters[:max_letters]\n",
    "        print('sample letters', n_letters, sample_letters)\n",
    "        return sample_letters # ', '.join(sample_letters)\n",
    "    \n",
    "    def __call__(self, obs, *args):\n",
    "        if len(self.keyword_df) == 0:\n",
    "            # Reset\n",
    "            self.keyword_df = self.keyword_df_init.copy()\n",
    "        self.filter_keywords(obs)\n",
    "        if obs.turnType == 'ask':\n",
    "            self.round += 1\n",
    "            if (self.round <= 3 and not self.found_category):\n",
    "                response = self.category_questions[self.round - 1]\n",
    "            else:\n",
    "                sample_letters = self.get_letters(obs)\n",
    "                if len(sample_letters) == 0:\n",
    "                    n_sample = min(len(self.keyword_df), 10)\n",
    "                    possible_keywords = \", \".join(self.keyword_df['keyword'].sample(n_sample).values.tolist())\n",
    "                    response = f\"Is the keyword one of the following? {possible_keywords}\"\n",
    "                else:\n",
    "                    sample_letters_str = str(sample_letters).replace('[','').replace(']','')\n",
    "#                     response = f'Does the keyword start with one of the following letters : {sample_letters_str} ?'\n",
    "                    response = format_first_letter_question(sample_letters)\n",
    "        elif obs.turnType == 'guess':\n",
    "            response = self.keyword_df['keyword'].sample(1).values[0]\n",
    "            # Remove the guessed word\n",
    "            updated_df = self.keyword_df.loc[self.keyword_df['keyword'] != response].reset_index(drop=True).copy()\n",
    "            if len(updated_df) >= 1:\n",
    "                self.keyword_df = updated_df.copy()\n",
    "            else:\n",
    "                self.keyword_df = self.keyword_df_init.copy() # Reset the df\n",
    "#         print(f'Round {self.round}')\n",
    "#         print(f\"{response=}\")\n",
    "#         print(f'keyword_df size {self.keyword_df.shape}')\n",
    "        return response\n",
    "\n",
    "\n",
    "keyword_df = pd.read_parquet(f'{g_agent_path}/keywords.parquet')\n",
    "question_agent = None\n",
    "\n",
    "async def observe(obs: t.Any) -> str:\n",
    "    observation = Observation(**obs.__dict__)\n",
    "    global question_agent\n",
    "    if question_agent is None:\n",
    "        question_agent = SimpleQuestionerAgent(keyword_df)\n",
    "\n",
    "    try:\n",
    "        match observation.turnType:\n",
    "            case \"ask\":\n",
    "#                 return await ask(base, observation)\n",
    "                return question_agent(obs)\n",
    "            case \"answer\":\n",
    "                return await answer(base, observation)\n",
    "            case \"guess\":\n",
    "#                 return await guess(base, observation)\n",
    "                return question_agent(obs)\n",
    "\n",
    "            case _:\n",
    "                raise ValueError(\"Unknown turn type\")\n",
    "    except Exception as e:\n",
    "        print(str(e), file=sys.stderr)\n",
    "        raise\n",
    "\n",
    "def agent_fn(obs: t.Any, _: t.Any) -> str:\n",
    "    # Async gate when executing in their framework\n",
    "    import asyncio\n",
    "    return asyncio.run(observe(obs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1713e27",
   "metadata": {},
   "source": [
    "# Test the Agent Against Itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:44.085235Z",
     "iopub.status.busy": "2024-06-18T20:07:44.084879Z",
     "iopub.status.idle": "2024-06-18T20:07:45.842315Z",
     "shell.execute_reply": "2024-06-18T20:07:45.841352Z",
     "shell.execute_reply.started": "2024-06-18T20:07:44.085207Z"
    }
   },
   "outputs": [],
   "source": [
    "def format_first_letter_question(letters):\n",
    "    if not letters:\n",
    "        return \"Does the keyword start with any letter?\"\n",
    "    \n",
    "    if len(letters) == 1:\n",
    "        return f\"Does the keyword start with the letter '{letters[0]}'\"\n",
    "    \n",
    "    formatted_letters = \", \".join(f\"'{letter}'\" for letter in letters[:-1])\n",
    "    formatted_letters += f\" or '{letters[-1]}'\"\n",
    "    \n",
    "    return f\"Does the keyword start with one of the letters {formatted_letters}?\"\n",
    "\n",
    "format_first_letter_question(['a','b','c'])\n",
    "\n",
    "import re\n",
    "\n",
    "def extract_letters_from_question(question):\n",
    "    pattern = r\"'([a-zA-Z])'\"\n",
    "    matches = re.findall(pattern, question)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:45.84427Z",
     "iopub.status.busy": "2024-06-18T20:07:45.843989Z",
     "iopub.status.idle": "2024-06-18T20:07:45.888115Z",
     "shell.execute_reply": "2024-06-18T20:07:45.887152Z",
     "shell.execute_reply.started": "2024-06-18T20:07:45.844245Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from main import Observation, agent_fn, observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:46.655031Z",
     "iopub.status.busy": "2024-06-18T20:07:46.6543Z",
     "iopub.status.idle": "2024-06-18T20:07:47.652045Z",
     "shell.execute_reply": "2024-06-18T20:07:47.650877Z",
     "shell.execute_reply.started": "2024-06-18T20:07:46.655Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check if vllm is running\n",
    "!ps -aef | grep vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T20:07:48.447833Z",
     "iopub.status.busy": "2024-06-18T20:07:48.447029Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "keyword_df = pd.read_parquet('keywords.parquet')\n",
    "sample = keyword_df.sample(1)\n",
    "\n",
    "obs = Observation(step = 0,\n",
    "    role = 'guesser',\n",
    "    turnType= \"ask\",\n",
    "    keyword= sample['keyword'].values[0],\n",
    "    category= sample['category'].values[0],\n",
    "    questions = [],\n",
    "    answers= [],\n",
    "    guesses= [],\n",
    ")\n",
    "\n",
    "\n",
    "for i in range(20):\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'ask'\n",
    "    question = await observe(obs)\n",
    "    print(f'[{i} Question]: {question}')\n",
    "    obs.questions.append(question)\n",
    "    obs.role = 'answerer'\n",
    "    obs.turnType = 'answer'\n",
    "    answer = await observe(obs)\n",
    "    obs.answers.append(answer)\n",
    "    \n",
    "    if obs.questions[-1].startswith('Are we playing 20 questions?'):\n",
    "        gt_answer = answer # whatever\n",
    "    elif obs.questions[-1].startswith('Is the keyword a thing that is not a place?'):\n",
    "        if sample['category'].values[0] == 'things':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword a place?'):\n",
    "        if sample['category'].values[0] == 'place':\n",
    "            gt_answer = 'yes'\n",
    "        else:\n",
    "            gt_answer = 'no'\n",
    "    elif obs.questions[-1].startswith('Does the keyword start'):\n",
    "        letters_guess = extract_letters_from_question(obs.questions[-1])\n",
    "        gt_answer = obs.keyword[0] in letters_guess\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "    elif obs.questions[-1].startswith('Is the keyword one of the following?'):\n",
    "        possible_kw = obs.questions[-1].replace('Is the keyword one of the following? ','').split(',')\n",
    "        possible_kw = [c.strip(' ') for c in possible_kw]\n",
    "        print(possible_kw)\n",
    "        gt_answer = obs.keyword in possible_kw\n",
    "        gt_answer = 'yes' if gt_answer else 'no'\n",
    "\n",
    "    print(f'[{i} Answer]: {answer} [True Answer]: {gt_answer}')\n",
    "    if answer != gt_answer:\n",
    "        break\n",
    "\n",
    "    obs.role = 'guesser'\n",
    "    obs.turnType = 'guess'\n",
    "    guess = await observe(obs)\n",
    "    print(f'[{i} Guess]: {guess} - [Keyword]: {obs.keyword}')\n",
    "    obs.guesses.append(guess)\n",
    "    if guess == obs.keyword:\n",
    "        print('GOT IT!')\n",
    "        break\n",
    "        \n",
    "    obs.step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bbc728",
   "metadata": {},
   "source": [
    "# Zipping Model and Code for Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2024-06-18T20:05:09.682043Z",
     "iopub.status.busy": "2024-06-18T20:05:09.681791Z",
     "iopub.status.idle": "2024-06-18T20:05:12.711224Z",
     "shell.execute_reply": "2024-06-18T20:05:12.710045Z",
     "shell.execute_reply.started": "2024-06-18T20:05:09.682021Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:16:49.684667Z",
     "iopub.status.busy": "2024-06-18T19:16:49.684329Z",
     "iopub.status.idle": "2024-06-18T19:19:49.999133Z",
     "shell.execute_reply": "2024-06-18T19:19:49.997937Z",
     "shell.execute_reply.started": "2024-06-18T19:16:49.684638Z"
    },
    "papermill": {
     "duration": 5.560311,
     "end_time": "2024-04-17T13:47:54.892856",
     "exception": false,
     "start_time": "2024-04-17T13:47:49.332545",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast' \\\n",
    "    -cf submission.tar.gz \\\n",
    "    --dereference \\\n",
    "    -C /kaggle/tmp model lib srvlib \\\n",
    "    -C /kaggle/working main.py util.py \\\n",
    "    -C /kaggle/working keywords.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:50.002318Z",
     "iopub.status.busy": "2024-06-18T19:19:50.001899Z",
     "iopub.status.idle": "2024-06-18T19:19:51.037256Z",
     "shell.execute_reply": "2024-06-18T19:19:51.036303Z",
     "shell.execute_reply.started": "2024-06-18T19:19:50.002277Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls -GFlash --color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405d4e00",
   "metadata": {},
   "source": [
    "# Submitting using Kaggle CLI\n",
    "\n",
    "Optionally you can submit using the kaggle cli interface without needing to re-run commit the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-18T19:19:51.040706Z",
     "iopub.status.busy": "2024-06-18T19:19:51.040378Z",
     "iopub.status.idle": "2024-06-18T19:19:51.086269Z",
     "shell.execute_reply": "2024-06-18T19:19:51.085134Z",
     "shell.execute_reply.started": "2024-06-18T19:19:51.040676Z"
    }
   },
   "outputs": [],
   "source": [
    "# !KAGGLE_USERNAME={KAGGLE_USERNAME} \\\n",
    "#  KAGGLE_KEY={KAGGLE_KEY} \\\n",
    "#  kaggle competitions submit -c llm-20-questions -f submission.tar.gz -m \"submit from notebook\""
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 169.923583,
   "end_time": "2024-04-17T13:50:23.369773",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-17T13:47:33.44619",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
