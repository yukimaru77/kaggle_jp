{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c314c727",
   "metadata": {},
   "source": [
    "# Qwen2-7b-it\n",
    "For your reference, I'll share an agent created with Qwen2-7b-it which has been performing well on the [LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T06:51:36.62623Z",
     "iopub.status.busy": "2024-07-11T06:51:36.625521Z",
     "iopub.status.idle": "2024-07-11T06:52:15.1781Z",
     "shell.execute_reply": "2024-07-11T06:52:15.177279Z",
     "shell.execute_reply.started": "2024-07-11T06:51:36.626199Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p /kaggle/working/submission\n",
    "pip install bitsandbytes accelerate\n",
    "pip install -U transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.180517Z",
     "iopub.status.busy": "2024-07-11T06:52:15.179979Z",
     "iopub.status.idle": "2024-07-11T06:52:15.191147Z",
     "shell.execute_reply": "2024-07-11T06:52:15.190164Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.180481Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile -a submission/main.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import re\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, AutoConfig, pipeline\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "KAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\n",
    "if os.path.exists(KAGGLE_AGENT_PATH):\n",
    "    model_id = os.path.join(KAGGLE_AGENT_PATH, \"1\")\n",
    "else:\n",
    "    model_id = \"/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1\"\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    quantization_config=quantization_config,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device_map=\"auto\")\n",
    "\n",
    "def parse_response(response):\n",
    "    match = re.search(\".+?\\?\", response.replace('*', ''))\n",
    "    if match is None:\n",
    "        question = 'Does the keyword begins with th letter \"s\"?'\n",
    "    else:\n",
    "        question = match.group()\n",
    "    return question\n",
    "\n",
    "def generate_answer(chat_template):\n",
    "    output = pipe(\n",
    "        chat_template,\n",
    "        max_new_tokens=32,\n",
    "        do_sample=False,\n",
    "#         temperature=0.01,\n",
    "#         top_p=0.1,\n",
    "#         top_k=1,\n",
    "        return_full_text=False,\n",
    "    )[0][\"generated_text\"]\n",
    "    output = re.sub('<end_of_turn>', '', output)\n",
    "    return output\n",
    "\n",
    "\n",
    "class Robot:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def on(self, mode, obs):\n",
    "        assert mode in [\"asking\", \"guessing\", \"answering\"], \"mode can only take one of these values: asking, answering, guessing\"\n",
    "        if mode == \"asking\":\n",
    "            output = self.asker(obs)\n",
    "        if mode == \"answering\":\n",
    "            output = self.answerer(obs)\n",
    "            if \"yes\" in output.lower() or \"Yes\" in output.lower():\n",
    "                output = \"yes\"\n",
    "            elif \"no\" in output.lower() or \"No\" in output.lower():\n",
    "                output = \"no\"\n",
    "            else:\n",
    "                output = \"no\"\n",
    "        if mode == \"guessing\":\n",
    "            output = self.asker(obs)\n",
    "        return output\n",
    "\n",
    "    def asker(self, obs):\n",
    "        sys_prompt = \"\"\"\n",
    "        You are an AI assistant designed to play the 20 Questions game. \n",
    "        In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner.\n",
    "        The keyword is a specific \"thing\" or \"place\".\n",
    "        \"\"\"\n",
    "        if obs.turnType ==\"ask\":\n",
    "            ask_prompt = sys_prompt + \"\"\"\n",
    "            Let's play 20 Questions. You are playing the role of the Questioner.\n",
    "            Please ask a yes-or-no question.\n",
    "            to help you, here's an example of how it should work:\n",
    "            examle: keyword is \"Morocco\":\n",
    "            <assistant: is it a city?\n",
    "            user: no\n",
    "            assistant: is it a country?\n",
    "            user: yes\n",
    "            assistant: is it in africa?\n",
    "            user: yes\n",
    "            assistant: is it a country name starting by m ?\n",
    "            user: yes\n",
    "            assistant: is it Morocco?\n",
    "            user: yes.>\n",
    "            \"\"\"\n",
    "\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}\\n\"\"\"\n",
    "\n",
    "            if len(obs.questions)>=1:\n",
    "                chat_template += \"Here's the conversation history so far:\\n\"\n",
    "                chat_template += \"<\\n\"\n",
    "                for q, a in zip(obs.questions, obs.answers):\n",
    "                    chat_template += f\"assistant:\\n{q}\\n\"\n",
    "                    chat_template += f\"user:\\n{a}\\n\"\n",
    "                chat_template += \">\\n\"\n",
    "                    \n",
    "            chat_template += \"\"\"\n",
    "            the user has chosen the word, ask your first question!\n",
    "            please be short and not verbose, give only one question, no extra word!<end_of_turn>\n",
    "            <start_of_turn>assistant\\n\n",
    "            \"\"\"\n",
    "#             print(chat_template)\n",
    "            output = generate_answer(chat_template) \n",
    "            output = parse_response(output)\n",
    "                    \n",
    "        elif obs.turnType == \"guess\":\n",
    "            conv = \"\"\n",
    "            for q, a in zip(obs.questions, obs.answers):\n",
    "                conv += f\"\"\"question:\\n{q}\\nanswer:\\n{a}\\n\"\"\"\n",
    "            guess_prompt =  sys_prompt + f\"\"\"\n",
    "            Your role is to predict keywords based on questions and answers.\n",
    "            to help you, here's an example of how it should work:\n",
    "            examle:\n",
    "            <question: is it a city?\n",
    "            answer: no\n",
    "            question: is it a country?\n",
    "            answer: yes\n",
    "            question: is it in africa?\n",
    "            answer: yes\n",
    "            question: is it a country name starting by m ?\n",
    "            answer: yes\n",
    "            you: Morocco>\n",
    "            \n",
    "            so far, the current state of the game is as following:\\n{conv}\n",
    "            based on the conversation, can you guess the word, please give only the word, no verbosity around<end_of_turn>\n",
    "            \"\"\"\n",
    "            chat_template = f\"\"\"<start_of_turn>system\\n{guess_prompt}\\n\"\"\"\n",
    "            chat_template += \"<start_of_turn>you:\\n\"\n",
    "#             print(chat_template)\n",
    "            output = generate_answer(chat_template)        \n",
    "        return output\n",
    "    \n",
    "    def answerer(self, obs):\n",
    "        ask_prompt = f\"\"\"\n",
    "        You are an AI assistant designed to play the 20 Questions game. \n",
    "        In this game, the Answerer thinks of a keyword and responds to yes-or-no questions by the Questioner.\n",
    "        to help you, here's an example of how it should work:\n",
    "        examle: keyword is \"Morocco\"\n",
    "        <user: is it a place?\n",
    "        you: yes\n",
    "        user: is it in europe?\n",
    "        you: no\n",
    "        user: is it in africa?\n",
    "        you: yes\n",
    "        user: is it a country name starting by m ?\n",
    "        you: yes\n",
    "        user: is it Morocco?\n",
    "        you: yes.>\n",
    "        About \"{obs.keyword}\", please answer the following question with \"yes\" or \"no\":\n",
    "        {obs.questions[-1]}\n",
    "        \"\"\"\n",
    "        chat_template = f\"\"\"<start_of_turn>system\\n{ask_prompt}<end_of_turn>\\n\"\"\"\n",
    "        chat_template += \"<start_of_turn>Assistant\\n\"\n",
    "        output = generate_answer(chat_template)\n",
    "        return output\n",
    "\n",
    "robot = Robot()\n",
    "\n",
    "def agent(obs, cfg):\n",
    "    \n",
    "    if obs.turnType ==\"ask\":\n",
    "        response = robot.on(mode = \"asking\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"guess\":\n",
    "        response = robot.on(mode = \"guessing\", obs = obs)\n",
    "        \n",
    "    elif obs.turnType ==\"answer\":\n",
    "        response = robot.on(mode = \"answering\", obs = obs)\n",
    "        \n",
    "    if response == None or len(response)<=1:\n",
    "        response = \"yes\"\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:15.192968Z",
     "iopub.status.busy": "2024-07-11T06:52:15.192649Z",
     "iopub.status.idle": "2024-07-11T06:52:22.612061Z",
     "shell.execute_reply": "2024-07-11T06:52:22.611154Z",
     "shell.execute_reply.started": "2024-07-11T06:52:15.192945Z"
    }
   },
   "outputs": [],
   "source": [
    "!apt install pigz pv > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:52:22.615033Z",
     "iopub.status.busy": "2024-07-11T06:52:22.614746Z",
     "iopub.status.idle": "2024-07-11T06:57:12.011816Z",
     "shell.execute_reply": "2024-07-11T06:57:12.010683Z",
     "shell.execute_reply.started": "2024-07-11T06:52:22.615005Z"
    }
   },
   "outputs": [],
   "source": [
    "!tar --use-compress-program='pigz --fast --recursive | pv' -cf submission.tar.gz -C /kaggle/input/qwen2/transformers/qwen2-7b-instruct . -C /kaggle/working/submission ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.013563Z",
     "iopub.status.busy": "2024-07-11T06:57:12.013245Z",
     "iopub.status.idle": "2024-07-11T06:57:12.018443Z",
     "shell.execute_reply": "2024-07-11T06:57:12.017575Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.013535Z"
    }
   },
   "outputs": [],
   "source": [
    "# # to see what's inside tar.gz file\n",
    "\n",
    "# import tarfile\n",
    "# tar = tarfile.open(\"/kaggle/working/submission.tar.gz\")\n",
    "# for file in tar.getmembers():\n",
    "#     print(file.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce385d8f",
   "metadata": {},
   "source": [
    "# Simulation\n",
    "If you want to simulate your agent before submitting it, uncomment the following and run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.019843Z",
     "iopub.status.busy": "2024-07-11T06:57:12.019546Z",
     "iopub.status.idle": "2024-07-11T06:57:12.027836Z",
     "shell.execute_reply": "2024-07-11T06:57:12.02707Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.019812Z"
    }
   },
   "outputs": [],
   "source": [
    "# !wget -O keywords_local.py https://raw.githubusercontent.com/Kaggle/kaggle-environments/master/kaggle_environments/envs/llm_20_questions/keywords.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.02921Z",
     "iopub.status.busy": "2024-07-11T06:57:12.028922Z",
     "iopub.status.idle": "2024-07-11T06:57:12.036912Z",
     "shell.execute_reply": "2024-07-11T06:57:12.036194Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.029187Z"
    }
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import pandas as pd\n",
    "# from submission.main import agent\n",
    "# from keywords_local import KEYWORDS_JSON\n",
    "\n",
    "# class Observation:\n",
    "#     def __init__(self):\n",
    "#         self.step = 0\n",
    "#         self.role = \"guesser\"\n",
    "#         self.turnType = \"ask\"\n",
    "#         self.keyword = \"Japan\"\n",
    "#         self.category = \"country\"\n",
    "#         self.questions = []\n",
    "#         self.answers = []\n",
    "#         self.guesses = []\n",
    "        \n",
    "# def create_keyword_df(KEYWORDS_JSON):\n",
    "#     json_data = json.loads(KEYWORDS_JSON)\n",
    "\n",
    "#     keyword_list = []\n",
    "#     category_list = []\n",
    "#     alts_list = []\n",
    "\n",
    "#     for i in range(len(json_data)):\n",
    "#         for j in range(len(json_data[i]['words'])):\n",
    "#             keyword = json_data[i]['words'][j]['keyword']\n",
    "#             keyword_list.append(keyword)\n",
    "#             category_list.append(json_data[i]['category'])\n",
    "#             alts_list.append(json_data[i]['words'][j]['alts'])\n",
    "\n",
    "#     data_pd = pd.DataFrame(columns=['keyword', 'category', 'alts'])\n",
    "#     data_pd['keyword'] = keyword_list\n",
    "#     data_pd['category'] = category_list\n",
    "#     data_pd['alts'] = alts_list\n",
    "    \n",
    "#     return data_pd\n",
    "    \n",
    "# keywords_df = create_keyword_df(KEYWORDS_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T06:57:12.038569Z",
     "iopub.status.busy": "2024-07-11T06:57:12.038129Z",
     "iopub.status.idle": "2024-07-11T06:57:12.049014Z",
     "shell.execute_reply": "2024-07-11T06:57:12.048315Z",
     "shell.execute_reply.started": "2024-07-11T06:57:12.038537Z"
    }
   },
   "outputs": [],
   "source": [
    "# obs = Observation()\n",
    "# cfg = \"_\"\n",
    "\n",
    "# sample_df = keywords_df.sample()\n",
    "# obs.keyword = sample_df[\"keyword\"].values[0]\n",
    "# obs.category = sample_df[\"category\"].values[0]\n",
    "# alts_list = sample_df[\"alts\"].values[0]\n",
    "# alts_list.append(obs.keyword)\n",
    "\n",
    "# print(f\"keyword:{obs.keyword}\")\n",
    "\n",
    "# for round in range(20):\n",
    "#     obs.step = round+1\n",
    "    \n",
    "#     obs.role = \"guesser\"\n",
    "#     obs.turnType = \"ask\"\n",
    "#     question = agent(obs, cfg)\n",
    "#     obs.questions.append(question)\n",
    "    \n",
    "#     obs.role = \"answerer\"\n",
    "#     obs.turnType = \"answer\"\n",
    "#     answer = agent(obs, cfg)\n",
    "#     obs.answers.append(answer)\n",
    "    \n",
    "#     obs.role = \"guesser\"\n",
    "#     obs.turnType = \"guess\"\n",
    "#     guess = agent(obs, cfg)\n",
    "#     obs.guesses.append(guess)\n",
    "    \n",
    "#     print(f\"round: {round+1}\")\n",
    "#     print(f\"question: {question}\")\n",
    "#     print(f\"answer: {answer}\")\n",
    "#     print(f\"guess: {guess}\")\n",
    "    \n",
    "#     if guess in alts_list:\n",
    "#         print(\"Win!!\")\n",
    "#         break"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    },
    {
     "modelInstanceId": 51944,
     "sourceId": 62188,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
