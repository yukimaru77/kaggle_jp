{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0b77ba",
   "metadata": {},
   "source": [
    "### The \"20 Questions LLM Competition\" is an innovative challenge that brings together the fields of artificial intelligence, natural language processing, and game theory. The competition revolves around the classic game of \"20 Questions,\" where one participant thinks of a secret word, and the other tries to guess it by asking up to 20 yes-or-no questions. This simple yet intriguing game is the foundation for a sophisticated AI competition.\n",
    "\n",
    "<figure>\n",
    "        <img src=\"https://www.kaggle.com/competitions/61247/images/header\" alt =\"Audio Art\" style='width:800px;height:500px;'>\n",
    "        <figcaption>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-04T08:47:05.564045Z",
     "iopub.status.busy": "2024-08-04T08:47:05.563775Z",
     "iopub.status.idle": "2024-08-04T08:47:13.977278Z",
     "shell.execute_reply": "2024-08-04T08:47:13.97615Z",
     "shell.execute_reply.started": "2024-08-04T08:47:05.56402Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model_id = \"deepset/roberta-base-squad2\"  # Fine-tuned RoBERTa model for QA\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "\n",
    "# Initialize question-answering pipeline for GPU\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)  # device=0 ensures GPU is used\n",
    "\n",
    "def generate_prompt(keyword, category, negate, continent):\n",
    "    if category in negate:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a {category}. {negate[category]} \"\n",
    "                  f\"This word has first letter {keyword[0]}. This {category} is located in {continent}.\")\n",
    "    else:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a thing. It is not a city. \"\n",
    "                  f\"It is not a country. It is not a landmark. This word has first letter {keyword[0]}.\")\n",
    "    return prompt\n",
    "\n",
    "def get_answer_from_model(prompt, question):\n",
    "    # Generate the answer from the model\n",
    "    response = qa_pipeline(question=question, context=prompt)\n",
    "    answer = response['answer']\n",
    "    return answer\n",
    "\n",
    "def main():\n",
    "    keyword = \"Mount Everest\"\n",
    "    category = \"mountain\"\n",
    "    negate = {\"mountain\": \"It is not a country, city, or person.\"}\n",
    "    continent = \"Asia\"\n",
    "\n",
    "    prompt = generate_prompt(keyword, category, negate, continent)\n",
    "\n",
    "    questions = [\n",
    "        \"Is this a country or city?\",\n",
    "        \"No, it is a mountain. What is the name of this mountain?\",\n",
    "        \"Where is this place?\"\n",
    "    ]\n",
    "\n",
    "    # Display the constructed prompt\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # Iterate over each question\n",
    "    for question in questions:\n",
    "        # Get the answer from the model\n",
    "        answer = get_answer_from_model(prompt, question)\n",
    "        \n",
    "        # Display the formatted output\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Answer: '{answer}'\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T08:47:17.974574Z",
     "iopub.status.busy": "2024-08-04T08:47:17.973938Z",
     "iopub.status.idle": "2024-08-04T08:47:17.982056Z",
     "shell.execute_reply": "2024-08-04T08:47:17.98108Z",
     "shell.execute_reply.started": "2024-08-04T08:47:17.974542Z"
    }
   },
   "outputs": [],
   "source": [
    "code = \"\"\"\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline\n",
    "\n",
    "# Load pre-trained RoBERTa model and tokenizer\n",
    "model_id = \"deepset/roberta-base-squad2\"  # Fine-tuned RoBERTa model for QA\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_id)\n",
    "\n",
    "# Initialize question-answering pipeline for GPU\n",
    "qa_pipeline = pipeline(\"question-answering\", model=model, tokenizer=tokenizer, device=0)  # device=0 ensures GPU is used\n",
    "\n",
    "def generate_prompt(keyword, category, negate, continent):\n",
    "    if category in negate:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a {category}. {negate[category]} \"\n",
    "                  f\"This word has first letter {keyword[0]}. This {category} is located in {continent}.\")\n",
    "    else:\n",
    "        prompt = (f\"We are playing 20 questions. The keyword is {keyword}. It is a thing. It is not a city. \"\n",
    "                  f\"It is not a country. It is not a landmark. This word has first letter {keyword[0]}.\")\n",
    "    return prompt\n",
    "\n",
    "def get_answer_from_model(prompt, question):\n",
    "    # Generate the answer from the model\n",
    "    response = qa_pipeline(question=question, context=prompt)\n",
    "    answer = response['answer']\n",
    "    return answer\n",
    "\n",
    "def main():\n",
    "    keyword = \"Mount Everest\"\n",
    "    category = \"mountain\"\n",
    "    negate = {\"mountain\": \"It is not a country, city, or person.\"}\n",
    "    continent = \"Asia\"\n",
    "\n",
    "    prompt = generate_prompt(keyword, category, negate, continent)\n",
    "\n",
    "    questions = [\n",
    "        \"Is this a country or city?\",\n",
    "        \"No, it is a mountain. What is the name of this mountain?\",\n",
    "        \"Where is this place?\"\n",
    "    ]\n",
    "\n",
    "    # Display the constructed prompt\n",
    "    print(f\"Prompt:\\n{prompt}\\n\")\n",
    "\n",
    "    # Iterate over each question\n",
    "    for question in questions:\n",
    "        # Get the answer from the model\n",
    "        answer = get_answer_from_model(prompt, question)\n",
    "        \n",
    "        # Display the formatted output\n",
    "        print(f\"Question: '{question}'\")\n",
    "        print(f\"Answer: '{answer}'\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() \n",
    "\"\"\"\n",
    "\n",
    "# Save the code to submission.py\n",
    "with open(\"/kaggle/working/submission.py\", \"w\") as f:\n",
    "    f.write(code)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8550470,
     "sourceId": 61247,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
