{
    "comments": [
        {
            "author": "shiv_314",
            "content": "This notebook is awesome! How did you come up with the fact that you can use a model with 8B parameters as an upper limit? Can you walk me through the guesstimate?\n\n",
            "date": "Posted 4 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "Using 1xT4 GPU, we can actually infer 17B model (because I show how to infer 34B model with 2xT4 [here](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm)). When we begin with an 17B model, after 4bit quantization it becomes 10GB on disk which fits easily into 1xT4's 16GB VRAM. (UPDATE: and maybe we can infer 20B model)\n\n",
                    "date": "Posted 4 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "1",
                    "reply": []
                }
            ]
        },
        {
            "author": "Istvan Habram",
            "content": "Where is it specified that the agent needs to implement a fuction called def agent_fn(obs, cfg)?\n\n",
            "date": "Posted 6 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "We learn this from Kaggle admin's starter notebook [here](https://www.kaggle.com/code/ryanholbrook/llm-20-questions-starter-notebook) or by reading Kaggle's GitHub [here](https://github.com/Kaggle/kaggle-environments/tree/master/kaggle_environments/envs/llm_20_questions) which shows the code that runs our agents.\n\n",
                    "date": "Posted 6 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "Istvan Habram",
                            "content": "It is clear now, thank you for your quick response!\n\n",
                            "date": "Posted 6 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Toshi",
            "content": "great notebook!\n\nwhen I run this, I found the above error\n\nImportError: numpy>=1.17,<2.0 is required for a normal functioning of this module, but found numpy==2.0.0.\n\nTry: pip install transformers -U or pip install -e '.[dev]' if you're working with git main\n\nDid you find this when you run??\n\n",
            "date": "Posted 15 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "2",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "No. I have never seen that error. Are you running this notebook as is? Or changing something? Do you set the notebook option environment to \"pin to original\"? Or are you using Kaggle's latest docker. We should use \"pin to original\" if Kaggle's latest is causing errors.\n\nIf you have changed something which is causing not liking numpy==2.0.0, you can try pip installing an older numpy like pip install numpy==1.17\n\n",
                    "date": "Posted 15 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "FullEmpty",
                            "content": "[@cdeotte](https://www.kaggle.com/cdeotte) Thank you for your notebook - I always learn a lot from your posts and comments. I haven't master Kaggle notebooks, but this notebook works when copied and run, but it doesn't with the above error when downloaded, imported back and run, even on the Pin to Original Environment. \n\nBut the issue can be solved with either:\n\n!rm -rf /tmp/submission/lib/numpy-2.0.1.dist-info\n\nor, preferably,\n\nos.system(\"pip install accelerate==0.33.0 bitsandbytes==0.43.2 huggingface_hub==0.24.2 -t /tmp/submission/lib\")\n\n",
                            "date": "Posted 6 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "2",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Kartikeya Pandey",
            "content": "Great notebook I have learned alot from this\n\n",
            "date": "Posted 18 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "1",
            "reply": []
        },
        {
            "author": "El haddad Mohamed",
            "content": "Great notebook! Very well-structured and informative. 👍\n\n",
            "date": "Posted 19 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "1",
            "reply": []
        },
        {
            "author": "ISAKA Tsuyoshi",
            "content": "Hi [@cdeotte](https://www.kaggle.com/cdeotte) . Thank you for sharing such an excellent notebook!\n\nCan you tell me more about the model (Llama-3-Smaug-8B) used in this notebook? Is it superior to the llama-3 model?\n\n",
            "date": "Posted 20 days ago  ·  Posted on Version 1 of \n        1",
            "votes": "1",
            "reply": [
                {
                    "author": "Chris DeotteTopic Author",
                    "content": "Llama-3-Smaug-8B is the model Llama-3-8B-Instruct with additional finetuning. It does better than Llama-3-8B-Instruct in some ways and probably does worse in some ways. There is a research paper describing how Smaug was trained [here](https://arxiv.org/abs/2402.13228). I discovered this model because at some point in time this model was higher on the Open LLM Leaderboard than plain Llama.\n\nAnother interesting model which was doing well on Open LLM Leaderboard at one time is jondurbin/bagel-7b-v0.5 [here](https://huggingface.co/jondurbin/bagel-7b-v0.5). This is Mistral-7B with additional finetuning. There is a GitHub describing how Bagel is trained [here](https://github.com/jondurbin/bagel)\n\nAnd another model which does well on Open LLM Leaderboard is Qwen/Qwen2-7B-Instruct [here](https://huggingface.co/Qwen/Qwen2-7B-Instruct). I'm a big fan of the Qwen2 models. This v2 series just got released a month ago (previously it was v1.5 series) and quickly became the top Open LLM Leaderboard model.\n\n(For completeness, I'll mention that the classic models are Llama3, Mistral, Gemma2, and Phi3)\n\n",
                    "date": "Posted 20 days ago  ·  Posted on Version 1 of \n        1",
                    "votes": "6",
                    "reply": [
                        {
                            "author": "ISAKA Tsuyoshi",
                            "content": "Thank you for the detailed explanation! There are so many options available.\n\n",
                            "date": "Posted 20 days ago  ·  Posted on Version 1 of \n        1",
                            "votes": "1",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}