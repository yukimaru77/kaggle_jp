{
    "main_topic": {
        "author": "Raki",
        "title": "LLM overview and Llama 3 setup",
        "content": "## Evaluation of Gemma and Other LLMs\n\n### Issues with Gemma\n\nGemma 7b it quant, which is used in the starting notebook, struggles with following instructions and doesn't achieve state-of-the-art performance for its size. \n\n### Recommended LLM Evaluation: LMSYS Arena Leaderboard\n\nTo identify a robust general LLM, I recommend the [LMSYS Arena Leaderboard](https://chat.lmsys.org/). This Elo rating system involves users asking questions and comparing answers from two different models (blind) to determine which one they prefer. This evaluation method is challenging to \"game\" as success depends on consistently satisfying user queries across various topics. Other metrics can suffer from benchmark leakage into the training set and often focus on narrower tasks, making them easier to optimize for specific performance rather than general utility.\n\n### Current Top Models\n\n- GPT-4: 1287 Elo\n\n- Gemini 1.5 Pro (Google): 1248 Elo\n\n- Best Anthropic Model: 1246 Elo\n\n- LLaMA 3 70B Instruct (Meta): 1203 Elo (best open-source model, but too large)\n\n### Gemma's Performance (like in starter)\n\n- Gemma 7B-IT: 1043 Elo\n\n- Quantized Version: Slightly worse (Quantization reduces weights from formats like FP32 to INT8, significantly lowering VRAM and compute requirements at the cost of some quality)\n\n### Alternative: LLaMA 3 8B Instruct\n\n- LLaMA 3 8B Instruct: 1155 Elo (a much stronger model overall)\n\nWe still need to quantize it to run it on the T4 available for submission, which has 16 GiB of VRAM.\n\n### Best Way to Run Inference: llama.cpp\n\nAs far as I know, the best way to run inference on a non-proprietary quantized LLM is with llama.cpp. This tool employs various techniques to speed up inference, such as quantization and KV caching.\n\n### Resources for LLaMA 3 8B Instruct\n\nI have previously set up a dataset with some quantized LLaMA 3 8B Instruct models:\n\n- [Dataset](https://www.kaggle.com/datasets/raki21/meta-llama-3-8b-gguf)\n\n- [Notebook](https://www.kaggle.com/code/raki21/llama-3-gguf-with-llama-cpp) demonstrating its use, I'm in the process of adding a Q20 example, with persistence across the chat.\n\nI recommend using the 8-bit quantized variant. Although I don't have the time to integrate it with the current agent setup, I hope this writeup helps some people get started! \n\nAI Note: I wrote the text myself, but ran it through GPT4o to format it into a nicer markdown structure and correct typos.\n\n",
        "date": "Sat May 18 2024 21:29:56 GMT+0900 (日本標準時)",
        "votes": "6"
    },
    "comments": [
        {
            "author": "Melinda",
            "content": "Hi, and thanks for posting this notebook! I am trying to get llama-cpp-python working in my submission, and if I make a copy of your notebook, I'm able to run the pip install command in it. However, if I try to pip install it into a folder with the \"-t /kaggle/working/submission/lib\" option so that I can package it up, I get all kinds of dependency resolver errors. (\"ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts. (etc - a huge list)\")\n\nI'm curious if you have any tips for how to get llama-cpp-python with llama-3-8b-instruct packaged for a submission? \n\n",
            "date": "Posted 2 months ago  ·  203rd in this Competition",
            "votes": "0",
            "reply": []
        }
    ]
}