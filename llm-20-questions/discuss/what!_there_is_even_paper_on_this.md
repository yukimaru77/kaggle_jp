# what!? there is even paper on this.

**hengck23** *Fri May 24 2024 02:40:16 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (33 votes)

[https://arxiv.org/pdf/2310.01468](https://arxiv.org/pdf/2310.01468)

Probing the Multi-turn Planning Capabilities of LLMs via 20 Question Games

code and data: [https://github.com/apple/ml-entity-deduction-arena](https://github.com/apple/ml-entity-deduction-arena)

[https://arxiv.org/abs/1808.07645](https://arxiv.org/abs/1808.07645)

Playing 20 Question Game with Policy-Based Reinforcement Learning

[https://arxiv.org/pdf/2301.01743](https://arxiv.org/pdf/2301.01743)

CHATBOTS AS PROBLEM SOLVERS: PLAYING TWENTY QUESTIONS WITH ROLE REVERSALS



---

 # Comments from other users

> ## Hongbin Na
> 
> ChatGPTâ€™s Information Seeking Strategy: Insights from the 20-Questions Game
> 
> [https://aclanthology.org/2023.inlg-main.11/](https://aclanthology.org/2023.inlg-main.11/)
> 
> 
> 


---

> ## Wayne Kimutai
> 
> Kinda like Akinator
> 
> 
> 


---

> ## kartikey bartwal
> 
> I love how LLM's have their own unique world for performance metrics. About to give this contest, really excited ðŸ˜Š
> 
> 
> 


---

> ## jazivxt
> 
> I believe key is in changing the questions from 'it' to the obs.keyword before asking the LLM the yes/no question in the game as would be interpreted by anyone answering. If there is no it or keyword and the question is something like is the grass green? the answer should be no :) but if asked directly to the LLM the answer will be yes.
> 
> 
> 


---

> ## Pavithra Devi M
> 
> little interesting 
> 
> 
> 


---

