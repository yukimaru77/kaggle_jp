# Let's share LLM models for making questions and answering!

**c-number** *Mon Jul 08 2024 10:46:38 GMT+0900 (日本標準時)* (4 votes)

What models do you use?

I use google/gemma-7b-it and meta-llama/Meta-Llama-3-8B-Instruct, both 8-bit quantized.



---

 # Comments from other users

> ## Chris Deotte
> 
> The basic 5 models are Llama3, Mistral, Gemma2, Phi3, Qwen2. And two popular upgrades are Smaug and Bagel. All have versions around 7B parameter size which work well in this competition.
> 
> 
> 


---

> ## Iqbal Singh
> 
> Phi3 Mini. No fine tuning!
> 
> 
> 


---

> ## TuMinhDang
> 
> i use gemma-9b-it fineturning
> 
> 
> 


---

> ## Kasahara
> 
> I experimented with llama3-8b-it, gemma2-9b-it, gemma-7b-it, and mistral-7b. In my experiments, llama3-8b-it performed the best.
> 
> 
> 
> > ## c-numberTopic Author
> > 
> > Same impression here.
> > 
> > 
> > 


---

> ## OminousDude
> 
> I also use llama meta-llama/Meta-Llama-3-8B-Instruct as it has a very high IF-Eval score. But I chose 4-bit quantization as it works faster and lets me make my prompts and strategy more lengthy without having to worry about my agent timing out. Also, if you do not intend to keep it a secret how do you use both models is it chosen based on the category of the keyword or what?
> 
> 
> 
> > ## c-numberTopic Author
> > 
> > I only submit one of the two models for now, but am testing both of them.
> > 
> > 
> > 
> > > ## OminousDude
> > > 
> > > Oh ok! Nice Gemma 2 is pretty promising and has a very good strategy for locations. Might use it later when the actually benchmarks and a working AWQ version come out
> > > 
> > > 
> > > 


---

> ## Matthew S Farmer
> 
> Phi3 mini here. 
> 
> 
> 


---

