{
    "main_topic": {
        "author": "torino",
        "title": "How to install new torch version in kaggle submit environment?",
        "content": "Has anyone successfully installed the new torch version when submit?\n\ni try to install torch 2.3.1 when submit but torch version always is 2.1.2, my submit file like:\n\n```\n%%writefile submission/main.py\nimport os\nimport subprocess\n\nKAGGLE_AGENT_PATH = \"/kaggle_simulations/agent/\"\nKAGGLE_DATA_PATH = \"/kaggle_simulations/agent/\"\nif not os.path.exists(KAGGLE_AGENT_PATH):\n    KAGGLE_AGENT_PATH = '/kaggle/working/'\n    KAGGLE_DATA_PATH = \"/kaggle/input/\"\n\nsubprocess.run(f'pip install --no-index --find-links {KAGGLE_DATA_PATH}torch_whl torch==2.3.1', shell=True, check=True, capture_output = True)\nprint('ok torch')\nimport torch\nprint('torch', torch.__version__) # stuck in 2.1.2\n\n```\n\nthen agent log:\n\n```\n[[{\"duration\": 98.399694, \"stdout\": \"ok torch\ntorch 2.1.2\n\nok torch\ntorch 2.1.2\n\n\", \"stderr\": \"Traceback (most recent call last):\n  File \\\"/opt/conda/lib/python3.10/site-packages/kaggle_environments/agent.py\\\", line 56, in get_last_callable\n    return [v for v in env.values() if callable(v)][-1]\nIndexError: list index out of range\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \\\"/opt/conda/lib/python3.10/site-packages/kaggle_environments/agent.py\\\", line 159, in act\n    action = self.agent(*args)\n  File \\\"/opt/conda/lib/python3.10/site-packages/kaggle_environments/agent.py\\\", line 125, in callable_agent\n    agent = get_last_callable(raw_agent, path=raw) or raw_agent\n  File \\\"/opt/conda/lib/python3.10/site-packages/kaggle_environments/agent.py\\\", line 64, in get_last_callable\n    raise InvalidArgument(\\\"Invalid raw Python: \\\" + repr(e))\nkaggle_environments.errors.InvalidArgument: Invalid raw Python: IndexError('list index out of range')\n\"}]]\n\n```\n\ntorch whl dataset [torch2.3.1](https://www.kaggle.com/code/pnmanh2123/try-install-torch2-3)\n\nmy submit test notebook  [here](https://www.kaggle.com/code/pnmanh2123/try-install-torch2-3)\n\n",
        "date": "Mon Jul 15 2024 12:14:08 GMT+0900 (日本標準時)",
        "votes": "6"
    },
    "comments": [
        {
            "author": "Valerio Morelli",
            "content": "I ran into the same problem and believe it's due to the environment already importing transformers here [https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/llm_20_questions/llm_20_questions.py](https://github.com/Kaggle/kaggle-environments/blob/master/kaggle_environments/envs/llm_20_questions/llm_20_questions.py) and therefore also importing torch already as a dependency.\n\nSince the torch module is already loaded in the Python interpreter your import does not actually import the new version. I tried importlib's reload and IPythons deep reload with no success. Did you manage do find a solution?\n\n",
            "date": "Posted 3 days ago",
            "votes": "1",
            "reply": [
                {
                    "author": "mxmm2123",
                    "content": "This is also why we can't use a newer model. That clear llm_20_questions.py file always runs before the main.py file, and the main.py file was compiled before running, so we can't do anything(at least for me).\n\n",
                    "date": "Posted 3 days ago",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "torinoTopic Author",
                            "content": "Hi [@mxmm2123](https://www.kaggle.com/mxmm2123) [@vmorelli](https://www.kaggle.com/vmorelli) ,\n\nI currently still using older models, with the transformers issue I think we can only wait for support from the host.\n\n",
                            "date": "Posted 3 days ago  ·  49th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        }
    ]
}