{
    "main_topic": {
        "author": "Yuang Wu",
        "title": "Encountering another problem when submitting",
        "content": "My model using now is Gemma 2 9b it. During Validation process, the agent 0 log shows like this, and there is no content in agent 1 log.\n\n[[{\"duration\": 1.089666, \"stdout\": \"\", \"stderr\": \"Traceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\\\", line 951, in from_pretrained\\n    config_class = CONFIG_MAPPING[config_dict[\\\"model_type\\\"]]\\n  File \\\"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\\\", line 653, in __getitem__\\n    raise KeyError(key)\\nKeyError: 'gemma2'\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.10/site-packages/kaggle_environments/agent.py\\\", line 50, in get_last_callable\\n    exec(code_object, env)\\n  File \\\"/kaggle_simulations/agent/main.py\\\", line 69, in <module>\\n    config = AutoConfig.from_pretrained(model_id)\\n  File \\\"/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py\\\", line 953, in from_pretrained\\n    raise ValueError(\\nValueError: The checkpoint you are trying to load has model typegemma2but Transformers does not recognize this architecture. This\"}]]\n\nHas anyone met this problem?\n\n",
        "date": "Wed Jul 17 2024 16:15:50 GMT+0900 (日本標準時)",
        "votes": "1"
    },
    "comments": [
        {
            "author": "gwh666",
            "content": "It seems that AutoConfig can not match Gemma-2 in transformers right now,you can only use AutoModelForCasualLM to load it.\n\n",
            "date": "Posted 18 days ago  ·  34th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "Yuang WuTopic Author",
                    "content": "Wow, good advice, I will try it. Thanks gwh\n\n",
                    "date": "Posted 18 days ago  ·  234th in this Competition",
                    "votes": "0",
                    "reply": []
                },
                {
                    "author": "Yuang WuTopic Author",
                    "content": "Seems like AutoModelForCausalLM will also use AutoConfig… Now I have no idea\n\n",
                    "date": "Posted 18 days ago  ·  234th in this Competition",
                    "votes": "0",
                    "reply": [
                        {
                            "author": "gwh666",
                            "content": "model = AutoModelForCausalLM.from_pretrained(\n\n    \"google/gemma-2-9b-it\",\n\n    device_map=\"auto\",\n\n    torch_dtype=torch.bfloat16\n\n)\n\ntry it?\n\n",
                            "date": "Posted 18 days ago  ·  34th in this Competition",
                            "votes": "0",
                            "reply": []
                        }
                    ]
                }
            ]
        },
        {
            "author": "Chris Deotte",
            "content": "You will need to pip install a more recent version of Transformers that includes code for Gemma2\n\n",
            "date": "Posted 19 days ago  ·  625th in this Competition",
            "votes": "0",
            "reply": [
                {
                    "author": "Yuang WuTopic Author",
                    "content": "Yeah, I saw the advice in Huggingface, but I have already run \"pip install -U transfromers\", but still useless\n\n",
                    "date": "Posted 19 days ago  ·  234th in this Competition",
                    "votes": "0",
                    "reply": []
                }
            ]
        }
    ]
}