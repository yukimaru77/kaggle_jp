{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_dir = r\"..\\llm-20-questions\"\n",
    "file_path = file_dir + r\"\\data.md\"\n",
    "overview_dir = file_dir + \"_jp\"\n",
    "overview_path = overview_dir + r\"\\overview.md\"\n",
    "output_dir = file_dir + \"_jp\"\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_path = output_dir + r\"\\data.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import c\n",
    "from urllib import response\n",
    "import nbformat\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ResourceExhausted(Exception):\n",
    "    pass\n",
    "from prompt_toolkit import prompt\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "import re\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "with open(overview_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "system_instruction = \"\"\"あなたはプロの翻訳家でかつデータサイエンティストです。ユーザーから送られてくるkaggleのコンペティションのデータ説明を翻訳するのがあなたの役割です。ただし、日本語に翻訳する際は単に直訳するのではなく、日本語として自然な文章になるよう心がけてください。また、以下にコンペティションの情報を提供するので必要であれば用いてください。\n",
    "\n",
    "コンペティションの概要:\n",
    "```markdown\n",
    "\"\"\" + content + \"\\n```\"\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    "    system_instruction=system_instruction\n",
    ")\n",
    "\n",
    "def split_text_by_tokens(file_path, target_tokens=4000):\n",
    "\n",
    "    # ファイルの読み込み\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "\n",
    "    for line in content.split('\\n'):\n",
    "        # 空の行をスキップ\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        line_tokens = model.count_tokens(line).total_tokens\n",
    "        \n",
    "        if current_tokens + line_tokens > target_tokens and current_chunk:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = \"\"\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_chunk += line + '\\n'\n",
    "        current_tokens += line_tokens\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    #全てのチャンクのトークン数を確認\n",
    "    for chunk in chunks:\n",
    "        print(model.count_tokens(chunk).total_tokens)\n",
    "\n",
    "    return chunks\n",
    "#リクエストの処理\n",
    "def send_request(chat_session,prompt):\n",
    "    try:\n",
    "        response = chat_session.send_message(prompt)\n",
    "    except ResourceExhausted as e:\n",
    "        print(\"Resource exhausted. Waiting for 1 minute before retrying...\")\n",
    "        for i in tqdm(range(60), desc=\"Waiting\", unit=\"second\"):\n",
    "            time.sleep(1)\n",
    "        response = chat_session.send_message(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    return response\n",
    "# 使用例\n",
    "text_chunks = split_text_by_tokens(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "responses = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    chat_session = model.start_chat(history=history)\n",
    "    response = send_request(chat_session,chunk)\n",
    "    user_history = {\"role\": \"user\", \"parts\": chunk}\n",
    "    model_history = {\"role\": \"model\", \"parts\": response.text}\n",
    "    history.append(user_history)\n",
    "    history.append(model_history)\n",
    "    responses.append(response.text)\n",
    "    print(f\"Chunk {i+1} response:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#responsesを結合してファイルに書き込む\n",
    "with open(output_path, 'w', encoding='utf-8') as file:\n",
    "    for response in responses:\n",
    "        file.write(response + '\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
