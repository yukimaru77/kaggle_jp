{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file_dir = r\"..\\llm-20-questions\"\n",
    "origin_notebook_dir = file_dir + r\"\\kernels\"\n",
    "overview_dir = file_dir + \"_jp\"\n",
    "overview_path = overview_dir + r\"\\overview.md\"\n",
    "output_dir = file_dir + \"_jp\"\n",
    "output_discuss_dir = output_dir + r\"\\kernels\"\n",
    "if not os.path.exists(output_discuss_dir):\n",
    "    os.makedirs(output_discuss_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_directories(path):\n",
    "    return [d for d in os.listdir(path) if os.path.isdir(os.path.join(path, d))]\n",
    "def list_extension_files(dirs,extension='.ipynb'):\n",
    "    ipynb_files = []\n",
    "    exit_indexs = []\n",
    "    for i,dir in enumerate(dirs):\n",
    "        with os.scandir(dir) as entries:\n",
    "            for entry in entries:\n",
    "                if entry.is_file() and entry.name.endswith(extension):\n",
    "                    ipynb_files.append(entry.path)\n",
    "                    exit_indexs.append(i)\n",
    "                    break\n",
    "    return ipynb_files,exit_indexs\n",
    "\n",
    "notebook_dirs = list_directories(origin_notebook_dir)\n",
    "output_dirs = [os.path.join(output_discuss_dir+\"\\\\\",dir) for dir in notebook_dirs]\n",
    "for output_dir in output_dirs:\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "notebook_dirs = [os.path.join(origin_notebook_dir+\"\\\\\",dir) for dir in notebook_dirs]\n",
    "notebook_files,exit_indexs = list_extension_files(notebook_dirs)\n",
    "tmp,tmp_ = [],[]\n",
    "for i in exit_indexs:\n",
    "    tmp.append(notebook_dirs[i])\n",
    "    tmp_.append(output_dirs[i])\n",
    "notebook_dirs = tmp\n",
    "output_dirs = tmp_\n",
    "# notebook_filesのファイル名を取得\n",
    "notebook_file_names = [os.path.basename(file) for file in notebook_files]\n",
    "#output_dirsとnotebook_file_namesを結合\n",
    "output_files = [os.path.join(output_dirs[i],notebook_file_names[i]) for i in range(len(notebook_files))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#output_dirsの中にファイルがないインデックス番号を取得\n",
    "empty_dirs = [i for i in range(len(output_dirs)) if not os.listdir(output_dirs[i])]\n",
    "\n",
    "#空の物のみを取得\n",
    "output_files = [output_files[i] for i in empty_dirs]\n",
    "notebook_files = [notebook_files[i] for i in empty_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from calendar import c\n",
    "from urllib import response\n",
    "import nbformat\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def remove_python_markers(text):\n",
    "    lines = text.split('\\n')\n",
    "    if lines[0] == '```python':\n",
    "        lines = lines[1:]\n",
    "    if lines[-1] == '```':\n",
    "        lines = lines[:-1]\n",
    "    return '\\n'.join(lines)\n",
    "\n",
    "class ResourceExhausted(Exception):\n",
    "    pass\n",
    "from prompt_toolkit import prompt\n",
    "os.environ[\"GEMINI_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=os.environ[\"GEMINI_API_KEY\"])\n",
    "import re\n",
    "from google.generativeai.types import HarmBlockThreshold, HarmCategory\n",
    "\n",
    "generation_config = {\n",
    "    \"temperature\": 0,\n",
    "    \"top_p\": 0.95,\n",
    "    \"top_k\": 64,\n",
    "    \"max_output_tokens\": 8192,\n",
    "    \"response_mime_type\": \"text/plain\",\n",
    "}\n",
    "\n",
    "with open(overview_path, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "system_instruction = \"\"\"あなたはプロの翻訳家兼プログラマーです。ユーザーから送られてくるkaggleの公開jupyter notebookの一部のセル群をそれぞれ日本語に翻訳するのがあなたの役割です。具体的にはマークダウンセル内の文章を日本語に翻訳したり、コードセル内のコードのコメントを日本語に翻訳してコメントが日本語になったコードを出力してください。ただし、コードセル内のコードそのものは絶対に変更や追記・削除やはしてはいけません。ただし追記の特例として、python初学者でもコード理解が出来るように、理解を促す簡潔なコメントをpythonのコメント形式#を用いて各コードの上に日本語で挿入し、理解を促すのは大丈夫です。むしろこれはあなたの役割なので、こちらに関しては積極的に行ってください。また、日本語に翻訳する際は単に直訳するのではなく、日本語として自然な文章になるよう心がけてください。これらの内容以外は出力せず、また出力の形式はユーザーの入力の形式と絶対に同じになるように、---Here, the cells separate. The following area is a {Markdown|Code} cell---\\nで定義される特殊な文字列で区切ってからそれぞれ出力することを忘れないでください。---Here, the cells separate. The following area is a {Markdown|Code} cell---で囲まれた領域が一つのセルになるので、一つ抜けるだけで意味が大きく変わります。例えば、マークダウンセル、コードセル、マークダウンセル、コードセルの場合以下のような入力が与えられます。\n",
    "\n",
    "ユーザーからの入力例(マークダウンセル、コードセル、マークダウンセル、コードセルの場合):\n",
    "---Here, the cells separate. The following area is a Markdown cell---\n",
    "{マークダウンセルの内容}\n",
    "---Here, the cells separate. The following area is a Code cell---\n",
    "{コードセルに書かれているコード}\n",
    "---Here, the cells separate. The following area is a Markdown cell---\n",
    "{マークダウンセルの内容}\n",
    "---Here, the cells separate. The following area is a Code cell---\n",
    "{コードセルに書かれているコード}\n",
    "\n",
    "上記の入力に対して期待する出力例:\n",
    "---Here, the cells separate. The following area is a Markdown cell---\n",
    "{マークダウンセルの内容の日本語訳したもの}\n",
    "---Here, the cells separate. The following area is a Code cell---\n",
    "{コードセルに書かれているコメントを日本語訳し、さらに分かりやすくなるようにコメントを追加したもの}\n",
    "---Here, the cells separate. The following area is a Markdown cell---\n",
    "{マークダウンセルの内容の日本語訳したもの}\n",
    "---Here, the cells separate. The following area is a Code cell---\n",
    "{コードセルに書かれているコメントを日本語訳し、さらに分かりやすくなるようにコメントを追加したもの}\n",
    "\n",
    "また、チャットの履歴は、送った内容以前のセルです。つまりユーザーから入力されたセル以前より前の内容なので内容としてはつながっているので適宜参考にしてください。また、コンペティションの概要は以下の通りです。文脈の理解に必要であれば用いて構いません。\n",
    "\n",
    "コンペティションの概要:\n",
    "```markdown\n",
    "\"\"\" + content + \"\\n```\"\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    model_name=\"gemini-1.5-pro\",\n",
    "    generation_config=generation_config,\n",
    "    system_instruction=system_instruction,\n",
    "    safety_settings={\n",
    "        HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "    }\n",
    "\n",
    ")\n",
    "\n",
    "chat_session = model.start_chat(history=[])\n",
    "\n",
    "#リクエストの処理\n",
    "def send_request(chat_session,prompt):\n",
    "    try:\n",
    "        response = chat_session.send_message(prompt)\n",
    "    except ResourceExhausted as e:\n",
    "        print(\"Resource exhausted. Waiting for 1 minute before retrying...\")\n",
    "        for i in tqdm(range(60), desc=\"Waiting\", unit=\"second\"):\n",
    "            time.sleep(1)\n",
    "        response = chat_session.send_message(prompt)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    return response\n",
    "\n",
    "def merge_markdown_cells(input_path, output_path=None):\n",
    "    with open(input_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    new_cells = []\n",
    "    current_markdown = \"\"\n",
    "\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            if current_markdown:\n",
    "                current_markdown += \"\\n\\n\"  # 各マークダウンセル間に空行を追加\n",
    "            current_markdown += cell.source\n",
    "        else:\n",
    "            if current_markdown:\n",
    "                new_cells.append(nbformat.v4.new_markdown_cell(current_markdown))\n",
    "                current_markdown = \"\"\n",
    "            new_cells.append(cell)\n",
    "\n",
    "    # 最後のマークダウンセルがある場合、それを追加\n",
    "    if current_markdown:\n",
    "        new_cells.append(nbformat.v4.new_markdown_cell(current_markdown))\n",
    "\n",
    "    notebook.cells = new_cells\n",
    "    if output_path:\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            nbformat.write(notebook, f)\n",
    "    return notebook\n",
    "\n",
    "def translate_batch(batch,historys=[]):\n",
    "    historys_ = []\n",
    "    for history in historys:\n",
    "        user_history = {\"role\": \"user\", \"parts\": history[0]}\n",
    "        model_history = {\"role\": \"model\", \"parts\": history[1]}\n",
    "        historys_.append(user_history)\n",
    "        historys_.append(model_history)\n",
    "    chat_session = model.start_chat(history=historys_)\n",
    "    #\n",
    "    input_text = \"\\n\".join([f\"---Here, the cells separate. The following area is a {cell_type} cell---\\n{content}\" for cell_type, content in batch])\n",
    "    print(\"モデルに送るテキスト\")\n",
    "    print(input_text)\n",
    "    response = send_request(chat_session,input_text)\n",
    "    print(\"モデルからの返答\")\n",
    "    print(response.text)\n",
    "    historys.append([input_text,response.text])\n",
    "    # ---Markdown---\\nまたは---Code---\\nで区切る。正規表現で分割する\n",
    "    response_parts = re.split(r'---Here, the cells separate. The following area is a (Markdown|Code) cell---\\n', response.text)\n",
    "    response_parts = response_parts[1:]  # 最初の要素は空文字列なので除外\n",
    "    # 奇数番目だけ取り出す\n",
    "    response_parts = response_parts[1::2]\n",
    "    return response_parts,historys\n",
    "\n",
    "def get_md_files_in_same_directory(path):\n",
    "    # 指定されたパスのディレクトリを取得\n",
    "    directory = os.path.dirname(path)\n",
    "    \n",
    "    # 指定されたディレクトリ内のファイルとディレクトリのリストを取得\n",
    "    files_and_dirs = os.listdir(directory)\n",
    "    \n",
    "    # .mdファイルのリストを作成\n",
    "    md_files = [os.path.join(directory, f) for f in files_and_dirs if f.endswith('.md')]\n",
    "    \n",
    "    return md_files\n",
    "def translate_notebook(notebook_path, output_path, max_tokens=3000):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "    md_paths = get_md_files_in_same_directory(notebook_path)\n",
    "    if len(md_paths) > 0:\n",
    "        md_path = md_paths[0]\n",
    "        with open(md_path, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "            content = \"---\\n\\n\" + content\n",
    "            #ノートブックの末尾に追加\n",
    "            content_cell = nbformat.v4.new_markdown_cell(content)\n",
    "            notebook.cells.append(content_cell)\n",
    "\n",
    "    cells_to_translate = []\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type in ['markdown', 'code']:\n",
    "            cells_to_translate.append((cell.cell_type.capitalize(), cell.source))\n",
    "\n",
    "    translated_cells = []\n",
    "    historys = []\n",
    "    current_batch = []\n",
    "    current_tokens = 0\n",
    "    # 累積トークン数(最後に表示するため)\n",
    "    total_tokens = 0\n",
    "    model = genai.GenerativeModel( # 文字数カウント用\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "    )\n",
    "    for cell_type, content in cells_to_translate:\n",
    "        cell_text = f\"---Here, the cells separate. The following area is a {cell_type} cell---\\n{content}\"\n",
    "        cell_tokens = model.count_tokens(cell_text).total_tokens\n",
    "\n",
    "        if current_tokens + cell_tokens > max_tokens and current_batch:\n",
    "            # 現在のバッチを翻訳\n",
    "            total_tokens += current_tokens\n",
    "            translated_batch, historys = translate_batch(current_batch, historys)\n",
    "            translated_cells.extend(translated_batch)\n",
    "            current_batch = []\n",
    "            current_tokens = 0\n",
    "\n",
    "        current_batch.append((cell_type, content))\n",
    "        current_tokens += cell_tokens\n",
    "\n",
    "    # 残りのセルを処理\n",
    "    if current_batch:\n",
    "        total_tokens += current_tokens\n",
    "        translated_batch, historys = translate_batch(current_batch, historys)\n",
    "        translated_cells.extend(translated_batch)\n",
    "    translated_index = 0\n",
    "    print(len(translated_cells))\n",
    "    print(len([cell for cell in notebook.cells if cell.cell_type in ['markdown', 'code']]))\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown' or cell.cell_type == 'code':\n",
    "            print(translated_index)\n",
    "            if cell.cell_type == \"code\":\n",
    "                cell.source = remove_python_markers(translated_cells[translated_index].strip())\n",
    "            else:\n",
    "                cell.source = translated_cells[translated_index]\n",
    "            translated_index += 1\n",
    "        # その他のセルタイプ（出力、図など）はそのまま保持\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "\n",
    "\n",
    "\n",
    "# ノートブックの要約を生成\n",
    "def summarize_notebook(notebook_path):\n",
    "    with open(notebook_path, 'r', encoding='utf-8') as f:\n",
    "        notebook = nbformat.read(f, as_version=4)\n",
    "\n",
    "    text = \"\"\n",
    "    for cell in notebook.cells:\n",
    "        if cell.cell_type == 'markdown':\n",
    "            text += \"マークダウンセル :\\n\"+cell.source+\"\\n\\n\"\n",
    "        elif cell.cell_type == 'code':\n",
    "            text += \"コードセル :\\n\"+cell.source+\"\\n\\n\"\n",
    "    system_instruction = \"\"\"あなたはプロのプログラマーです。ユーザーから送られてくるkaggleの公開jupyter notebookに関するタスクを遂行してください。また、コンペティションの概要は以下の通りです。文脈の理解に必要であれば用いて構いません。\n",
    "\n",
    "    コンペティションの概要:\n",
    "    ```markdown\n",
    "    \"\"\" + content + \"\\n```\"\n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=\"gemini-1.5-pro\",\n",
    "        generation_config=generation_config,\n",
    "        system_instruction=system_instruction,\n",
    "        safety_settings={\n",
    "            HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
    "            HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
    "        }\n",
    "    )\n",
    "    chat_session = model.start_chat(history=[])\n",
    "    total_tokens = 0\n",
    "    prompt_summary = \"以下に示すjupyter notebookの要約を生成してください。特にそのnotebookはどのような問題に取り組み、どのような手法やライブラリで解決しているのかは必ず要約に含めてください。\\n\\n ノートブックの内容 :\\n\"+text\n",
    "    prompt_know = \"以下に示すjupyter notebookのを読んで、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説を列挙してください。ただし、初心者なだけで大学の学部でやるようなレベルの機械学習・深層学習の知識はあるので簡単なものや有名なものは説明不要です。ある程度マイナーなものや、実務を経験していないと馴染みのないもの、このノートブック特有のドメイン知識等に焦点を当ててください。\\n\\n ノートブックの内容 :\\n\"+text\n",
    "    total_tokens += model.count_tokens(prompt_summary).total_tokens\n",
    "    response = send_request(chat_session,prompt_summary)\n",
    "    summary = response.text\n",
    "    summary = \"# 要約 \\n\"+summary+\"\\n\\n---\\n\"\n",
    "    # 要約を既存のnotebookの先頭にマークダウンセルとして追加\n",
    "    summary_cell = nbformat.v4.new_markdown_cell(summary)\n",
    "    notebook.cells.insert(0, summary_cell)\n",
    "    total_tokens += model.count_tokens(prompt_summary).total_tokens\n",
    "    response = send_request(chat_session,prompt_know)\n",
    "    known = response.text\n",
    "    known = \"# 用語概説 \\n\"+known+\"\\n\\n---\\n\"\n",
    "    # 要約を既存のnotebookの先頭にマークダウンセルとして追加\n",
    "    known_cell = nbformat.v4.new_markdown_cell(known)\n",
    "    notebook.cells.insert(1, known_cell)\n",
    "    print(f\"Total tokens: {total_tokens}\")\n",
    "    with open(notebook_path, 'w', encoding='utf-8') as f:\n",
    "        nbformat.write(notebook, f)\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(notebook_files)):\n",
    "    _ = merge_markdown_cells(notebook_files[i], notebook_files[i])\n",
    "    _ = translate_notebook(notebook_files[i], output_files[i])\n",
    "    _ = summarize_notebook(output_files[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kaggle-scraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
