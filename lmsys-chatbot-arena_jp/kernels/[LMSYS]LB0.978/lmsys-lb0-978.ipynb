{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "869d7900",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、KaggleのLMSYS - Chatbot Arenaコンペティションにおいて、ユーザーの応答の好みを予測するための機械学習モデルを構築することに取り組んでいます。特に、二つの言語モデルの応答のうちどちらが好まれるかを予測するための特徴量を生成し、最終的にモデルの予測を結合するアプローチを取っています。\n",
    "\n",
    "### 主要な手法とライブラリ\n",
    "1. **使用ライブラリ**:\n",
    "   - `transformers`: 言語モデルを扱うためのライブラリ（例: Llamaモデルを利用）。\n",
    "   - `torch`: PyTorchでの深層学習。\n",
    "   - `lightgbm`: 効率的な勾配ブースティングフレームワーク。\n",
    "   - `pandas`: データ操作用。\n",
    "\n",
    "2. **モデルの設定**:\n",
    "   - Llamaモデル（LlamaForSequenceClassification）を利用し、CUDAデバイスを使用して8ビットでモデルを読み込み、メモリ効率を向上させる設定を施しています。\n",
    "   - LoRa（Low-Rank Adaptation）方式を使ってモデルの重みを適用しており、推論時にモデルの評価を行います。\n",
    "\n",
    "3. **データ準備と前処理**:\n",
    "   - テストデータとサンプル提出ファイルを読み込み、応答を処理する関数を定義して、テキストを適切に整形します。\n",
    "   - トークナイザーを用いてテキストをトークン化し、注意マスクを生成します。\n",
    "\n",
    "4. **推論プロセス**:\n",
    "   - 推論を効率的に行うためにバッチ処理を行い、複数のスレッドを用いて異なるGPU上でモデルを並行して実行します。\n",
    "   - 各モデルから得られた確率をデータフレームに保存し、最終的な出力を構築します。\n",
    "\n",
    "5. **特徴量生成**:\n",
    "   - LightGBMとCountVectorizerを使用して特徴量を抽出し、応答の差異やプロンプトの統計的特性を計算します。\n",
    "\n",
    "6. **予測の融合**:\n",
    "   - LlamaモデルとLightGBMモデルからの予測をブレンドし、最終的な予測結果を生成します。\n",
    "\n",
    "### 結果の保存\n",
    "生成した予測結果は、`submission.csv`というファイル名で保存され、Kaggleに提出できる形式に整形されています。\n",
    "\n",
    "このNotebookは、現実のユーザープロフィールや対話の特性を考慮した深層学習モデルと機械学習モデルの組み合わせにより、ユーザーの好みを予測するための実用的なアプローチを示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79b88da",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、機械学習・深層学習ノートブックに特有で、初心者がつまずきそうな専門用語の解説です。\n",
    "\n",
    "1. **BitsAndBytesConfig**: これは、モデルのメモリ使用量を削減するための設定を管理するクラスです。特に、8ビットモードでのモデルの読み込みや計算を可能にし、メモリ効率を向上させます。\n",
    "\n",
    "2. **LoRa (Low-Rank Adaptation)**: LoRaは、モデルの重みを更新するために使用される手法で、特に小さな計算コストでモデルの適応を行える方法です。この方法では、モデルの特定の部分を選択的に適応させ、全体の構造を保持しつつ新しいタスクへの微調整を行います。\n",
    "\n",
    "3. **PeftModel/PeftConfig**: これらは、PEFT（Parameter-Efficient Fine-Tuning）に関連するクラスで、元のモデルのパラメータを必要最小限だけ変更して、新しいタスクに適応させるための設定を定義します。これにより、全てのパラメータを訓練するよりも効率よくモデルを適応させることができます。\n",
    "\n",
    "4. **autocast**: PyTorchにおける自動混合精度（Automatic Mixed Precision）のためのコンテキストマネージャーです。異なる精度のデータ型間で計算を自動的に調整し、トレーニングや推論の速度を向上させると同時に、メモリの使用量も削減します。\n",
    "\n",
    "5. **attention mask**: 自然言語処理においてトークンが有効か無効かを示すバイナリのマスクで、各トークンの重要度や有効性を制御します。特に、パディングトークンが無視されるように指定します。\n",
    "\n",
    "6. ** symlog変換**: 対数変換の一種で、値がゼロや負の数でも計算できるようにするための変換方法です。負の値を適切に扱うため、数値の符号を保ちつつ対数を取ることができます。\n",
    "\n",
    "7. **CountVectorizer**: テキストデータを数値データに変換するためのツールで、文書中の単語の出現回数をカウントし、特徴量として利用します。これは主に、機械学習アルゴリズムにテキスト情報を入力するために使用されます。\n",
    "\n",
    "8. **Softmax**: 多クラス分類のための出力層で、モデルの出力を確率に変換する関数です。各クラスに属する確率を表すため、出力が正規化された状態になります。\n",
    "\n",
    "これらの用語の理解は、ノートブック内で使われている技術と手法を理解するために重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:31.881515Z",
     "iopub.status.busy": "2024-06-13T15:44:31.881147Z",
     "iopub.status.idle": "2024-06-13T15:45:25.464117Z",
     "shell.execute_reply": "2024-06-13T15:45:25.462947Z",
     "shell.execute_reply.started": "2024-06-13T15:44:31.881486Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なパッケージをインストールします\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860bad97",
   "metadata": {},
   "source": [
    "このノートブックの作業は、以下のノートブックに触発されています：\n",
    "* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n",
    "* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "## もしこれが役に立ったら、いいねを押してくれると嬉しいです\n",
    "\n",
    "## ライブラリのインポート\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.805645Z",
     "iopub.status.busy": "2024-06-13T15:45:31.805368Z",
     "iopub.status.idle": "2024-06-13T15:45:31.816648Z",
     "shell.execute_reply": "2024-06-13T15:45:31.815718Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.805621Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from IPython.display import display\n",
    "import torch.nn.functional as F\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.818268Z",
     "iopub.status.busy": "2024-06-13T15:45:31.817891Z",
     "iopub.status.idle": "2024-06-13T15:45:31.825778Z",
     "shell.execute_reply": "2024-06-13T15:45:31.824978Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.818231Z"
    }
   },
   "outputs": [],
   "source": [
    "# CUDAのメモリ効率を向上させる設定を有効にします\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデル名の指定\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'  # 重みのパス\n",
    "MAX_LENGTH = 2048  # 最大シーケンス長\n",
    "BATCH_SIZE = 4  # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # GPUデバイスの指定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0030d5a2",
   "metadata": {},
   "source": [
    "## データの準備\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.827124Z",
     "iopub.status.busy": "2024-06-13T15:45:31.826869Z",
     "iopub.status.idle": "2024-06-13T15:45:31.847758Z",
     "shell.execute_reply": "2024-06-13T15:45:31.846742Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.8271Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータとサンプル提出ファイルを読み込む\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.849516Z",
     "iopub.status.busy": "2024-06-13T15:45:31.849026Z",
     "iopub.status.idle": "2024-06-13T15:45:31.883908Z",
     "shell.execute_reply": "2024-06-13T15:45:31.882997Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.849481Z"
    }
   },
   "outputs": [],
   "source": [
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)  # 文をスペースで結合して返す\n",
    "\n",
    "# 各カラムにprocess関数を適用\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# サンプル提出ファイルとテストデータの先頭5行を表示\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# モデル用のテキストを準備\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])  # テキストの最初の行を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd5338",
   "metadata": {},
   "source": [
    "## トークナイズ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.40071Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40106Z",
     "shell.execute_reply": "2024-06-13T15:44:22.400895Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.400882Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークナイザーの読み込み\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# テキストをトークン化し、パディングや切り捨てを適用\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "# 入力IDと注意マスクをデバイスに移動\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "# データフレーム作成\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu  # 入力IDを追加\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu  # 注意マスクを追加\n",
    "data[:2]  # 最初の2行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffa8092",
   "metadata": {},
   "source": [
    "## モデルの読み込み\n",
    "> 各GPUに1つのモデルを読み込みます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.403623Z",
     "iopub.status.idle": "2024-06-13T15:44:22.404009Z",
     "shell.execute_reply": "2024-06-13T15:44:22.403846Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.403831Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットでの読み込みを有効に\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 8ビット計算のデータ型\n",
    "    bnb_8bit_use_double_quant=False)  # 二重量子化を無効に\n",
    "\n",
    "# GPU 0にベースモデルを読み込み\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # クラスラベル数\n",
    "    torch_dtype=torch.float16,  # 計算に使用するデータ型\n",
    "    quantization_config=bnb_config,  # 量子化設定\n",
    "    device_map='cuda:0')  # デバイスマップの設定\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディング用トークンIDを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.405382Z",
     "iopub.status.idle": "2024-06-13T15:44:22.405698Z",
     "shell.execute_reply": "2024-06-13T15:44:22.405553Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.405539Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU 1にベースモデルを読み込み\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # クラスラベル数\n",
    "    torch_dtype=torch.float16,  # 計算に使用するデータ型\n",
    "    quantization_config=bnb_config,  # 量子化設定\n",
    "    device_map='cuda:1')  # デバイスマップの設定\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディング用トークンIDを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b851b53",
   "metadata": {},
   "source": [
    "## 重みの読み込み\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.407084Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40743Z",
     "shell.execute_reply": "2024-06-13T15:44:22.407278Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.407264Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRaの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # ランク\n",
    "    lora_alpha=32,  # LoRaのアルファ値\n",
    "    lora_dropout=0.10,  # ドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効に\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    "    target_modules=['o_proj', 'v_proj'])  # 対象モジュールの指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.408351Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4087Z",
     "shell.execute_reply": "2024-06-13T15:44:22.408536Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.408521Z"
    }
   },
   "outputs": [],
   "source": [
    "# PEFTモデルの取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "# 重みを読み込む\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()  # 評価モードに設定\n",
    "\n",
    "# 学習可能なパラメータを出力\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.410059Z",
     "iopub.status.idle": "2024-06-13T15:44:22.410418Z",
     "shell.execute_reply": "2024-06-13T15:44:22.410257Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.410242Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()  # ガーベジコレクションを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1454507d",
   "metadata": {},
   "source": [
    "## 推論\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.411343Z",
     "iopub.status.idle": "2024-06-13T15:44:22.411663Z",
     "shell.execute_reply": "2024-06-13T15:44:22.411513Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.4115Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルに変換\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # 注意マスクをテンソルに変換\n",
    "    \n",
    "    generated_class_a = []  # モデルAの生成したクラス確率を保存\n",
    "    generated_class_b = []  # モデルBの生成したクラス確率を保存\n",
    "    generated_class_c = []  # タイのクラス確率を保存\n",
    "\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):  # バッチ処理でデータを処理\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチの注意マスクをデバイスに転送\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算を無効に\n",
    "            with autocast():  # 自動混合精度を有効に\n",
    "                outputs = model(  # モデルに入力IDと注意マスクを渡して出力を得る\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # 出力をソフトマックスで確率に変換\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])  # モデルAの確率を追加\n",
    "        generated_class_b.extend(probabilities[:, 1])  # モデルBの確率を追加\n",
    "        generated_class_c.extend(probabilities[:, 2])  # タイの確率を追加\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a  # モデルAの勝者確率を追加\n",
    "    df['winner_model_b'] = generated_class_b  # モデルBの勝者確率を追加\n",
    "    df['winner_tie'] = generated_class_c  # タイの勝者確率を追加\n",
    "\n",
    "    torch.cuda.empty_cache()  # CUDAキャッシュをクリア\n",
    "\n",
    "    return df  # 結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.412909Z",
     "iopub.status.idle": "2024-06-13T15:44:22.413243Z",
     "shell.execute_reply": "2024-06-13T15:44:22.413073Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41306Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時刻を記録\n",
    "\n",
    "N_SAMPLES = len(data)  # サンプル数を取得\n",
    "\n",
    "# データを2つのサブセットに分割\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()  # 最初の半分をサブ1に\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りをサブ2に\n",
    "\n",
    "# スレッドで推論を実行する関数\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 推論結果を保存\n",
    "\n",
    "# スレッド用のリストを作成\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.414669Z",
     "iopub.status.idle": "2024-06-13T15:44:22.414992Z",
     "shell.execute_reply": "2024-06-13T15:44:22.414841Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.414828Z"
    }
   },
   "outputs": [],
   "source": [
    "# スレッドの開始\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # サブ1の推論スレッド\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # サブ2の推論スレッド\n",
    "\n",
    "t0.start()  # スレッドを開始\n",
    "t1.start()  # スレッドを開始\n",
    "\n",
    "# 全てのスレッドが完了するのを待つ\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のデータフレームに結合\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"処理が完了しました。トータル時間: {time.time() - st}\")  # 処理完了時刻を出力\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲット列を指定\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]  # サンプル提出ファイルに予測値を追加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.416588Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41692Z",
     "shell.execute_reply": "2024-06-13T15:44:22.416764Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41675Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values  # 予測値を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d9523a5",
   "metadata": {},
   "source": [
    "## LGBM + tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.417972Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41834Z",
     "shell.execute_reply": "2024-06-13T15:44:22.418143Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.418129Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ\n",
    "RUNPOD = os.path.exists('/workspace/')  # 実行環境がワークポッドか確認\n",
    "KAGGLE = not RUNPOD  # 実行環境がKaggleか確認\n",
    "if KAGGLE: \n",
    "    print('kaggle')  # Kaggle環境である旨を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.419857Z",
     "iopub.status.idle": "2024-06-13T15:44:22.420268Z",
     "shell.execute_reply": "2024-06-13T15:44:22.420061Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.420047Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd  # pandasをインポート\n",
    "except:\n",
    "    # 必要なパッケージがインポートできない場合、インストールする\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.421762Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4221Z",
     "shell.execute_reply": "2024-06-13T15:44:22.421948Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.421934Z"
    }
   },
   "outputs": [],
   "source": [
    "# データパスの指定\n",
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "# ワークポッド環境の場合、Kaggle APIキーの設定\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/  # コンペティションのデータをダウンロード\n",
    "        \n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ダウンロードしたzipファイルを展開"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.42329Z",
     "iopub.status.idle": "2024-06-13T15:44:22.423599Z",
     "shell.execute_reply": "2024-06-13T15:44:22.423456Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.423443Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = '/kaggle/input/'  # Kaggleの入力パス\n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'  # モデルパスを設定\n",
    "print(MODEL_PATH)  # モデルパスを出力\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスの設定\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.425238Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42556Z",
     "shell.execute_reply": "2024-06-13T15:44:22.425407Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.425394Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # トークナイザーの並列処理を無効に"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.426467Z",
     "iopub.status.idle": "2024-06-13T15:44:22.426784Z",
     "shell.execute_reply": "2024-06-13T15:44:22.426631Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.426618Z"
    }
   },
   "outputs": [],
   "source": [
    "# データを読み込む\n",
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # トレーニングデータを読み込み\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))  # テストデータを読み込み\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込み\n",
    "print(len(train), len(test))  # トレーニングデータとテストデータの行数を出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.427766Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42813Z",
     "shell.execute_reply": "2024-06-13T15:44:22.427955Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.42793Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}  # パラメータを初期化\n",
    "if False: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    params['fold'] = -1  # フォールド設定\n",
    "\n",
    "params['n_epochs'] = 1  # エポック数を設定\n",
    "params['n_lgb'] = 1  # LightGBMモデル数を設定\n",
    "params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.429052Z",
     "iopub.status.idle": "2024-06-13T15:44:22.429388Z",
     "shell.execute_reply": "2024-06-13T15:44:22.429238Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.429225Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータを設定\n",
    "FULL = params.get('fold', 0) < 0  # フルデータ使用フラグを設定\n",
    "N_FOLDS = int(params.get('n_folds', 3));  # フォールド数を設定\n",
    "FOLD = int(params.get('fold', 0))  # 現在のフォールドを設定\n",
    "SEED = int(params.get('seed', 3))  # シード値を設定\n",
    "SS = int(params.get('subsample', 1))  # サブサンプリング設定\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)  # 設定したパラメータを出力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:22.459744Z",
     "iopub.status.busy": "2024-06-13T15:44:22.459023Z",
     "iopub.status.idle": "2024-06-13T15:44:23.504374Z",
     "shell.execute_reply": "2024-06-13T15:44:23.502893Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.45971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Stratified K-Foldを使用してフォールドを取得する関数\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))  # 表示用のフォールドを取得\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]  # フォールドのIDを取得\n",
    "if SS > 1:\n",
    "    train_ids, test_ids = train_ids[::SS], test_ids[::SS]  # サブサンプリングを適用\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # データが重複していないことを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.505134Z",
     "iopub.status.idle": "2024-06-13T15:44:23.505514Z",
     "shell.execute_reply": "2024-06-13T15:44:23.505348Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.505333Z"
    }
   },
   "outputs": [],
   "source": [
    "# ランダムシードを現在のマイクロ秒で設定\n",
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.506746Z",
     "iopub.status.idle": "2024-06-13T15:44:23.507061Z",
     "shell.execute_reply": "2024-06-13T15:44:23.50692Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.506907Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN = False  # トレーニングモードのフラグ\n",
    "INFER = True   # 推論モードのフラグ \n",
    "SAVE = False   # 保存モードのフラグ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:23.666596Z",
     "iopub.status.busy": "2024-06-13T15:44:23.666257Z",
     "iopub.status.idle": "2024-06-13T15:44:27.845064Z",
     "shell.execute_reply": "2024-06-13T15:44:27.844121Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.66657Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # LightGBMをインポート\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # 文書のカウントベクトル化をインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:27.847295Z",
     "iopub.status.busy": "2024-06-13T15:44:27.846672Z",
     "iopub.status.idle": "2024-06-13T15:44:27.985709Z",
     "shell.execute_reply": "2024-06-13T15:44:27.984569Z",
     "shell.execute_reply.started": "2024-06-13T15:44:27.847259Z"
    }
   },
   "outputs": [],
   "source": [
    "LGB = True  # LightGBMを使用するフラグ\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # トレーニングフラグの設定\n",
    "INFER_LGB = not TRAIN and LGB  # 推論フラグの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前に保存したCountVectorizerを読み込む\n",
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # カウントベクトライザーを読み込み\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # 追加のカウントベクトライザーを読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンメトリック対数変換を定義\n",
    "def symlog(x):\n",
    "    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "# 行列を密な形式に変換し、symlog変換を適用\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())\n",
    "    x = symlog(x)\n",
    "    return x\n",
    "\n",
    "# 特徴量を取得する関数\n",
    "def get_features(df):\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])  # プロンプトの特徴量\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])  # モデルAの応答の特徴量\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])  # モデルBの応答の特徴量\n",
    "    \n",
    "    v = np.hstack([\n",
    "          afeat - bfeat, np.abs(afeat - bfeat),  # A応答とB応答の差を特徴量に追加\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # 投票モデルの数で正規化\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    extras = []  # 追加の特徴量リスト\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']  # 追加の特徴量に使用する文字列\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)  # 各列での文字のカウント\n",
    "    \n",
    "    extras.append(df[c].str.len())  # 各列の長さを追加\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 各列の単語数を追加\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)  # 追加特徴量をスタック\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 特徴量を平方根と対数変換\n",
    "    return np.hstack([v, extras])  # 特徴量を結合して返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みLightGBMモデルを読み込む\n",
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論モードでLightGBMを使用する設定\n",
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test  # テストデータを使用\n",
    "    yps = []  # 予測結果を格納するリスト\n",
    "    b = 1000  # バッチサイズ\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得\n",
    "        ypms = []  # 各モデルの予測を格納するリスト\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))  # 各モデルの確率を予測\n",
    "        yps.append(np.stack(ypms).mean(0))  # 各モデルの平均確率を計算\n",
    "        print('.', end = '')  # 進捗を表示\n",
    "        \n",
    "        if len(yps) % 2 == 0:  # 2回ごとにガーベジコレクションを実行\n",
    "            gc.collect()\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)  # 予測結果を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds = yp  # LightGBMの予測結果を格納"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2b7cd3",
   "metadata": {},
   "source": [
    "## 予測のブレンド\n",
    "\n",
    "$\\operatorname{preds} = 0.05 \\cdot \\operatorname{lgbm \\, boosting \\, preds} + 0.8 \\cdot \\operatorname{llama \\, preds}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_wt = 0.05  # LightGBMの重みを設定\n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 重みに基づき予測をブレンド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測結果をデータフレームに変換\n",
    "out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])\n",
    "display(out.head())  # 最初の5行を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv('submission.csv')  # 提出用ファイルに予測結果を保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
