{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f58f9c6",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYSの「Chatbot Arena」コンペティションにおける、2つのチャットボットの応答の優劣を予測する問題に取り組んでいます。具体的には、与えられたプロンプトやチャットボットの応答からキーワードを抽出し、これらの情報を基に予測モデルを構築しています。\n",
    "\n",
    "主な手法としては、以下の要素が使用されています：\n",
    "\n",
    "1. **KeyBERTライブラリ**: キーワード抽出のために、KeyBERTを使用しています。このモデルは、文書から重要なキーワードを抽出するためのもので、事前トレーニングされたDistilBERTモデルと組み合わせて利用されています。\n",
    "\n",
    "2. **PyTorchおよびTransformersライブラリ**: PyTorchを基盤とした深層学習モデルを使用して、提案されたデータセットに対して応答の優劣を予測します。具体的には、RoBERTaモデルを用いてシーケンス分類を行っています。\n",
    "\n",
    "3. **データの処理**: テストデータを整形し、プロンプトや応答からキーワードを抽出した後、これらのキーワードを新しい特徴量としてデータフレームに追加しています。抽出したキーワードは、モデルの入力として使用されます。\n",
    "\n",
    "4. **データセットクラスの定義**: BERTに基づくデータセットクラスを定義し、トークン化やパディングなど前処理を行っています。これにより、モデルに与えるためにデータが適切に整形されています。\n",
    "\n",
    "5. **予測関数の実装**: 除外ファイルからモデルの保存状態を読み込み、テストデータに対して予測を行います。最終的に、モデルAとモデルBの勝者の予測結果を集約し、適切な形式で出力しています。\n",
    "\n",
    "このNotebookでは、指定されたタスクに対する実行時間の制限を考慮しつつ、ライブラリのインストールやデータ処理を行い、提出用のCSVファイルの生成までを行っています。最終的に、モデルによる予測に基づいた結果を含んだ提出ファイルを作成しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1763f987",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebook内で使用されている機械学習・深層学習の専門用語に関する簡単な解説です。初心者がつまずきやすい、ややマイナーまたは実務経験がなければ理解しにくい内容に焦点を当てています。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **KeyBERT**\n",
    "   - 文書から重要なキーワードを抽出するためのライブラリ。BERTモデルを用いて文のコンテキストを理解し、最も関連性の高い単語やフレーズを特定します。\n",
    "\n",
    "2. **Gradient Scaler (GradScaler)**\n",
    "   - 自動混合精度トレーニングを行うためのPyTorchの機能。計算の精度を保ちながらメモリ使用量を減らし、学習の速度を向上させるために、モデルの計算を浮動小数点16ビットに変換する際のスケールを管理します。\n",
    "\n",
    "3. **Deterministic**\n",
    "   - モデルやアルゴリズムが常に同じ結果を返す特性。特に乱数を使用する処理で再現性を確保するために、特定の乱数シードを設定して実行します。\n",
    "\n",
    "4. **Tokenizer**\n",
    "   - テキストをトークン（単語やサブワードなどの単位）に変換するためのコンポーネント。BERTやRoBERTaのようなモデルの入力としてテキストを適切な形式に変換する役割を持っています。\n",
    "\n",
    "5. **Attention Mask**\n",
    "   - トークン化された文において、モデルがどのトークンに注意を払うべきかを示すマスク。実際のデータのトークンとパディングトークンを区別するために使用され、モデルの計算効率化に寄与します。\n",
    "\n",
    "6. **Logits**\n",
    "   - モデルの出力で、各クラスの信頼度を表現した生のスコア。通常、ロジスティック回帰や分類タスクにおいて、これらのスコアは確率に変換されます。\n",
    "\n",
    "7. **BERTDataSet**\n",
    "   - PyTorchのDatasetクラスを拡張したカスタムクラス。文とそのターゲットラベルを管理し、BERTモデル用にエンコードされた入力を生成するための方法を提供します。\n",
    "\n",
    "8. **DataLoader**\n",
    "   - PyTorchの機能で、大規模なデータセットを効率的にバッチ処理し、モデルに読み込むためのツール。シャッフルやマルチプロセッシングを使用してデータの供給を行います。\n",
    "\n",
    "9. **Mean Squared Error (MSE)**\n",
    "   - 回帰分析やモデル評価において使用される指標。予測値と実際の値との差の二乗平均をとることで、モデルの誤差を測定します。\n",
    "\n",
    "10. **Stratified K-Fold**\n",
    "    - クロスバリデーションの手法の一つで、データセットを一様に分割し、分布を保ったままモデルの性能を評価します。特にクラスの不均衡に対処する際に使用されます。\n",
    "\n",
    "これらの用語は、初心者には馴染みのない場合が多く、特に実務経験がないと理解が難しいことがあります。この解説が、Jupyter Notebookを読む手助けとなることを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1915030a",
   "metadata": {},
   "source": [
    "# LMSYS キーワード torch RoBERTa での提出\n",
    "インターネットオフ条件で\n",
    "\n",
    "- https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert <br/>\n",
    "トレーニングデータの処理\n",
    "\n",
    "- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta <br/>\n",
    "処理されたトレーニングデータを使用したモデルのトレーニング\n",
    "\n",
    "- https://www.kaggle.com/code/stpeteishii/download-keybert <br/>\n",
    "KeyBERTのダウンロード\n",
    "\n",
    "- https://www.kaggle.com/code/stpeteishii/save-distilbert-base-nli-mean-tokens <br/>\n",
    "distilbert-base-nli-mean-tokensのダウンロード\n",
    "\n",
    "- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta-for-submission <br/>\n",
    "テストデータの処理、推論（このノートブック）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keybert --no-index --find-links=file:///kaggle/input/download-keybert\n",
    "# KeyBERTライブラリをインストールします。\n",
    "# --no-indexフラグは、PyPIインデックスからの検索を無効にします。\n",
    "# --find-linksオプションは、指定されたローカルのファイルパスからパッケージをインストールするために使用します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T14:16:28.599254Z",
     "iopub.status.busy": "2024-07-28T14:16:28.598364Z",
     "iopub.status.idle": "2024-07-28T14:16:46.349637Z",
     "shell.execute_reply": "2024-07-28T14:16:46.348879Z",
     "shell.execute_reply.started": "2024-07-28T14:16:28.599209Z"
    }
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "# KeyBERTライブラリからKeyBERTクラスをインポートします。\n",
    "# KeyBERTは、文書からキーワードを抽出するためのモデルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:16:46.357455Z",
     "iopub.status.busy": "2024-07-28T14:16:46.35717Z",
     "iopub.status.idle": "2024-07-28T14:16:46.370646Z",
     "shell.execute_reply": "2024-07-28T14:16:46.3698Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.357422Z"
    },
    "papermill": {
     "duration": 6.118647,
     "end_time": "2023-06-28T07:17:46.877542",
     "exception": false,
     "start_time": "2023-06-28T07:17:40.758895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import os\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt \n",
    "import transformers\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore')  # 警告を無視するように設定します。\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()  # 自動混合精度トレーニングのスケーラーを作成します。\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればGPUを使用し、そうでなければCPUを使用します。\n",
    "device  # 現在のデバイス（GPUまたはCPU）を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:16:46.374186Z",
     "iopub.status.busy": "2024-07-28T14:16:46.373848Z",
     "iopub.status.idle": "2024-07-28T14:16:46.397994Z",
     "shell.execute_reply": "2024-07-28T14:16:46.397088Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.374157Z"
    },
    "papermill": {
     "duration": 0.026662,
     "end_time": "2023-06-28T07:17:46.916292",
     "exception": false,
     "start_time": "2023-06-28T07:17:46.88963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def random_seed(SEED):\n",
    "    # 乱数シードを設定する関数です。\n",
    "    random.seed(SEED)  # Pythonのrandomモジュールのシードを設定します。\n",
    "    os.environ['PYTHONHASHSEED'] = str(SEED)  # 環境変数でハッシュシードを設定します。\n",
    "    np.random.seed(SEED)  # NumPyの乱数シードを設定します。\n",
    "    torch.manual_seed(SEED)  # PyTorchのCPU乱数シードを設定します。\n",
    "    torch.cuda.manual_seed(SEED)  # PyTorchのGPU乱数シードを設定します。\n",
    "    torch.cuda.manual_seed_all(SEED)  # 全てのGPUの乱数シードを設定します。\n",
    "    torch.backends.cudnn.deterministic = True  # CuDNNの決定論的動作を有効にします。\n",
    "    \n",
    "SEED = 508  # 使用する乱数シードを508に設定します。\n",
    "random_seed(SEED)  # 上記のシードで乱数の再現性を確保します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee27bf9",
   "metadata": {},
   "source": [
    "# テストデータの処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:16:46.399361Z",
     "iopub.status.busy": "2024-07-28T14:16:46.399047Z",
     "iopub.status.idle": "2024-07-28T14:16:46.804949Z",
     "shell.execute_reply": "2024-07-28T14:16:46.803417Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.399338Z"
    }
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# SentenceTransformerライブラリからSentenceTransformerクラスをインポートします。\n",
    "local_model = SentenceTransformer('/kaggle/input/save-distilbert-base-nli-mean-tokens')  \n",
    "# 保存したDistilBERTモデルをローカルパスから読み込みます。\n",
    "\n",
    "modelky = KeyBERT(model=local_model)  \n",
    "# KeyBERTのインスタンスを作成し、先ほど読み込んだモデルを使用します。これはキーワード抽出のための準備です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.808066Z",
     "iopub.status.idle": "2024-07-28T14:16:46.809171Z",
     "shell.execute_reply": "2024-07-28T14:16:46.809005Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.808989Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。\n",
    "# encoding='iso-8859-1'という引数は、必要に応じてコメントアウトされています。\n",
    "\n",
    "# 新しい列をデータフレームに追加し、初期値を'-'に設定します。\n",
    "test['prompt_kw'] = '-'  \n",
    "test['res_a_kw'] = '-'  \n",
    "test['res_b_kw'] = '-'  \n",
    "\n",
    "# 'prompt'列からトップ5のキーワードを抽出します。\n",
    "tkw0 = modelky.extract_keywords(test['prompt'], top_n=5)  \n",
    "# 'response_a'列からトップ10のキーワードを抽出します。\n",
    "tkw1 = modelky.extract_keywords(test['response_a'], top_n=10)  \n",
    "# 'response_b'列からトップ10のキーワードを抽出します。\n",
    "tkw2 = modelky.extract_keywords(test['response_b'], top_n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.81042Z",
     "iopub.status.idle": "2024-07-28T14:16:46.810784Z",
     "shell.execute_reply": "2024-07-28T14:16:46.810613Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.810599Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, w in enumerate(tkw0): \n",
    "    ws = []  # 空のリストを作成してキーワードを保存します。\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合\n",
    "            ws += [wi[0]]  # キーワードをリストに追加します。\n",
    "    test.loc[i, 'prompt_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。\n",
    "    \n",
    "for i, w in enumerate(tkw1): \n",
    "    ws = []  # 空のリストを作成してキーワードを保存します。\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合\n",
    "            ws += [wi[0]]  # キーワードをリストに追加します。\n",
    "    test.loc[i, 'res_a_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。\n",
    "\n",
    "for i, w in enumerate(tkw2): \n",
    "    ws = []  # 空のリストを作成してキーワードを保存します。\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合\n",
    "            ws += [wi[0]]  # キーワードをリストに追加します。\n",
    "    test.loc[i, 'res_b_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。\n",
    "\n",
    "# 'res_a_kw'列のキーワードと'prompt_kw'列のキーワードを結合します。\n",
    "test['res_a_kw'] = test['res_a_kw'] + ' // ' + test['prompt_kw']\n",
    "# 'res_b_kw'列のキーワードと'prompt_kw'列のキーワードを結合します。\n",
    "test['res_b_kw'] = test['res_b_kw'] + ' // ' + test['prompt_kw']\n",
    "test = test.iloc[:, 4:]  # 最初の4列を除外して新しいデータフレームを作成します。\n",
    "display(test)  # 最終的なデータフレームを表示します。\n",
    "\n",
    "# test.to_csv('test_key.csv', index=False)  # （コメントアウトされた）データフレームをCSVファイルとして保存する行です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.812298Z",
     "iopub.status.idle": "2024-07-28T14:16:46.812609Z",
     "shell.execute_reply": "2024-07-28T14:16:46.81247Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.812457Z"
    }
   },
   "outputs": [],
   "source": [
    "testA = test[['res_a_kw']]  # 'res_a_kw'列のみを抽出して新しいデータフレームtestAを作成します。\n",
    "testA['label'] = 0  # 新しい列'label'を追加し、すべての値を0に設定します。\n",
    "testA.columns = ['text', 'label']  # 列名を'text'と'label'に変更します。\n",
    "\n",
    "testB = test[['res_b_kw']]  # 'res_b_kw'列のみを抽出して新しいデータフレームtestBを作成します。\n",
    "testB['label'] = 0  # 新しい列'label'を追加し、すべての値を0に設定します。\n",
    "testB.columns = ['text', 'label']  # 列名を'text'と'label'に変更します。\n",
    "\n",
    "TEST = pd.concat([testA, testB], axis=0)  # testAとtestBを行方向（axis=0）で結合して新しいデータフレームTESTを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.814034Z",
     "iopub.status.idle": "2024-07-28T14:16:46.814387Z",
     "shell.execute_reply": "2024-07-28T14:16:46.814207Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.814194Z"
    },
    "papermill": {
     "duration": 0.030376,
     "end_time": "2023-06-28T07:18:37.166117",
     "exception": false,
     "start_time": "2023-06-28T07:18:37.135741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_sens = 8  # 最大文の数を8に設定します。これはデータの処理やモデル入力の際に使用される可能性があります。\n",
    "p_test = TEST.reset_index(drop=True)  # TESTデータフレームのインデックスをリセットし、新しいインデックスでデータフレームp_testを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.815465Z",
     "iopub.status.idle": "2024-07-28T14:16:46.815804Z",
     "shell.execute_reply": "2024-07-28T14:16:46.81563Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.815618Z"
    },
    "papermill": {
     "duration": 0.023548,
     "end_time": "2023-06-28T07:18:37.228036",
     "exception": false,
     "start_time": "2023-06-28T07:18:37.204488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BERTDataSet(Dataset):\n",
    "    # BERTに基づくデータセットクラスを定義します。このクラスはPyTorchのDatasetを継承しています。\n",
    "    \n",
    "    def __init__(self, sentences, targets):        \n",
    "        # コンストラクタ：文とターゲットを初期化します。\n",
    "        self.sentences = sentences  # 文を保存します。\n",
    "        self.targets = targets  # 対応するターゲットを保存します。\n",
    "        \n",
    "    def __len__(self):        \n",
    "        # データセットの長さ（文の数）を返します。\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):        \n",
    "        # 指定されたインデックスの文とターゲットを取得します。\n",
    "        sentence = self.sentences[idx]  \n",
    "        \n",
    "        # 文をBERTモデルの入力形式にエンコードします。\n",
    "        bert_sens = tokenizer.encode_plus(\n",
    "            sentence,\n",
    "            add_special_tokens = True,  # 特殊トークンを追加します。\n",
    "            max_length = max_sens,  # 最大長を設定します。\n",
    "            pad_to_max_length = True,  # 最大長にパディングを行います。\n",
    "            return_attention_mask = True  # 注意マスクを返します。\n",
    "        )\n",
    "\n",
    "        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)  # 入力IDをテンソルに変換します。\n",
    "        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)  # 注意マスクをテンソルに変換します。\n",
    "\n",
    "        target = torch.tensor(self.targets[idx], dtype=torch.float)  # 対応するターゲットをテンソルに変換します。\n",
    "        \n",
    "        return {\n",
    "            'ids': ids,  # 入力IDを戻します。\n",
    "            'mask': mask,  # 注意マスクを戻します。\n",
    "            'targets': target  # ターゲットを戻します。\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.817682Z",
     "iopub.status.idle": "2024-07-28T14:16:46.818074Z",
     "shell.execute_reply": "2024-07-28T14:16:46.817898Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.817883Z"
    },
    "papermill": {
     "duration": 0.023996,
     "end_time": "2023-06-28T07:18:37.264616",
     "exception": false,
     "start_time": "2023-06-28T07:18:37.24062",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = BERTDataSet(p_test[\"text\"], p_test[\"label\"])  \n",
    "# p_testの'text'と'label'を使用してBERTDataSetのインスタンスを作成します。このデータセットはテストデータを扱います。\n",
    "\n",
    "test_batch = 32  # テストデータのバッチサイズを32に設定します。\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_batch, shuffle=False, num_workers=8, pin_memory=True)  \n",
    "# DataLoaderを使用して、テストデータをバッチ処理します。\n",
    "# - num_workers=8: データの読み込みを並列で処理するために8つのワーカーを使用します。\n",
    "# - pin_memory=True: GPUへのデータ転送の効率を上げるためにメモリをピン留めします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1bffa0",
   "metadata": {},
   "source": [
    "# 予測を行う関数\n",
    "保存したモデルを使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.819091Z",
     "iopub.status.idle": "2024-07-28T14:16:46.819417Z",
     "shell.execute_reply": "2024-07-28T14:16:46.819271Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.819258Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルの初期化\n",
    "tokenizer = transformers.RobertaTokenizer.from_pretrained(\"/kaggle/input/roberta-base\")  \n",
    "# 指定したパスからRoBERTaトークナイザーを読み込みます。\n",
    "\n",
    "model = transformers.RobertaForSequenceClassification.from_pretrained(\"/kaggle/input/roberta-base\", num_labels=1)  \n",
    "# 指定したパスからRoBERTaモデルを読み込み、性質分類のためのモデルを作成します。\n",
    "# num_labels=1: 出力ラベルの数を1に設定します。\n",
    "\n",
    "# 指定したディレクトリ内の.pthファイルのパスをリストとして取得します。\n",
    "pths = [os.path.join(\"/kaggle/input/lmsys-keywords-torch-roberta\", s) for s in os.listdir(\"/kaggle/input/lmsys-keywords-torch-roberta\") if \".pth\" in s]\n",
    "print(pths)  # 取得したパスを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.820922Z",
     "iopub.status.idle": "2024-07-28T14:16:46.821365Z",
     "shell.execute_reply": "2024-07-28T14:16:46.821156Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.821137Z"
    },
    "papermill": {
     "duration": 0.043725,
     "end_time": "2023-06-28T07:40:50.691507",
     "exception": false,
     "start_time": "2023-06-28T07:40:50.647782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predicting(\n",
    "    test_dataloader,\n",
    "    model,\n",
    "    pths \n",
    "):\n",
    "    # モデルを使用して予測を行う関数です。\n",
    "    allpreds = []  # すべての予測結果を保存するリストです。    \n",
    "    for pth in pths:  # 指定された各モデルのパスについてループします。  \n",
    "        state = torch.load(pth, map_location=torch.device('cpu'))  # モデルの状態をロードします。\n",
    "        model.load_state_dict(state[\"state_dict\"])  # モデルにロードした状態を設定します。\n",
    "        model.to(device)  # モデルをデバイス（GPUまたはCPU）に移動します。\n",
    "        model.eval()  # モデルを評価モードに設定します。これにより、ドロップアウトやバッチ正規化が無効になります。      \n",
    "        \n",
    "        preds = []  # 各モデルの予測結果を保存するリストです。\n",
    "        allvalloss = 0  # オプションで全体損失を蓄積するための変数ですが、現時点では使用されていません。\n",
    "\n",
    "        with torch.no_grad():  # 勾配計算を無効にします（メモリの節約と計算速度の向上）。\n",
    "            for a in test_dataloader:  # テストデータローダーからバッチを繰り返します。\n",
    "                ids = a[\"ids\"].to(device)  # バッチの入力IDをデバイスに移動します。\n",
    "                mask = a[\"mask\"].to(device)  # バッチの注意マスクをデバイスに移動します。\n",
    "                output = model(ids, mask)  # モデルに入力を渡し、出力を取得します。\n",
    "                output = output[\"logits\"].squeeze(-1)  # 出力からロジットを取得し、次元を縮小します。\n",
    "                preds.append(output.cpu().numpy())  # 予測をCPUに移動し、NumPy配列として追加します。\n",
    "\n",
    "            preds = np.concatenate(preds)  # すべてのバッチの予測を結合します。           \n",
    "            allpreds.append(preds)  # モデルによる予測をリストに追加します。\n",
    "\n",
    "    return allpreds  # すべてのモデルからの予測結果を返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.822585Z",
     "iopub.status.idle": "2024-07-28T14:16:46.82305Z",
     "shell.execute_reply": "2024-07-28T14:16:46.822838Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.822812Z"
    },
    "papermill": {
     "duration": 71.891932,
     "end_time": "2023-06-28T07:42:05.088325",
     "exception": false,
     "start_time": "2023-06-28T07:40:53.196393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tpreds = predicting(test_dataloader, model, pths)  \n",
    "# テストデータローダー、モデル、およびモデルのパスを使って予測を行います。\n",
    "# 予測結果はtpredsに保存されます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d6915",
   "metadata": {},
   "source": [
    "# 予測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.824294Z",
     "iopub.status.idle": "2024-07-28T14:16:46.824603Z",
     "shell.execute_reply": "2024-07-28T14:16:46.824465Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.824452Z"
    }
   },
   "outputs": [],
   "source": [
    "test_pred = []  # 予測結果を保存するための空のリストを作成します。\n",
    "for p in tpreds[0]:  # 最初のモデルの予測結果についてループします。\n",
    "    test_pred += [p]  # それぞれの予測をリストに追加します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:16:46.82561Z",
     "iopub.status.idle": "2024-07-28T14:16:46.825972Z",
     "shell.execute_reply": "2024-07-28T14:16:46.825811Z",
     "shell.execute_reply.started": "2024-07-28T14:16:46.825796Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  \n",
    "# サンプル提出ファイルをCSVから読み込みます。\n",
    "\n",
    "submit['winner_model_a'] = test_pred[0:len(test)]  # モデルAの勝者の予測を提出データフレームに追加します。\n",
    "submit['winner_model_b'] = test_pred[len(test):]  # モデルBの勝者の予測を提出データフレームに追加します。\n",
    "\n",
    "pa = submit['winner_model_a']  # モデルAの予測結果を変数に保存します。\n",
    "pb = submit['winner_model_b']  # モデルBの予測結果を変数に保存します。\n",
    "\n",
    "# モデルAとモデルBの予測の合計をクリップして、0から1の範囲に制限します（この値を勝者が引き分けた場合のための列に使用します）。\n",
    "submit['winner_tie'] = np.clip((pa + pb), 0, 1)  \n",
    "display(submit)  # 最終的な提出データフレームを表示します。\n",
    "\n",
    "# 提出ファイルをCSV形式で保存します。\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 575905,
     "sourceId": 2795202,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 190145759,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 190167696,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 190167861,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 190173631,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
