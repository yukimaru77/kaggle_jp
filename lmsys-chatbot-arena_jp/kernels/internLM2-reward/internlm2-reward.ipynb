{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "105b0f04",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のための深層学習モデルを構築・評価するためのコードを提供しています。以下にその要約を示します。\n",
    "\n",
    "### 問題設定\n",
    "Notebookは、人間の好みを予測するためのモデルを構築するという課題に取り組んでいます。具体的には、二つの異なるチャットボット（モデル）からの応答のうち、どちらがユーザーに好まれるかを予測することが目標です。このプロセスでは、使用するモデルのトークン化やスコアリングを行い、最終的に予測結果を生成します。\n",
    "\n",
    "### 使用する手法とライブラリ\n",
    "1. **ライブラリのインストール**:\n",
    "   - `transformers`: 大規模言語モデル（LLM）を扱うライブラリ。\n",
    "   - `peft`, `bitsandbytes`, `accelerate`: モデルの最適化や量子化に役立つライブラリ。\n",
    "   - `einops`: テンソルの操作を簡素化するライブラリ。\n",
    "   - `torch`: PyTorchライブラリを使用し、深層学習フレームワークとしての基盤を提供。\n",
    "\n",
    "2. **モデルの準備**:\n",
    "   - 複数の事前トレーニング済みモデル（1.8B、2.7B、20B）のロードと量子化を行い、GPUに配置します。\n",
    "   - `BitsAndBytesConfig`を使用して、モデルのメモリ効率を向上させる4ビット量子化を実施。\n",
    "\n",
    "3. **データの前処理**:\n",
    "   - CSV形式のテストデータを読み込み、プロンプトと応答の長さを最大7200トークンに制限するカットオフ処理を行います。\n",
    "   - 各データ例に対して、プロンプトと応答の整形を実施。\n",
    "\n",
    "4. **推論の実行**:\n",
    "   - モデルを使って推論し、各応答に対するスコアを計算します。スコアの差が小さい場合は引き分けとし、それ以外の場合は勝者モデルのスコアを確率に変換します。\n",
    "   - 推論関数を2つのスレッドで並列実行し、効率的に処理を行います。\n",
    "\n",
    "5. **予測結果の保存**:\n",
    "   - 最終的な予測結果をDataFrameにまとめ、`submission.csv`として保存します。また、エラーカウントや予測性能評価（対数損失など）も行う準備があります。\n",
    "\n",
    "### 結論\n",
    "このNotebookは、効果的に人間の好みを予測するための一連の処理を構築し、特定のチャットボットの応答の好まれ方を理解するための基盤を提供します。使用されている技術的な手法やライブラリは、最新の深層学習モデルを精細に調整し、評価するためのものです。利用者は、最終的に生成された`submission.csv`ファイルを用いてコンペティションに参加することが可能です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abd79a8",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語に関する簡単な解説を列挙します。特に、マイナーなものや実務経験のない人に馴染みがない可能性のある用語に焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **transformers**:\n",
    "   - 自然言語処理（NLP）モデルのアーキテクチャであり、特に注意機構に基づくモデル群を指します。BERTやGPTシリーズなど、最近のさまざまなLLMがこのアーキテクチャに基づいています。\n",
    "\n",
    "2. **peft (Parameter-Efficient Fine-Tuning)**:\n",
    "   - 設計したモデルの大部分のパラメータを固定し、必要な部分だけを調整する方法です。これは大規模モデルの微調整時に必要な計算リソースを削減します。\n",
    "\n",
    "3. **bitsandbytes**:\n",
    "   - モデルのエフィシエンシーやメモリ使用効率を向上させるためのライブラリで、量子化技術を利用してモデルのサイズを小さくすることができます。\n",
    "\n",
    "4. **accelerate**:\n",
    "   - PyTorchを利用した分散トレーニングの短縮化や効率化のためのライブラリです。複数のGPUやTPUを簡単に活用するためのツールを提供します。\n",
    "\n",
    "5. **量子化 (Quantization)**:\n",
    "   - モデルの重みとアクティベーションを低ビットの整数で表現する手段です。これにより、モデルのメモリ使用量が減少し、推論を高速化できます。\n",
    "\n",
    "6. **ダブル量子化 (Double Quantization)**:\n",
    "   - 量子化を2回行うことで、モデルの精度を維持しながらさらなるメモリ削減を図る技術です。一度目の量子化後に再度量子化を行います。\n",
    "\n",
    "7. **tensors**:\n",
    "   - NumPyの配列に似た多次元のデータ構造で、PyTorchやTensorFlowなどで計算やデータの格納に使用されます。特に深層学習では重要な役割を果たします。\n",
    "\n",
    "8. **プロンプト (Prompt)**:\n",
    "   - 言語モデルに与えられる入力テキストのことです。モデルはこのプロンプトに基づいて応答を生成します。\n",
    "\n",
    "9. **エラーカウント (Error Count)**:\n",
    "   - モデルが推論中に遭遇したエラーの数をカウントすることです。これにより、推論の安定性を評価することができます。\n",
    "\n",
    "10. **&delta; (デルタ)**:\n",
    "    - 数学や統計で用いられる差異を示すシンボル。特にスコアなどの変化を表すのに使われます。\n",
    "\n",
    "11. **IPython.display**:\n",
    "    - Jupyter Notebook内でのオブジェクトの表示を管理するためのモジュールです。ここでは、ファイルへのリンクを作成するために用いられています。\n",
    "\n",
    "これらの用語は、特にこのノートブックでのプロセスや実装に関連しており、初心者にとっては実務上現れることの少ないものが多いです。理解を深めるために、それぞれの用語についてさらに調査することをお勧めします。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:45:59.107778Z",
     "iopub.status.busy": "2024-07-29T11:45:59.106818Z",
     "iopub.status.idle": "2024-07-29T11:46:23.200683Z",
     "shell.execute_reply": "2024-07-29T11:46:23.199339Z",
     "shell.execute_reply.started": "2024-07-29T11:45:59.107744Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformers、peft、bitsandbytes、accelerateのライブラリを最新バージョンにアップデートしてインストールします。\n",
    "# --no-indexオプションを使用して、PyPIから直接ではなく、指定したリンクからインストールします。\n",
    "!pip install -U transformers peft bitsandbytes accelerate --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "\n",
    "# einopsライブラリを最新バージョンにアップデートしてインストールします。\n",
    "# 同様に、指定されたリンクからインストールします。\n",
    "!pip install -q -U einops --no-index --find-links /kaggle/input/einops-v0-8-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:47:26.243859Z",
     "iopub.status.busy": "2024-07-29T11:47:26.243449Z",
     "iopub.status.idle": "2024-07-29T11:47:26.254891Z",
     "shell.execute_reply": "2024-07-29T11:47:26.254079Z",
     "shell.execute_reply.started": "2024-07-29T11:47:26.243824Z"
    }
   },
   "outputs": [],
   "source": [
    "# PyTorchライブラリをインポートします。これは深層学習フレームワークです。\n",
    "import torch\n",
    "\n",
    "# AutoModelおよびAutoTokenizerクラスをtransformersライブラリからインポートします。\n",
    "# AutoModelは、指定されたモデル名に基づいて適切なモデルを自動的にロードします。\n",
    "# AutoTokenizerは、入力テキストをトークンに変換するためのツールです。\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "# BitsAndBytesConfigクラスをtransformersライブラリからインポートします。\n",
    "# これは、モデルの量子化やメモリ効率の向上に関連する設定を扱います。\n",
    "from transformers import BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:50:06.217983Z",
     "iopub.status.busy": "2024-07-29T11:50:06.217365Z",
     "iopub.status.idle": "2024-07-29T11:56:18.97818Z",
     "shell.execute_reply": "2024-07-29T11:56:18.977391Z",
     "shell.execute_reply.started": "2024-07-29T11:50:06.217953Z"
    }
   },
   "outputs": [],
   "source": [
    "# 使用するモデルのパスを指定します。\n",
    "# 最初は1.8Bモデル、次に2.7Bモデル、最後に20Bモデルのパスが示されています。\n",
    "model_path = \"/kaggle/input/internlm2-1.8b-reward/transformers/default/1/internlm_internlm2-1_8b-reward\"\n",
    "model_path = \"/kaggle/input/internlm-2-7b/transformers/default/1/internlm_internlm2-7b-reward\"\n",
    "model_path = \"/kaggle/input/iternlm2-20b-reward/transformers/default/1/internlm_internlm2-20b-reward\"\n",
    "\n",
    "# 指定したモデルパスからトークナイザーをロードします。\n",
    "# trust_remote_code=Trueは、モデル構成のリモートコードを信頼する設定です。\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# BitsAndBytesConfigの設定を作成します。\n",
    "# load_in_4bit=Trueは、4ビットモデルのロードを有効にします。\n",
    "# bnb_4bit_quant_typeで量子化のタイプを指定します。\n",
    "# bnb_4bit_use_double_quant=Trueは、ダブル量子化を使用するかどうかを示します。\n",
    "# bnb_4bit_compute_dtypeは、計算時に使用するデータ型を指定します。\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "# model_0として指定したパスから事前トレーニング済みのモデルをロードします。\n",
    "# device_map=\"cuda:0\"は、最初のGPUデバイスを指定しています。\n",
    "# torch_dtype=torch.float16は、モデルが使用するデータ型を指定しています。\n",
    "model_0 = AutoModel.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cuda:0\", \n",
    "    torch_dtype=torch.float16, \n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# model_1として同様に指定したパスから事前トレーニング済みのモデルをロードします。\n",
    "# device_map=\"cuda:1\"は、2番目のGPUデバイスを指定しています。\n",
    "model_1 = AutoModel.from_pretrained(\n",
    "    model_path, \n",
    "    device_map=\"cuda:1\", \n",
    "    torch_dtype=torch.float16, \n",
    "    trust_remote_code=True,\n",
    "    quantization_config=bnb_config,\n",
    ")\n",
    "\n",
    "# 次に、同じモデルをGPUデバイス2000に向けてタブ使用でロードするコードのコメントアウトされた部分があります。\n",
    "# コメントアウトされているため、実行されませんが、同様の意味を持っています。\n",
    "# model_0 = AutoModel.from_pretrained(\n",
    "#     model_path, \n",
    "#     device_map=\"cuda:0\", \n",
    "#     torch_dtype=torch.float16, \n",
    "#     trust_remote_code=True,\n",
    "# )\n",
    "# model_1 = AutoModel.from_pretrained(\n",
    "#     model_path, \n",
    "#     device_map=\"cuda:1\", \n",
    "#     torch_dtype=torch.float16, \n",
    "#     trust_remote_code=True,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T11:56:50.790183Z",
     "iopub.status.busy": "2024-07-29T11:56:50.789519Z",
     "iopub.status.idle": "2024-07-29T11:57:24.122377Z",
     "shell.execute_reply": "2024-07-29T11:57:24.121263Z",
     "shell.execute_reply.started": "2024-07-29T11:56:50.790153Z"
    }
   },
   "outputs": [],
   "source": [
    "# 4ビット量子化されたモデルの保存先パスを指定します。\n",
    "# ここでは\"internlm2-20b-rm-bnb-4bit\"という名前です。\n",
    "# bnb_4bit_path = \"internlm2-20b-rm-bnb-4bit\"\n",
    "\n",
    "# model_0（最初のモデル）を指定したパスに保存します。\n",
    "# save_pretrainedメソッドを使用して、モデルの状態をディスクに保存します。\n",
    "# model_0.save_pretrained(bnb_4bit_path)\n",
    "\n",
    "# tokenizer（トークナイザー）を指定したパスに保存します。\n",
    "# これにより、このトークナイザーの設定や辞書などが保存されます。\n",
    "# tokenizer.save_pretrained(bnb_4bit_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:43:12.08275Z",
     "iopub.status.busy": "2024-07-29T12:43:12.081999Z",
     "iopub.status.idle": "2024-07-29T12:43:12.088672Z",
     "shell.execute_reply": "2024-07-29T12:43:12.087675Z",
     "shell.execute_reply.started": "2024-07-29T12:43:12.082717Z"
    }
   },
   "outputs": [],
   "source": [
    "# IPython.displayライブラリからFileLinkとdisplay関数をインポートします。\n",
    "# FileLinkは、ファイルへのリンクを作成するためのツールです。\n",
    "# display関数は、与えられたオブジェクトを表示します。\n",
    "# from IPython.display import FileLink, display\n",
    "\n",
    "# 指定されたファイルへのリンクを作成し、そのリンクを表示します。\n",
    "# ファイルのパスは\"/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors\"です。\n",
    "# display(FileLink(\"/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:06:23.314271Z",
     "iopub.status.busy": "2024-07-29T12:06:23.313989Z",
     "iopub.status.idle": "2024-07-29T12:06:23.329576Z",
     "shell.execute_reply": "2024-07-29T12:06:23.328678Z",
     "shell.execute_reply.started": "2024-07-29T12:06:23.314248Z"
    }
   },
   "outputs": [],
   "source": [
    "# model_0の内容を表示します。\n",
    "# これにより、モデルの情報や設定、構造などの詳細が表示されます。\n",
    "model_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:42:53.646581Z",
     "iopub.status.busy": "2024-07-29T12:42:53.645694Z",
     "iopub.status.idle": "2024-07-29T12:42:53.67154Z",
     "shell.execute_reply": "2024-07-29T12:42:53.670676Z",
     "shell.execute_reply.started": "2024-07-29T12:42:53.646547Z"
    }
   },
   "outputs": [],
   "source": [
    "# pandasライブラリをインポートします。\n",
    "# pandasはデータ解析用の強力なツールです。\n",
    "import pandas as pd\n",
    "\n",
    "# DEBUGフラグを初期化します。このフラグは、デバッグモードを示します。\n",
    "# DEBUG = False\n",
    "\n",
    "# test.csvファイルを読み込み、DataFrameとしてdfに格納します。\n",
    "df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# DataFrameの行数が3の場合、デバッグモードを有効にしてトレーニング用のデータセットを読み込むようにします。\n",
    "# デバッグモードであれば、train.csvファイルを読み込み、最初の1000行のみを取得します。\n",
    "# if len(df) == 3:\n",
    "#     DEBUG = True\n",
    "#     df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "#     df = df.head(1000)\n",
    "\n",
    "# 読み込んだDataFrame dfを表示します。\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:20:28.432868Z",
     "iopub.status.busy": "2024-07-29T12:20:28.432133Z",
     "iopub.status.idle": "2024-07-29T12:20:28.761143Z",
     "shell.execute_reply": "2024-07-29T12:20:28.760142Z",
     "shell.execute_reply.started": "2024-07-29T12:20:28.432838Z"
    }
   },
   "outputs": [],
   "source": [
    "# 与えられたデータ例のプロンプトと応答を処理する関数を定義します。\n",
    "# この関数は、テキストの長さが指定された最大値を超えないようにカットします。\n",
    "def cut_off(example, max_length=7200):\n",
    "    \n",
    "    # 指定されたインデックスまでのテキストの長さをカウントする内部関数を定義します。\n",
    "    def _count(example, idx):\n",
    "        _len = 0    \n",
    "        # プロンプトと両方の応答の長さを合計します。\n",
    "        for s in example['prompt'][:idx] + example['response_a'][:idx] + example['response_b'][:idx]:\n",
    "            _len += len(s)\n",
    "        return _len\n",
    "\n",
    "    # 再帰的にカットを実行する内部関数を定義します。\n",
    "    def _recusive_cut(idx):\n",
    "        if idx == 0:\n",
    "            # インデックスが0の場合、最大の長さに切り捨てます。\n",
    "            example['prompt'] = [example['prompt'][0][:1000]]\n",
    "            example['response_a'] = [example['response_a'][0][:3000]]\n",
    "            example['response_b'] = [example['response_b'][0][:3000]]\n",
    "            return example\n",
    "            \n",
    "        # 指定したインデックスまでの長さが最大値を超えているか確認します。\n",
    "        if _count(example, idx) > max_length:\n",
    "            # 超えている場合、インデックスを1つ減らして再帰的に実行します。\n",
    "            return _recusive_cut(idx-1)\n",
    "        else:\n",
    "            # 最大値を超えていない場合、プロンプトと応答を適切なインデックスまで切り捨てます。\n",
    "            example['prompt'] = example['prompt'][:idx]\n",
    "            example['response_a'] = example['response_a'][:idx]\n",
    "            example['response_b'] = example['response_b'][:idx]\n",
    "            return example\n",
    "    # プロンプトの長さに基づいて再帰的にカットを実行します。\n",
    "    return _recusive_cut(len(example['prompt']))\n",
    "\n",
    "\n",
    "# 例を処理するための関数を定義します。\n",
    "def process_fn(example):\n",
    "    # evalを使用して、文字列をPythonのオブジェクトに変換します。\n",
    "    example['prompt'] = eval(example['prompt'], {\"null\": \"\"})\n",
    "    example['response_a'] = eval(example['response_a'], {\"null\": \"\"})\n",
    "    example['response_b'] = eval(example['response_b'], {\"null\": \"\"})\n",
    "    # カットオフを実行します。\n",
    "    return cut_off(example)\n",
    "\n",
    "# DataFrame dfの各行に対してprocess_fnを適用し、新しいDataFrame new_dfを作成します。\n",
    "new_df = df.apply(lambda x: process_fn(x), axis=1)\n",
    "\n",
    "# 新しく生成されたDataFrame new_dfを表示します。\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:20:33.77159Z",
     "iopub.status.busy": "2024-07-29T12:20:33.770795Z",
     "iopub.status.idle": "2024-07-29T12:20:33.782691Z",
     "shell.execute_reply": "2024-07-29T12:20:33.781561Z",
     "shell.execute_reply.started": "2024-07-29T12:20:33.771557Z"
    }
   },
   "outputs": [],
   "source": [
    "# コードの実行時間を計測するためのマジックコマンドです。\n",
    "%%time\n",
    "\n",
    "# mathライブラリとnumpyライブラリをインポートします。\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# データフレームとモデルを引数に取り、推論を行う関数を定義します。\n",
    "def inference(df, model):\n",
    "\n",
    "    error_cnt = 0  # エラーのカウントを初期化します。\n",
    "    y_pred = []  # 予測結果を保存するリストを初期化します。\n",
    "    \n",
    "    # DataFrameの各行を繰り返します。\n",
    "    for idx, row in df.iterrows():\n",
    "        chat_a = []  # モデルA用のチャットデータを初期化します。\n",
    "        chat_b = []  # モデルB用のチャットデータを初期化します。\n",
    "        \n",
    "        # 各プロンプトに対して応答を追加します。\n",
    "        for i in range(len(row['prompt'])):\n",
    "            chat_a.append({\"role\": \"user\", \"content\": row['prompt'][i]})  # ユーザーのプロンプトを追加します。\n",
    "            chat_a.append({\"role\": \"assistant\", \"content\": row['response_a'][i]})  # モデルAの応答を追加します。\n",
    "\n",
    "            chat_b.append({\"role\": \"user\", \"content\": row['prompt'][i]})  # ユーザーのプロンプトを追加します。\n",
    "            chat_b.append({\"role\": \"assistant\", \"content\": row['response_b'][i]})  # モデルBの応答を追加します。\n",
    "            \n",
    "        # モデルを使ってスコアを取得します。\n",
    "        try:\n",
    "            score1, score2 = model.get_scores(tokenizer, [chat_a, chat_b])  # モデルからスコアを取得します。\n",
    "            \n",
    "            # スコアの差が0.08未満の場合は、両モデルの勝ちをほぼ同じとみなします。\n",
    "            if abs(score1 - score2) < 0.08:\n",
    "                y_pred.append([0.00005, 0.00005, 0.9999])  # 引き分けの場合の予測を追加します。\n",
    "            else:\n",
    "                # スコアを指数関数的に変換し、確率を計算します。\n",
    "                score1, score2 = math.exp(score1), math.exp(score2)\n",
    "                sum_ = score1 + score2  # スコアの合計を計算します。\n",
    "                y_pred.append([score1/sum_ - 0.0001, score2/sum_ - 0.0001, 0.0002])  # 確率をリストに追加します。\n",
    "        except:\n",
    "            # エラーが発生した場合は、均等な確率を割り当てます。\n",
    "            y_pred.append([0.33334, 0.33333, 0.33333])\n",
    "            error_cnt += 1  # エラーカウントを増やします。\n",
    "        \n",
    "    # 予測結果をNumPy配列に変換します。\n",
    "    y_pred = np.array(y_pred)\n",
    "    \n",
    "    # DataFrameに各モデルの予測結果を追加します。\n",
    "    df['winner_model_a_pred'] = y_pred[:, 0]  # モデルAの勝者予測を追加します。\n",
    "    df['winner_model_b_pred'] = y_pred[:, 1]  # モデルBの勝者予測を追加します。\n",
    "    df['winner_tie_pred'] = y_pred[:, 2]  # 引き分け予測を追加します。\n",
    "    \n",
    "    print(error_cnt)  # エラーの数を表示します。\n",
    "    return df  # 予測結果を含むDataFrameを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:19:14.031198Z",
     "iopub.status.busy": "2024-07-29T12:19:14.030237Z",
     "iopub.status.idle": "2024-07-29T12:19:19.783271Z",
     "shell.execute_reply": "2024-07-29T12:19:19.782357Z",
     "shell.execute_reply.started": "2024-07-29T12:19:14.031168Z"
    }
   },
   "outputs": [],
   "source": [
    "# 推論関数をテストします。\n",
    "# new_dfの最初の20行を用いて、model_0を使った推論を実行します。\n",
    "# inference(new_df[:20], model_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:20:39.017836Z",
     "iopub.status.busy": "2024-07-29T12:20:39.017063Z",
     "iopub.status.idle": "2024-07-29T12:40:17.219023Z",
     "shell.execute_reply": "2024-07-29T12:40:17.218066Z",
     "shell.execute_reply.started": "2024-07-29T12:20:39.017802Z"
    }
   },
   "outputs": [],
   "source": [
    "# コードの実行時間を計測するためのマジックコマンドです。\n",
    "%%time\n",
    "\n",
    "# new_dfの偶数番目の行を選択し、コピーを作成します。これがsub_0になります。\n",
    "sub_0 = new_df.iloc[0::2].copy()\n",
    "# new_dfの奇数番目の行を選択し、コピーを作成します。これがsub_1になります。\n",
    "sub_1 = new_df.iloc[1::2].copy()\n",
    "\n",
    "# concurrent.futuresからThreadPoolExecutorをインポートします。\n",
    "from concurrent.futures import ThreadPoolExecutor \n",
    "\n",
    "# ThreadPoolExecutorを使用して、2つのスレッドで推論を並列実行します。\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    # inference関数をそれぞれのサブDataFrameとモデルに対して実行します。\n",
    "    results = executor.map(inference, (sub_0, sub_1), (model_0, model_1))\n",
    "\n",
    "# 結果をDataFrameに結合し、新しいDataFrame result_dfを作成します。\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "\n",
    "# 結果DataFrameの最初の5行を表示します。\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:40:17.222011Z",
     "iopub.status.busy": "2024-07-29T12:40:17.221299Z",
     "iopub.status.idle": "2024-07-29T12:40:17.261673Z",
     "shell.execute_reply": "2024-07-29T12:40:17.260528Z",
     "shell.execute_reply.started": "2024-07-29T12:40:17.221973Z"
    }
   },
   "outputs": [],
   "source": [
    "# result_dfの列名を変更します。\n",
    "# 'winner_model_a_pred'を'winner_model_a'に、\n",
    "# 'winner_model_b_pred'を'winner_model_b'に、\n",
    "# 'winner_tie_pred'を'winner_tie'に変更します。\n",
    "result_df = result_df.rename(columns={'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})\n",
    "\n",
    "# 列名を変更した結果のDataFrame result_dfを表示します。\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:40:17.263889Z",
     "iopub.status.busy": "2024-07-29T12:40:17.263498Z",
     "iopub.status.idle": "2024-07-29T12:40:17.289937Z",
     "shell.execute_reply": "2024-07-29T12:40:17.289073Z",
     "shell.execute_reply.started": "2024-07-29T12:40:17.263839Z"
    }
   },
   "outputs": [],
   "source": [
    "# result_dfから'id', 'winner_model_a', 'winner_model_b', 'winner_tie'の列を選択し、'submission.csv'ファイルに書き出します。\n",
    "# index=Falseは、行番号をCSVに含めないようにするオプションです。\n",
    "result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)\n",
    "\n",
    "# 書き出した'submission.csv'ファイルを読み込み、最初の5行を表示します。\n",
    "pd.read_csv('submission.csv').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T12:19:07.131134Z",
     "iopub.status.busy": "2024-07-29T12:19:07.130804Z",
     "iopub.status.idle": "2024-07-29T12:19:07.135459Z",
     "shell.execute_reply": "2024-07-29T12:19:07.134671Z",
     "shell.execute_reply.started": "2024-07-29T12:19:07.131103Z"
    }
   },
   "outputs": [],
   "source": [
    "# sklearn.metricsからlog_lossとaccuracy_scoreをインポートします。\n",
    "# from sklearn.metrics import log_loss, accuracy_score\n",
    "\n",
    "# デバッグモードが有効な場合に以下の処理を実行します。\n",
    "# if DEBUG:\n",
    "#     # 実際の結果を選択し、y_trueとしてリストに変換します。\n",
    "#     y_true = result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.tolist()\n",
    "#     # 予測結果を選択し、y_predとしてリストに変換します。\n",
    "#     y_pred = result_df[['winner_model_a_pred', 'winner_model_b_pred', 'winner_tie_pred']].values.tolist()\n",
    "#     # log_lossを計算し、表示します。\n",
    "#     print(log_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-25T16:44:02.07165Z",
     "iopub.status.busy": "2024-07-25T16:44:02.07137Z",
     "iopub.status.idle": "2024-07-25T16:44:02.077642Z",
     "shell.execute_reply": "2024-07-25T16:44:02.076635Z",
     "shell.execute_reply.started": "2024-07-25T16:44:02.071627Z"
    }
   },
   "outputs": [],
   "source": [
    "# デバッグモードが無効な場合に以下の処理を実行します。\n",
    "# if not DEBUG:\n",
    "#     # result_dfの列名を再度変更します。\n",
    "#     result_df = result_df.rename({'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})\n",
    "#     # 'id', 'winner_model_a', 'winner_model_b', 'winner_tie'の列を選択し、'submission.csv'ファイルに書き出します。\n",
    "#     result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)\n",
    "#     # 書き出した'submission.csv'ファイルを読み込み、最初の5行を表示します。\n",
    "#     pd.read_csv('submission.csv').head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37ce323c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Dlond Mike\n",
    "> \n",
    "> 8u nb lol:)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4924785,
     "sourceId": 8290453,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 188190593,
     "sourceType": "kernelVersion"
    },
    {
     "modelId": 94105,
     "modelInstanceId": 68973,
     "sourceId": 82087,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 94570,
     "modelInstanceId": 69432,
     "sourceId": 82656,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 95660,
     "modelInstanceId": 70610,
     "sourceId": 84069,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 95971,
     "modelInstanceId": 70978,
     "sourceId": 84495,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
