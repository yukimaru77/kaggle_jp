{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32bdc877",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」における人間による好み予測の問題に取り組んでいます。目的は、異なる大規模言語モデル（LLM）が生成した応答のどちらがユーザーに好まれるかを予測することです。具体的には、プロンプトとそれに対する応答を基に、各モデルの勝者を予測するための機械学習モデルを構築します。\n",
    "\n",
    "ノートブックでは、以下の手法やライブラリを使用しています：\n",
    "\n",
    "1. **データ処理**:\n",
    "   - `pandas`を利用して、CSVファイルからトレーニングデータとテストデータを読み込みます。\n",
    "   - テキストデータの前処理として、プロンプトと応答を結合し、新たな特徴量（text_a, text_b）を生成します。\n",
    "\n",
    "2. **特徴抽出**:\n",
    "   - `TfidfVectorizer`を使って、テキストデータをベクトル化します。これにより、テキストの重要度に基づく数値表現が得られ、機械学習モデルの入力として使用されます。\n",
    "   \n",
    "3. **データ分割とスケーリング**:\n",
    "   - `train_test_split`を使って、データセットを訓練用と検証用に分割します。\n",
    "   - 特徴量のスケーリングには`StandardScaler`を使用し、データの標準化を行います。\n",
    "\n",
    "4. **モデル構築**:\n",
    "   - `tensorflow`と`Keras`を用いて、シンプルなニューラルネットワークモデルを構築します。モデルは3つの出力を持ち、それぞれモデルA、モデルB、そしてタイの勝者の確率を予測します。\n",
    "\n",
    "5. **モデル訓練と評価**:\n",
    "   - モデルは10エポックで訓練され、バリデーションデータを利用した評価として対数損失（log loss）を計算します。\n",
    "\n",
    "6. **予測と提出ファイル作成**:\n",
    "   - テストデータに対する予測を行い、その結果を基に提出用のCSVファイル（submission.csv）を生成します。\n",
    "\n",
    "最終的に、このノートブックは、異なる応答からどのモデルが好まれるかを予測し、その結果をKaggleに提出するための全体的なフローを提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c20b6",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブックの内容に関連する専門用語の簡単な解説です。初心者がつまずきやすいマイナーな用語やドメイン特有の知識に焦点を当てています。\n",
    "\n",
    "1. **TfidfVectorizer**:\n",
    "   - テキストデータを数値ベクトルに変換するための手法で、各単語の重要度を「Term Frequency-Inverse Document Frequency（TF-IDF）」という尺度で評価します。この方法では、特に頻繁に出現する単語や、他の文書には少ない単語の影響を強調します。\n",
    "\n",
    "2. **StandardScaler**:\n",
    "   - 特徴量を標準化するための方法で、データの平均を0、分散を1に変換します。これにより、異なるスケールを持つ特徴量間のバイアスを軽減して、モデルの収束を早める効果があります。\n",
    "\n",
    "3. **Dropout**:\n",
    "   - ニューラルネットワークの学習中に、一部のニューロンをランダムに無効化する手法です。これにより、モデルが特定の特徴に対して過度に適合する（過学習）ことを防ぎ、汎用性を高めることができます。\n",
    "\n",
    "4. **Categorical Crossentropy**:\n",
    "   - 多クラス分類問題に用いられる損失関数で、モデルが出力するクラス確率と実際のクラスラベルとの誤差を測定します。この損失関数を最小化することで、モデルの精度を向上させることができます。\n",
    "\n",
    "5. **val_predictions**:\n",
    "   - バリデーションデータに対するモデルの予測結果を保持する変数であり、モデルの性能を評価する際に使用されます。バリデーションセットは、モデルの訓練中に使用されないデータで、過学習を防ぐための確認手段です。\n",
    "\n",
    "6. **log_loss**:\n",
    "   - 予測確率と実際のクラスラベルとの間の誤差を測定する指標であり、クロスエントロピー損失とも呼ばれます。値が小さいほど予測が正しいことを示します。\n",
    "\n",
    "7. **features (特徴量)**:\n",
    "   - 機械学習モデルに入力するデータの属性や変数のことを指し、問題に関する情報を数値化してモデルが学習しやすくしたものです。\n",
    "\n",
    "8. **epochs (エポック)**:\n",
    "   - データセット全体を使ってモデルの学習を一回行うことを指します。エポック数が多いほど、モデルはより多くの情報を学ぶことが期待されますが、過学習に注意が必要です。\n",
    "\n",
    "9. **batch_size (バッチサイズ)**:\n",
    "   - モデルを訓練する際に、一度に処理するトレーニングデータのサンプル数です。小さすぎるバッチサイズでは学習が不安定になり、大きすぎるとメモリ使用量が増加し、学習速度が遅くなることがあります。\n",
    "\n",
    "10. **fit**:\n",
    "    - モデルを訓練データに適合させるためのメソッドで、指定されたエポック数の間にモデルがデータを学習し、重みを更新します。\n",
    "\n",
    "これらの用語は、深層学習や機械学習の文脈で頻繁に使用され、特に特定のデータ処理の手法やモデルの構築に関して重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-23T09:15:07.480204Z",
     "iopub.status.busy": "2024-07-23T09:15:07.479596Z",
     "iopub.status.idle": "2024-07-23T09:15:07.488424Z",
     "shell.execute_reply": "2024-07-23T09:15:07.487556Z",
     "shell.execute_reply.started": "2024-07-23T09:15:07.480174Z"
    }
   },
   "outputs": [],
   "source": [
    "# このPython 3環境には多くの便利な分析ライブラリがインストールされています\n",
    "# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python\n",
    "# 以下は、いくつかの便利なパッケージの読み込み例です\n",
    "\n",
    "import numpy as np # 線形代数用ライブラリ\n",
    "import pandas as pd # データ処理用ライブラリ、CSVファイルの入出力（例: pd.read_csv）\n",
    "\n",
    "# 入力データファイルは、読み取り専用の \"../input/\" ディレクトリにあります\n",
    "# 例えば、これを実行することで（クリックするかShift+Enterを押す）、入力ディレクトリ内のすべてのファイルをリスト表示します\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# 現在のディレクトリ（/kaggle/working/）に最大20GBのデータを書き込むことができます\n",
    "# これは、「保存してすべてを実行」機能を使用してバージョンを作成する際に出力として保持されます\n",
    "# また、一時ファイルは/kaggle/temp/に書き込むことができますが、現在のセッションが終了すると保存されません"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:15:07.490477Z",
     "iopub.status.busy": "2024-07-23T09:15:07.490126Z",
     "iopub.status.idle": "2024-07-23T09:15:07.495502Z",
     "shell.execute_reply": "2024-07-23T09:15:07.494634Z",
     "shell.execute_reply.started": "2024-07-23T09:15:07.490454Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:15:07.496882Z",
     "iopub.status.busy": "2024-07-23T09:15:07.496534Z",
     "iopub.status.idle": "2024-07-23T09:15:07.505405Z",
     "shell.execute_reply": "2024-07-23T09:15:07.504674Z",
     "shell.execute_reply.started": "2024-07-23T09:15:07.496848Z"
    }
   },
   "outputs": [],
   "source": [
    "#pip install pandas numpy scikit-learn tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T09:15:07.506796Z",
     "iopub.status.busy": "2024-07-23T09:15:07.506492Z",
     "iopub.status.idle": "2024-07-23T09:17:00.162547Z",
     "shell.execute_reply": "2024-07-23T09:17:00.161821Z",
     "shell.execute_reply.started": "2024-07-23T09:15:07.506774Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import log_loss\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "\n",
    "# データセットを読み込む\n",
    "train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# プロンプトと応答を結合して特徴抽出を行う\n",
    "train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']\n",
    "train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']\n",
    "\n",
    "# TF-IDFを使用してテキストデータをベクトル化する\n",
    "tfidf = TfidfVectorizer(max_features=5000) # 特徴数を5000に制限\n",
    "X_a = tfidf.fit_transform(train_data['text_a']).toarray() # text_aをベクトル化\n",
    "X_b = tfidf.transform(train_data['text_b']).toarray() # text_bをベクトル化\n",
    "\n",
    "# ベクトル化された応答を結合する\n",
    "X = np.hstack([X_a, X_b]) # 二つのベクトルを横に結合\n",
    "\n",
    "# 目標変数（ターゲット）\n",
    "y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values # 勝者モデルの情報を取得\n",
    "\n",
    "# データをトレーニングセットとバリデーションセットに分割する\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # 80%をトレーニング、20%をバリデーション\n",
    "\n",
    "# 特徴量をスケーリングする\n",
    "scaler = StandardScaler() # 標準化のためのスケーラーを作成\n",
    "X_train = scaler.fit_transform(X_train) # トレーニングデータをスケーリング\n",
    "X_val = scaler.transform(X_val) # バリデーションデータをスケーリング\n",
    "\n",
    "# シンプルなニューラルネットワークモデルを構築する\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(X_train.shape[1],)), # 入力層\n",
    "    Dropout(0.5), # ドロップアウト層（過学習を防ぐ）\n",
    "    Dense(256, activation='relu'), # 隠れ層\n",
    "    Dropout(0.5), # ドロップアウト層\n",
    "    Dense(128, activation='relu'), # 隠れ層\n",
    "    Dropout(0.5), # ドロップアウト層\n",
    "    Dense(3, activation='softmax') # 出力層、3つのクラスに対する確率を出力\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # モデルをコンパイル\n",
    "\n",
    "# モデルを訓練する\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val)) # 学習を実行\n",
    "\n",
    "# モデルを評価する\n",
    "val_predictions = model.predict(X_val) # バリデーションデータで予測\n",
    "loss = log_loss(y_val, val_predictions) #ロスを計算\n",
    "print(f'Validation Log Loss: {loss}') # ロスの結果を表示\n",
    "\n",
    "# テストデータを準備する\n",
    "test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a'] # テストデータのためにtext_aを作成\n",
    "test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b'] # テストデータのためにtext_bを作成\n",
    "X_test_a = tfidf.transform(test_data['text_a']).toarray() # テストデータのtext_aをベクトル化\n",
    "X_test_b = tfidf.transform(test_data['text_b']).toarray() # テストデータのtext_bをベクトル化\n",
    "X_test = np.hstack([X_test_a, X_test_b]) # テストデータのベクトルを結合\n",
    "X_test = scaler.transform(X_test) # テストデータをスケーリング\n",
    "\n",
    "# テストセットに対して予測を行う\n",
    "test_predictions = model.predict(X_test) # テストデータで予測\n",
    "\n",
    "# 提出ファイルを作成する\n",
    "submission = pd.DataFrame(test_data['id']) # 提出用のデータフレームを作成\n",
    "submission['winner_model_a'] = test_predictions[:, 0] # モデルAの勝者確率を追加\n",
    "submission['winner_model_b'] = test_predictions[:, 1] # モデルBの勝者確率を追加\n",
    "submission['winner_tie'] = test_predictions[:, 2] # タイの勝者確率を追加\n",
    "submission.to_csv('submission.csv', index=False) # CSVファイルとして保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
