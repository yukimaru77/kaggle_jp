{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "595c538a",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyter Notebookã§ã¯ã€ã€ŒLMSYS - Chatbot Arenaã€ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«å‚åŠ ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€å¤§è¦æ¨¡è¨€èªžãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã‚’ç”¨ã„ã¦ã€ç•°ãªã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ã‚’æ¯”è¼ƒã—ã€ã©ã¡ã‚‰ã®å¿œç­”ãŒå¥½ã¾ã‚Œã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ã‚¿ã‚¹ã‚¯ã«å–ã‚Šçµ„ã‚“ã§ã„ã¾ã™ã€‚\n",
    "\n",
    "### å•é¡Œã«å–ã‚Šçµ„ã‚“ã§ã„ã‚‹å†…å®¹\n",
    "Notebookã¯ã€äººé–“ã«ã‚ˆã‚‹ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ã®é¸æŠžã«åŸºã¥ã„ã¦ã€ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šå¥½ã¾ã—ã„ã‹ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒé¸ã‚“ã å¿œç­”ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚ãã®ãŸã‚ã€å„å¿œç­”ãŒå‹ã¤ç¢ºçŽ‡ã‚’è¨ˆç®—ã—ã€ãã‚Œã«åŸºã¥ã„ã¦äºˆæ¸¬ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "### ä½¿ç”¨ã—ã¦ã„ã‚‹æ‰‹æ³•ã‚„ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "1. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒª**\n",
    "   - `transformers`: Hugging Faceã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã€DeBERTa V3ãƒ¢ãƒ‡ãƒ«ã‚’å«ã‚€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡žã‚¿ã‚¹ã‚¯ã‚’å‡¦ç†ã€‚\n",
    "   - `datasets`: ãƒ‡ãƒ¼ã‚¿ã‚’ç°¡å˜ã«å‡¦ç†ã™ã‚‹ãŸã‚ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã€‚\n",
    "   - `sklearn`: ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã¨ãƒ‡ãƒ¼ã‚¿å‡¦ç†ã«åˆ©ç”¨ã€‚\n",
    "   - `torch`: PyTorchã‚’ä½¿ç”¨ã—ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’æ§‹ç¯‰ã—ã€è¨“ç·´ã—ã¾ã™ã€‚\n",
    "\n",
    "2. **ä¸»è¦ãªæ‰‹æ³•**\n",
    "   - **ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†**: ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã‚’è¡Œã„ã€ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ã«ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›ã—ã¾ã™ã€‚\n",
    "   - **ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰**: DeBERTa V3ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€åˆ†é¡žã‚¿ã‚¹ã‚¯ã¨ã—ã¦è¨­å®šã€‚\n",
    "   - **ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°**: Stratified K-Foldäº¤å·®æ¤œè¨¼ã‚’ç”¨ã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã¨æ¤œè¨¼ã‚’è¡Œã„ã€æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠžã—ã¾ã™ã€‚\n",
    "   - **è©•ä¾¡æŒ‡æ¨™**: ãƒ­ã‚°æå¤±(log loss)ã¨ç²¾åº¦(accuracy)ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "\n",
    "3. **è¨“ç·´ãŠã‚ˆã³æŽ¨è«–è¨­å®š**\n",
    "   - è¨“ç·´ã¨è©•ä¾¡ã®ãŸã‚ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆå­¦ç¿’çŽ‡ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºãªã©ï¼‰ã‚’è¨­å®šã—ã€æ··åˆç²¾åº¦è¨“ç·´ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "   - å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ãŸå¾Œã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ãƒ¢ãƒ‡ãƒ«ãŒç”Ÿæˆã—ãŸäºˆæ¸¬ã‚’å–å¾—ã—ã€çµæžœã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®Notebookã¯ã€ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã€æŽ¨è«–ã€çµæžœã®ä¿å­˜ã¾ã§ã®å…¨éŽç¨‹ã‚’ã‚«ãƒãƒ¼ã—ã¦ãŠã‚Šã€ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³å‚åŠ è€…ãŒè¿…é€Ÿã«ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã€æå‡ºã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8472acaa",
   "metadata": {},
   "source": [
    "# ç”¨èªžæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã¯ã€Jupyter Notebookå†…ã®ã‚³ãƒ¼ãƒ‰ã‚„ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã«å‡ºã¦ãã‚‹å°‚é–€ç”¨èªžã®è§£èª¬ã§ã™ã€‚ç‰¹ã«åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªéƒ¨åˆ†ã‚„ã€ã“ã®ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç‰¹æœ‰ã®ç”¨èªžã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å°‚é–€ç”¨èªžã®è§£èª¬\n",
    "\n",
    "1. **StratifiedKFold**:\n",
    "   - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸€æ‰‹æ³•ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ©ãƒ™ãƒ«åˆ†å¸ƒãŒãã‚Œãžã‚Œã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã«å‡ç­‰ã«åˆ†é…ã•ã‚Œã‚‹ã‚ˆã†ã«è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†ã‘ã‚‹æ–¹æ³•ã€‚ã‚¯ãƒ©ã‚¹ã®ä¸å‡è¡¡å•é¡Œã‚’è»½æ¸›ã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚\n",
    "\n",
    "2. **AutoTokenizer**:\n",
    "   - Hugging Faceã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«ãŠã‘ã‚‹ã‚¯ãƒ©ã‚¹ã§ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã‚€ãŸã‚ã®ã‚‚ã®ã€‚ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ•°å€¤ã®åˆ—ï¼‰ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ã€‚\n",
    "\n",
    "3. **attention_mask**:\n",
    "   - ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒæ„å‘³ã‚’æŒã¡ã€ã©ã‚ŒãŒãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§ã‚ã‚‹ã‹ã‚’ç¤ºã™ãƒã‚¤ãƒŠãƒªãƒžã‚¹ã‚¯ã€‚1ã¯æœ‰åŠ¹ãªãƒˆãƒ¼ã‚¯ãƒ³ã€0ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç¤ºã™ã€‚\n",
    "\n",
    "4. **DataCollatorWithPadding**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ã®ä¸€éƒ¨ã§ã€ãƒãƒƒãƒå†…ã®ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã‚’æƒãˆã‚‹ãŸã‚ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã™ã‚‹å½¹å‰²ã‚’æŒã¤ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒŸãƒ‹ãƒãƒƒãƒãŒåŒã˜å½¢çŠ¶ã‚’æŒã¤ã‚ˆã†ã«ãªã‚‹ã€‚\n",
    "\n",
    "5. **log_loss**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ç¢ºçŽ‡ã¨å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«ã¨ã®é–“ã®èª¤å·®ã‚’æ¸¬ã‚‹æŒ‡æ¨™ã€‚ç‰¹ã«ã‚¯ãƒ©ã‚¹ç¢ºçŽ‡ã®ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã«ä½¿ã‚ã‚Œã‚‹ã€‚å€¤ãŒå°ã•ã„ã»ã©ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ãŒè‰¯ã„ã“ã¨ã‚’ç¤ºã™ã€‚\n",
    "\n",
    "6. **fp16 (åŠç²¾åº¦æµ®å‹•å°æ•°ç‚¹)**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã«ãŠã„ã¦ä½¿ç”¨ã•ã‚Œã‚‹ãƒ‡ãƒ¼ã‚¿åž‹ã®ä¸€ã¤ã§ã€é€šå¸¸ã®32ãƒ“ãƒƒãƒˆæµ®å‹•å°æ•°ç‚¹æ•°ï¼ˆfp32ï¼‰ã‚ˆã‚Šã‚‚ãƒ¡ãƒ¢ãƒªã‚’å°‘ãªãã™ã‚‹ã“ã¨ãŒã§ãã€è¨ˆç®—é€Ÿåº¦ã‚‚å‘ä¸Šã™ã‚‹ã€‚ä¸€éƒ¨ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã§ã¯ç‰¹ã«åŠ¹çŽ‡çš„ã€‚\n",
    "\n",
    "7. **EarlyStoppingCallback**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ä¸­ã«ã€æŒ‡å®šã•ã‚ŒãŸå›žæ•°ï¼ˆpatienceï¼‰ã§è©•ä¾¡ãƒ¡ãƒˆãƒªãƒƒã‚¯ãŒæ”¹å–„ã—ãªã„å ´åˆã«è¨“ç·´ã‚’åœæ­¢ã™ã‚‹ãŸã‚ã®ä»•çµ„ã¿ã€‚ã‚ªãƒ¼ãƒãƒ¼ãƒ•ã‚£ãƒƒãƒ†ã‚£ãƒ³ã‚°ã‚’é˜²ãã®ã«å½¹ç«‹ã¤ã€‚\n",
    "\n",
    "8. **num_labels**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®ã‚¢ã‚¦ãƒˆãƒ—ãƒƒãƒˆå±¤ã«ãŠã„ã¦äºˆæ¸¬å¯èƒ½ãªã‚¯ãƒ©ã‚¹ã®æ•°ã‚’æŒ‡å®šã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ãƒã‚¤ãƒŠãƒªåˆ†é¡žã‚„å¤šã‚¯ãƒ©ã‚¹åˆ†é¡žã®å ´åˆã«ã€ãã‚Œãžã‚Œã®ã‚¯ãƒ©ã‚¹æ•°ã«è¨­å®šã•ã‚Œã‚‹ã€‚\n",
    "\n",
    "9. **weight_decay**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹éš›ã«ã€é‡ã¿ã®å¤§ãã•ã‚’æŠ‘ãˆã€éŽå­¦ç¿’ã‚’é˜²ããŸã‚ã®ãƒšãƒŠãƒ«ãƒ†ã‚£ã®è¨­å®šã€‚L2æ­£å‰‡åŒ–ã¨ã‚‚å‘¼ã°ã‚Œã€è¨­å®šã•ã‚ŒãŸå€¤ã«æ¯”ä¾‹ã—ã¦é‡ã¿ã‚’æ¸›è¡°ã•ã›ã‚‹ã€‚\n",
    "\n",
    "10. **train_fraction**:\n",
    "    - ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®ä¸­ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®å‰²åˆã‚’æŒ‡å®šã™ã‚‹ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‚ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚„ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ã®é©åˆ‡ãªåˆ†å‰²ã‚’è¡Œã†ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªžãŒç†è§£ã§ãã‚‹ã‚ˆã†ã«ãªã‚‹ã¨ã€Notebookã®å†…å®¹ã‚’ã‚ˆã‚Šè‰¯ãæŠŠæ¡ã—ã€å®Ÿè·µçš„ãªæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™ºã«å½¹ç«‹ã¤ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-06T16:45:31.260392Z",
     "iopub.status.busy": "2024-07-06T16:45:31.260005Z",
     "iopub.status.idle": "2024-07-06T16:45:31.582586Z",
     "shell.execute_reply": "2024-07-06T16:45:31.581769Z",
     "shell.execute_reply.started": "2024-07-06T16:45:31.26036Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorWithPadding\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import log_loss\n",
    "import torch\n",
    "from functools import partial\n",
    "import warnings\n",
    "from transformers import logging as transformers_logging\n",
    "from transformers import EarlyStoppingCallback\n",
    "import json\n",
    "from pprint import pformat\n",
    "from tqdm import trange\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "TYPE = \"large\"\n",
    "VER= 14\n",
    "DATE = \"0717\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "# ãƒ­ã‚®ãƒ³ã‚°ã®è¨­å®š\n",
    "transformers_logging.set_verbosity_error()\n",
    "logging.basicConfig(level=logging.INFO, filename=f'logs_v{VER}.log', filemode='a',\n",
    "                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PATHS:\n",
    "    train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    sub_path = '/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'  # æå‡ºã‚µãƒ³ãƒ—ãƒ«ã®ãƒ‘ã‚¹\n",
    "    model_name = f\"deberta-v3-{TYPE}\"  # ãƒ¢ãƒ‡ãƒ«å\n",
    "    model_path = f\"/root/autodl-tmp/ase2/huggingfacedebertav3variants/{model_name}\"  # ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ãƒ‘ã‚¹\n",
    "    tokenizer_path = f\"/kaggle/input/lmsys-{TYPE}{VER}-{DATE}/fold_0/tokenizer\"  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ã‚¹\n",
    "    general_tokenizer = \"/kaggle/input/lmsys-base4-0704/fold_0/tokenizer\"  # ä¸€èˆ¬çš„ãªãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®ãƒ‘ã‚¹\n",
    "\n",
    "class CFG:\n",
    "    seed = 42  # ä¹±æ•°ã‚·ãƒ¼ãƒ‰\n",
    "    max_length = 512  # æœ€å¤§å…¥åŠ›é•·\n",
    "    lr = 5e-5  # å­¦ç¿’çŽ‡\n",
    "    weight_decay = 0.01  # é‡ã¿æ¸›è¡°\n",
    "    warmup_ratio = 0 # å­¦ç¿’çŽ‡ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”çŽ‡\n",
    "    max_grad_norm = 1000  # æœ€å¤§å‹¾é…ãƒŽãƒ«ãƒ \n",
    "    lr_scheduler_type = 'linear'  # å­¦ç¿’çŽ‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚¿ã‚¤ãƒ—\n",
    "    frozen_embedding = False # åŸ‹ã‚è¾¼ã¿å±¤ã‚’å‡çµã™ã‚‹ã‹ã©ã†ã‹\n",
    "    frozen_num = 6  # å‡çµã™ã‚‹å±¤ã®æ•°\n",
    "    train_batch_size = 32  # è¨“ç·´ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    eval_batch_size = 64  # è©•ä¾¡ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    evaluation_strategy = 'steps'  # è©•ä¾¡æˆ¦ç•¥ã‚’ã‚¹ãƒ†ãƒƒãƒ—ã«è¨­å®š\n",
    "    metric_for_best_model = \"eval_log_loss\"  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«é¸æŠžã®ãŸã‚ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯\n",
    "    save_strategy = 'steps'  # ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ä¿å­˜\n",
    "    save_steps = 200  # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "    save_total_limit = 1  # ä¿å­˜ã•ã‚Œã‚‹ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®åˆè¨ˆæ•°åˆ¶é™\n",
    "    train_epochs = 5  # è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "    num_labels = 6  # ãƒ©ãƒ™ãƒ«ã®æ•°\n",
    "    output_dir = f'/kaggle/input/lmsys-{TYPE}{VER}-{DATE}'  # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    fp16 = True  # æ··åˆç²¾åº¦è¨“ç·´ã‚’ä½¿ç”¨\n",
    "    load_best_model_at_end = True  # è¨“ç·´çµ‚äº†æ™‚ã«æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "    report_to = 'none'  # å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã«è¨“ç·´ãƒ­ã‚°ã‚’å ±å‘Šã—ãªã„\n",
    "    optim = 'adamw_torch'  # ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã‚¿ã‚¤ãƒ—\n",
    "    logging_first_step = True  # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹\n",
    "    logging_steps = 200  # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "    logging_dir =f'logs_v{VER}'  # ãƒ­ã‚°ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "    n_splits = 5  # ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®åˆ†å‰²æ•°\n",
    "    model_name = PATHS.model_name  # ãƒ¢ãƒ‡ãƒ«å\n",
    "    greater_is_better = False  # ãƒ¡ãƒˆãƒªãƒƒã‚¯ãŒå¤§ãã„ã»ã©è‰¯ã„ã‹ã©ã†ã‹\n",
    "    early_stop = False  # æ—©æœŸåœæ­¢ã‚’ä½¿ç”¨ã™ã‚‹ã‹\n",
    "    early_stopping_patience = 3  # æ”¹å–„ãªã—ã§ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’åœæ­¢ã™ã‚‹è©•ä¾¡ã®å‘¼ã³å‡ºã—å›žæ•°\n",
    "    early_stopping_threshold = 0.001  # æ”¹å–„ã¨è¦‹ãªã™ãŸã‚ã®æœ€å°å¤‰åŒ–é‡\n",
    "\n",
    "def seed_everything(seed):\n",
    "    import random\n",
    "    import os\n",
    "    import numpy as np\n",
    "    import torch\n",
    "    \n",
    "    random.seed(seed)  # Pythonã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # ãƒãƒƒã‚·ãƒ¥ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    np.random.seed(seed)  # NumPyã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    torch.manual_seed(seed)  # PyTorchã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    torch.cuda.manual_seed(seed)  # CUDAã®ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    torch.backends.cudnn.deterministic = True  # å†ç¾æ€§ã‚’æŒãŸã›ã‚‹\n",
    "\n",
    "seed_everything(seed=CFG.seed)  # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã®è¨­å®š\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(PATHS.tokenizer_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATHS.general_tokenizer)  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "sep_token = tokenizer.sep_token_id  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³ã®IDã‚’å–å¾—\n",
    "\n",
    "def log_parameters(logger):\n",
    "    \"\"\"PATHSã¨CFGã‚¯ãƒ©ã‚¹ã‹ã‚‰ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²ã—ã¾ã™ã€‚\"\"\"\n",
    "    logger.info(\"=== ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š ===\")\n",
    "    \n",
    "    logger.info(\"PATHS:\")\n",
    "    for key, value in PATHS.__dict__.items():\n",
    "        if not key.startswith('__'):\n",
    "            logger.info(f\"  {key}: {value}\")  # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "    \n",
    "    logger.info(\"CFG:\")\n",
    "    for key, value in CFG.__dict__.items():\n",
    "        if not key.startswith('__'):\n",
    "            logger.info(f\"  {key}: {value}\")  # å„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "    \n",
    "    logger.info(\"=*100\")\n",
    "\n",
    "def tokenize_function(row, tokenizer):\n",
    "    max_len = CFG.max_length - 2  # ã‚»ãƒ‘ãƒ¬ãƒ¼ã‚¿ãƒˆãƒ¼ã‚¯ãƒ³ã®ãŸã‚ã«2ã‚’å¼•ã\n",
    "    tokens_prompt = tokenizer(row['prompt'], truncation=True, max_length=max_len//4, add_special_tokens=False)['input_ids']  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    remaining_length = max_len - len(tokens_prompt)  # æ®‹ã‚Šã®é•·ã•ã‚’è¨ˆç®—\n",
    "    \n",
    "    tokens_response_a = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2, add_special_tokens=False)['input_ids']  # å¿œç­”Aã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    remaining_length -= len(tokens_response_a)  # æ®‹ã‚Šã®é•·ã•ã‚’æ›´æ–°\n",
    "    tokens_response_b = tokenizer(row['response_b'], truncation=True, max_length=remaining_length, add_special_tokens=False)['input_ids']  # å¿œç­”Bã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    \n",
    "    input_ids = [tokenizer.cls_token_id] + tokens_prompt + [sep_token] + tokens_response_a + [sep_token] + tokens_response_b  # å…¥åŠ›IDã‚’ä½œæˆ\n",
    "    token_type_ids = [0] * (len(tokens_prompt) + 2) + [1] * (len(tokens_response_a) + 1) + [2] * len(tokens_response_b)  # ãƒˆãƒ¼ã‚¯ãƒ³ã‚¿ã‚¤ãƒ—IDsã‚’ä½œæˆ\n",
    "    attention_mask = [1] * len(input_ids)  # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒžã‚¹ã‚¯ã‚’ä½œæˆ\n",
    "    \n",
    "    padding_length = CFG.max_length - len(input_ids)  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®é•·ã•ã‚’è¨ˆç®—\n",
    "    if padding_length > 0:\n",
    "        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ \n",
    "        token_type_ids = token_type_ids + [0] * padding_length  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ \n",
    "        attention_mask = attention_mask + [0] * padding_length  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ \n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids[:CFG.max_length],  # æœ€å¤§é•·ã«åˆ¶é™ã™ã‚‹\n",
    "        'token_type_ids': token_type_ids[:CFG.max_length],  # æœ€å¤§é•·ã«åˆ¶é™ã™ã‚‹\n",
    "        'attention_mask': attention_mask[:CFG.max_length],  # æœ€å¤§é•·ã«åˆ¶é™ã™ã‚‹\n",
    "    }\n",
    "\n",
    "def add_label(df):\n",
    "    labels = np.zeros(len(df), dtype=np.int32)  # ãƒ©ãƒ™ãƒ«ã®é…åˆ—ã‚’ä½œæˆ\n",
    "    labels[df['winner_model_a'] == 1] = 0  # ãƒ¢ãƒ‡ãƒ«AãŒå‹ã£ãŸå ´åˆã®ãƒ©ãƒ™ãƒ«\n",
    "    labels[df['winner_model_b'] == 1] = 1  # ãƒ¢ãƒ‡ãƒ«BãŒå‹ã£ãŸå ´åˆã®ãƒ©ãƒ™ãƒ«\n",
    "    labels[df['winner_tie'] == 1] = 2  # å¼•ãåˆ†ã‘ã®å ´åˆã®ãƒ©ãƒ™ãƒ«\n",
    "    df['labels'] = labels  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \n",
    "    return df\n",
    "\n",
    "def process_data(df, mode='train'):\n",
    "    dataset = Dataset.from_pandas(df)  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ\n",
    "    tokenized_dataset = dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=False)  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "    remove_cols = ['id', 'prompt', 'response_a', 'response_b']  # å‰Šé™¤ã™ã‚‹åˆ—ã‚’æŒ‡å®š\n",
    "    if mode == 'train':\n",
    "        remove_cols += ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ¢ãƒ¼ãƒ‰ã®å ´åˆã€ã•ã‚‰ã«åˆ—ã‚’å‰Šé™¤\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(remove_cols)  # æŒ‡å®šã—ãŸåˆ—ã‚’å‰Šé™¤\n",
    "    return tokenized_dataset\n",
    "\n",
    "def split_train_val(dataset, train_fraction):\n",
    "    np.random.seed(0)  # ä¹±æ•°ã‚·ãƒ¼ãƒ‰ã‚’è¨­å®š\n",
    "    ixs = np.arange(len(dataset))  # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ç”Ÿæˆ\n",
    "    cutoff = int(len(ixs) * train_fraction)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®ã‚«ãƒƒãƒˆã‚ªãƒ•ãƒã‚¤ãƒ³ãƒˆ\n",
    "    np.random.shuffle(ixs)  # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«\n",
    "    ixs_train = ixs[:cutoff]  # è¨“ç·´ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    ixs_val = ixs[cutoff:]  # æ¤œè¨¼ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹\n",
    "    fit_train = dataset.select(ixs_train)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠž\n",
    "    fit_val = dataset.select(ixs_val)  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠž\n",
    "    return fit_train, fit_val\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred  # è©•ä¾¡äºˆæ¸¬ã‹ã‚‰ãƒ­ã‚¸ãƒƒãƒˆã¨ãƒ©ãƒ™ãƒ«ã‚’å–å¾—\n",
    "    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)  # ç¢ºçŽ‡ã‚’è¨ˆç®—\n",
    "    return {\n",
    "        'eval_log_loss': log_loss(labels, probabilities),  # ãƒ­ã‚°æå¤±ã‚’è¨ˆç®—\n",
    "        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()  # ç²¾åº¦ã‚’è¨ˆç®—\n",
    "    }\n",
    "    \n",
    "def train_model():\n",
    "    log_parameters(logger)  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "    train_df = pd.read_csv(PATHS.train_path)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "    train_df = add_label(train_df)  # ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ \n",
    "    train_tokenized = process_data(train_df, mode='train')  # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)  # ã‚¹ãƒˆãƒ©ãƒ†ã‚£ãƒ•ã‚¡ã‚¤ãƒ‰Kãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã‚’ä½œæˆ\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(train_tokenized, train_tokenized['labels'])):  # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨æ¤œè¨¼ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åˆ†å‰²\n",
    "        print(f\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1} ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\")  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ç•ªå·ã‚’å‡ºåŠ›\n",
    "        logger.info(f\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1} ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\")  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ç•ªå·ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        \n",
    "        fit_train = train_tokenized.select(train_idx)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠž\n",
    "        fit_val = train_tokenized.select(val_idx)  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚’é¸æŠž\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(  # äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "            PATHS.model_path,\n",
    "            num_labels=3,  # ãƒ©ãƒ™ãƒ«æ•°ã‚’æŒ‡å®š\n",
    "            problem_type=\"single_label_classification\"  # å•é¡Œã‚¿ã‚¤ãƒ—ã‚’æŒ‡å®š\n",
    "        )\n",
    "        \n",
    "        training_args = TrainingArguments(  # è¨“ç·´å¼•æ•°ã‚’è¨­å®š\n",
    "            output_dir=f\"{CFG.output_dir}/fold_{fold}\",  # ãƒ¢ãƒ‡ãƒ«ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "            fp16=CFG.fp16,  # æ··åˆç²¾åº¦è¨“ç·´ã‚’ä½¿ç”¨\n",
    "            learning_rate=CFG.lr,  # å­¦ç¿’çŽ‡\n",
    "            per_device_train_batch_size=CFG.train_batch_size,  # å„ãƒ‡ãƒã‚¤ã‚¹ã§ã®è¨“ç·´ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "            per_device_eval_batch_size=CFG.eval_batch_size,  # å„ãƒ‡ãƒã‚¤ã‚¹ã§ã®è©•ä¾¡ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "            num_train_epochs=CFG.train_epochs,  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ã‚¨ãƒãƒƒã‚¯æ•°\n",
    "            weight_decay=CFG.weight_decay,  # é‡ã¿æ¸›è¡°\n",
    "            evaluation_strategy=CFG.evaluation_strategy,  # è©•ä¾¡æˆ¦ç•¥\n",
    "            metric_for_best_model=CFG.metric_for_best_model,  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’é¸æŠžã™ã‚‹ãŸã‚ã®ãƒ¡ãƒˆãƒªãƒƒã‚¯\n",
    "            save_strategy=CFG.save_strategy,  # ä¿å­˜æˆ¦ç•¥\n",
    "            save_total_limit=CFG.save_total_limit,  # ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®ç·ä¿å­˜æ•°åˆ¶é™\n",
    "            load_best_model_at_end=CFG.load_best_model_at_end,  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´çµ‚äº†æ™‚ã«ãƒ­ãƒ¼ãƒ‰\n",
    "            report_to=CFG.report_to,  # ãƒ­ã‚°ã‚’å¤–éƒ¨ãƒ„ãƒ¼ãƒ«ã«å ±å‘Šã—ãªã„\n",
    "            warmup_ratio=CFG.warmup_ratio,  # å­¦ç¿’çŽ‡ã®ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æ¯”çŽ‡\n",
    "            lr_scheduler_type=CFG.lr_scheduler_type,  # å­¦ç¿’çŽ‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼ã®ã‚¿ã‚¤ãƒ—\n",
    "            optim=CFG.optim,  # ä½¿ç”¨ã™ã‚‹ã‚ªãƒ—ãƒ†ã‚£ãƒžã‚¤ã‚¶ã®ã‚¿ã‚¤ãƒ—\n",
    "            logging_first_step=CFG.logging_first_step,  # æœ€åˆã®ã‚¹ãƒ†ãƒƒãƒ—ã®ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "            greater_is_better=CFG.greater_is_better,  # ãƒ¡ãƒˆãƒªãƒƒã‚¯ãŒå¤§ãã„ã»ã©è‰¯ã„ã‹ã©ã†ã‹\n",
    "            \n",
    "            # max_grad_norm=CFG.max_grad_norm,  # å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã®è¨­å®š\n",
    "            \n",
    "            logging_steps=CFG.logging_steps,  # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ­ã‚°ã‚’è¨˜éŒ²\n",
    "            logging_dir =f'logs_v{VER}',  # ãƒ­ã‚°ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "        \n",
    "            save_steps=CFG.save_steps,  # 200ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "            eval_steps=CFG.save_steps,  # eval_stepsãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¿½åŠ ã€save_stepsã¨ä¸€è‡´ã•ã›ã‚‹\n",
    "        )\n",
    "\n",
    "         # è¨“ç·´å¼•æ•°ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        logger.info(\"è¨“ç·´å¼•æ•°:\")\n",
    "        logger.info(pformat(training_args.to_dict()))\n",
    "\n",
    "        if CFG.frozen_embedding:  # åŸ‹ã‚è¾¼ã¿å±¤ã‚’å‡çµã™ã‚‹è¨­å®šã®å ´åˆ\n",
    "            n = CFG.frozen_num\n",
    "            # åŸ‹ã‚è¾¼ã¿å±¤ã‚’å‡çµ\n",
    "            for i, layer in enumerate(model.deberta.encoder.layer[:n]):\n",
    "                for param in layer.parameters():\n",
    "                    param.requires_grad = False  # è¨“ç·´ã‹ã‚‰é™¤å¤–\n",
    "            for param in model.deberta.embeddings.parameters():\n",
    "                param.requires_grad = False  # è¨“ç·´ã‹ã‚‰é™¤å¤–\n",
    "\n",
    "        # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®åˆæœŸåŒ–\n",
    "        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ©ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼ã®åˆæœŸåŒ–\n",
    "\n",
    "        # EarlyStoppingCallbackã®ä½œæˆ\n",
    "        if CFG.early_stop:  # æ—©æœŸåœæ­¢ã‚’ä½¿ç”¨ã™ã‚‹è¨­å®šã®å ´åˆ\n",
    "            early_stopping_callback = EarlyStoppingCallback(\n",
    "                early_stopping_patience=CFG.early_stopping_patience,  # è¨“ç·´åœæ­¢ã®è€ä¹…æœŸé–“\n",
    "                early_stopping_threshold=CFG.early_stopping_threshold,  # æ”¹å–„ã¨è¦‹ãªã™æœ€å°å¤‰åŒ–\n",
    "            )\n",
    "        \n",
    "            trainer = Trainer(  # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=fit_train,  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "                data_collator=data_collator,  # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ©ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼\n",
    "                eval_dataset=fit_val,  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "                compute_metrics=compute_metrics,  # ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®è¨ˆç®—é–¢æ•°\n",
    "                callbacks=[early_stopping_callback],  # æ—©æœŸåœæ­¢ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ \n",
    "            )\n",
    "        else:\n",
    "            trainer = Trainer(  # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–\n",
    "                model=model,\n",
    "                args=training_args,\n",
    "                train_dataset=fit_train,  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "                data_collator=data_collator,  # ãƒ‡ãƒ¼ã‚¿ã‚³ãƒ©ãƒˆãƒ¬ãƒ¼ã‚¿ãƒ¼\n",
    "                eval_dataset=fit_val,  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ\n",
    "                compute_metrics=compute_metrics,  # ãƒ¡ãƒˆãƒªãƒƒã‚¯ã®è¨ˆç®—é–¢æ•°\n",
    "            )\n",
    "        \n",
    "        trainer.train()  # ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚’é–‹å§‹\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "        trainer.save_model(f\"{CFG.output_dir}/fold_{fold}/best_model\")  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜\n",
    "        tokenizer.save_pretrained(f\"{CFG.output_dir}/fold_{fold}/tokenizer\")  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿å­˜\n",
    "        \n",
    "        # çµæžœã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        eval_result = trainer.evaluate()  # è©•ä¾¡ã‚’å®Ÿæ–½\n",
    "        logger.info(f\"ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ {fold + 1} - è©•ä¾¡çµæžœ: {eval_result}\")  # è©•ä¾¡çµæžœã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "        logger.info(\"=*100\")\n",
    "\n",
    "def predict_test():\n",
    "    test_df = pd.read_csv(PATHS.test_path)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿\n",
    "    test_tokenized = process_data(test_df, mode='test')  # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "    \n",
    "    predictions = []  # äºˆæ¸¬çµæžœã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "    \n",
    "    for fold in trange(CFG.n_splits):  # å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§äºˆæ¸¬ã‚’è¡Œã†\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(f\"{CFG.output_dir}/fold_{fold}/best_model\")  # æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "        model.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        \n",
    "        trainer = Trainer(model=model)  # ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–\n",
    "        fold_preds = trainer.predict(test_tokenized).predictions  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ã‚’å–å¾—\n",
    "        fold_preds = np.exp(fold_preds) / np.sum(np.exp(fold_preds), axis=1, keepdims=True)  # ç¢ºçŽ‡ã«å¤‰æ›\n",
    "        predictions.append(fold_preds)  # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®äºˆæ¸¬çµæžœã‚’è¿½åŠ \n",
    "    \n",
    "    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰é–“ã®äºˆæ¸¬ã‚’å¹³å‡åŒ–\n",
    "    final_preds = np.mean(predictions, axis=0)  # æœ€çµ‚äºˆæ¸¬ã‚’è¨ˆç®—\n",
    "    display(predictions)  # äºˆæ¸¬çµæžœã‚’è¡¨ç¤º\n",
    "    logger.info(f\"æœ€çµ‚äºˆæ¸¬: {final_preds}\")  # æœ€çµ‚äºˆæ¸¬ã‚’ãƒ­ã‚°ã«è¨˜éŒ²\n",
    "    \n",
    "    # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_df['id'],  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ID\n",
    "        'winner_model_a': final_preds[:, 0],  # ãƒ¢ãƒ‡ãƒ«Aã®å‹è€…ç¢ºçŽ‡\n",
    "        'winner_model_b': final_preds[:, 1],  # ãƒ¢ãƒ‡ãƒ«Bã®å‹è€…ç¢ºçŽ‡\n",
    "        'winner_tie': final_preds[:, 2]  # å¼•ãåˆ†ã‘ç¢ºçŽ‡\n",
    "    })\n",
    "    \n",
    "    submission.to_csv('submission.csv', index=False)  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVå½¢å¼ã§ä¿å­˜\n",
    "    display(submission)  # æå‡ºçµæžœã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T16:45:31.584644Z",
     "iopub.status.busy": "2024-07-06T16:45:31.584158Z",
     "iopub.status.idle": "2024-07-06T16:45:35.020468Z",
     "shell.execute_reply": "2024-07-06T16:45:35.019751Z",
     "shell.execute_reply.started": "2024-07-06T16:45:31.584608Z"
    }
   },
   "outputs": [],
   "source": [
    "%time  # ã‚³ãƒ¼ãƒ‰ã®å®Ÿè¡Œæ™‚é–“ã‚’è¨ˆæ¸¬ã—ã¾ã™\n",
    "if __name__ == \"__main__\":  # ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆãŒãƒ¡ã‚¤ãƒ³ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã¨ã—ã¦å®Ÿè¡Œã•ã‚Œã¦ã„ã‚‹å ´åˆ\n",
    "#     train_model()  # ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã™ã‚‹é–¢æ•°ã‚’å‘¼ã³å‡ºã™ï¼ˆã‚³ãƒ¡ãƒ³ãƒˆã‚¢ã‚¦ãƒˆä¸­ï¼‰\n",
    "    predict_test()  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬ã‚’è¡Œã†é–¢æ•°ã‚’å‘¼ã³å‡ºã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T14:28:38.542344Z",
     "iopub.status.busy": "2024-07-04T14:28:38.541923Z",
     "iopub.status.idle": "2024-07-04T14:28:38.547292Z",
     "shell.execute_reply": "2024-07-04T14:28:38.546354Z",
     "shell.execute_reply.started": "2024-07-04T14:28:38.542313Z"
    }
   },
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™\n",
    "# print(tokenizers.__version__)  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã‚’å‡ºåŠ›ã—ã¾ã™"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d044991",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ã‚³ãƒ¡ãƒ³ãƒˆ\n",
    "\n",
    "> ## Rise_Hand\n",
    "> \n",
    "> ã„ã¤ã‚‚ç´ æ™´ã‚‰ã—ã„ä»•äº‹ã ã­ã€å…„å¼Ÿã€‚\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Yuxi Xue\n",
    "> \n",
    "> ç´ æ™´ã‚‰ã—ã„ä»•äº‹ï¼ï¼ï¼ï¼ï¼\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Korey Ma\n",
    "> \n",
    "> ã„ã„ä»•äº‹ã ã­ï¼ï¼ï¼ðŸ¥³\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## YEI0907\n",
    "> \n",
    "> å…„å¼Ÿã€æŽ¨è«–ã«ã©ã®ãã‚‰ã„æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã—ãŸã‹ï¼Ÿ\n",
    "> \n",
    "> \n",
    "> > ## Roschild.Ruiï¼ˆãƒˆãƒ”ãƒƒã‚¯ä½œæˆè€…ï¼‰\n",
    "> > \n",
    "> > ã ã„ãŸã„2ã€œ3æ™‚é–“ãã‚‰ã„ã ã¨æ€ã„ã¾ã™ã€‚\n",
    "> > \n",
    "> > \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5333407,
     "sourceId": 8859017,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5342754,
     "sourceId": 8875732,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5345416,
     "sourceId": 8882976,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5331941,
     "sourceId": 8857082,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5349810,
     "sourceId": 8898264,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5349807,
     "sourceId": 8898258,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5349814,
     "sourceId": 8898272,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5352224,
     "sourceId": 8902791,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5352375,
     "sourceId": 8902994,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5363851,
     "sourceId": 8918757,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369514,
     "sourceId": 8926669,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5404689,
     "sourceId": 8976237,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 177816615,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
