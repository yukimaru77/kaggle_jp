{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fdec13",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、Hugging FaceのTransformersライブラリを使用して、テキスト分類モデルのロード、前処理、およびトレーニングに関する包括的なガイドを提供しています。具体的には、人間による好み予測に関連するデータセットを扱い、プロンプトに対する応答モデルのパフォーマンスを評価します。以下にノートブックの各セクションの概要を示します。\n",
    "\n",
    "1. **データのロードとサンプリング**:\n",
    "   - CSVファイルからデータを読み込み、全体のデータセットの10%をランダムにサンプリングします。これにより、計算リソースを効率的に使いながら処理を迅速化します。\n",
    "\n",
    "2. **データの分割**:\n",
    "   - サンプリングしたデータをトレーニングセットとバリデーションセットに分割し、モデル性能の評価を可能にします。\n",
    "\n",
    "3. **トークナイザーの準備**:\n",
    "   - 小型のRobertaモデルのトークナイザーを使用して、テキストデータをトークン化します。この処理には、パディング、切り捨て、およびPyTorchテンソル形式での戻り値が含まれています。\n",
    "\n",
    "4. **モデル設定とトレーニング**:\n",
    "   - モデルの設定を行い、トレーニング引数を定義した後、トレーニングを開始します。デバイスは、利用可能な場合はGPUを使用します。\n",
    "\n",
    "5. **推論関数の定義**:\n",
    "   - トレーニングされたモデルを使用して、トークン化されたデータに対して推論を行う関数を定義します。これにより、ノートブックは推論時のメモリ管理を最適化します。\n",
    "\n",
    "6. **評価と結果の保存**:\n",
    "   - テストデータをトークン化し、推論を行った結果をCSVファイルに保存します。最終的に、モデルのロジット結果とデータフレームを結合し、提出用のファイルを作成します。\n",
    "\n",
    "このノートブックは、データサンプリング、トークン化、モデル設定、推論、および結果保存という体系的なアプローチを通じて、テキスト分類モデルのトレーニングの流れを示しています。特に、計算リソースの効率的な使用を保ちながらモデルのパフォーマンスを維持することに重点が置かれています。使用されている主要なライブラリは、Pandas、Scikit-learn、およびHugging Face Transformersです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1e8d0",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyterノートブックで使用されている専門用語や概念の簡単な解説を示します。特に初心者がつまずきそうなものやこのノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **トークナイザー**:\n",
    "   - テキストデータをモデルが理解できる形に変換するためのツールです。具体的には文字や単語を「トークン」と呼ばれる単位に分割し、数値的な表現に変換します。\n",
    "\n",
    "2. **ロバート (RoBERTa)**:\n",
    "   - BERT（Bidirectional Encoder Representations from Transformers）を基にしたモデルで、自然言語処理タスクにおける性能を向上させるために、より多くのデータと計算資源を使用してトレーニングされています。\n",
    "\n",
    "3. **エンコーディング**:\n",
    "   - トークン化されたデータに対して、モデルが理解できる数値表現（通常はベクトル）の形式に変換するプロセスです。これは、トークンに対応する数値を用意することを含みます。\n",
    "\n",
    "4. **バッチサイズ**:\n",
    "   - モデルが一度に処理するデータのサンプル数を指します。大きすぎるとメモリが不足し、小さすぎると効率が悪くなります。\n",
    "\n",
    "5. **ウォームアップステップ**:\n",
    "   - 学習の初期段階で学習率を徐々に上げていく手法です。これにより、最初のエポックでの急激な変化による不安定さを避けます。\n",
    "\n",
    "6. **トレーニング引数 (Training Arguments)**:\n",
    "   - モデルトレーニング時に指定する設定やハイパーパラメータのことです。これにはエポック数、バッチサイズ、ロギングの頻度などが含まれます。\n",
    "\n",
    "7. **ロジット (Logits)**:\n",
    "   - モデルの出力で、各クラスに対するスコアを示す値です。これをsoftmax関数に通すことで、確率に変換されます。ロジットが高いほど、そのクラスが選ばれる可能性が高いとなります。\n",
    "\n",
    "8. **アテンションマスク (Attention Mask)**:\n",
    "   - トークン化されたデータの中で、どのトークンが有効か（例えば、パディングしたトークンを無視するためのマスク）を示すバイナリマスクです。1は有効、0は無効を表します。\n",
    "\n",
    "9. **カスタムデータセットクラス**:\n",
    "   - PyTorchのデータセットクラスを継承して作成したクラスで、トレーニングデータとラベルを格納し、データを効率よく取得できるようにカスタマイズされています。\n",
    "\n",
    "これらの用語や概念は、特に実務経験がない初心者にとって馴染みが少ないかもしれませんが、機械学習プロジェクトを実施する際には重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0f67a1",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "このノートブックは、Hugging Face Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する包括的なガイドを提供します。計算リソースを効率的に管理するために、ロバートトークナイザーと小型のロバートモデルを使用します。プロセスには、データのロード、サンプリング、トークナイゼーション、およびモデルの準備が含まれます。\n",
    "\n",
    "# ステップ 1: データのロードとサンプリング\n",
    "最初に、CSVファイルからデータセットをロードし、処理を迅速化するためにデータセットのサイズを減少させるためにランダムサンプルを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:15.596812Z",
     "iopub.status.busy": "2024-07-28T22:20:15.59644Z",
     "iopub.status.idle": "2024-07-28T22:20:17.317719Z",
     "shell.execute_reply": "2024-07-28T22:20:17.316693Z",
     "shell.execute_reply.started": "2024-07-28T22:20:15.596784Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データをロードする\n",
    "df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "\n",
    "# データの10%をサンプリングする\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# サンプルデータを確認する\n",
    "print(df_sample.head())  # サンプルデータの最初の5行を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f5d0d1",
   "metadata": {},
   "source": [
    "# ステップ 2: データをトレーニングセットとバリデーションセットに分割する\n",
    "データをトレーニングセットとバリデーションセットに分割し、モデルのパフォーマンスを評価します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:17.319584Z",
     "iopub.status.busy": "2024-07-28T22:20:17.319291Z",
     "iopub.status.idle": "2024-07-28T22:20:17.329557Z",
     "shell.execute_reply": "2024-07-28T22:20:17.328559Z",
     "shell.execute_reply.started": "2024-07-28T22:20:17.31956Z"
    }
   },
   "outputs": [],
   "source": [
    "# データをトレーニングセットとバリデーションセットに分割する\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_sample['prompt'].tolist(),  # プロンプト（入力テキスト）のリストを取得します\n",
    "    df_sample['winner_model_a'],    # モデルAの勝者ラベルを取得します\n",
    "    test_size=0.1,                  # データの10%をテストセットとします\n",
    "    random_state=42                 # 再現性のために乱数シードを設定します\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78b7029",
   "metadata": {},
   "source": [
    "# ステップ 3: トークナイザーをロードして準備する\n",
    "効率のために小型のロバートモデルのトークナイザーを使用し、テキストデータをトークン化します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:17.331562Z",
     "iopub.status.busy": "2024-07-28T22:20:17.330852Z",
     "iopub.status.idle": "2024-07-28T22:20:24.153345Z",
     "shell.execute_reply": "2024-07-28T22:20:24.152411Z",
     "shell.execute_reply.started": "2024-07-28T22:20:17.331527Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# トークナイザーをロードする\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # ロバートベースモデルのトークナイザーを取得します\n",
    "\n",
    "# トークン化関数\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # トークンのパディングを最大長に設定します\n",
    "        truncation=True,        # テキストが最大長を超えた場合に切り捨てます\n",
    "        max_length=128,        # 最大トークン数を128に設定します\n",
    "        return_tensors='pt'    # PyTorchテンソルとして戻します\n",
    "    )\n",
    "\n",
    "# テキストをトークン化する\n",
    "train_encodings = tokenize_function(train_texts)  # トレーニングデータをトークン化します\n",
    "val_encodings = tokenize_function(val_texts)      # バリデーションデータをトークン化します\n",
    "\n",
    "# トークン化されたデータを確認する\n",
    "print(train_encodings)  # トークン化されたトレーニングデータを表示します\n",
    "print(val_encodings)    # トークン化されたバリデーションデータを表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a4a54",
   "metadata": {},
   "source": [
    "# ステップ 4: モデルの設定を準備する\n",
    "モデルの設定を行い、適切なデバイス（利用可能な場合はGPU）を使用するようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:24.155417Z",
     "iopub.status.busy": "2024-07-28T22:20:24.15514Z",
     "iopub.status.idle": "2024-07-28T22:24:09.126503Z",
     "shell.execute_reply": "2024-07-28T22:24:09.125607Z",
     "shell.execute_reply.started": "2024-07-28T22:20:24.155391Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 設定クラス\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUが使用可能であればGPUを、そうでなければCPUを使用します\n",
    "        self.batch_size = 16  # バッチサイズを16に設定します\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# モデルをロードする\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(cfg.device)  # ロバートベースモデルを2つのラベルで分類するためにロードし、指定したデバイスに移動します\n",
    "\n",
    "# カスタムデータセットクラス\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # エンコーディングを格納します\n",
    "        self.labels = np.array(labels, dtype=int)  # ラベルを整数のNumPy配列として格納します\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 指定したインデックスのエンコーディングとラベルを取得します\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}  # エンコーディングをテンソル化します\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # ラベルをテンソル化します\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # データセットの長さを返します\n",
    "\n",
    "# データセットを作成する\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)  # トレーニングデータセットを作成します\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)        # バリデーションデータセットを作成します\n",
    "\n",
    "# トレーニング引数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                # 結果を保存するディレクトリ\n",
    "    num_train_epochs=3,                    # トレーニングを行うエポック数\n",
    "    per_device_train_batch_size=cfg.batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "    per_device_eval_batch_size=cfg.batch_size,   # デバイスごとの評価バッチサイズ\n",
    "    warmup_steps=500,                       # ウォームアップステップ数\n",
    "    weight_decay=0.01,                     # 重み減衰率\n",
    "    logging_dir='./logs',                   # ログを保存するディレクトリ\n",
    "    logging_steps=10,                       # ロギングのステップ頻度\n",
    ")\n",
    "\n",
    "# トレーナーを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,                            # モデル\n",
    "    args=training_args,                     # トレーニング引数\n",
    "    train_dataset=train_dataset,            # トレーニングデータセット\n",
    "    eval_dataset=val_dataset                 # バリデーションデータセット\n",
    ")\n",
    "\n",
    "# モデルをトレーニングする\n",
    "trainer.train()  # モデルのトレーニングを開始します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf321ff6",
   "metadata": {},
   "source": [
    "# ステップ 5: 推論関数\n",
    "トレーニングされたモデルを使用してトークン化されたデータに対して推論を行う関数を定義します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:24:09.128408Z",
     "iopub.status.busy": "2024-07-28T22:24:09.128045Z",
     "iopub.status.idle": "2024-07-28T22:24:09.13874Z",
     "shell.execute_reply": "2024-07-28T22:24:09.137869Z",
     "shell.execute_reply.started": "2024-07-28T22:24:09.128367Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):\n",
    "    model.eval()  # モデルを評価モードに設定します\n",
    "    results = []  # 結果を格納するリストを初期化します\n",
    "    with torch.no_grad():  # 勾配計算を無効にしてメモリを節約します\n",
    "        for i in range(0, len(input_ids), batch_size):  # バッチサイズごとにループします\n",
    "            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)  # バッチの入力IDをデバイスに移動します\n",
    "            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)  # バッチのアテンションマスクをデバイスに移動します\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)  # モデルの出力を取得します\n",
    "            results.extend(outputs.logits.cpu().numpy())  # 出力のロジットをCPUに移動し、NumPy配列に変換して結果に追加します\n",
    "    return np.array(results)  # 結果をNumPy配列として返します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed11c7d",
   "metadata": {},
   "source": [
    "# ステップ 6: 評価と結果の保存\n",
    "テストデータをトークン化し、推論を行い、結果をCSVファイルに保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:24:09.140244Z",
     "iopub.status.busy": "2024-07-28T22:24:09.139927Z",
     "iopub.status.idle": "2024-07-28T22:24:52.526467Z",
     "shell.execute_reply": "2024-07-28T22:24:52.52548Z",
     "shell.execute_reply.started": "2024-07-28T22:24:09.140214Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータをトークン化する\n",
    "test_texts = df_sample['prompt'].tolist()  # プロンプトのリストを取得します\n",
    "test_encodings = tokenize_function(test_texts)  # テストデータをトークン化します\n",
    "\n",
    "# 推論を行う\n",
    "results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])  # トークン化されたデータに対して推論を実行します\n",
    "\n",
    "# 結果をDataFrameに変換し、CSVとして保存する\n",
    "results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])  # ロジットの結果をDataFrameに変換します\n",
    "submission = pd.concat([df_sample[['id']], results_df], axis=1)  # IDと結果を結合します\n",
    "submission.to_csv('submission.csv', index=False)  # CSVファイルとして保存します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f393e0",
   "metadata": {},
   "source": [
    "# 結論\n",
    "このノートブックでは、Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する体系的なアプローチを提供しました。ステップにはデータのサンプリング、トークン化、モデルの設定、推論が含まれています。最終的な結果はさらなる分析のために保存されました。この方法論は、モデルのパフォーマンスを維持しながら計算リソースの効率的な使用を確保します。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 58238,
     "sourceId": 69788,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
