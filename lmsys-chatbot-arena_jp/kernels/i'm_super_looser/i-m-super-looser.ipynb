{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "566bf70d",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のための機械学習モデルの構築に取り組んでいます。主な目標は、ユーザーのプロンプトに対する二つの異なるLLM（大規模言語モデル）が生成した応答のどちらが好まれるかを予測することです。\n",
    "\n",
    "### 問題の概要\n",
    "ノートブックでは、公開されているデータセットからトレーニングデータとテストデータを読み込み、それに基づいて予測を行うモデルを開発しています。\n",
    "\n",
    "### 使用している手法とライブラリ\n",
    "1. **データ処理**:\n",
    "   - `pandas`や`datasets`ライブラリを用いて、データの読み込みや前処理を行っています。\n",
    "   - テキストデータに対するクリーニング処理（小文字変換、URLやストップワードの削除など）を行う関数を定義しています。\n",
    "\n",
    "2. **トークナイゼーション**:\n",
    "   - `transformers`ライブラリのBERTモデルを使用して、テキストデータをトークン化しています。このプロセスでは、トークナイザーを初期化し、データセットへのトークン化を施しています。\n",
    "\n",
    "3. **モデル構築**:\n",
    "   - TensorFlowとKerasを利用し、カスタムBERTモデルを定義しています。`TFBertModel`を入れ子にしたクラスとして実装しており、データをBERTに通じて処理する層を作成しています。\n",
    "\n",
    "4. **学習と評価**:\n",
    "   - モデルは、`Adam`オプティマイザーを使用してコンパイルされ、エポックごとにトレーニングが行われます。訓練中の進捗は`TqdmCallback`を用いて表示されています。\n",
    "\n",
    "5. **予測の生成**:\n",
    "   - テストデータに対してモデルを用いて予測を実施し、その結果を`DataFrame`として整形してCSVファイルとして保存します。\n",
    "\n",
    "ノートブック全体を通じて、高度な深層学習フレームワークや自然言語処理のテクニックが使用されており、特にBERTモデルの利用により、テキストデータに対して強力な表現学習を行なっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2c9daa",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、機械学習・深層学習の初心者がつまずきそうな専門用語の解説を示します。コンペティションのノートブックに特有の文脈を考慮し、あまり知られていない用語や実務経験がないと馴染みが薄いものに焦点を当てています。\n",
    "\n",
    "1. **TPU（Tensor Processing Unit）**:\n",
    "   - Googleが設計した、特に機械学習タスクに最適化されたハードウェア加速装置。従来のCPUやGPUよりも、高速に計算を行うことができる。Kaggleなどのプラットフォームで利用され、深層学習モデルのトレーニングを効率化する。\n",
    "\n",
    "2. **Layer（レイヤー）**:\n",
    "   - ニューラルネットワークの一部を構成する単位。入力データを受け取り、重みを掛け算し、活性化関数を通して出力を生成する。レイヤーは様々な種類があり、具体的な機能によって異なる（例えば、畳み込みレイヤー、全結合レイヤーなど）。\n",
    "\n",
    "3. **Data Collator**:\n",
    "   - トレーニングデータをバッチ処理する際にデータをまとめる役割を持つ。異なる長さの入力データを同じサイズにパディングし、一つのバッチにまとめる機能を提供する。\n",
    "\n",
    "4. **attention_mask（アテンションマスク）**:\n",
    "   - Transformerモデルにおいて、どのトークンに注目すべきかを示すマスク。特に、パディングされたトークンを無視するために使用される。これにより、無関係なトークンが計算に影響を与えないようにする。\n",
    "\n",
    "5. **Sparse Categorical Crossentropy（疎形式のカテゴリカル交差エントロピー）**:\n",
    "   - 多クラス分類問題において用いられる損失関数の一種。各クラスの正解ラベルが整数で表される場合に使用される。この損失関数は、モデルが予測した確率と実際のクラスとの違いを測る。\n",
    "\n",
    "6. **Global Average Pooling（グローバル平均プーリング）**:\n",
    "   - CNN（畳み込みニューラルネットワーク）などのモデルにおけるレイヤーで、特徴マップ全体の平均値を計算して出力とする方法。これにより、入力データの空間的な特徴を集約でき、多数のパラメータを減らすことができる。\n",
    "\n",
    "7. **Tokenization（トークナイゼーション）**:\n",
    "   - テキストデータを小さな単位（トークン）に分割するプロセス。自然言語処理において、単語や文を処理可能な形式にするために行われ、トークンはモデルの入力として使用される。\n",
    "\n",
    "8. **BERT（Bidirectional Encoder Representations from Transformers）**:\n",
    "   - 自然言語処理のための事前学習されたモデル。双方向的に文脈を捉えることができ、様々なNLPタスクで高い性能を発揮する。トークン化とエンコーディングの機能を備えており、このモデルを使用して特徴を抽出する。\n",
    "\n",
    "9. **MirroredStrategy**:\n",
    "   - TensorFlowの分散学習において、モデルを単一のマシン上の複数のGPUにミラーリングするための戦略。モデルのパラメータを各GPUに同期させ、トレーニングプロセスを並列化する。\n",
    "\n",
    "10. **Gradient Descent（勾配降下法）**:\n",
    "    - 機械学習モデルを最適化するために利用されるアルゴリズム。損失関数の勾配を計算し、その負の方向にパラメータを更新していくことで、最小値を見つけることを目的とする。最適化手法の一つで、最も基本的な形態の一つ。\n",
    "\n",
    "これらの用語は、ノートブック内で使用される背景や概念の理解を深めるのに役立つでしょう。初心者がこれらの専門用語に遭遇した際に、スムーズに理解できるように解説しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:27:35.818778Z",
     "iopub.status.busy": "2024-08-04T04:27:35.818503Z",
     "iopub.status.idle": "2024-08-04T04:27:55.999849Z",
     "shell.execute_reply": "2024-08-04T04:27:55.998858Z",
     "shell.execute_reply.started": "2024-08-04T04:27:35.818753Z"
    }
   },
   "outputs": [],
   "source": [
    "# osモジュールをインポートします。OS関連の機能を扱うために使用します。\n",
    "import os\n",
    "# tensorflowライブラリをインポートします。深層学習のフレームワークです。\n",
    "import tensorflow as tf\n",
    "# datasetsモジュールからload_datasetとDatasetDictをインポートします。データセットの読み込みと管理に使用します。\n",
    "from datasets import load_dataset, DatasetDict\n",
    "# transformersモジュールからモデルとトークナイザをインポートします。自然言語処理に用います。\n",
    "from transformers import BertTokenizer, BertTokenizerFast, TFBertModel, DataCollatorWithPadding, TFAutoModel\n",
    "# kerasのレイヤーをインポートします。ニューラルネットワークの構築に必要なレイヤーです。\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout, GlobalAveragePooling2D\n",
    "# kerasモデルをインポートします。モデルの構築と訓練に使います。\n",
    "from tensorflow.keras.models import Model\n",
    "# shutilモジュールをインポートします。ファイルやフォルダの操作に使用します。\n",
    "import shutil\n",
    "# pandasライブラリをインポートします。データの操作と解析用のライブラリです。\n",
    "import pandas as pd\n",
    "# tqdmのkerasコールバックをインポートします。進捗バーを表示するために使用します。\n",
    "from tqdm.keras import TqdmCallback\n",
    "# 正規表現を使うためのreモジュールをインポートします。\n",
    "import re\n",
    "# 数学関数を利用するためにmathモジュールをインポートします。\n",
    "import math\n",
    "# matplotlibのpyplotをインポートします。データの可視化に使用するグラフ描画ライブラリです。\n",
    "import matplotlib.pyplot as plt\n",
    "# multiprocessingモジュールをインポートします。マルチプロセッシングを扱うためのライブラリです。\n",
    "import multiprocessing\n",
    "# nltkライブラリをインポートします。自然言語処理に関連する機能を提供します。\n",
    "import nltk\n",
    "# nltkからストップワードをインポートします。テキスト処理で無視される単語のリストです。\n",
    "from nltk.corpus import stopwords\n",
    "# nltkから単語トークナイザをインポートします。文章を単語に分割するために使用します。\n",
    "from nltk.tokenize import word_tokenize\n",
    "# 学習率スケジューラのコールバックをインポートします。学習率を動的に変更するために使用します。\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "# numpyライブラリをインポートします。数値計算のためのライブラリです。\n",
    "import numpy as np\n",
    "# kerasをインポートします。深層学習のためのライブラリです。\n",
    "from tensorflow import keras\n",
    "# Adam最適化アルゴリズムをインポートします。モデルの訓練で広く使用される最適化手法です。\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:27:56.002662Z",
     "iopub.status.busy": "2024-08-04T04:27:56.001615Z",
     "iopub.status.idle": "2024-08-04T04:27:56.354389Z",
     "shell.execute_reply": "2024-08-04T04:27:56.353351Z",
     "shell.execute_reply.started": "2024-08-04T04:27:56.002624Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPUの利用可能性をチェックします。\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# GPUが存在する場合、以下の処理を行います。\n",
    "if gpus:\n",
    "    try:\n",
    "        # すべての物理GPUに対してメモリ成長を設定します。\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        # 論理GPUのリストを取得します。\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        # 物理GPUの数と論理GPUの数を表示します。\n",
    "        print(len(gpus), \"物理GPU,\", len(logical_gpus), \"論理GPU\")\n",
    "    # ランタイムエラーが発生した場合、そのエラーメッセージを表示します。\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:27:56.356362Z",
     "iopub.status.busy": "2024-08-04T04:27:56.355745Z",
     "iopub.status.idle": "2024-08-04T04:27:56.395689Z",
     "shell.execute_reply": "2024-08-04T04:27:56.394684Z",
     "shell.execute_reply.started": "2024-08-04T04:27:56.356335Z"
    }
   },
   "outputs": [],
   "source": [
    "# ハードウェアを検出し、適切な分散戦略を返します。\n",
    "try:\n",
    "    # TPUの検出を行います。TPU_NAME環境変数が設定されている場合はパラメータは必要ありません。\n",
    "    # これはKaggle上では常に当てはまります。\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "    # TPUのマスターアドレスを表示します。\n",
    "    print('TPUで実行中: ', tpu.master())\n",
    "# 値エラーが発生した場合、TPUをNoneに設定します。\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "# TPUが存在する場合\n",
    "if tpu:\n",
    "    # TPUクラスタに接続します。\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    # TPUシステムを初期化します。\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    # TPU用の分散戦略を設定します。\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    # TensorFlowのデフォルトの分散戦略を使用します。これはCPUおよび単一GPUで機能します。\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "# 現在の戦略で同期しているレプリカの数を表示します。\n",
    "print(\"レプリカの数: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:27:56.398926Z",
     "iopub.status.busy": "2024-08-04T04:27:56.398335Z",
     "iopub.status.idle": "2024-08-04T04:28:41.150646Z",
     "shell.execute_reply": "2024-08-04T04:28:41.14973Z",
     "shell.execute_reply.started": "2024-08-04T04:27:56.398897Z"
    }
   },
   "outputs": [],
   "source": [
    "# データセットのパスを設定します。\n",
    "train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのパス\n",
    "test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'    # テストデータのパス\n",
    "\n",
    "# データセットを読み込みます。\n",
    "# CSVファイルからトレーニングデータを読み込みます。\n",
    "train_dataset = load_dataset('csv', data_files={'train': train_path})['train']\n",
    "# CSVファイルからテストデータを読み込みます。\n",
    "test_dataset = load_dataset('csv', data_files={'test': test_path})['test']\n",
    "\n",
    "# テストデータセットからIDを保存します。\n",
    "test_ids = test_dataset['id']  # テストデータのIDを取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:28:41.152281Z",
     "iopub.status.busy": "2024-08-04T04:28:41.15193Z",
     "iopub.status.idle": "2024-08-04T04:29:07.461536Z",
     "shell.execute_reply": "2024-08-04T04:29:07.46063Z",
     "shell.execute_reply.started": "2024-08-04T04:28:41.152257Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストセットに不足している列を追加します。\n",
    "for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:\n",
    "    # 各列がテストデータセットに存在しない場合\n",
    "    if col not in test_dataset.column_names:\n",
    "        # 空の文字列で列を追加します。行数はテストデータセットの長さと同じです。\n",
    "        test_dataset = test_dataset.add_column(col, [\"\"] * len(test_dataset))\n",
    "\n",
    "# 列のデータ型をint64に変換します。\n",
    "for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:\n",
    "    # トレーニングデータセットに対して、指定した列を整数に変換します。\n",
    "    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})\n",
    "    # テストデータセットに対して、指定した列を整数に変換します。\n",
    "    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != \"\" else 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:07.463595Z",
     "iopub.status.busy": "2024-08-04T04:29:07.462893Z",
     "iopub.status.idle": "2024-08-04T04:29:19.760404Z",
     "shell.execute_reply": "2024-08-04T04:29:19.759476Z",
     "shell.execute_reply.started": "2024-08-04T04:29:07.463557Z"
    }
   },
   "outputs": [],
   "source": [
    "# ローカルでbert-base-casedのファイルを使用します。\n",
    "source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'  # ソースディレクトリのパス\n",
    "\n",
    "# モデルの保存先ディレクトリを指定します。\n",
    "model_dir = '/kaggle/working/bert-base-cased'\n",
    "# 保存先ディレクトリが存在しない場合は作成します。\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# 必要なファイルをソースディレクトリからモデルディレクトリにコピーします。\n",
    "shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)  # 設定ファイルのコピー\n",
    "shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)  # PyTorchモデルファイルのコピー\n",
    "shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)  # TensorFlowモデルファイルのコピー\n",
    "shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)  # トークナイザー設定ファイルのコピー\n",
    "shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)  # ボキャブラリファイルのコピー\n",
    "shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)  # モデルカードのコピー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:19.76185Z",
     "iopub.status.busy": "2024-08-04T04:29:19.761565Z",
     "iopub.status.idle": "2024-08-04T04:29:19.766684Z",
     "shell.execute_reply": "2024-08-04T04:29:19.76577Z",
     "shell.execute_reply.started": "2024-08-04T04:29:19.761826Z"
    }
   },
   "outputs": [],
   "source": [
    "# ストップワードのファイルパスを指定します。\n",
    "stopwords_path = '/kaggle/input/stopwords/stopwords/english'\n",
    "\n",
    "# ストップワードをファイルから読み込むための関数を定義します。\n",
    "def load_stopwords(stopwords_path):\n",
    "    # 指定したパスのファイルを開きます。\n",
    "    with open(stopwords_path, 'r') as file:\n",
    "        # ファイルの内容を読み込み、行ごとのリストに分割します。\n",
    "        stopwords = file.read().splitlines()\n",
    "    # ストップワードのリストをセットとして返します。集合にすることで重複を排除します。\n",
    "    return set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:19.768007Z",
     "iopub.status.busy": "2024-08-04T04:29:19.76774Z",
     "iopub.status.idle": "2024-08-04T04:29:19.83253Z",
     "shell.execute_reply": "2024-08-04T04:29:19.831765Z",
     "shell.execute_reply.started": "2024-08-04T04:29:19.767985Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークナイザーを初期化します。\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_dir)  # 事前学習済みのトークナイザーを指定したディレクトリから読み込みます。\n",
    "# ストップワードをダウンロードします。\n",
    "stopwords = load_stopwords(stopwords_path)  # ストップワードをファイルから読み込みます。\n",
    "\n",
    "# テキストクリーニングのための関数を定義します。\n",
    "def clean_text(text):\n",
    "    # テキストを小文字に変換します。\n",
    "    text = text.lower()\n",
    "    # URLを削除します。\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    # @メンションや#ハッシュタグを削除します。\n",
    "    text = re.sub(r'\\@\\w+|\\#','', text)\n",
    "    # 句読点を削除します（英数字およびスペース以外の文字）。\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    # ストップワードを削除します。テキストを単語に分割し、ストップワードに含まれない単語だけを結合します。\n",
    "    text = ' '.join([word for word in text.split() if word not in stopwords]) \n",
    "    return text  # クリーンアップされたテキストを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:19.833715Z",
     "iopub.status.busy": "2024-08-04T04:29:19.833429Z",
     "iopub.status.idle": "2024-08-04T04:29:19.83965Z",
     "shell.execute_reply": "2024-08-04T04:29:19.838752Z",
     "shell.execute_reply.started": "2024-08-04T04:29:19.83369Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # 各テキストフィールドをクリーンアップします。\n",
    "    cleaned_prompts = [clean_text(text) for text in examples['prompt']]  # プロンプトのクリーニング\n",
    "    cleaned_responses_a = [clean_text(text) for text in examples['response_a']]  # 応答Aのクリーニング\n",
    "    cleaned_responses_b = [clean_text(text) for text in examples['response_b']]  # 応答Bのクリーニング\n",
    "    \n",
    "    # クリーンアップされたテキストをトークン化します。\n",
    "    return tokenizer(cleaned_prompts,\n",
    "                     cleaned_responses_a,\n",
    "                     cleaned_responses_b,\n",
    "                     padding=\"max_length\",  # 最大長さまでパディングします。\n",
    "                     truncation=True,  # 最大長さを超えるテキストは切り捨てます。\n",
    "                     max_length=512)  # 最大のトークン長を512に設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:19.843076Z",
     "iopub.status.busy": "2024-08-04T04:29:19.842517Z",
     "iopub.status.idle": "2024-08-04T04:29:19.853633Z",
     "shell.execute_reply": "2024-08-04T04:29:19.852719Z",
     "shell.execute_reply.started": "2024-08-04T04:29:19.843043Z"
    }
   },
   "outputs": [],
   "source": [
    "# 関数の使用例を示します。\n",
    "examples = {\n",
    "    'prompt': [\"これはサンプルプロンプトです。\"],  # サンプルプロンプト\n",
    "    'response_a': [\"これはサンプル応答Aです。\"],  # サンプル応答A\n",
    "    'response_b': [\"これはサンプル応答Bです。\"]   # サンプル応答B\n",
    "}\n",
    "\n",
    "# トークン化された出力を得ます。\n",
    "tokenized_output = tokenize_function(examples)\n",
    "# トークン化された出力を表示します。\n",
    "print(tokenized_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:29:19.855306Z",
     "iopub.status.busy": "2024-08-04T04:29:19.854925Z",
     "iopub.status.idle": "2024-08-04T04:30:55.623425Z",
     "shell.execute_reply": "2024-08-04T04:30:55.622493Z",
     "shell.execute_reply.started": "2024-08-04T04:29:19.855274Z"
    }
   },
   "outputs": [],
   "source": [
    "# マルチプロセッシングを使用してトークン化とクリーニング関数を適用します。num_procには使用するプロセッサの数を指定します。\n",
    "num_proc = multiprocessing.cpu_count()  # 利用可能なCPUのコア数を取得します。\n",
    "\n",
    "# より良いエラーハンドリングのためにtry-exceptブロックを追加します。\n",
    "try:\n",
    "    # トレーニングデータセットに対してトークン化関数を適用します。\n",
    "    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)\n",
    "    # テストデータセットに対してトークン化関数を適用します。\n",
    "    test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)\n",
    "# エラーが発生した場合、そのエラーメッセージを表示します。\n",
    "except Exception as e:\n",
    "    print(f\"トークン化中にエラーが発生しました: {e}\")\n",
    "    \n",
    "# トークナイザーを使用して、パディングを行うためのデータコレータを作成します。\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:30:55.625297Z",
     "iopub.status.busy": "2024-08-04T04:30:55.624821Z",
     "iopub.status.idle": "2024-08-04T04:30:55.635319Z",
     "shell.execute_reply": "2024-08-04T04:30:55.63426Z",
     "shell.execute_reply.started": "2024-08-04T04:30:55.625262Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークン化後にデバッグ用の印刷を追加します。\n",
    "print(\"サンプルトークン化されたトレーニングデータセットのエントリー:\")\n",
    "print(tokenized_datasets[0])  # トークン化された最初のトレーニングデータセットのエントリーを表示します。\n",
    "# トークン化されたトレーニングデータセットが空の場合、エラーを発生させます。\n",
    "if len(tokenized_datasets) == 0:\n",
    "    raise ValueError(\"トークン化されたトレーニングデータセットが空です。\")\n",
    "# トークン化されたテストデータセットが空の場合、エラーを発生させます。\n",
    "if len(test_tokenized_datasets) == 0:\n",
    "    raise ValueError(\"トークン化されたテストデータセットが空です。\")\n",
    "# デバッグ用にカラム名を印刷します。\n",
    "print(f\"トークン化されたトレーニングデータセットのカラム名: {tokenized_datasets.column_names}\")\n",
    "print(f\"トークン化されたテストデータセットのカラム名: {test_tokenized_datasets.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:30:55.636992Z",
     "iopub.status.busy": "2024-08-04T04:30:55.636644Z",
     "iopub.status.idle": "2024-08-04T04:30:55.75732Z",
     "shell.execute_reply": "2024-08-04T04:30:55.756491Z",
     "shell.execute_reply.started": "2024-08-04T04:30:55.636961Z"
    }
   },
   "outputs": [],
   "source": [
    "# tf.data.Datasetに変換し、正しい形状にします。\n",
    "def convert_to_tf_dataset(dataset, label_col=None, for_inference=False):\n",
    "    input_columns = tokenizer.model_input_names  # トークナイザーのモデル入力名を取得します。\n",
    "    \n",
    "    # ラベル列が指定されており、推論用でない場合\n",
    "    if label_col and not for_inference:\n",
    "        # ラベル列を除く他の列を削除します。\n",
    "        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in input_columns])\n",
    "    else:\n",
    "        # インプット列に含まれない他の列を削除します。\n",
    "        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in input_columns])\n",
    "    \n",
    "    # ラベルがシーケンスでないことを確認します。\n",
    "    if label_col:\n",
    "        dataset = dataset.map(lambda x: {label_col: int(x[label_col])})  # ラベルを整数に変換します。\n",
    "    \n",
    "    shuffle = not for_inference  # 推論用でない場合はデータをシャッフルします。\n",
    "    batch_size = 16 if for_inference else 450  # 推論用の場合はバッチサイズを16、それ以外は450に設定します。\n",
    "\n",
    "    # データセットをtf.data.Datasetに変換します。\n",
    "    tf_dataset = dataset.to_tf_dataset(\n",
    "        columns=input_columns,  # 入力列\n",
    "        label_cols=[label_col] if label_col and not for_inference else None,  # ラベル列\n",
    "        shuffle=shuffle,  # シャッフルオプション\n",
    "        batch_size=batch_size,  # バッチサイズ\n",
    "        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)  # パディングを行うための関数\n",
    "    )\n",
    "\n",
    "    return tf_dataset  # 変換されたtf.data.Datasetを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:30:55.758748Z",
     "iopub.status.busy": "2024-08-04T04:30:55.758474Z",
     "iopub.status.idle": "2024-08-04T04:31:01.799613Z",
     "shell.execute_reply": "2024-08-04T04:31:01.798622Z",
     "shell.execute_reply.started": "2024-08-04T04:30:55.758723Z"
    }
   },
   "outputs": [],
   "source": [
    "# 変換を実行します。\n",
    "try:\n",
    "    # トークン化されたデータセットをtf.data.Dataset形式に変換します。ラベルとして'winner_model_a'を指定します。\n",
    "    train_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')\n",
    "    # テストデータセットも同様に変換します。ここでもラベルとして'winner_model_a'を指定しますが、通常は異なるラベルを使用したほうが良いでしょう。\n",
    "    test_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')\n",
    "# 変換中にエラーが発生した場合、そのエラーメッセージを表示します。\n",
    "except Exception as e:\n",
    "    print(f\"データセットの変換中にエラーが発生しました: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:31:01.801088Z",
     "iopub.status.busy": "2024-08-04T04:31:01.800784Z",
     "iopub.status.idle": "2024-08-04T04:31:03.111396Z",
     "shell.execute_reply": "2024-08-04T04:31:03.110403Z",
     "shell.execute_reply.started": "2024-08-04T04:31:01.801062Z"
    }
   },
   "outputs": [],
   "source": [
    "# データセットの変換後にデバッグ用の印刷を追加します。\n",
    "print(\"変換されたトレーニングtf.data.Datasetのサンプル:\")\n",
    "# トレーニングデータセットから1つのバッチを取得します。\n",
    "for batch in train_tf_dataset.take(1):\n",
    "    inputs, labels = batch  # 入力とラベルを取得します。\n",
    "    # 入力IDの形状を表示します。\n",
    "    print(f'入力IDの形状: {inputs[\"input_ids\"].shape}')\n",
    "    # アテンションマスクの形状を表示します。\n",
    "    print(f'アテンションマスクの形状: {inputs[\"attention_mask\"].shape}')\n",
    "    # ラベルの形状を表示します。\n",
    "    print(f'ラベルの形状: {labels.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:31:03.113201Z",
     "iopub.status.busy": "2024-08-04T04:31:03.112778Z",
     "iopub.status.idle": "2024-08-04T04:31:03.121011Z",
     "shell.execute_reply": "2024-08-04T04:31:03.120034Z",
     "shell.execute_reply.started": "2024-08-04T04:31:03.113168Z"
    }
   },
   "outputs": [],
   "source": [
    "# カスタムモデルを構築します。\n",
    "class BertLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "        # 事前学習済みのBERTモデルを読み込みます。\n",
    "        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # 入力からinput_idsとattention_maskを取得します。\n",
    "        input_ids, attention_mask = inputs\n",
    "        # BERTモデルに入力を渡して出力を取得します。\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        return outputs.last_hidden_state  # BERTの最終隠れ状態を返します。\n",
    "\n",
    "def create_keras_model():\n",
    "    # 入力ID用の入力レイヤーを定義します。\n",
    "    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')\n",
    "    # アテンションマスク用の入力レイヤーを定義します。\n",
    "    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "    # BERT層を通して出力を取得します。\n",
    "    bert_output = BertLayer()([input_ids, attention_mask])\n",
    "    # 出力をグローバル平均プーリングします。\n",
    "    pooled_output = GlobalAveragePooling1D()(bert_output)\n",
    "    # 最終出力レイヤーを定義します。ここでは3つのクラス用にソフトマックス活性化関数を使用します。\n",
    "    output = Dense(3, activation='softmax')(pooled_output)\n",
    "\n",
    "    # モデルを構築します。入力と出力を指定します。\n",
    "    model = Model(inputs=[input_ids, attention_mask], outputs=output)\n",
    "    return model  # 作成したモデルを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-04T04:31:03.122591Z",
     "iopub.status.busy": "2024-08-04T04:31:03.122224Z"
    }
   },
   "outputs": [],
   "source": [
    "# ストラテジースコープ内でモデルを構築し、コンパイルします。\n",
    "with strategy.scope():\n",
    "    # Kerasモデルを作成します。\n",
    "    model = create_keras_model()\n",
    "    # モデルをコンパイルします。最適化手法としてAdamを使用し、学習率は5e-5に設定します。\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "                  loss='sparse_categorical_crossentropy',  # 損失関数には疎形式のカテゴリカル交差エントロピーを使用します。\n",
    "                  metrics=['accuracy'])  # 評価指標には精度を使用します。\n",
    "\n",
    "    # トレーニングデータセットでモデルを訓練します。エポック数は3です。\n",
    "    model.fit(train_tf_dataset, epochs=3, callbacks=[TqdmCallback(verbose=1)])  # TqdmCallbackを使用して進捗を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測を取得します。\n",
    "predictions = model.predict(test_tf_dataset)  # テストデータセットに対して予測を行います。\n",
    "\n",
    "# 長さをチェックします。\n",
    "print(f\"テストIDの長さ: {len(test_ids)}\")  # テストIDの数を表示します。\n",
    "print(f\"予測の形状: {predictions.shape}\")  # 予測の形状を表示します。\n",
    "# テストIDの数と予測の数が一致しない場合、予測をテストIDの数に合わせて調整します。\n",
    "if len(test_ids) != predictions.shape[0]:\n",
    "    predictions = predictions[:len(test_ids)]  # 余分な予測を切り捨てます。\n",
    "\n",
    "# データフレームを作成します。\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,  # テストIDをID列として追加します。\n",
    "    'winner_model_a': predictions[:, 0],  # モデルAの予測を列として追加します。\n",
    "    'winner_model_b': predictions[:, 1],  # モデルBの予測を列として追加します。\n",
    "    'winner_model_tie': predictions[:, 2]  # タイの予測を列として追加します。\n",
    "})\n",
    "\n",
    "# データフレームをCSVファイルとして保存します。\n",
    "submission.to_csv('submission.csv', index=False)  # インデックスなしでCSVファイルに保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0751b3e3",
   "metadata": {},
   "source": [
    "# コメント\n",
    "\n",
    "> ## ハッサン・シャヒディ\n",
    "> \n",
    "> 素晴らしい仕事です！その調子でがんばってください🔥\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## ギータ・ケオテ\n",
    "> \n",
    "> よくやりました！その調子でがんばってください👍\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## フー・フランシー\n",
    "> \n",
    "> あなたの仕事が好きです！\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## デッドQ\n",
    "> \n",
    "> Kaggleコミュニティのための良いコードです。ありがとうございます！その調子でがんばってください！\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## ヴァンシュ・シャルマ\n",
    "> \n",
    "> [@fulgrimthe](https://www.kaggle.com/fulgrimthe) あなたの分析は明確で非常に役立ちます。素晴らしい仕事です！\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 2120,
     "sourceId": 3570,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 934701,
     "sourceId": 9054357,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30734,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
