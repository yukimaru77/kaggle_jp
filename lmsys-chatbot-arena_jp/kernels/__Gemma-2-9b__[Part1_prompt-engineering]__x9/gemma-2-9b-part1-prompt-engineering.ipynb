{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3729d496",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、チャットボットの応答の好みを予測する問題に取り組んでいます。具体的には、2つの異なる言語モデルが生成した応答のうち、どちらがユーザーに好まれるかを判定するモデルを構築することを目指しています。\n",
    "\n",
    "### 主要な手法とライブラリ\n",
    "- **ライブラリ**: 主に`pandas`（データ処理）、`torch`（深層学習処理）、および`transformers`（言語モデルの利用）を使用しています。\n",
    "  \n",
    "- **データ処理**: `process_data`関数を用いて、CSVファイルから読込んだプロンプトおよび応答を整形しています。このプロセスでは、前後の角括弧やダブルクオーテーションを取り除き、文章を結合しています。\n",
    "\n",
    "- **モデルのロード**: `AutoTokenizer`と`AutoModelForCausalLM`を利用して、事前トレーニングされたGemma-2-9Bモデルをロードしています。モデルはbfloat16の形式でCUDAデバイスにロードされ、推論に対応しています。\n",
    "\n",
    "- **推論処理**: `predict`関数を定義し、入力クエリと2つの応答を元にどちらの応答が優れているかを判断します。評価基準には関連性、正確性、完全性、一貫性が考慮されており、応答の判定結果に基づいて出力が行われます。\n",
    "\n",
    "- **結果の蓄積**: ループ処理を通じて、全テストデータに対して予測を行い、その結果をリストにまとめ、最終的に`submission.csv`という形式で出力します。各エントリは、クエリID、モデルAの勝者、モデルBの勝者、および引き分けのフラグを含みます。\n",
    "\n",
    "全体的には、このノートブックは機械学習を介して言語モデルの応答を評価し、人間の好みに基づいて最適な応答を予測するための手法を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9116ba6f",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookで使用されている専門用語に関する簡易解説を示します。初心者がつまずきやすいマイナーな用語や、特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **bfloat16**: \n",
    "   - バイナリ浮動小数点数の一種で、16ビットの精度を持ちます。機械学習においては、メモリの使用量を削減し、計算効率の向上を図るために用いられます。特に、TPU（Tensor Processing Unit）でよく利用されるフォーマットです。\n",
    "\n",
    "2. **プロンプトエンジニアリング**:\n",
    "   - 大規模言語モデル（LLM）に対する入出力を最適化するための手法で、モデルに与える指示や質問（プロンプト）を慎重に設計することです。これにより、モデルから得られる応答の品質を向上させることが目的です。\n",
    "\n",
    "3. **CUDA**:\n",
    "   - NVIDIAが開発した、GPUを用いた計算を効率的に行うための並列計算プラットフォームおよびAPIです。機械学習や深層学習のトレーニングおよび推論において、GPUを利用するために広く用いられます。\n",
    "\n",
    "4. **トークン化**:\n",
    "   - 自然言語処理において、テキストを分析可能な単位（トークン）に分割するプロセスです。これは、モデルが理解できる形式に変換するための前処理の一環として行われます。\n",
    "\n",
    "5. **生成モデル**:\n",
    "   - 入力データを基に新しいデータを生成することを目的とする機械学習モデルの一種です。本ノートブックでは、特に言語生成に関与するモデル（例えば、文や会話の生成）を指します。\n",
    "\n",
    "6. **メモリ効率の良いSDP (Stochastic Dynamic Programming)**:\n",
    "   - モデルの計算を効率化する手法で、メモリの使用を抑えつつ動的プログラミングを行うものです。大規模なモデルのトレーニング時に特に重要です。\n",
    "\n",
    "7. **重み**:\n",
    "   - 機械学習モデルがデータから学習するパラメータのこと。重みは、モデルの性能に大きな影響を与え、トレーニングの過程で調整されます。\n",
    "\n",
    "8. **次元削減**:\n",
    "   - 多次元空間のデータを少ない次元に圧縮する手法で、データの可視化や処理の効率化に用いられます。具体的な手法にはPCA（主成分分析）やt-SNEが含まれますが、ノートブック内では直接触れられていないかもしれません。\n",
    "\n",
    "9. **スニペット**:\n",
    "   - プログラムコードの短い部分やフラグメントを示す用語で、通常は再利用可能なものを指します。コーディングの際によく用いられます。\n",
    "\n",
    "10. **グラデーション降下法**:\n",
    "    - 最適化アルゴリズムの一つで、誤差を最小化するためにモデルの重みを更新する方法です。勾配を計算し、その反対方向に重みを調整することで、より良いパラメータを見つけ出します。\n",
    "\n",
    "これらの用語は、特に実務経験がない初心者にとっては馴染みが薄いか、理解が難しいことがあります。これらの解説が理解の助けとなれば幸いです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:06.677065Z",
     "iopub.status.busy": "2024-07-15T06:48:06.676325Z",
     "iopub.status.idle": "2024-07-15T06:48:07.827666Z",
     "shell.execute_reply": "2024-07-15T06:48:07.826627Z",
     "shell.execute_reply.started": "2024-07-15T06:48:06.677019Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-15T07:59:57.252098Z",
     "iopub.status.busy": "2024-07-15T07:59:57.25174Z",
     "iopub.status.idle": "2024-07-15T07:59:57.256785Z",
     "shell.execute_reply": "2024-07-15T07:59:57.255788Z",
     "shell.execute_reply.started": "2024-07-15T07:59:57.25207Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import ast\n",
    "import json\n",
    "import re\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:11.113603Z",
     "iopub.status.busy": "2024-07-15T06:48:11.11297Z",
     "iopub.status.idle": "2024-07-15T06:48:11.118344Z",
     "shell.execute_reply": "2024-07-15T06:48:11.117441Z",
     "shell.execute_reply.started": "2024-07-15T06:48:11.113569Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効にする\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T07:48:01.813641Z",
     "iopub.status.busy": "2024-07-15T07:48:01.813261Z",
     "iopub.status.idle": "2024-07-15T07:48:01.822292Z",
     "shell.execute_reply": "2024-07-15T07:48:01.821373Z",
     "shell.execute_reply.started": "2024-07-15T07:48:01.813612Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_data(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 文字列の前後の角括弧を削除する\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文章を分割し、各部分の前後のダブルクオーテーションを削除\n",
    "    sentences = ' '.join(sentences)  # 分割した文章をスペースで結合\n",
    "    return sentences\n",
    "\n",
    "def get_data(path, system_prompt=None):\n",
    "    df = pd.read_csv(path)  # 指定したパスからCSVファイルを読み込む\n",
    "    df['prompt'] = df['prompt'].apply(process_data)  # プロンプト列に対してprocess_dataを適用\n",
    "    df['response_a'] = df['response_a'].apply(process_data)  # 応答A列に対してprocess_dataを適用\n",
    "    df['response_b'] = df['response_b'].apply(process_data)  # 応答B列に対してprocess_dataを適用\n",
    "    return df  # 処理済みのデータフレームを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T07:48:02.367062Z",
     "iopub.status.busy": "2024-07-15T07:48:02.366648Z",
     "iopub.status.idle": "2024-07-15T07:48:02.382044Z",
     "shell.execute_reply": "2024-07-15T07:48:02.381191Z",
     "shell.execute_reply.started": "2024-07-15T07:48:02.367032Z"
    }
   },
   "outputs": [],
   "source": [
    "test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'  # テストデータのパスを指定\n",
    "test_df = get_data(test_path)  # テストデータを取得\n",
    "test_df.head()  # テストデータの最初の5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ca329",
   "metadata": {},
   "source": [
    "## Gemma-2-9Bモデルのロード\n",
    "> google/gemma-2-9b-it \n",
    "\n",
    "私はすでにbfloat16の重みをダウンロードして保存しました。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:14.572235Z",
     "iopub.status.busy": "2024-07-15T06:48:14.571879Z",
     "iopub.status.idle": "2024-07-15T06:48:15.465156Z",
     "shell.execute_reply": "2024-07-15T06:48:15.464383Z",
     "shell.execute_reply.started": "2024-07-15T06:48:14.572207Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer  # Transformersライブラリからモデルとトークナイザーをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:15.467426Z",
     "iopub.status.busy": "2024-07-15T06:48:15.466814Z",
     "iopub.status.idle": "2024-07-15T06:48:15.471513Z",
     "shell.execute_reply": "2024-07-15T06:48:15.470542Z",
     "shell.execute_reply.started": "2024-07-15T06:48:15.467393Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-tokenizer'  # トークナイザーのパスを指定\n",
    "model_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-model'  # モデルのパスを指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:15.517944Z",
     "iopub.status.busy": "2024-07-15T06:48:15.517688Z",
     "iopub.status.idle": "2024-07-15T06:48:16.445695Z",
     "shell.execute_reply": "2024-07-15T06:48:16.444823Z",
     "shell.execute_reply.started": "2024-07-15T06:48:15.517922Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)  # トークナイザーを指定したパスからロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:48:22.429128Z",
     "iopub.status.busy": "2024-07-15T06:48:22.428103Z",
     "iopub.status.idle": "2024-07-15T06:51:09.409078Z",
     "shell.execute_reply": "2024-07-15T06:51:09.40808Z",
     "shell.execute_reply.started": "2024-07-15T06:48:22.429097Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.bfloat16)  # モデルを指定したパスからロードし、デバイスマップを自動で取得、データ型をbfloat16に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:51:09.411816Z",
     "iopub.status.busy": "2024-07-15T06:51:09.410924Z",
     "iopub.status.idle": "2024-07-15T06:51:09.421473Z",
     "shell.execute_reply": "2024-07-15T06:51:09.420593Z",
     "shell.execute_reply.started": "2024-07-15T06:51:09.41178Z"
    }
   },
   "outputs": [],
   "source": [
    "model  # モデルオブジェクトを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa34207",
   "metadata": {},
   "source": [
    "## シンプルな推論を行ってみましょう\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T06:51:09.42281Z",
     "iopub.status.busy": "2024-07-15T06:51:09.422507Z",
     "iopub.status.idle": "2024-07-15T06:51:42.214722Z",
     "shell.execute_reply": "2024-07-15T06:51:42.213751Z",
     "shell.execute_reply.started": "2024-07-15T06:51:09.422785Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "prompt = 'Write a conversation between gemma and llama llm models'  # プロンプトを定義\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')  # プロンプトをトークン化し、テンソル形式に変換してCUDAデバイスに移動\n",
    "\n",
    "outputs = model.generate(**input_ids, max_new_tokens=200)  # モデルを使って新しいトークンを生成\n",
    "response = tokenizer.decode(outputs[0])  # 生成したトークンをデコードして応答を取得\n",
    "print(response)  # 応答を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2063b99",
   "metadata": {},
   "source": [
    "## テストセットに対して、プロンプトエンジニアリングを行った推論を行いましょう\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:09:50.637151Z",
     "iopub.status.busy": "2024-07-15T08:09:50.636469Z",
     "iopub.status.idle": "2024-07-15T08:09:50.646948Z",
     "shell.execute_reply": "2024-07-15T08:09:50.64597Z",
     "shell.execute_reply.started": "2024-07-15T08:09:50.637121Z"
    }
   },
   "outputs": [],
   "source": [
    "test_df  # テストデータフレームを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T07:37:15.073395Z",
     "iopub.status.busy": "2024-07-15T07:37:15.072935Z",
     "iopub.status.idle": "2024-07-15T07:37:15.078959Z",
     "shell.execute_reply": "2024-07-15T07:37:15.078025Z",
     "shell.execute_reply.started": "2024-07-15T07:37:15.073366Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_prompt(query, response_a, response_b):\n",
    "    prompt = f\"\"\"\n",
    "あなたは、異なるモデルによって生成された2つの応答を評価し、どちらの方が優れているかを判断する任務を負っています。クエリと2つの応答（モデルAからの応答AとモデルBからの応答B）を与えられた場合、各応答の質を関連性、正確性、完全性、全体的な一貫性に基づいて評価します。両方の応答が同等に良いか悪い場合は、引き分けと宣言することができます。\n",
    "\n",
    "指示：\n",
    "\n",
    "クエリ: {query}\n",
    "応答A（モデルA）: {response_a}\n",
    "応答B（モデルB）: {response_b}\n",
    "\n",
    "評価基準：\n",
    "\n",
    "関連性: 応答はクエリにどれだけ対応しているか？\n",
    "正確性: 提供された情報は正確で信頼性があるか？\n",
    "完全性: 応答は包括的な回答を提供しているか？\n",
    "一貫性: 応答は論理的に構成されており、理解しやすいか？\n",
    "出力：\n",
    "\n",
    "応答Aが優れている場合、出力: 応答A\n",
    "応答Bが優れている場合、出力: 応答B\n",
    "両方の応答が同等に良いまたは悪い場合、出力: 引き分け\n",
    "\n",
    "単一行を出力する必要があります – 応答Aまたは応答Bまたは引き分けのいずれかの単語\\n\n",
    "出力: \n",
    "\n",
    "    \"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:01:15.237464Z",
     "iopub.status.busy": "2024-07-15T08:01:15.236745Z",
     "iopub.status.idle": "2024-07-15T08:01:15.243954Z",
     "shell.execute_reply": "2024-07-15T08:01:15.243011Z",
     "shell.execute_reply.started": "2024-07-15T08:01:15.237431Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(query, response_a, response_b, max_new_tokens=50, do_sample=False, temperature=1.0):\n",
    "    prompt = get_prompt(query=query, response_a=response_a, response_b=response_b)\n",
    "\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')  # プロンプトをトークン化し、テンソル形式に変換してCUDAデバイスに移動\n",
    "\n",
    "    outputs = model.generate(**input_ids, max_new_tokens=50, do_sample=do_sample, temperature=temperature)  # モデルが新しいトークンを生成\n",
    "    response = tokenizer.decode(outputs[0])  # 生成したトークンをデコードして応答を取得\n",
    "\n",
    "    pattern = r\"OUTPUT:\\s*(RESPONSE_A|RESPONSE_B|TIE)\"  # 出力のパターンを定義\n",
    "    match = re.search(pattern, response)  # 応答の中から出力パターンを検索\n",
    "    if match:\n",
    "        pred = match.group(1)  # マッチした場合、出力を取得\n",
    "    else:\n",
    "        pred = None  # マッチしなかった場合はNoneを設定\n",
    "    return pred  # 予測結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:07:33.054676Z",
     "iopub.status.busy": "2024-07-15T08:07:33.054312Z",
     "iopub.status.idle": "2024-07-15T08:08:01.168403Z",
     "shell.execute_reply": "2024-07-15T08:08:01.167462Z",
     "shell.execute_reply.started": "2024-07-15T08:07:33.054648Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "id_list = []\n",
    "winner_model_a_list = []\n",
    "winner_model_b_list = []\n",
    "winner_tie_list = []\n",
    "for idx in tqdm(range(0, len(test_df))):  # テストデータフレームの各行についてループ\n",
    "    query_id = test_df.iloc[idx]['id']  # クエリIDを取得\n",
    "    query = test_df.iloc[idx]['prompt']  # クエリを取得\n",
    "    response_a = test_df.iloc[idx]['response_a']  # 応答Aを取得\n",
    "    response_b = test_df.iloc[idx]['response_b']  # 応答Bを取得\n",
    "    pred = predict(query, response_a, response_b, max_new_tokens=20, do_sample=True, temperature=0.7)  # 予測を実行\n",
    "    id_list.append(query_id)  # IDをリストに追加\n",
    "    if pred is not None:  # 予測結果がNoneでない場合\n",
    "        if 'A' in pred or 'a' in pred:  # 応答Aが優れている場合\n",
    "            winner_model_a_list.append(1)  # 応答Aの勝者リストに1を追加\n",
    "            winner_model_b_list.append(0)  # 応答Bの勝者リストに0を追加                    \n",
    "            winner_tie_list.append(0)  # 引き分けリストに0を追加\n",
    "        if 'B' in pred or 'b' in pred:  # 応答Bが優れている場合\n",
    "            winner_model_a_list.append(0)  # 応答Aの勝者リストに0を追加\n",
    "            winner_model_b_list.append(1)  # 応答Bの勝者リストに1を追加        \n",
    "            winner_tie_list.append(0)  # 引き分けリストに0を追加            \n",
    "    else:\n",
    "        winner_model_a_list.append(0)  # 応答Aの勝者リストに0を追加\n",
    "        winner_model_b_list.append(0)  # 応答Bの勝者リストに0を追加        \n",
    "        winner_tie_list.append(1)  # 引き分けリストに1を追加\n",
    "    torch.cuda.empty_cache()  # CUDAメモリのキャッシュをクリア"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:08:32.901906Z",
     "iopub.status.busy": "2024-07-15T08:08:32.901214Z",
     "iopub.status.idle": "2024-07-15T08:08:32.906789Z",
     "shell.execute_reply": "2024-07-15T08:08:32.905883Z",
     "shell.execute_reply.started": "2024-07-15T08:08:32.901875Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame({'id': id_list, 'winner_model_a': winner_model_a_list, 'winner_model_b': winner_model_b_list, 'winner_tie': winner_tie_list})  # 提出用のデータフレームを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:08:36.702362Z",
     "iopub.status.busy": "2024-07-15T08:08:36.701617Z",
     "iopub.status.idle": "2024-07-15T08:08:36.711417Z",
     "shell.execute_reply": "2024-07-15T08:08:36.71021Z",
     "shell.execute_reply.started": "2024-07-15T08:08:36.70233Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df  # 提出用データフレームを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-15T08:10:31.19526Z",
     "iopub.status.busy": "2024-07-15T08:10:31.19451Z",
     "iopub.status.idle": "2024-07-15T08:10:31.202885Z",
     "shell.execute_reply": "2024-07-15T08:10:31.201907Z",
     "shell.execute_reply.started": "2024-07-15T08:10:31.195228Z"
    }
   },
   "outputs": [],
   "source": [
    "submission_df.to_csv('submission.csv', index=False)  # 提出データフレームをCSVファイルとして保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5390170,
     "sourceId": 8956282,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
