{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3b6eaf8",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、LMSYS - Chatbot Arenaコンペティションにおける人間の好み予測の問題に取り組んでいます。具体的には、異なるバージョンのQwen2モデル（特に1.5bバージョン）を使用して、与えられたプロンプトに対する応答の好みを予測するためのトレーニングと推論を実行しています。ノートブックは、チャットボットの応答の選好を予測するモデルの性能向上を目的としています。\n",
    "\n",
    "主な手法として、以下の手法とライブラリを使用しています：\n",
    "\n",
    "1. **モデルとトークナイザー**: `transformers`ライブラリの`Qwen2ForSequenceClassification`と`AutoTokenizer`を利用し、Qwen2モデルを使用している。事前トレーニング済みのモデルを用いて、シーケンス分類タスクを実行します。\n",
    "\n",
    "2. **トレーニング戦略**: `PeftModel`を使用して、パラメータ効率の良いファインチューニング（PEFT）を行う。特にLoRA（Low-Rank Adaptation）に基づくアプローチを使用し、モデルの特定の部分に対しての適応を行います。\n",
    "\n",
    "3. **データの前処理**: ''sprompt''、''response_a''、''response_b''の各列に対してテキスト処理を行い、トークン化を実施しています。トークン化の際には、プロンプトや応答に対して特定の形式の前処理（接頭辞の追加）を行います。\n",
    "\n",
    "4. **推論手法**: 並列処理を使用して、2つのモデルによって生成された応答に対する勝率を予測します。`ThreadPoolExecutor`を用いて、同時に複数のモデルでの推論を効率化しています。\n",
    "\n",
    "5. **評価指標**: モデルの評価には対数損失および精度を使用し、これらの指標を用いてトレーニングと評価の過程でモデルの性能を評価します。\n",
    "\n",
    "このノートブックは、最終的にテストデータに対する予測結果を保存したCSVファイル（\"submission.csv\"）を生成します。全体として、複数のモデルを活用して、与えられたプロンプトに対する応答の優劣を予測する精度を高めるためのフレームワークが整備されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20f561e",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブック内で使われている専門用語の簡単な解説です。特に初心者がつまずきやすいマイナーな用語やドメイン特有の用語に焦点を当てています。\n",
    "\n",
    "1. **SwiGLU活性化**:\n",
    "   - SwiGLUは、Tanh活性化関数と線形変換を組み合わせた新しいタイプの活性化関数です。これにより、情報の流れを改善し、学習の効率を向上させることが期待されます。\n",
    "\n",
    "2. **QKVバイアス**:\n",
    "   - QKVは、Query、Key、Valueの略で、Transformerアーキテクチャにおける注意機構の基本要素です。QKVバイアスは、これらの要素にバイアス項を追加することで、モデルの性能を向上させる手法の一つです。\n",
    "\n",
    "3. **グループクエリアテンション**:\n",
    "   - グループクエリアテンションは、複数のクエリに対して同時に注意を払う技術です。これにより、計算効率が向上し、特定の情報に迅速にアクセスできるようになります。\n",
    "\n",
    "4. **スライディングウィンドウアテンション**:\n",
    "   - スライディングウィンドウアテンションは、入力シーケンスの局所部分に注目し、処理する手法です。これにより、長いシーケンスの処理が効率化され、計算リソースを節約できます。\n",
    "\n",
    "5. **フルアテンション**:\n",
    "   - フルアテンションは、入力シーケンス全体に対して注意を払う方法です。全てのトークン間の相互作用を考慮することで、モデルがより多くの情報を考慮できますが、その分計算コストが高くなります。\n",
    "\n",
    "6. **LoRA (Low-Rank Adaptation)**:\n",
    "   - LoRAは、大規模言語モデルを効率的に微調整するための手法で、低ランクの近似を利用してトレーニングパラメータの数を減らし、計算資源を節約することを目的としています。\n",
    "\n",
    "7. **TTa (Test-Time Augmentation)**:\n",
    "   - TTAは、推論時にデータを拡張し、その予測精度を向上させる手法です。異なる視点からのデータを使用することで、モデルのロバスト性を高める効果があります。\n",
    "\n",
    "8. **注意マスク (Attention Mask)**:\n",
    "   - 注意マスクは、Transformerモデルで、どの入力トークンに注意を払うべきかを示すためのバイナリマスクです。これにより、無関係なトークンを無視し、モデルが重要な情報に集中できるようにします。\n",
    "\n",
    "9. **adaptive softmax**:\n",
    "   - Adaptive softmaxは、多クラス分類問題において、クラス数が非常に多い場合に計算効率を向上させるための手法です。より頻繁に発生するクラスに対しては、高解像度の分布を学び、稀なクラスに対しては低解像度の分布を学びます。\n",
    "\n",
    "10. **最大長 (max_length)**:\n",
    "    - モデルが処理できる入力シーケンスの最大長を指定します。これにより、入力テキストが過剰な長さになることを防ぎ、計算リソースの消費を制御します。\n",
    "\n",
    "これらの用語は、ノートブック内で特に重要な役割を果たすものであり、初心者が理解するための基礎になります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f56ec",
   "metadata": {},
   "source": [
    "# はじめに\n",
    "\n",
    "このノートブックは[こちら](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)からフォークされたものです。\n",
    "\n",
    "Qwen2は、異なるモデルサイズのデコーダー言語モデルを含む言語モデルシリーズです。各サイズごとに、基本の言語モデルと整合されたチャットモデルをリリースしています。これは、SwiGLU活性化、注意のQKVバイアス、グループクエリアテンション、スライディングウィンドウアテンションとフルアテンションの組み合わせなどに基づくトランスフォーマーアーキテクチャに基づいています。さらに、複数の自然言語やコードに適応した改善されたトークナイザーも搭載しています。\n",
    "\n",
    "このノートブックでは、バッチサイズ4、1エポックでQwen2 1.5bバージョンのトレーニングを行っています。トレーニング時間はA100で約1時間です。Qwen2 7bはこのタスクに対してより良いパフォーマンスを発揮することが期待されます。トレーニング用のコードは、ノートブックの最後に添付されています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-20T00:21:28.054679Z",
     "iopub.status.busy": "2024-07-20T00:21:28.053903Z"
    },
    "papermill": {
     "duration": 31.479497,
     "end_time": "2024-07-10T01:13:41.690971",
     "exception": false,
     "start_time": "2024-07-10T01:13:10.211474",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files\n",
    "# transformers, peft, accelerate, bitsandbytesのライブラリをインストールします。\n",
    "# -Uオプションはアップグレードを意味し,\n",
    "# --no-indexはPyPIからではなくローカルのリンクを使用することを指定します。\n",
    "# --find-linksは指定したディレクトリ（この場合は/kaggle/input/lmsys-wheel-files）からパッケージを探します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 19.200405,
     "end_time": "2024-07-10T01:14:00.90474",
     "exception": false,
     "start_time": "2024-07-10T01:13:41.704335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Qwen2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "\n",
    "# 必要なライブラリをインポートします。\n",
    "# - time: 時間関連の関数を提供します。\n",
    "# - dataclass: データクラスを作成するためのデコレーターです。\n",
    "# - ThreadPoolExecutor: スレッドプールを使って非同期処理を実行するためのクラスです。\n",
    "# - torch: PyTorchライブラリで、深層学習を行うための基本的なツールです。\n",
    "# - sklearn: 機械学習用のライブラリです。\n",
    "# - numpy: 数値計算ライブラリで、配列操作に使います。\n",
    "# - pandas: データ分析のためのライブラリで、データフレームを扱います。\n",
    "# - transformers: Hugging Faceのトランスフォーマーモデルに関連するライブラリです。\n",
    "# - PeftModel: PEFT (Parameter-Efficient Fine-Tuning) に関連するモデルです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.225826Z",
     "iopub.status.idle": "2024-07-20T00:16:19.226184Z",
     "shell.execute_reply": "2024-07-20T00:16:19.226021Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.226008Z"
    },
    "papermill": {
     "duration": 0.047799,
     "end_time": "2024-07-10T01:14:00.965921",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.918122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2\n",
    "# GPUの数が2であることを確認するためのアサーションです。\n",
    "# torch.cuda.device_count()は使用可能なGPUの数を返します。\n",
    "# この条件が満たされない場合、エラーが発生します。\n",
    "# つまり、2つのGPUが必要な環境でこのコードを実行することを示しています。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b41514b",
   "metadata": {},
   "source": [
    "## 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.228103Z",
     "iopub.status.idle": "2024-07-20T00:16:19.228495Z",
     "shell.execute_reply": "2024-07-20T00:16:19.228332Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.228317Z"
    },
    "papermill": {
     "duration": 0.021338,
     "end_time": "2024-07-10T01:14:01.000606",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.979268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_dir = '/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1'  # モデルの保存先ディレクトリ\n",
    "    lora_dir = '/kaggle/input/lmsys-qwen2-1-5b-checkpoint/checkpoint-5748'  # LoRAチェックポイントの保存先ディレクトリ\n",
    "    max_length = 2048  # 入力の最大長\n",
    "    batch_size = 4  # バッチサイズ\n",
    "    device = torch.device(\"cuda\")  # 使用するデバイスをCUDA（GPU）に設定\n",
    "    tta = False  # テスト時の拡張を有効にするかどうか。<prompt>-<model-bの応答>-<model-aの応答>\n",
    "    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結された入力にmax_lengthを適用するか\n",
    "\n",
    "cfg = Config()  # 設定を初期化するためのConfigクラスのインスタンスを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a484f74b",
   "metadata": {},
   "source": [
    "# データのロードと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.22986Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23022Z",
     "shell.execute_reply": "2024-07-20T00:16:19.230068Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.230053Z"
    },
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-07-10T01:14:01.06946",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.03979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  \n",
    "# テストデータセットをCSVファイルから読み込みます。\n",
    "# pandasのread_csv関数を使用して、指定されたパスのテストデータをDataFrameとして読み取ります。\n",
    "# このDataFrameには、後のモデル推論で使用するデータが含まれています。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.231625Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23215Z",
     "shell.execute_reply": "2024-07-20T00:16:19.231915Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.231895Z"
    },
    "papermill": {
     "duration": 0.040127,
     "end_time": "2024-07-10T01:14:01.12241",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.082283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))  # 文字列を評価して、nullを空文字に置き換えた後、空白で結合します。\n",
    "\n",
    "# データフレーム内の'sprompt'、'response_a'、'response_b'列のテキストを前処理します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)  # 各プロンプトにprocess_text関数を適用して前処理を行う\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)  # 応答Aに対しても同様に前処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)  # 応答Bに対しても前処理\n",
    "\n",
    "display(test.head(5))  # 前処理したデータの最初の5行を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2b1f8",
   "metadata": {},
   "source": [
    "# トークナイズ（トークン化）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.23378Z",
     "iopub.status.idle": "2024-07-20T00:16:19.23424Z",
     "shell.execute_reply": "2024-07-20T00:16:19.234021Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.234003Z"
    },
    "papermill": {
     "duration": 0.030237,
     "end_time": "2024-07-10T01:14:01.194318",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.164081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    # プロンプトと応答をトークン化します。\n",
    "    prompt = [\"<User prompt>: \" + p for p in prompt]  # 各プロンプトの先頭に\"<User prompt>: \"を追加\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]  # 応答Aの先頭に改行と\"<response_a>: \"を追加\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]  # 応答Bの先頭に改行と\"<response_b>: \"を追加\n",
    "    \n",
    "    if spread_max_length:\n",
    "        # 各入力（プロンプト、応答A、応答B）に対してmax_lengthの1/3を適用\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        # トークンを結合し、入力IDを作成\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # 各トークンに対する注意マスクを作成\n",
    "    else:\n",
    "        # プロンプト、応答A、応答Bを結合してトークン化\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークン化\n",
    "        input_ids = tokenized.input_ids  # トークンのIDを取得\n",
    "        attention_mask = tokenized.attention_mask  # 注意マスクを取得\n",
    "    \n",
    "    return input_ids, attention_mask  # トークンのIDと注意マスクを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.235624Z",
     "iopub.status.idle": "2024-07-20T00:16:19.236104Z",
     "shell.execute_reply": "2024-07-20T00:16:19.23588Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.235861Z"
    },
    "papermill": {
     "duration": 1.169844,
     "end_time": "2024-07-10T01:14:02.377579",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.207735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)  # 事前学習済みのトークナイザーをモデルディレクトリから読み込みます。\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加します。\n",
    "tokenizer.padding_side = \"right\"  # パディングを右側に設定します。\n",
    "\n",
    "data = pd.DataFrame()  # 新しいデータフレームを作成します。\n",
    "data[\"id\"] = test[\"id\"]  # テストデータのID列をコピーします。\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # トークン化を実行し、入力IDと注意マスクを取得します。\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # 各入力IDの長さを計算して新しい列に保存します。\n",
    "\n",
    "aug_data = pd.DataFrame()  # 拡張データ用の新しいデータフレームを作成します。\n",
    "aug_data[\"id\"] = test[\"id\"]  # テストデータのID列をコピーします。\n",
    "# 応答Aと応答Bを入れ替えます。\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])  # 応答を入れ替えてトークン化を実行します。\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # 各入力IDの長さを計算して新しい列に保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.237697Z",
     "iopub.status.idle": "2024-07-20T00:16:19.238075Z",
     "shell.execute_reply": "2024-07-20T00:16:19.237906Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.23789Z"
    },
    "papermill": {
     "duration": 0.024759,
     "end_time": "2024-07-10T01:14:02.419091",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.394332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  \n",
    "# データフレームの最初の入力IDをデコードして、元のテキストを表示します。\n",
    "# tokenizer.decode関数を使用して、トークンIDからそのテキスト表現を取得します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.23967Z",
     "iopub.status.idle": "2024-07-20T00:16:19.240033Z",
     "shell.execute_reply": "2024-07-20T00:16:19.239858Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.239844Z"
    },
    "papermill": {
     "duration": 0.021982,
     "end_time": "2024-07-10T01:14:02.454045",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.432063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  \n",
    "# 拡張データフレームの最初の入力IDをデコードして、元のテキストを表示します。\n",
    "# tokenizer.decode関数を使用して、トークンIDからそのテキスト表現を取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d22ca2",
   "metadata": {},
   "source": [
    "# モデルのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.242975Z",
     "iopub.status.idle": "2024-07-20T00:16:19.243396Z",
     "shell.execute_reply": "2024-07-20T00:16:19.24319Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.243174Z"
    },
    "papermill": {
     "duration": 83.919146,
     "end_time": "2024-07-10T01:15:26.412583",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.493437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU 0にベースモデルをロードします\n",
    "device_0 = torch.device('cuda:0')  # GPU 0をデバイスとして指定します。\n",
    "model_0 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    cfg.model_dir,  # 設定されたモデルディレクトリから事前学習済みモデルを読み込みます。\n",
    "    num_labels=3,  # 分類するラベルの数を指定します。\n",
    "    device_map=device_0,  # モデルをGPU 0にマッピングします。\n",
    "    use_cache=False,  # キャッシュを使用しない設定です。\n",
    ")\n",
    "\n",
    "# GPU 1にベースモデルをロードします\n",
    "device_1 = torch.device('cuda:1')  # GPU 1をデバイスとして指定します。\n",
    "model_1 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    cfg.model_dir,  # 設定されたモデルディレクトリから事前学習済みモデルを読み込みます。\n",
    "    num_labels=3,  # 分類するラベルの数を指定します。\n",
    "    device_map=device_1,  # モデルをGPU 1にマッピングします。\n",
    "    use_cache=False,  # キャッシュを使用しない設定です。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209288d9",
   "metadata": {},
   "source": [
    "#### LoRAアダプターのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.244803Z",
     "iopub.status.idle": "2024-07-20T00:16:19.245324Z",
     "shell.execute_reply": "2024-07-20T00:16:19.245117Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.245101Z"
    },
    "papermill": {
     "duration": 1.265087,
     "end_time": "2024-07-10T01:15:27.719297",
     "exception": false,
     "start_time": "2024-07-10T01:15:26.45421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0にロードします。\n",
    "model_0.config.pad_token_id = model_0.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1にロードします。\n",
    "model_1.config.pad_token_id = model_1.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5500f21e",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.247098Z",
     "iopub.status.idle": "2024-07-20T00:16:19.247484Z",
     "shell.execute_reply": "2024-07-20T00:16:19.247312Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.247296Z"
    },
    "papermill": {
     "duration": 0.026726,
     "end_time": "2024-07-10T01:15:27.838497",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.811771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配計算を無効にします。推論時に計算を節約できます。\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を使用して、計算の効率を向上させます。\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # モデルAの勝ち、モデルBの勝ち、引き分けの結果を格納するリストを初期化します。\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):  # データフレームをバッチサイズごとに処理します。\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # 現在のバッチの終了インデックスを計算します。\n",
    "        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得します。\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストに変換します。\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # 注意マスクをリストに変換します。\n",
    "        \n",
    "        # トークンのパディングを行います。注意警告を表示しないようにします。\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # 最も長い入力に合わせてパディングします。\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchのテンソルとして返します。\n",
    "        )\n",
    "        \n",
    "        # モデルによる出力を取得します。\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットからソフトマックスを計算し、CPUに移動します。\n",
    "        \n",
    "        # 各モデルの勝率をリストに追加します。\n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率\n",
    "        tie.extend(proba[:, 2].tolist())  # 引き分けの確率\n",
    "    \n",
    "    # データフレームに結果を追加します。\n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df  # 更新されたデータフレームを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.248879Z",
     "iopub.status.idle": "2024-07-20T00:16:19.24943Z",
     "shell.execute_reply": "2024-07-20T00:16:19.249062Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.249048Z"
    },
    "papermill": {
     "duration": 4.598663,
     "end_time": "2024-07-10T01:15:32.45234",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.853677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時間を記録します。\n",
    "\n",
    "# 入力の長さに基づいてソートし、動的パディングを最大限に活用します。\n",
    "data = data.sort_values(\"length\", ascending=False)  \n",
    "# total #tokens in sub_1 and sub_2はほぼ同じである必要があります。\n",
    "sub_1 = data.iloc[0::2].copy()  # データを偶数行で取得してサブセット1を作成します。\n",
    "sub_2 = data.iloc[1::2].copy()  # データを奇数行で取得してサブセット2を作成します。\n",
    "\n",
    "# ThreadPoolExecutorを使用して、モデル0とモデル1の推論を並列で実行します。\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 推論を実行\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果をデータフレームに結合します。\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝率のデータを抽出します。\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")  # 経過時間を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.250756Z",
     "iopub.status.idle": "2024-07-20T00:16:19.251103Z",
     "shell.execute_reply": "2024-07-20T00:16:19.25094Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.250926Z"
    },
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-07-10T01:15:32.491283",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.466724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時間を記録します。\n",
    "\n",
    "if cfg.tta:  # テスト時の拡張が有効な場合\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # 入力の長さに基づいてソートして速度を向上させます。\n",
    "    sub_1 = data.iloc[0::2].copy()  # データを偶数行で取得してサブセット1を作成します。\n",
    "    sub_2 = data.iloc[1::2].copy()  # データを奇数行で取得してサブセット2を作成します。\n",
    "\n",
    "    # ThreadPoolExecutorを使用して、モデル0とモデル1の推論を並列で実行します。\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 推論を実行\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # 結果をデータフレームに結合します。\n",
    "    # TTAの順序は反転しています。\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  # TTAの結果を取得します。\n",
    "    # 元の結果とTTA結果の平均を取ります。\n",
    "    proba = (proba + tta_proba) / 2  # 勝率の平均を計算します。\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")  # 経過時間を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.25235Z",
     "iopub.status.idle": "2024-07-20T00:16:19.252721Z",
     "shell.execute_reply": "2024-07-20T00:16:19.252556Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.252541Z"
    },
    "papermill": {
     "duration": 0.034664,
     "end_time": "2024-07-10T01:15:32.539974",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.50531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # 勝率のデータを結果データフレームに更新します。\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝率を更新します。\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # 引き分けの勝率を更新します。\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成します。\n",
    "submission_df.to_csv('submission.csv', index=False)  # 提出ファイルをCSV形式で保存します。\n",
    "display(submission_df)  # 提出データフレームを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d409a4",
   "metadata": {},
   "source": [
    "# トレーニングコード\n",
    "\n",
    "このトレーニングコードは、https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning からフォークされ、インスパイアを受けています。\n",
    "\n",
    "## Kaggleデータの転送"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.254501Z",
     "iopub.status.idle": "2024-07-20T00:16:19.254894Z",
     "shell.execute_reply": "2024-07-20T00:16:19.254724Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.254708Z"
    }
   },
   "outputs": [],
   "source": [
    "# # 重要: このセルを実行してKaggleデータソースを正しい位置（/kaggle/input）にインポートします。\n",
    "# # その後、このセルは削除しても問題ありません。\n",
    "# # 注意: このノートブック環境はKaggleのPython環境とは異なるため、\n",
    "# # お使いのノートブックで使用されるライブラリが欠落している可能性があります。\n",
    "\n",
    "# import os\n",
    "# import sys\n",
    "# from tempfile import NamedTemporaryFile\n",
    "# from urllib.request import urlopen\n",
    "# from urllib.parse import unquote, urlparse\n",
    "# from urllib.error import HTTPError\n",
    "# from zipfile import ZipFile\n",
    "# import tarfile\n",
    "# import shutil\n",
    "\n",
    "# CHUNK_SIZE = 40960\n",
    "# DATA_SOURCE_MAPPING = 'lmsys-chatbot-arena:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66631%2F8346466%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ae2cd10c5a4750deca45551519904c5858980d5cf8cd8ade09b2299d926c86c895b50ba333acf0db5210d0dd29197c9a9c5a525c8afd0186b88f17d3ca756f0562ad5acfa2e856e8b159f554e61f72102865abd60add751dd59bed5126536d977d6fe54d2e85f8e5baa8d3d75337d0a222a89f0f30fa6dd7c360e4a192363dc417e9e4a9c9c23368991db65b4994c2200bee494d8d5e2684f754ab1b1a511f7db3652e01ab658b04d26cc1321e783fa5509f67d4c438808adc7932a0e79a21849375023b36e90cbe288cf68a6185b2ce950464b71c9b6133d49769c67e77a5298809fb63da23c0655165e80661623bd9bb908bc9a486dbc9e09caebf2a01392,qwen2/transformers/qwen2-1.5b-instruct/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F52038%2F62308%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80f6a1f964073d960129f611693afca0666128c1a319b7ab93769a1993d5910e4995cf3c6f2f87323452999b069eafb9f7b01be97dbb80a3441cd0b4871d9d35379750a3b5614397253624b097961c886c48df889ac7b1100231715e2b60bf4f3ffccd5e7fc68b3d7d0668f350a8fefbddb90275770e75aaa7f74fae68b2f5314f610ec2f1abf0436156e9426e6173e229172ca0c4ee91eb2d768de3190c9f07e6c28b73bc8c5553e2dac6320842103524591b663021a41801bb2274b5fd91dd62f174ba8976c74995012ad3ed34ecf554a9e2cb08f91813e9cacc9b8d554c6a7a037414635f30e506ea39f63fb4db01f5cf3322dca02097f9550b1a5454ae99'\n",
    "\n",
    "# KAGGLE_INPUT_PATH='/kaggle/input'\n",
    "# KAGGLE_WORKING_PATH='/kaggle/working'\n",
    "# KAGGLE_SYMLINK='kaggle'\n",
    "\n",
    "# !umount /kaggle/input/ 2> /dev/null\n",
    "# shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
    "# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
    "# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
    "\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "# try:\n",
    "#   os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
    "# except FileExistsError:\n",
    "#   pass\n",
    "\n",
    "# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
    "#     directory, download_url_encoded = data_source_mapping.split(':')\n",
    "#     download_url = unquote(download_url_encoded)\n",
    "#     filename = urlparse(download_url).path\n",
    "#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
    "#     try:\n",
    "#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
    "#             total_length = fileres.headers['content-length']\n",
    "#             print(f'Downloading {directory}, {total_length} bytes compressed')\n",
    "#             dl = 0\n",
    "#             data = fileres.read(CHUNK_SIZE)\n",
    "#             while len(data) > 0:\n",
    "#                 dl += len(data)\n",
    "#                 tfile.write(data)\n",
    "#                 done = int(50 * dl / int(total_length))\n",
    "#                 sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
    "#                 sys.stdout.flush()\n",
    "#                 data = fileres.read(CHUNK_SIZE)\n",
    "#             if filename.endswith('.zip'):\n",
    "#               with ZipFile(tfile) as zfile:\n",
    "#                 zfile.extractall(destination_path)\n",
    "#             else:\n",
    "#               with tarfile.open(tfile.name) as tarfile:\n",
    "#                 tarfile.extractall(destination_path)\n",
    "#             print(f'\\nDownloaded and uncompressed: {directory}')\n",
    "#     except HTTPError as e:\n",
    "#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "#     except OSError as e:\n",
    "#         print(f'Failed to load {download_url} to path {destination_path}')\n",
    "#         continue\n",
    "\n",
    "# print('データソースのインポートが完了しました。')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f863364",
   "metadata": {},
   "source": [
    "## ライブラリのインストールとロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.255983Z",
     "iopub.status.idle": "2024-07-20T00:16:19.256372Z",
     "shell.execute_reply": "2024-07-20T00:16:19.256174Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.256159Z"
    }
   },
   "outputs": [],
   "source": [
    "## gemma-2はtransformers>=4.42.3から利用可能です。\n",
    "# !pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft datasets\n",
    "# transformersライブラリのバージョンを4.42.3以上にアップグレードし、その他の必要なライブラリ（bitsandbytes、accelerate、peft、datasets）をインストールします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.25779Z",
     "iopub.status.idle": "2024-07-20T00:16:19.258299Z",
     "shell.execute_reply": "2024-07-20T00:16:19.258051Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.258031Z"
    }
   },
   "outputs": [],
   "source": [
    "# import os  # オペレーティングシステムの機能を提供するライブラリをインポート\n",
    "# import copy  # オブジェクトのコピーを作成するためのライブラリをインポート\n",
    "# from dataclasses import dataclass  # データクラスの作成に使用するデコレーターをインポート\n",
    "\n",
    "# import numpy as np  # 数値計算ライブラリをインポート\n",
    "# import torch  # 深層学習ライブラリPyTorchをインポート\n",
    "# from datasets import Dataset  # データセットの扱いに使用するクラスをインポート\n",
    "# from transformers import (  # Transformersライブラリから必要なクラスをインポート\n",
    "#     BitsAndBytesConfig,\n",
    "#     AutoTokenizer,\n",
    "#     Qwen2ForSequenceClassification,\n",
    "#     PreTrainedTokenizerBase,\n",
    "#     EvalPrediction,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorWithPadding,\n",
    "# )\n",
    "# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTに関連するクラスをインポート\n",
    "# from sklearn.metrics import log_loss, accuracy_score  # 機械学習の評価指標をインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e49717",
   "metadata": {},
   "source": [
    "## トレーニング設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.260099Z",
     "iopub.status.idle": "2024-07-20T00:16:19.260609Z",
     "shell.execute_reply": "2024-07-20T00:16:19.260369Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.260348Z"
    }
   },
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Config:\n",
    "#     output_dir: str = \"output\"  # 出力先ディレクトリの指定\n",
    "#     model_name: str = \"/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1\"  # 使用するモデルのパス\n",
    "#     checkpoint: str = \"/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1\"  # Qwenモデルのローカルディレクトリ\n",
    "#     max_length: int = 2048  # 入力の最大長\n",
    "#     n_splits: int = 5  # クロスバリデーションの分割数\n",
    "#     fold_idx: int = 0  # 現在のフォールドインデックス\n",
    "#     optim_type: str = \"adamw_8bit\"  # オプティマイザーの種類\n",
    "#     per_device_train_batch_size: int = 4  # デバイスごとのトレーニングバッチサイズ\n",
    "#     gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8\n",
    "#     per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ\n",
    "#     n_epochs: int = 1  # エポック数\n",
    "#     freeze_layers: int = 16  # 最初の16層にはアダプターを追加しない\n",
    "#     lr: float = 2e-4  # 学習率\n",
    "#     warmup_steps: int = 20  # ウォームアップステップ数\n",
    "#     lora_r: int = 16  # LoRAのR値\n",
    "#     lora_alpha: float = lora_r * 2  # LoRAのアルファ値\n",
    "#     lora_dropout: float = 0.05  # LoRAのドロップアウト率\n",
    "#     lora_bias: str = \"none\"  # LoRAのバイアス設定\n",
    "\n",
    "# config = Config()  # 設定のインスタンスを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14927cb",
   "metadata": {},
   "source": [
    "## トレーニング引数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.263021Z",
     "iopub.status.idle": "2024-07-20T00:16:19.263439Z",
     "shell.execute_reply": "2024-07-20T00:16:19.263227Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.263212Z"
    }
   },
   "outputs": [],
   "source": [
    "# training_args = TrainingArguments(\n",
    "#     output_dir=\"output\",  # 出力先ディレクトリの指定\n",
    "#     overwrite_output_dir=True,  # 出力ディレクトリを上書きするかどうか\n",
    "#     report_to=\"none\",  # ログの報告先（ここでは無効化）\n",
    "#     num_train_epochs=config.n_epochs,  # トレーニングエポック数\n",
    "#     per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "#     gradient_accumulation_steps=config.gradient_accumulation_steps,  # 勾配累積のステップ数\n",
    "#     per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ\n",
    "#     logging_steps=10,  # ロギングの頻度\n",
    "#     eval_strategy=\"epoch\",  # 評価戦略（エポックごとに評価）\n",
    "#     save_strategy=\"steps\",  # モデル保存戦略（ステップごとに保存）\n",
    "#     save_steps=200,  # モデルを保存するステップ数\n",
    "#     optim=config.optim_type,  # 使用するオプティマイザー\n",
    "#     fp16=True,  # 半精度学習を有効にするかどうか\n",
    "#     learning_rate=config.lr,  # 学習率\n",
    "#     warmup_steps=config.warmup_steps,  # ウォームアップステップ数\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e9c7f5",
   "metadata": {},
   "source": [
    "## LoRA設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.264398Z",
     "iopub.status.idle": "2024-07-20T00:16:19.264765Z",
     "shell.execute_reply": "2024-07-20T00:16:19.264599Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.264584Z"
    }
   },
   "outputs": [],
   "source": [
    "# lora_config = LoraConfig(\n",
    "#     r=config.lora_r,  # LoRAのR値\n",
    "#     lora_alpha=config.lora_alpha,  # LoRAのアルファ値\n",
    "#     # 自己注意のみを対象とする\n",
    "#     target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # ターゲットモジュール（自己注意のプロジェクション）\n",
    "#     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],  # 変換するレイヤーのリスト（フリーズされていないレイヤー）\n",
    "#     lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率\n",
    "#     bias=config.lora_bias,  # LoRAのバイアス設定\n",
    "#     task_type=TaskType.SEQ_CLS,  # タスクのタイプ（シーケンス分類）\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a19b4b",
   "metadata": {},
   "source": [
    "## モデルの初期化とトークン化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.26592Z",
     "iopub.status.idle": "2024-07-20T00:16:19.266307Z",
     "shell.execute_reply": "2024-07-20T00:16:19.266116Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.266102Z"
    }
   },
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(config.model_name)  # 設定されたモデル名からトークナイザーを初期化します。\n",
    "# tokenizer.add_eos_token = True  # 終了トークンを追加します。\n",
    "# tokenizer.padding_side = \"right\"  # パディングを右側に設定します。\n",
    "# model = Qwen2ForSequenceClassification.from_pretrained(\n",
    "#     config.model_name,  # 設定されたモデル名からモデルを初期化します。\n",
    "#     num_labels=3,  # 分類するラベルの数\n",
    "#     torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "#     device_map=\"auto\",  # 自動的にデバイスをマッピングします。\n",
    "# )\n",
    "# model.config.use_cache = False  # キャッシュの使用を無効にします。\n",
    "# model = prepare_model_for_kbit_training(model)  # モデルを8bitトレーニング用に準備します。\n",
    "# model = get_peft_model(model, lora_config)  # LoRAモデルを取得します。\n",
    "# model.config.pad_token_id = model.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。\n",
    "# model  # 初期化されたモデルを表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.267566Z",
     "iopub.status.idle": "2024-07-20T00:16:19.267923Z",
     "shell.execute_reply": "2024-07-20T00:16:19.267762Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.267747Z"
    }
   },
   "outputs": [],
   "source": [
    "# model.print_trainable_parameters()  # トレーニング可能なパラメータを表示します。\n",
    "# この関数は、モデル内で訓練されるパラメータの数や詳細を示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ce0f31",
   "metadata": {},
   "source": [
    "# トレーニングデータのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.269677Z",
     "iopub.status.idle": "2024-07-20T00:16:19.270024Z",
     "shell.execute_reply": "2024-07-20T00:16:19.269866Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.269851Z"
    }
   },
   "outputs": [],
   "source": [
    "# ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # トレーニングデータをCSVファイルから読み込みます。\n",
    "# class CustomTokenizer:\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         tokenizer: PreTrainedTokenizerBase,  # トークナイザーの初期化\n",
    "#         max_length: int  # 最大長の設定\n",
    "#     ) -> None:\n",
    "#         self.tokenizer = tokenizer  # トークナイザーを保存\n",
    "#         self.max_length = max_length  # 最大長を保存\n",
    "\n",
    "#     def __call__(self, batch: dict) -> dict:  # バッチデータを処理するメソッド\n",
    "#         prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]  # プロンプトに接頭辞を追加\n",
    "#         response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]  # 応答Aに接頭辞を追加\n",
    "#         response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]  # 応答Bに接頭辞を追加\n",
    "#         texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # テキストを結合\n",
    "#         tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークン化\n",
    "#         labels = []  # ラベルを初期化\n",
    "#         for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):  # 勝率に基づくラベルを設定\n",
    "#             if a_win:\n",
    "#                 label = 0  # モデルAが勝つ場合\n",
    "#             elif b_win:\n",
    "#                 label = 1  # モデルBが勝つ場合\n",
    "#             else:\n",
    "#                 label = 2  # 引き分けの場合\n",
    "#             labels.append(label)  # ラベルを追加\n",
    "#         return {**tokenized, \"labels\": labels}  # トークン化されたデータとラベルを返す\n",
    "\n",
    "#     @staticmethod\n",
    "#     def process_text(text: str) -> str:  # テキストを処理する静的メソッド\n",
    "#         return \" \".join(eval(text, {\"null\": \"\"}))  # nullを空文字に置き換えた後、空白で結合します。\n",
    "# encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化\n",
    "# ds = ds.map(encode, batched=True)  # データセットにトークナイザーを適用します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be123a58",
   "metadata": {},
   "source": [
    "## トレーニングの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.27132Z",
     "iopub.status.idle": "2024-07-20T00:16:19.27167Z",
     "shell.execute_reply": "2024-07-20T00:16:19.271506Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.271492Z"
    }
   },
   "outputs": [],
   "source": [
    "# def compute_metrics(eval_preds: EvalPrediction) -> dict:  # 評価指標を計算する関数\n",
    "#     preds = eval_preds.predictions  # 予測結果を取得\n",
    "#     labels = eval_preds.label_ids  # ラベルを取得\n",
    "#     probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # ソフトマックスを適用して確率を計算\n",
    "#     loss = log_loss(y_true=labels, y_pred=probs)  # ログ損失を計算\n",
    "#     acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算\n",
    "#     return {\"acc\": acc, \"log_loss\": loss}  # 精度とログ損失を辞書として返す\n",
    "\n",
    "# folds = [  # クロスバリデーションのためのフォールドを作成\n",
    "#     (\n",
    "#         [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニングインデックス\n",
    "#         [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 検証インデックス\n",
    "#     )\n",
    "#     for fold_idx in range(config.n_splits)  # 注目するフォールドのインデックス\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-20T00:16:19.272998Z",
     "iopub.status.idle": "2024-07-20T00:16:19.273371Z",
     "shell.execute_reply": "2024-07-20T00:16:19.273181Z",
     "shell.execute_reply.started": "2024-07-20T00:16:19.273167Z"
    }
   },
   "outputs": [],
   "source": [
    "# train_idx, eval_idx = folds[config.fold_idx]  # 指定されたフォールドインデックスに基づいてトレーニングと評価のインデックスを取得\n",
    "\n",
    "# trainer = Trainer(  # トレーニング用のTrainerインスタンスを作成\n",
    "#     args=training_args,  # トレーニング引数\n",
    "#     model=model,  # トレーニングするモデル\n",
    "#     tokenizer=tokenizer,  # 使用するトークナイザー\n",
    "#     train_dataset=ds.select(train_idx),  # トレーニングデータセット\n",
    "#     eval_dataset=ds.select(eval_idx),  # 評価データセット\n",
    "#     compute_metrics=compute_metrics,  # 評価指標の計算を行う関数\n",
    "#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # パディングを行うデータコレータ\n",
    "# )\n",
    "# trainer.train()  # トレーニングを開始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c35c40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Jiadi Wang トピック著者\n",
    "> \n",
    "> これはトレーニングの損失データです。\n",
    "> \n",
    "> もしよろしければ、UPVOTEしていただけると非常に助かります！ありがとうございます！\n",
    "> \n",
    "> \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5395179,
     "sourceId": 8963385,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 71342,
     "modelInstanceId": 52038,
     "sourceId": 62308,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.347272,
   "end_time": "2024-07-10T01:15:35.655682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T01:13:07.30841",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f59addf0d2f40309e025976c382cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_268e3946106b4e41849bf11c5a375dac",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb130c471af4927a6644f932ae47523",
       "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
      }
     },
     "19ef2d43bafa44a8b20dc5230aea5ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca042ffa14e4dbebdc66435f7b1f07f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d03719518b8423099b8b68a92e449d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64ef70cfa9c04764868fa52963323322",
       "placeholder": "​",
       "style": "IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d89705c74d34016bbc1e0601ead825c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d409334237014614bf9ae742597d98ea",
       "placeholder": "​",
       "style": "IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6",
       "value": " 2/2 [01:18&lt;00:00, 34.68s/it]"
      }
     },
     "1ef6d64d40d8461d9e6adddd513089b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268e3946106b4e41849bf11c5a375dac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "324c2396f44f45c89d9ec264007ef9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5bb130c471af4927a6644f932ae47523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d243712a1a545e99fa858b0cf19831d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e81b324ca1b46a2a96d02bb2acadc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64ef70cfa9c04764868fa52963323322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ae4b02a13f4570ad729c437ebd28ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f",
        "IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef",
        "IPY_MODEL_0f59addf0d2f40309e025976c382cad8"
       ],
       "layout": "IPY_MODEL_9422a87b93ab4473913e601da3a18689"
      }
     },
     "81b4a9a7cde64b17856b89dbd238c0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff",
       "value": 2
      }
     },
     "85d217d6b45847869cea506db59e8b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d243712a1a545e99fa858b0cf19831d",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96385fd98f304649ab5c4ae81333fb63",
       "value": 2
      }
     },
     "875758123e4f41f0b5c3fa0cd4fb47c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9422a87b93ab4473913e601da3a18689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96385fd98f304649ab5c4ae81333fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d409334237014614bf9ae742597d98ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d576a283e6424206ab4c25d809241c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8a7714cd80d479e859b6ae31ebce7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d98918fae8174629b4819a1114f21202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d03719518b8423099b8b68a92e449d7",
        "IPY_MODEL_85d217d6b45847869cea506db59e8b42",
        "IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"
       ],
       "layout": "IPY_MODEL_d576a283e6424206ab4c25d809241c21"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
