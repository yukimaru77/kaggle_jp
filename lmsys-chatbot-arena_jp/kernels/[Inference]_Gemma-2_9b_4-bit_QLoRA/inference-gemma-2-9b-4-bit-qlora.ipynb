{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c87ef42",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックでは、4ビット量子化された「Gemma-2 9b Instruct」モデルに基づくLoRAアダプターを使用した推論の手法が示されています。主な目的は、Chatbot Arenaコンペティションにおける、2つの言語モデル間のユーザー応答の選好予測を行うことです。\n",
    "\n",
    "### 問題の扱い\n",
    "ノートブックは、LoRAアダプターを用いて、量子化による誤差の影響を軽減しつつ、推論を迅速化する方法を探求しています。モデルをマージすることで誤差が生じる可能性があるため、LoRAアダプターを維持した状態での推論を推奨しています。また、モデルの性能は評価セットでの対数損失が0.9371、公開リーダーボードでの対数損失が0.941であると報告しています。\n",
    "\n",
    "### 使用ライブラリと手法\n",
    "ノートブックでは、主に以下のライブラリが使用されています:\n",
    "- **Transformers**: 「Gemma2ForSequenceClassification」と「GemmaTokenizerFast」を使用してモデルの呼び出しやトークン化を実施。\n",
    "- **Peft**: LoRAアダプターの適用に用いられます。\n",
    "- **Torch**: GPU利用や自動混合精度計算をサポートし、モデルの推論が行われます。\n",
    "- **PandasとNumPy**: データ処理と操作のためにデータフレームを使用し、結果の格納に役立てています。\n",
    "\n",
    "### データ処理と推論\n",
    "データはCSVファイルから読み込まれ、テキストの前処理を行った後、トークナイズされます。推論は、2つのGPUを用いてバッチごとに行われ、モデルAとモデルBの各々の勝率を計算します。結果として、モデルごとの勝率や引き分け確率をデータフレームに格納し、最終的に提出用のCSVファイルとして成果物を保存します。\n",
    "\n",
    "全体として、このノートブックは、量子化されたモデルを最適に利用するための手法と細かな設定を提供し、効率的に予測を行うフレームワークを構築しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff1ae8",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebook内で使用されている専門用語の簡単な解説です。これらの用語は、初心者が特に理解しにくい可能性があるものです。\n",
    "\n",
    "1. **Gemma-2**: 大規模言語モデル（LLM）の一種で、テキスト生成や分類タスクに用いられます。特に今回のノートブックでは、「Gemma-2 9b Instruct」というバージョンが使用されています。\n",
    "\n",
    "2. **LoRA (Low-Rank Adaptation)**: 転移学習やファインチューニングを行うための手法で、もともとのモデルの重みを劣化させることなく、新しい情報を効率的に学習するために低ランクの更新を加える手法です。\n",
    "\n",
    "3. **量子化 (Quantization)**: モデルの重みやアクティベーションの精度を減らして、メモリ使用量を削減し、計算効率を向上させる技術です。特に4ビット量子化は、重みを4ビットで表現することを意味します。\n",
    "\n",
    "4. **アテンションマスク (Attention Mask)**: モデルに入力されるデータのどの部分を注目すべきかを示すバイナリのマスクです。通常は、パディングされた部分（無視すべき部分）を除外するために使われます。\n",
    "\n",
    "5. **トークナイズ (Tokenization)**: 文章やテキストを処理可能な単位（トークン）に分解するプロセスです。これにより、テキストがモデルに入力できる形式に変換されます。\n",
    "\n",
    "6. **トークンID (Token ID)**: トークナイズされた各トークンに対応する整数値で、単語やサブワードを表現します。\n",
    "\n",
    "7. **ソフトマックス (Softmax)**: 複数のクラスに対する確率を計算する関数です。出力層で使用され、各クラスのスコアを確率に変換します。\n",
    "\n",
    "8. **TTA (Test-Time Augmentation)**: テストデータに対してデータ拡張を適用する手法で、モデルの予測の安定性を向上させます。複数の異なる視点やデータの拡張から得られる予測を平均化することが一般的です。\n",
    "\n",
    "9. **Pad_without_fast_tokenizer_warning**: 特定のトークナイザー使用時に、パディングに関する警告を避けるための関数です。入力のパディングを行う際には、慎重に扱う必要があります。\n",
    "\n",
    "10. **自動混合精度 (Automatic Mixed Precision)**: 計算の精度を自動的に調整する技術で、処理速度を向上させるために、必要に応じて32ビット（float32）と16ビット（float16）の演算を切り替えます。\n",
    "\n",
    "11. **キャッシュ (Cache)**: 計算結果を保存しておき、再使用することで計算を効率化すること。特に推論時間を短縮するために有効です。\n",
    "\n",
    "これらの用語は、特にこのノートブックやその設定において重要な部分を占めており、理解を深める手助けとなります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79015a1c",
   "metadata": {},
   "source": [
    "## このノートブックについて\n",
    "\n",
    "これは、4ビット量子化された [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) と、私がアップロードしたスクリプトを使用してトレーニングしたLoRAアダプターを利用した推論ノートブックです [ここ](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune)で確認できます。\n",
    "LoRAアダプターをベースモデルにマージすることで推論を速くすることもできますが、安易にそうすると無視できない量子化誤差が発生する可能性があります。そのため、私はLoRAアダプターをマージせずに維持することにしました。\n",
    "\n",
    "## 結果\n",
    "\n",
    "| サブセット | 対数損失 |\n",
    "| - | - |\n",
    "| 評価セット | 0.9371 |\n",
    "| 公開LB | 0.941 |\n",
    "\n",
    "提出には約4時間かかります。`max_length=2048`でTTAは使用していません。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T02:18:10.634578Z",
     "iopub.status.busy": "2024-07-11T02:18:10.634229Z",
     "iopub.status.idle": "2024-07-11T02:18:44.692497Z",
     "shell.execute_reply": "2024-07-11T02:18:44.691383Z",
     "shell.execute_reply.started": "2024-07-11T02:18:10.634551Z"
    },
    "papermill": {
     "duration": 31.479497,
     "end_time": "2024-07-10T01:13:41.690971",
     "exception": false,
     "start_time": "2024-07-10T01:13:10.211474",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install transformers peft accelerate bitsandbytes \\\n",
    "    -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:18:44.694926Z",
     "iopub.status.busy": "2024-07-11T02:18:44.694593Z",
     "iopub.status.idle": "2024-07-11T02:19:11.589404Z",
     "shell.execute_reply": "2024-07-11T02:19:11.588563Z",
     "shell.execute_reply.started": "2024-07-11T02:18:44.694888Z"
    },
    "papermill": {
     "duration": 19.200405,
     "end_time": "2024-07-10T01:14:00.90474",
     "exception": false,
     "start_time": "2024-07-10T01:13:41.704335",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.591498Z",
     "iopub.status.busy": "2024-07-11T02:19:11.590785Z",
     "iopub.status.idle": "2024-07-11T02:19:11.624019Z",
     "shell.execute_reply": "2024-07-11T02:19:11.623308Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.591463Z"
    },
    "papermill": {
     "duration": 0.047799,
     "end_time": "2024-07-10T01:14:00.965921",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.918122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d20e0e",
   "metadata": {},
   "source": [
    "## 設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.626639Z",
     "iopub.status.busy": "2024-07-11T02:19:11.626385Z",
     "iopub.status.idle": "2024-07-11T02:19:11.632251Z",
     "shell.execute_reply": "2024-07-11T02:19:11.631296Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.626616Z"
    },
    "papermill": {
     "duration": 0.021338,
     "end_time": "2024-07-10T01:14:01.000606",
     "exception": false,
     "start_time": "2024-07-10T01:14:00.979268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # テスト時のデータ拡張。<prompt>-<model-bの応答>-<model-aの応答>\n",
    "    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するか\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5eafd8b",
   "metadata": {},
   "source": [
    "# データの読み込みと前処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.634389Z",
     "iopub.status.busy": "2024-07-11T02:19:11.633723Z",
     "iopub.status.idle": "2024-07-11T02:19:11.656286Z",
     "shell.execute_reply": "2024-07-11T02:19:11.655537Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.634353Z"
    },
    "papermill": {
     "duration": 0.02967,
     "end_time": "2024-07-10T01:14:01.06946",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.03979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.657576Z",
     "iopub.status.busy": "2024-07-11T02:19:11.657297Z",
     "iopub.status.idle": "2024-07-11T02:19:11.686718Z",
     "shell.execute_reply": "2024-07-11T02:19:11.685894Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.657552Z"
    },
    "papermill": {
     "duration": 0.040127,
     "end_time": "2024-07-10T01:14:01.12241",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.082283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8e3589",
   "metadata": {},
   "source": [
    "# トークナイズ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.688575Z",
     "iopub.status.busy": "2024-07-11T02:19:11.687957Z",
     "iopub.status.idle": "2024-07-11T02:19:11.697488Z",
     "shell.execute_reply": "2024-07-11T02:19:11.696538Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.688541Z"
    },
    "papermill": {
     "duration": 0.030237,
     "end_time": "2024-07-10T01:14:01.194318",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.164081",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]  # プロンプトに\"<prompt>: \"を追加\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]  # 応答Aにプレフィックスを追加\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]  # 応答Bにプレフィックスを追加\n",
    "    if spread_max_length:  # spread_max_lengthがTrueの場合\n",
    "        # 各要素をmax_length//3でトークナイズ\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 各リストを結合\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # 各入力の長さに応じたアテンションマスクを作成\n",
    "    else:\n",
    "        # 各要素を結合してトークン化\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイズ\n",
    "        input_ids = tokenized.input_ids  # トークンIDを取得\n",
    "        attention_mask = tokenized.attention_mask  # アテンションマスクを取得\n",
    "    return input_ids, attention_mask  # トークンIDとアテンションマスクを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:11.699202Z",
     "iopub.status.busy": "2024-07-11T02:19:11.698926Z",
     "iopub.status.idle": "2024-07-11T02:19:12.964537Z",
     "shell.execute_reply": "2024-07-11T02:19:12.96355Z",
     "shell.execute_reply.started": "2024-07-11T02:19:11.699179Z"
    },
    "papermill": {
     "duration": 1.169844,
     "end_time": "2024-07-10T01:14:02.377579",
     "exception": false,
     "start_time": "2024-07-10T01:14:01.207735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)  # Gemmaトークナイザーを読み込む\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加\n",
    "tokenizer.padding_side = \"right\"  # パディングの位置を右に設定\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # トークン化した結果をデータフレームに格納\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # 各入力の長さを計算して追加\n",
    "\n",
    "aug_data = pd.DataFrame()  # 拡張データ用のデータフレームを作成\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# response_aとresponse_bを入れ替える\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])  # トークナイズして格納\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # 長さを計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:12.966235Z",
     "iopub.status.busy": "2024-07-11T02:19:12.96586Z",
     "iopub.status.idle": "2024-07-11T02:19:12.972312Z",
     "shell.execute_reply": "2024-07-11T02:19:12.971444Z",
     "shell.execute_reply.started": "2024-07-11T02:19:12.966202Z"
    },
    "papermill": {
     "duration": 0.024759,
     "end_time": "2024-07-10T01:14:02.419091",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.394332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # トークナイズしたデータの最初の要素をデコードして表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:12.975296Z",
     "iopub.status.busy": "2024-07-11T02:19:12.975025Z",
     "iopub.status.idle": "2024-07-11T02:19:12.983841Z",
     "shell.execute_reply": "2024-07-11T02:19:12.983071Z",
     "shell.execute_reply.started": "2024-07-11T02:19:12.975274Z"
    },
    "papermill": {
     "duration": 0.021982,
     "end_time": "2024-07-10T01:14:02.454045",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.432063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  # 拡張データの最初の要素をデコードして表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4b40a",
   "metadata": {},
   "source": [
    "# モデルを読み込む\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:19:12.985347Z",
     "iopub.status.busy": "2024-07-11T02:19:12.98502Z",
     "iopub.status.idle": "2024-07-11T02:21:11.536966Z",
     "shell.execute_reply": "2024-07-11T02:21:11.536086Z",
     "shell.execute_reply.started": "2024-07-11T02:19:12.985317Z"
    },
    "papermill": {
     "duration": 83.919146,
     "end_time": "2024-07-10T01:15:26.412583",
     "exception": false,
     "start_time": "2024-07-10T01:14:02.493437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU 0にベースモデルを読み込む\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# GPU 1にベースモデルを読み込む\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3887df57",
   "metadata": {},
   "source": [
    "#### LoRAアダプターを読み込む\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:21:11.538825Z",
     "iopub.status.busy": "2024-07-11T02:21:11.538462Z",
     "iopub.status.idle": "2024-07-11T02:21:12.856724Z",
     "shell.execute_reply": "2024-07-11T02:21:12.85585Z",
     "shell.execute_reply.started": "2024-07-11T02:21:11.538788Z"
    },
    "papermill": {
     "duration": 1.265087,
     "end_time": "2024-07-10T01:15:27.719297",
     "exception": false,
     "start_time": "2024-07-10T01:15:26.45421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0に適用\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1に適用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73289a",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:21:12.880147Z",
     "iopub.status.busy": "2024-07-11T02:21:12.879819Z",
     "iopub.status.idle": "2024-07-11T02:21:12.89002Z",
     "shell.execute_reply": "2024-07-11T02:21:12.889299Z",
     "shell.execute_reply.started": "2024-07-11T02:21:12.880121Z"
    },
    "papermill": {
     "duration": 0.026726,
     "end_time": "2024-07-10T01:15:27.838497",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.811771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配計算を無効にする\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を使って演算を行う\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝率と引き分けを記録するリスト\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終端インデックスを計算\n",
    "        tmp = df.iloc[start_idx:end_idx]  # データフレームからバッチを取得\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDを取得\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクを取得\n",
    "        # トークナイザーを使ってデータをパディング\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # モデルで出力を計算\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックスで確率に変換\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率をリストに追加\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率をリストに追加\n",
    "        tie.extend(proba[:, 2].tolist())  # 引き分けの確率をリストに追加\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # モデルAの勝率をデータフレームに追加\n",
    "    df[\"winner_model_b\"] = b_win  # モデルBの勝率をデータフレームに追加\n",
    "    df[\"winner_tie\"] = tie  # 引き分けの確率をデータフレームに追加\n",
    "    \n",
    "    return df  # 更新されたデータフレームを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:21:12.891456Z",
     "iopub.status.busy": "2024-07-11T02:21:12.891104Z",
     "iopub.status.idle": "2024-07-11T02:21:17.780279Z",
     "shell.execute_reply": "2024-07-11T02:21:17.779297Z",
     "shell.execute_reply.started": "2024-07-11T02:21:12.891431Z"
    },
    "papermill": {
     "duration": 4.598663,
     "end_time": "2024-07-10T01:15:32.45234",
     "exception": false,
     "start_time": "2024-07-10T01:15:27.853677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時間を記録\n",
    "\n",
    "# 入力の長さでソートして動的パディングを最大限に活用する\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# sub_1とsub_2のトークン数がほぼ同じになるように分ける\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータを選択\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータを選択\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝率の配列を取得\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 処理時間を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:21:17.781953Z",
     "iopub.status.busy": "2024-07-11T02:21:17.781617Z",
     "iopub.status.idle": "2024-07-11T02:21:17.789824Z",
     "shell.execute_reply": "2024-07-11T02:21:17.788916Z",
     "shell.execute_reply.started": "2024-07-11T02:21:17.781925Z"
    },
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-07-10T01:15:32.491283",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.466724",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:  # TTAが有効な場合\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # 入力の長さでソートしてスピードを向上させる\n",
    "    sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータ\n",
    "    sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータ\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # TTAの結果を結合\n",
    "    # TTAの順序が反転するので調整\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # 元の結果とTTA結果を平均\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 処理時間を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T02:21:17.791305Z",
     "iopub.status.busy": "2024-07-11T02:21:17.791033Z",
     "iopub.status.idle": "2024-07-11T02:21:17.818196Z",
     "shell.execute_reply": "2024-07-11T02:21:17.81731Z",
     "shell.execute_reply.started": "2024-07-11T02:21:17.791282Z"
    },
    "papermill": {
     "duration": 0.034664,
     "end_time": "2024-07-10T01:15:32.539974",
     "exception": false,
     "start_time": "2024-07-10T01:15:32.50531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝率を更新\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝率を更新\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # 引き分けの勝率を更新\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用のデータフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルに保存\n",
    "display(submission_df)  # 提出データを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9430f540",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Cody_Null\n",
    "> \n",
    "> 推論時間を速めるアイデアはありますか？パフォーマンスを失わずに。\n",
    "> \n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > LoRA_A x LoRA_Bを最初に計算したときにキャッシュするのは簡単な方法ですが、それほど速度向上は見込めないかもしれません。\n",
    "> > \n",
    "> > TensorRTやvLLMのような最適化ライブラリを使えるのかも気になります。\n",
    "> > \n",
    "> > \n",
    "> > > ## Cody_Null\n",
    "> > > \n",
    "> > > vLLMを試したことがありますか？私は試しましたが、うまく動かす方法がわかりませんでした。 \n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Eisuke Mizutaniトピック作成者\n",
    "> > > \n",
    "> > > まだ試していません。max_lengthを増やすと対数損失が減少することは認識していますが、2048を越えると改善は非常に小さいです。私のケースでは2048から4096にすると対数損失が0.002減少しました。残りの時間で最適化できる他の方法を検討しています。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## carvingfate\n",
    "> \n",
    "> 私は以前30位でしたが、このコードのおかげで私の努力が無駄に思えてしまいます。しかし、共有する精神を尊重しており、これがインターネットの精神であると思います。\n",
    "> \n",
    "> \n",
    "> > ## jointcc2\n",
    "> > \n",
    "> > 業界の状況もそうだと思います、一つのモデルが全ての過去の努力を上回りますね。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Van chrn\n",
    "> \n",
    "> なぜvLLMではなく？それはもっと速いかもしれません！\n",
    "> \n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > 私はそれに取り組んでいます！\n",
    "> > \n",
    "> > \n",
    "> > > ## Cody_Null\n",
    "> > > \n",
    "> > > この使用方法を実現しましたか？私はvLLMを使ったことがなく、動作しているのを見てみたいです！\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Dai LinLing\n",
    "> \n",
    "> 共有してくれてありがとう。これは私にとって非常に助けになり、理解も深まりました。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Turbo\n",
    "> \n",
    "> [@emiz6413](https://www.kaggle.com/emiz6413)   ノートブックを共有してくれてありがとう。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Vitalii Bozheniuk\n",
    "> \n",
    "> なぜシルバーティアの解決策を公開するのかわかりません。この0.88のノートブックを公開すれば、全員が1位になれるのですか？人々がアイデアやノートブックを共有するのは理解できますが、30位のノートブックを共有するのは意味がありません。競争とチャレンジの雰囲気が消えてしまいます。\n",
    "> \n",
    "> \n",
    "> > ## G John Rao\n",
    "> > \n",
    "> > まだ1ヶ月残っていますが、初心者にとってはブーストになります。経験豊富な専門家にとっては、1ヶ月は新しいアイデアを構築したり実装するには十分な時間です。 \n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Korey Ma\n",
    "> \n",
    "> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！私はいくつかの追加パラメータを微調整し、cv&lb(0.912&0.924)を達成しました。さらに良くするために他のトリックを試したいです😆\n",
    "> \n",
    "> \n",
    "> > ## Yichuan Gao\n",
    "> > \n",
    "> > もう少し詳細を共有していただけますか？ハイパーパラメータを調整しているのか、LoRAレイヤーやランクを増やしているのか？\n",
    "> > \n",
    "> > > ## Korey Ma\n",
    "> > > \n",
    "> > > \"o_proj\"と\"gate_proj\"を追加しただけです。 \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## samson\n",
    "> \n",
    "> かなり良いノートブックを作成されていて、コメントに対する見解も妥当です。\n",
    "> \n",
    "> ですが、なぜ重みを共有したのでしょう？真剣に学びたいと思っている人は、あなたの方法を使ったりそれを自分に適応したりするでしょう。しかし、あなたのせいで100以上のチームが盲目的にコピー+提出することになりました。これにより、中間にいる人々が適切なチームメートを見つけるのは不可能です。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## superferg\n",
    "> \n",
    "> すごいですね。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## yuanzhe zhou\n",
    "> \n",
    "> よくやりました！つまり、LLMを使うことが鍵ですか？BERTタイプのモデルは収束するには小さすぎるようです。\n",
    "> \n",
    "> > ## Valentin Werner\n",
    "> > \n",
    "> > まさにそうです。私もLLMがシーケンス生成で十分に微調整されていると思います。AI生成テキストをより認識し、テキストがどうあるべきかに最適化されているため、このタスクに適しています。これにより、基本的に火に火をもって戦うことになります。\n",
    "> > \n",
    "> > > ## Eisuke Mizutaniトピック作成者\n",
    "> > > \n",
    "> > > 実際、deberta-v3-smallを完全に微調整したところ、約1.1になりました。\n",
    "> > > > BERTスタイルのエンコーダアーキテクチャは、理論的にはこれらの分類タスクにより適していると思います。\n",
    "> > > しかし、あなたが指摘したように、実際にはLLMははるかに大きい（deberta v2 xxlargeは1.5B）ため、過学習を避けることができ、より多くのメモリ容量を持つことができます。\n",
    "> > > もう一つの理由は、指示微調整という、非常に競技に似たデータを使用しているからかもしれません。\n",
    "> > > 私はバニラgemma-2-9bでテストしたことはありませんが、どのように動作するかを見るのは興味深いです。\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## ano\n",
    "> \n",
    "> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！微調整したモデルの検証データとcvスコアについて教えていただけますか？あなたのトレーニングノートブックに基づいて、私は行数を5で割った数に基づいて、約20％のデータを検証用に使用しました。その後、対数損失を計算しましたが、cvスコアは0.9未満でした。明らかに、検証データに間違いがあったため、cvスコアはあなたのトレーニングノートブックで書かれた0.9371よりも低くなりました。微調整モデルの検証データをどのように作成すればよいか教えていただけますか？\n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > 私のトレーニングノートブックで検証データを作成するには、次の行が実行されないことを確認してください。\n",
    "> > \n",
    "> > ```\n",
    "> > # ds = ds.select(torch.arange(100))\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > 次に、最後のセルにあるこの行で検証セットを選択する必要があります。\n",
    "> > \n",
    "> > ```\n",
    "> > ds.select(eval_idx)\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > > ## ano\n",
    "> > > \n",
    "> > > 返信ありがとうございます。もちろん、データを減らすための行を削除しました。\n",
    "> > > \n",
    "> > > では、検証データはあなたのトレーニングノートブックでのn_splits = 5およびfold_idx = 0で選択されるのですね。うーん、そうすると、私のコードにCVスコアを計算する上での間違いがあったかもしれません。 \n",
    "> > > > [UPDATE] バグを見つけました。ありがとうございます！\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Guillermo Perez G\n",
    "> \n",
    "> 素晴らしい！しかし、スコアをさらに下げることはできないと思います。それとも、ノートブックの質によるのでしょうか？\n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > 現在のトップスコアは0.9未満です。スコアを改善するためのアイデアが残っていると思います。\n",
    "> > \n",
    "> > > ## floriandev\n",
    "> > > \n",
    "> > > Eisukeさん、素晴らしいノートブックをありがとう！あなたのノートブックを使うことで0.9未満になる可能性はありますか？\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Sparsh Tewatia\n",
    "> \n",
    "> 終了までにどれくらいの時間がかかりますか？時間が許せば、LLAMA3とGemma 2の2つのLLMのアンサンブルを行うことができます。\n",
    "> \n",
    "> > ## Lorry Zou\n",
    "> > \n",
    "> > ノートブックでは約4時間と記載されていますので、llama3とgemma2のアンサンブルは実行可能のようです。 \n",
    "> > \n",
    "> > > ## Eisuke Mizutaniトピック作成者\n",
    "> > > \n",
    "> > > 私はllama3とgemma2のアンサンブルを9時間以内で実行できました。max_length=2048およびper_device_batch_size=4を使用しました。\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Lorry Zou\n",
    "> \n",
    "> 私が行った全ての作業が無駄になりました…悲しい😅 でも素晴らしい作品です。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Sam\n",
    "> \n",
    "> 私はこのノートブックで提供されているのと同じ推論パラメータ（batch_size、max_length）を使ってGemmaモデルを試すことに決めました。（llamaではなくBert-likeモデルを使用している以外はすべて同じです）。このノートブックはT4x2で9時間以上かかっても終わらず、途中で止まってしまいます。\n",
    "> \n",
    "> 25,000の例をGemmaモデルで処理すると、約17時間かかることがわかりました。\n",
    "> \n",
    "> 何が問題かアドバイスをいただけますか？Gemmaモデルの推論を速くするにはどうすればいいですか？\n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > [@andreysemenovmax](https://www.kaggle.com/andreysemenovmax) \n",
    "> > \n",
    "> > 8ビット重量とfp16アクティベーションでのGemma2 9bの動的量子化を行うと、提出のために4.5時間以内に実行されます。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## capyun007\n",
    "> \n",
    "> Kaggleの初心者で質問があります。次のコードを実行してDataFrameをCSVファイルとして保存した後：\n",
    "> \n",
    "> submission_df.to_csv('submission.csv', index=False)\n",
    "> \n",
    "> submission.csvファイルはどこにありますか？\n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > ノートブックをコミット&保存すると、出力タブに表示されるはずです。インタラクティブセッションで実行した場合は、作業ディレクトリ（./submission.csv）で確認できます。\n",
    "> > \n",
    "> > > ## capyun007\n",
    "> > > \n",
    "> > > 自分のノートブックをコミット&保存しましたが、出力タブに表示されません。\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## kanishka sriramoju\n",
    "> \n",
    "> こんにちは、私はここで初心者です。あなたがKaggle入力ディレクトリから事前にトレーニングされたモデルを読み込んだことを見ました。この競技では、ノートブックが独立した環境で実行され、これらの作成されたディレクトリにアクセスできないということはありますか？\n",
    "> \n",
    "> > ## Eisuke Mizutaniトピック作成者\n",
    "> > \n",
    "> > それらのデータセットは公開にしているので、提出時にはアクセスできるはずです。\n",
    "> > \n",
    "> > "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 148.347272,
   "end_time": "2024-07-10T01:15:35.655682",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-10T01:13:07.30841",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0f59addf0d2f40309e025976c382cad8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_268e3946106b4e41849bf11c5a375dac",
       "placeholder": "​",
       "style": "IPY_MODEL_5bb130c471af4927a6644f932ae47523",
       "value": " 2/2 [00:03&lt;00:00,  1.48s/it]"
      }
     },
     "19ef2d43bafa44a8b20dc5230aea5ae4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1ca042ffa14e4dbebdc66435f7b1f07f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_19ef2d43bafa44a8b20dc5230aea5ae4",
       "placeholder": "​",
       "style": "IPY_MODEL_d8a7714cd80d479e859b6ae31ebce7e5",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d03719518b8423099b8b68a92e449d7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_64ef70cfa9c04764868fa52963323322",
       "placeholder": "​",
       "style": "IPY_MODEL_5e81b324ca1b46a2a96d02bb2acadc0a",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1d89705c74d34016bbc1e0601ead825c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d409334237014614bf9ae742597d98ea",
       "placeholder": "​",
       "style": "IPY_MODEL_875758123e4f41f0b5c3fa0cd4fb47c6",
       "value": " 2/2 [01:18&lt;00:00, 34.68s/it]"
      }
     },
     "1ef6d64d40d8461d9e6adddd513089b8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "268e3946106b4e41849bf11c5a375dac": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "324c2396f44f45c89d9ec264007ef9ff": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "5bb130c471af4927a6644f932ae47523": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "5d243712a1a545e99fa858b0cf19831d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5e81b324ca1b46a2a96d02bb2acadc0a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "64ef70cfa9c04764868fa52963323322": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "68ae4b02a13f4570ad729c437ebd28ee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1ca042ffa14e4dbebdc66435f7b1f07f",
        "IPY_MODEL_81b4a9a7cde64b17856b89dbd238c0ef",
        "IPY_MODEL_0f59addf0d2f40309e025976c382cad8"
       ],
       "layout": "IPY_MODEL_9422a87b93ab4473913e601da3a18689"
      }
     },
     "81b4a9a7cde64b17856b89dbd238c0ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_1ef6d64d40d8461d9e6adddd513089b8",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_324c2396f44f45c89d9ec264007ef9ff",
       "value": 2
      }
     },
     "85d217d6b45847869cea506db59e8b42": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_5d243712a1a545e99fa858b0cf19831d",
       "max": 2,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96385fd98f304649ab5c4ae81333fb63",
       "value": 2
      }
     },
     "875758123e4f41f0b5c3fa0cd4fb47c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "9422a87b93ab4473913e601da3a18689": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "96385fd98f304649ab5c4ae81333fb63": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "d409334237014614bf9ae742597d98ea": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d576a283e6424206ab4c25d809241c21": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d8a7714cd80d479e859b6ae31ebce7e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d98918fae8174629b4819a1114f21202": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_1d03719518b8423099b8b68a92e449d7",
        "IPY_MODEL_85d217d6b45847869cea506db59e8b42",
        "IPY_MODEL_1d89705c74d34016bbc1e0601ead825c"
       ],
       "layout": "IPY_MODEL_d576a283e6424206ab4c25d809241c21"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
