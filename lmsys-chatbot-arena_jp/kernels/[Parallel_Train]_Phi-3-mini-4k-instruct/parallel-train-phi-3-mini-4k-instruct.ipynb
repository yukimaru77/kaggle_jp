{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f6a52a",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleコンペティション「LMSYS - Chatbot Arena」における人間による好みの予測を行うための機械学習モデルの開発を目的としています。具体的には、異なる大規模言語モデル（LLM）が生成した応答のうち、ユーザーに最も好まれるものを予測するモデルを設計しています。\n",
    "\n",
    "## 問題の取り組み\n",
    "このNotebookでは、特に以下の課題に取り組んでいます：\n",
    "- LLMの応答の中から、どのモデルがユーザーの好みに合致するかを予測する。\n",
    "- LoRA（Low-Rank Adaptation）を使用したモデルの微調整。\n",
    "- 最大シーケンス長を768に設定し、メモリ制約を考慮する。\n",
    "\n",
    "## 使用手法とライブラリ\n",
    "Notebookは多くのPyTorchとTransformersライブラリのモジュールを活用しています。以下に主な手法とライブラリを示します：\n",
    "\n",
    "- **データ前処理**: サンプリング、欠損値の除去、ラベルの変換などを行っています。データはCSVファイルから読み込まれ、必要に応じてJSON形式に変換されます。\n",
    "- **カスタムデータセット**:\n",
    "  - `CustomDataset`クラスを使用して、トークン化されたプロンプトと応答をデータローダーに組み込みます。\n",
    "  - `create_dataloaders`関数を介して、訓練と評価用のデータローダーを作成します。\n",
    "  \n",
    "- **モデル構築**: \n",
    "  - MicrosoftのPhi-3-mini-4k-instructモデルをベースに、LoRAアダプターを追加したカスタムモデルを定義しています。これにより、少ないパラメータでモデルのパフォーマンスを向上させることができます。\n",
    "  - `Phi3Attention`クラスをカスタマイズして、LoRAアダプターが適用されたマルチヘッダーアテンションを実装しています。\n",
    "\n",
    "- **トレーニングプロセス**:\n",
    "  - 半精度トレーニングを行い、モデルのトレーニングと評価をパラレルに処理します。\n",
    "  - 最適化手法にはAdamWが使用され、学習率スケジューラも設定されています。\n",
    "  - 評価指標として、精度とF1スコアを計算・表示しています。\n",
    "\n",
    "- **量子化**: \n",
    "  - モデルの重みを量子化する関数が実装されており、モデルのサイズを削減し、効率的な推論を行うことが可能です。\n",
    "\n",
    "このNotebookでは、最終的にトレーニングしたモデルを保存し、後で利用できるようにしています。このプロセス全体を通じて、特にユーザーの好みに基づく予測を行うための効果的な機械学習手法を実装しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b08396",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、機械学習・深層学習の初心者がつまずきやすい専門用語を解説します。ノートブック特有のドメイン知識や実務経験が必要な用語に焦点を当てています。\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**:\n",
    "   - LoRAは、深層学習モデルのパラメータを効率的に更新するための手法です。この手法では、重み行列を低ランクで分解し、追加のパラメータを訓練することで少ないリソースでモデルの適応を行います。特に大規模モデルの微調整に利用されます。\n",
    "\n",
    "2. **量子化 (Quantization)**:\n",
    "   - 量子化とは、モデルのパラメータや計算を低ビット数で表現する技術です。これにより、モデルのサイズが小さくなり、メモリや計算資源の消費が削減されます。特にGPUやTPUなどのハードウェアを効果的に使用する際に有用です。\n",
    "\n",
    "3. **Attention Mask**:\n",
    "   - Attention Maskは、自己注意機構がどの入力トークンを考慮するかを示すためのバイナリマスクです。特にパディングのトークンを無視したり、一部のトークンのみを参照したりする場合に使用されます。\n",
    "\n",
    "4. **アダプタレイヤー (Adapter Layer)**:\n",
    "   - アダプタレイヤーは、既存のモデルに対して新しい知識を効率的に追加できるように設計された層です。この層を追加することで、モデル全体を再訓練せずに特定のタスクに特化した機能を持たせることができます。\n",
    "\n",
    "5. **Flash Attention**:\n",
    "   - Flash Attentionは、メモリ使用量を最適化し、自己注意をより効率的に計算するための手法です。特に長いシーケンスを扱うタスクで有用で、計算速度の向上とメモリ効率の改善を実現します。\n",
    "\n",
    "6. **Rotary Positional Embedding (RoPE)**:\n",
    "   - RoPEは、トークン間の位置情報を表現するための手法で、特に長距離依存性を持つシーケンスモデルにおいて有効です。この手法により、位置情報のエンコーディングがより柔軟かつ効率的になります。\n",
    "\n",
    "7. **Dropout**:\n",
    "   - Dropoutは、過学習を防ぐための正則化手法で、トレーニング中にランダムに一定割合のニューロンを無効化しますが、ここでは特にLoRA構造内の使用がソフトウェアエンジニアにとって新しい概念であることが考えられます。\n",
    "\n",
    "8. **Mixed Precision Training**:\n",
    "   - Mixed Precision Trainingは、モデルのトレーニングにおいて異なる精度（例：16ビット浮動小数点数と32ビット浮動小数点数）を混ぜて使用する手法です。この手法は、GPUメモリの使用効率を向上させ、計算速度を速めるために使用されます。\n",
    "\n",
    "これらの用語は、機械学習の特定の側面や、特定のライブラリの機能に関連しており、初学者が遭遇することがよくある重要な概念です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2338cbe",
   "metadata": {},
   "source": [
    "# インストール不要\n",
    "\n",
    "microsoft/Phi-3-mini-4k-instruct + LoRA > GPUでの並列トレーニング\n",
    "\n",
    "最大シーケンス長はモデルのパフォーマンスに大きな影響を与えますが、メモリ不足のため、最大長は768に設定されました。\n",
    "\n",
    "\n",
    "## ライブラリの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:40.963732Z",
     "iopub.status.busy": "2024-07-27T22:50:40.962925Z",
     "iopub.status.idle": "2024-07-27T22:50:40.978385Z",
     "shell.execute_reply": "2024-07-27T22:50:40.977577Z",
     "shell.execute_reply.started": "2024-07-27T22:50:40.963692Z"
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "mp.set_start_method('spawn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:41.221379Z",
     "iopub.status.busy": "2024-07-27T22:50:41.221089Z",
     "iopub.status.idle": "2024-07-27T22:50:56.767411Z",
     "shell.execute_reply": "2024-07-27T22:50:56.766638Z",
     "shell.execute_reply.started": "2024-07-27T22:50:41.221356Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss\n",
    "\n",
    "from accelerate import notebook_launcher, Accelerator, PartialState\n",
    "from accelerate.utils import write_basic_config\n",
    "from accelerate.inference import prepare_pippy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.769675Z",
     "iopub.status.busy": "2024-07-27T22:50:56.769113Z",
     "iopub.status.idle": "2024-07-27T22:50:56.775287Z",
     "shell.execute_reply": "2024-07-27T22:50:56.774322Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.769648Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータ設定\n",
    "model_name = \"/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1\"  # モデルの名前\n",
    "model_path = \"model_checkpoint.pth\"  # モデルのチェックポイントのパス\n",
    "seed = 42  # 乱数シード\n",
    "lora_r = 2  # LoRAのランク\n",
    "quantize_bit = 16  # 量子化ビット数\n",
    "learning_rate = 5e-4  # 学習率\n",
    "weight_decay = 0.1  # 重み減衰\n",
    "beta1 = 0.9  # Adamオプティマイザのβ1\n",
    "beta2 = 0.999  # Adamオプティマイザのβ2\n",
    "eps = 1e-9  # 許容誤差\n",
    "l1_rate = 1e-10  # L1正則化率\n",
    "batch_size = 1  # バッチサイズ\n",
    "max_len = 256  # 最大シーケンス長\n",
    "n_sample = 0.10  # サンプリング比率\n",
    "n_epoch = 2  # エポック数\n",
    "device = \"cuda\"  # 使用するデバイス\n",
    "file_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのファイルパス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08cba3",
   "metadata": {},
   "source": [
    "## 前処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.776658Z",
     "iopub.status.busy": "2024-07-27T22:50:56.776375Z",
     "iopub.status.idle": "2024-07-27T22:50:56.78264Z",
     "shell.execute_reply": "2024-07-27T22:50:56.781862Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.776635Z"
    }
   },
   "outputs": [],
   "source": [
    "def cl(x):\n",
    "  if x == [1,0,0]:\n",
    "    return 0  # モデルAが勝者の場合\n",
    "  elif x == [0,1,0]:\n",
    "    return 1  # モデルBが勝者の場合\n",
    "  else:\n",
    "    return 2  # 引き分けの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.785091Z",
     "iopub.status.busy": "2024-07-27T22:50:56.784682Z",
     "iopub.status.idle": "2024-07-27T22:50:56.798035Z",
     "shell.execute_reply": "2024-07-27T22:50:56.797073Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.785068Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_data(file_path, sample=False):\n",
    "    train = pd.read_csv(file_path)  # 訓練データをCSVから読み込む\n",
    "    clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]\n",
    "\n",
    "    # JSON形式から最初の要素を抽出\n",
    "    clf_train.loc[:, \"prompt\"] = clf_train[\"prompt\"].apply(lambda x: json.loads(x)[0])\n",
    "    clf_train.loc[:, \"response_a\"] = clf_train[\"response_a\"].apply(lambda x: json.loads(x)[0])\n",
    "    clf_train.loc[:, \"response_b\"] = clf_train[\"response_b\"].apply(lambda x: json.loads(x)[0])\n",
    "\n",
    "    clf_train = clf_train.dropna()  # 欠損値を削除\n",
    "    clf_train = clf_train.reset_index(drop=True)  # インデックスをリセット\n",
    "\n",
    "    # 各勝者を含むターゲット列を作成\n",
    "    clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train))]\n",
    "\n",
    "    clf_train = clf_train[['prompt','response_a','response_b','target']]\n",
    "\n",
    "    # ターゲットをラベルに変換\n",
    "    clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))\n",
    "\n",
    "    # 各要素の長さを計算\n",
    "    clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))\n",
    "    clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))\n",
    "    clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))\n",
    "\n",
    "    clf_train['len'] = clf_train['p_len'] + clf_train['a_len'] + clf_train['b_len']\n",
    "    \n",
    "    if sample:  # サンプリングを行う場合\n",
    "        clf_train = clf_train.sample(int(len(clf_train) * n_sample), weights=\"len\", random_state=seed).reset_index(drop=True)\n",
    "\n",
    "    t_dat, v_dat = train_test_split(clf_train, test_size=0.2, random_state=42, stratify=clf_train['labels'])  # データセットを訓練と検証に分割\n",
    "\n",
    "    t_dat = t_dat.reset_index(drop=True)\n",
    "    v_dat = v_dat.reset_index(drop=True)\n",
    "\n",
    "    t_dat = t_dat.drop(labels='target', axis=1)  # 'target'列を削除\n",
    "    v_dat = v_dat.drop(labels='target', axis=1)  # 'target'列を削除\n",
    "    return t_dat, v_dat  # 訓練データと検証データを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.800082Z",
     "iopub.status.busy": "2024-07-27T22:50:56.799429Z",
     "iopub.status.idle": "2024-07-27T22:50:56.813998Z",
     "shell.execute_reply": "2024-07-27T22:50:56.81315Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.800051Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.prompt = df['prompt']\n",
    "        self.response_a = df['response_a']\n",
    "        self.response_b = df['response_b']\n",
    "        self.max_len = max_len\n",
    "        self.targets = df.get('labels', None)  # ラベルを取得\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)  # プロンプトの数を返す\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # インデックスに基づいてプロンプトおよび応答を取得\n",
    "        prompt = str(self.prompt[index])\n",
    "        response_a = str(self.response_a[index])\n",
    "        response_b = str(self.response_b[index])\n",
    "\n",
    "        # 各テキストのトークナイズされた長さを計算\n",
    "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])\n",
    "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])\n",
    "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])\n",
    "\n",
    "        # 最大長を超えないように長さを制限\n",
    "        final_prompt_len = min(self.max_len, prompt_len)\n",
    "        final_a_len = min(self.max_len, response_a_len)\n",
    "        final_b_len = min(self.max_len, response_b_len)\n",
    "\n",
    "        # トークナイズ\n",
    "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "        # 入力IDとアテンションマスクを結合\n",
    "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
    "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            labels = torch.LongTensor([self.targets[index]])\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}  # 入力ID、アテンションマスク、ラベルを返す\n",
    "        else:\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}  # 入力IDとアテンションマスクだけを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.815829Z",
     "iopub.status.busy": "2024-07-27T22:50:56.815186Z",
     "iopub.status.idle": "2024-07-27T22:50:56.825043Z",
     "shell.execute_reply": "2024-07-27T22:50:56.824259Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.815797Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer):\n",
    "\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_masks = [item['attention_mask'] for item in batch]\n",
    "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None\n",
    "\n",
    "    # バッチ内のシーケンスの最大長を見つける\n",
    "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
    "\n",
    "    # 新しい最大長で再トークナイズ\n",
    "    new_input_ids = []\n",
    "    new_attention_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        input_ids = item['input_ids'][:max_len]  # 最大長で切り取る\n",
    "        attention_mask = item['attention_mask'][:max_len]  # 最大長で切り取る\n",
    "\n",
    "        new_input_ids.append(input_ids)  # 新しい入力IDに追加\n",
    "        new_attention_masks.append(attention_mask)  # 新しいアテンションマスクに追加\n",
    "\n",
    "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)  # パディングを適用\n",
    "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)  # パディングを適用\n",
    "\n",
    "    output = {\n",
    "    'input_ids': new_input_ids,\n",
    "    'attention_mask': new_attention_masks}\n",
    "\n",
    "    if labels is not None:\n",
    "        output['labels'] = labels  # ラベルがある場合は追加\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.826275Z",
     "iopub.status.busy": "2024-07-27T22:50:56.826003Z",
     "iopub.status.idle": "2024-07-27T22:50:56.835718Z",
     "shell.execute_reply": "2024-07-27T22:50:56.834859Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.826254Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, max_len, batch_size, shuffle=True):\n",
    "    # データローダーを作成するための関数\n",
    "    dataloader = DataLoader(\n",
    "        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff4b22",
   "metadata": {},
   "source": [
    "## モデルの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.838013Z",
     "iopub.status.busy": "2024-07-27T22:50:56.837391Z",
     "iopub.status.idle": "2024-07-27T22:50:56.846877Z",
     "shell.execute_reply": "2024-07-27T22:50:56.845932Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.837983Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, num_bits=quantize_bit):\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.  # 量子化の最大値を計算\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()  # テンソルの最小値と最大値を取得\n",
    "    scale = (max_val - min_val) / (qmax - qmin)  # スケールを計算\n",
    "    zero_point = qmin - min_val / scale  # ゼロポイントを計算\n",
    "\n",
    "    quantized_tensor = torch.round(tensor / scale + zero_point)  # テンソルを量子化\n",
    "    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)  # 最大・最小値でクリップ\n",
    "    quantized_tensor = (quantized_tensor - zero_point) * scale  # 再スケーリング\n",
    "\n",
    "    return quantized_tensor\n",
    "\n",
    "def quantize_model(model, num_bits=8):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):  # 線形層の場合\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化\n",
    "        elif isinstance(module, nn.Conv2d):  # 畳み込み層の場合\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# import torch.quantization\n",
    "\n",
    "# def quantize_model_dynamic(model):\n",
    "#     model.qconfig = torch.quantization.default_dynamic_qconfig\n",
    "#     torch.quantization.prepare(model, inplace=True)\n",
    "#     torch.quantization.convert(model, inplace=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90398fff",
   "metadata": {},
   "source": [
    "## アダプタレイヤーの追加\n",
    "transformers.models.phi3.modeling_phi3.Phi3Attentionからコピー\n",
    "\n",
    "[GitHubのURL](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.848239Z",
     "iopub.status.busy": "2024-07-27T22:50:56.847937Z",
     "iopub.status.idle": "2024-07-27T22:50:56.856779Z",
     "shell.execute_reply": "2024-07-27T22:50:56.85601Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.848216Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout=0.05):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.alpha = alpha  # Alphaパラメータ\n",
    "        self.rank = rank  # LoRAのランク\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)  # LoRA A行列\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)  # LoRA B行列\n",
    "        self.dropout = nn.Dropout(lora_dropout)  # ドロップアウト層\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_out = self.alpha * self.lora_b(self.lora_a(x))  # LoRAの出力を計算\n",
    "        lora_out = self.dropout(lora_out)  # ドロップアウトを適用\n",
    "        return lora_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.86011Z",
     "iopub.status.busy": "2024-07-27T22:50:56.859835Z",
     "iopub.status.idle": "2024-07-27T22:50:56.915738Z",
     "shell.execute_reply": "2024-07-27T22:50:56.914895Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.860088Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.phi3.modeling_phi3 import (\n",
    "Phi3RotaryEmbedding, \n",
    "# Phi3LongRoPEScaledRotaryEmbedding,\n",
    "apply_rotary_pos_emb,\n",
    "repeat_kv\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.917012Z",
     "iopub.status.busy": "2024-07-27T22:50:56.916766Z",
     "iopub.status.idle": "2024-07-27T22:50:56.943771Z",
     "shell.execute_reply": "2024-07-27T22:50:56.942785Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.916991Z"
    }
   },
   "outputs": [],
   "source": [
    "class Phi3Attention(nn.Module):\n",
    "    \"\"\" 'Attention Is All You Need' 論文に基づくマルチヘッダーアテンション \"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"{self.__class__.__name__} を layer_idx を指定せずにインスタンス化するのは推奨されず、\"\n",
    "                \"キャッシュを使用している場合にフォワード呼び出し時にエラーが発生する可能性があります。\"\n",
    "                \"このクラスを作成する際は、layer_idx を指定してください。\"\n",
    "            )\n",
    "\n",
    "        # 各種設定を初期化\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.original_max_position_embeddings = config.original_max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.rope_scaling = config.rope_scaling\n",
    "        self.is_causal = True\n",
    "\n",
    "        # hidden_size が num_heads で割り切れない場合にエラーを発生させる\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size は num_heads で割り切れる必要があります (得られた `hidden_size`: {self.hidden_size}\"\n",
    "                f\" と `num_heads`: {self.num_heads})。\"\n",
    "            )\n",
    "\n",
    "        # 線形変換のためのサイズを計算\n",
    "        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)\n",
    "        self._init_rope()\n",
    "        \n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)  # LoRAアダプタの初期化\n",
    "        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)  # LoRAアダプタの初期化\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        \n",
    "    def _init_rope(self):\n",
    "        if self.rope_scaling is None:\n",
    "            self.rotary_emb = Phi3RotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            if scaling_type == \"longrope\":\n",
    "                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)\n",
    "            else:\n",
    "                raise ValueError(f\"未知の RoPE スケーリングタイプ {scaling_type}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value=None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # logger.warning_once(\"フラッシュアテンションの実装を実行していないため、数値的な差異が予想されます。\")\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)  # LoRAアダプタを適用\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        query_pos = self.num_heads * self.head_dim\n",
    "        query_states = qkv[..., :query_pos]\n",
    "        key_states = qkv[..., query_pos:query_pos + self.num_key_value_heads * self.head_dim]\n",
    "        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim:]\n",
    "\n",
    "        # 各状態の形状を整形\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            if self.layer_idx is None:\n",
    "                raise ValueError(\n",
    "                    f\"キャッシュ構造はバージョンv4.36以降変更されました。{self.__class__.__name__} を使用して自動回帰デコーディングを行っている場合、\"\n",
    "                    \"レイヤーインデックスで初期化されていることを確認してください。\"\n",
    "                )\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # RoPEモデル専用の引数\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        # k/v ヘッドが n_kv_heads < n_heads の場合は重複\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights は {(bsz, self.num_heads, q_len, kv_seq_len)} サイズであるべきですが、\"\n",
    "                f\" {attn_weights.size()} です。\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n",
    "            attn_weights += causal_mask\n",
    "\n",
    "        # atención を fp32 にアップキャスト\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` は {(bsz, self.num_heads, q_len, self.head_dim)} サイズであるべきですが、\"\n",
    "                f\" {attn_output.size()} です。\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)  # LoRAを適用\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.945391Z",
     "iopub.status.busy": "2024-07-27T22:50:56.945023Z",
     "iopub.status.idle": "2024-07-27T22:50:56.953323Z",
     "shell.execute_reply": "2024-07-27T22:50:56.952431Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.945338Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_attention_module(config, layer, layer_idx):\n",
    "    if hasattr(layer, 'self_attn') and layer_idx > 12:\n",
    "\n",
    "        new_attention = Phi3Attention(config, layer_idx)  # 新しいPhi3Attentionを作成\n",
    "\n",
    "        # 重みをコピー\n",
    "        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)\n",
    "        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)\n",
    "\n",
    "        layer.self_attn = new_attention  # 置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.955186Z",
     "iopub.status.busy": "2024-07-27T22:50:56.954567Z",
     "iopub.status.idle": "2024-07-27T22:50:56.962953Z",
     "shell.execute_reply": "2024-07-27T22:50:56.962104Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.955157Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # クロスエントロピー損失関数の初期化\n",
    "\n",
    "class LoraModelForClassification(nn.Module):\n",
    "    def __init__(self, lora_model):  # LoRAモデルを受け取る\n",
    "        super(LoraModelForClassification, self).__init__()\n",
    "        self.config = lora_model.config  # 設定を保存\n",
    "        self.peft_model = lora_model  # LoRAモデルを保存\n",
    "        self.dropout = nn.Dropout(0.1)  # ドロップアウト層\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 3)  # 最終分類層の初期化\n",
    "#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)  # 重みのデータ型を変更\n",
    "#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)  # バイアスのデータ型を変更\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.peft_model(input_ids, attention_mask=attention_mask)  # モデルからの出力\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)  # 出力の平均値を計算\n",
    "        output_dropout = self.dropout(pooled_output)  # ドロップアウトを適用\n",
    "        logits = self.classifier(output_dropout)  # 最終的なロジットを取得\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels\n",
    "            loss = loss_fn(logits, labels)  # 損失を計算\n",
    "        return loss, logits  # 損失とロジットを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a43a6",
   "metadata": {},
   "source": [
    "## パラレルトレーニング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.964449Z",
     "iopub.status.busy": "2024-07-27T22:50:56.964151Z",
     "iopub.status.idle": "2024-07-27T22:50:56.984294Z",
     "shell.execute_reply": "2024-07-27T22:50:56.983454Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.964426Z"
    }
   },
   "outputs": [],
   "source": [
    "def parallel_function(model_name, attention_name, file_path):\n",
    "    mp.set_start_method('spawn', force=True)  # プロセスの開始方法を設定\n",
    "\n",
    "    accelerator = Accelerator(mixed_precision=\"fp16\")  # 半精度トレーニングを使用\n",
    "    if accelerator.is_main_process:  # メインプロセスの場合\n",
    "        datasets.utils.logging.set_verbosity_warning()  # 警告のログレベルを設定\n",
    "        transformers.utils.logging.set_verbosity_info()  # 情報のログレベルを設定\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()  # エラーログレベルを設定\n",
    "        transformers.utils.logging.set_verbosity_error()  # エラーログレベルを設定\n",
    "\n",
    "    set_seed(seed)  # 乱数シードを設定\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)  # トークナイザを読み込む\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを設定\n",
    "    tokenizer.padding_side = \"right\"  # fp16トレーニングの問題を修正\n",
    "    \n",
    "    model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)  # モデルを読み込み\n",
    "    model = quantize_model(model)  # モデルを量子化\n",
    "    for idx, layer in enumerate(model.layers):\n",
    "        replace_attention_module(model.config, layer, idx)  # アテンションモジュールを置き換え\n",
    "    model = LoraModelForClassification(model)  # LoRAモデルを初期化\n",
    "\n",
    "    # 学習可能なパラメータの設定\n",
    "    for param in model.peft_model.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in model.classifier.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # LoRAの重みを学習可能にする\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, attention_name):\n",
    "            module.qkv_lora.lora_a.weight.requires_grad = True  # LoRA A層の重み\n",
    "            module.qkv_lora.lora_b.weight.requires_grad = True  # LoRA B層の重み\n",
    "            module.o_lora.lora_a.weight.requires_grad = True  # 出力LoRA A層の重み\n",
    "            module.o_lora.lora_b.weight.requires_grad = True  # 出力LoRA B層の重み\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"総学習可能パラメータ数: {total_params}\")  # 学習可能パラメータ数を表示\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  # オプティマイザの初期化\n",
    "\n",
    "    # データを前処理し、データローダーを作成\n",
    "    t_dat, v_dat = preprocess_data(file_path, sample=True)\n",
    "    train_dataloader = create_dataloaders(t_dat, tokenizer, max_len, batch_size=batch_size, shuffle=True)\n",
    "    eval_dataloader = create_dataloaders(v_dat, tokenizer, max_len, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # モデルとオプティマイザを準備\n",
    "    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "        model, optimizer, train_dataloader, eval_dataloader)\n",
    "\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(  # 学習率スケジューラを設定\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=100,\n",
    "        num_training_steps=len(train_dataloader) * n_epoch,\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(n_epoch * len(train_dataloader)), disable=not accelerator.is_main_process)  # プログレスバーの初期化\n",
    "\n",
    "    # トレーニングと評価のループ\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        model.train()  # モデルをトレーニングモードに設定\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            loss, _ = model(**batch)  # バッチをモデルに渡して損失を計算\n",
    "            accelerator.backward(loss)  # バックプロパゲーション\n",
    "\n",
    "            optimizer.step()  # オプティマイザのステップ\n",
    "            lr_scheduler.step()  # 学習率スケジューラのステップ\n",
    "            optimizer.zero_grad()  # 勾配をゼロにリセット\n",
    "            progress_bar.update(1)  # プログレスバーを更新\n",
    "\n",
    "            train_loss += loss.item()  # トレーニングロスを累積\n",
    "            \n",
    "        all_predictions = []\n",
    "        all_labels = []\n",
    "        model.eval()  # モデルを評価モードに設定\n",
    "        for step, batch in enumerate(eval_dataloader):\n",
    "            with torch.no_grad():\n",
    "                loss, logits = model(**batch)  # 評価バッチに対する損失とロジットを計算\n",
    "            predictions = logits.argmax(dim=-1)  # 最大のロジットを予測として抽出\n",
    "            all_predictions.append(accelerator.gather(predictions))  # 予測を収集\n",
    "            all_labels.append(accelerator.gather(batch[\"labels\"]))  # ラベルを収集\n",
    "            \n",
    "            valid_loss += loss.item()  # 検証ロスを累積\n",
    "\n",
    "        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()  # すべての予測を結合\n",
    "        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()  # すべてのラベルを結合\n",
    "\n",
    "        acc_metric = accuracy_score(all_labels, all_predictions)  # 精度を計算\n",
    "        eval_metric = f1_score(all_labels, all_predictions, average=\"macro\")  # F1スコアを計算\n",
    "        train_loss_avg = train_loss / len(train_dataloader)  # トレーニングロスの平均を計算\n",
    "        valid_loss_avg = valid_loss / len(eval_dataloader)  # 検証ロスの平均を計算\n",
    "        \n",
    "        # 結果を表示\n",
    "        accelerator.print(f\"エポック: {epoch} \\n 精度: {acc_metric:.3f} \\n F1スコア: {eval_metric:.3f} \\n トレーニングロス: {train_loss_avg:.3f} \\n 検証ロス: {valid_loss_avg:.3f}\")\n",
    "\n",
    "    model = accelerator.unwrap_model(model)  # モデルのラッピングを解除\n",
    "    accelerator.wait_for_everyone()  # すべてのプロセスが完了するのを待つ\n",
    "\n",
    "    # モデルの状態を保存\n",
    "    if accelerator.is_main_process:\n",
    "        torch.save(model.state_dict(), model_path)  # モデルの状態を保存\n",
    "\n",
    "    # 同期完了のメッセージ\n",
    "    accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-27T22:50:56.985884Z",
     "iopub.status.busy": "2024-07-27T22:50:56.985531Z",
     "iopub.status.idle": "2024-07-27T22:55:49.631987Z",
     "shell.execute_reply": "2024-07-27T22:55:49.63091Z",
     "shell.execute_reply.started": "2024-07-27T22:50:56.985855Z"
    }
   },
   "outputs": [],
   "source": [
    "notebook_launcher(parallel_function, args=(model_name, Phi3Attention, file_path,), num_processes=2)  # パラレルファンクションを起動"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 95068,
     "modelInstanceId": 69944,
     "sourceId": 83272,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
