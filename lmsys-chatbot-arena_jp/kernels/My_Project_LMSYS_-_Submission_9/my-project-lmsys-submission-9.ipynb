{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "955be20d",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、「LMSYS - Chatbot Arena」コンペティションにおける不均衡データを扱うためのSMOTE（Synthetic Minority Over-sampling Technique）を適用し、機械学習モデルの訓練と推論プロセスを実施するものです。以下に、特に重要な側面を要約します。\n",
    "\n",
    "### 問題の内容\n",
    "- データセットには、モデルA、モデルB、引き分けの勝者を示すカラム（`winner_model_a`, `winner_model_b`, `winner_tie`）があり、これらのクラスには不均衡が存在します。この不均衡は、モデルの学習中にバイアスを引き起こし、特に少数派クラスに対する予測精度を低下させる恐れがあります。\n",
    "\n",
    "### 解決手法\n",
    "- SMOTE手法を利用して、トレーニングデータの不均衡を解消するため、合成データを生成し、全体のデータセットをリサンプリングします。これにより、各クラスに対するデータポイントの数を均等化します。\n",
    "\n",
    "### ライブラリおよびツール\n",
    "- **Pandas**: データの読み込み、結合、処理に使用。\n",
    "- **NumPy**: 数値計算に使用。\n",
    "- **imblearn**: SMOTEの実装を利用し、不均衡データの処理を行う。\n",
    "- **Transformers**: `Gemma2ForSequenceClassification`モデルを使用して、応答を分類するためのモデル構築とトレーニングを行うために必要な機能を提供。\n",
    "- **torch**: ディープラーニングモデルのトレーニングに使用。\n",
    "- **datasets**: データセットの扱いや読み込みを容易にするライブラリ。\n",
    "- **sklearn**: モデルの評価メトリック（log_loss, accuracy_score）を計算。\n",
    "\n",
    "### プロセスの概要\n",
    "1. トレーニングデータとテストデータを読み込む。\n",
    "2. SMOTEを用いて不均衡なデータセットをリサンプリングする。\n",
    "3. 読み込んだデータセットに対し、Gemmaモデルなどの事前学習済みのトランスフォーマモデルを設定し、トレーニング用のパラメータを構成する。\n",
    "4. データをトークン化し、分類タスク用にモデルを訓練する。\n",
    "5. テストデータに対して推論を行い、結果を整形して提出用CSVファイルとして保存する。\n",
    "\n",
    "このノートブックは、機械学習モデルの不均衡データに対処するための重要な手法を示す良い例であり、最終的な提出ファイルでは、各モデルの勝者確率が求められています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d99f2e9",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきやすい専門用語の解説を示します。この解説は、実務経験のない方が特に馴染みのないかもしれない用語や、特定のノートブックに関連するドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **SMOTE (合成少数オーバーサンプリング技術)**:\n",
    "   - 不均衡データに対処するために使われる手法で、少数派クラスのデータポイントを生成します。既存の少数派サンプル間の距離に基づいて合成サンプルを作成し、モデルがより多くの学習を行えるようにします。\n",
    "\n",
    "2. **クラス不均衡**:\n",
    "   - 機械学習において、あるクラスのデータポイントが他のクラスに比べて著しく少ない状態を指します。この状況は、モデルの性能に偏りをもたらし、少数派クラスの識別が難しくなることがあります。\n",
    "\n",
    "3. **データコラトレーター (Data Collator)**:\n",
    "   - モデルにデータを供給する際に使用されるもので、バッチ処理を行う際にデータを適切に整形する役割を果たします。異なる長さのシーケンスを適切にパディングしてから供給することが一般的です。\n",
    "\n",
    "4. **LoRA (Low-Rank Adaptation)**:\n",
    "   - 大規模なモデルのファインチューニングを効率的に行うための手法です。主にモデルの一部の層のみに変更を加えることで、全体のパラメータ数を大幅に減らしつつ性能を維持します。\n",
    "\n",
    "5. **グラデーション蓄積 (Gradient Accumulation)**:\n",
    "   - メモリ不足を回避するために、複数のバッチのグラデーションを蓄積してからモデルのパラメータを更新します。このプロセスにより、実際のバッチサイズをサポートして高性能を維持することができます。\n",
    "\n",
    "6. **トークナイザー**:\n",
    "   - テキストデータを数値（トークン）に変換するためのツールです。自然言語処理のモデルは、文字列を扱うことができないため、トークナイザーを用いて前処理を行い、数値的な形式に変換します。\n",
    "\n",
    "7. **アテンションマスク (Attention Mask)**:\n",
    "   - 注意機構を持つモデルで使用される、モデルが各トークンにどれだけ注意を払うかを示すマスクです。パディングされた部分を無視するために使用され、効率的な個別トークン処理を助けます。\n",
    "\n",
    "8. **オプティマイザ (Optimizer)**:\n",
    "   - 学習アルゴリズムにおいて、モデルのパラメータを更新して損失を最小化するための手法です。異なるオプティマイザにはそれぞれ異なる特徴があり、学習の収束速度や結果に影響を与えます。\n",
    "\n",
    "9. **評価戦略 (Evaluation Strategy)**:\n",
    "   - モデルの評価をどのように行うかを決める方針。例えば、エポックごとに評価を行う「epoch」戦略や、指定した間隔（保存ステップ）で評価を行う方法があります。\n",
    "\n",
    "10. **トレーニング引数 (Training Arguments)**:\n",
    "    - モデルの訓練時に使用される設定の集合で、エポック数、バッチサイズ、学習率など、訓練プロセスを調整するパラメータを含みます。\n",
    "\n",
    "これらの用語は、初心者が機械学習や深層学習のプロセスを理解する上で重要です。特に、実務経験がない場合には、これらの概念が具体的にどのように関連するのかを把握することがキーとなります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa265e46",
   "metadata": {},
   "source": [
    "# トレインcsvの不均衡データのためのSMOTEを作成する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:31.438732Z",
     "iopub.status.busy": "2024-07-23T12:09:31.438382Z",
     "iopub.status.idle": "2024-07-23T12:09:37.112563Z",
     "shell.execute_reply": "2024-07-23T12:09:37.111523Z",
     "shell.execute_reply.started": "2024-07-23T12:09:31.438703Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# トレインデータとテストデータ、サンプル提出ファイルを読み込む\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレインデータの読み込み\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')    # テストデータの読み込み\n",
    "sample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルの読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08ddca8",
   "metadata": {},
   "source": [
    "Kaggleの[このリンク](https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning?scriptVersionId=187770530)から手法をコピーしましたが、自分のバランスの取れたデータとgemmaモデルを使用して変更しました。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.115204Z",
     "iopub.status.busy": "2024-07-23T12:09:37.114783Z",
     "iopub.status.idle": "2024-07-23T12:09:37.157922Z",
     "shell.execute_reply": "2024-07-23T12:09:37.156859Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.115156Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# トレインデータの最初の51行を表示する\n",
    "train.head(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.159315Z",
     "iopub.status.busy": "2024-07-23T12:09:37.159036Z",
     "iopub.status.idle": "2024-07-23T12:09:37.171021Z",
     "shell.execute_reply": "2024-07-23T12:09:37.170067Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.159291Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# テストデータの最初の2行を表示する\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.172583Z",
     "iopub.status.busy": "2024-07-23T12:09:37.172204Z",
     "iopub.status.idle": "2024-07-23T12:09:37.19172Z",
     "shell.execute_reply": "2024-07-23T12:09:37.190775Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.172558Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# データセットを結合する\n",
    "combined_df = pd.concat([train, test], ignore_index=True)  # トレインデータとテストデータを結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:37.194727Z",
     "iopub.status.busy": "2024-07-23T12:09:37.19445Z",
     "iopub.status.idle": "2024-07-23T12:09:39.575687Z",
     "shell.execute_reply": "2024-07-23T12:09:39.574742Z",
     "shell.execute_reply.started": "2024-07-23T12:09:37.194703Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ターゲットカラムの欠損値を0で埋める（欠損値は勝利しなかったとみなす）\n",
    "combined_df['winner_model_a'] = combined_df['winner_model_a'].fillna(0)\n",
    "combined_df['winner_model_b'] = combined_df['winner_model_b'].fillna(0)\n",
    "combined_df['winner_tie'] = combined_df['winner_tie'].fillna(0)\n",
    "\n",
    "# SMOTEのためにターゲットカラムを一つのカラムに結合する\n",
    "combined_df['winner'] = combined_df[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)\n",
    "\n",
    "# winnerカラムを数値にマッピングする\n",
    "winner_mapping = {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}\n",
    "combined_df['winner'] = combined_df['winner'].map(winner_mapping)\n",
    "\n",
    "# 結合データセットにSMOTEを適用する\n",
    "smote = SMOTE()\n",
    "X = combined_df.drop(columns=['winner', 'winner_model_a', 'winner_model_b', 'winner_tie'])  # 特徴量を取り出す\n",
    "y = combined_df['winner']  # ターゲットラベルを取り出す\n",
    "\n",
    "# 簡単のために、テキストデータを単純な数値エンコーディングでエンコードする\n",
    "X_encoded = X.apply(lambda col: col.astype('category').cat.codes if col.dtype == 'object' else col)\n",
    "\n",
    "# SMOTEを適用する\n",
    "X_resampled, y_resampled = smote.fit_resample(X_encoded, y)\n",
    "\n",
    "# リサンプリングされたデータからデータフレームを作成する\n",
    "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "resampled_df['winner'] = y_resampled\n",
    "\n",
    "# winnerカラムを元の形に戻す\n",
    "inverse_winner_mapping = {v: k for k, v in winner_mapping.items()}\n",
    "resampled_df['winner'] = resampled_df['winner'].map(inverse_winner_mapping)\n",
    "\n",
    "# winnerカラムを元の3つのカラムに分割する\n",
    "resampled_df['winner_model_a'] = (resampled_df['winner'] == 'winner_model_a').astype(int)\n",
    "resampled_df['winner_model_b'] = (resampled_df['winner'] == 'winner_model_b').astype(int)\n",
    "resampled_df['winner_tie'] = (resampled_df['winner'] == 'winner_tie').astype(int)\n",
    "\n",
    "# 結合されたwinnerカラムを削除する\n",
    "resampled_df = resampled_df.drop(columns=['winner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.577426Z",
     "iopub.status.busy": "2024-07-23T12:09:39.576973Z",
     "iopub.status.idle": "2024-07-23T12:09:39.591628Z",
     "shell.execute_reply": "2024-07-23T12:09:39.590631Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.577398Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# リサンプリングされたデータの最初の10行を表示する\n",
    "resampled_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.593395Z",
     "iopub.status.busy": "2024-07-23T12:09:39.593071Z",
     "iopub.status.idle": "2024-07-23T12:09:39.683427Z",
     "shell.execute_reply": "2024-07-23T12:09:39.682529Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.593368Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 欠損値を最頻値で埋める\n",
    "combined_df['model_a'].fillna(combined_df['model_a'].mode()[0], inplace=True)\n",
    "combined_df['model_b'].fillna(combined_df['model_b'].mode()[0], inplace=True)\n",
    "combined_df['winner_model_a'].fillna(combined_df['winner_model_a'].mode()[0], inplace=True)\n",
    "combined_df['winner_model_b'].fillna(combined_df['winner_model_b'].mode()[0], inplace=True)\n",
    "combined_df['winner_tie'].fillna(combined_df['winner_tie'].mode()[0], inplace=True)\n",
    "\n",
    "# 結合データフレーム内の欠損値をチェックする\n",
    "missing_values = combined_df.isnull().sum()\n",
    "\n",
    "# 欠損値を表示する\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:39.684964Z",
     "iopub.status.busy": "2024-07-23T12:09:39.684687Z",
     "iopub.status.idle": "2024-07-23T12:09:47.407961Z",
     "shell.execute_reply": "2024-07-23T12:09:47.407106Z",
     "shell.execute_reply.started": "2024-07-23T12:09:39.684941Z"
    }
   },
   "outputs": [],
   "source": [
    "# 結合データフレームを新しいCSVファイルに保存する\n",
    "combined_df.to_csv('combined_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aeeaac1",
   "metadata": {},
   "source": [
    "# なぜ不均衡（SMOTE）手法を作成したのか？\n",
    "\n",
    "データセットはクラス不均衡に対処する必要があります。なぜなら、クラス不均衡なデータセットはモデル性能にバイアスをもたらす可能性があるからです。不均衡なデータセットで機械学習モデルをトレーニングすると、モデルは多数派クラスに偏りがちになり、少数派クラスへの性能が低下します。これは、モデルが新しい未学習データに対して一般化する能力に大きな影響を与える可能性があります。\n",
    "\n",
    "このデータセットでは、勝者に関連するカラム（winner_model_a、winner_model_b、winner_tie）が不均衡です。以下は、問題の内訳です：\n",
    "\n",
    "winner_model_a: このカラムは、モデルAが勝ったかどうかを示します。1のカウント（モデルAの勝利を示す）が他の勝者カラムと比較して極端に低いまたは高い場合、不均衡を生じます。\n",
    "\n",
    "winner_model_b: このカラムは、モデルBが勝ったかどうかを示します。同様に、このカラムの1のカウントが他のカラムに対して不均衡である場合、問題を引き起こします。\n",
    "\n",
    "winner_tie: このカラムは、結果が引き分けかどうかを示します。ここでの1の数が他の2つと比較して有意に少ない場合、不均衡が強調されます。\n",
    "\n",
    "初期分析から、winner_tieカラムが他のカラムに比べて少ないインスタンスを持っていることが判明し、引き分けの発生頻度がモデルAまたはモデルBの勝利に対して不均衡であることを示しています。この不均衡は、モデルの学習プロセスを歪め、引き分けを予測する能力を低下させます。この不均衡をSMOTE（合成少数オーバーサンプリング技術）などの手法を使って解決することで、モデルがすべての可能な結果に対してよりバランスの取れた見方ができ、全体的なパフォーマンスが向上します。\n",
    "\n",
    "# Gemmaモデルの使用\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:09:47.40936Z",
     "iopub.status.busy": "2024-07-23T12:09:47.409067Z",
     "iopub.status.idle": "2024-07-23T12:10:23.562038Z",
     "shell.execute_reply": "2024-07-23T12:10:23.558604Z",
     "shell.execute_reply.started": "2024-07-23T12:09:47.409336Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# gemma-2はtransformers>=4.42.3から利用可能です\n",
    "#!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:23.564219Z",
     "iopub.status.busy": "2024-07-23T12:10:23.563809Z",
     "iopub.status.idle": "2024-07-23T12:10:52.13093Z",
     "shell.execute_reply": "2024-07-23T12:10:52.129568Z",
     "shell.execute_reply.started": "2024-07-23T12:10:23.564178Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7f5d21",
   "metadata": {},
   "source": [
    "# 設定\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.13298Z",
     "iopub.status.busy": "2024-07-23T12:10:52.132314Z",
     "iopub.status.idle": "2024-07-23T12:10:52.142939Z",
     "shell.execute_reply": "2024-07-23T12:10:52.141938Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.132954Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"  # 出力ディレクトリ\n",
    "    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4ビット量子化されたgemma-2-9b-instructモデル\n",
    "    max_length: int = 1024  # 最大シーケンス長\n",
    "    n_splits: int = 5  # 分割数\n",
    "    fold_idx: int = 0  # フォールドインデックス\n",
    "    optim_type: str = \"adamw_8bit\"  # オプティマイザのタイプ\n",
    "    per_device_train_batch_size: int = 2  # デバイスあたりのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8\n",
    "    per_device_eval_batch_size: int = 8  # デバイスあたりの評価バッチサイズ\n",
    "    n_epochs: int = 1  # エポック数\n",
    "    freeze_layers: int = 16  # 合計42層中最初の16層にはアダプターを追加しない\n",
    "    lr: float = 2e-4  # 学習率\n",
    "    warmup_steps: int = 20  # ウォームアップステップ数\n",
    "    lora_r: int = 16  # LoRAのレイヤー数\n",
    "    lora_alpha: float = lora_r * 2  # LoRAのアルファ値\n",
    "    lora_dropout: float = 0.05  # LoRAのドロップアウト値\n",
    "    lora_bias: str = \"none\"  # LoRAのバイアス設定\n",
    "    \n",
    "config = Config()  # Configクラスのインスタンスを生成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7474725",
   "metadata": {},
   "source": [
    "# トレーニング引数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.144638Z",
     "iopub.status.busy": "2024-07-23T12:10:52.144254Z",
     "iopub.status.idle": "2024-07-23T12:10:52.220815Z",
     "shell.execute_reply": "2024-07-23T12:10:52.219891Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.144607Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書きする\n",
    "    report_to=\"none\",  # ロギングの設定\n",
    "    num_train_epochs=config.n_epochs,  # エポック数\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスあたりのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,  # グラデーション蓄積ステップ数\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスあたりの評価バッチサイズ\n",
    "    logging_steps=10,  # ロギングステップ\n",
    "    eval_strategy=\"epoch\",  # 評価戦略\n",
    "    save_strategy=\"steps\",  # 保存戦略\n",
    "    save_steps=200,  # 保存ステップ\n",
    "    optim=config.optim_type,  # オプティマイザのタイプ\n",
    "    fp16=True,  # 16ビット浮動小数点を使用する\n",
    "    learning_rate=config.lr,  # 学習率\n",
    "    warmup_steps=config.warmup_steps,  # ウォームアップステップ数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dbf0ed",
   "metadata": {},
   "source": [
    "# LoRA設定\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.222318Z",
     "iopub.status.busy": "2024-07-23T12:10:52.222023Z",
     "iopub.status.idle": "2024-07-23T12:10:52.228669Z",
     "shell.execute_reply": "2024-07-23T12:10:52.227538Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.222294Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,  # LoRAのレイヤー数\n",
    "    lora_alpha=config.lora_alpha,  # LoRAのアルファ値\n",
    "    # 自己注意層をターゲットにする\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],  # 変換する層の定義\n",
    "    lora_dropout=config.lora_dropout,  # LoRAのドロップアウト値\n",
    "    bias=config.lora_bias,  # LoRAのバイアス設定\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクタイプをシーケンス分類にする\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5435e88",
   "metadata": {},
   "source": [
    "# トークナイザーとモデルのインスタンス化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:52.233974Z",
     "iopub.status.busy": "2024-07-23T12:10:52.233669Z",
     "iopub.status.idle": "2024-07-23T12:10:54.035041Z",
     "shell.execute_reply": "2024-07-23T12:10:54.033927Z",
     "shell.execute_reply.started": "2024-07-23T12:10:52.233949Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)  # 事前学習済みのトークナイザーを読み込む\n",
    "tokenizer.add_eos_token = True  # テキストの末尾に<eos>トークンを追加\n",
    "tokenizer.padding_side = \"right\"  # パディングを右側に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:10:54.036892Z",
     "iopub.status.busy": "2024-07-23T12:10:54.036481Z",
     "iopub.status.idle": "2024-07-23T12:11:35.437885Z",
     "shell.execute_reply": "2024-07-23T12:11:35.436859Z",
     "shell.execute_reply.started": "2024-07-23T12:10:54.036854Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=3,  # ラベル数を3に設定\n",
    "    torch_dtype=torch.float16,  # 16ビット浮動小数点を使用\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを使用しない設定\n",
    "model = prepare_model_for_kbit_training(model)  # kビットトレーニング用のモデルを準備\n",
    "model = get_peft_model(model, lora_config)  # LoRAモデルを取得\n",
    "model  # モデルを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:35.439329Z",
     "iopub.status.busy": "2024-07-23T12:11:35.439006Z",
     "iopub.status.idle": "2024-07-23T12:11:35.451467Z",
     "shell.execute_reply": "2024-07-23T12:11:35.450523Z",
     "shell.execute_reply.started": "2024-07-23T12:11:35.439301Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "model.print_trainable_parameters()  # 学習可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b38d6a8",
   "metadata": {},
   "source": [
    "# トレインファイルの読み込み\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:35.452941Z",
     "iopub.status.busy": "2024-07-23T12:11:35.452621Z",
     "iopub.status.idle": "2024-07-23T12:11:39.93426Z",
     "shell.execute_reply": "2024-07-23T12:11:39.933292Z",
     "shell.execute_reply.started": "2024-07-23T12:11:35.452911Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "ds = Dataset.from_csv(\"/kaggle/working/combined_df.csv\")  # CSVファイルからデータセットを作成\n",
    "ds = ds.select(torch.arange(100))  # デモ用に最初の100データを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:39.935749Z",
     "iopub.status.busy": "2024-07-23T12:11:39.935455Z",
     "iopub.status.idle": "2024-07-23T12:11:39.945941Z",
     "shell.execute_reply": "2024-07-23T12:11:39.944964Z",
     "shell.execute_reply.started": "2024-07-23T12:11:39.935724Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer  # トークナイザーを初期化\n",
    "        self.max_length = max_length  # 最大長を設定\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        # プロンプトを生成\n",
    "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]\n",
    "        # レスポンスAを生成\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]\n",
    "        # レスポンスBを生成\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]\n",
    "        # テキストを結合\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        # トークン化\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)\n",
    "        labels = []\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "            # 勝者に応じたラベルを作成\n",
    "            if a_win:\n",
    "                label = 0\n",
    "            elif b_win:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 2\n",
    "            labels.append(label)\n",
    "        return {**tokenized, \"labels\": labels}  # トークン化されたデータとラベルを返す\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))  # テキストを処理して結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:39.947511Z",
     "iopub.status.busy": "2024-07-23T12:11:39.94713Z",
     "iopub.status.idle": "2024-07-23T12:11:40.964043Z",
     "shell.execute_reply": "2024-07-23T12:11:40.96298Z",
     "shell.execute_reply.started": "2024-07-23T12:11:39.94748Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化\n",
    "ds = ds.map(encode, batched=True)  # データセットにマッピングする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c5aef9",
   "metadata": {},
   "source": [
    "# メトリックloglossを計算する\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.965804Z",
     "iopub.status.busy": "2024-07-23T12:11:40.965506Z",
     "iopub.status.idle": "2024-07-23T12:11:40.975126Z",
     "shell.execute_reply": "2024-07-23T12:11:40.974369Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.965779Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # 予測結果を取得\n",
    "    labels = eval_preds.label_ids  # ラベルを取得\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # ソフトマックスで確率に変換\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)  # loglossを計算\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算\n",
    "    return {\"acc\": acc, \"log_loss\": loss}  # 精度とloglossを辞書形式で返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.976878Z",
     "iopub.status.busy": "2024-07-23T12:11:40.976233Z",
     "iopub.status.idle": "2024-07-23T12:11:40.993606Z",
     "shell.execute_reply": "2024-07-23T12:11:40.992576Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.976846Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# クロスバリデーションのためのインデックスセットを作成する\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:11:40.995705Z",
     "iopub.status.busy": "2024-07-23T12:11:40.994867Z",
     "iopub.status.idle": "2024-07-23T12:16:58.97206Z",
     "shell.execute_reply": "2024-07-23T12:16:58.971067Z",
     "shell.execute_reply.started": "2024-07-23T12:11:40.995672Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練インデックスと評価インデックスを取得する\n",
    "train_idx, eval_idx = folds[config.fold_idx]\n",
    "\n",
    "# Trainerオブジェクトを設定する\n",
    "trainer = Trainer(\n",
    "    args=training_args, \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=ds.select(train_idx),  # 訓練データセット\n",
    "    eval_dataset=ds.select(eval_idx),    # 評価データセット\n",
    "    compute_metrics=compute_metrics,      # メトリック計算関数\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # データコラトレーター\n",
    ")\n",
    "trainer.train()  # モデルの訓練を開始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c68216a",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.429109Z",
     "iopub.status.busy": "2024-07-23T12:17:24.428206Z",
     "iopub.status.idle": "2024-07-23T12:17:24.43499Z",
     "shell.execute_reply": "2024-07-23T12:17:24.434121Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.429068Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# CUDAデバイスの数が2であることを確認する\n",
    "assert torch.cuda.device_count() == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.437302Z",
     "iopub.status.busy": "2024-07-23T12:17:24.436577Z",
     "iopub.status.idle": "2024-07-23T12:17:24.448863Z",
     "shell.execute_reply": "2024-07-23T12:17:24.448008Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.437268Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'  # Gemmaモデルのディレクトリ\n",
    "    lora_dir = '/kaggle/working/output/checkpoint-20'  # LoRAのチェックポイントディレクトリ\n",
    "    max_length = 2048  # 最大シーケンス長\n",
    "    batch_size = 4  # バッチサイズ\n",
    "    device = torch.device(\"cuda\")  # デバイスをCUDAに設定    \n",
    "    tta = False  # テスト時のデータ拡張\n",
    "    spread_max_length = False  # 各入力にmax_length//3を適用するか、結合された入力にmax_lengthを適用するか\n",
    "\n",
    "cfg = Config()  # Configのインスタンスを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.450467Z",
     "iopub.status.busy": "2024-07-23T12:17:24.45018Z",
     "iopub.status.idle": "2024-07-23T12:17:24.465361Z",
     "shell.execute_reply": "2024-07-23T12:17:24.464324Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.450444Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# テストデータを読み込む\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.466785Z",
     "iopub.status.busy": "2024-07-23T12:17:24.466509Z",
     "iopub.status.idle": "2024-07-23T12:17:24.48208Z",
     "shell.execute_reply": "2024-07-23T12:17:24.481009Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.466751Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def process_text(text: str) -> str:\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "# テストデータのプロンプトとレスポンスを処理する\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)\n",
    "\n",
    "display(test.head(5))  # 最初の5行を表示する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.484224Z",
     "iopub.status.busy": "2024-07-23T12:17:24.483928Z",
     "iopub.status.idle": "2024-07-23T12:17:24.500736Z",
     "shell.execute_reply": "2024-07-23T12:17:24.499687Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.484189Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    # プロンプトとレスポンスをトークン化する\n",
    "    prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        # 最大長を各入力に適用する場合\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        # プロンプトとレスポンスを結合して最大長を適用する場合\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask  # トークン化されたIDとアテンションマスクを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:24.50217Z",
     "iopub.status.busy": "2024-07-23T12:17:24.501816Z",
     "iopub.status.idle": "2024-07-23T12:17:25.538457Z",
     "shell.execute_reply": "2024-07-23T12:17:25.537483Z",
     "shell.execute_reply.started": "2024-07-23T12:17:24.502138Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Gemmaトークナイザーを読み込む\n",
    "tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加\n",
    "tokenizer.padding_side = \"right\"  # 右側パディングを設定\n",
    "\n",
    "# テストデータのトークン化を実行\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "# 拡張データを作成（response_aとresponse_bを入れ替える）\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.540235Z",
     "iopub.status.busy": "2024-07-23T12:17:25.539846Z",
     "iopub.status.idle": "2024-07-23T12:17:25.54738Z",
     "shell.execute_reply": "2024-07-23T12:17:25.546393Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.540201Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# トークナイザーを使用して最初のデータをデコード\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.548742Z",
     "iopub.status.busy": "2024-07-23T12:17:25.548466Z",
     "iopub.status.idle": "2024-07-23T12:17:25.562109Z",
     "shell.execute_reply": "2024-07-23T12:17:25.561243Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.548714Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 拡張データの最初のデータをデコード\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:17:25.563662Z",
     "iopub.status.busy": "2024-07-23T12:17:25.563361Z",
     "iopub.status.idle": "2024-07-23T12:18:58.380996Z",
     "shell.execute_reply": "2024-07-23T12:18:58.380081Z",
     "shell.execute_reply.started": "2024-07-23T12:17:25.563637Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# GPU 0に基盤モデルを読み込む\n",
    "device_0 = torch.device('cuda:0')\n",
    "model_0 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_0,\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# GPU 1に基盤モデルを読み込む\n",
    "device_1 = torch.device('cuda:1')\n",
    "model_1 = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,\n",
    "    device_map=device_1,\n",
    "    use_cache=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:58.382991Z",
     "iopub.status.busy": "2024-07-23T12:18:58.382306Z",
     "iopub.status.idle": "2024-07-23T12:18:59.003837Z",
     "shell.execute_reply": "2024-07-23T12:18:59.002849Z",
     "shell.execute_reply.started": "2024-07-23T12:18:58.382954Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# LoRAモデルを基盤モデルに追加\n",
    "model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)\n",
    "model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:59.005429Z",
     "iopub.status.busy": "2024-07-23T12:18:59.005087Z",
     "iopub.status.idle": "2024-07-23T12:18:59.015504Z",
     "shell.execute_reply": "2024-07-23T12:18:59.014256Z",
     "shell.execute_reply.started": "2024-07-23T12:18:59.005401Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # 各勝者の確率を保持するリスト\n",
    "    \n",
    "    # データをバッチ処理で推論を行う\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # モデルで出力を取得\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # 確率に変換\n",
    "        \n",
    "        # 各勝者の確率をリストに追加\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # モデルAが勝つ確率\n",
    "    df[\"winner_model_b\"] = b_win  # モデルBが勝つ確率\n",
    "    df[\"winner_tie\"] = tie        # 引き分けの確率\n",
    "    \n",
    "    return df  # 結果のデータフレームを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:18:59.017085Z",
     "iopub.status.busy": "2024-07-23T12:18:59.016807Z",
     "iopub.status.idle": "2024-07-23T12:19:03.335739Z",
     "shell.execute_reply": "2024-07-23T12:19:03.334689Z",
     "shell.execute_reply.started": "2024-07-23T12:18:59.017062Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "# 入力長でソートしてダイナミックパディングを最大限活用\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# サブ1とサブ2のトークン数をほぼ同じにする\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "# スレッドプールを使って推論を並行処理する\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 確率を取得\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:19:03.337429Z",
     "iopub.status.busy": "2024-07-23T12:19:03.337028Z",
     "iopub.status.idle": "2024-07-23T12:19:03.346082Z",
     "shell.execute_reply": "2024-07-23T12:19:03.345023Z",
     "shell.execute_reply.started": "2024-07-23T12:19:03.337395Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:  # テスト時のデータ拡張が有効な場合\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # 入力長でソートして速度を向上\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "    # TTAの順序を反転\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # 元の結果とTTAの結果を平均\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:32:09.646006Z",
     "iopub.status.busy": "2024-07-23T12:32:09.64565Z",
     "iopub.status.idle": "2024-07-23T12:32:09.656235Z",
     "shell.execute_reply": "2024-07-23T12:32:09.655442Z",
     "shell.execute_reply.started": "2024-07-23T12:32:09.64598Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# テストデータの順序に基づいて確率を整列させる\n",
    "aligned_proba = proba[[result_df.index[result_df['id'] == id][0] for id in test['id']]]\n",
    "\n",
    "# 予測を取得\n",
    "test_pred_a = aligned_proba[:, 0]  # モデルAの予測\n",
    "test_pred_b = aligned_proba[:, 1]  # モデルBの予測\n",
    "test_pred_tie = aligned_proba[:, 2]  # 引き分けの予測\n",
    "\n",
    "# 提出ファイルを準備する\n",
    "submission = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'winner_model_a': test_pred_a,  # モデルAの勝利確率\n",
    "    'winner_model_b': test_pred_b,  # モデルBの勝利確率\n",
    "    'winner_tie': test_pred_tie  # 引き分けの確率\n",
    "})\n",
    "\n",
    "# 提出ファイルを保存する\n",
    "submission_path = '/kaggle/working/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)  # CSVファイルに保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T12:32:12.743462Z",
     "iopub.status.busy": "2024-07-23T12:32:12.743066Z",
     "iopub.status.idle": "2024-07-23T12:32:12.755173Z",
     "shell.execute_reply": "2024-07-23T12:32:12.754301Z",
     "shell.execute_reply.started": "2024-07-23T12:32:12.74343Z"
    }
   },
   "outputs": [],
   "source": [
    "submission.head()  # 提出ファイルの先頭を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ee1d20",
   "metadata": {},
   "source": [
    "ログロスが少ないほど、パフォーマンスの結果が良いことを示します。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
