{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8abb389",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」コンペティションで使用されるモデルを用いた人間の好み予測に取り組んでいます。具体的には、Gemma2とLlama3の大規模言語モデルを用いて、ユーザーからのプロンプトに対する両モデルの応答の勝率を予測するタスクに焦点を当てています。\n",
    "\n",
    "### 主な問題\n",
    "- **目的**: 2つの異なるチャットボット（GemmaとLlama）が生成した応答のどちらがユーザーに好まれるかを予測し、その勝率を計算すること。\n",
    "- **評価**: モデルの出力冒険の確率を算出し、最終的に提出用データフレームを生成します。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **ライブラリ**: \n",
    "  - `transformers`: モデルとトークナイザーの操作に使用。\n",
    "  - `peft`: LoRA（Low-Rank Adaptation）技術を使用してモデルの性能を向上させる。\n",
    "  - `torch`: モデルの実行に必要なPyTorchライブラリ。\n",
    "  - `sklearn`, `numpy`, `pandas`: データ処理と機械学習に関連するライブラリ。\n",
    "\n",
    "- **手法**: \n",
    "  - GemmaとLlamaモデルをそれぞれ異なるGPUに配置し、並列処理を行っています。\n",
    "  - テストデータはCSVファイルから読み込み、プロンプトと応答をトークン化し、モデルに供給するための処理を実施。\n",
    "  - 推論では、両モデルが生成した応答に基づいて勝つ確率を算出し、結果を統合します。\n",
    "  - 最後に、確率をCSVファイルとして出力し、Kaggleコンペティションに提出する準備を整えます。\n",
    "\n",
    "このノートブックのアプローチは、ゲームの中での応答を比較し、どのモデルがユーザーの好みにより合致しているかを判定するための強力なテクニックを示しています。結果を提出するために最終的に生成されるCSVファイルは、ユーザーの選好をモデル化する精度に基づいて評価されます。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19045a4d",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "このノートブックに含まれる専門用語の中で、初心者がつまずきそうなものについて解説します。以下に挙げる用語は、特にマイナーまたは実務での経験がなければ馴染みが薄いものや、ノートブック特有のドメイン知識に関連するものです。\n",
    "\n",
    "1. **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "   - モデルの全パラメータを更新するのではなく、一部のパラメータのみを調整することを目的とした技術。これにより、少ない計算リソースでモデルを微調整することが可能になる。\n",
    "\n",
    "2. **LoRA (Low-Rank Adaptation)**:\n",
    "   - 長大なモデルの微調整手法の一つで、モデルのパラメータ行列を低ランクの形式で表現し、それに基づいて微調整する。従来の方法よりもパラメータ数が少なく、計算量も削減できる。\n",
    "\n",
    "3. **BitsAndBytes**:\n",
    "   - モデルの重みを効率的に格納し、量子化を通じてメモリ使用量を削減するためのライブラリ。特に、8ビット量子化に関連した設定も含まれる。\n",
    "\n",
    "4. **AutoTokenizer**:\n",
    "   - Hugging FaceのTransformersライブラリに組み込まれた、モデルに最適なトークナイザーを自動的に選択・初期化するクラス。ユーザーが別のトークナイザーを選択する必要がないため、使い勝手が良い。\n",
    "\n",
    "5. **Datasets**:\n",
    "   - 機械学習や深層学習において、モデルのトレーニングや評価に用いるデータの集合。CSVファイルから直接読み込むなどの操作が行われる。\n",
    "\n",
    "6. **アテンションマスク (Attention Mask)**:\n",
    "   - トランスフォーマーモデルにおいて、入力中のどのトークンを注意したかを示すためのマスク。通常、パディングされたトークンには注意を向けないように設定される。\n",
    "\n",
    "7. **torch.cuda.amp.autocast**:\n",
    "   - 自動混合精度を有効にするデコレーター。精度を落としつつ計算を高速化し、GPUのメモリ使用量を削減するために用いられる。\n",
    "\n",
    "8. **検証用データセット (Validation Dataset)**:\n",
    "   - モデルの性能を評価するために使用されるデータセット。トレーニング中に使用されず、過学習を防ぐために重要。\n",
    "\n",
    "9. **トークン化 (Tokenization)**:\n",
    "   - 入力テキストをモデルが理解できる形式に変換するプロセス。テキストをトークン（単語やサブワード）に分割し、それぞれに数値的なIDを割り当てる。\n",
    "\n",
    "10. **推論 (Inference)**:\n",
    "    - 学習されたモデルを用いて、新しいデータに対して予測を行うプロセス。トレーニングと異なり、モデルの重みは更新されない。\n",
    "\n",
    "これらの用語を理解することで、ノートブックや関連する機械学習・深層学習のトピックについての理解が深まることでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-26T07:03:59.61667Z",
     "iopub.status.busy": "2024-07-26T07:03:59.616067Z",
     "iopub.status.idle": "2024-07-26T07:03:59.630139Z",
     "shell.execute_reply": "2024-07-26T07:03:59.629298Z",
     "shell.execute_reply.started": "2024-07-26T07:03:59.616636Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "クレジット:\n",
    "\n",
    "https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference\n",
    "https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora\n",
    "\n",
    "リーダーボードスコア: 0.945\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:03:59.647261Z",
     "iopub.status.busy": "2024-07-26T07:03:59.646755Z",
     "iopub.status.idle": "2024-07-26T07:04:18.020077Z",
     "shell.execute_reply": "2024-07-26T07:04:18.018911Z",
     "shell.execute_reply.started": "2024-07-26T07:03:59.647236Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリをインストールします。\n",
    "# transformers, peft, accelerate, bitsandbytesの最新バージョンを指定してインストールします。\n",
    "# --no-indexオプションを使用することで、PyPI（Python Package Index）を使用せずに、\n",
    "# 指定したリンクからのみパッケージを取得することを意味します。\n",
    "\n",
    "!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:04:18.023082Z",
     "iopub.status.busy": "2024-07-26T07:04:18.022391Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポートします。\n",
    "import time  # タイマー処理用\n",
    "from dataclasses import dataclass  # データクラスの作成用\n",
    "from concurrent.futures import ThreadPoolExecutor  # スレッドプールによる非同期処理用\n",
    "\n",
    "import torch  # PyTorchライブラリ\n",
    "import sklearn  # 機械学習ライブラリ\n",
    "import numpy as np  # 数値計算ライブラリ\n",
    "import pandas as pd  # データ操作ライブラリ\n",
    "from transformers import (\n",
    "    Gemma2ForSequenceClassification,  # Gemma2モデルのシーケンス分類用クラス\n",
    "    GemmaTokenizerFast,  # Gemmaモデル用のトークナイザー\n",
    "    AutoTokenizer,  # 自動トークナイザー\n",
    "    LlamaForSequenceClassification,  # Llamaモデルのシーケンス分類用クラス\n",
    "    BitsAndBytesConfig  # BitsAndBytesの設定\n",
    ")\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning  # トークナイザーの警告を抑制するためのデータコレータ\n",
    "from peft import PeftModel, get_peft_model, LoraConfig, TaskType  # PEFTライブラリのモデル・設定用クラス\n",
    "\n",
    "# メモリ効率の良いSDP（スパースディストリビューションプロセス）をCUDAバックエンドで有効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "# フラッシュSDPも有効にします。\n",
    "torch.backends.cuda.enable_flash_sdp(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Gemmaモデルのディレクトリパスを指定します。\n",
    "    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'\n",
    "    # GemmaのLoRAチェックポイントのディレクトリパスを指定します。\n",
    "    gemma_lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'\n",
    "    # Llamaモデルの名前（ディレクトリパス）を指定します。\n",
    "    llama_model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    # Llamaモデルの重みファイルのパスを指定します。\n",
    "    llama_weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    # 最大シーケンス長を設定します（ここでは2048トークン）。\n",
    "    max_length = 2048\n",
    "    # バッチサイズを設定します（ここでは4）。\n",
    "    batch_size = 4\n",
    "    # テスト時のデータ拡張を使用するかどうかを指定します（ここでは使用しない）。\n",
    "    tta = False\n",
    "    # 最大シーケンス長の拡張を使用するかどうかを指定します（ここでは使用しない）。\n",
    "    spread_max_length = False\n",
    "\n",
    "# 新しいConfigオブジェクトを生成します。\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テストデータセットをCSVファイルから読み込みます。\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# テキストを処理する関数を定義します。\n",
    "def process_text(text: str) -> str:\n",
    "    # 評価関数を用いて、テキスト内の'null'を空の文字列に置き換えます。\n",
    "    return \" \".join(eval(text, {\"null\": \"\"}))\n",
    "\n",
    "# 'prompt'列のテキストを処理します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process_text)\n",
    "# 'response_a'列のテキストを処理します。\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process_text)\n",
    "# 'response_b'列のテキストを処理します。\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークナイザーを使用して入力データをトークン化する関数を定義します。\n",
    "def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):\n",
    "    # トークナイザーの種類によって処理を分けます。\n",
    "    if isinstance(tokenizer, GemmaTokenizerFast):\n",
    "        # Gemmaトークナイザーの場合は特定のプレフィックスを追加します。\n",
    "        prompt = [\"<prompt>: \" + p for p in prompt]\n",
    "        response_a = [\"\\n\\n<response_a>: \" + r_a for r_a in response_a]\n",
    "        response_b = [\"\\n\\n<response_b>: \" + r_b for r_b in response_b]\n",
    "    else:\n",
    "        # その他のトークナイザーの場合は別のプレフィックスを追加します。\n",
    "        prompt = [\"User prompt: \" + p for p in prompt]\n",
    "        response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "        response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    \n",
    "    # max_lengthをスプレッドするかどうかで処理を分けます。\n",
    "    if spread_max_length:\n",
    "        # 各入力の長さを最大値の1/3に設定し、トークン化します。\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        # 入力IDsを結合します。\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        # アテンションマスクを作成します。\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        # 各入力をトークン化し、max_lengthに基づいて制限します。\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    # 入力IDsとアテンションマスクを返します。\n",
    "    return input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-26T07:04:36.823168Z",
     "shell.execute_reply": "2024-07-26T07:04:36.822425Z",
     "shell.execute_reply.started": "2024-07-26T07:04:35.178994Z"
    }
   },
   "outputs": [],
   "source": [
    "# Gemmaトークナイザーを初期化します。\n",
    "gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)\n",
    "# 終了トークンを追加することを指定します。\n",
    "gemma_tokenizer.add_eos_token = True\n",
    "# パディングを右側に設定します。\n",
    "gemma_tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Llamaトークナイザーを初期化します。\n",
    "llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# 両方のモデル用にデータを準備します。\n",
    "\n",
    "# Gemma用のデータフレームを作成します。\n",
    "gemma_data = pd.DataFrame()\n",
    "# テストデータのIDを設定します。\n",
    "gemma_data[\"id\"] = test[\"id\"]\n",
    "# トークナイザーを使って入力IDsとアテンションマスクを取得します。\n",
    "gemma_data[\"input_ids\"], gemma_data[\"attention_mask\"] = tokenize(gemma_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "# 各入力の長さを計算します。\n",
    "gemma_data[\"length\"] = gemma_data[\"input_ids\"].apply(len)\n",
    "\n",
    "# Llama用のデータフレームを作成します。\n",
    "llama_data = pd.DataFrame()\n",
    "# テストデータのIDを設定します。\n",
    "llama_data[\"id\"] = test[\"id\"]\n",
    "# トークナイザーを使って入力IDsとアテンションマスクを取得します。\n",
    "llama_data[\"input_ids\"], llama_data[\"attention_mask\"] = tokenize(llama_tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "# 各入力の長さを計算します。\n",
    "llama_data[\"length\"] = llama_data[\"input_ids\"].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-26T07:04:36.824439Z",
     "iopub.status.busy": "2024-07-26T07:04:36.82418Z"
    }
   },
   "outputs": [],
   "source": [
    "# GemmaモデルをGPU 0に読み込みます。\n",
    "device_0 = torch.device('cuda:0')\n",
    "gemma_model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    cfg.gemma_dir,  # Gemmaモデルのディレクトリを指定します。\n",
    "    device_map=device_0,  # モデルを配置するデバイスを指定します。\n",
    "    use_cache=False  # キャッシュを使用しないように設定します。\n",
    ")\n",
    "# LoRAモデルをGemmaモデルに読み込みます。\n",
    "gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)  # LoRAチェックポイントのディレクトリを指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.idle": "2024-07-26T07:08:14.149927Z",
     "shell.execute_reply": "2024-07-26T07:08:14.147703Z",
     "shell.execute_reply.started": "2024-07-26T07:06:08.525096Z"
    }
   },
   "outputs": [],
   "source": [
    "# LlamaモデルをGPU 1に読み込みます。\n",
    "device_1 = torch.device('cuda:1')\n",
    "# 8ビット量子化の設定を行います。\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットでモデルをロードします。\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 計算時のデータ型をfloat16に設定します。\n",
    "    bnb_8bit_use_double_quant=False  # ダブル量子化を使用しない設定です。\n",
    ")\n",
    "# Llamaの基本モデルを読み込みます。\n",
    "llama_base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.llama_model_name,  # Llamaモデルの名前（ディレクトリ）を指定します。\n",
    "    num_labels=3,  # 分類するラベルの数を設定します（ここでは3つのラベル）。\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定します。\n",
    "    quantization_config=bnb_config,  # 量子化の設定を指定します。\n",
    "    device_map='cuda:1'  # モデルを配置するデバイスを指定します。\n",
    ")\n",
    "# LlamaトークナイザーのパディングトークンIDを設定します。\n",
    "llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id\n",
    "\n",
    "# LoRAの設定を行います。\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # LoRAのランク。\n",
    "    lora_alpha=32,  # LoRAのスケーリングファクター。\n",
    "    lora_dropout=0.10,  # LoRAのドロップアウト率。\n",
    "    bias='none',  # バイアスの設定（ここではなし）。\n",
    "    inference_mode=True,  # 推論モードを有効にします。\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類をシーケンス分類に設定します。\n",
    "    target_modules=['o_proj', 'v_proj']  # ターゲットモジュールの名前を指定します。\n",
    ")\n",
    "# LoRAモデルを取得し、GPU 1に移動します。\n",
    "llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)\n",
    "# Llamaモデルの重みを指定されたパスから読み込みます。\n",
    "llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)\n",
    "# モデルを評価モードに設定します。\n",
    "llama_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.150889Z",
     "iopub.status.idle": "2024-07-26T07:08:14.151222Z",
     "shell.execute_reply": "2024-07-26T07:08:14.15108Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.151066Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配の計算をオフにします（推論中は必要ないため）。\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を有効にします（計算の効率を上げるため）。\n",
    "def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    # 各モデルの勝利確率を保存するリストを初期化します。\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    # データフレームをバッチ単位で処理します。\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算します。\n",
    "        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得します。\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDsをリストとして取得します。\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストとして取得します。\n",
    "        \n",
    "        # トークナイザーを使って入力を整形します。\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},  # 入力IDsとアテンションマスクを指定します。\n",
    "            padding=\"longest\",  # 最長の入力にパディングします。\n",
    "            pad_to_multiple_of=None,  # 特に指定しない場合はNoneにします。\n",
    "            return_tensors=\"pt\",  # PyTorchのテンソル形式で返します。\n",
    "        )\n",
    "        \n",
    "        # モデルを使用して推論を行います。\n",
    "        outputs = model(**inputs.to(device))  # 入力を指定されたデバイスに移動させてモデルに渡します。\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットからソフトマックスを適用して確率を計算します。\n",
    "        \n",
    "        # 各モデルの勝率をリストに追加します。\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    # 勝率をデータフレームに追加します。\n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df  # 処理したデータフレームを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.152742Z",
     "iopub.status.idle": "2024-07-26T07:08:14.153108Z",
     "shell.execute_reply": "2024-07-26T07:08:14.152929Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.152915Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時刻を記録します。\n",
    "\n",
    "# 入力の長さでデータをソートします（長い順）。\n",
    "gemma_data = gemma_data.sort_values(\"length\", ascending=False)\n",
    "llama_data = llama_data.sort_values(\"length\", ascending=False)\n",
    "\n",
    "# スレッドプールを使用して並列処理を実行します。\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, \n",
    "                           (gemma_data, llama_data),  # GemmaとLlamaのデータを引数として渡します。\n",
    "                           (gemma_model, llama_model),  # 各モデルを引数として渡します。\n",
    "                           (gemma_tokenizer, llama_tokenizer),  # 各トークナイザーを引数として渡します。\n",
    "                           (device_0, device_1))  # 各モデルが配置されているデバイスを引数として渡します。\n",
    "\n",
    "gemma_result_df, llama_result_df = list(results)  # 結果をリストに変換して変数に格納します。\n",
    "\n",
    "# 結果を組み合わせます（簡単な平均を計算します）。\n",
    "combined_result_df = gemma_result_df.copy()  # Gemmaの結果データフレームをコピーします。\n",
    "combined_result_df[\"winner_model_a\"] = (gemma_result_df[\"winner_model_a\"] + llama_result_df[\"winner_model_a\"]) / 2  # モデルAの勝率を平均します。\n",
    "combined_result_df[\"winner_model_b\"] = (gemma_result_df[\"winner_model_b\"] + llama_result_df[\"winner_model_b\"]) / 2  # モデルBの勝率を平均します。\n",
    "combined_result_df[\"winner_tie\"] = (gemma_result_df[\"winner_tie\"] + llama_result_df[\"winner_tie\"]) / 2  # 引き分け率を平均します。\n",
    "\n",
    "# 推論にかかった時間を表示します。\n",
    "print(f\"Inference time: {time.time() - st:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-26T07:08:14.155209Z",
     "iopub.status.idle": "2024-07-26T07:08:14.155526Z",
     "shell.execute_reply": "2024-07-26T07:08:14.155387Z",
     "shell.execute_reply.started": "2024-07-26T07:08:14.155374Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提出用のデータフレームを作成します。\n",
    "# 'id'、'winner_model_a'、'winner_model_b'、'winner_tie'の列を選択します。\n",
    "submission_df = combined_result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "# データフレームをCSVファイルに書き出します（インデックスは含めません）。\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "# 提出データの先頭5行を表示します。\n",
    "display(submission_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2502bc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Nikita Glazunov\n",
    "> \n",
    "> こんにちは！良いノートブックです。Llama 3モデルを読み込もうとしたときに、このエラーが出ました >OSError: Incorrect path_or_model_id: '/kaggle/input/llama-3/transformers/8b-chat-hf/1'。ローカルフォルダへのパスまたはHub上のモデルのrepo_idを提供してください。どうすれば修正できますか？コードは一切変更していません。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## YEI0907\n",
    "> \n",
    "> 推論にどれくらいの時間がかかりましたか？\n",
    "> \n",
    "> \n",
    "> > ## G John Rao（トピック作成者）\n",
    "> > \n",
    "> > 約9時間かかります。\n",
    "> > \n",
    "> \n",
    "> "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5297895,
     "sourceId": 8897601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5369301,
     "sourceId": 8926343,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 39106,
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelId": 86587,
     "modelInstanceId": 63082,
     "sourceId": 75103,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
