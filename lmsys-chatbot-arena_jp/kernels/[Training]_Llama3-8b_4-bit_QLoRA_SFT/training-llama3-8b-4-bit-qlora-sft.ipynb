{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7048a9",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションで使用される、チャットボットの応答の好みを予測するための機械学習モデルの訓練に関するものです。具体的には、Llama3という言語モデルを用いて、ユーザーが好む応答を予測し、その性能を評価しています。\n",
    "\n",
    "### 問題\n",
    "このノートブックでは、Chatbot Arenaデータセットに基づいて、どちらのチャットボット応答が好まれるかを予測するモデルを構築しています。ユーザーからのフィードバックに基づいて、異なるモデルの応答の優劣を判断する問題に取り組みます。\n",
    "\n",
    "### 手法とライブラリ\n",
    "- **モデル:** Llama3（8ビット精度）をベースとして使用し、LoRA (Low-Rank Adaptation)を導入したカスタムモデル `Llama3ForSFT`を作成しています。これにより、モデルの重量を軽量化し、効率的にトレーニングを行えるようにしています。\n",
    "  \n",
    "- **トークナイザー:** `transformers` ライブラリの `AutoTokenizer`を使用して、テキストデータをトークン化しています。\n",
    "\n",
    "- **データ処理:** `pandas`を使用してデータを読み込み、一部のデータを利用してモデルを訓練しています。また、`datasets` ライブラリを用いてデータセットを管理しています。\n",
    "\n",
    "- **評価指標:** モデルの性能評価には、対数損失 (`log_loss`) と精度 (`accuracy_score`) が使用されています。`scikit-learn`の機能を活用して計算しています。\n",
    "\n",
    "### 結果\n",
    "ノートブック内では、評価用データセットのログ損失が 0.9231 で、リーダーボードのスコアが0.936と示されており、モデルの性能が確認されています。また、訓練時のバッチサイズやエポック数、評価手法についても詳細に説明がなされ、特にGPU環境での実行を考慮した条件が記載されています。\n",
    "\n",
    "全体として、このノートブックは、ユーザーの応答の好みを予測するための機械学習モデルの実装と、そのトレーニングプロセスを包括的に示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e7d0de",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下はノートブックで使われているが、初心者には馴染みが薄いかもしれない専門用語の簡単な解説です。\n",
    "\n",
    "1. **ログ損失 (Log Loss)**:\n",
    "   - 予測モデルの性能を評価するための指標。確率的な予測が実際のクラスラベルとどれだけズレているかを測定します。小さい値が良いとされ、予測が実際の正解に近いほど小さくなります。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**:\n",
    "   - 自然言語処理で使用するテキストを、機械が理解できる数値データ（トークン）に変換するためのツール。トークンは単語やサブワードのような単位で、これによってモデルがテキストを処理します。\n",
    "\n",
    "3. **アテンションマスク (Attention Mask)**:\n",
    "   - アテンション機構を実装する際に使用するマスクで、どの部分が計算の対象であるか（1）または無視すべき部分（0）を示します。従って、パディングされたデータの影響を排除します。\n",
    "\n",
    "4. **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "   - 限られたパラメータのみを調整してモデルを微調整する手法。これにより、訓練に必要なリソースを削減しつつ、モデルのパフォーマンスを向上させます。このノートブックでは、LoRA（Low-Rank Adaptation）という手法が利用されています。\n",
    "\n",
    "5. **LoRA (Low-Rank Adaptation)**:\n",
    "   - 大規模モデルの微調整において、全体の重みを更新するのではなく、低ランクの行列を使用して効率的にモデルを調整する方法。トレーニングの効率を向上させ、必要な計算資源を削減します。\n",
    "\n",
    "6. **データコレーター (Data Collator)**:\n",
    "   - バッチ処理の際にデータをまとめるためのツール。特に異なる長さのシーケンスを適切にパディングするなど、データを連結してバッチにする役割を担います。\n",
    "\n",
    "7. **Gradient Accumulation (勾配蓄積)**:\n",
    "   - 小さなバッチサイズでモデルをトレーニングする際に、複数のバッチの勾配を蓄積してから更新を行う技術。これにより、実際のバッチサイズを大きくすることができます。\n",
    "\n",
    "8. **評価戦略 (Evaluation Strategy)**:\n",
    "   - モデルのトレーニング中に評価を行うタイミングや方法を定義する設定。例えば、エポックごとの評価や特定のステップごとに行うなどがあります。\n",
    "\n",
    "9. **half-precision (fp16)**:\n",
    "   - 計算の精度の一つで、16ビットの浮動小数点数を使用すること。これによりメモリ使用量が削減され、計算速度が向上することがありますが、数値の精度は低下することがあります。\n",
    "\n",
    "10. **タスクタイプ (Task Type)**:\n",
    "    - モデルが実行する特定のタスクの種類で、ここでは因果推論（Causal Language Modeling）の設定がされている。これは、次に来るトークンを予測するタスクです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bfb27a",
   "metadata": {},
   "source": [
    "## 結果\n",
    "- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    \n",
    "\n",
    "- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)\n",
    "\n",
    "| サブセット | ログ損失 |\n",
    "| - | - |\n",
    "| 評価 | 0.9231 |\n",
    "| LB | 0.936 |\n",
    "\n",
    "## 注意\n",
    "コードを再現したい場合は、以下の点に注意してください：\n",
    "- すべてのデータを使用すること\n",
    "- per_device_train_batch_size=4に設定すること\n",
    "- A10を使用して1エポックは約15時間かかること\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-20T19:19:49.702162Z",
     "iopub.status.busy": "2024-07-20T19:19:49.701381Z",
     "iopub.status.idle": "2024-07-20T19:20:02.714689Z",
     "shell.execute_reply": "2024-07-20T19:20:02.71369Z",
     "shell.execute_reply.started": "2024-07-20T19:19:49.702128Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft  # transformers, bitsandbytes, accelerate, peftのパッケージを最新バージョンでインストールします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:02.716715Z",
     "iopub.status.busy": "2024-07-20T19:20:02.7163Z",
     "iopub.status.idle": "2024-07-20T19:20:11.016318Z",
     "shell.execute_reply": "2024-07-20T19:20:11.015266Z",
     "shell.execute_reply.started": "2024-07-20T19:20:02.716674Z"
    }
   },
   "outputs": [],
   "source": [
    "import os  # OS関連の機能をインポート\n",
    "import copy  # オブジェクトをコピーするためのモジュールをインポート\n",
    "from dataclasses import dataclass  # データクラスを作成するためのモジュールをインポート\n",
    "\n",
    "import torch  # PyTorchのライブラリをインポート\n",
    "import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポート\n",
    "import torch.nn.functional as F  # PyTorchの関数型ニューラルネットワークモジュールをインポート\n",
    "import pandas as pd  # データ処理のためのパンダスライブラリをインポート\n",
    "import numpy as np  # 数値計算のためのNumPyライブラリをインポート\n",
    "from datasets import Dataset  # データセット管理のためのライブラリをインポート\n",
    "from scipy.special import softmax  # softmax関数をインポート\n",
    "from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングのためのモジュールをインポート\n",
    "from transformers import (  # Transformersライブラリから以下のクラスをインポート\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast  # モデルの出力のデータ型をインポート\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTのための設定やモデルをインポート\n",
    "from sklearn.metrics import log_loss, accuracy_score  # 精度とログ損失の評価指標をインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2299c0c4",
   "metadata": {},
   "source": [
    "### 設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:11.018203Z",
     "iopub.status.busy": "2024-07-20T19:20:11.017544Z",
     "iopub.status.idle": "2024-07-20T19:20:12.896737Z",
     "shell.execute_reply": "2024-07-20T19:20:12.895826Z",
     "shell.execute_reply.started": "2024-07-20T19:20:11.018173Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"  # 訓練データのCSVファイルのパスを指定\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"  # モデルのパスを指定\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能な場合はGPUを使用、そうでなければCPUを使用\n",
    "MAX_LENGTH = 1024  # トークンの最大長を設定\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなるカラムをリスト化\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]  # ベクトル化するカラムをリスト化\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)  # 訓練データをCSVファイルから読み込み\n",
    "train = train.head(100)  # 最初の100行を取得\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行で最も大きいインデックスをラベルとして設定\n",
    "label_encoder = LabelEncoder()  # ラベルエンコーダのインスタンスを作成\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコード\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要なカラムのみを保持"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff6bb1a",
   "metadata": {},
   "source": [
    "### トークナイザーとデータセットの準備、評価指標\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:12.8993Z",
     "iopub.status.busy": "2024-07-20T19:20:12.899001Z",
     "iopub.status.idle": "2024-07-20T19:20:13.397796Z",
     "shell.execute_reply": "2024-07-20T19:20:13.39683Z",
     "shell.execute_reply.started": "2024-07-20T19:20:12.899271Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # 事前学習済みモデルのトークナイザーを取得\n",
    "tokenizer.add_eos_token = True  # EOSトークンを追加\n",
    "tokenizer.padding_side = 'right'  # パディングは右側に設定\n",
    "\n",
    "# ラベルのトークンIDを取得\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):  # トークン化のための関数を定義\n",
    "    # プロンプト、応答A、応答Bをそれぞれトークン化\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    # 最大長を超える場合は部分的にトークン化\n",
    "    if len(prompt + response_a + response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nこのプロンプトに対してどちらの応答が良いか？ a または b または tie か？\\n\\n答え: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]  # 正解ラベルのトークンIDを取得\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]  # アテンションマスクを作成\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]  # ラベルを準備\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:13.399477Z",
     "iopub.status.busy": "2024-07-20T19:20:13.399103Z",
     "iopub.status.idle": "2024-07-20T19:20:13.99278Z",
     "shell.execute_reply": "2024-07-20T19:20:13.991862Z",
     "shell.execute_reply.started": "2024-07-20T19:20:13.399443Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):  # データをロードする関数を定義\n",
    "    raw_datasets = Dataset.from_pandas(df)  # pandas DataFrameからデータセットを作成\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize,  # トークン化関数をマップ\n",
    "        remove_columns=raw_datasets.column_names,  # 使用しないカラムは削除\n",
    "        fn_kwargs={'tokenizer': tokenizer}  # トークナイザーを引数として渡す\n",
    "    )\n",
    "    return tokenized_datasets  # トークナイズされたデータセットを返す\n",
    "\n",
    "def compute_metrics(pred):  # 精度や損失を計算する関数を定義\n",
    "    logits, labels = pred  # 予測値とラベルを取得\n",
    "    preds = logits.argmax(axis=-1)  # 最大のロジットからクラスを予測\n",
    "    label_tokens_ids = np.array(LABEL_IDS)  # ラベルトークンのIDをNumPy配列に変換\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]  # ラベルをフィルタリング\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])  # ラベルをインデックスに変換\n",
    "    acc = accuracy_score(labels, preds)  # 精度を計算\n",
    "    probs = softmax(logits, axis=-1)  # ソフトマックスで確率を計算\n",
    "    log_loss_ = log_loss(labels, probs)  # ログ損失を計算\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログ損失を辞書で返す\n",
    "\n",
    "n_splits = 5  # データの分割数を指定\n",
    "fold_idx = 0  # 現在のフォールドインデックスを初期化\n",
    "ds = load_data(train, tokenizer)  # データをロード\n",
    "# n分割交差検証のためのインデックスを作成\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246b2233",
   "metadata": {},
   "source": [
    "### モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:13.994402Z",
     "iopub.status.busy": "2024-07-20T19:20:13.994039Z",
     "iopub.status.idle": "2024-07-20T19:20:14.009661Z",
     "shell.execute_reply": "2024-07-20T19:20:14.008776Z",
     "shell.execute_reply.started": "2024-07-20T19:20:13.994369Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):  # LlamaPreTrainedModelを継承したクラスを定義\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 結びつけられた重みのキーを指定\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # 親クラスの初期化\n",
    "        self.model = LlamaModel(config)  # Llamaモデルのインスタンスを作成\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズを設定\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 出力層を定義\n",
    "        self.post_init()  # 初期化処理\n",
    "\n",
    "    def forward(  # フォワード関数を定義\n",
    "        self,\n",
    "        input_ids=None,  # 入力ID\n",
    "        attention_mask=None,  # アテンションマスク\n",
    "        position_ids=None,  # 位置ID\n",
    "        past_key_values=None,  # 過去のキーと値\n",
    "        inputs_embeds=None,  # 入力の埋め込みベクトル\n",
    "        labels=None,  # ラベル\n",
    "        use_cache=None,  # キャッシュを使用するか\n",
    "        output_attentions=None,  # アテンションを出力するか\n",
    "        output_hidden_states=None,  # 隠れ状態を出力するか\n",
    "        return_dict=None,  # 辞書形式で返すか\n",
    "        cache_position=None,  # キャッシュの位置\n",
    "    ):\n",
    "        outputs = self.model(  # モデルを通して出力を取得\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ状態を取得\n",
    "        if self.config.pretraining_tp > 1:  # モデル並列化が必要な場合\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)  # 重みを分割\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]  # 各スライスに対するロジットを計算\n",
    "            logits = torch.cat(logits, dim=-1)  # ロジットを結合\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # 隠れ状態を基にロジットを計算\n",
    "        logits = logits.float()  # ロジットをfloat型に変換\n",
    "\n",
    "        loss = None  # 損失を初期化\n",
    "        if labels is not None:  # ラベルが与えられている場合\n",
    "            # トークンをシフトして予測を行う\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # 最後のトークンを除外\n",
    "            shift_labels = labels[..., 1:].contiguous()  # 最初のトークンを除外\n",
    "            # トークンをフラット化\n",
    "            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失関数を定義\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラットに変換\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルをフラットに変換\n",
    "            # モデル並列化を有効化\n",
    "            shift_labels = shift_labels.to(shift_logits.device)  # デバイスを統一\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルトークンIDをテンソルに変換\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 正しいラベルをフィルタリング\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)  # インデックスに変更\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 正しいロジットを取得\n",
    "            loss = loss_fct(true_logits, true_labels)  # 損失を計算\n",
    "\n",
    "        return CausalLMOutputWithPast(  # 出力を返す\n",
    "            loss=loss,  # 損失\n",
    "            logits=true_logits,  # 正しいロジット\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:14.011164Z",
     "iopub.status.busy": "2024-07-20T19:20:14.010827Z",
     "iopub.status.idle": "2024-07-20T19:20:18.581908Z",
     "shell.execute_reply": "2024-07-20T19:20:18.580961Z",
     "shell.execute_reply.started": "2024-07-20T19:20:14.011106Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(  # PEFT構成を定義\n",
    "    r=16,  # ランク\n",
    "    lora_alpha=32,  # LoRAアルファ\n",
    "    lora_dropout=0.05,  # LoRAドロップアウト率\n",
    "    bias='none',  # バイアス\n",
    "    inference_mode=False,  # 推論モード\n",
    "    task_type=TaskType.CAUSAL_LM,  # タスクタイプ\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj'],  # 対象モジュールを指定\n",
    ")\n",
    "\n",
    "model = Llama3ForSFT.from_pretrained(  # 事前学習済みのモデルをロード\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16,  # 半精度でロード\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを使用しない設定\n",
    "model = prepare_model_for_kbit_training(model)  # kビットトレーニング用にモデルを準備\n",
    "model = get_peft_model(model, peft_config)  # PEFTモデルを取得\n",
    "print(model)  # モデルの概要を表示\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fde043b",
   "metadata": {},
   "source": [
    "#### 訓練引数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:18.583377Z",
     "iopub.status.busy": "2024-07-20T19:20:18.583066Z",
     "iopub.status.idle": "2024-07-20T19:20:18.613586Z",
     "shell.execute_reply": "2024-07-20T19:20:18.612705Z",
     "shell.execute_reply.started": "2024-07-20T19:20:18.583352Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(  # 訓練引数を定義\n",
    "    output_dir='output',  # 出力ディレクトリ\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書きするか\n",
    "    evaluation_strategy=\"epoch\",  # 評価の戦略\n",
    "    save_strategy=\"steps\",  # 保存の戦略\n",
    "    save_steps=200,  # 200ステップごとに保存\n",
    "    save_total_limit=1,  # 最大保存数\n",
    "    logging_strategy=\"steps\",  # ロギングの戦略\n",
    "    logging_steps=10,  # 10ステップごとにログ\n",
    "    warmup_steps=20,  # ウォームアップステップ数\n",
    "    optim=\"adamw_8bit\",  # 最適化アルゴリズム\n",
    "    learning_rate=2e-4,  # 学習率\n",
    "    per_device_train_batch_size=2,  # デバイスごとのトレーニングバッチサイズ\n",
    "    per_device_eval_batch_size=4,  # デバイスごとの評価バッチサイズ\n",
    "    gradient_accumulation_steps=2,  # 勾配蓄積ステップ数\n",
    "    num_train_epochs=1,  # 訓練エポック数\n",
    "    fp16=True,  # フロート16を使用\n",
    "    metric_for_best_model=\"log_loss\",  # 最良モデルの評価指標\n",
    "    greater_is_better=False,  # 指標が大きいほど良いか\n",
    "    report_to=\"none\",  # どこに報告するか\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9d66d9",
   "metadata": {},
   "source": [
    "### 訓練開始！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-20T19:20:18.615543Z",
     "iopub.status.busy": "2024-07-20T19:20:18.6149Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(  # トレーナーインスタンスを作成\n",
    "    args=args,  # 訓練引数\n",
    "    model=model,  # 使用するモデル\n",
    "    train_dataset=ds.select(train_idx),  # トレーニングデータセット\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセット\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレーター\n",
    "    compute_metrics=compute_metrics,  # 評価指標計算用関数\n",
    ")\n",
    "trainer.train()  # 訓練を開始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b76eb0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## KeShuang Liu\n",
    "> \n",
    "> こんにちは、トレーニングにmax_lengthが1024、推論に2400と設定されている理由を知りたいです。トレーニングでmax_length=2400を使用したことはありますか？\n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## OHIRA\n",
    "> \n",
    ">素晴らしい作品をありがとうございます！\n",
    "> \n",
    "> コードについて質問があります。\n",
    "> \n",
    "> load_best_model_at_end = Trueを設定した場合、\n",
    "> \n",
    "> args = TrainingArguments(\n",
    ">     output_dir='/kaggle/output',\n",
    ">     overwrite_output_dir=True,\n",
    ">     evaluation_strategy=\"steps\",\n",
    ">     save_strategy=\"steps\",\n",
    ">     save_steps=20,\n",
    ">     save_total_limit=5,\n",
    ">     logging_strategy=\"steps\",\n",
    ">     logging_steps=20,\n",
    ">     warmup_steps=20,\n",
    ">     optim=\"adamw_8bit\",\n",
    ">     learning_rate=2e-4,\n",
    ">     per_device_train_batch_size=2,\n",
    ">     per_device_eval_batch_size=4,\n",
    ">     gradient_accumulation_steps=2,\n",
    ">     num_train_epochs=1,\n",
    ">     fp16=True,\n",
    ">     metric_for_best_model=\"log_loss\",\n",
    ">     greater_is_better=False,\n",
    ">     report_to=\"none\",\n",
    ">     load_best_model_at_end=True\n",
    "> )\n",
    "> \n",
    "> 評価セットで5つの最良モデルのパラメータを取得できますか？\n",
    "> \n",
    "> それとも最後の5つのモデルのパラメータを取得するのですか？\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## daichisaito-cs\n",
    "> \n",
    "> 素晴らしい作品を共有していただきありがとうございます！\n",
    "> \n",
    "> 質問があります：\n",
    "> \n",
    "> Evalのスコア0.9231、LBのスコア0.936を再現するには何エポック必要ですか？\n",
    "> \n",
    "> デフォルトの訓練エポック数は1に設定されていますが、これがこれらのスコアを得るために使用された値と同じですか？\n",
    "> \n",
    "> ありがとう\n",
    "> \n",
    "> \n",
    "> > ## ShelterWTopic 著者\n",
    "> > \n",
    "> はい。       \n",
    "> > \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Lorry Zou\n",
    "> \n",
    "> Gemma2 9Bをこの方法（次の単語予測）で訓練しようとしましたか？Llama3の場合、この方法はLlamaForSequenceClassificationを直接使用するよりもはるかに良いパフォーマンスを持つようです。\n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Stringersolo\n",
    "> \n",
    "> こんにちは [@shelterw](https://www.kaggle.com/shelterw)、共有していただきありがとうございます。同じ結果を再現する際に問題があります。具体的には、トレーニング中は指標がほぼ同じですが、LBでスコアを計算すると約1.2で、非常に奇妙で平均的な/ランダムな予測に近いです。\n",
    "> \n",
    "> モデルの重みを直接読み取ろうとしましたが、役に立ちませんでした：\n",
    "> \n",
    "> model_0.load_state_dict(torch.load(RAW_WEIGHTS), strict=False)\n",
    "> \n",
    "> \n",
    "> > ## ShelterWTopic 著者\n",
    "> > \n",
    "> モデルをロードする際にlm headの重みがランダムに再ロードされることが原因かもしれません。\n",
    "> > \n",
    "> transformersとpeftのバージョンを更新するか、LlamaPretrainedModelの代わりにLlamaCausalModelクラスを継承するようにしてください。\n",
    "> > \n",
    "> Gemma2を使用したときにも同じ現象が起きましたが、奇妙です。\n",
    "> > \n",
    "> [こちら](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518408#2912471)を参照してください。\n",
    "> > \n",
    "> \n",
    "> > > ## Stringersolo\n",
    "> > > \n",
    "> > > ありがとう[@shelterw](https://www.kaggle.com/shelterw)、試してみます。このリンクでスコアレイヤーを保存するように提案されました：\n",
    "> > > \n",
    "> > > torch.save(classifier.score.state_dict(), f'{output_directory_path}/score_state_dict.pth')\n",
    "> > > \n",
    "> > > あなたの場合は次のようになります：\n",
    "> > > \n",
    "> > > torch.save(trainer.model.lm_head.state_dict(), f'output/lm_head_dict.pth')\n",
    "> > > \n",
    "> > > ですよね？\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Yi-Fu Chen\n",
    "> \n",
    "> なぜ.autoModelForCausalLMを直接使用するのではなく、Llama3ForSFTを実装する必要があるのか教えていただけますか？特別な理由はありますか？\n",
    "> \n",
    "> \n",
    "> > ## ShelterWTopic 著者\n",
    "> > \n",
    "> > ラベルトークンのロジットと損失のためです。\n",
    "> > \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## Eido Mike\n",
    "> \n",
    "> 優れた作品です！共有していただきありがとうございます。\n",
    "> \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## AbaoJiang\n",
    "> \n",
    "> こんにちは [@shelterw](https://www.kaggle.com/shelterw)、\n",
    "> \n",
    "> 共有していただきありがとうございます。トレーニングにCAUSAL_LMタスクを使用したことに気付きました。LlamaForSequenceClassificationを使用してトレーニングした場合のパフォーマンスと比較しましたか？\n",
    "> \n",
    "> \n",
    "> > ## ShelterWTopic 著者\n",
    "> > \n",
    "> > llama3-8bとSEQ_CLSを比較したことはありませんが、以前の実験ではllama3-8bの方が悪かったですが、gemma2-9bのSEQ_CLSよりは良かったです。\n",
    "> > \n",
    "> \n",
    "\n",
    "---\n",
    "\n",
    "> ## __ChrisQ__\n",
    "> \n",
    "> こんにちは、このノートブックをありがとうございます。\n",
    "> \n",
    "> 一つ質問があります：すべてのデータを使用する場合、評価スコアはどのように計算しますか？\n",
    "> \n",
    "> \n",
    "> > ## ShelterWTopic 著者\n",
    "> > \n",
    "> > スクリプトの'compute_metrics'関数は、1エポック後に自動的に計算されます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## raconion\n",
    "> > > \n",
    "> > > 'compute_metrics'関数はトレーニングデータの20%をクロスバリデーションに使用します。評価データを使用してモデルをさらにトレーニングしたのでしょうか？「すべてのデータを使用する」とはどういう意味ですか？5分割CVのためにすべてのデータを使用するという意味でしょうか、それともモデルをすべてのデータでトレーニングしますか？\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## ShelterWTopic 著者\n",
    "> > > > \n",
    "> > > > トレーニングの80%を使用し、評価の20%を使用します。\n",
    "> > > \n",
    "> > > \n",
    "> > > ## raconion\n",
    "> > > \n",
    "> > > ご確認いただきありがとうございます :)\n",
    "> > > \n",
    "> > > \n",
    "> > > \n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
