{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9c361ce",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックでは、Gemma-2 9bモデルのトレーニング方法とその結果について説明されています。このノートブックの主な目的は、大規模言語モデル（LLM）のファインチューニングにおける最新の手法であるQLoRA (Quantized Low-Rank Adaptation)を使用して、競技会でのパフォーマンスを向上させることです。\n",
    "\n",
    "### 問題点\n",
    "ノートブックは、ユーザーが好みのチャットボット応答を予測するために、Gemma-2のトレーニングによる評価パフォーマンスを改善しようとしています。具体的には、評価セットのロスを最小化することで、実際の競技会や評価基準のスコア（LB: 0.941）を向上させることを目指しています。\n",
    "\n",
    "### 使用手法とライブラリ\n",
    "以下の手法およびライブラリが使用されています。\n",
    "\n",
    "1. **QLoRA**: 低ランクの適応法を用いてモデルのトレーニングを効率化し、大規模モデルのメモリ使用を減らします。モデルの重みを量子化しつつトレーニングを行うため、計算コストを削減します。\n",
    "2. **LoRA**: 通常のファインチューニングの代わりに、特定の層に小さな行列を介してパラメータを調整することで、トレーニング中に元の重みの更新を最小限に抑えます。\n",
    "3. **Hugging Face Transformers**: 一般的なトランスフォーマーモデルのフレームワークとして使用され、`Gemma2ForSequenceClassification`などの特定のモデルやトークナイザーが利用されています。\n",
    "4. **PyTorch**: モデルのトレーニングと評価に使用される深層学習ライブラリです。\n",
    "5. **Sklearn**: 精度や対数損失の計算を行うために用いられています。\n",
    "\n",
    "### ノートブックのフロー\n",
    "- **データ準備**: Kaggleのデータセットを読み込み、カスタムトークナイザーを設定してデータを前処理します。\n",
    "- **モデル設定**: Gemma-2の設定やトレーニングメソッドを指定し、LoRAおよびQLoRAのパラメータを設定します。\n",
    "- **トレーニング**: 計算されたいくつかの評価指標（例えば、ロスと精度）を用いて、モデルのトレーニングを実施します。\n",
    "- **結果の評価**: 最後に、評価セットに対するモデルのロスと精度を報告します。\n",
    "\n",
    "このノートブックは、全体のプロセスを通じて、LLMのファインチューニングを改善し、実用的なパフォーマンスを提供するための具体的なステップと結果を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4904516",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブックに含まれる専門用語に関する簡単な解説です。初心者がつまずきそうな、実務経験が少ないと馴染みのない用語に焦点を当てています。\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**:\n",
    "   LoRAは、モデルの重みを効率的に更新する技術で、大規模なモデルに対してファインチューニングを行う際、重みの更新を行うのではなく、低ランクの行列を利用して重みの変化を表現します。これにより、メモリ使用量が削減され、トレーニングの効率が向上します。\n",
    "\n",
    "2. **QLoRA (Quantized Low-Rank Adaptation)**:\n",
    "   LoRAの拡張で、量子化技術を用いてモデルの重みをさらに低精度で圧縮します。これにより、大規模モデルの必要なメモリ量を大幅に減少させながら、通常のファインチューニングに匹敵する性能を保持します。\n",
    "\n",
    "3. **モデルの凍結 (Freezing a model)**:\n",
    "   特定のパラメータや層をトレーニング中に固定し、学習を行わないようにすることです。これにより、トレーニング時のメモリ使用量を削減し、新しく追加されたパラメータにのみ学習を集中させることができます。\n",
    "\n",
    "4. **データコレーター (Data Collator)**:\n",
    "   バッチ処理の際にデータを整形する役割を持つ機能です。複数の入力をまとめて一つのバッチとして扱うために必要な前処理を行います。特にパディング（長さを揃える作業）などを自動で行ないます。\n",
    "\n",
    "5. **バッチサイズ (Batch Size)**:\n",
    "   モデルが一度に処理するデータの数を指します。大きいバッチサイズを設定することで、トレーニングのスピードを向上させることができますが、メモリ負荷が増す可能性もあります。一方、小さいバッチサイズはメモリの負担を軽減しますが、トレーニングが遅くなることがあります。\n",
    "\n",
    "6. **勾配蓄積 (Gradient Accumulation)**:\n",
    "   小さいバッチサイズで複数回のパスを行い、その勾配を累積して、最後に一度にパラメータを更新する手法です。これにより、実質的に大きなバッチサイズを使用したかのような効果を得ることができます。\n",
    "\n",
    "7. **ドロップアウト (Dropout)**:\n",
    "   ニューラルネットワークのトレーニング時に一部のニューロンをランダムに無効化する手法です。これは過学習を防ぐために有効で、モデルの一般化性能を向上させる効果があります。\n",
    "\n",
    "8. **トークナイザー (Tokenizer)**:\n",
    "   テキストデータをモデルが処理しやすい形式に変換するためのツールです。テキストを単語やサブワードに分割し、数値として表現します。\n",
    "\n",
    "9. **フォワード/バックワード計算**:\n",
    "   フォワード計算は、モデルに入力を与えた時の出力を計算するプロセスです。バックワード計算は誤差逆伝播法を利用して、出力の誤りに基づいてモデルの重みを更新するプロセスです。\n",
    "\n",
    "10. **ファインチューニング (Fine-tuning)**:\n",
    "   すでにトレーニングされたモデルを特定のタスクに合わせて追加のトレーニングを行うプロセスです。これにより、モデルは新しいデータに対してより適した性能を発揮できます。\n",
    "\n",
    "これらの用語は、初学者が理解を深める上で重要なものですので、しっかりと理解しておくと良いでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcae374",
   "metadata": {},
   "source": [
    "## このノートブックについて\n",
    "このノートブックでは、Gemma-2 9bをどのようにトレーニングしてLB: 0.941を取得したかを示します。推論コードは[こちら](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)にあります。\n",
    "私は、unslothチームがアップロードした4ビット量子化された[Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit)をベースモデルとして使用し、LoRAアダプターを追加して1エポックのトレーニングを行いました。\n",
    "\n",
    "## 結果\n",
    "\n",
    "評価セットとして`id % 5 == 0`を使用し、残りをすべてトレーニングに使用しました。\n",
    "\n",
    "| サブセット | ロス |\n",
    "| - | - |\n",
    "| eval | 0.9371 |\n",
    "| LB | 0.941 |\n",
    "\n",
    "## QLoRAファインチューニングとは？\n",
    "\n",
    "従来のファインチューニングでは、重み（$\\mathbf{W}$）は次のように更新されます：\n",
    "\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} - \\eta \\frac{{\\partial L}}{{\\partial \\mathbf{W}}} = \\mathbf{W} + \\Delta \\mathbf{W}\n",
    "$$\n",
    "\n",
    "ここで、$L$はこのステップでの損失、$\\eta$は学習率です。\n",
    "\n",
    "[LoRA](https://arxiv.org/abs/2106.09685)は、$\\Delta \\mathbf{W} \\in \\mathbb{R}^{\\text{d} \\times \\text{k}}$を2つの（はるかに）小さな行列、$\\mathbf{B} \\in \\mathbb{R}^{\\text{d} \\times \\text{r}}$と$\\mathbf{A} \\in \\mathbb{R}^{\\text{r} \\times \\text{k}}$に因子分解して近似しようとします。ここで$r \\ll \\text{min}(\\text{d}, \\text{k})$です。\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W}_{s} \\approx \\mathbf{B} \\mathbf{A}\n",
    "$$\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/pii_data_detection/lora_diagram.png\">\n",
    "\n",
    "トレーニング中、元の重みを凍結し、$\\mathbf{A}$と$\\mathbf{B}$のみが更新されるため、トレーニング中に更新する必要がある元の重みの割合はごくわずか（例：<1%）です。この方式により、トレーニング中のGPUメモリ使用量を大幅に削減しながら、通常の（フル）ファインチューニングと同じ性能を達成できます。\n",
    "\n",
    "[QLoRA](https://arxiv.org/abs/2305.14314)は、LLMを量子化することでさらに効率を高めています。例えば、8Bパラメータモデルは32ビットで32GBのVRAMを占有しますが、量子化された8ビット/4ビットの8Bモデルはそれぞれ8GB/4GBしか必要ありません。\n",
    "QLoRAは、LLMの重みを低精度（例：8ビット）で量子化する一方で、フォワード/バックワードの計算は高精度（例：16ビット）で行い、LoRAアダプターの重みも高精度で維持されることに注意してください。\n",
    "\n",
    "4ビットでのA6000使用の1エポックは約15時間かかり、8ビットでは約24時間かかりましたが、ロスの違いは目立ちませんでした。\n",
    "\n",
    "## 注意\n",
    "Kaggleカーネルでの完全なトレーニングの実行には非常に長い時間がかかります。完全なトレーニングを実行するためには外部の計算リソースを使用することをお勧めします。\n",
    "このノートブックではデモ目的でわずか100サンプルを使用していますが、その他はすべて私の設定と同じです。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-11T07:13:50.404672Z",
     "iopub.status.busy": "2024-07-11T07:13:50.404354Z",
     "iopub.status.idle": "2024-07-11T07:14:20.94426Z",
     "shell.execute_reply": "2024-07-11T07:14:20.943289Z",
     "shell.execute_reply.started": "2024-07-11T07:13:50.404645Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# gemma-2はtransformers>=4.42.3から使用可能です\n",
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:20.946359Z",
     "iopub.status.busy": "2024-07-11T07:14:20.946064Z",
     "iopub.status.idle": "2024-07-11T07:14:38.94904Z",
     "shell.execute_reply": "2024-07-11T07:14:38.948106Z",
     "shell.execute_reply.started": "2024-07-11T07:14:20.946332Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    Gemma2ForSequenceClassification,\n",
    "    GemmaTokenizerFast,\n",
    "    Gemma2Config,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61db61d",
   "metadata": {},
   "source": [
    "### 設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:38.951055Z",
     "iopub.status.busy": "2024-07-11T07:14:38.950324Z",
     "iopub.status.idle": "2024-07-11T07:14:38.959597Z",
     "shell.execute_reply": "2024-07-11T07:14:38.958706Z",
     "shell.execute_reply.started": "2024-07-11T07:14:38.951021Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"  # 出力フォルダ\n",
    "    checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4ビット量子化されたgemma-2-9b-instruct\n",
    "    max_length: int = 1024  # 最大長\n",
    "    n_splits: int = 5  # データ分割数\n",
    "    fold_idx: int = 0  # 現在のフォールドインデックス\n",
    "    optim_type: str = \"adamw_8bit\"  # 使用するオプティマイザの種類\n",
    "    per_device_train_batch_size: int = 2  # デバイスごとのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8\n",
    "    per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ\n",
    "    n_epochs: int = 1  # エポック数\n",
    "    freeze_layers: int = 16  # 総レイヤ数は42、最初の16レイヤにはアダプターを追加しない\n",
    "    lr: float = 2e-4  # 学習率\n",
    "    warmup_steps: int = 20  # ウォームアップステップ数\n",
    "    lora_r: int = 16  # LoRAのランク\n",
    "    lora_alpha: float = lora_r * 2  # LoRAのアルファ\n",
    "    lora_dropout: float = 0.05  # LoRAのドロップアウト率\n",
    "    lora_bias: str = \"none\"  # LoRAのバイアス\n",
    "\n",
    "config = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5344ccb",
   "metadata": {},
   "source": [
    "#### トレーニング引数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:38.962833Z",
     "iopub.status.busy": "2024-07-11T07:14:38.962415Z",
     "iopub.status.idle": "2024-07-11T07:14:39.037571Z",
     "shell.execute_reply": "2024-07-11T07:14:39.036838Z",
     "shell.execute_reply.started": "2024-07-11T07:14:38.962803Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output\",  # 出力ディレクトリ\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書きする\n",
    "    report_to=\"none\",  # ログの報告先\n",
    "    num_train_epochs=config.n_epochs,  # トレーニングエポック数\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,  # グラデーション蓄積ステップ数\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ\n",
    "    logging_steps=10,  # ログの出力間隔\n",
    "    eval_strategy=\"epoch\",  # 評価戦略\n",
    "    save_strategy=\"steps\",  # 保存戦略\n",
    "    save_steps=200,  # 保存ステップ数\n",
    "    optim=config.optim_type,  # オプティマイザの種類\n",
    "    fp16=True,  # FP16を使用するか\n",
    "    learning_rate=config.lr,  # 学習率\n",
    "    warmup_steps=config.warmup_steps,  # ウォームアップステップ数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4efc3a",
   "metadata": {},
   "source": [
    "#### LoRA設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:39.038969Z",
     "iopub.status.busy": "2024-07-11T07:14:39.038641Z",
     "iopub.status.idle": "2024-07-11T07:14:39.044206Z",
     "shell.execute_reply": "2024-07-11T07:14:39.043317Z",
     "shell.execute_reply.started": "2024-07-11T07:14:39.038945Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,  # LoRAのランク\n",
    "    lora_alpha=config.lora_alpha,  # LoRAのアルファ\n",
    "    # 自己注意のみにターゲットを絞る\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # ターゲットモジュール\n",
    "    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],  # 変換対象レイヤー\n",
    "    lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率\n",
    "    bias=config.lora_bias,  # LoRAのバイアス\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c35517a",
   "metadata": {},
   "source": [
    "### トークナイザーとモデルのインスタンス化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:39.045553Z",
     "iopub.status.busy": "2024-07-11T07:14:39.0453Z",
     "iopub.status.idle": "2024-07-11T07:14:41.322578Z",
     "shell.execute_reply": "2024-07-11T07:14:41.321566Z",
     "shell.execute_reply.started": "2024-07-11T07:14:39.04553Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)  # トークナイザーの読み込み\n",
    "tokenizer.add_eos_token = True  # <eos>を末尾に追加\n",
    "tokenizer.padding_side = \"right\"  # パディングを右側に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:14:41.324066Z",
     "iopub.status.busy": "2024-07-11T07:14:41.323778Z",
     "iopub.status.idle": "2024-07-11T07:15:52.220632Z",
     "shell.execute_reply": "2024-07-11T07:15:52.219728Z",
     "shell.execute_reply.started": "2024-07-11T07:14:41.324043Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Gemma2ForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,  # モデルの読み込み\n",
    "    num_labels=3,  # 分類ラベルの数\n",
    "    torch_dtype=torch.float16,  # データ型の設定\n",
    "    device_map=\"auto\",  # デバイスマッピング\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを使用しない\n",
    "model = prepare_model_for_kbit_training(model)  # kビットトレーニングのためのモデル準備\n",
    "model = get_peft_model(model, lora_config)  # LoRAモデルの取得\n",
    "model  # モデルを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:52.222528Z",
     "iopub.status.busy": "2024-07-11T07:15:52.222259Z",
     "iopub.status.idle": "2024-07-11T07:15:52.233448Z",
     "shell.execute_reply": "2024-07-11T07:15:52.232506Z",
     "shell.execute_reply.started": "2024-07-11T07:15:52.222506Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()  # トレーニング可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c435889",
   "metadata": {},
   "source": [
    "### データセットのインスタンス化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:52.234875Z",
     "iopub.status.busy": "2024-07-11T07:15:52.234511Z",
     "iopub.status.idle": "2024-07-11T07:15:55.421506Z",
     "shell.execute_reply": "2024-07-11T07:15:55.420793Z",
     "shell.execute_reply.started": "2024-07-11T07:15:52.234845Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # CSVからデータセットを作成\n",
    "ds = ds.select(torch.arange(100))  # デモ目的で最初の100データのみを使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:55.422997Z",
     "iopub.status.busy": "2024-07-11T07:15:55.422634Z",
     "iopub.status.idle": "2024-07-11T07:15:55.43295Z",
     "shell.execute_reply": "2024-07-11T07:15:55.432028Z",
     "shell.execute_reply.started": "2024-07-11T07:15:55.422961Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase, \n",
    "        max_length: int\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer  # トークナイザーの初期化\n",
    "        self.max_length = max_length  # 最大長の設定\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:\n",
    "        prompt = [\"<prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]  # プロンプトの整形\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]  # 応答Aの整形\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]  # 応答Bの整形\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # プロンプトと応答を結合\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークナイズ\n",
    "        labels=[]\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):\n",
    "            if a_win:  # モデルAが勝った場合\n",
    "                label = 0  # ラベルは0\n",
    "            elif b_win:  # モデルBが勝った場合\n",
    "                label = 1  # ラベルは1\n",
    "            else:  # 同点の場合\n",
    "                label = 2  # ラベルは2\n",
    "            labels.append(label)  # ラベルをリストに追加\n",
    "        return {**tokenized, \"labels\": labels}  # トークン化されたデータとラベルを返す\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))  # テキストを処理して返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:55.43584Z",
     "iopub.status.busy": "2024-07-11T07:15:55.435517Z",
     "iopub.status.idle": "2024-07-11T07:15:56.036485Z",
     "shell.execute_reply": "2024-07-11T07:15:56.035519Z",
     "shell.execute_reply.started": "2024-07-11T07:15:55.435805Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化\n",
    "ds = ds.map(encode, batched=True)  # データセットにマッピング"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645436e",
   "metadata": {},
   "source": [
    "### 指標の計算\n",
    "\n",
    "LBで使用するロスと補助的な指標として精度を計算します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.037921Z",
     "iopub.status.busy": "2024-07-11T07:15:56.037608Z",
     "iopub.status.idle": "2024-07-11T07:15:56.043676Z",
     "shell.execute_reply": "2024-07-11T07:15:56.042738Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.037896Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:\n",
    "    preds = eval_preds.predictions  # 予測値の取得\n",
    "    labels = eval_preds.label_ids  # ラベルの取得\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 予測値をソフトマックスで確率に変換\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)  # ロスの計算\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度の計算\n",
    "    return {\"acc\": acc, \"log_loss\": loss}  # 精度とロスを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf35b02",
   "metadata": {},
   "source": [
    "### データ分割\n",
    "\n",
    "ここでは、トレーニングと評価を`id % 5`に基づいて分割します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.045232Z",
     "iopub.status.busy": "2024-07-11T07:15:56.044967Z",
     "iopub.status.idle": "2024-07-11T07:15:56.057744Z",
     "shell.execute_reply": "2024-07-11T07:15:56.056724Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.04521Z"
    }
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニングインデックス\n",
    "        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 評価インデックス\n",
    "    ) \n",
    "    for fold_idx in range(config.n_splits)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:15:56.059267Z",
     "iopub.status.busy": "2024-07-11T07:15:56.058902Z",
     "iopub.status.idle": "2024-07-11T07:21:12.617061Z",
     "shell.execute_reply": "2024-07-11T07:21:12.616188Z",
     "shell.execute_reply.started": "2024-07-11T07:15:56.059235Z"
    }
   },
   "outputs": [],
   "source": [
    "train_idx, eval_idx = folds[config.fold_idx]  # トレーニングと評価のインデックス取得\n",
    "\n",
    "trainer = Trainer(\n",
    "    args=training_args,  # トレーナー引数\n",
    "    model=model,  # モデル\n",
    "    tokenizer=tokenizer,  # トークナイザー\n",
    "    train_dataset=ds.select(train_idx),  # トレーニングデータセット\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセット\n",
    "    compute_metrics=compute_metrics,  # 指標計算関数\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # データのコラトレータ\n",
    ")\n",
    "trainer.train()  # トレーニングを開始"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d979e22d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Yichuan Gao\n",
    "> \n",
    "> このノートブックから結果を再現するのが難しいです。具体的には、このノートブックをダウンロードし、入力パスを変更しただけで、サンプル(100)を削除しました。\n",
    "> \n",
    "> 4090でトレーニングした後、train_lossは約1.0までしか下がらず、eval_lossも約0.98で、主張されている0.937のCVロスよりもかなり劣っています。異なるマシンの異なるGPU構成や異なるバッチサイズでトレーニングを試みましたが、すべて似たような悪い結果でした。\n",
    "> \n",
    "> 何が原因か非常に疑問に思っています。アドバイスがあれば歓迎します。\n",
    "> \n",
    "> > ## skurita\n",
    "> > \n",
    "> > 私も同様の問題を経験しています。\n",
    "> > >\n",
    "> > > ノートブックをダウンロードし、入力パスを変更し、サンプル(100)を削除しましたが、トレーニング/評価ロスは図のようにしか減少せず、主張されている0.937のCVロスよりも悪いです。\n",
    "> > >\n",
    "> > > \"unsloth/gemma-2-9b-it-bnb-4bit\"を4エポックでトレーニングし、各エポックの後にバリデーションを計算しました。\n",
    "> > >\n",
    "> > > このトレーニングノートブックから結果を再現する方法は誰か知っていますか？\n",
    "> > >\n",
    "> > > どんな洞察や提案も感謝されます。\n",
    "> > >\n",
    "> > > 著者[@emiz6413](https://www.kaggle.com/emiz6413)にも言及します。\n",
    "> > >\n",
    "> > > ありがとう。\n",
    "> > >\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## S J Moudry\n",
    "> > [@emiz6413](https://www.kaggle.com/emiz6413)  ノートブックの共有、ありがとうございます。非常に多くを学び、あなたの仕事に全て投票しました。いくつか質問があります：\n",
    "> >\n",
    "> > トレーニング後のモデルの評価精度はどのくらいでしたか？\n",
    "> > トレーニングにはどのプラットフォームを使用していますか？ おすすめのものはありますか？\n",
    "> > ハイパーパラメータの調整に関するヒントはありますか？ また、どのようにハイパーパラメータを決めましたか？ フルデータセットで何度もトレーニングしましたか、それとも効果を測るためにいくつかのテストランを行いましたか？\n",
    "> >\n",
    "> > > ## Eisuke MizutaniTopic Author\n",
    "> > > \n",
    "> > > コメントありがとうございます。\n",
    "> > > \n",
    "> > > - 1エポック後の評価セットのロスは0.9371でした。\n",
    "> > > \n",
    "> > > - 私はpaperspaceを使用しています。合理的な固定価格で多くの実験を行いたい場合は、そのプラットフォームをお勧めします。\n",
    "> > > \n",
    "> > > - ハイパーパラメータの調整にはあまり時間をかけておらず、gemma2のトレーニングには非常に長い時間がかかります。いくつかの実行を行い、学習曲線を見て、不可能そうなものは手動で中止しました。したがって、ハイパーパラメータを調整する余地はたくさんあると思います。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Lorry Zou\n",
    "> \n",
    "> なぜ以前公表したLlama3のトレーニングノートブックを再利用しなかったのですか？ モデル名/パスをllama3からgemma2に変更するだけでTPUで約6時間トレーニングできます。\n",
    "> \n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > あなたが言っているのはkishanvavdaraのノートブックだと思います（私はトレーニングノートブックを発表していません）。\n",
    "> > >\n",
    "> > > 私はGPUでモデルをトレーニングしており、TPUで再現するかどうかは不明でした。 特に量子化はTPUではサポートされていません。 OOMが発生しない場合は量子化は必要ありませんが。\n",
    "> > >\n",
    "> > > あなたはTPUでgemma2をトレーニングすることができますか？\n",
    "> > >\n",
    "> > > > ## Lorry Zou\n",
    "> > > > はい、私はKishanvavdaraのノートブックについて話していました。彼は以前に2つのGemma 2ノートブックを投稿しました[https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1](url)\n",
    "> > > > アップロードされたデータセットとして\"gemma-2-9b-hf\"が含まれています。 TPUでトレーニングでき、早かったです。\n",
    "> > > > \n",
    "> > > > > ## Eisuke MizutaniTopic Author\n",
    "> > > > > > あなたはgemmaの最終隠れ状態の上にcatboost（または何かしらの）分類器をトレーニングしただけですか？\n",
    "> > > > > > gemma2-9bの読み込みを試みるとOOMが発生し、量子化はTPUではサポートされていません。したがって、gemmaの重みを微調整するためにTPUの使用をあきらめざるを得ませんでした。\n",
    "> > > > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Muhammad Haroon ul Hasnain\n",
    "> \n",
    "> 素晴らしいノートブックと説明をありがとうございます。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Mohamadreza Momeni\n",
    "> \n",
    "> 素晴らしい仕事です。\n",
    "> >\n",
    "> > 親愛なる[@emiz6413](https://www.kaggle.com/emiz6413)への偉業です。\n",
    "\n",
    "---\n",
    "\n",
    "> ## floriandev\n",
    "> \n",
    "> まず第一に[@emiz6413](https://www.kaggle.com/emiz6413)  このノートブックには大きな感謝を!!\n",
    "> \n",
    "> 不幸にも、ファインチューニングの後、モデルをハギングフェイスにプッシュして、hfからのモデルを使用して推論ノートブックを呼び出す際に次のエラーが発生しました：\n",
    "> \n",
    "> …このエラーが発生します…\n",
    "> \n",
    "> > ## Lorry Zou\n",
    "> > \n",
    "> > num_classes=3を指定しないと、デフォルト値は2になります。\n",
    "> > \n",
    "> > > ## floriandev\n",
    "> > > > こんにちはLorry、迅速な返信ありがとうございます!!\n",
    "> > > > \n",
    "> > > > トレーニング/推論ノートブックは同じ著者のものです。変更を加えずにトレーニングに使用したため、精度やロスは問題なかったのですが、保存後に推論ノートブックに読み込むとエラーが出ます。\n",
    "> > > > \n",
    "> > > > トレーニングされたモデルは、アーキテクチャ的に推論モデルと一致するはずです...うーん\n",
    "> > > >\n",
    "> > > > 推論ノートブックでクラス数をどのように導入するか、ということですが、新たにトレーニングされたモデルには1つの追加のパラメータがあるようです。\n",
    "> > > > \n",
    "> > > > 教えてください、サポートの感謝します。\n",
    "> > > > \n",
    "> > > > フロリアン。\n",
    "> > > > \n",
    "> > > > > ## floriandev\n",
    "> > > > > ああ、ローカルで生成されたチェックポイントを使用して（現在試しています）見つかりました...すぐに報告します！\n",
    "> > > > \n",
    "> > > > > ## floriandev\n",
    "> > > > > トレーニング出力がgemma_dirに使用されたため、問題が解決できます。\n",
    "> > > > >\n",
    "> > > > > 解決策：トレーニング出力をlora_dirとして使用しました。\n",
    "> > > > >\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Nikhil Tumbde\n",
    "> \n",
    "> ノートブックに感謝します。\n",
    "> \n",
    "> fp16のトレーニング引数について質問がありますが、ファインチューニングの際にbf16を使用したらどうなりますか？ llama 3 8b（ベースモデル）で4ビットの量子化を使用したとき、数千ステップ後に勾配がNaNになりました。何か考えがありますか？\n",
    "\n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > 私のデバイスで利用可能な場合は、bf16が一般的に安全なオプションだと思います。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Vavilkin Alexander\n",
    "> \n",
    "> こんにちは[@emiz6413](https://www.kaggle.com/emiz6413)! あなたのノートブックに感謝します、これはLLMのファインチューニングの実践に非常に役立ちます。モデル全体をどのように保存したか教えていただけますか？ \"Trainer\"で微調整する際、アダプターの重みしか保存されないと理解していますが、分類ヘッドの重みは保存されません。同時に推論ノートブックのモデルは通常の量子化モデルではなく、すでに分類用のヘッドが付いています。このことについてあなたからのコメントを非常に楽しみにしています。\n",
    "\n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > 分類ヘッドはModulesToSaveWrapperとしてラップされているため、Trainerによって自動的に保存されます。\n",
    "> > >\n",
    "> > > チェックポイントからロードすると、トレーニングされた重みが分類ヘッドに読み込まれます。\n",
    "> > >\n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Mattia Vanzetto\n",
    "> \n",
    "> 質問してもいいですか？私はこれに対して初心者ですが、ノートブックが完了し出力が得られたら、出力を保存し、推論ノートブックで再読み込みする手順は何ですか？ ありがとうございます！\n",
    "\n",
    "> > ## raconion\n",
    "> > \n",
    "> > あなたのノートブックと同じレベルに出力フォルダがあるはずです。推論には最も大きな番号（ステップ）のサブフォルダを使用してください。この場合、5748であるべきです。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## raconion\n",
    "> \n",
    "> こんにちは！ この素晴らしいノートブックをありがとう。私はH100*2でこのノートブックを実行した結果、あなたの結果では5748の代わりに11495ステップが出ました。また、cvスコアはやや悪化し0.964697です。\n",
    "> \n",
    "> いくつかのコメントを読んだ後、バッチサイズに問題があると感じました。\n",
    "> \n",
    "> 私の設定は次の通りです。\n",
    "> \n",
    "> ```\n",
    "> class Config:\n",
    ">     output_dir: str = \"output\"  \n",
    ">     checkpoint: str = \"unsloth/gemma-2-9b-it-bnb-4bit\"  # 4ビット量子化されたgemma-2-9b-instruct\n",
    ">     max_length: int = 1024\n",
    ">     n_splits: int = 5\n",
    ">     fold_idx: int = 0\n",
    ">     optim_type: str = \"adamw_8bit\"\n",
    ">     per_device_train_batch_size: int = 2 \n",
    ">     gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8\n",
    ">     per_device_eval_batch_size: int = 8\n",
    ">     n_epochs: int = 1\n",
    ">     freeze_layers: int = 16  # 総レイヤ数は42、最初の16層にはアダプターを追加しない\n",
    ">     lr: float = 2e-4\n",
    ">     warmup_steps: int = 20\n",
    ">     lora_r: int = 16\n",
    ">     lora_alpha: float = lora_r * 2 \n",
    ">     lora_dropout: float = 0.05\n",
    ">     lora_bias: str = \"none\" \n",
    "> ```\n",
    "> \n",
    "> そして、\n",
    "> \n",
    "> ```\n",
    "> training_args = TrainingArguments(\n",
    ">     output_dir=\"output\",\n",
    ">     overwrite_output_dir=True,\n",
    ">     report_to=\"none\",\n",
    ">     num_train_epochs=config.n_epochs,\n",
    ">     per_device_train_batch_size=config.per_device_train_batch_size,\n",
    ">     gradient_accumulation_steps=config.gradient_accumulation_steps,\n",
    ">     per_device_eval_batch_size=config.per_device_eval_batch_size,\n",
    ">     logging_steps=10,\n",
    ">     eval_strategy=\"epoch\",\n",
    ">     save_strategy=\"steps\",\n",
    ">     save_steps=200,\n",
    ">     optim=config.optim_type,\n",
    ">     fp16=True,\n",
    ">     learning_rate=config.lr,\n",
    ">     warmup_steps=config.warmup_steps,\n",
    "> )\n",
    "> ```\n",
    "> \n",
    "> モデルを実際にトレーニングする際に異なる設定を使用していますか？ ありがとう！\n",
    "\n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > あなたのコードで両方のGPUが表示されていますか？\n",
    "> > > \n",
    "> > \n",
    "\n",
    "> > > > ## raconion\n",
    "> > > > ノートブックのコードを直接再利用しています。デバイスマップによるとモデルの並列化が使用されているようです。nvidia-smiも両方のGPUが使用されていることを示しています。\n",
    "> > > > \n",
    "> > > > デバイスマップ：\n",
    "> > > > \n",
    "> > > > ```\n",
    "> > > > {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 1, 'model.norm': 1, 'score': 1}\n",
    "> > > > ```\n",
    "> > > > \n",
    "> > > > もしこの場合、以下のように\n",
    "> > > > \n",
    "> > > > ```\n",
    "> > > > per_device_train_batch_size: int = 2\n",
    "> > > > gradient_accumulation_steps: int = 2\n",
    "> > > > ```\n",
    "> > > > グローバルバッチサイズは4になります。トレーニング時にグローバルバッチサイズが8になるようにper_device_train_batch_sizeとgradient_accumulation_stepsを異なる値に設定しますか？\n",
    "> > > > \n",
    "> > > > > ## Eisuke MizutaniTopic Author\n",
    "> > > > > > [@raconion](https://www.kaggle.com/raconion) \n",
    "> > > > > > 応答が遅れてすみません。\n",
    "> > > > > > \n",
    "> > > > > > グローバルバッチサイズが8になるようにper_device_train_batch_sizeとgradient_accumulation_stepsを設定しています。グローバルバッチサイズが4だとスコアが悪化しました。\n",
    "> > > > > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## HZM\n",
    "> \n",
    "> こんにちはEisuke、Llama3などの他のllmを試しましたか？ それは私にはうまくいきませんでした。\n",
    "> \n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > Llama3を試しましたが、約0.98になりました。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Kim Kumuha\n",
    "> \n",
    "> トレーニングにはどれくらいの時間がかかりましたか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## yuanzhe zhou\n",
    "> \n",
    "> こんにちは、ノートブックの1エポック実行後の結果はどうなりますか？\n",
    "\n",
    "> > ## Eisuke MizutaniTopic Author\n",
    "> > \n",
    "> > 評価セット(id % 5 == 0)でのロスは0.9371です。\n",
    "> > >\n",
    "> > > チェックポイントを使用して結果を再現できます[こちら](https://www.kaggle.com/datasets/emiz6413/73zap2gx/data)。\n",
    "> > >\n",
    "> > > \n",
    "> > \n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
