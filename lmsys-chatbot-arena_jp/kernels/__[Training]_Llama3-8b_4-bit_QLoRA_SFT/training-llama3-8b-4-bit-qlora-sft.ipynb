{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "067c5651",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションにおける人間の好み予測のための機械学習モデルのトレーニングに取り組んでいます。具体的には、Llama-3言語モデルを基にしたファインチューニングを行い、与えられたプロンプトに対してユーザーが好む応答のモデルを構築しています。\n",
    "\n",
    "### 問題の背景\n",
    "本コンペティションでは、ユーザーが提示したプロンプトに対する異なる応答の中から、どちらの応答が好まれるかを予測することが求められます。この予測は、ログ損失を用いて評価されます。\n",
    "\n",
    "### 手法\n",
    "このNotebookでは以下の手法とライブラリを使用しています：\n",
    "\n",
    "- **ライブラリ**:\n",
    "  - `transformers`: 大規模言語モデルやトークナイザーを用いるために使用。\n",
    "  - `datasets`: データの読み込みと前処理を行うために利用。\n",
    "  - `scikit-learn`: ログ損失や精度などのメトリクス計算を行うために使用。\n",
    "  - `torch`: PyTorchによるニューラルネットワークの構築および訓練に使用。\n",
    "\n",
    "- **モデル**:\n",
    "  - Llama-3ベースモデル（`llama-3-8b-Instruct-bnb-4bit`）を利用し、これをファインチューニングして人間の選好を予測するモデルを構築しています。\n",
    "\n",
    "- **データ処理**:\n",
    "  - データセットからプロンプトと応答を抽出し、それらをトークン化。\n",
    "  - 各応答に対するラベル（「winner_model_a」「winner_model_b」「winner_tie」）を生成。\n",
    "\n",
    "- **トレーニング戦略**:\n",
    "  - 交差検証を用いてモデルの性能を評価。\n",
    "  - 勾配蓄積やバッチサイズ、エポック数を調整し、効率的にトレーニング。\n",
    "\n",
    "- **評価**:\n",
    "  - 訓練後、ログ損失と精度を検証し、最終的なパフォーマンスを評価しています。\n",
    "\n",
    "### 結果\n",
    "Notebookの結果セクションでは、推論コードへのリンクと評価データセットおよびリーダーボードでのログ損失（Eval: 0.9231, LB: 0.936）が示されています。また、成功裏に再現可能な設定として、使用するデータやハイパーパラメータ（例：トレーニング中のバッチサイズ、エポック数など）が明記されています。\n",
    "\n",
    "このNotebookは、提供された情報に基づいて、ユーザーに好まれるチャットボットの応答を予測するための効率的な機械学習アプローチを示しており、実用的な応用が期待されます。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51103eb",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づいて、機械学習・深層学習初心者がつまずきそうな専門用語の簡単な解説です。\n",
    "\n",
    "1. **Log Loss**:\n",
    "   - **解説**: ログ損失は、分類問題の評価指標の一つ。予測確率と実際のラベルとの間の対数損失を計算します。小さいほど良いモデルを示します。\n",
    "\n",
    "2. **Label Encoder**:\n",
    "   - **解説**: ラベルエンコーダは、カテゴリ変数を数値形式に変換するためのツール。例えば、文字列のクラスラベルを整数に変換し、機械学習モデルに入力できるようにします。\n",
    "\n",
    "3. **Causal Language Model (CAUSAL_LM)**:\n",
    "   - **解説**: 原因のある言語モデルで、与えられた文脈に基づいて次に来る単語を予測するモデル。例えば、文中の前の単語を考慮しながら次の単語を生成します。\n",
    "\n",
    "4. **Lora**:\n",
    "   - **解説**: Lora（Low-Rank Adaptation）は大規模なモデルのトレーニングを効率化する手法。モデルの重みを低ランクのパラメータで補正し、計算コストを削減します。\n",
    "\n",
    "5. **k-bit Training**:\n",
    "   - **解説**: k-bitトレーニングは、モデルの重みをkビットで表現する手法。これにより、メモリ消費量を削減し、高速でトレーニングが行えるようになります。\n",
    "\n",
    "6. **Attention Mask**:\n",
    "   - **解説**: 注意マスクは、トークン間での注意計算において、どのトークンを無視するべきかを示すマスク。無視したいトークンをマスクすることで、モデルが効率的に学習できるようにします。\n",
    "\n",
    "7. **Gradient Accumulation**:\n",
    "   - **解説**: 勾配蓄積は、通常のバッチサイズよりも小さいバッチでトレーニングを行った後、複数の勾配を蓄積して一度に更新する手法。これにより、大規模データセットでのメモリ効率を改善できます。\n",
    "\n",
    "8. **Data Collator**:\n",
    "   - **解説**: データコレータは、ミニバッチを作成するためのツール。異なるサイズの入力を均一なバッチに整形し、モデルに渡す役割を担います。\n",
    "\n",
    "9. **EvalPrediction**:\n",
    "   - **解説**: EvalPredictionは、評価プロセス中に使われるデータ構造で、モデルの予測と関連ラベルを含む。これにより評価メトリクスを計算できます。\n",
    "\n",
    "10. **Softmax Function**:\n",
    "    - **解説**: ソフトマックス関数は、モデルの出力を確率に変換するための関数。出力の和が1になるように正規化し、クラス分類タスクにおいて有用です。\n",
    "\n",
    "これらの用語は、Jupyter Notebookの特定の操作や機能に関連しており、初心者が理解しにくい部分です。十分な前知識を持つ読者にとっても重要な概念となるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689abaf",
   "metadata": {},
   "source": [
    "## 結果\n",
    "- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    \n",
    "\n",
    "- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)\n",
    "\n",
    "| サブセット | log loss |\n",
    "| - | - |\n",
    "| Eval | 0.9231 |\n",
    "| LB | 0.936 |\n",
    "\n",
    "## 注意\n",
    "コードを再現したい場合は、以下の点に注意してください：\n",
    "- すべてのデータを使用する\n",
    "- per_device_train_batch_size=4を設定する\n",
    "- 1エポックのトレーニングにはA10を使用して約15時間かかりました\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:00.436001Z",
     "iopub.status.busy": "2024-07-23T02:48:00.435641Z",
     "iopub.status.idle": "2024-07-23T02:48:30.272547Z",
     "shell.execute_reply": "2024-07-23T02:48:30.271445Z",
     "shell.execute_reply.started": "2024-07-23T02:48:00.435972Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:30.274819Z",
     "iopub.status.busy": "2024-07-23T02:48:30.274517Z",
     "iopub.status.idle": "2024-07-23T02:48:48.854217Z",
     "shell.execute_reply": "2024-07-23T02:48:48.853427Z",
     "shell.execute_reply.started": "2024-07-23T02:48:30.274791Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fb69c5",
   "metadata": {},
   "source": [
    "### 設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:48.855763Z",
     "iopub.status.busy": "2024-07-23T02:48:48.855193Z",
     "iopub.status.idle": "2024-07-23T02:48:52.431712Z",
     "shell.execute_reply": "2024-07-23T02:48:52.430895Z",
     "shell.execute_reply.started": "2024-07-23T02:48:48.855736Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(100)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4b83e6",
   "metadata": {},
   "source": [
    "### トークナイザーとデータセットの準備、メトリクス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:52.435094Z",
     "iopub.status.busy": "2024-07-23T02:48:52.434437Z",
     "iopub.status.idle": "2024-07-23T02:48:56.08418Z",
     "shell.execute_reply": "2024-07-23T02:48:56.083219Z",
     "shell.execute_reply.started": "2024-07-23T02:48:52.435058Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "# ラベルに対応するトークンIDを取得\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "# トークン化の関数を定義\n",
    "def tokenize(example, tokenizer):\n",
    "    # プロンプトと応答をトークン化\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    # トークンIDの長さが最大長を超える場合、トリミング\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        \n",
    "    # 追加のプロンプトを定義\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # ラベルのトークンIDを取得\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.086412Z",
     "iopub.status.busy": "2024-07-23T02:48:56.086086Z",
     "iopub.status.idle": "2024-07-23T02:48:56.724024Z",
     "shell.execute_reply": "2024-07-23T02:48:56.723106Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.086368Z"
    }
   },
   "outputs": [],
   "source": [
    "# データを読み込み、トークン化する関数を定義\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)  # pandas DataFrameをDatasetに変換\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize,  # トークナイズ関数を適用\n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "# メトリクスを計算する関数を定義\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred  # 予測とラベルを取得\n",
    "    preds = logits.argmax(axis=-1)  # 最も高いロジットを持つインデックスを予測\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])  # ラベルをインデックスにマッピング\n",
    "    \n",
    "    acc = accuracy_score(labels, preds)  # 精度を計算\n",
    "    probs = softmax(logits, axis=-1)  # ソフトマックスを計算\n",
    "    log_loss_ = log_loss(labels, probs)  # log lossを計算\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}\n",
    "\n",
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)  # データの読み込み\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)  # n分割交差検証のインデックスを生成\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを分ける"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68bc5b11",
   "metadata": {},
   "source": [
    "### モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.725702Z",
     "iopub.status.busy": "2024-07-23T02:48:56.725215Z",
     "iopub.status.idle": "2024-07-23T02:48:56.740638Z",
     "shell.execute_reply": "2024-07-23T02:48:56.739744Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.725661Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みを共有するためのキー\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)  # Llamaモデルの初期化\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズの取得\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 線形層の初期化\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        \n",
    "        hidden_states = outputs[0]  # 隠れ状態を取得\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # 隠れ状態からロジットを取得\n",
    "        logits = logits.float()  # ロジットを浮動小数点に変換\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # トークンをずらして予測に使用\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # トークンをフラット化\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # モデルの並列処理を有効にする\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)  # ロスを計算\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.742612Z",
     "iopub.status.busy": "2024-07-23T02:48:56.741994Z",
     "iopub.status.idle": "2024-07-23T02:48:56.752799Z",
     "shell.execute_reply": "2024-07-23T02:48:56.752051Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.742586Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:48:56.754274Z",
     "iopub.status.busy": "2024-07-23T02:48:56.75395Z",
     "iopub.status.idle": "2024-07-23T02:49:25.370393Z",
     "shell.execute_reply": "2024-07-23T02:49:25.369414Z",
     "shell.execute_reply.started": "2024-07-23T02:48:56.754242Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/kaggle/working/model\"\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを使用しないよう設定\n",
    "model = prepare_model_for_kbit_training(model)  # k-bitトレーニングのためにモデルを準備\n",
    "model = get_peft_model(model, peft_config)  # Loraモデルを取得\n",
    "print(model)  # モデルの構造を表示\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38267f7",
   "metadata": {},
   "source": [
    "#### 学習引数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:49:25.372766Z",
     "iopub.status.busy": "2024-07-23T02:49:25.372167Z",
     "iopub.status.idle": "2024-07-23T02:49:25.405271Z",
     "shell.execute_reply": "2024-07-23T02:49:25.404419Z",
     "shell.execute_reply.started": "2024-07-23T02:49:25.37273Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir = True,  # 出力ディレクトリを上書き\n",
    "    evaluation_strategy = \"epoch\",  # 評価戦略\n",
    "    save_strategy = \"steps\",  # 保存戦略\n",
    "    save_steps=200,  # 保存するステップ数\n",
    "    save_total_limit=1,  # 保存するモデルの最大数\n",
    "    logging_strategy=\"steps\",  # ロギング戦略\n",
    "    logging_steps=10,  # ロギングするステップ数\n",
    "    warmup_steps=20,  # ウォームアップステップ数\n",
    "    optim=\"adamw_8bit\",  # オプティマイザ\n",
    "    learning_rate=2e-4,  # 学習率\n",
    "    per_device_train_batch_size=2,  # デバイスごとの訓練バッチサイズ\n",
    "    per_device_eval_batch_size=4,  # デバイスごとの評価バッチサイズ\n",
    "    gradient_accumulation_steps=2,  # 勾配蓄積ステップ数\n",
    "    num_train_epochs=1,  # 訓練エポック数\n",
    "    fp16=True,  # 16ビット浮動小数点の使用\n",
    "    metric_for_best_model=\"log_loss\",  # ベストモデルのメトリック\n",
    "    greater_is_better = False,  # メトリックが大きい方が良いかどうか\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56bd691",
   "metadata": {},
   "source": [
    "### 学習中 !\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:49:25.408265Z",
     "iopub.status.busy": "2024-07-23T02:49:25.407977Z",
     "iopub.status.idle": "2024-07-23T02:56:58.882175Z",
     "shell.execute_reply": "2024-07-23T02:56:58.88118Z",
     "shell.execute_reply.started": "2024-07-23T02:49:25.40824Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),  # トレーニングデータセット\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセット\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータ\n",
    "    compute_metrics=compute_metrics,  # メトリクス計算の関数\n",
    ")\n",
    "trainer.train()  # 学習を開始"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:58:21.537341Z",
     "iopub.status.busy": "2024-07-23T02:58:21.536598Z",
     "iopub.status.idle": "2024-07-23T02:58:22.106251Z",
     "shell.execute_reply": "2024-07-23T02:58:22.105397Z",
     "shell.execute_reply.started": "2024-07-23T02:58:21.537313Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('pretrained_model')  # 学習したモデルを保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T02:56:58.885003Z",
     "iopub.status.busy": "2024-07-23T02:56:58.883844Z",
     "iopub.status.idle": "2024-07-23T02:56:58.891142Z",
     "shell.execute_reply": "2024-07-23T02:56:58.890124Z",
     "shell.execute_reply.started": "2024-07-23T02:56:58.884966Z"
    }
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# # 事前に学習されたモデルとLoraアダプタのパス\n",
    "# model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "# lora_adapter_path = \"/kaggle/input/model-1\"\n",
    "\n",
    "# # ベースモデルをロード\n",
    "# model_1 = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# # トークナイザーをロード\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# # Loraの設定\n",
    "# lora_config = LoraConfig(\n",
    "#     r=8,            # Loraのランク\n",
    "#     lora_alpha=16,  # Loraのスケーリングファクター\n",
    "#     task_type=TaskType.CAUSAL_LM  # モデルのタスクタイプ\n",
    "# )\n",
    "\n",
    "# # モデルをk-bitトレーニング用に準備\n",
    "# model_1 = prepare_model_for_kbit_training(model_1)\n",
    "\n",
    "# # モデルにLoraアダプタを適用\n",
    "# model_1 = get_peft_model(model_1, lora_config)\n",
    "\n",
    "# # 保存されたLoraアダプタのパラメータをロード\n",
    "# model_1.load_adapter(lora_adapter_path, adapter_name=\"test\")\n",
    "\n",
    "# # モデルが使用可能になった\n",
    "# model_1.eval()  # 評価モードに設定\n",
    "\n",
    "# # 例文をトークン化してテキストを生成\n",
    "# sentence = \"Hello, how are you?\"\n",
    "# inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "# outputs = model_1.generate(**inputs)\n",
    "\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-23T01:36:23.566344Z",
     "iopub.status.busy": "2024-07-23T01:36:23.565754Z",
     "iopub.status.idle": "2024-07-23T01:36:23.570455Z",
     "shell.execute_reply": "2024-07-23T01:36:23.569455Z",
     "shell.execute_reply.started": "2024-07-23T01:36:23.566313Z"
    }
   },
   "outputs": [],
   "source": [
    "# !zip -r model_2.zip /kaggle/working/saved_model_2"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5430972,
     "sourceId": 9013695,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
