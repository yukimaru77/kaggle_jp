{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbad83f",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、ユーザーの好みを予測する問題に取り組んでいます。具体的には、複数の大規模言語モデル（LLM）からの応答に対して、どの応答がより好まれるかを予測するためのモデルを構築しています。\n",
    "\n",
    "### 主な内容と手法\n",
    "\n",
    "1. **データの前処理**: \n",
    "   - `pandas`を用いてテストデータを読み込み、各プロンプトと応答のテキストを処理する関数を定義します。この処理では、特定の文字を除去し、テキストをクレンジングしてから新しいカラムに格納しています。\n",
    "\n",
    "2. **カスタムデータセットの作成**:\n",
    "   - `torch.utils.data.Dataset`を継承した`Senmamtic_news`クラスを作成し、プロンプトと応答をトークナイズ（BERTトークナイザーを使用）して、モデルへの入力形式に整えています。\n",
    "   - 応答が長すぎる場合には適切にトリミングを行う処理を実装しています。\n",
    "\n",
    "3. **データローダーの取得**:\n",
    "   - データローダーは、バッチ処理およびデータのシャッフルを行うための関数`get_semantic_data_loader`を通じて取得され、トレーニングとテストの両方のモードに対応しています。\n",
    "\n",
    "4. **モデルの定義と推論**:\n",
    "   - `BertPET`クラスとして、事前学習済みのBERTモデルを利用したマスク言語モデルを定義しています。\n",
    "   - モデルを使用して推論を行い、得られた確率的出力を`inference`関数で計算します。\n",
    "\n",
    "5. **出力結果の生成**:\n",
    "   - 最終的に、テストデータに対する予測結果を保存するために、提出用のCSVファイル`submission.csv`を作成しています。出力フォーマットはコンペティションで要求される形式に従っています。\n",
    "\n",
    "### 使用ライブラリ\n",
    "- `transformers`（BERTモデル用）\n",
    "- `torch`（PyTorch）\n",
    "- `pandas`（データ処理）\n",
    "- `tqdm`（進捗表示）\n",
    "\n",
    "このノートブックは、言語モデルとユーザーの選好を結びつけるための強力な機械学習技術を活用しており、チャットボットの応答の質を向上させるための具体的なアプローチを示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e59d1",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語の簡単な解説を挙げます。特に、実務経験が少ない方や、このノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **トークナイジング (Tokenization)**:\n",
    "   - 自然言語処理(NLP)において、文章を構成する単語やサブワードに分割するプロセスです。モデルは整数の配列でデータを扱うため、テキストをトークンと呼ばれる小さな単位に変換する必要があります。\n",
    "\n",
    "2. **マスクトークン (Mask Token)**:\n",
    "   - 言語モデルの学習において、特定の単語を予測するためにその単語を隠すために使われる特殊なトークンのことです。モデルはこのマスクされた部分を推測するように訓練されます。BERTなどのモデルで一般的に使用されます。\n",
    "\n",
    "3. **パディング (Padding)**:\n",
    "   - 異なる長さのシーケンス（例: トークンのリスト）を固定の長さに揃えるために、特別なトークン（通常はゼロトークン）を追加するプロセスです。これにより、バッチ処理が可能になります。\n",
    "\n",
    "4. **アテンションマスク (Attention Mask)**:\n",
    "   - トランスフォーマー系のモデルで使用され、どのトークンに注意を向けるかを示すためのマスクです。通常、トークンが存在する場合は1、存在しない場合は0の値を持ちます。\n",
    "\n",
    "5. **ロジット (Logits)**:\n",
    "   - モデルの出力層から得られる、各クラス（またはトークン）に対する未正規化のスコアのことです。これをソフトマックス関数に通すことで、クラスの確率に変換できます。\n",
    "\n",
    "6. **データローダー (DataLoader)**:\n",
    "   - PyTorchにおけるデータ操作のためのツールで、データセットをバッチ処理に分けて効率良く読み込むためのクラスです。データのシャッフルや並行処理も可能です。\n",
    "\n",
    "7. **Masked Language Model (MLM)**:\n",
    "   - 文中の特定のトークンを隠し、そのトークンを予測することを目的とした言語モデルです。BERTなどのトランスフォーマーモデルがこのアプローチを使用しています。\n",
    "\n",
    "8. **注意機構 (Attention Mechanism)**:\n",
    "   - 特にトランスフォーマーにおいてよく用いられる手法で、入力シーケンスの異なる部分に異なる重みを与えて、モデルが文脈に基づいて情報を処理することを可能にします。\n",
    "\n",
    "9. **ファインチューニング (Fine-tuning)**:\n",
    "   - 事前学習済みのモデルを特定のタスク向けに再調整するプロセスです。一般的なデータセットで学習したモデルを、特定のデータセットに合わせて微調整します。\n",
    "\n",
    "10. **トランスフォーマー (Transformer)**:\n",
    "    - 自然言語処理において非常に人気のあるモデルアーキテクチャで、自己注意機構を利用しています。BERTやGPTなど、多くの最新のNLPモデルはこのアーキテクチャに基づいています。\n",
    "\n",
    "これらの用語は、ノートブックのコードや処理フローと関連しており、特に初学者や実務経験が浅い方によくつまずくポイントです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:31:27.934536Z",
     "iopub.status.busy": "2024-07-17T06:31:27.934207Z",
     "iopub.status.idle": "2024-07-17T06:31:31.786168Z",
     "shell.execute_reply": "2024-07-17T06:31:31.785184Z",
     "shell.execute_reply.started": "2024-07-17T06:31:27.934512Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import zip_longest\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import BertTokenizer, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import pandas as pd\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# データを処理する関数\n",
    "def process(example):\n",
    "    # プロンプトの文をリストに変換し、特定の文字を取り除く\n",
    "    sentences = [s.strip('\"').replace('[MASK]','') for s in example['prompt'].strip('[]').split('\",\"')]\n",
    "    # 応答Aの文をリストに変換し、特定の文字を取り除く\n",
    "    sentences_a = [s.strip('\"').replace('[MASK]','') for s in example['response_a'].strip('[]').split('\",\"')]\n",
    "    # 応答Bの文をリストに変換し、特定の文字を取り除く\n",
    "    sentences_b = [s.strip('\"').replace('[MASK]','') for s in example['response_b'].strip('[]').split('\",\"')]\n",
    "    # プロンプトと応答Aの文を組み合わせ、空の文は除外\n",
    "    texts_a = [p for pair in zip_longest(sentences, sentences_a, fillvalue='') for p in pair if p]\n",
    "    # プロンプトと応答Bの文を組み合わせ、空の文は除外\n",
    "    texts_b = [p for pair in zip_longest(sentences, sentences_b, fillvalue='') for p in pair if p]\n",
    "    # プロセス結果をデータシリーズとして返す\n",
    "    return pd.Series([' '.join(sentences), ' '.join(sentences_a), ' '.join(sentences_b), '\\n'.join(texts_a), '\\n'.join(texts_b)], index=['prompt', 'response_a', 'response_b', 'text_a','text_b'])\n",
    "\n",
    "# データフレームに対して処理関数を適用\n",
    "test[['prompt', 'response_a', 'response_b', 'text_a','text_b']] = test.apply(process, axis=1)\n",
    "# 欠損値を含む行を削除\n",
    "test = test.dropna()\n",
    "\n",
    "# 自作データセットクラス\n",
    "class Senmamtic_news(Dataset):\n",
    "    def __init__(self, data, tokenizer):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.sep_token = tokenizer.sep_token\n",
    "        self.mask_token = tokenizer.mask_token\n",
    "\n",
    "        prompt_new = []\n",
    "        # プロンプトのトークナイジング\n",
    "        tk0 = tqdm(data['prompt'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            # プロンプトが長すぎる場合、前半と後半を結合する\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            prompt_new.append(text)\n",
    "        \n",
    "        print(f'== response_a ==')\n",
    "        response_a_new = []\n",
    "        # 応答Aのトークナイジング\n",
    "        tk0 = tqdm(data['response_a'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            response_a_new.append(text)\n",
    "\n",
    "        print(f'== response_b ==')\n",
    "        response_b_new = []\n",
    "        # 応答Bのトークナイジング\n",
    "        tk0 = tqdm(data['response_b'].fillna(\"\").values, total=len(data))\n",
    "        for text in tk0:\n",
    "            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])\n",
    "            if (length > 512):\n",
    "                text = tokenizer.convert_tokens_to_string(\n",
    "                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])\n",
    "            response_b_new.append(text)\n",
    "        \n",
    "        # 新しいプロンプトと応答をデータフレームに格納\n",
    "        self.data['prompt'] = prompt_new\n",
    "        self.data['response_a'] = response_a_new\n",
    "        self.data['response_b'] = response_b_new\n",
    "\n",
    "    def __len__(self):\n",
    "        # データの長さを返す\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # データポイントの取得\n",
    "        prompt = self.data['prompt'][idx]\n",
    "        response_a = self.data['response_a'][idx]\n",
    "        response_b = self.data['response_b'][idx]\n",
    "        system_prompt = f\"\"\"{prompt}:\n",
    "                        Response A: {response_a}\n",
    "                        Response B: {response_b}\n",
    "                        Which is better? Choose 'A', 'B', or 'both'.\n",
    "\"\"\" + self.mask_token\n",
    "\n",
    "        # モデルの入力用にトークン化\n",
    "        inputs = self.tokenizer(system_prompt, truncation=True, max_length=1600)\n",
    "        input_ids  = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "        mask_index = torch.where(input_ids == self.tokenizer.mask_token_id)[0]\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n",
    "            'mask_index': mask_index\n",
    "        }\n",
    "\n",
    "# データをバッチにするためのカスタム関数\n",
    "def collate_fn_semantic(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    mask_index = [item['mask_index'] for item in batch]\n",
    "\n",
    "    # パディングを施し、バッチ処理\n",
    "    input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True,)\n",
    "    mask_index = torch.stack(mask_index)\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'mask_index': mask_index\n",
    "    }\n",
    "\n",
    "# データローダーを取得する関数\n",
    "def get_semantic_data_loader(batch_size=32, mode='train', shuffle=True):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/deberta-small/deberta')\n",
    "    if mode == 'train':\n",
    "        dataset = Senmamtic_news(train, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn_semantic)\n",
    "        return dataloader\n",
    "    else:\n",
    "        dataset = Senmamtic_news(test, tokenizer)\n",
    "        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_semantic, drop_last=False)\n",
    "        return dataloader\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoModelForMaskedLM, AutoConfig\n",
    "from tqdm import tqdm\n",
    "\n",
    "# モデルクラスの定義\n",
    "class BertPET(nn.Module):\n",
    "    def __init__(self, model_path):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModelForMaskedLM.from_pretrained(model_path)\n",
    "        self.config = AutoConfig.from_pretrained(model_path)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, mask_index, labels=None, return_logits=False):\n",
    "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "        mask_index = mask_index.unsqueeze(-1).expand(-1, -1, output.size(-1))\n",
    "        output = torch.gather(output, 1, mask_index).squeeze(1)\n",
    "        if return_logits:\n",
    "            # 特定インデックスのlogitsを返す\n",
    "            logits = output[:,[336, 732, 462]]\n",
    "            return logits\n",
    "\n",
    "# 推論用の関数\n",
    "def inference(model, dataloader, device):\n",
    "    model.eval()\n",
    "    softmax = nn.Softmax(dim=-1)\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Inference\", leave=False):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            mask_index = batch['mask_index'].to(device)\n",
    "\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask, mask_index=mask_index, return_logits=True)\n",
    "            probs = softmax(logits)\n",
    "            probs_list = probs.cpu().numpy().tolist()\n",
    "            for i in probs_list:\n",
    "                all_probs.append(i)\n",
    "\n",
    "    return all_probs\n",
    "\n",
    "# デバイス設定\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# モデルと重みをロード\n",
    "model_path = '/kaggle/input/deberta-small/deberta'  # 事前学習済みモデルのパス\n",
    "checkpoint_path = '/kaggle/input/deberta-v3/model_checkpoint_epoch_2.pt'  # 最良重みファイルのパス\n",
    "model = BertPET(model_path)\n",
    "model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "model.to(device)\n",
    "\n",
    "# テストデータを取得\n",
    "test_loader = get_semantic_data_loader(2, mode='test')\n",
    "\n",
    "# 推論を実施\n",
    "test_probs = inference(model, test_loader, device)\n",
    "\n",
    "# 提出ファイルの作成\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "test[['winner_model_a', 'winner_model_b', 'winner_tie']] = test_probs\n",
    "sample_sub = sample_sub[['id']].merge(test[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']], on='id', how='left')\n",
    "sample_sub.to_csv('submission.csv', index=False)\n",
    "display(sample_sub.head())"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5401422,
     "sourceId": 8971973,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5401268,
     "sourceId": 8971994,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
