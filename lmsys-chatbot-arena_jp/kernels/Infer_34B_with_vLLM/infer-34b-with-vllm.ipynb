{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a47cf11",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」における、LLM（大規模言語モデル）34Bを用いた問題解決に取り組んでいます。具体的には、モデルの推論を5時間で完了させる方法を示しています。以下に、ノートブックの主な内容と使用手法について要約します。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "- **迅速なモデル提出**: 大規模言語モデルを短時間で提出する方法を求めています。具体的には、LLM 34Bモデルを用いて、提出の際に求められるレスポンスを迅速に生成することを目的としています。\n",
    "\n",
    "### 使用手法・ライブラリ\n",
    "1. **vLLM**: 高速なLLM推論ライブラリであり、推論速度を向上させるために使用されています。環境によってはエラーが出るため、再インストールが推奨されています。\n",
    "\n",
    "2. **AWQ（Adaptive Weight Quantization）**: 4ビット量子化を用いることで、GPUのVRAMを効率的に使用する手法です。これにより、大きなモデルをメモリ制約のある環境で実行可能にしています。\n",
    "\n",
    "3. **トークン制限**: \n",
    "   - **入力サイズ**: 最大1024トークンに制限し、速度を向上させます。\n",
    "   - **出力サイズ**: 生成される応答のトークンを1トークンに制限することで、処理速度を加速させています。\n",
    "\n",
    "4. **トークナイザー**: モデルに最適化されたトークナイザーを使用し、入力を整形します。\n",
    "\n",
    "5. **プロンプト工夫**: モデルが出力するトークンを制御し、特定のトークン(A、B、tie)に対して予測を強化するためのロジットプロセッサーを定義しています。これにより、ユーザーが選択するべき応答を促進します。\n",
    "\n",
    "6. **CSVファイルの生成**: 提出用のCSVファイルを作成するプロセスが含まれており、推論結果を適切な形式で出力します。\n",
    "\n",
    "7. **CVスコア計算**: 提出を通じて、交差検証スコア（CVスコア）を計算し、結果の性能を評価します。\n",
    "\n",
    "このノートブックは、モデルのトレーニングや微調整に加え、推論速度や精度を管理するための細やかな配慮がなされており、特にリソースの限られた環境での実用的なアプローチを提示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64672513",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、初心者がつまずきそうな専門用語の簡単な解説です。特に、実務経験が少ない場合や、この特定のノートブックに関連した知識が必要な用語に焦点を当てています。\n",
    "\n",
    "1. **vLLM**:\n",
    "   - *Very Large Language Model*の略。大規模な言語モデルの推論のために設計された非常に高速なライブラリ。このライブラリを使用することで、サンプリングやトークン生成が効率的に行えます。\n",
    "\n",
    "2. **AWQ (Adaptive Weight Quantization)**:\n",
    "   - 重みを量子化する手法。モデルのストレージや推論時のメモリ消費を削減するために、モデルの重みをより少ないビットで表現します。この手法は特に4ビット量子化で使用され、性能とメモリ利用のバランスを取るのに役立ちます。\n",
    "\n",
    "3. **トークン**:\n",
    "   - モデルが処理する基本的な単位で、通常は単語や文字の断片。入力テキストをトークンに分割し、これを基にしてモデルが予測を行います。\n",
    "\n",
    "4. **ロジットプロセッサ (LogitsProcessor)**:\n",
    "   - 出力のスコア（ロジット）を操作するためのクラス。このクラスを通じて、モデルの出力トークンの選擇を制御したり、特定のトークンを優先させたりできます。\n",
    "\n",
    "5. **グラデーションチェックポイント (Gradient Checkpointing)**:\n",
    "   - メモリを節約するための手法で、バックプロパゲーション時に全ての中間結果を保存せず、いくつかの重要な中間結果だけを保存します。この手法により、より大きなモデルの訓練が可能になりますが、計算コストが増加します。\n",
    "\n",
    "6. **信頼されたリモートコード (trust_remote_code)**:\n",
    "   - リモートのコードを実行する際の設定。これを`True`に設定すると、リモートのソースから取得したコードを実行することを許可します。この設定は、セキュリティ上のリスクを伴うことがあります。\n",
    "\n",
    "7. **半精度 (half precision)**:\n",
    "   - 浮動小数点数の表現方法の一つで、32ビット（single precision）よりも少ない16ビットで数値を表現します。これにより、計算速度が向上し、メモリ利用量も減少するため、特に深層学習でよく使用されます。\n",
    "\n",
    "8. **トップP (top-p sampling)**:\n",
    "   - 確率的なトークンサンプリング手法の一つで、選択肢を累積確率に基づいて制限します。例えば、累積確率が95%になるまでのトークン群を考慮し、その中からランダムにサンプリングすることで、多様性を持った出力が得られます。\n",
    "\n",
    "9. **温度 (temperature)**:\n",
    "   - サンプリングの際に、出力の多様性を調整するパラメータ。低い温度は決定的な出力をもたらし、高い温度はより多様でランダムな出力を生成します。\n",
    "\n",
    "10. **CVスコア (Cross-Validation Score)**:\n",
    "   - モデルの一般化性能を評価するために、異なるデータセットで訓練と評価を繰り返した際のスコア。通常は、様々なトレーニングデータの分割を使用して、モデルがデータにどれだけ適応できるかを計測します。\n",
    "\n",
    "これらの解説が、ノートブックの内容を理解する上で役立つことを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87246d74",
   "metadata": {},
   "source": [
    "# LLM 34Bモデルを5時間で提出する方法！\n",
    "このノートブックでは、わずか5時間でLLM 34Bモデルを提出する方法を示します！すごいですね！主なポイントは以下の通りです：\n",
    "* 速度のためにvLLMを使用する\n",
    "* GPUのVRAM不足を避けるためにAWQ 4ビット量子化を使用する\n",
    "* 入力サイズを1024トークンに制限する（速度向上のため）\n",
    "* 出力サイズを1トークンに制限する（速度向上のため）\n",
    "\n",
    "# vLLMのインストール\n",
    "vLLMパッケージは非常に高速なLLM推論ライブラリです！KaggleノートブックにインストールされているvLLMではエラーが発生するため、再インストールが必要です。以下のコードはノートブック[こちら][1]から取得したものです。\n",
    "\n",
    "[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:49:10.667256Z",
     "iopub.status.busy": "2024-07-17T05:49:10.666899Z",
     "iopub.status.idle": "2024-07-17T05:49:10.681111Z",
     "shell.execute_reply": "2024-07-17T05:49:10.68007Z",
     "shell.execute_reply.started": "2024-07-17T05:49:10.667207Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np  # osモジュール、mathモジュール、numpyライブラリをインポートします。\n",
    "\n",
    "# 環境変数CUDA_VISIBLE_DEVICESを設定します。\n",
    "# これにより、どのGPUデバイスを使用するかを指定できます。\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"  # 使用するGPUデバイスとして0番と1番を指定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-17T05:49:10.683275Z",
     "iopub.status.busy": "2024-07-17T05:49:10.682943Z",
     "iopub.status.idle": "2024-07-17T05:52:22.306408Z",
     "shell.execute_reply": "2024-07-17T05:52:22.305139Z",
     "shell.execute_reply.started": "2024-07-17T05:49:10.683249Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time  # このセルの実行にかかる時間を計測します。\n",
    "\n",
    "# torchパッケージをアンインストールします。\n",
    "# -yオプションを使用して、確認なしでアンインストールを行います。\n",
    "!pip uninstall -y torch  \n",
    "\n",
    "# vllmパッケージをアップグレードしてインストールします。\n",
    "# --no-indexオプションを使用してPyPIからのインストールを無効にし、\n",
    "# --find-linksオプションで指定したディレクトリからパッケージを探します。\n",
    "!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm  \n",
    "\n",
    "# grpcioの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  \n",
    "\n",
    "# rayの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a31bd",
   "metadata": {},
   "source": [
    "# vLLMを使って34B量子化モデルをロードする！\n",
    "LLM 34B Bagelモデルを[こちら][1]からロードして使用します。これは非常に強力なモデルです。\n",
    "\n",
    "[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:52:25.217523Z",
     "iopub.status.busy": "2024-07-17T05:52:25.217197Z",
     "iopub.status.idle": "2024-07-17T05:54:46.759248Z",
     "shell.execute_reply": "2024-07-17T05:54:46.756148Z",
     "shell.execute_reply.started": "2024-07-17T05:52:25.217498Z"
    }
   },
   "outputs": [],
   "source": [
    "import vllm  # vLLMライブラリをインポートします。\n",
    "\n",
    "# LLMオブジェクトを作成します。\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/bagel-v3-343\",  # モデルのパスを指定します。\n",
    "    quantization=\"awq\",  # AWQ量子化を使用します。\n",
    "    tensor_parallel_size=2,  # テンソル並列処理のサイズを2に設定します。\n",
    "    gpu_memory_utilization=0.95,  # GPUメモリの使用率を95%に設定します。\n",
    "    trust_remote_code=True,  # リモートコードを信頼する設定です。\n",
    "    dtype=\"half\",  # データ型を半精度に設定します（メモリ効率を向上します）。\n",
    "    enforce_eager=True,  # イージー実行を強制します。\n",
    "    max_model_len=1024,  # モデルの最大長を1024トークンに設定します。\n",
    "    #distributed_executor_backend=\"ray\",  #（コメントアウト）分散実行のバックエンドをRayに設定します。\n",
    ")\n",
    "\n",
    "# トークナイザーを取得します。\n",
    "tokenizer = llm.get_tokenizer()  # モデルに関連付けられたトークナイザーを取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175b726",
   "metadata": {},
   "source": [
    "# テストデータをロードする\n",
    "**コミット**の際にはCVスコアを計算するために128行のトレーニングデータをロードします。**提出**の際にはテストデータをロードします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:54:46.822513Z",
     "iopub.status.busy": "2024-07-17T05:54:46.82197Z",
     "iopub.status.idle": "2024-07-17T05:54:53.847478Z",
     "shell.execute_reply": "2024-07-17T05:54:53.8454Z",
     "shell.execute_reply.started": "2024-07-17T05:54:46.82246Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # pandasライブラリをインポートします。\n",
    "\n",
    "VALIDATE = 128  # バリデーション用の行数を128に設定します。\n",
    "\n",
    "# テストデータをCSVファイルから読み込みます。\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\") \n",
    "\n",
    "# 読み込んだテストデータの行数が3の場合、\n",
    "# トレーニングデータを読み込み、最初の128行を取得します。\n",
    "if len(test) == 3:\n",
    "    test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test = test.iloc[:VALIDATE]  # 最初の128行を取得します。\n",
    "\n",
    "# テストデータの形状を出力します。\n",
    "print(test.shape)  # テストデータの行数と列数を表示します。\n",
    "\n",
    "# テストデータの最初の1行を表示します。\n",
    "test.head(1)  # テストデータの最初の1行を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b2fde",
   "metadata": {},
   "source": [
    "# プロンプトの工夫\n",
    "ゼロショットLLMを提出したい場合、CVスコアを改善するためにさまざまなシステムプロンプトを試す必要があります。モデルをファインチューニングする場合、システムプロンプトはそれほど重要ではなくなります。なぜなら、モデルはターゲットから何をすべきかを学ぶため、どのシステムプロンプトを使用しても影響を受けないからです。\n",
    "\n",
    "ロジットプロセッサを使用して、モデルが私たちが興味を持っている3つのトークンを出力するよう強制します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:06:47.562754Z",
     "iopub.status.busy": "2024-07-17T06:06:47.562396Z",
     "iopub.status.idle": "2024-07-17T06:06:47.571032Z",
     "shell.execute_reply": "2024-07-17T06:06:47.570126Z",
     "shell.execute_reply.started": "2024-07-17T06:06:47.562718Z"
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List  # 必要な型をインポートします。\n",
    "from transformers import LogitsProcessor  # LogitsProcessorクラスをインポートします。\n",
    "import torch  # PyTorchライブラリをインポートします。\n",
    "\n",
    "choices = [\"A\", \"B\", \"tie\"]  # 選択肢を定義します。\n",
    "\n",
    "KEEP = []  # 出力トークンとして保持するトークンIDのリストを初期化します。\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]  # トークンをエンコードし、リストから最初の要素を取得します。\n",
    "    KEEP.append(c)  # 取得したトークンIDをKEEPリストに追加します。\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")  # どのトークンを強制するか出力します。\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP  # 許可されたトークンIDを初期化します。\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100  # 許可されたトークンIDのスコアを100増加させます。\n",
    "        return scores  # 修正されたスコアを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:54:53.883237Z",
     "iopub.status.busy": "2024-07-17T05:54:53.882323Z",
     "iopub.status.idle": "2024-07-17T05:54:53.890156Z",
     "shell.execute_reply": "2024-07-17T05:54:53.887959Z",
     "shell.execute_reply.started": "2024-07-17T05:54:53.883211Z"
    }
   },
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"以下のプロンプトと2つの応答を読み、それぞれの応答がどちらが優れているかを判断してください。\n",
    "もし応答が比較的新しい場合は、「tie」と返信してください。それ以外の場合は、「A」または「B」と応答し、どちらが優れているかを示してください。\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = \"#\"*25 + \"\\n\"  # \"#\"を25個連結し、改行を追加した文字列をSSに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:54:53.918708Z",
     "iopub.status.busy": "2024-07-17T05:54:53.917724Z",
     "iopub.status.idle": "2024-07-17T05:54:54.168859Z",
     "shell.execute_reply": "2024-07-17T05:54:54.166722Z",
     "shell.execute_reply.started": "2024-07-17T05:54:53.918626Z"
    }
   },
   "outputs": [],
   "source": [
    "all_prompts = []  # プロンプトを保存するリストを初期化します。\n",
    "for index, row in test.iterrows():  # テストデータの各行をイテレートします。\n",
    "    \n",
    "    # プロンプト、応答A、応答Bを取得し、nullを空文字に置き換えます。\n",
    "    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))  # プロンプトを評価し、nullを空文字に置き換えた後、スペースで結合します。\n",
    "    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))  # 応答Aを評価・結合します。\n",
    "    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))  # 応答Bを評価・結合します。\n",
    "    \n",
    "    # フォーマットされたプロンプトを作成します。\n",
    "    prompt = f\"{SS}PROMPT: \" + a + f\"\\n\\n{SS}RESPONSE A: \" + b + f\"\\n\\n{SS}RESPONSE B: \" + c + \"\\n\\n\"\n",
    "    \n",
    "    # システムプロンプトとフォーマットされたプロンプトを結合します。\n",
    "    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n",
    "    \n",
    "    # フォーマットされたサンプルをリストに追加します。\n",
    "    all_prompts.append(formatted_sample)  # フォーマットされたサンプルをall_promptsリストに追加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c860bfb",
   "metadata": {},
   "source": [
    "# テストを推論する\n",
    "高速なvLLMを使用してテストを推論します。vLLMに対し、最初のトークンで予測される上位5つのトークンの確率を出力するように依頼します。また、推論速度を向上させるために、予測を1トークンに制限します。\n",
    "\n",
    "128のトレーニングサンプルを推論するのにかかる時間に基づいて、25,000のテストサンプルを推論するのにかかる時間を推測できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-17T06:00:22.241023Z",
     "iopub.status.busy": "2024-07-17T06:00:22.240185Z",
     "iopub.status.idle": "2024-07-17T06:02:04.057293Z",
     "shell.execute_reply": "2024-07-17T06:02:04.056389Z",
     "shell.execute_reply.started": "2024-07-17T06:00:22.240983Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time  # このセルの実行にかかる時間を計測します。\n",
    "\n",
    "from time import time  # timeモジュールからtime関数をインポートします。\n",
    "start = time()  # 実行開始時刻を記録します。\n",
    "\n",
    "# ロジットプロセッサを定義します。\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "# vLLMを使用して応答を生成します。\n",
    "responses = llm.generate(\n",
    "    all_prompts,  # すべてのプロンプトを指定します。\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # 各プロンプトに対して返す出力シーケンスの数。\n",
    "        top_p=0.9,  # 上位トークンを考慮する際の累積確率を制御する浮動小数点数。\n",
    "        temperature=0,  # サンプリングのランダム性。\n",
    "        seed=777,  # 再現性のためのシード。\n",
    "        skip_special_tokens=True,  # 出力で特殊トークンをスキップするかどうか。\n",
    "        max_tokens=1,  # 各出力シーケンスに生成する最大トークン数。\n",
    "        logits_processors=logits_processors,  # 使用するロジットプロセッサ。\n",
    "        logprobs=5  # 上位5つのトークン確率を出力。\n",
    "    ),\n",
    "    use_tqdm=True  # プログレスバーを表示するためのオプション。\n",
    ")\n",
    "\n",
    "end = time()  # 実行終了時刻を記録します。\n",
    "elapsed = (end - start) / 60.  # 経過時間を分単位で計算します。\n",
    "print(f\"{VALIDATE}サンプルの推論に{elapsed}分かかりました！\")  # 推論にかかった時間を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:02:13.41828Z",
     "iopub.status.busy": "2024-07-17T06:02:13.417593Z",
     "iopub.status.idle": "2024-07-17T06:02:13.423213Z",
     "shell.execute_reply": "2024-07-17T06:02:13.422183Z",
     "shell.execute_reply.started": "2024-07-17T06:02:13.418247Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = 25_000 / 128 * elapsed / 60  # 25,000サンプルの推論にかかる時間を計算します。\n",
    "print(f\"提出には{submit}時間かかります\")  # 推論にかかる時間を出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264f7fe",
   "metadata": {},
   "source": [
    "# 推論確率の抽出\n",
    "これからvLLMの予測から「A」、「B」、「tie」の確率を抽出します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:02:33.681684Z",
     "iopub.status.busy": "2024-07-17T06:02:33.681318Z",
     "iopub.status.idle": "2024-07-17T06:02:33.691347Z",
     "shell.execute_reply": "2024-07-17T06:02:33.690341Z",
     "shell.execute_reply.started": "2024-07-17T06:02:33.681654Z"
    }
   },
   "outputs": [],
   "source": [
    "results = []  # 推論結果を格納するリストを初期化します。\n",
    "errors = 0  # エラーのカウンタを初期化します。\n",
    "\n",
    "for i, response in enumerate(responses):  # レスポンスの各要素をイテレートします。\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]  # 最初の出力のロジットを取得します。\n",
    "        logprobs = []  # ログ確率を格納するリストを初期化します。\n",
    "        for k in KEEP:  # KEEPに含まれる各トークンIDについて\n",
    "            if k in x:  # トークンIDがログ確率に存在する場合\n",
    "                logprobs.append(math.exp(x[k].logprob))  # ログ確率の指数を計算して追加します。\n",
    "            else:\n",
    "                logprobs.append(0)  # トークンIDが存在しない場合は0を追加します。\n",
    "                print(f\"bad logits {i}\")  # 不正なロジットが見つかったことを記録します。\n",
    "        logprobs = np.array(logprobs)  # ログ確率リストをNumPy配列に変換します。\n",
    "        logprobs /= logprobs.sum()  # 確率を正規化します。\n",
    "        results.append(logprobs)  # 結果リストに追加します。\n",
    "    except:  # エラーが発生した場合\n",
    "        # print(f\"error {i}\")  # エラーを記録（コメントアウトされています）。\n",
    "        results.append(np.array([1/3., 1/3., 1/3.]))  # 一様分布の確率を追加します。\n",
    "        errors += 1  # エラーカウンタを増やします。\n",
    "        \n",
    "print(f\"{i+1}回の推論のうちエラーは{errors}回発生しました。\")  # エラーの総数を出力します。\n",
    "results = np.vstack(results)  # 結果を垂直スタックして2次元配列を作成します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d166e",
   "metadata": {},
   "source": [
    "# 提出用CSVを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:02:54.392772Z",
     "iopub.status.busy": "2024-07-17T06:02:54.392381Z",
     "iopub.status.idle": "2024-07-17T06:02:54.410711Z",
     "shell.execute_reply": "2024-07-17T06:02:54.409747Z",
     "shell.execute_reply.started": "2024-07-17T06:02:54.392737Z"
    }
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # サンプル提出CSVファイルを読み込みます。\n",
    "\n",
    "# テストデータの長さがVALIDATEでない場合、結果をサブミッション用データフレームに代入します。\n",
    "if len(test) != VALIDATE:\n",
    "    sub[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = results  # 結果を対応する列に割り当てます。\n",
    "    \n",
    "sub.to_csv(\"submission.csv\", index=False)  # 提出用CSVを作成します。インデックスは出力しません。\n",
    "sub.head()  # サブミッションデータフレームの最初の数行を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d3a55",
   "metadata": {},
   "source": [
    "# CVスコアを計算する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:03:21.152367Z",
     "iopub.status.busy": "2024-07-17T06:03:21.15151Z",
     "iopub.status.idle": "2024-07-17T06:03:21.158548Z",
     "shell.execute_reply": "2024-07-17T06:03:21.157417Z",
     "shell.execute_reply.started": "2024-07-17T06:03:21.152333Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合\n",
    "    true = test[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # 真のラベルを取得します。\n",
    "    print(true.shape)  # 真のラベルの形状を出力します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:03:23.265602Z",
     "iopub.status.busy": "2024-07-17T06:03:23.264976Z",
     "iopub.status.idle": "2024-07-17T06:03:23.273439Z",
     "shell.execute_reply": "2024-07-17T06:03:23.272475Z",
     "shell.execute_reply.started": "2024-07-17T06:03:23.265556Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合\n",
    "    from sklearn.metrics import log_loss  # log_loss関数をインポートします。\n",
    "    print(f\"CV loglossは {log_loss(true, results)} です\")  # CVのログ損失を計算して出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df67311",
   "metadata": {},
   "source": [
    "# コメント\n",
    "\n",
    "> ## Cody_Null\n",
    "> \n",
    "> すごい仕事ですね、クリス。vLLMのパフォーマンス低下について何か知っていますか？私は何も知らないので、他のノートブックでこれを使う際の期待値を知りたいと思っています。\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > vLLMによるパフォーマンス低下はないと思います。vLLMパッケージは単に私たちのモデルを実行するだけです。選択するオプション（vllm.SamplingParams、量子化の有無、量子化の種類、max_model_length、トークナイザの切り捨てなど）がパフォーマンスに影響を与えます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Cody_Null\n",
    "> > > \n",
    "> > > すばらしい！ONNXに似ているのか、正確に私が必要としているものなのかわからなかったです（笑）。\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > モデルの変換（ONNXのような）は必要ありません。Hugging Faceから非量子化モデルと量子化モデルの両方を直接取得し、推論を実行できます。（つまり、モデルは全く変更されません）。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## SeshuRaju 🧘‍♂️\n",
    "> \n",
    "> [@cdeotte](https://www.kaggle.com/cdeotte) ありがとうございます。\n",
    "> \n",
    "> このモデルはSFTまたはDPOでファインチューニングされていますか？\n",
    "> \n",
    "> ファインチューニングにはどれほどのGPUが必要ですか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはSFTでファインチューニングされています。選択したLoRAランクパラメータr、max_model_len、バッチサイズ、およびトレーニングデータの量に応じて、任意のGPU数でファインチューニングできます。\n",
    "> > \n",
    "> > ファインチューニング中に、34Bを4ビットに量子化し、サイズを20GBに削減します。したがって、要件は、GPUの総VRAMがトレーニングに十分な20GBを超えることです。Kaggleの2xT4（合計VRAM 32GB）上で、上記のパラメータを減らせばファインチューニングできると思います。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## SeshuRaju 🧘‍♂️\n",
    "> > > \n",
    "> > > 限られたKaggle GPU時間のため、16GB VRAMで訓練しようとしています。バッチ=1、r=3、max_model_len=1024で訓練する他の方法を探しています。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Chris Deotte（トピック作成者）\n",
    "> > > > \n",
    "> > > > 16GB VRAMで34B LLMをファインチューニングする方法はわかりません。最小要件は32GB VRAMに近いと思います。効率的なファインチューニングに関する詳細を説明した素敵なYouTubeビデオがあります [こちら](https://www.youtube.com/watch?v=XpoKB3usmKc) をご覧ください。ファインチューニング中には、次のメモリが必要です：\n",
    "> > > > \n",
    "> > > > - モデルの重み\n",
    "> > > > \n",
    "> > > > - 勾配\n",
    "> > > > \n",
    "> > > > - オプティマイザ\n",
    "> > > > \n",
    "> > > > メモリを最も削減するには、4ビット量子化されたモデルをロード（QLoRAを使用してモデル重みのメモリ使用量を削減）し、llm_int8_enable_fp32_cpu_offloadを使用してGPUとCPUメモリ間でモデルの重みをスワップできるようにし、PEFTを使用して少ないrパラメータ（QLoRA）でファインチューニングし、勾配チェックポイントを使用して（勾配メモリ使用量を削減）、paged_adam_8bitオプティマイザを使用します（小さなバッチと小さな最大トークン長で）。このオプティマイザは、必要に応じてGPUからCPUに保存された変数をスワップし、オプティマイザ変数のサイズを削減するために8ビットを使用します。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## ano\n",
    "> \n",
    "> この素晴らしいモデルを共有していただきありがとうございます！トレーニングに使用したデータセットについて教えていただけますか？トレーニングデータセット全体を使用しましたか、それともその一部ですか？または外部データセットを使用しましたか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはコンペティションデータの80%でファインチューニングしました。5行ごとにすべてのデータを除外しました。したがって、正しい検証は以下のデータを使用することです pd.read_csv(\"train.csv\").iloc[0::5]。現在、このノートブックでは pd.read_csv(\"train.csv\").iloc[:128] を使用していますが、より正確な迅速な検証は \n",
    "> > \n",
    "> > ```\n",
    "> > VALIDATE = 128\n",
    "> > test = test.iloc[0:VALIDATE*5:5]\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > その後、最初のインデックス % 5 == 0 のサンプル128を使用します。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## JM\n",
    "> \n",
    "> 提出段階で実際に5時間かかっていますか？私のはずっとオーバーしています。\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > いいえ、私は5時間かかっていません。私のは8から9時間かかっています。このノートブックの別のバージョンをリリースするときに、コードセル#10の時間推定コードを修正します（「推論エラー」を削除することが含まれます）。また、導入文を「5時間」ではなく「9時間未満」と更新します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## ano\n",
    "> > > \n",
    "> > > 推論時間の推定（5時間）が実際のもの（8〜9時間）と異なる理由に心当たりはありますか？私は1000サンプルの推論を試してみましたが、25000サンプルのための推定時間（5-6時間）は似たようなものでした。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> こんにちは、GPTQの量子化結果がAWQとどう比較されるか、またKaggleでの可行性についてお尋ねしますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > GPTQを使用するためには、vLLMパラメータで量子化=\"gptq\"を設定し、事前にモデルをGPTQとして保存します。私はKaggleで試したことはありませんが、過去にT4 GPUでAWQとGPTQをどちらも成功裏に利用してきました。それぞれの精度は基本的に似ています。また、推論時間も似ています。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Akhila datta dola\n",
    "> > > \n",
    "> > > 共有していただきありがとうございます！\n",
    "> > > \n",
    "> > > vLLMはbitsandbytesのようなオンザフライ4ビット量子化をサポートしていますか？一般的にgptqとawqと比較してどうですか？\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> すばらしい共有をありがとうございます!!!\n",
    "> \n",
    "> 推論プロセスで「128回中33回の推論エラー」が発生した理由を教えていただけますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > 128の入力テキストのうち33回は、モデルが生成テキストを予測できませんでした。したがって、これらの33回には確率を抽出するためのトークンがありません。私たちのtry/exceptコードブロックがこれをキャッチし、これらのケースでは[1/3, 1/3, 1/3]を予測します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Luan Ngo Dinh\n",
    "> > > \n",
    "> > > モデルはファインチューニングされていても、トークン「a」、「b」、「tie」を生成しなかったという意味ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > はい。問題は、モデルをmax_model_len=1024に切り捨てたため（速度のため）、入力テキストが1024を超えるとvLLMが1024に切り捨てられ、生成されたテキストを出力するための余地がなくなるからです。よりスマートなプロンプトエンジニアリングまたは切り捨て戦略を使用することで、これらの推論エラーを回避できます。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > プロンプトエンジニアリングまたは切り捨て戦略に取り組んでいます。正しい方向に進んでいるかもしれないと聞いてうれしいです;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Xinian Guo\n",
    "> \n",
    "> こんにちは、このモデルはコンペティションデータでファインチューニングされていますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > はい。このモデル（ノートブックバージョン8）はコンペデータでファインチューニングされています。ノートブックバージョン6 [こちら](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm?scriptVersionId=188642633) はファインチューニングなしのゼロショットです。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## yechenzhi1\n",
    "> > > \n",
    "> > > こんにちは、ファインチューニングされたバージョンのLBスコアは何ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > 私はそのノートブックで0.972を得ました。頑張ってください;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Qihang Wang\n",
    "> \n",
    "> こんにちは、クリス、あなたのプロセスを確認したいです：\n",
    "> \n",
    "> CMIIW\n",
    "> \n",
    "> qloraファインチューニング4ビットモデル？\n",
    "> \n",
    "> qloraをモデルにマージ？\n",
    "> \n",
    "> 4ビットを変換？\n",
    "> \n",
    "> AWQ量子化\n",
    "> \n",
    "> vLLMを使用して推論\n",
    "> \n",
    "> 私は最初の3つのステップにあまり慣れていないので、これがあなたのやり方かどうかは確信がありません。\n",
    "> \n",
    "> もう少し詳しく説明していただけますか？"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5409557,
     "sourceId": 8982890,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
