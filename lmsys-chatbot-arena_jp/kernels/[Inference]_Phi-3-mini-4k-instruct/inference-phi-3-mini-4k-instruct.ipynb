{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c2175b",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションに参加するための機械学習モデルの構築と推論プロセスに焦点を当てています。以下にノートブックの要約を示します。\n",
    "\n",
    "### 問題の概要\n",
    "本ノートブックの目的は、異なる大規模言語モデル（LLM）からの応答を比較して、どちらのモデルの応答がユーザーに好まれるかを予測することです。このタスクは、人間の好みを予測するためのモデルの開発を通じて、LLMとの相互作用を改善しようとしています。\n",
    "\n",
    "### 手法\n",
    "1. **ライブラリの使用**:\n",
    "   - **PyTorch**: ニューラルネットワークの構築とトレーニングに使用。\n",
    "   - **Transformers**: 事前訓練済みのモデルのロードとトークナイザーの利用に使用。\n",
    "   - **Datasets**: データのロードや前処理に使用。\n",
    "   - **Accelerate**: モデルを効率的にトレーニングするために使用し、マルチGPUトレーニングに対応しています。\n",
    "\n",
    "2. **モデルアーキテクチャ**:\n",
    "   - **Phi-3 miniモデル**: Microsoftから提供される事前訓練済みのモデルを使用し、LoRA（Low-Rank Adaptation）を利用してモデルを微調整しています。\n",
    "   - カスタムデータセットクラスとデータローダーを定義し、トークン化されたプロンプトと応答を準備します。\n",
    "\n",
    "3. **トレーニング手順**:\n",
    "   - ハイパーパラメータ（最大シーケンス長やバッチサイズなど）を設定。\n",
    "   - LoRAの適用や量子化を通じてモデルの効率を向上。\n",
    "   - 自動混合精度を用いて、推論中に計算リソースを最適化。\n",
    "\n",
    "4. **データの前処理**:\n",
    "   - テストデータセットを読み込み、プロンプトと応答をJSONから抽出。\n",
    "   - データを2つのバッチに分割し、各GPUで別々に推論を実行。\n",
    "\n",
    "5. **推論と結果の取得**:\n",
    "   - モデルを評価モードに設定し、推論を行う関数を使用して結果を収集。\n",
    "   - 複数のスレッドを用いて推論を並列実行し、処理時間を短縮。\n",
    "\n",
    "6. **結果の保存**:\n",
    "   - 予測結果を含むデータフレームを生成し、最終的な提出フォーマットに変換してCSVファイルとして保存。\n",
    "   - 不要なデータやファイルの削除を行い、リソースを効率的に管理。\n",
    "\n",
    "### 結論\n",
    "このノートブックは、LMSYS Chatbot Arenaコンペティションにおける人間の好み予測モデルの構築を目的としており、効率的なトレーニングと推論の手法を採用しています。PyTorchとTransformersライブラリを用いて、LoRAトレーニングや量子化を組み合わせることで、モデルの性能を向上させています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6324f095",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連する専門用語の簡単な解説を示します。これらは初心者にとって馴染みが薄い可能性があるため、注意が必要です。\n",
    "\n",
    "1. **LoRA (Low-Rank Adaptation)**:\n",
    "   LoRAは、大規模モデルのパラメータを凍結したまま、少ないパラメータで適応させる手法です。モデルの重みを低ランク行列で補正することで、効率的に学習ができるようになります。\n",
    "\n",
    "2. **量子化**:\n",
    "   モデルのサイズを小さくし、推論速度を向上させるために、フロート数値を整数に変換するプロセスです。量子化により、メモリ使用量が減少し、デバイス上での計算が効率化されます。\n",
    "\n",
    "3. **アテンションマスク**:\n",
    "   シーケンスデータを処理する際に、特定の部分に注意を集中させるために用いるマスクです。無関係なトークンに対して注意を払わないようにするために使用され、通常、0または1の値で表されます。\n",
    "\n",
    "4. **Rotary Position Embedding (RoPE)**:\n",
    "   位置情報をモデルに追加する手法の一つで、特にTransformerアーキテクチャで使用されます。RoPEは、トークンの相対的位置関係を学習するために、回転行列を利用して位置情報を表現します。\n",
    "\n",
    "5. **因果関係 (Causal Relation)**:\n",
    "   モデルが過去の情報に基づいて未来を予測することを指します。具体的には、系列データを処理する際、未来の情報にアクセスできないようにすることで、信頼性のある予測を行います。\n",
    "\n",
    "6. **ソフトマックス**:\n",
    "   多クラスの出力を確率分布に変換するための関数です。各クラスの出力を指数関数に通して、全体の合計が1になるように正規化し、確率として解釈できるようにします。\n",
    "\n",
    "7. **データコレータ (Data Collator)**:\n",
    "   モデルのトレーニング中に、バッチ単位でデータを整理・整形するための関数やクラスです。特に異なる長さのシーケンスを適切にパディングしてバッチを構成することに役立ちます。\n",
    "\n",
    "8. **バッチサイズ**:\n",
    "   モデルに入力されるデータのグループ数を指します。トレーニングや推論時にデータを一度に処理するための最小単位となります。\n",
    "\n",
    "9. **パディング**:\n",
    "   シーケンスデータの長さを揃えるために、短いシーケンスに特定の値（通常は0）を追加するプロセスです。これにより、計算を効率よく行えるようになります。\n",
    "\n",
    "10. **クロスエントロピー損失 (CrossEntropyLoss)**:\n",
    "    多クラス分類問題で用いられる損失関数です。モデルの予測確率と実際のラベルの違いを測定し、訓練中に最小化することを目指します。\n",
    "\n",
    "これらの用語は、ノートブック内で使用されるコンテキスト特有のものであり、初心者の理解を助けるために特に重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8d8d27",
   "metadata": {},
   "source": [
    "# インストール不要\n",
    "microsoft/Phi-3-mini-4k-instruct + LoRA > GPU並列トレーニング\n",
    "\n",
    "最大シーケンス長はモデルのパフォーマンスに大きな影響を与えますが、メモリ不足のため最大長は768に設定されました。\n",
    "\n",
    "[トレーニングコード](https://www.kaggle.com/code/argozero01/parallel-train-phi-3-mini-4k-instruct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:42:52.52789Z",
     "iopub.status.busy": "2024-07-28T12:42:52.527573Z",
     "iopub.status.idle": "2024-07-28T12:43:01.446177Z",
     "shell.execute_reply": "2024-07-28T12:43:01.445408Z",
     "shell.execute_reply.started": "2024-07-28T12:42:52.527862Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss\n",
    "\n",
    "from accelerate import notebook_launcher, Accelerator, PartialState\n",
    "from accelerate.utils import write_basic_config\n",
    "from accelerate.inference import prepare_pippy\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    set_seed,\n",
    "    AutoTokenizer,\n",
    "    AutoModel,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoConfig\n",
    ")\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "# ここでは、PyTorchと関連ライブラリをインポートしています。\n",
    "# Torchを使ってニューラルネットワークモデルを構築し、トレーニングや評価を行う準備をします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.452034Z",
     "iopub.status.busy": "2024-07-28T12:43:01.451479Z",
     "iopub.status.idle": "2024-07-28T12:43:01.456896Z",
     "shell.execute_reply": "2024-07-28T12:43:01.456053Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.452001Z"
    }
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータの設定\n",
    "model_name = \"/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1\"  # モデルの名前\n",
    "model_path = \"/kaggle/input/checkpoint-phi3/model_checkpoint.pth\"  # モデルのチェックポイントのパス\n",
    "seed = 42  # 乱数シード\n",
    "lora_r = 2  # LoRAのランク\n",
    "quantize_bit = 16  # 量子化ビット数\n",
    "test_batch_size = 1  # テスト時のバッチサイズ\n",
    "test_max_len = 256  # テスト時の最大シーケンス長\n",
    "device = \"cuda\"  # 使用するデバイス（GPU）\n",
    "\n",
    "# これらの変数はモデルの設定やトレーニング設定のために使用されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.458241Z",
     "iopub.status.busy": "2024-07-28T12:43:01.457984Z",
     "iopub.status.idle": "2024-07-28T12:43:01.47395Z",
     "shell.execute_reply": "2024-07-28T12:43:01.473229Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.458218Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    # カスタムデータセットクラスの定義\n",
    "\n",
    "    def __init__(self, df, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer  # トークナイザーの初期化\n",
    "        self.prompt = df['prompt']  # データフレームからプロンプトを取得\n",
    "        self.response_a = df['response_a']  # データフレームから応答Aを取得\n",
    "        self.response_b = df['response_b']  # データフレームから応答Bを取得\n",
    "        self.max_len = max_len  # 最大シーケンス長\n",
    "        self.targets = df.get('labels', None)  # ラベルの取得（存在すれば）\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)  # データセットのサンプル数を返す\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # 指定したインデックスのデータを取得\n",
    "        \n",
    "        prompt = str(self.prompt[index])  # プロンプトを文字列として取得\n",
    "        response_a = str(self.response_a[index])  # 応答Aを文字列として取得\n",
    "        response_b = str(self.response_b[index])  # 応答Bを文字列として取得\n",
    "\n",
    "        prompt_len = len(self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True)['input_ids'])  # プロンプトのトークン数を計算\n",
    "        response_a_len = len(self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True)['input_ids'])  # 応答Aのトークン数を計算\n",
    "        response_b_len = len(self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True)['input_ids'])  # 応答Bのトークン数を計算\n",
    "\n",
    "        final_prompt_len = min(self.max_len, prompt_len)  # プロンプトの最終長を決定\n",
    "        final_a_len = min(self.max_len, response_a_len)  # 応答Aの最終長を決定\n",
    "        final_b_len = min(self.max_len, response_b_len)  # 応答Bの最終長を決定\n",
    "\n",
    "        # トークン化とパディングを行い、入力IDとアテンションマスクを作成\n",
    "        prompt_token = self.tokenizer(\"##prompt: \" + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_a_token = self.tokenizer(\"##response_a: \" + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "        response_b_token = self.tokenizer(\"##response_b: \" + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')\n",
    "\n",
    "        # 入力IDとアテンションマスクを結合\n",
    "        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)\n",
    "        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)\n",
    "\n",
    "        if self.targets is not None:\n",
    "            labels = torch.LongTensor([self.targets[index]])  # ラベルをTensorに変換\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}  # データを辞書形式で返す\n",
    "        else:\n",
    "            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}  # ラベルがない場合はラベルなしでデータを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.476599Z",
     "iopub.status.busy": "2024-07-28T12:43:01.476247Z",
     "iopub.status.idle": "2024-07-28T12:43:01.48831Z",
     "shell.execute_reply": "2024-07-28T12:43:01.487415Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.476576Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_collate_fn(batch, tokenizer):\n",
    "    # カスタムコレート関数の定義\n",
    "\n",
    "    input_ids = [item['input_ids'] for item in batch]  # バッチ内の入力IDを取得\n",
    "    attention_masks = [item['attention_mask'] for item in batch]  # バッチ内のアテンションマスクを取得\n",
    "    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None  # ラベルが含まれている場合は結合\n",
    "\n",
    "    # バッチ内のシーケンスの最大長を見つける\n",
    "    max_len = max([input_id.size(0) for input_id in input_ids])\n",
    "\n",
    "    # 新しい最大長で再トークン化\n",
    "    new_input_ids = []\n",
    "    new_attention_masks = []\n",
    "\n",
    "    for item in batch:\n",
    "        # 各アイテムのデータを新しい最大長に合わせて切り詰め\n",
    "        input_ids = item['input_ids'][:max_len]\n",
    "        attention_mask = item['attention_mask'][:max_len]\n",
    "\n",
    "        new_input_ids.append(input_ids)  # 新しい入力IDを追加\n",
    "        new_attention_masks.append(attention_mask)  # 新しいアテンションマスクを追加\n",
    "\n",
    "    # パディングを施してテンソルを作成\n",
    "    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
    "    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)\n",
    "\n",
    "    output = {\n",
    "        'input_ids': new_input_ids,\n",
    "        'attention_mask': new_attention_masks\n",
    "    }\n",
    "\n",
    "    if labels is not None:\n",
    "        output['labels'] = labels  # ラベルがある場合は出力に追加\n",
    "\n",
    "    return output  # 結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.489657Z",
     "iopub.status.busy": "2024-07-28T12:43:01.489398Z",
     "iopub.status.idle": "2024-07-28T12:43:01.499196Z",
     "shell.execute_reply": "2024-07-28T12:43:01.498418Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.489635Z"
    }
   },
   "outputs": [],
   "source": [
    "def create_dataloaders(df, tokenizer, max_len, batch_size, shuffle=True):\n",
    "    # データローダーを作成する関数の定義\n",
    "    dataloader = DataLoader(\n",
    "        CustomDataset(df, tokenizer, max_len),  # カスタムデータセットを使ったデータローダー\n",
    "        shuffle=shuffle,  # シャッフルオプション\n",
    "        batch_size=batch_size,  # バッチサイズ\n",
    "        collate_fn=lambda x: custom_collate_fn(x, tokenizer)  # コレート関数を指定\n",
    "    )\n",
    "    return dataloader  # データローダーを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.500877Z",
     "iopub.status.busy": "2024-07-28T12:43:01.500164Z",
     "iopub.status.idle": "2024-07-28T12:43:01.510489Z",
     "shell.execute_reply": "2024-07-28T12:43:01.509774Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.500848Z"
    }
   },
   "outputs": [],
   "source": [
    "def quantize_tensor(tensor, num_bits=quantize_bit):\n",
    "    # テンソルを量子化する関数の定義\n",
    "    qmin = 0.\n",
    "    qmax = 2.**num_bits - 1.\n",
    "\n",
    "    min_val, max_val = tensor.min(), tensor.max()  # テンソルの最小値と最大値を取得\n",
    "    scale = (max_val - min_val) / (qmax - qmin)  # スケールを計算\n",
    "    zero_point = qmin - min_val / scale  # ゼロポイントを計算\n",
    "\n",
    "    quantized_tensor = torch.round(tensor / scale + zero_point)  # テンソルを量子化\n",
    "    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)  # クランプ処理\n",
    "    quantized_tensor = (quantized_tensor - zero_point) * scale  # スケールを戻して再スケーリング\n",
    "\n",
    "    return quantized_tensor  # 量子化されたテンソルを返す\n",
    "\n",
    "def quantize_model(model, num_bits=8):\n",
    "    # モデル全体を量子化する関数の定義\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化\n",
    "        elif isinstance(module, nn.Conv2d):\n",
    "            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 畳み込み層の重みを量子化\n",
    "            if module.bias is not None:\n",
    "                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # 畳み込み層のバイアスを量子化\n",
    "\n",
    "    return model  # 量子化されたモデルを返す\n",
    "\n",
    "# import torch.quantization\n",
    "\n",
    "# def quantize_model_dynamic(model):\n",
    "#     model.qconfig = torch.quantization.default_dynamic_qconfig\n",
    "#     torch.quantization.prepare(model, inplace=True)\n",
    "#     torch.quantization.convert(model, inplace=True)\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.512227Z",
     "iopub.status.busy": "2024-07-28T12:43:01.511688Z",
     "iopub.status.idle": "2024-07-28T12:43:01.524661Z",
     "shell.execute_reply": "2024-07-28T12:43:01.523844Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.512189Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoRA(nn.Module):\n",
    "    # LoRAクラスの定義\n",
    "    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout=0.05):\n",
    "        super(LoRA, self).__init__()\n",
    "        self.alpha = alpha  # スケーリングファクタ\n",
    "        self.rank = rank  # LoRAのランク\n",
    "        self.lora_a = nn.Linear(in_features, rank, bias=False)  # 変換A\n",
    "        self.lora_b = nn.Linear(rank, out_features, bias=False)  # 変換B\n",
    "        self.dropout = nn.Dropout(lora_dropout)  # ドロップアウト層\n",
    "\n",
    "    def forward(self, x):\n",
    "        lora_out = self.alpha * self.lora_b(self.lora_a(x))  # LoRAの出力を計算\n",
    "        lora_out = self.dropout(lora_out)  # ドロップアウトを適用\n",
    "        return lora_out  # 出力を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.525997Z",
     "iopub.status.busy": "2024-07-28T12:43:01.525684Z",
     "iopub.status.idle": "2024-07-28T12:43:01.594784Z",
     "shell.execute_reply": "2024-07-28T12:43:01.594077Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.525967Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.models.phi3.modeling_phi3 import (\n",
    "    Phi3RotaryEmbedding,\n",
    "    # Phi3LongRoPEScaledRotaryEmbedding,  # コメントアウトされたコード\n",
    "    apply_rotary_pos_emb,\n",
    "    repeat_kv\n",
    ")\n",
    "\n",
    "# Phi3RotaryEmbeddingなどのモジュールをインポートしています。\n",
    "# これにより、ロタリー埋め込みや位置埋め込みの適用を行えるようになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.596455Z",
     "iopub.status.busy": "2024-07-28T12:43:01.596066Z",
     "iopub.status.idle": "2024-07-28T12:43:01.623393Z",
     "shell.execute_reply": "2024-07-28T12:43:01.622631Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.596423Z"
    }
   },
   "outputs": [],
   "source": [
    "class Phi3Attention(nn.Module):\n",
    "    \"\"\"マルチヘッドアテンション 'Attention Is All You Need' 論文から\"\"\"\n",
    "\n",
    "    def __init__(self, config, layer_idx: Optional[int] = None):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        if layer_idx is None:\n",
    "            logger.warning_once(\n",
    "                f\"layer_idxを渡さずに{self.__class__.__name__}をインスタンス化することは推奨されず、\"\n",
    "                \"キャッシングを使用する際にフォワードコール中にエラーが発生します。必ず`layer_idx`を指定してこのクラスを作成してください。\"\n",
    "            )\n",
    "\n",
    "        self.attention_dropout = config.attention_dropout  # アテンションドロップアウト率\n",
    "        self.hidden_size = config.hidden_size  # 隠れ層のサイズ\n",
    "        self.num_heads = config.num_attention_heads  # アテンションヘッドの数\n",
    "        self.head_dim = self.hidden_size // self.num_heads  # 各ヘッドの次元数\n",
    "        self.num_key_value_heads = config.num_key_value_heads  # キー・バリューのヘッド数\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads  # キー・バリューのグループ数\n",
    "        self.max_position_embeddings = config.max_position_embeddings  # 最大位置埋め込み\n",
    "        self.original_max_position_embeddings = config.original_max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta  # RoPEのスケーリングパラメータ\n",
    "        self.rope_scaling = config.rope_scaling  # RoPEのスケーリング方法\n",
    "        self.is_causal = True  # 因果関係を持つかどうか\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_sizeはnum_headsで割り切れる必要があります（`hidden_size`: {self.hidden_size}\"\n",
    "                f\" と `num_heads`: {self.num_heads}を受け取りました）。\"\n",
    "            )\n",
    "\n",
    "        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)  # 出力サイズ\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)  # 出力の線形変換\n",
    "        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)  # QKVの線形変換\n",
    "        self._init_rope()  # RoPEの初期化\n",
    "\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)  # QKVのLoRAアダプタ\n",
    "        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)  # 出力のLoRAアダプタ\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "\n",
    "    def _init_rope(self):\n",
    "        # RoPEの初期化\n",
    "        if self.rope_scaling is None:\n",
    "            self.rotary_emb = Phi3RotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            if scaling_type == \"longrope\":\n",
    "                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)\n",
    "            else:\n",
    "                raise ValueError(f\"未知のRoPEスケーリングタイプ {scaling_type}\")\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        # logger.warning_once(\"フラッシュアテンション実装を実行していないため、数値の違いが予想されます。\")\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()  # バッチサイズとクエリ長を取得\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)  # QKVプロジェクションの計算\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        \n",
    "        query_pos = self.num_heads * self.head_dim  # クエリ位置の計算\n",
    "        query_states = qkv[..., :query_pos]  # クエリ状態の取得\n",
    "        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]  # キー状態の取得\n",
    "        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]  # バリュー状態の取得\n",
    "\n",
    "        # 状態を再構成\n",
    "        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]  # キー・バリューのシーケンス長\n",
    "        if past_key_value is not None:\n",
    "            if self.layer_idx is None:\n",
    "                raise ValueError(\n",
    "                    f\"キャッシュ構造はバージョンv4.36以降変更されました。k/vキャッシングを使用するために{self.__class__.__name__}を使用している場合、\"\n",
    "                    \"レイヤーインデックスでこのアテンションクラスを初期化してください。\"\n",
    "                )\n",
    "            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)  # 過去のキー・バリューの長さを取得\n",
    "        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)  # RoPEの計算\n",
    "\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)  # RoPEの適用\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # RoPEモデルに特有のキーワード引数\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # キャッシュの更新\n",
    "\n",
    "        # n_kv_heads < n_headsの場合にk/vヘッドを繰り返す\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)  # キー状態の繰り返し\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)  # バリュー状態の繰り返し\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)  # アテンション重みの計算\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"アテンション重みは{(bsz, self.num_heads, q_len, kv_seq_len)}のサイズであるべきですが、\"\n",
    "                f\"{attn_weights.size()}になっています。\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]  # 因果マスクの取得\n",
    "            attn_weights += causal_mask  # マスクの適用\n",
    "\n",
    "        # アテンションをfp32にアップキャスト\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)  # ソフトマックスの適用\n",
    "        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)  # ドロップアウトの適用\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)  # アテンションの出力計算\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"attn_outputは{(bsz, self.num_heads, q_len, self.head_dim)}のサイズであるべきですが、\"\n",
    "                f\"{attn_output.size()}になっています。\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()  # 転置と連続性の保証\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)  # 隠れ層のサイズに変形\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)  # 出力プロジェクションとLoRAアダプタの適用\n",
    "        ########################## LoRAアダプタ ##########################\n",
    "        \n",
    "        if not output_attentions:\n",
    "            attn_weights = None  # アテンション重みを返さない場合はNoneを設定\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value  # 結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.625081Z",
     "iopub.status.busy": "2024-07-28T12:43:01.624642Z",
     "iopub.status.idle": "2024-07-28T12:43:01.638084Z",
     "shell.execute_reply": "2024-07-28T12:43:01.637254Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.625025Z"
    }
   },
   "outputs": [],
   "source": [
    "def replace_attention_module(config, layer, layer_idx):\n",
    "    # アテンションモジュールを置き換える関数\n",
    "    if hasattr(layer, 'self_attn') and layer_idx > 12:  # レイヤーがself_attnを持ち、レイヤーインデックスが12より大きい場合\n",
    "\n",
    "        new_attention = Phi3Attention(config, layer_idx)  # 新しいアテンションを作成\n",
    "\n",
    "        # ウェイトのコピー\n",
    "        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)\n",
    "        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)\n",
    "\n",
    "        layer.self_attn = new_attention  # 置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.640016Z",
     "iopub.status.busy": "2024-07-28T12:43:01.639281Z",
     "iopub.status.idle": "2024-07-28T12:43:01.648891Z",
     "shell.execute_reply": "2024-07-28T12:43:01.648021Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.639992Z"
    }
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()  # クロスエントロピー損失関数の定義\n",
    "\n",
    "class LoraModelForClassification(nn.Module):\n",
    "    def __init__(self, lora_model):  # LoRAモデルの初期化\n",
    "        super(LoraModelForClassification, self).__init__()\n",
    "        self.config = lora_model.config  # モデルの設定\n",
    "        self.peft_model = lora_model  # LoRAモデルの保存\n",
    "        self.dropout = nn.Dropout(0.1)  # ドロップアウト層\n",
    "        self.classifier = nn.Linear(self.config.hidden_size, 3)  # 分類器の定義（3クラス分類）\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.peft_model(input_ids, attention_mask=attention_mask)  # LoRAモデルのフォワードパス\n",
    "        pooled_output = outputs.last_hidden_state.mean(dim=1)  # プーリング操作\n",
    "        output_dropout = self.dropout(pooled_output)  # ドロップアウト適用\n",
    "        logits = self.classifier(output_dropout)  # 分類器による出力\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = loss_fn(logits, labels)  # ラベルが存在する場合は損失を計算\n",
    "        return loss, logits  # 損失とロジットを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.650213Z",
     "iopub.status.busy": "2024-07-28T12:43:01.649876Z",
     "iopub.status.idle": "2024-07-28T12:43:01.672271Z",
     "shell.execute_reply": "2024-07-28T12:43:01.671442Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.650188Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータの読み込み\n",
    "len(test)  # テストデータのサンプル数を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.675399Z",
     "iopub.status.busy": "2024-07-28T12:43:01.675065Z",
     "iopub.status.idle": "2024-07-28T12:43:01.681955Z",
     "shell.execute_reply": "2024-07-28T12:43:01.680916Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.675365Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# JSON形式の文字列をデータフレームの各カラムに適用する\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(lambda x: json.loads(x)[0])  # プロンプトの抽出\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(lambda x: json.loads(x)[0])  # 応答Aの抽出\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(lambda x: json.loads(x)[0])  # 応答Bの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.683609Z",
     "iopub.status.busy": "2024-07-28T12:43:01.683057Z",
     "iopub.status.idle": "2024-07-28T12:43:01.692052Z",
     "shell.execute_reply": "2024-07-28T12:43:01.691221Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.683579Z"
    }
   },
   "outputs": [],
   "source": [
    "test_0 = test[:len(test)//2].reset_index(drop=True)  # テストデータの前半を取得\n",
    "test_1 = test[len(test)//2:].reset_index(drop=True)  # テストデータの後半を取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.693824Z",
     "iopub.status.busy": "2024-07-28T12:43:01.693506Z",
     "iopub.status.idle": "2024-07-28T12:43:01.70146Z",
     "shell.execute_reply": "2024-07-28T12:43:01.700419Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.693795Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast\n",
    "\n",
    "def infer(model, dataloader, device):\n",
    "    # モデルを評価モードに設定\n",
    "    model.eval()\n",
    "\n",
    "    target_list = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        with torch.no_grad():\n",
    "            with autocast():  # 自動混合精度の適用\n",
    "                input_ids = batch[\"input_ids\"].to(device)  # 入力IDをデバイスに転送\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)  # アテンションマスクをデバイスに転送\n",
    "                _, logits = model(input_ids=input_ids, attention_mask=attention_mask)  # モデル推論\n",
    "                softmax_logits = torch.nn.functional.softmax(logits, dim=1)  # ソフトマックスを適用\n",
    "                target_list.append(softmax_logits)  # 結果をリストに追加\n",
    "\n",
    "    return target_list  # 推論結果を返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.703165Z",
     "iopub.status.busy": "2024-07-28T12:43:01.702729Z",
     "iopub.status.idle": "2024-07-28T12:43:01.71508Z",
     "shell.execute_reply": "2024-07-28T12:43:01.714342Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.703135Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "\n",
    "gpu0 = \"cuda:0\"  # 使用するGPUデバイス\n",
    "gpu1 = \"cuda:1\"  # 使用するGPUデバイス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T12:43:01.716349Z",
     "iopub.status.busy": "2024-07-28T12:43:01.716089Z",
     "iopub.status.idle": "2024-07-28T12:43:14.445907Z",
     "shell.execute_reply": "2024-07-28T12:43:14.444988Z",
     "shell.execute_reply.started": "2024-07-28T12:43:01.716327Z"
    }
   },
   "outputs": [],
   "source": [
    "model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16\n",
    "                                  , device_map=\"cpu\")  # モデルのロード\n",
    "model0 = quantize_model(model0)  # モデルの量子化\n",
    "for idx, layer in enumerate(model0.layers):\n",
    "    replace_attention_module(model0.config, layer, idx)  # アテンションモジュールの置き換え\n",
    "model0 = LoraModelForClassification(model0)  # LoRAモデルの構築\n",
    "\n",
    "model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16\n",
    "                                  , device_map=\"cpu\")  # モデルのロード\n",
    "model1 = quantize_model(model1)  # モデルの量子化\n",
    "for idx, layer in enumerate(model1.layers):\n",
    "    replace_attention_module(model1.config, layer, idx)  # アテンションモジュールの置き換え\n",
    "model1 = LoraModelForClassification(model1)  # LoRAモデルの構築\n",
    "\n",
    "\n",
    "model0.load_state_dict(torch.load(model_path))  # モデルの重みをロード\n",
    "model1.load_state_dict(torch.load(model_path))  # モデルの重みをロード\n",
    "model0.to(gpu0)  # GPUデバイスにモデルを転送\n",
    "model1.to(gpu1)  # GPUデバイスにモデルを転送"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer0 = AutoTokenizer.from_pretrained(model_name)  # トークナイザーのロード\n",
    "\n",
    "if tokenizer0.pad_token is None:\n",
    "    tokenizer0.pad_token = tokenizer0.eos_token  # パディングトークンの設定\n",
    "tokenizer0.padding_side = \"right\"  # fp16トレーニングでの変 overflow問題を修正\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(model_name)  # トークナイザーのロード\n",
    "\n",
    "if tokenizer1.pad_token is None:\n",
    "    tokenizer1.pad_token = tokenizer1.eos_token  # パディングトークンの設定\n",
    "tokenizer1.padding_side = \"right\"  # fp16トレーニングでの変 overflow問題を修正\n",
    "\n",
    "test_dataloader0 = create_dataloaders(test_0, tokenizer0, test_max_len, test_batch_size, shuffle=False)  # データローダの作成\n",
    "test_dataloader1 = create_dataloaders(test_1, tokenizer1, test_max_len, test_batch_size, shuffle=False)  # データローダの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, dataloader, device, results, index):\n",
    "    # 推論処理を別スレッドで実行\n",
    "    results[index] = infer(model, dataloader, device)  # 推論結果を格納\n",
    "\n",
    "results = {}\n",
    "\n",
    "process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results, 0))  # スレッドの作成\n",
    "process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results, 1))  # スレッドの作成\n",
    "\n",
    "# プロセスを開始\n",
    "process0.start()\n",
    "process1.start()\n",
    "\n",
    "# 両方のプロセスが終了するのを待つ\n",
    "process0.join()\n",
    "process1.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'  # 移動するデバイスを指定\n",
    "for k, v in results.items():\n",
    "    for i in range(len(v)):\n",
    "        results[k][i] = v[i].to(device)  # 結果を指定デバイスに転送\n",
    "\n",
    "# 辞書の値を一つにまとめる\n",
    "target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)  # 結果を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルの読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []  # データフレームを格納するリストを初期化\n",
    "for tensor in target_list:\n",
    "    # テンソルをデータフレームに変換\n",
    "    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "    df_list.append(df)  # データフレームをリストに追加\n",
    "\n",
    "combined_df = pd.concat(df_list, axis=0, ignore_index=True)  # リスト内のデータフレームを結合\n",
    "\n",
    "sub = sub.set_index(pd.Index(combined_df.index))  # インデックスを設定\n",
    "\n",
    "final_df = pd.concat([sub[['id']], combined_df], axis=1)  # ID列と結果を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_files_and_folders(path):\n",
    "    # 指定したパスが存在するか確認\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"エラー: {path} は存在しません。\")\n",
    "        return\n",
    "\n",
    "    # パス内のすべてのファイルとフォルダを走査\n",
    "    for root, dirs, files in os.walk(path, topdown=False):\n",
    "        # ファイルを削除\n",
    "        for name in files:\n",
    "            if name == \"submission.csv\":\n",
    "                print(f\"スキップするファイル: {os.path.join(root, name)}\")\n",
    "                continue\n",
    "            file_path = os.path.join(root, name)  # ファイルパスを組み立て\n",
    "            print(f\"削除するファイル: {file_path}\")  # 削除するファイルを出力\n",
    "            os.remove(file_path)  # ファイルを削除\n",
    "\n",
    "    print(f\"{path}内のすべてのファイルとフォルダが削除されました。\")  # 削除完了メッセージ\n",
    "\n",
    "# 例としてのパス\n",
    "path_to_delete = \"/kaggle/working/\"  # 削除対象のパス\n",
    "\n",
    "# ファイルとフォルダ削除関数を呼び出し\n",
    "delete_files_and_folders(path_to_delete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('submission.csv', index=False)  # 結果をCSVファイルとして保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()  # データフレームの先頭を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPUメモリをクリアする関数\n",
    "def clear_gpu_memory():\n",
    "    torch.cuda.empty_cache()  # GPUメモリのキャッシュをクリア\n",
    "    gc.collect()  # ガベージコレクタを呼び出し\n",
    "\n",
    "# 学習後にGPUメモリをクリア\n",
    "clear_gpu_memory()  # メモリをクリアする関数を実行"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5456005,
     "sourceId": 9055296,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 95068,
     "modelInstanceId": 69944,
     "sourceId": 83272,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
