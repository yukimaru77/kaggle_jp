{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da488a59",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、「LMSYS - Chatbot Arena」コンペティションのためにユーザーの好みに基づいた予測を行うモデルの構築に取り組んでいます。具体的には、2つの異なる応答からどちらがユーザーに好まれるかを予測し、その結果を確率形式で出力することを目指しています。\n",
    "\n",
    "### 問題の概要\n",
    "Notebookは、与えられたチャットボットの応答に対して、どちらが優れているかを判断するための機械学習モデルを開発することに焦点を当てています。ユーザーが与えたプロンプトに対して生成された2つの応答を基に、いずれかの応答が優れているか、または同等であるかを予測します。\n",
    "\n",
    "### 使用されている手法・ライブラリ\n",
    "- **Transformersライブラリ**: Hugging FaceのTransformersを利用して大規模言語モデル（特にLlamaモデル）を扱います。\n",
    "- **Datasetライブラリ**: PyTorchのデータセット、特に`datasets`ライブラリを使用してデータの読み込みと前処理を行います。\n",
    "- **PEFT (Parameter-Efficient Fine-Tuning)**: モデルのファインチューニングには、データ効率を高めるためにLoRA（Low-Rank Adaptation）を利用しています。\n",
    "- **Scikit-learn**: おもに評価メトリクス（精度と対数損失の計算）を行うために使用されています。\n",
    "- **PandasとNumPy**: データ操作に利用され、特にデータフレームを使用してデータの読み込みや変換を行っています。\n",
    "\n",
    "### 主要なプロセス\n",
    "1. **データのセットアップ**: Kaggleのデータセットを読み込んで20行分のサンプルデータを取得し、ラベルをエンコードします。\n",
    "2. **トークナイゼーションと前処理**: 入力データのトークン化を行い、モデルに適した形式に整形します。\n",
    "3. **モデルアーキテクチャの定義**: カスタムLlamaモデルを定義し、適切なロジックを実装します。損失計算やトークンシフトなどの処理も含まれています。\n",
    "4. **トレーニングの設定**: トレーニング引数を設定し、評価戦略や最適化手法を明確にします。\n",
    "5. **モデルのトレーニング**: Trainerを使用してモデルをトレーニングし、トレーニング済みモデルを保存します。\n",
    "6. **推論プロセスの実装**: 新しいデータに対してモデルを評価し、応答の優劣に基づく確率を算出します。\n",
    "\n",
    "### 結果の出力\n",
    "最終的に、Notebookは選択された応答に対するモデルAとモデルBの勝利確率、及び引き分け確率を返す結果データフレームを生成します。この確率は、提出用のCSVファイルのフォーマットに適合しています。\n",
    "\n",
    "このノートブックは、ユーザーの好みを効果的に予測するためのモデル構築とその実行フローを示しており、実践的な機械学習のスキルを活用しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eead2f0e",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に基づいて、機械学習や深層学習の初心者がつまずきそうな専門用語の簡単な解説を示します。特に、実務経験がない場合やこのノートブック特有のドメイン知識に関するものに焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **LoraConfig**: Low-Rank Adaptation（LoRA）に関連する設定クラス。大規模言語モデル（LLM）のパラメータを効率的に調整するために使用される。学習に必要なパラメータ数を劇的に減らし、特にメモリ制限のある環境でのトレーニングを可能にする。\n",
    "\n",
    "2. **k-bitトレーニング**: モデルの重みをkビットの精度で保存し、計算にもkビットを使用することで、メモリ効率を高める技術。トレーニングや推論が高速化し、リソースの節約が図られるが、通常の浮動小数点数よりも精度が低くなる。\n",
    "\n",
    "3. **DataCollator**: トレーニングデータをバッチ化する際に、データを適切な形に整形するクラス。例えば、異なる長さのサンプルを同じ形に整えるためにパディングを行う。\n",
    "\n",
    "4. **CausalLMOutputWithPast**: 自然言語生成モデルの出力に特化したクラス。過去の隠れ層の状態（hidden states）を保持しつつ、モデルの予測結果や損失を返す。\n",
    "\n",
    "5. **Attention Mask**: モデルに入力を与える際に、どのトークンがモデルによって処理されるべきかを指定するためのマスク。通常、パディングされた部分を無視させるために使用される。\n",
    "\n",
    "6. **shift_logits**: モデルが次のトークンを予測するために、以前のトークンの予測結果から1ステップ分シフトさせたロジット。次のトークン予測のためには、前のトークンまでの情報が必要なため。\n",
    "\n",
    "7. **Logits**: モデルが出力する生の予測値群。通常は、クラスごとのスコアで、これにソフトマックス関数を適用することで確率に変換される。\n",
    "\n",
    "8. **Metric for best model**: モデルの性能を評価するための指標。特に、トレーニング中に最良のモデルを保存する際に基準となるメトリクスを指定する。ここでは「log_loss」が使われている。\n",
    "\n",
    "9. **Gradient Accumulation**: バッチサイズを増やさずに、数ステップの勾配を結合してパラメータを更新する手法。メモリを節約しつつ、大きなバッチサイズの効果を得ることができる。\n",
    "\n",
    "10. **np.isin**: NumPyの関数で、指定した配列内の要素が別の配列に存在するかどうかを判断する。データをフィルタリングする際に便利。\n",
    "\n",
    "これらの用語は、実務経験がないと具体的な意味や用途がわかりにくいことがありますが、ノートブックの文脈で何を行おうとしているかを理解する助けになります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330de5f2",
   "metadata": {},
   "source": [
    "# セットアップ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:01.882737Z",
     "iopub.status.busy": "2024-07-28T14:10:01.882391Z",
     "iopub.status.idle": "2024-07-28T14:10:32.192327Z",
     "shell.execute_reply": "2024-07-28T14:10:32.191393Z",
     "shell.execute_reply.started": "2024-07-28T14:10:01.882707Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U \"transformers>=4.42.3\" bitsandbytes accelerate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:32.19456Z",
     "iopub.status.busy": "2024-07-28T14:10:32.194259Z",
     "iopub.status.idle": "2024-07-28T14:10:50.243769Z",
     "shell.execute_reply": "2024-07-28T14:10:50.243013Z",
     "shell.execute_reply.started": "2024-07-28T14:10:32.194534Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaPreTrainedModel,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95686997",
   "metadata": {},
   "source": [
    "## モデルの設定\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:50.24555Z",
     "iopub.status.busy": "2024-07-28T14:10:50.24483Z",
     "iopub.status.idle": "2024-07-28T14:10:50.283887Z",
     "shell.execute_reply": "2024-07-28T14:10:50.282869Z",
     "shell.execute_reply.started": "2024-07-28T14:10:50.245513Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e3f0e9",
   "metadata": {},
   "source": [
    "## サンプルデータの読み込み\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:50.287241Z",
     "iopub.status.busy": "2024-07-28T14:10:50.286357Z",
     "iopub.status.idle": "2024-07-28T14:10:53.662602Z",
     "shell.execute_reply": "2024-07-28T14:10:53.661834Z",
     "shell.execute_reply.started": "2024-07-28T14:10:50.287216Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(20)  # データセットの最初の20行を取得\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行の最大値のインデックスを 'label' 列に追加\n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # 'label' 列をエンコード\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要な列のみを残す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8952dc",
   "metadata": {},
   "source": [
    "## トークナイザーとデータセットの前処理\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:53.663955Z",
     "iopub.status.busy": "2024-07-28T14:10:53.663655Z",
     "iopub.status.idle": "2024-07-28T14:10:56.889319Z",
     "shell.execute_reply": "2024-07-28T14:10:56.888515Z",
     "shell.execute_reply.started": "2024-07-28T14:10:53.663917Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path)  # モデルからトークナイザーを読み込む\n",
    "tokenizer.add_eos_token = True  # 終端トークンを追加\n",
    "tokenizer.padding_side = 'right'  # パディングを右側に設定\n",
    "\n",
    "# 各ラベルに対するトークンIDを取得\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "# 各サンプルをトークナイズする関数\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    # 最大長を超える場合、プロンプトと応答を切り詰める\n",
    "    if len(prompt + response_a + response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    \n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]  # ラベルのトークンIDを取得\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]  # アテンションマスクの作成\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:56.890762Z",
     "iopub.status.busy": "2024-07-28T14:10:56.890455Z",
     "iopub.status.idle": "2024-07-28T14:10:57.212413Z",
     "shell.execute_reply": "2024-07-28T14:10:57.211643Z",
     "shell.execute_reply.started": "2024-07-28T14:10:56.890737Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)  # Pandasデータフレームをデータセットに変換\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,  # 元の列を削除\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "n_splits = 5  # 交差検証の分割数\n",
    "fold_idx = 0  # 使用するフォールドのインデックス\n",
    "ds = load_data(train, tokenizer)  # トークン化されたデータをロード\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],  # トレーニングインデックス\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]  # 評価インデックス\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c5c4a4",
   "metadata": {},
   "source": [
    "## メトリクス\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:02:43.326732Z",
     "iopub.status.busy": "2024-07-28T14:02:43.326452Z",
     "iopub.status.idle": "2024-07-28T14:02:43.334273Z",
     "shell.execute_reply": "2024-07-28T14:02:43.333179Z",
     "shell.execute_reply.started": "2024-07-28T14:02:43.326708Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    logits, labels = pred  # 予測結果とラベルを取得\n",
    "    preds = logits.argmax(axis=-1)  # 最も高いロジット値を持つインデックスを取得\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスのマッピングを作成\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]  # 有効なラベルのみをフィルタリング\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])  # マッピングされたラベルを取得\n",
    "    acc = accuracy_score(labels, preds)  # 精度を計算\n",
    "    probs = softmax(logits, axis=-1)  # ソフトマックスを計算して確率を取得\n",
    "    log_loss_ = log_loss(labels, probs)  # ログロスを計算\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログロスを辞書で返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d8f48a",
   "metadata": {},
   "source": [
    "## モデル\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomLlama3(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みの関連付けを作成\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)  # Llamaモデルの構成\n",
    "        self.vocab_size = config.vocab_size  # 単語のボキャブラリサイズ\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 言語モデルのヘッド\n",
    "        self.post_init()  # 初期化後の処理\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ層の状態を取得\n",
    "        if self.config.pretraining_tp > 1:  # モデル並列化を使用する場合\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)  # ロジットを結合\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # ロジットを算出\n",
    "\n",
    "        logits = logits.float()  # ロジットを浮動小数点数に変換\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # 次のトークンを予測するためにシフトする\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # シフトしたロジット\n",
    "            shift_labels = labels[..., 1:].contiguous()  # シフトしたラベル\n",
    "            # トークンをフラット化する\n",
    "            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失を定義\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # フラット化\n",
    "            shift_labels = shift_labels.view(-1)  # フラット化\n",
    "            # モデル並列化を有効にする\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルのトークンIDをテンソルに変換\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 有効なラベルのみを取得\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 有効なロジットを取得\n",
    "            loss = loss_fct(true_logits, true_labels)  # 損失を計算\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLlama3.from_pretrained(\n",
    "    model_path, \n",
    "    load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    cache_dir=\"/kaggle/working/model\"\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを使用しない\n",
    "model = prepare_model_for_kbit_training(model)  # k-bitトレーニングの準備\n",
    "model = get_peft_model(model, peft_config)  # PEFTモデルの取得\n",
    "print(model)  # モデルの情報を表示\n",
    "model.print_trainable_parameters()  # 学習可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e118f323",
   "metadata": {},
   "source": [
    "### トレーニング引数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:10:47.631227Z",
     "iopub.status.busy": "2024-07-28T13:10:47.630555Z",
     "iopub.status.idle": "2024-07-28T13:10:47.660935Z",
     "shell.execute_reply": "2024-07-28T13:10:47.659985Z",
     "shell.execute_reply.started": "2024-07-28T13:10:47.63119Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(\n",
    "    output_dir='output',  # 出力ディレクトリ\n",
    "    overwrite_output_dir=True,  # 上書きを許可\n",
    "    evaluation_strategy=\"epoch\",  # エポックごとに評価\n",
    "    save_strategy=\"steps\",  # ステップごとに保存\n",
    "    save_steps=5,  # 5ステップごとに保存\n",
    "    save_total_limit=1,  # 保存される最大数\n",
    "    logging_strategy=\"steps\",  # ステップごとのロギング\n",
    "    logging_steps=10,  # 10ステップごとにログを記録\n",
    "    warmup_steps=20,  # ウォームアップステップ数\n",
    "    optim=\"adamw_8bit\",  # 最適化手法\n",
    "    learning_rate=2e-4,  # 学習率\n",
    "    per_device_train_batch_size=2,  # トレーニングのバッチサイズ（デバイスあたり）\n",
    "    per_device_eval_batch_size=2,  # 評価のバッチサイズ（デバイスあたり）\n",
    "    gradient_accumulation_steps=2,  # 勾配の累積ステップ数\n",
    "    num_train_epochs=1,  # トレーニングエポック数\n",
    "    fp16=True,  # 混合精度トレーニングを有効にする\n",
    "    metric_for_best_model=\"log_loss\",  # 最良モデルのメトリクス\n",
    "    greater_is_better=False,  # メトリクスの良好さ\n",
    "    report_to=\"none\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6b248c",
   "metadata": {},
   "source": [
    "# トレーニング（トレーニングまたは推論のいずれかを実行）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:10:47.662368Z",
     "iopub.status.busy": "2024-07-28T13:10:47.662033Z",
     "iopub.status.idle": "2024-07-28T13:18:14.683109Z",
     "shell.execute_reply": "2024-07-28T13:18:14.682131Z",
     "shell.execute_reply.started": "2024-07-28T13:10:47.662344Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),  # トレーニングデータセットを選択\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセットを選択\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータの設定\n",
    "    compute_metrics=compute_metrics,  # メトリクスの計算\n",
    ")\n",
    "trainer.train()  # トレーニングを実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T13:18:14.685291Z",
     "iopub.status.busy": "2024-07-28T13:18:14.684401Z",
     "iopub.status.idle": "2024-07-28T13:18:15.204301Z",
     "shell.execute_reply": "2024-07-28T13:18:15.20348Z",
     "shell.execute_reply.started": "2024-07-28T13:18:14.685255Z"
    }
   },
   "outputs": [],
   "source": [
    "model.save_pretrained('pretrained_model')  # モデルを保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa7ea9",
   "metadata": {},
   "source": [
    "# 推論（トレーニングまたは推論のいずれかを実行）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:57.213717Z",
     "iopub.status.busy": "2024-07-28T14:10:57.213437Z",
     "iopub.status.idle": "2024-07-28T14:10:57.218427Z",
     "shell.execute_reply": "2024-07-28T14:10:57.217578Z",
     "shell.execute_reply.started": "2024-07-28T14:10:57.213693Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:10:57.220185Z",
     "iopub.status.busy": "2024-07-28T14:10:57.219623Z",
     "iopub.status.idle": "2024-07-28T14:12:37.907647Z",
     "shell.execute_reply": "2024-07-28T14:12:37.906698Z",
     "shell.execute_reply.started": "2024-07-28T14:10:57.220153Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# 事前にトレーニングされたモデルとLoraアダプタのファイルパス\n",
    "model_path = \"unsloth/llama-3-8b-Instruct-bnb-4bit\"\n",
    "lora_adapter_path = \"/kaggle/working/pretrained_model\"\n",
    "\n",
    "# 原モデルを読み込む\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "\n",
    "# 対応するトークナイザーを読み込む\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Loraの設定\n",
    "lora_config = peft_config\n",
    "\n",
    "# 必要に応じてk-bitトレーニングの準備\n",
    "model_1 = prepare_model_for_kbit_training(model_1)\n",
    "\n",
    "# モデルにLoraアダプタを適用\n",
    "model_1 = get_peft_model(model_1, lora_config)\n",
    "\n",
    "# 保存されたLoraアダプタのパラメータを読み込む\n",
    "model_1.load_adapter(lora_adapter_path, adapter_name=\"test\")\n",
    "\n",
    "# 完全なモデルが使用できる状態になりました\n",
    "model_1.eval()  # 評価モードに設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:37:05.617083Z",
     "iopub.status.busy": "2024-07-28T14:37:05.616197Z",
     "iopub.status.idle": "2024-07-28T14:37:05.622623Z",
     "shell.execute_reply": "2024-07-28T14:37:05.621685Z",
     "shell.execute_reply.started": "2024-07-28T14:37:05.617048Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def softmax(row):\n",
    "    e_row = np.exp(row - np.max(row))  # ソフトマックス計算のための安定化\n",
    "    return e_row / e_row.sum()  # ソフトマックスを計算して正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:12:37.917502Z",
     "iopub.status.busy": "2024-07-28T14:12:37.917246Z",
     "iopub.status.idle": "2024-07-28T14:12:37.954873Z",
     "shell.execute_reply": "2024-07-28T14:12:37.954057Z",
     "shell.execute_reply.started": "2024-07-28T14:12:37.917481Z"
    }
   },
   "outputs": [],
   "source": [
    "data = ds.to_pandas()[0:10]  # データセットをPandasデータフレームに変換し最初の10行を取得\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)  # 各入力の長さを計算\n",
    "display(data[:3])  # 最初の3行を表示\n",
    "print()\n",
    "\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 0番目の入力のデコード結果を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:12:37.956701Z",
     "iopub.status.busy": "2024-07-28T14:12:37.956134Z",
     "iopub.status.idle": "2024-07-28T14:12:37.967859Z",
     "shell.execute_reply": "2024-07-28T14:12:37.966691Z",
     "shell.execute_reply.started": "2024-07-28T14:12:37.956669Z"
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配計算を無効にする\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を有効にする\n",
    "def inference(df, model, device, batch_size=2, max_length=1024):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝ち確率を格納するリスト\n",
    "\n",
    "    model.eval()  # モデルを評価モードに設定\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを決定\n",
    "        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチデータを取得\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストに変換\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストに変換\n",
    "        labels = tmp[\"labels\"].to_list()  # ラベルをリストに変換\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # 最長の長さに合わせてパディング\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # Pytorchテンソルとして返す\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)  # デバイスに移動\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)  # デバイスに移動\n",
    "        pad_labels = []  # パディングされたラベルを格納するリスト\n",
    "        for label in labels:\n",
    "            label = list(label) + [tokenizer.pad_token_id] * (input_ids[0].shape[0] - label.shape[0])  # ラベルをパディング\n",
    "            pad_labels.append(label)\n",
    "        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルの出力を取得\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ロジットにソフトマックスを適用して確率に変換\n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝ち確率を追加\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝ち確率を追加\n",
    "        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加\n",
    "    df['winner_model_a'] = a_win  # データフレームにモデルAの勝ち確率を追加\n",
    "    df['winner_model_b'] = b_win  # データフレームにモデルBの勝ち確率を追加\n",
    "    df['winner_tie'] = tie  # データフレームに引き分けの確率を追加\n",
    "    return df  # 結果のデータフレームを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:38:08.823708Z",
     "iopub.status.busy": "2024-07-28T14:38:08.822854Z",
     "iopub.status.idle": "2024-07-28T14:38:17.783376Z",
     "shell.execute_reply": "2024-07-28T14:38:17.782458Z",
     "shell.execute_reply.started": "2024-07-28T14:38:08.823675Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df = inference(data[0:4], model_1, device, batch_size=2, max_length=1024)  # 推論を実行\n",
    "\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 確率の値を取得される\n",
    "\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝ち確率をデータフレームに設定\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝ち確率をデータフレームに設定\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # 引き分けの確率をデータフレームに設定\n",
    "\n",
    "# 確率のリストをフラットにする\n",
    "result_df['winner_model_a'] = result_df['winner_model_a'].apply(lambda x: x[0])  \n",
    "result_df['winner_model_b'] = result_df['winner_model_b'].apply(lambda x: x[0])  \n",
    "result_df['winner_tie'] = result_df['winner_tie'].apply(lambda x: x[0])  \n",
    "\n",
    "result_df  # 最終結果のデータフレームを表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
