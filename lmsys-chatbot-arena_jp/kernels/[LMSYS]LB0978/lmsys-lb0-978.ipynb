{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1361f779",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションの人間による好み予測タスクに取り組んでいます。本コンペティションの目的は、大規模言語モデル（LLM）によって生成されたテキストの中から、ユーザーがどの応答を好むかを予測することです。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "研究者は、ユーザーからのテキストプロンプトに対する2つのモデル（AとB）の応答を比較し、人間の好みを予測するためのモデルを開発しています。本Notebookでは、与えられたデータを処理し、LLM（具体的にはLlamaモデル）とLightGBMを組み合わせて、各モデルの勝者（どちらのモデルの応答が好まれるか）を分類するタスクを行っています。\n",
    "\n",
    "### 使用する手法とライブラリ\n",
    "1. **ライブラリのインストール**:\n",
    "   - `bitsandbytes`, `transformers`, `tokenizers`, `peft`などのライブラリを使用し、LLMの効率的な推論及びパラメータの微調整に活用しています。\n",
    "\n",
    "2. **データ加工**:\n",
    "   - テストデータを読み込み、テキストを前処理してモデル入力形式に整形します。\n",
    "\n",
    "3. **トークン化**:\n",
    "   - `transformers`ライブラリを用いて、テキストをトークン化し、モデルに適した形式に変換しています。\n",
    "\n",
    "4. **モデルのロードと設定**:\n",
    "   - LlamaモデルをGPUにロードし、量子化（BitsAndBytes）を使用してメモリ効率を向上させています。\n",
    "   - PEFT（Parameter-Efficient Fine-Tuning）を使用して、LoRa（Low-Rank Adaptation）に基づく設定を行い、モデルの重みをロードしています。\n",
    "\n",
    "5. **推論の実施**:\n",
    "   - 自動混合精度（autocast）を利用して、効率的に推論を行います。推論結果をデータフレームに格納します。\n",
    "\n",
    "6. **LightGBMによるモデルの統合**:\n",
    "   - テキスト特徴量を抽出し、LightGBMモデルも用いて予測を行い、最終的にLlamaモデルによる予測結果とブレンドします。\n",
    "\n",
    "7. **結果の保存**:\n",
    "   - ブレンドした予測結果をCSVファイルに保存し、コンペティションへの提出用データ形式を整えています。\n",
    "\n",
    "このNotebookは、複数の機械学習アルゴリズムと効率的なデータ処理手法を駆使して、LLMによる応答の好み予測を体系的に解決するアプローチを示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e58e72",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、初心者がつまずきそうな専門用語の簡単な解説を列挙します。\n",
    "\n",
    "1. **BitsAndBytes**:\n",
    "    - モデルのメモリ使用量を削減し、より効率的な計算を可能にするための技術やライブラリ。特に、8ビットの浮動小数点数でモデルの重みを表現し、メモリ使用量を削減する。\n",
    "\n",
    "2. **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "    - モデルのパラメータを効果的に微調整するためのアプローチ。少ないパラメータの変更で性能を向上させる手法で、特に大規模モデルにおいて、効率的に学習することができる。\n",
    "\n",
    "3. **LoraConfig**:\n",
    "    - PEFTの一部で、Low-Rank Adaptation (LoRA) の設定を定義するクラス。モデルの重みを低次元の行列に圧縮し、少ないリソースで微調整を行う。\n",
    "\n",
    "4. **autocast**:\n",
    "    - 自動的に混合精度計算を行うための機能。計算過程で使用する数値の精度（16ビット、32ビットなど）を動的に切り替えることで、計算の効率を上げたりメモリ使用量を削減する。\n",
    "\n",
    "5. **アテンションマスク (Attention Mask)**:\n",
    "    - モデルが入力データ中のどの部分に注目すべきかを示すためのマスク。トークンが無視されるべきか、処理されるべきかを指定するのに使用される。\n",
    "\n",
    "6. **デバイスマップ（device_map）**:\n",
    "    - 複数の計算デバイス（CPUやGPU）を使用してモデルを分散させるための設定。どの部分のモデルをどのデバイスに配置するかを指定する。\n",
    "\n",
    "7. **シンメトリック対数変換 (Symlog transformation)**:\n",
    "    - データのスケールを調整し、特に外れ値の影響を軽減させるために使用される変換方法。数値の正の値と負の値の対数を取る際に使用する。\n",
    "\n",
    "8. **LightGBM**:\n",
    "    - Microsoftが開発した、決定木に基づく勾配ブースティングフレームワーク。大規模データを効率的に扱えるように設計されており、高速かつ高精度な分類を実現する。\n",
    "\n",
    "9. **CountVectorizer**:\n",
    "    - テキストデータを数値の特徴に変換するための方法。特定の語の出現回数を数え、それを特徴量として使用する。\n",
    "\n",
    "10. **ガーベジコレクション (Garbage Collection)**:\n",
    "    - プログラム内で不要になったメモリを自動的に解放する機能。メモリリークを防ぎ、プログラムのパフォーマンスを維持する。\n",
    "\n",
    "これらの用語の理解は、ノートブックの内容をより深く理解するために役立ちます。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:31.881515Z",
     "iopub.status.busy": "2024-06-13T15:44:31.881147Z",
     "iopub.status.idle": "2024-06-13T15:45:25.464117Z",
     "shell.execute_reply": "2024-06-13T15:45:25.462947Z",
     "shell.execute_reply.started": "2024-06-13T15:44:31.881486Z"
    }
   },
   "outputs": [],
   "source": [
    "# bitsandbytesをインストールします。-qオプションは静かにインストールを行います。\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "# transformersをインストールします。-qオプションは静かにインストールを行います。\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "# tokenizersをインストールします。-qオプションは静かにインストールを行います。\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "# peftをインストールします。-qオプションは静かにインストールを行います。\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fdf724",
   "metadata": {},
   "source": [
    "このノートブックの作業は、以下のノートブックからインスパイアを受けています：\n",
    "* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n",
    "* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "## もしこれが役に立ちましたら、評価をいただけると嬉しいです\n",
    "\n",
    "## ライブラリのインポート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.805645Z",
     "iopub.status.busy": "2024-06-13T15:45:31.805368Z",
     "iopub.status.idle": "2024-06-13T15:45:31.816648Z",
     "shell.execute_reply": "2024-06-13T15:45:31.815718Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.805621Z"
    }
   },
   "outputs": [],
   "source": [
    "from threading import Thread\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# PyTorchをインポート\n",
    "import torch\n",
    "# NumPyをインポート\n",
    "import numpy as np\n",
    "# Pandasをインポート\n",
    "import pandas as pd\n",
    "# Transformersライブラリから必要なクラスをインポート\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "# PEFT（Parameter-Efficient Fine-Tuning）関連のクラスをインポート\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "# 自動混合精度を使用するためのクラスをインポート\n",
    "from torch.cuda.amp import autocast\n",
    "# ディスプレイ用の機能をインポート\n",
    "from IPython.display import display\n",
    "# PyTorchの関数をインポート\n",
    "import torch.nn.functional as F\n",
    "# tokenizersをインポート\n",
    "import tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.818268Z",
     "iopub.status.busy": "2024-06-13T15:45:31.817891Z",
     "iopub.status.idle": "2024-06-13T15:45:31.825778Z",
     "shell.execute_reply": "2024-06-13T15:45:31.824978Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.818231Z"
    }
   },
   "outputs": [],
   "source": [
    "# CUDAのメモリ効率的なSDPを有効化\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "# CUDAのフラッシュSDPを有効化\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "# モデル名の設定\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "# 重みのパスの設定\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "# 最大入力長の設定\n",
    "MAX_LENGTH = 2048\n",
    "# バッチサイズの設定\n",
    "BATCH_SIZE = 4\n",
    "# デバイスの設定（GPUを使用）\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a7c03a",
   "metadata": {},
   "source": [
    "## データの準備 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.827124Z",
     "iopub.status.busy": "2024-06-13T15:45:31.826869Z",
     "iopub.status.idle": "2024-06-13T15:45:31.847758Z",
     "shell.execute_reply": "2024-06-13T15:45:31.846742Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.8271Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータとサンプル提出データを読み込みます\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:45:31.849516Z",
     "iopub.status.busy": "2024-06-13T15:45:31.849026Z",
     "iopub.status.idle": "2024-06-13T15:45:31.883908Z",
     "shell.execute_reply": "2024-06-13T15:45:31.882997Z",
     "shell.execute_reply.started": "2024-06-13T15:45:31.849481Z"
    }
   },
   "outputs": [],
   "source": [
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    # 余分な括弧を取り除く\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # 各文を取り出してリストに格納\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # 文を空白で連結して返す\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "# 各列に対してprocess関数を適用\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# サンプル提出データとテストデータの最初の5行を表示\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# モデル用のテキストを準備します\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "# 最初のテキストを表示\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec988794",
   "metadata": {},
   "source": [
    "## トークン化\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.40071Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40106Z",
     "shell.execute_reply": "2024-06-13T15:44:22.400895Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.400882Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークナイザーをロードします\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# テキストデータをトークン化\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "# トークンIDとアテンションマスクをデバイスに移動\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "# 入力IDとアテンションマスクからデータフレームを作成\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63b0df2",
   "metadata": {},
   "source": [
    "## モデルのロード \n",
    "> 各GPUに1つのモデルをロードします。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.403623Z",
     "iopub.status.idle": "2024-06-13T15:44:22.404009Z",
     "shell.execute_reply": "2024-06-13T15:44:22.403846Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.403831Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットの重みをロード\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 計算に使用するデータ型\n",
    "    bnb_8bit_use_double_quant=False)  # ダブル量子化を使用しない\n",
    "\n",
    "# GPU 0にベースモデルをロード\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "# LlamaForSequenceClassificationモデルをロード\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # ラベル数を設定\n",
    "    torch_dtype=torch.float16,  # 使用するデータ型を設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:0')  # GPU 0にマッピング\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.405382Z",
     "iopub.status.idle": "2024-06-13T15:44:22.405698Z",
     "shell.execute_reply": "2024-06-13T15:44:22.405553Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.405539Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU 1にベースモデルをロード\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # ラベル数を設定\n",
    "    torch_dtype=torch.float16,  # 使用するデータ型を設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:1')  # GPU 1にマッピング\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDの設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc744556",
   "metadata": {},
   "source": [
    "## 重みのロード \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.407084Z",
     "iopub.status.idle": "2024-06-13T15:44:22.40743Z",
     "shell.execute_reply": "2024-06-13T15:44:22.407278Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.407264Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRaの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # ローラ次元\n",
    "    lora_alpha=32,  # ローラアルファ\n",
    "    lora_dropout=0.10,  # ドロップアウト率\n",
    "    bias='none',  # バイアス設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクのタイプ\n",
    "    target_modules=['o_proj', 'v_proj'])  # 対象モジュールの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.408351Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4087Z",
     "shell.execute_reply": "2024-06-13T15:44:22.408536Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.408521Z"
    }
   },
   "outputs": [],
   "source": [
    "# PEFTモデルを取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "# 重みをロード\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "# モデル1を設定\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()  # 評価モードに設定\n",
    "\n",
    "# 学習可能なパラメータを表示\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.410059Z",
     "iopub.status.idle": "2024-06-13T15:44:22.410418Z",
     "shell.execute_reply": "2024-06-13T15:44:22.410257Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.410242Z"
    }
   },
   "outputs": [],
   "source": [
    "gc.collect()  # ガーベジコレクションを実行"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf47cee8",
   "metadata": {},
   "source": [
    "## 推論\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.411343Z",
     "iopub.status.idle": "2024-06-13T15:44:22.411663Z",
     "shell.execute_reply": "2024-06-13T15:44:22.411513Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.4115Z"
    }
   },
   "outputs": [],
   "source": [
    "# 推論を行う関数\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []  # 出力クラスA\n",
    "    generated_class_b = []  # 出力クラスB\n",
    "    generated_class_c = []  # 出力クラスC\n",
    "\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    # データフレームをバッチサイズごとに繰り返す\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに移動\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに移動\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算を無効にする\n",
    "            with autocast():  # 自動混合精度を使用\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,  # 入力IDをモデルに渡す\n",
    "                    attention_mask=batch_attention_mask  # アテンションマスクをモデルに渡す\n",
    "                )\n",
    "        \n",
    "        # 出力の確率を計算\n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # 各クラスの確率を格納\n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    # データフレームに結果を追加\n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  # GPUメモリをクリア\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.412909Z",
     "iopub.status.idle": "2024-06-13T15:44:22.413243Z",
     "shell.execute_reply": "2024-06-13T15:44:22.413073Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41306Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 処理開始時間を記録\n",
    "\n",
    "N_SAMPLES = len(data)  # サンプル数を取得\n",
    "\n",
    "# データを2つのサブセットに分割\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()  # 前半のデータ\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 後半のデータ\n",
    "\n",
    "# スレッド内で推論を実行する関数\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 推論結果を格納\n",
    "\n",
    "# スレッドからの結果を格納するための辞書\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.414669Z",
     "iopub.status.idle": "2024-06-13T15:44:22.414992Z",
     "shell.execute_reply": "2024-06-13T15:44:22.414841Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.414828Z"
    }
   },
   "outputs": [],
   "source": [
    "# スレッドの開始\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッド0\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッド1\n",
    "\n",
    "t0.start()  # スレッド0を開始\n",
    "t1.start()  # スレッド1を開始\n",
    "\n",
    "# すべてのスレッドが終了するのを待つ\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 元のデータフレームに結果を結合\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"処理が完了しました。総時間: {time.time() - st}\")  # 処理時間を表示\n",
    "\n",
    "# サンプル提出に結果を追加\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.416588Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41692Z",
     "shell.execute_reply": "2024-06-13T15:44:22.416764Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.41675Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values  # 予測結果を配列に格納"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5ca230",
   "metadata": {},
   "source": [
    "## LGBM + tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.417972Z",
     "iopub.status.idle": "2024-06-13T15:44:22.41834Z",
     "shell.execute_reply": "2024-06-13T15:44:22.418143Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.418129Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ\n",
    "RUNPOD = os.path.exists('/workspace/')  # 実行環境の確認\n",
    "KAGGLE = not RUNPOD  # Kaggle環境であるかの確認\n",
    "if KAGGLE: \n",
    "    print('kaggle')  # Kaggle環境である場合、メッセージを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.419857Z",
     "iopub.status.idle": "2024-06-13T15:44:22.420268Z",
     "shell.execute_reply": "2024-06-13T15:44:22.420061Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.420047Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd  # Pandasをインポート\n",
    "except:\n",
    "    # Kaggle環境でPandasがない場合、インストール\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.421762Z",
     "iopub.status.idle": "2024-06-13T15:44:22.4221Z",
     "shell.execute_reply": "2024-06-13T15:44:22.421948Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.421934Z"
    }
   },
   "outputs": [],
   "source": [
    "# データのパスを設定\n",
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "# 実行環境がRUNPODの場合\n",
    "if RUNPOD:\n",
    "    # Kaggle APIの設定ファイルが存在しない場合\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle  # ディレクトリを作成\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # 設定ファイルをコピー\n",
    "        !chmod 600 /root/.kaggle/kaggle.json  # アクセス権を設定\n",
    "\n",
    "    # データファイルが存在しない場合、Kaggleからダウンロード\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/ \n",
    "        \n",
    "    # ダウンロード後、データを展開\n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.42329Z",
     "iopub.status.idle": "2024-06-13T15:44:22.423599Z",
     "shell.execute_reply": "2024-06-13T15:44:22.423456Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.423443Z"
    }
   },
   "outputs": [],
   "source": [
    "# パスを設定\n",
    "INPUT_PATH = '/kaggle/input/'  \n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n",
    "print(MODEL_PATH)  # モデルパスを表示\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスの設定\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.425238Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42556Z",
     "shell.execute_reply": "2024-06-13T15:44:22.425407Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.425394Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークナイザーの並列化を無効にする\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.426467Z",
     "iopub.status.idle": "2024-06-13T15:44:22.426784Z",
     "shell.execute_reply": "2024-06-13T15:44:22.426631Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.426618Z"
    }
   },
   "outputs": [],
   "source": [
    "# トレーニングデータ、テストデータ、サンプル提出データを読み込む\n",
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')\n",
    "# データの長さを表示\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.427766Z",
     "iopub.status.idle": "2024-06-13T15:44:22.42813Z",
     "shell.execute_reply": "2024-06-13T15:44:22.427955Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.42793Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}\n",
    "if False: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    params['fold'] = -1  # フォールドを設定\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1  # エポック数の設定\n",
    "params['n_lgb'] = 1  # LightGBMの数の設定\n",
    "params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルを設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:22.429052Z",
     "iopub.status.idle": "2024-06-13T15:44:22.429388Z",
     "shell.execute_reply": "2024-06-13T15:44:22.429238Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.429225Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0  # フルデータかどうかを設定\n",
    "N_FOLDS = int(params.get('n_folds', 3));  # フォールド数の設定\n",
    "FOLD = int(params.get('fold', 0))  # 現在のフォールドの設定\n",
    "SEED = int(params.get('seed', 3))  # シード値の設定\n",
    "SS = int(params.get('subsample', 1))  # サブサンプル数の設定\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)  # 設定値を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:22.459744Z",
     "iopub.status.busy": "2024-06-13T15:44:22.459023Z",
     "iopub.status.idle": "2024-06-13T15:44:23.504374Z",
     "shell.execute_reply": "2024-06-13T15:44:23.502893Z",
     "shell.execute_reply.started": "2024-06-13T15:44:22.45971Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# StratifiedKFoldの設定を行う関数\n",
    "def get_folds(train): \n",
    "    # StratifiedKFoldを使ってフォールドを作成\n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n",
    "\n",
    "# トレーニングとテスト用のIDを取得\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n",
    "# サブサンプル数に基づいてIDを選択\n",
    "if SS > 1:\n",
    "    train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # IDの重複がないことを確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.505134Z",
     "iopub.status.idle": "2024-06-13T15:44:23.505514Z",
     "shell.execute_reply": "2024-06-13T15:44:23.505348Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.505333Z"
    }
   },
   "outputs": [],
   "source": [
    "# 現在のマイクロ秒に基づいてシードを設定します\n",
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-13T15:44:23.506746Z",
     "iopub.status.idle": "2024-06-13T15:44:23.507061Z",
     "shell.execute_reply": "2024-06-13T15:44:23.50692Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.506907Z"
    }
   },
   "outputs": [],
   "source": [
    "# トレーニング、推論、および保存のフラグを設定\n",
    "TRAIN = False\n",
    "INFER = True \n",
    "SAVE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:23.666596Z",
     "iopub.status.busy": "2024-06-13T15:44:23.666257Z",
     "iopub.status.idle": "2024-06-13T15:44:27.845064Z",
     "shell.execute_reply": "2024-06-13T15:44:27.844121Z",
     "shell.execute_reply.started": "2024-06-13T15:44:23.66657Z"
    }
   },
   "outputs": [],
   "source": [
    "# LightGBMと特徴量抽出のためのライブラリをインポート\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-13T15:44:27.847295Z",
     "iopub.status.busy": "2024-06-13T15:44:27.846672Z",
     "iopub.status.idle": "2024-06-13T15:44:27.985709Z",
     "shell.execute_reply": "2024-06-13T15:44:27.984569Z",
     "shell.execute_reply.started": "2024-06-13T15:44:27.847259Z"
    }
   },
   "outputs": [],
   "source": [
    "# LightGBM関連のフラグ\n",
    "LGB = True\n",
    "# トレーニングするかどうかのフラグ\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n",
    "# 推論を行うかどうかのフラグ\n",
    "INFER_LGB = not TRAIN and LGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前に保存したCountVectorizerモデルを読み込む\n",
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # 主要なCountVectorizer\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # カスタムCountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# シンメトリック対数変換を行う関数\n",
    "def symlog(x):\n",
    "    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "# Dense行列を処理する関数\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())  # dense形式に変換\n",
    "    x = symlog(x)  # シンメトリック対数変換を適用\n",
    "    return x\n",
    "\n",
    "# 特徴量を取得する関数\n",
    "def get_features(df):\n",
    "    # promptから特徴量を抽出\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    # response_aから特徴量を抽出\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    # response_bから特徴量を抽出\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    # 特徴量を計算\n",
    "    v = np.hstack([\n",
    "          afeat - bfeat, np.abs(afeat - bfeat), \n",
    "        ])\n",
    "    try: \n",
    "        # 投票モデルの数で特徴量を割る\n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    extras = []  # 追加の特徴量を格納するリスト\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']  # 特徴量として使用する文字列\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)  # 特定の文字のカウントを追加\n",
    "            \n",
    "    # 文字列の長さと単語数を追加\n",
    "    extras.append(df[c].str.len())\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)  # スタックして配列に変換\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 特徴量を拡張\n",
    "    return np.hstack([v, extras])  # 全特徴量を結合して返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事前に保存したLightGBMモデルを読み込む\n",
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 推論を行う場合の処理\n",
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test  # テストデータセットを使用\n",
    "    yps = []  # 予測値を格納するリスト\n",
    "    b = 1000  # バッチサイズ\n",
    "    # テストデータをバッチごとに処理\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得\n",
    "        ypms = []  # 各モデルの予測を格納するリスト\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))  # 各モデルで予測\n",
    "        \n",
    "        yps.append(np.stack(ypms).mean(0))  # 予測値の平均を格納\n",
    "        print('.', end = '')  # 進行状況を表示\n",
    "        \n",
    "        # メモリ管理\n",
    "        if len(yps) % 2 == 0:\n",
    "            gc.collect()  # ガーベジコレクションを実行\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)  # すべての予測を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_preds = yp  # LightGBMの予測値を格納"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72fb239",
   "metadata": {},
   "source": [
    "## 予測のブレンド\n",
    "\n",
    "$\\operatorname{preds} = 0.05 \\cdot \\operatorname{lgbm \\ boosting \\ preds} + 0.8 \\cdot \\operatorname{llama \\ preds}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 予測のブレンドを行う\n",
    "lgb_wt = 0.05  # LightGBMの重み\n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測の加重平均"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をデータフレームに保存\n",
    "out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])  # 予測結果のデータフレームを作成\n",
    "display(out.head())  # 最初の数行を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 結果をCSVファイルに保存\n",
    "out.to_csv('submission.csv')  # 提出用ファイルを保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
