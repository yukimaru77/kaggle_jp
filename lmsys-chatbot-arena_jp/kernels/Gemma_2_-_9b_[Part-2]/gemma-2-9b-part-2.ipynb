{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b808c1f3",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleのコンペティション「LMSYS - Chatbot Arena」において、与えられた埋め込みデータを用いてシンプルな分類器を訓練し、推論を行うことを目的としています。具体的には、異なるLLM（大規模言語モデル）による応答に対して、どちらのモデルの応答がユーザーに好まれるかを予測するモデルを構築します。\n",
    "\n",
    "### 問題に取り組む背景\n",
    "このNotebookは、Chatbot Arenaからの埋め込みを使用して、ユーザーがどのモデルの応答を選好するかを予測するというタスクに挑戦しています。モデルは、与えられた埋め込みを基に応答の優劣を判断する必要があります。\n",
    "\n",
    "### 使用する手法とライブラリ\n",
    "1. **埋め込みの計算**: 'Gemma 2'という大規模言語モデルを使用して、テキストデータから埋め込みを生成します。これをPyTorchとTransformersライブラリを用いて実装します。埋め込みは数値的な表現で、これが後の分類タスクに利用されます。\n",
    "\n",
    "2. **分類器の訓練**: `CatBoostClassifier`を使用して、訓練データに基づいてモデルを構築します。CatBoostは、勾配ブースティングフレームワークの一種で、多くのデータセットにおいて良好な性能を示すとされています。パラメータ調整や学習率、イテレーション数の設定も行います。\n",
    "\n",
    "3. **データの前処理とトークナイジング**: テストデータに対して、ユーザープロンプトとモデルの応答を適切に整形し、トークナイザーを使用してトークン化を行います。\n",
    "\n",
    "4. **推論**: 訓練された分類器を用いて、テストデータに対する予測確率を計算します。\n",
    "\n",
    "5. **出力形式**: 最終的な予測結果はCSV形式で保存され、提出可能な形式に整形されます。\n",
    "\n",
    "全体として、このNotebookは深層学習と機械学習の技術を統合し、特に自然言語処理のタスクにおけるユーザーの好みを予測するための方法論を示しています。また、効率的なメモリ管理や多くの機能が備わったライブラリの利用を通じて、スムーズな実行を実現しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e774e5",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連し、初心者がつまずきやすい専門用語や概念に関する簡単な解説を示します。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **埋め込み (Embeddings)**:\n",
    "   - 言語やデータを高次元のベクトルに変換する技術。語や文の意味を保持したまま数値表現することで、機械学習モデルに入力しやすくする。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**:\n",
    "   - テキストデータをモバイルや単語などのトークンに分割する役割を持つ。このプロセスは、自然言語処理タスクにおいて非常に重要で、モデルがテキストを理解できる形式に変換する。\n",
    "\n",
    "3. **アテンションマスク (Attention Masks)**:\n",
    "   - トランスフォーマーモデルで用いられるもので、どのトークンに注意を払うかを示すバイナリのマスク。例えば、パディングトークンに対しては注意を払わない設定など。\n",
    "\n",
    "4. **量子化 (Quantization)**:\n",
    "   - モデルのパラメータを省メモリかつ効率的に扱うために、浮動小数点から整数型に変換する技術。特に、GPUメモリや計算資源の制約がある環境でのモデルの効率化に役立つ。\n",
    "\n",
    "5. **ガベージコレクション (Garbage Collection)**:\n",
    "   - 使用しなくなったメモリを自動的に解放するプロセス。特にPythonでは、メモリ管理を向上させるために用いられる。\n",
    "\n",
    "6. **自動混合精度 (Automatic Mixed Precision, AMP)**:\n",
    "   - 訓練中に異なるデータ型（例えば、float32 と float16）を組み合わせて使用することで、計算速度を向上させつつメモリ使用量を削減する技術。\n",
    "\n",
    "7. **早期停止 (Early Stopping)**:\n",
    "   - モデルが訓練データに過剰適合するのを防ぐため、検証セットのスコアが改善しなくなった時点で訓練を中止する方法。これにより、汎化性能が向上する。\n",
    "\n",
    "8. **スレッド (Thread)**:\n",
    "   - プロセスの実行単位で、並行処理に用いる。特にデータ処理や計算を効率的に行うために、複数のスレッドを利用して並行にタスクを実行する。\n",
    "\n",
    "9. **池 (Pool)**:\n",
    "   - CatBoostにおいて、データを内部で効率的に処理するためのデータ構造。通常、データのサンプルや特徴量を含む。\n",
    "\n",
    "10. **マルチクラス分類 (Multi-class Classification)**:\n",
    "    - 複数のクラス（カテゴリ）から一つを選択する問題設定。例えば、複数のモデルの中から最も好まれるモデルを選ぶ場合など。\n",
    "\n",
    "これらの用語は、具体的な実装やプロセスに関連しており、特に事前の実務経験がないと理解が難しい場合があるため、詳細な説明を加えました。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258b602",
   "metadata": {},
   "source": [
    "# Gemma 2 - 9b\n",
    "ここでは、[こちら](https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1?scriptVersionId=186083288)から得られた計算された埋め込みを入力として使用して、シンプルな分類器を訓練し、テスト用の埋め込みを計算し、訓練した分類器を用いて推論を行います。それでは始めましょう！\n",
    "\n",
    "もしこの内容が役に立ったと思ったら、いいねを押してください！\n",
    "\n",
    "# ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:44:19.939828Z",
     "iopub.status.busy": "2024-06-30T19:44:19.939541Z",
     "iopub.status.idle": "2024-06-30T19:44:58.916383Z",
     "shell.execute_reply": "2024-06-30T19:44:58.915213Z",
     "shell.execute_reply.started": "2024-06-30T19:44:19.939804Z"
    }
   },
   "outputs": [],
   "source": [
    "# bitsandbytesライブラリをインストールします。\n",
    "# -qオプションは出力を抑制し、-Uオプションは最新バージョンにアップグレードします。\n",
    "# --no-indexオプションはPyPIインデックスを使用しないことを指定します。\n",
    "# --find-linksオプションは、指定したローカルのパスを使ってライブラリをインストールします。\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip\n",
    "\n",
    "# transformersライブラリをインストールします。\n",
    "!pip install -q -U transformers --no-index --find-links ../input/libs-install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:45:17.484133Z",
     "iopub.status.busy": "2024-06-30T19:45:17.48338Z",
     "iopub.status.idle": "2024-06-30T19:45:24.216568Z",
     "shell.execute_reply": "2024-06-30T19:45:24.215788Z",
     "shell.execute_reply.started": "2024-06-30T19:45:17.484097Z"
    }
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポートします。\n",
    "import os  # オペレーティングシステムとの対話を行うためのライブラリ\n",
    "import gc  # ガベージコレクタを扱うためのライブラリ\n",
    "import re  # 正規表現を扱うためのライブラリ\n",
    "from time import time  # 時間を計測するためのライブラリ\n",
    "\n",
    "import torch  # PyTorchライブラリ\n",
    "import transformers  # Hugging FaceのTransformersライブラリ\n",
    "import sklearn  # Scikit-learnライブラリ\n",
    "import random  # ランダム数生成のためのライブラリ\n",
    "import numpy as np  # NumPyライブラリ（数値計算のため）\n",
    "import pandas as pd  # Pandasライブラリ（データ操作のため）\n",
    "import matplotlib.pyplot as plt  # データの可視化を行うためのライブラリ\n",
    "\n",
    "from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig  # Gemma2モデルとトークナイザーのインポート\n",
    "\n",
    "import time  # 再度時間計測用のライブラリをインポート\n",
    "from catboost import CatBoostClassifier, Pool  # CatBoost分類器のインポート\n",
    "from sklearn.model_selection import train_test_split  # データ分割のための関数をインポート\n",
    "from sklearn.metrics import accuracy_score, log_loss  # 評価指標の関数をインポート\n",
    "\n",
    "from torch.cuda.amp import autocast  # 自動混合精度訓練のためのモジュールをインポート\n",
    "from threading import Thread  # スレッドを扱うためのライブラリ\n",
    "\n",
    "# メモリ効率の良いシステムダイナミクスプログラミングを有効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# CUDAが使用できない場合はエラーメッセージを表示します。\n",
    "if (not torch.cuda.is_available()): \n",
    "    print(\"Sorry - GPU required!\")  # GPUが必要です。申し訳ありません！"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a22694",
   "metadata": {},
   "source": [
    "# 分類器の訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:38:03.532707Z",
     "iopub.status.busy": "2024-06-30T19:38:03.532342Z",
     "iopub.status.idle": "2024-06-30T19:38:09.162491Z",
     "shell.execute_reply": "2024-06-30T19:38:09.161636Z",
     "shell.execute_reply.started": "2024-06-30T19:38:03.532676Z"
    }
   },
   "outputs": [],
   "source": [
    "# 訓練データをCSVファイルから読み込みます。\n",
    "train_df = pd.read_csv('/kaggle/input/gemma-2-9b-part-1/train_embed.csv')\n",
    "\n",
    "# 訓練用の埋め込みデータをNumPy配列として読み込みます。\n",
    "train_embed = np.load('/kaggle/input/gemma-2-9b-part-1/gemma2_train_embed.npy')\n",
    "\n",
    "# 'winner_model_a', 'winner_model_b', 'winner_tie'の列の中で最大の値のインデックスを取得し、\n",
    "# それを'label'列としてデータフレームに追加します。\n",
    "train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  # どのモデルが勝者かを示すラベルを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:38:09.16517Z",
     "iopub.status.busy": "2024-06-30T19:38:09.16452Z",
     "iopub.status.idle": "2024-06-30T19:38:09.295013Z",
     "shell.execute_reply": "2024-06-30T19:38:09.293963Z",
     "shell.execute_reply.started": "2024-06-30T19:38:09.165118Z"
    }
   },
   "outputs": [],
   "source": [
    "# データを訓練セットとテストセットに分割します。\n",
    "Targets = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなる列を定義します。\n",
    "\n",
    "y = train_df['label'].values  # ラベルの配列を取得します。\n",
    "# train_test_split関数を使って、訓練データとテストデータのインデックスを分割します。\n",
    "train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)\n",
    "\n",
    "# 訓練データとそのラベルを設定します。\n",
    "X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values\n",
    "# テストデータとそのラベルを設定します。\n",
    "X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values\n",
    "\n",
    "# 訓練データとテストデータの形状を表示します。\n",
    "print(X_train.shape, y_train.shape)  # 訓練データの形状\n",
    "print(X_test.shape, y_test.shape)  # テストデータの形状"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:38:09.296384Z",
     "iopub.status.busy": "2024-06-30T19:38:09.29607Z",
     "iopub.status.idle": "2024-06-30T19:40:25.215201Z",
     "shell.execute_reply": "2024-06-30T19:40:25.214402Z",
     "shell.execute_reply.started": "2024-06-30T19:38:09.296357Z"
    }
   },
   "outputs": [],
   "source": [
    "# ここでは、デフォルト設定で分類器を使用します。パラメータの調整を試みることもできます。\n",
    "\n",
    "# CatBoostClassifierを初期化します。\n",
    "model_cb = CatBoostClassifier(\n",
    "    iterations=1000,  # 最大イテレーション数\n",
    "    learning_rate=0.03,  # 学習率\n",
    "    loss_function='MultiClass',  # マルチクラス分類の損失関数\n",
    "    eval_metric='MultiClass',  # 評価指標\n",
    "    early_stopping_rounds=10,  # 早期停止のためのラウンド数\n",
    "    task_type='GPU',  # GPUを使用します。\n",
    "    devices='0:1',  # 使用するGPUデバイスの指定\n",
    "    verbose=100)  # 進捗状況の表示間隔\n",
    "\n",
    "# モデルをフィッティングします。\n",
    "model_cb.fit(X_train, y_train, \n",
    "              eval_set=(X_test, y_test),  # 検証セットとしてテストデータを指定します。\n",
    "              early_stopping_rounds=50)  # 早期停止のためのラウンド数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:40:25.217867Z",
     "iopub.status.busy": "2024-06-30T19:40:25.21705Z",
     "iopub.status.idle": "2024-06-30T19:40:39.524493Z",
     "shell.execute_reply": "2024-06-30T19:40:39.523612Z",
     "shell.execute_reply.started": "2024-06-30T19:40:25.217834Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータに対する予測確率を取得します。\n",
    "y_pred_proba = model_cb.predict_proba(X_test)\n",
    "# テストデータに対する予測ラベルを取得します。\n",
    "y_pred = model_cb.predict(X_test)\n",
    "\n",
    "# モデルの評価を行います。\n",
    "logloss = log_loss(y_test, y_pred_proba)  # ログ損失を計算します。\n",
    "accuracy = accuracy_score(y_test, y_pred)  # 精度を計算します。\n",
    "gc.collect()  # ガベージコレクションを実行してメモリを解放します。\n",
    "\n",
    "# 結果を表示します。\n",
    "print(f'Log Loss: {logloss:.3f}')  # ログ損失を表示します。\n",
    "print(f'Accuracy: {accuracy:.3f}')  # 精度を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b59476",
   "metadata": {},
   "source": [
    "この分類器を推論に使用します。\n",
    "\n",
    "# Gemma 2の読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:45:34.255517Z",
     "iopub.status.busy": "2024-06-30T19:45:34.255007Z",
     "iopub.status.idle": "2024-06-30T19:48:09.97744Z",
     "shell.execute_reply": "2024-06-30T19:48:09.976588Z",
     "shell.execute_reply.started": "2024-06-30T19:45:34.255485Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルのパスと設定を定義します。\n",
    "MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'  # Gemma 2のモデルパス\n",
    "MAX_LENGTH = 1024  # 最大シーケンス長\n",
    "BATCH_SIZE = 2  # バッチサイズ\n",
    "    \n",
    "# 使用するGPUデバイスを指定します。\n",
    "device0 = torch.device('cuda:0')  # デバイス0を設定\n",
    "device1 = torch.device('cuda:1')  # デバイス1を設定\n",
    "\n",
    "# トークナイザーをモデルから読み込みます。\n",
    "tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# 4ビット量子化の設定を定義します。\n",
    "bnb_config_4bit = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4ビットで読み込む設定\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定\n",
    "    bnb_4bit_use_double_quant=False)  # ダブル量子化を使用しない設定\n",
    "\n",
    "# モデル0を読み込みます。\n",
    "model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        revision=\"float16\",  # float16バージョンを指定\n",
    "                                        device_map='cuda:0',  # デバイスマップをデバイス0に設定\n",
    "                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用        \n",
    "\n",
    "# モデル1を読み込みます。\n",
    "model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,\n",
    "                                        revision=\"float16\",  # float16バージョンを指定\n",
    "                                        device_map='cuda:1',  # デバイスマップをデバイス1に設定\n",
    "                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-30T19:40:41.08643Z",
     "iopub.status.idle": "2024-06-30T19:40:41.086888Z",
     "shell.execute_reply": "2024-06-30T19:40:41.086675Z",
     "shell.execute_reply.started": "2024-06-30T19:40:41.086656Z"
    }
   },
   "outputs": [],
   "source": [
    "# 入力文字列を処理する関数を定義します。\n",
    "def process(input_str):\n",
    "    # 文字列の前後の角括弧を取り除きます。\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # 文字列をカンマで分割し、各文から前後の引用符を取り除きます。\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # 最後の文を返します。文がなければ空文字列を返します。\n",
    "    return sentences[-1] if sentences else ''\n",
    "  \n",
    "# テストデータをCSVファイルから読み込みます。\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# 各カラムに対してprocess関数を適用し、結果を新しいカラムに保存します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # ユーザープロンプトの処理\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答の処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答の処理\n",
    "\n",
    "# テキストを指定の形式で組み合わせて新しいカラムを作成します。\n",
    "test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n----\\n\\nModel B:\\n'  + test['response_b'] + '<end_of_turn><eos>'\n",
    "\n",
    "# 生成されたテキストの最初の要素を表示します。\n",
    "print(test['text'][0])  # 最初のテキストを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d7d6ff",
   "metadata": {},
   "source": [
    "# トークナイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:06:53.303056Z",
     "iopub.status.busy": "2024-06-30T19:06:53.302703Z",
     "iopub.status.idle": "2024-06-30T19:06:53.350283Z",
     "shell.execute_reply": "2024-06-30T19:06:53.349471Z",
     "shell.execute_reply.started": "2024-06-30T19:06:53.303027Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータのテキストをトークナイズします。\n",
    "tokens = tokenizer(test['text'].tolist(),\n",
    "                   padding='max_length',  # 最大長さに合わせてパディングを行います。\n",
    "                   max_length=MAX_LENGTH,  # 最大長さを設定します。\n",
    "                   truncation=True,  # 長さが超えた場合は切り捨てます。\n",
    "                   return_tensors='pt')  # PyTorchのテンソルを返します。\n",
    "\n",
    "# トークナイズされたデータをデータフレームに変換します。\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]  # 入力IDをリストとして保存\n",
    "data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in tokens['attention_mask']]  # アテンションマスクをリストとして保存\n",
    "\n",
    "# データフレームの最初の2行を表示します。\n",
    "data[:2]  # 最初の2行のデータを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccfab0c",
   "metadata": {},
   "source": [
    "# 埋め込みを取得する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:07:31.83978Z",
     "iopub.status.busy": "2024-06-30T19:07:31.838907Z",
     "iopub.status.idle": "2024-06-30T19:07:31.84909Z",
     "shell.execute_reply": "2024-06-30T19:07:31.848135Z",
     "shell.execute_reply.started": "2024-06-30T19:07:31.839747Z"
    }
   },
   "outputs": [],
   "source": [
    "# 埋め込みを取得する関数を定義します。\n",
    "def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  \n",
    "    # データフレームから入力IDとアテンションマスクを取得します。\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルとして生成\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # アテンションマスクをテンソルとして生成\n",
    "\n",
    "    embed_list = []  # 埋め込みリストの初期化\n",
    "\n",
    "    # バッチサイズに応じてデータを処理します。\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを計算\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送\n",
    "        \n",
    "        gc.collect()  # ガベージコレクションを実行してメモリを解放\n",
    "        torch.cuda.empty_cache()  # CUDAメモリをクリアする\n",
    "\n",
    "        with torch.no_grad():  # 勾配計算を無効にしてメモリを節約\n",
    "            # モデルに入力して埋め込みを取得します。\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)\n",
    "            embed = outputs.hidden_states[-1]  # 最後の隠れ層の出力を取得\n",
    "            embed_mean = torch.mean(embed, dim=1).cpu()  # 平均プーリングを行い、CPUに転送\n",
    "            embed_list.append(embed_mean)  # 埋め込みリストに追加\n",
    "            \n",
    "            torch.cuda.empty_cache()  # CUDAメモリをクリアする\n",
    "        \n",
    "    # 埋め込みリストを結合して一つのテンソルを作成します。\n",
    "    embeddings = torch.cat(embed_list, dim=0)\n",
    "    return embeddings  # 埋め込みを返します。\n",
    "\n",
    "# 埋め込みを計算する関数を定義します。\n",
    "def compute_embed(df, model, device, results, index):\n",
    "    results[index] = get_embeddings(df, model, device)  # 指定されたインデックスに埋め込みを格納します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:07:54.565046Z",
     "iopub.status.busy": "2024-06-30T19:07:54.564396Z",
     "iopub.status.idle": "2024-06-30T19:08:10.694665Z",
     "shell.execute_reply": "2024-06-30T19:08:10.693693Z",
     "shell.execute_reply.started": "2024-06-30T19:07:54.565016Z"
    }
   },
   "outputs": [],
   "source": [
    "# 処理開始時刻を記録します。\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)  # データのサンプル数を取得\n",
    "half = round(N_SAMPLES / 2)  # サンプル数の半分を計算\n",
    "sub1 = data.iloc[0:half].copy()  # データの最初の半分をコピー\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # データの後半をコピー\n",
    "\n",
    "results = {}  # 埋め込み結果を格納する辞書を初期化\n",
    "\n",
    "# 2つのスレッドを作成し、それぞれのモデルとデバイスを指定します。\n",
    "t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))  # モデル0のためのスレッド\n",
    "t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))  # モデル1のためのスレッド\n",
    "\n",
    "# スレッドを開始します。\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# スレッドの終了を待ちます。\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 処理が完了したことを表示し、処理にかかった時間を表示します。\n",
    "print(f\"Processing complete. Total time: {time.time() - st:.2f} seconds\")  # 処理の合計時間を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:09:36.314841Z",
     "iopub.status.busy": "2024-06-30T19:09:36.313867Z",
     "iopub.status.idle": "2024-06-30T19:09:36.320831Z",
     "shell.execute_reply": "2024-06-30T19:09:36.319932Z",
     "shell.execute_reply.started": "2024-06-30T19:09:36.314808Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2つのスレッドから得られた埋め込みを結合します。\n",
    "test_embeddings = torch.cat([results[0], results[1]], dim=0)\n",
    "\n",
    "# 最終的な埋め込みの形状を表示します。\n",
    "test_embeddings.shape  # 埋め込みの形状を表示します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:08:52.876211Z",
     "iopub.status.busy": "2024-06-30T19:08:52.875516Z",
     "iopub.status.idle": "2024-06-30T19:08:53.470247Z",
     "shell.execute_reply": "2024-06-30T19:08:53.469297Z",
     "shell.execute_reply.started": "2024-06-30T19:08:52.87618Z"
    }
   },
   "outputs": [],
   "source": [
    "# ガベージコレクションを実行してメモリを解放します。\n",
    "gc.collect()  \n",
    "\n",
    "# モデル1を削除してメモリを解放します。\n",
    "del model_1  \n",
    "# モデル0を削除してメモリを解放します。\n",
    "del model_0  \n",
    "\n",
    "# CUDAメモリをクリアします。\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9e82ee",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:09:53.812765Z",
     "iopub.status.busy": "2024-06-30T19:09:53.812362Z",
     "iopub.status.idle": "2024-06-30T19:09:53.843767Z",
     "shell.execute_reply": "2024-06-30T19:09:53.842674Z",
     "shell.execute_reply.started": "2024-06-30T19:09:53.812734Z"
    }
   },
   "outputs": [],
   "source": [
    "# テスト埋め込みに対する予測確率を取得します。\n",
    "preds = model_cb.predict_proba(test_embeddings.numpy())  # テスト埋め込みをNumPy配列に変換してモデルに渡します。\n",
    "preds  # 予測確率を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2804c786",
   "metadata": {},
   "source": [
    "# 提出用ファイルの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:11:43.392109Z",
     "iopub.status.busy": "2024-06-30T19:11:43.391752Z",
     "iopub.status.idle": "2024-06-30T19:11:43.407772Z",
     "shell.execute_reply": "2024-06-30T19:11:43.406801Z",
     "shell.execute_reply.started": "2024-06-30T19:11:43.392082Z"
    }
   },
   "outputs": [],
   "source": [
    "# サンプル提出ファイルをCSVから読み込みます。\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# ターゲット列に予測結果を代入します。\n",
    "sample_sub[Targets] = preds\n",
    "\n",
    "# 最終的な提出用データを表示します。\n",
    "display(sample_sub)  # 提出用データの表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-30T19:11:58.588605Z",
     "iopub.status.busy": "2024-06-30T19:11:58.588251Z",
     "iopub.status.idle": "2024-06-30T19:11:58.599109Z",
     "shell.execute_reply": "2024-06-30T19:11:58.598312Z",
     "shell.execute_reply.started": "2024-06-30T19:11:58.58858Z"
    }
   },
   "outputs": [],
   "source": [
    "# 提出用ファイルをCSV形式で保存します。\n",
    "sample_sub.to_csv('submission.csv', index=False)  # インデックスなしで'submission.csv'として保存します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b740f98",
   "metadata": {},
   "source": [
    "# まとめ \n",
    "\n",
    "これで終わりです！アイデアを共有したかっただけです！分類器の調整や他の分類器の使用を試してみてください。ありがとうございました！\n",
    "\n",
    "もし何かを学んだなら、ぜひいいねを押してください:)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c528170",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## superferg\n",
    "> \n",
    "> Gemma2を直接分類用に訓練する予定はありますか？私はローカルのバリデーションセットで良いスコアを得たのですが、公開リーダーボードで対応するスコアを達成できていません。Llama3の推論コードが直接Gemma2に変更されてしまっているのではないかと疑っています。\n",
    "> \n",
    "> [詳細](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518408)\n",
    "> \n",
    "> \n",
    "> > ## Valentin Werner\n",
    "> > \n",
    "> > 推論ノートブックで同じ処理をしていることを確認してください。トークナイザー、モデルクラス、入力処理はすべて同一であるべきです。私の場合、提出ノートブック内でCVをテストして、同様のスコアが得られているかを見るのが役に立ちます。提出時に異なる量子化を行うと、トレーニング時とは異なる結果が出ることがあるかもしれませんが（ただし0.95から1.0のような乖離ではないにしても）。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5299906,
     "sourceId": 8811121,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186083288,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186270439,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
