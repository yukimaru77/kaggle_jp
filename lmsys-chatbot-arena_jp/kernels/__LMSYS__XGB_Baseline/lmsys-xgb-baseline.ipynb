{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46f77047",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおけるXGBoostを使用したベースラインモデルの構築に関連しています。具体的には、ユーザーが好む応答を予測するためのモデルをデータを用いてトレーニングし、その精度を評価することを目的としています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "Notebookでは、異なる大規模言語モデル（LLM）が生成した応答の中から、ユーザーがどの応答を好むかを予測するという課題に対処しています。これは、データセットに含まれる「prompt」、「response_a」、「response_b」に基づいて、どちらの応答が優れているかを識別するためのものであり、最終的にはその選好をモデル学習することに繋がります。\n",
    "\n",
    "### 使用される手法とライブラリ\n",
    "- **ライブラリ**: \n",
    "  - `numpy`, `pandas`: データ操作のため。\n",
    "  - `nltk`: 自然言語処理とテキストトークン化。\n",
    "  - `matplotlib`, `seaborn`: データビジュアライゼーション。\n",
    "  - `xgboost`: 機械学習モデルの構築用。\n",
    "  - `sklearn`: 特徴量の抽出やモデル評価に利用。\n",
    "\n",
    "- **モデルの構築**:\n",
    "  - データを読み込み、前処理を行った後、特徴量エンジニアリングを実施。コサイン類似度やJaccard類似度を計算し、n-gramの重複数などの特徴を生成します。\n",
    "  - `XGBoost`を使用して多クラスの分類モデルを構築し、ストラティファイドKフォールド交差検証を用いてモデルの性能を評価します。\n",
    "  - 評価基準として対数損失（log loss）を使用し、モデルの精度を測定している点が特徴的です。\n",
    "\n",
    "### 結果の提出\n",
    "最終的には、テストデータに基づいた予測結果を生成し、指定されたフォーマットで「submission.csv」として保存します。このプロセスにより、モデルがどの応答を「勝者」として予測するかを示すことができます。\n",
    "\n",
    "このNotebookは、異なる言語モデル間でのユーザーの選好を理解し、予測できるモデルを構築するための体系的なアプローチを採用しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f885cdda",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、jupyter notebookの中で使われている専門用語やコンセプトについて、特に初心者がつまずきやすいと思われる項目の簡単な解説を示します。\n",
    "\n",
    "1. **n-gram**: 連続するn個の単語の組み合わせを指します。例えば、\"I love AI\"という文から生成される1-gram（単語単位）は \"I\", \"love\", \"AI\"、2-gram（隣接する2単語の組み合わせ）は \"I love\", \"love AI\" となります。n-gramはテキスト解析に用いられ、頻出のフレーズを把握するのに役立ちます。\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**: 単語の重要度を計算するための指標です。特定の文書内での単語の出現頻度（TF）と、その単語が他の文書に出現する頻度の逆数（IDF）を掛け合わせることで、頻繁に使われるが特有の単語を重視します。情報検索やテキストマイニングでよく使われます。\n",
    "\n",
    "3. **cosine similarity**: 2つのベクトル間のコサイン角度を基にした類似性を測る指標です。コサインの値が1に近いほど、2つのベクトルは似ていると評価されます。文書間の類似性を測定する際によく使用されます。\n",
    "\n",
    "4. **Jaccard similarity**: 2つの集合の共通部分のサイズを、両者の和集合のサイズで割ったものです。たとえば、文書内の単語の集合を比較する際に利用され、重複する単語の割合を計算します。\n",
    "\n",
    "5. **StratifiedKFold**: StratifiedKFoldは、データをK個の部分に分けるときに、各部分におけるクラスの比率を保つ方法です。これにより、訓練データと検証データの分布が似るため、モデルの評価が安定します。\n",
    "\n",
    "6. **early stopping**: モデルの訓練過程で、特定の評価指標（例: 検証データのロス）が一定の回数改善しなかった場合に、訓練を中止する技術です。オーバーフィッティングを防ぐために役立ちます。\n",
    "\n",
    "7. **XGBoost**: \"Extreme Gradient Boosting\"の略で、決定木をベースにした強力な機械学習アルゴリズムです。加重されたボースト法に基づいており、高速で高精度な予測を実現するため、多くのデータサイエンスコンペティションで使用されます。\n",
    "\n",
    "8. **log loss**: またはクロスエントロピー損失関数、モデルの確率的予測がどれだけ正解から離れているかを評価する指標です。特に二値分類や多クラス分類で使われ、値が小さいほどモデルの予測が良好であることを示します。\n",
    "\n",
    "これらの用語は、特に実務経験がない初心者にとっては少し馴染みが薄いものが多いですが、機械学習や深層学習のプロセスの中で非常に重要な概念です。理解しておくことで、今後の学習や実装がスムーズになるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982c8ecc",
   "metadata": {},
   "source": [
    "# LMSYS | XGB ベースライン\n",
    "\n",
    "# 1. ライブラリ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-06T22:53:47.250349Z",
     "iopub.status.busy": "2024-07-06T22:53:47.249376Z",
     "iopub.status.idle": "2024-07-06T22:53:47.259892Z",
     "shell.execute_reply": "2024-07-06T22:53:47.258305Z",
     "shell.execute_reply.started": "2024-07-06T22:53:47.250304Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f8c7ad",
   "metadata": {},
   "source": [
    "# 2. 設定\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:53:47.266784Z",
     "iopub.status.busy": "2024-07-06T22:53:47.265928Z",
     "iopub.status.idle": "2024-07-06T22:53:47.281569Z",
     "shell.execute_reply": "2024-07-06T22:53:47.280051Z",
     "shell.execute_reply.started": "2024-07-06T22:53:47.26674Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")  # 訓練データのファイルパス\n",
    "    test_path = os.path.join(root, \"test.csv\")    # テストデータのファイルパス\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")  # サンプル提出ファイルのパス\n",
    "    seed = 42  # 乱数シード\n",
    "    n_splits = 10  # クロスバリデーションの分割数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d43931",
   "metadata": {},
   "source": [
    "# 3. データの読み込み\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:53:47.294082Z",
     "iopub.status.busy": "2024-07-06T22:53:47.293054Z",
     "iopub.status.idle": "2024-07-06T22:53:49.593844Z",
     "shell.execute_reply": "2024-07-06T22:53:49.592669Z",
     "shell.execute_reply.started": "2024-07-06T22:53:47.294044Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.train_path)  # 訓練データを読み込む\n",
    "test = pd.read_csv(config.test_path)    # テストデータを読み込む\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込む\n",
    "\n",
    "if test.shape[0] < 10:  # テストデータの行数が10未満なら\n",
    "    train = train.iloc[:10000]  # 訓練データの最初の10000行を使用する\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文字列を分割し、前後のダブルクォートを削除\n",
    "    return  ' '.join(sentences)  # スペースで結合して返す\n",
    "\n",
    "# \"prompt\", \"response_a\", \"response_b\"のデータを加工\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")  # 訓練データの形状を表示\n",
    "print(f\"test shape: {test.shape}\")    # テストデータの形状を表示\n",
    "print(\"-\" * 90)  # 区切り線を表示\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")  # 訓練データの欠損値の合計を表示\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")    # テストデータの欠損値の合計を表示\n",
    "print(\"-\" * 90)\n",
    "\n",
    "train.head()  # 訓練データの先頭5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb02024f",
   "metadata": {},
   "source": [
    "# 4. 特徴量エンジニアリング\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:53:49.597014Z",
     "iopub.status.busy": "2024-07-06T22:53:49.596633Z",
     "iopub.status.idle": "2024-07-06T22:53:49.62899Z",
     "shell.execute_reply": "2024-07-06T22:53:49.627576Z",
     "shell.execute_reply.started": "2024-07-06T22:53:49.596974Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # TF-IDFベクトライザーを定義 (1-3のn-gram)\n",
    "            vectorizer.fit([text1, text2])  # テキストを基にベクトライザーをフィッティング\n",
    "            output = vectorizer.transform([text1, text2]).toarray()  # テキストをベクトル化\n",
    "            cos_sim = cosine_similarity(output)  # コサイン類似度を計算\n",
    "            return cos_sim[0][1]  # テキスト1とテキスト2のコサイン類似度を返す\n",
    "        except:\n",
    "            return np.nan  # エラーが発生した場合はNaNを返す\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # テキスト1を単語の集合に変換\n",
    "        set2 = set(text2.split())  # テキスト2を単語の集合に変換\n",
    "        intersection = set1.intersection(set2)  # 共通する単語を取得\n",
    "        union = set1.union(set2)  # 単語の和集合を取得\n",
    "        return len(intersection) / len(union)  # Jaccard類似度を計算して返す\n",
    "    \n",
    "    def count_new_lines(self, text: str) -> int:\n",
    "        return text.count('\\\\n')  # テキスト内の改行の数をカウントして返す \n",
    "\n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # 単一引用符のパターン\n",
    "        double_quote_pattern = r'\"(.*?)\"'   # 二重引用符のパターン\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用符の全てを取得\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用符の全てを取得\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用符の数を計算\n",
    "        return len(single_quotes) + len(double_quotes)  # 合計数を返す\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())  # テキストを小文字にしてトークン化 \n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # トークン化されたテキストを取得\n",
    "        return list(ngrams(tokens, n))  # n-gramを生成して返す\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # テキスト1のn-gramを生成\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # テキスト2のn-gramを生成\n",
    "            counter1 = Counter(ngrams1)  # テキスト1のn-gramのカウントを作成\n",
    "            counter2 = Counter(ngrams2)  # テキスト2のn-gramのカウントを作成\n",
    "            overlap = counter1 & counter2  # 重複するn-gramを取得\n",
    "            overlap_count = sum(overlap.values())  # 重複の合計数を計算\n",
    "            return overlap_count  # 重複数を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        # \"response_a\"と\"response_b\"のn-gram重複数を計算し、新しいカラムに追加\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # \"response_a\"のトークン数をカウント\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # \"response_b\"のトークン数をカウント\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # \"prompt\"のトークン数をカウント\n",
    "        \n",
    "        data[\"respa_new_lines\"] = data[\"response_a\"].apply(lambda x: self.count_new_lines(x))  # \"response_a\"の改行数をカウント\n",
    "        data[\"respb_new_lines\"] = data[\"response_b\"].apply(lambda x: self.count_new_lines(x))  # \"response_b\"の改行数をカウント\n",
    "        data[\"prompt_new_lines\"] = data[\"prompt\"].apply(lambda x: self.count_new_lines(x))  # \"prompt\"の改行数をカウント\n",
    "        \n",
    "        # 各長さの比率や差分を計算し新しいカラムに格納\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # \"response_a\"と\"prompt\"の長さ比\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # \"response_b\"と\"prompt\"の長さ比\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # \"response_a\"と\"response_b\"の長さ比\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # \"response_a\"と\"response_b\"の長さの差\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # \"response_a\"と\"prompt\"の長さの差\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # \"response_b\"と\"prompt\"の長さの差\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_len_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # unigramの比率\n",
    "        data[\"respa_prompt_overlap_bigram_len_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # bigramの比率\n",
    "        data[\"respa_prompt_overlap_trigram_len_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # trigramの比率\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_len_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # unigramの比率\n",
    "        data[\"respb_prompt_overlap_bigram_len_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # bigramの比率\n",
    "        data[\"respb_prompt_overlap_trigram_len_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # trigramの比率\n",
    "        \n",
    "        data[\"overlap_unigram_diff\"] = data[\"respa_prompt_overlap_unigram\"] - data[\"respb_prompt_overlap_unigram\"]  # unigramの差\n",
    "        data[\"overlap_bigram_diff\"] = data[\"respa_prompt_overlap_bigram\"] - data[\"respb_prompt_overlap_bigram\"]  # bigramの差\n",
    "        data[\"overlap_trigram_diff\"] = data[\"respa_prompt_overlap_trigram\"] - data[\"respb_prompt_overlap_trigram\"]  # trigramの差\n",
    "        \n",
    "        data[\"overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"respa_prompt_overlap_unigram\"]  # unigramの比率\n",
    "        data[\"overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"respa_prompt_overlap_bigram\"]  # bigramの比率\n",
    "        data[\"overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"respa_prompt_overlap_trigram\"]  # trigramの比率\n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # \"response_a\"の引用符の数\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # \"response_b\"の引用符の数\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # \"prompt\"の引用符の数\n",
    "        \n",
    "        # コサイン類似度とJaccard類似度を計算\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"jaccard_sim_diff\"] = data[\"respa_prompt_jaccard_sim\"] - data[\"respb_prompt_jaccard_sim\"]  # Jaccard類似度の差\n",
    "        data[\"jaccard_sim_ratio\"] = data[\"respb_prompt_jaccard_sim\"] / data[\"respa_prompt_jaccard_sim\"]  # Jaccard類似度の比率\n",
    "        \n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T22:53:49.631559Z",
     "iopub.status.busy": "2024-07-06T22:53:49.631147Z",
     "iopub.status.idle": "2024-07-06T23:04:32.599122Z",
     "shell.execute_reply": "2024-07-06T23:04:32.597677Z",
     "shell.execute_reply.started": "2024-07-06T22:53:49.631529Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "preprocessor = Preprocessor()  # Preprocessorのインスタンスを作成\n",
    "train = preprocessor.run(train)  # 訓練データに前処理を適用\n",
    "test = preprocessor.run(test)  # テストデータに前処理を適用\n",
    "train.head()  # 訓練データの先頭5行を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T23:08:34.442062Z",
     "iopub.status.busy": "2024-07-06T23:08:34.441571Z",
     "iopub.status.idle": "2024-07-06T23:08:34.488055Z",
     "shell.execute_reply": "2024-07-06T23:08:34.486837Z",
     "shell.execute_reply.started": "2024-07-06T23:08:34.44202Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # 削除するカラムのリスト\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ターゲットカラムのリスト\n",
    "target = \"target\"  # ターゲット名\n",
    "\n",
    "train[target] = np.nan  # ターゲットカラムを初期化\n",
    "for idx, t in enumerate(target_cols):  # ターゲットカラムを走査\n",
    "    train.loc[train[t] == 1, target] = idx  # 勝者モデルをターゲットカラムに設定\n",
    "train[target] = train[target].astype(\"int32\")  # ターゲットカラムのデータ型をint32に変換\n",
    "    \n",
    "train.head()  # 訓練データの先頭5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444e1ecc",
   "metadata": {},
   "source": [
    "# 5. モデリング\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T23:08:34.490712Z",
     "iopub.status.busy": "2024-07-06T23:08:34.490159Z",
     "iopub.status.idle": "2024-07-06T23:08:38.142354Z",
     "shell.execute_reply": "2024-07-06T23:08:38.140954Z",
     "shell.execute_reply.started": "2024-07-06T23:08:34.490671Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=target_cols + drop_cols + [target] + [\"model_a\", \"model_b\"], axis=1)  # 特徴量マトリックスを定義\n",
    "y = train[target]  # ターゲットを定義\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # テストデータから不要なカラムを削除\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)  # 無限大をNaNに置き換え\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)  # 無限大をNaNに置き換え"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T23:08:38.143948Z",
     "iopub.status.busy": "2024-07-06T23:08:38.143572Z"
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # ストラティファイドKフォールド交差検証の設定\n",
    "test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テストデータの予測結果を格納する配列\n",
    "cv_scores = list()  # クロスバリデーションのスコアを格納するリスト\n",
    "\n",
    "features = X.columns.tolist()  # 特徴量のリストを作成\n",
    "feat_imp_df = pd.DataFrame({\"feature\": features})  # 特徴量の重要度を格納するデータフレームを作成\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):  # 各Foldについて\n",
    "    print(f\"| Fold {idx+1} |\".center(90, \"=\"))  # 現在のFold番号を表示\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # 訓練データを分割\n",
    "    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # 検証データを分割\n",
    "\n",
    "    print(f'train: {X_train.shape}')  # 訓練データの形状を表示\n",
    "    print(f'val: {X_val.shape}')  # 検証データの形状を表示\n",
    "    \n",
    "    model = xgb.XGBClassifier(  # XGBoostの分類器モデルを定義\n",
    "        objective='multi:softprob',  # 多クラス分類のための設定\n",
    "        num_class=3,  # クラスの数\n",
    "        eval_metric='mlogloss',  # 評価指標に対数損失を使用\n",
    "        subsample=0.8,  # サンプリング比率\n",
    "        n_estimators=650,  # 弱学習器の数\n",
    "        learning_rate=0.045,  # 学習率\n",
    "        max_depth=5,  # 木の深さ\n",
    "        random_state=config.seed  # 乱数シード\n",
    "    )\n",
    "    \n",
    "    model.fit(  # モデルを訓練\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],  # 訓練と検証データセット\n",
    "        early_stopping_rounds=75,  # 早期停止の設定\n",
    "        verbose=75  # 訓練過程を75ステップごとに表示\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict_proba(X_val)  # 検証データに対する予測確率を計算\n",
    "    val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # 検証データの対数損失を計算\n",
    "    print(f\"val log loss: {val_log_loss:.5f}\")  # 検証データの対数損失を表示\n",
    "    cv_scores.append(val_log_loss)  # クロスバリデーションのスコアを追加\n",
    "    \n",
    "    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # テストデータの予測を加算（平均化）\n",
    "\n",
    "    feat_imp_df = feat_imp_df.merge(  # 特徴量の重要度を追加\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": features,\n",
    "                f\"fold_{idx+1}_feat_imp\": model.feature_importances_,  # 各Foldの特徴量重要度を取得\n",
    "            }\n",
    "        ),\n",
    "        on=[\"feature\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "print(\"=\"*90)  # 区切り線を表示\n",
    "print(f\"CV: {np.mean(cv_scores):.5f}\")  # クロスバリデーションスコアの平均を表示\n",
    "\n",
    "feat_imp_df[\"avg_importance\"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴の平均重要度を計算\n",
    "plt.figure(figsize=(12, 10))  # グラフのサイズを設定\n",
    "sns.barplot(\n",
    "    data=feat_imp_df.sort_values(by=\"avg_importance\", ascending=False).iloc[:50],  # 平均重要度でソートし上位50件を選択\n",
    "    x=\"avg_importance\",\n",
    "    y=\"feature\",\n",
    "    color=\"royalblue\",\n",
    "    width=0.75,\n",
    ")\n",
    "plt.title(\"全Foldの特徴量の平均重要度\", size=12)  # グラフのタイトル\n",
    "plt.show()  # グラフを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73592411",
   "metadata": {},
   "source": [
    "# 6. 提出の保存\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):  # 各ターゲットカラムについて\n",
    "    sample_submission[t] = test_preds[:, idx]  # テストデータの予測結果をサンプル提出データに格納\n",
    "sample_submission.head()  # サンプル提出データの先頭5行を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)  # 提出ファイルをCSV形式で保存（インデックスなし）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb10ae8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント\n",
    "\n",
    "> ## Ilya Turaev\n",
    ">\n",
    "> こんにちは [@sercanyesiloz](https://www.kaggle.com/sercanyesiloz) ! 一つのXGBで素晴らしい結果です。コサイン類似度の関数がうまく動作していないようです。\n",
    ">\n",
    "> ```\n",
    "> vectors = vectorizer.toarray()\n",
    "> ```\n",
    ">\n",
    "> これを修正すれば、スコアが改善すると思いますか？それとも、他の特徴量だけで十分ですか？\n",
    ">\n",
    "> > ## Sercan Yeşilöz トピック作成者\n",
    "> > \n",
    "> > こんにちは！そのコードでは、vectorizerは実際には訓練セットとテストセットでフィットした出力です。間違いはないと思います。\n",
    "> >\n",
    "> > > ## Ilya Turaev\n",
    "> > > \n",
    "> > > 間違いなくそうすべきです、もしあなたがそれを次のように上書きする場合：\n",
    "> > > \n",
    "> > > ```\n",
    "> > > vectorizer = vectorizer.fit_transform([text1, text2])\n",
    "> > > ```\n",
    "> > > \n",
    "> > > そうでなければ、vectorizerはまだTfIdfオブジェクトであり、toarray()メソッドを適用しています。私は間違っていますか？\n",
    "> > > \n",
    "> > > \n",
    "> > > ## Sercan Yeşilöz トピック作成者\n",
    "> > > \n",
    "> > > 私は、その行の後でvectorizerがtfidfオブジェクトになるとは思いません、なぜならfit_transform関数を呼び出すと、変換された値が返されるからです。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
