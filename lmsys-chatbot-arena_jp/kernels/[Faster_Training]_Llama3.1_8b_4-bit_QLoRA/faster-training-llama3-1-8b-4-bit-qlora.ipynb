{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aaea4d85",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションに関連する問題に取り組んでいます。具体的には、ユーザーが選択する応答の優劣を予測するための機械学習モデルを構築することを目指しています。このタスクは、異なるチャットボットからの応答のどちらが好まれるかを学習するものであり、ユーザーの好みを精度良く予測することが求められています。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **ライブラリ**:\n",
    "   - `transformers`: 事前学習済みのLLM（Llamaモデル）の読み込みと使用に利用。\n",
    "   - `peft`: LoRA（Low-Rank Adaptation）技術を用いてモデルを効率的にトレーニングするためのライブラリ。\n",
    "   - `datasets`: データセットの作成と扱いに使用。\n",
    "\n",
    "2. **手法**:\n",
    "   - **LoRA**: トレーニング時にメモリと計算資源を削減するための手法であり、モデルの特定の層にアダプタを追加してパラメータを微調整。\n",
    "   - **SFTTrainer**: シーケンス分類タスクに特化したトレーナーを使用して、モデルへ入力データを供給し、訓練を行う。\n",
    "   - **カスタムトークナイザー**: プロンプトと応答を適切な形式に整形し、モデルに供給する際にトークン化を行うクラスを実装。\n",
    "\n",
    "### ワークフローの概要\n",
    "1. 依存ライブラリのインストール。\n",
    "2. モデルとデータの設定を行い、データの前処理（欠損値の削除、重複応答の削除等）を実施。\n",
    "3. データセットからトレーニングと評価用のインデックスを生成し、Foldクロスバリデーションの準備。\n",
    "4. LoRAの設定を適用し、モデルを準備してトレーニングを開始。\n",
    "5. トレーニング後、各Foldに対して検証を行い、評価結果を出力。\n",
    "\n",
    "このNotebookは、特に事前学習済みモデルの効率的な微調整を通じて、競合する応答の選好を予測するための強力なフレームワークを提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c71ea1",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提示されたJupyter Notebookの内容に基づいて、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説です。初心者向けの一般的な説明は省略し、特にノートブック内で使用されている特有の用語や概念に焦点を当てています。\n",
    "\n",
    "1. **bitsandbytes**: メモリの使用量を最適化するためのライブラリで、特に大規模なモデルを小さいメモリFootprintで運用できるように設計されています。通常、ハードウェアリソースの制約で扱えないサイズのモデルを扱うために使用されます。\n",
    "\n",
    "2. **PEFT (Parameter-Efficient Fine-Tuning)**: 機械学習モデルのファインチューニングの一手法で、大規模なモデルの特定のパラメータのみを調整することで、学習リソースを効率的に使用することを目的としています。これにより、大規模モデルを小さいデータセットで迅速に適応させることが可能になります。\n",
    "\n",
    "3. **Lora (Low-Rank Adaptation)**: PEFTの一形式で、モデルの重みを低ランクの行列で近似することにより、必要なパラメータの数を大幅に減らすアプローチです。これにより、モデルのトレーニングがより効率的に行えます。\n",
    "\n",
    "4. **gradient accumulation**: 複数のバッチにわたって勾配を蓄積してからモデルを更新する手法です。これにより、大きなバッチサイズを個別のGPUメモリではなく、より少ないメモリ使用量で模倣することができます。\n",
    "\n",
    "5. **torch_dtype**: PyTorchにおいて、テンソルのデータ型を指定するための引数です。`torch.float16`を指定することで、データの精度を落としながら計算速度を速くし、メモリ使用量を削減することができます。\n",
    "\n",
    "6. **SFT (Supervised Fine-Tuning)**: 教師あり学習によるファインチューニングの方法で、訓練データを使用してモデルの性能を向上させることを目的としています。このプロセスは、特定のタスクに対してモデルのパフォーマンスを最適化するのに役立ちます。\n",
    "\n",
    "7. **EvalPrediction**: 評価の際に使用されるデータ構造で、モデルの予測結果と対応するラベルを格納します。このオブジェクトは、評価メトリクスの計算やモデルの性能を測定するのに使用されます。\n",
    "\n",
    "8. **Kビットトレーニング**: モデルの重みを低ビット精度（例: 4ビットなど）で保存・計算することにより、大規模なパラメータを持つモデルをメモリに収容できるようにするトレーニング手法です。メモリ帯域幅やストレージの節約に貢献します。\n",
    "\n",
    "9. **Llama**: 特定の大規模言語モデルやアーキテクチャを指すもので、この文脈では特に「LlamaForSequenceClassification」や「LlamaTokenizerFast」として現れることが多いです。このモデルは、シーケンス分類タスクに特化しています。\n",
    "\n",
    "10. **DataCollatorWithPadding**: バッチ処理の際にデータにパディングを追加し、各入力データが同じサイズになるように整形するためのオブジェクトです。シーケンスデータを扱う際に重要です。\n",
    "\n",
    "これらの用語や概念は、実務経験の不足から初心者にとっては難解に感じられることがあるため、しっかり理解しておくことが重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:51:04.175257Z",
     "iopub.status.busy": "2024-07-29T13:51:04.174328Z",
     "iopub.status.idle": "2024-07-29T13:51:54.577962Z",
     "shell.execute_reply": "2024-07-29T13:51:54.57686Z",
     "shell.execute_reply.started": "2024-07-29T13:51:04.175208Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llama3-1-dependencies/dependencies/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llama3-1-dependencies/dependencies/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:51:54.580225Z",
     "iopub.status.busy": "2024-07-29T13:51:54.579915Z",
     "iopub.status.idle": "2024-07-29T13:52:07.53373Z",
     "shell.execute_reply": "2024-07-29T13:52:07.532613Z",
     "shell.execute_reply.started": "2024-07-29T13:51:54.580196Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:07.535399Z",
     "iopub.status.busy": "2024-07-29T13:52:07.535087Z",
     "iopub.status.idle": "2024-07-29T13:52:07.924142Z",
     "shell.execute_reply": "2024-07-29T13:52:07.923271Z",
     "shell.execute_reply.started": "2024-07-29T13:52:07.53537Z"
    }
   },
   "outputs": [],
   "source": [
    "# このPython 3環境には多くの役立つ分析ライブラリがインストールされています。\n",
    "# この環境はkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python\n",
    "# たとえば、以下は読み込むのに役立つパッケージのいくつかです。\n",
    "\n",
    "import numpy as np # 線形 algebra（線形代数）\n",
    "import pandas as pd # データ処理、CSVファイルの入出力（例: pd.read_csv）\n",
    "\n",
    "# 入力データファイルは読み取り専用の\"../input/\"ディレクトリにあります。\n",
    "# 例えば、これを実行すると（実行ボタンをクリックするかShift+Enterを押すことで）入力ディレクトリ内のすべてのファイルがリスト表示されます。\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# 現在のディレクトリ（/kaggle/working/）には最大20GBまで書き込むことができ、\n",
    "# これは「すべて保存して実行」ボタンを使用してバージョンを作成すると出力として保持されます。\n",
    "# また、一時ファイルは/kaggle/temp/に書き込むこともできますが、現在のセッションの外部には保存されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:07.927368Z",
     "iopub.status.busy": "2024-07-29T13:52:07.926794Z",
     "iopub.status.idle": "2024-07-29T13:52:14.42691Z",
     "shell.execute_reply": "2024-07-29T13:52:14.425946Z",
     "shell.execute_reply.started": "2024-07-29T13:52:07.927334Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    DataCollatorWithPadding,\n",
    "    LlamaForSequenceClassification,\n",
    "    LlamaTokenizerFast,\n",
    "    PreTrainedTokenizerBase,\n",
    "    EvalPrediction,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel\n",
    "from sklearn.metrics import log_loss, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.429323Z",
     "iopub.status.busy": "2024-07-29T13:52:14.428309Z",
     "iopub.status.idle": "2024-07-29T13:52:14.438543Z",
     "shell.execute_reply": "2024-07-29T13:52:14.437188Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.429286Z"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    output_dir: str = \"output\"  # 出力ディレクトリ\n",
    "    checkpoint: str = \"/kaggle/input/unsloth-meta-llama-3.1-8b-bnb-4bit/transformers/default/1/Meta-Llama-3.1-8B-bnb-4bit\"  # チェックポイントのパス\n",
    "    max_length: int = 2048  # 最大入力長\n",
    "    n_splits: int = 5  # データを分割する数\n",
    "    fold_idx: int = 0  # フォールドのインデックス\n",
    "    optim_type: str = \"adamw_8bit\"  # 最適化アルゴリズムの種類\n",
    "    per_device_train_batch_size: int = 4  # デバイスごとのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps: int = 4  # 勾配蓄積ステップ\n",
    "    per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ\n",
    "    n_epochs: int = 1  # エポック数\n",
    "    freeze_layers: int = 16  # 合計32層のうち、最初の16層はアダプタを追加しない\n",
    "    lr: float = 2e-4  # 学習率\n",
    "    warmup_steps: int = 20  # ウォームアップステップ数\n",
    "    lora_r: int = 4  # LoRAの次元数\n",
    "    lora_alpha: float = lora_r * 2  # LoRAのスケーリング係数\n",
    "    lora_dropout: float = 0.05  # LoRAのドロップアウト率\n",
    "    lora_bias: str = \"none\"  # LoRAのバイアスの種類\n",
    "\n",
    "config = Config()  # 設定クラスのインスタンスを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.439956Z",
     "iopub.status.busy": "2024-07-29T13:52:14.439666Z",
     "iopub.status.idle": "2024-07-29T13:52:14.454669Z",
     "shell.execute_reply": "2024-07-29T13:52:14.453826Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.439931Z"
    }
   },
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=config.lora_r,  # LoRAの次元数\n",
    "    lora_alpha=config.lora_alpha,  # LoRAのスケーリング係数\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # 変更対象のモジュール\n",
    "    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],  # 変換対象層\n",
    "    lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率\n",
    "    bias=config.lora_bias,  # LoRAのバイアスの種類\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類（シーケンス分類）\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.456298Z",
     "iopub.status.busy": "2024-07-29T13:52:14.45587Z",
     "iopub.status.idle": "2024-07-29T13:52:14.947589Z",
     "shell.execute_reply": "2024-07-29T13:52:14.946815Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.456266Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizerFast.from_pretrained(config.checkpoint)  # トークナイザーの初期化\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id  # パディングトークンIDをEOSトークンIDに設定\n",
    "tokenizer.pad_token = tokenizer.eos_token  # パディングトークンをEOSトークンに設定\n",
    "tokenizer.add_eos_token = True  # テキストの最後に<EOS>を追加\n",
    "tokenizer.padding_side = \"right\"  # パディングの追加位置を右側に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:52:14.948901Z",
     "iopub.status.busy": "2024-07-29T13:52:14.948607Z",
     "iopub.status.idle": "2024-07-29T13:53:01.613151Z",
     "shell.execute_reply": "2024-07-29T13:53:01.612176Z",
     "shell.execute_reply.started": "2024-07-29T13:52:14.948865Z"
    }
   },
   "outputs": [],
   "source": [
    "model = LlamaForSequenceClassification.from_pretrained(\n",
    "    config.checkpoint,\n",
    "    num_labels=3,  # 分類するラベルの数\n",
    "    torch_dtype=torch.float16,  # 使用するデータ型\n",
    "    device_map=\"auto\"  # デバイスマッピングを自動決定\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.614976Z",
     "iopub.status.busy": "2024-07-29T13:53:01.614625Z",
     "iopub.status.idle": "2024-07-29T13:53:01.741845Z",
     "shell.execute_reply": "2024-07-29T13:53:01.740868Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.614948Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.use_cache = False  # キャッシュの使用を無効にする\n",
    "model = prepare_model_for_kbit_training(model)  # Kビットトレーニングのためにモデルを準備\n",
    "model = get_peft_model(model, lora_config)  # PEFTモデルを取得\n",
    "model  # モデルの状態を確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.746928Z",
     "iopub.status.busy": "2024-07-29T13:53:01.746237Z",
     "iopub.status.idle": "2024-07-29T13:53:01.755231Z",
     "shell.execute_reply": "2024-07-29T13:53:01.754267Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.746869Z"
    }
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()  # トレーニング可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.757223Z",
     "iopub.status.busy": "2024-07-29T13:53:01.75664Z",
     "iopub.status.idle": "2024-07-29T13:53:01.766005Z",
     "shell.execute_reply": "2024-07-29T13:53:01.764991Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.757197Z"
    }
   },
   "outputs": [],
   "source": [
    "model.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "model.config.use_cache = False  # キャッシュの使用を無効にする\n",
    "model.config.pretraining_tp = 1  # プリトレーニングのTPを1に設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:01.768169Z",
     "iopub.status.busy": "2024-07-29T13:53:01.767361Z",
     "iopub.status.idle": "2024-07-29T13:53:04.250674Z",
     "shell.execute_reply": "2024-07-29T13:53:04.249787Z",
     "shell.execute_reply.started": "2024-07-29T13:53:01.768136Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # CSVファイルを読み込む\n",
    "df = df.dropna()  # 欠損値を削除\n",
    "df = df.drop_duplicates(subset=['response_a', 'response_b'], keep=False)  # 重複した応答を削除\n",
    "df[\"len\"] = df[\"prompt\"].apply(len) + df[\"response_a\"].apply(len) + df[\"response_b\"].apply(len)  # 総長を計算\n",
    "df = df.sort_values(by=['len'])  # 長さでソート\n",
    "df  # データフレームを表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:04.252112Z",
     "iopub.status.busy": "2024-07-29T13:53:04.251818Z",
     "iopub.status.idle": "2024-07-29T13:53:05.960179Z",
     "shell.execute_reply": "2024-07-29T13:53:05.959221Z",
     "shell.execute_reply.started": "2024-07-29T13:53:04.252087Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = Dataset.from_pandas(df)  # pandas DataFrameからデータセットを作成\n",
    "ds = ds.select(torch.arange(1000)) # デモ目的のために1000行を選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:05.961664Z",
     "iopub.status.busy": "2024-07-29T13:53:05.961368Z",
     "iopub.status.idle": "2024-07-29T13:53:05.970848Z",
     "shell.execute_reply": "2024-07-29T13:53:05.969929Z",
     "shell.execute_reply.started": "2024-07-29T13:53:05.961637Z"
    }
   },
   "outputs": [],
   "source": [
    "class CustomTokenizer:\n",
    "    def __init__(\n",
    "        self, \n",
    "        tokenizer: PreTrainedTokenizerBase,  # トークナイザー\n",
    "        max_length: int  # 最大長\n",
    "    ) -> None:\n",
    "        self.tokenizer = tokenizer  # トークナイザーを初期化\n",
    "        self.max_length = max_length  # 最大長を設定\n",
    "        \n",
    "    def __call__(self, batch: dict) -> dict:  # バッチを處理するメソッド\n",
    "        prompt = [\"Which is the better response for the prompt? response_a or response_b or tie? \\n'n give score for each lable \\n\\n <prompt>: \" + self.process_text(t) for t in batch[\"prompt\"]]  # プロンプトを構築\n",
    "        response_a = [\"\\n\\n<response_a>: \" + self.process_text(t) for t in batch[\"response_a\"]]  # 応答Aを処理\n",
    "        response_b = [\"\\n\\n<response_b>: \" + self.process_text(t) for t in batch[\"response_b\"]]  # 応答Bを処理\n",
    "        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # テキストを結合\n",
    "        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークン化\n",
    "        labels=[]  # ラベルのリストを初期化\n",
    "        for a_win, b_win in zip(batch[\"winner_model_a\"], batch[\"winner_model_b\"]):  # 各応答の勝者を評価\n",
    "            if a_win:\n",
    "                label = 0  # 応答Aが勝者\n",
    "            elif b_win:\n",
    "                label = 1  # 応答Bが勝者\n",
    "            else:\n",
    "                label = 2  # 引き分け\n",
    "            labels.append(label)  # ラベルを追加\n",
    "        return {**tokenized, \"labels\": labels}  # トークン化結果とラベルを返す\n",
    "        \n",
    "    @staticmethod\n",
    "    def process_text(text: str) -> str:  # テキストを処理する静的メソッド\n",
    "        return \" \".join(eval(text, {\"null\": \"\"}))  # テキストを評価して文字列を結合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:05.972086Z",
     "iopub.status.busy": "2024-07-29T13:53:05.971815Z",
     "iopub.status.idle": "2024-07-29T13:53:06.33407Z",
     "shell.execute_reply": "2024-07-29T13:53:06.333205Z",
     "shell.execute_reply.started": "2024-07-29T13:53:05.972064Z"
    }
   },
   "outputs": [],
   "source": [
    "encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンスを生成\n",
    "ds = ds.map(encode, batched=True)  # データセットにトークナイザーを適用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.335337Z",
     "iopub.status.busy": "2024-07-29T13:53:06.335075Z",
     "iopub.status.idle": "2024-07-29T13:53:06.34184Z",
     "shell.execute_reply": "2024-07-29T13:53:06.340816Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.335315Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds: EvalPrediction) -> dict:  # 評価予測に基づいてメトリクスを計算する関数\n",
    "    preds = eval_preds.predictions  # 予測結果を取得\n",
    "    labels = eval_preds.label_ids  # ラベルを取得\n",
    "    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 確率を計算\n",
    "    # 予測結果やラベルにNaNが含まれていないか確認\n",
    "    if np.isnan(probs).any() or np.isnan(labels).any():\n",
    "        raise ValueError(\"NaN values found in predictions or labels\")  # NaNが見つかった場合はエラーをスロー\n",
    "\n",
    "    loss = log_loss(y_true=labels, y_pred=probs)  # ログ損失を計算\n",
    "    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 正確度を計算\n",
    "    return {\"acc\": acc, \"log_loss\": loss}  # メトリクスを返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.343396Z",
     "iopub.status.busy": "2024-07-29T13:53:06.343073Z",
     "iopub.status.idle": "2024-07-29T13:53:06.356154Z",
     "shell.execute_reply": "2024-07-29T13:53:06.355302Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.34336Z"
    }
   },
   "outputs": [],
   "source": [
    "folds = [\n",
    "        (\n",
    "            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニング用インデックス\n",
    "            [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 評価用インデックス\n",
    "        ) \n",
    "        for fold_idx in range(config.n_splits)  # フォールドインデックスに基づいて分割\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.357518Z",
     "iopub.status.busy": "2024-07-29T13:53:06.357275Z",
     "iopub.status.idle": "2024-07-29T13:53:06.646525Z",
     "shell.execute_reply": "2024-07-29T13:53:06.64568Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.357497Z"
    }
   },
   "outputs": [],
   "source": [
    "from trl import SFTTrainer, SFTConfig  # SFTHandlerと設定をインポートする\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"output\",  # 出力ディレクトリ\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書き\n",
    "    report_to=\"none\",  # レポートを行わない\n",
    "    num_train_epochs=config.n_epochs,  # 学習エポック数\n",
    "    per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "    gradient_accumulation_steps=config.gradient_accumulation_steps,  # 勾配蓄積のステップ数\n",
    "    per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ\n",
    "    logging_steps=1000,  # ロギングのステップ数\n",
    "    save_strategy=\"epoch\",  # エポックごとにモデルを保存\n",
    "    save_steps=100,  # 100ステップごとに保存\n",
    "    optim=config.optim_type,  # 最適化アルゴリズムの種類\n",
    "    fp16=True,  # 半精度浮動小数点を使用する\n",
    "    learning_rate=config.lr,  # 学習率\n",
    "    warmup_steps=config.warmup_steps,  # ウォームアップステップ数\n",
    "    packing=True,  # Packingを有効にする\n",
    "    dataset_text_field=\"text\",  # データセットのテキストフィールド名\n",
    "    max_seq_length=config.max_length,  # 最大シーケンス長\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.647907Z",
     "iopub.status.busy": "2024-07-29T13:53:06.647613Z",
     "iopub.status.idle": "2024-07-29T13:53:06.984498Z",
     "shell.execute_reply": "2024-07-29T13:53:06.983565Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.64787Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "        model,\n",
    "        train_dataset=ds,  # トレーニングデータセット\n",
    "        args=sft_config  # トレーニング設定\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-29T13:53:06.986055Z",
     "iopub.status.busy": "2024-07-29T13:53:06.985713Z",
     "iopub.status.idle": "2024-07-29T14:33:18.62325Z",
     "shell.execute_reply": "2024-07-29T14:33:18.622123Z",
     "shell.execute_reply.started": "2024-07-29T13:53:06.986026Z"
    }
   },
   "outputs": [],
   "source": [
    "for fold_idx in range(config.n_splits):  # 各フォールドに対して\n",
    "    train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを取得\n",
    "\n",
    "    train_data = ds.select(train_idx).sort(\"len\")  # トレーニングデータを選択してソート\n",
    "    val_data = ds.select(eval_idx).sort(\"len\")  # 評価データを選択してソート\n",
    "    \n",
    "    # 同じ長さの範囲でトレーニングデータをバッチに分割\n",
    "    batch_size = 200  # バッチサイズ\n",
    "    num_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)  # バッチ数を計算\n",
    "    \n",
    "    for batch_idx in range(num_batches):  # 各バッチに対して\n",
    "        start_idx = batch_idx * batch_size  # 開始インデックス\n",
    "        end_idx = min(start_idx + batch_size, len(train_data))  # 終了インデックス\n",
    "        ds_temp = train_data.select(range(start_idx, end_idx))  # 一時的なデータセットを作成\n",
    "        \n",
    "        trainer.train_dataset = ds_temp  # トレーナーに一時データセットを設定\n",
    "        \n",
    "        print(f\"Training batch {batch_idx + 1}/{num_batches} on fold {fold_idx + 1}/{config.n_splits}...\")  # トレーニング進行状況を表示\n",
    "        \n",
    "        trainer.train()  # モデルをトレーニング\n",
    "        \n",
    "        trainer.save_model(f\"model_fold_{fold_idx}_batch{batch_idx}\")  # モデルを保存\n",
    "\n",
    "    \n",
    "    # すべてのバッチでのトレーニング後に検証\n",
    "    trainer.eval_dataset = val_data  # 評価データセットを設定\n",
    "    \n",
    "    print(f\"Validating on fold {fold_idx + 1}/{config.n_splits}...\")  # 検証進行状況を表示\n",
    "    eval_results = trainer.evaluate()  # モデルを評価します\n",
    "\n",
    "    # メトリクスが必要な場合に保存\n",
    "    print(f\"Evaluation results for fold {fold_idx + 1}: {eval_results}\")  # 評価結果を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8bb5a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## PaulRRR\n",
    "> \n",
    "> こんにちは、推論コードはありますか？\n",
    "> \n",
    "> \n",
    "> \n",
    "\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5457551,
     "sourceId": 9051512,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 95591,
     "modelInstanceId": 70540,
     "sourceId": 83988,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
