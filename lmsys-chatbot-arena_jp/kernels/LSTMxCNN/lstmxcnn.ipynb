{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a73f8d1e",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、LMSYS - Chatbot Arenaのコンペティションにおける「人間による好み予測」のための機械学習モデルを構築しています。具体的には、ユーザーが好むチャットボットの応答を予測することを目的としています。ノートブックはPythonを使用し、さまざまなデータ処理や深層学習技術を活用しています。\n",
    "\n",
    "### 主な内容と手法\n",
    "\n",
    "1. **ライブラリのインポート**:\n",
    "   - データ処理には`pandas`と`numpy`を使用。\n",
    "   - トークナイゼーションやモデルには、`transformers`ライブラリの各種トークナイザーとモデル（特にDeBERTa V2）を利用。\n",
    "   - モデルの構築には`TensorFlow`とそのKeras APIを使用。\n",
    "\n",
    "2. **データの読み込み**:\n",
    "   - トレーニングデータ（`train.csv`）とテストデータ（`test.csv`）を読み込み、それぞれのデータフレームを作成。\n",
    "\n",
    "3. **データの前処理**:\n",
    "   - テキストデータの整形や結合を行う`combine_text`関数を定義し、プロンプトおよびレスポンスを処理。\n",
    "   - ラベルを生成するための`create_label`関数も定義し、互換性のある形式でラベルを作成。\n",
    "\n",
    "4. **トークナイゼーション**:\n",
    "   - `DebertaV2Tokenizer`を用いて、プロンプトとレスポンスに対するトークンを生成し、必要なトークン型、注意マスクを作成。\n",
    "\n",
    "5. **モデルの構築**:\n",
    "   - CNN、LSTM、またはそのハイブリッドモデルとしてCNN-LSTMモデルを構築。モデルは、埋め込み層、畳み込み層、LSTM層を含み、最終的な出力層は3クラスのラベルに対応。\n",
    "\n",
    "6. **モデルのトレーニング**:\n",
    "   - モデルを訓練（フィッティング）するための設定を行い、早期終了の仕組みを導入。トレーニングは、トレーニングデータからバリデーションを使用して行われます。\n",
    "\n",
    "7. **テストと予測**:\n",
    "   - テストデータに対して同様の前処理を行い、モデルを用いて予測を実施。\n",
    "   - 予測結果はデータフレームに変換され、最終的に`submission.csv`として保存されます。\n",
    "\n",
    "全体を通じて、このノートブックは特に大規模言語モデルを活用したコンペティションに適したデータ処理とモデル設計に焦点を当てています。モデルの選択やトレーニング方法は、ユーザーの応答の好みを正確に予測するために設計されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1599e90",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookに登場する専門用語について、初心者がつまずきやすいマイナーなものや実務経験がないと馴染みが薄い言葉の簡単な解説を示します。\n",
    "\n",
    "1. **前処理 (Preprocessing)**: データ分析や機械学習モデルのトレーニングの前に、データを整理・変換するステップのこと。ノイズを除去したり、形式を統一したりすることで、モデルの性能を向上させる。\n",
    "\n",
    "2. **トークナイザー (Tokenizer)**: テキストデータを処理するためのツールで、文や単語をトークンに分割する役割を持つ。トークン化は、自然言語処理において非常に重要で、モデルがテキストを理解できる形に変える。\n",
    "\n",
    "3. **入力ID (Input IDs)**: トークナイザーによって変換されたテキストのトークンが数値化されたもので、モデルに与える際の入力データ。各トークンは語彙のインデックスによって表現される。\n",
    "\n",
    "4. **アテンションマスク (Attention Mask)**: 入力シーケンスにおいて、各トークンがモデルに対して無視される（パディングなどの場合）か考慮されるかを示すバイナリマスク。1はトークンが有効、0は無効を意味する。\n",
    "\n",
    "5. **グローバルマックスプーリング (Global Max Pooling)**: 畳み込みニューラルネットワークにおいて、各フィルターの出力の中で最大値を取得する操作のこと。これにより、異なる特徴を集約し、次の層へ送る。\n",
    "\n",
    "6. **カテゴリカルラベル (Categorical Labels)**: クラス分類問題において、各データがどのクラスに属するかを示すラベル。通常、数値の配列で表現され、複数のクラスの中から正しいクラスを予測するために使用される。\n",
    "\n",
    "7. **ドロップアウト (Dropout)**: ニューラルネットワークの過学習を防ぐために、各エポックの間にランダムに一定の割合のユニット（ノード）を無効にする手法。これにより、モデルの一般化能力が向上する。\n",
    "\n",
    "8. **エポック (Epoch)**: モデルが全ての訓練データを1回処理することを指す。訓練プロセスにおいてエポック数を指定することで、モデルが何回データを学習するかを制御する。\n",
    "\n",
    "9. **フィッティング (Fitting)**: モデルがトレーニングデータに対して学習するプロセス。モデルのパラメータを調整し、データのパターンを学ぶ。\n",
    "\n",
    "10. **ハイパーパラメータ (Hyperparameters)**: モデルの学習プロセスに関わる設定で、自動的に学習されないパラメータ。例としてはエポック数、バッチサイズ、学習率などがある。\n",
    "\n",
    "これらの用語は、機械学習や深層学習の初心者が実務において理解し、使いこなすために重要です。また、具体的な文脈やプロジェクトにおいてどう定義され、どう使われるかを知ることが理解を深める助けとなります。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:38.738037Z",
     "iopub.status.busy": "2024-08-03T08:03:38.73779Z",
     "iopub.status.idle": "2024-08-03T08:03:38.775Z",
     "shell.execute_reply": "2024-08-03T08:03:38.774155Z",
     "shell.execute_reply.started": "2024-08-03T08:03:38.738015Z"
    }
   },
   "outputs": [],
   "source": [
    "# このPython 3環境には、多くの便利な分析ライブラリがインストールされています\n",
    "# これは、kaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python\n",
    "# 例えば、以下のいくつかの便利なパッケージをロードします\n",
    "\n",
    "import numpy as np # 線形代数ライブラリ\n",
    "import pandas as pd # データ処理ライブラリ、CSVファイルの入出力（例: pd.read_csv）\n",
    "\n",
    "# 入力データファイルは、読み取り専用の\"../input/\"ディレクトリで利用できます\n",
    "# 例えば、これを実行すると（実行ボタンをクリックするかShift + Enterを押すことで）、入力ディレクトリ内のすべてのファイルがリストされます\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        # 各ファイルのパスを表示します\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# 現在のディレクトリ（/kaggle/working/）に最大20GBまで書き込むことができます\n",
    "# これは、「Save & Run All」を使用してバージョンを作成する際に出力として保存されます\n",
    "# また、一時ファイルを/kaggle/temp/に書き込むこともできますが、これらは現在のセッションの外部に保存されません"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e638faef",
   "metadata": {},
   "source": [
    "## ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:38.77685Z",
     "iopub.status.busy": "2024-08-03T08:03:38.776597Z",
     "iopub.status.idle": "2024-08-03T08:03:45.038125Z",
     "shell.execute_reply": "2024-08-03T08:03:45.037347Z",
     "shell.execute_reply.started": "2024-08-03T08:03:38.776828Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd  # データを処理するためのpandasライブラリをインポートします\n",
    "import numpy as np  # 数値計算のためのnumpyライブラリをインポートします\n",
    "from datasets import Dataset  # datasetsライブラリからDatasetクラスをインポートします（データセットの扱いに便利です）\n",
    "from functools import partial  # 関数を部分的に適用するためのpartial関数をインポートします\n",
    "from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割するためのtrain_test_split関数をインポートします\n",
    "from transformers import AutoTokenizer, TFAutoModel  # Hugging Faceのトランスフォーマーモデルとトークナイザーをインポートします\n",
    "from transformers import DebertaV2Tokenizer  # DeBERTa V2用のトークナイザーをインポートします\n",
    "import tensorflow as tf  # 深層学習ライブラリのTensorFlowをインポートします\n",
    "from tensorflow.keras.models import Sequential  # Kerasの順次モデルをインポートします\n",
    "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input  # Kerasのレイヤーをインポートします\n",
    "from keras.preprocessing import sequence as sq  # シーケンス処理のためのKerasのpreprocessingモジュールをインポートします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:45.040011Z",
     "iopub.status.busy": "2024-08-03T08:03:45.039313Z",
     "iopub.status.idle": "2024-08-03T08:03:48.113071Z",
     "shell.execute_reply": "2024-08-03T08:03:48.112184Z",
     "shell.execute_reply.started": "2024-08-03T08:03:45.039983Z"
    }
   },
   "outputs": [],
   "source": [
    "# train.csvファイルを読み込み、データをtrainデータフレームに格納します\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "\n",
    "# データフレームの最初の5行を表示します\n",
    "train.head(5)  # データの最初の5行を表示して、内容を確認します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:48.115769Z",
     "iopub.status.busy": "2024-08-03T08:03:48.115448Z",
     "iopub.status.idle": "2024-08-03T08:03:48.130037Z",
     "shell.execute_reply": "2024-08-03T08:03:48.129112Z",
     "shell.execute_reply.started": "2024-08-03T08:03:48.115742Z"
    }
   },
   "outputs": [],
   "source": [
    "# test.csvファイルを読み込み、データをtestデータフレームに格納します\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# データフレームの最初の5行を表示します\n",
    "test.head(5)  # データの最初の5行を表示して、内容を確認します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9cb7b9",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:48.131534Z",
     "iopub.status.busy": "2024-08-03T08:03:48.131184Z",
     "iopub.status.idle": "2024-08-03T08:03:48.13793Z",
     "shell.execute_reply": "2024-08-03T08:03:48.137022Z",
     "shell.execute_reply.started": "2024-08-03T08:03:48.131498Z"
    }
   },
   "outputs": [],
   "source": [
    "def combine_text(df):\n",
    "    def process(input_str):\n",
    "        # 文字列の前後のブラケットを取り除きます\n",
    "        stripped_str = input_str.strip('[]')\n",
    "        # 文字列を分割し、各文の前後の引用符を取り除きます\n",
    "        sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "        # 文をスペースで結合して返します\n",
    "        return  ' '.join(sentences)\n",
    "\n",
    "    # 配列を単一の文字列に変換します\n",
    "    df['prompt'] = df['prompt'].apply(process)  # 'prompt'列の各要素にprocess関数を適用します\n",
    "    df['response_a'] = df['response_a'].apply(process)  # 'response_a'列の各要素にprocess関数を適用します\n",
    "    df['response_b'] = df['response_b'].apply(process)  # 'response_b'列の各要素にprocess関数を適用します\n",
    "    \n",
    "    # テキストデータを結合します\n",
    "    # df['combined_text'] = '[PROMPT] ' + df['prompt'] + ' [RESPONSE_A] ' + df['response_a'] + ' [RESPONSE_B] ' + df['response_b']  # 結合済みテキストを新しい列に格納するためのコメントがあります"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:48.139695Z",
     "iopub.status.busy": "2024-08-03T08:03:48.139245Z",
     "iopub.status.idle": "2024-08-03T08:03:48.690674Z",
     "shell.execute_reply": "2024-08-03T08:03:48.689875Z",
     "shell.execute_reply.started": "2024-08-03T08:03:48.139664Z"
    }
   },
   "outputs": [],
   "source": [
    "combine_text(train)  # trainデータフレームに対してcombine_text関数を呼び出して、テキストを結合します\n",
    "\n",
    "# print(train['combined_text'][69])  # 結合されたテキストの69番目の要素を表示するためのコメントがあります（現在はコメントアウトされています）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:48.692026Z",
     "iopub.status.busy": "2024-08-03T08:03:48.691747Z",
     "iopub.status.idle": "2024-08-03T08:03:48.697713Z",
     "shell.execute_reply": "2024-08-03T08:03:48.696689Z",
     "shell.execute_reply.started": "2024-08-03T08:03:48.692003Z"
    }
   },
   "outputs": [],
   "source": [
    "# ラベルを作成します\n",
    "def create_label(df):\n",
    "    def process(row):\n",
    "        # model_aが勝者の場合、ラベル0を返します\n",
    "        if row['winner_model_a'] == 1:\n",
    "            return 0\n",
    "        # model_bが勝者の場合、ラベル1を返します\n",
    "        elif row['winner_model_b'] == 1:\n",
    "            return 1\n",
    "        # 引き分けの場合、ラベル2を返します\n",
    "        elif row['winner_tie'] == 1:\n",
    "            return 2\n",
    "        \n",
    "    # 各行に対してprocess関数を適用し、新しい'label'列を作成します\n",
    "    df['label'] = df.apply(process, axis=1)  # axis=1は行ごとに処理することを示しています"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:48.699028Z",
     "iopub.status.busy": "2024-08-03T08:03:48.698776Z",
     "iopub.status.idle": "2024-08-03T08:03:49.617907Z",
     "shell.execute_reply": "2024-08-03T08:03:49.61689Z",
     "shell.execute_reply.started": "2024-08-03T08:03:48.699006Z"
    }
   },
   "outputs": [],
   "source": [
    "create_label(train)  # trainデータフレームに対してcreate_label関数を呼び出して、ラベルを作成します\n",
    "print(train['label'][69])  # 作成されたラベルの69番目の要素を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:49.619396Z",
     "iopub.status.busy": "2024-08-03T08:03:49.619056Z",
     "iopub.status.idle": "2024-08-03T08:03:49.632744Z",
     "shell.execute_reply": "2024-08-03T08:03:49.631804Z",
     "shell.execute_reply.started": "2024-08-03T08:03:49.619369Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainデータフレームの形状を表示します\n",
    "print(\"train.shape\", train.shape)  # 行数と列数を出力します\n",
    "\n",
    "# trainデータフレームの最初の5行を表示します\n",
    "train.head()  # データの最初の5行を確認して、内容を表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389205fd",
   "metadata": {},
   "source": [
    "## トークナイザー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:49.636967Z",
     "iopub.status.busy": "2024-08-03T08:03:49.636648Z",
     "iopub.status.idle": "2024-08-03T08:03:49.642558Z",
     "shell.execute_reply": "2024-08-03T08:03:49.641664Z",
     "shell.execute_reply.started": "2024-08-03T08:03:49.636941Z"
    }
   },
   "outputs": [],
   "source": [
    "# トークンの最大長を設定します\n",
    "max_length = 1024  # モデルに入力できるトークンの最大数を1024に設定します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:49.644046Z",
     "iopub.status.busy": "2024-08-03T08:03:49.643715Z",
     "iopub.status.idle": "2024-08-03T08:03:50.066304Z",
     "shell.execute_reply": "2024-08-03T08:03:50.065011Z",
     "shell.execute_reply.started": "2024-08-03T08:03:49.644009Z"
    }
   },
   "outputs": [],
   "source": [
    "# DebertaV2Tokenizerを使用したトークナイゼーション\n",
    "model_name = \"/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1\"  # 使用するモデルのパスを指定します\n",
    "# model_name = \"/kaggle/input/deberta-v3/pytorch/large/1\"  # 別のモデル名をコメントアウトしています\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # 指定したモデルからトークナイザーをロードします\n",
    "# 語彙のサイズを制限します\n",
    "# tokenizer.model_max_length = max_length  # 最大トークン長を設定するコメントがあります\n",
    "tokenizer.add_tokens(['[CLS]', '[SEP]', '[PAD]'], special_tokens=True)  # 特殊トークンを追加します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:50.068743Z",
     "iopub.status.busy": "2024-08-03T08:03:50.068322Z",
     "iopub.status.idle": "2024-08-03T08:03:50.087034Z",
     "shell.execute_reply": "2024-08-03T08:03:50.086035Z",
     "shell.execute_reply.started": "2024-08-03T08:03:50.068705Z"
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_df(df, tokenizer):\n",
    "    # 特殊トークンが存在しない場合、チェックして設定します\n",
    "    if tokenizer.cls_token_id is None:\n",
    "        tokenizer.cls_token_id = tokenizer.convert_tokens_to_ids('[CLS]')  # '[CLS]'トークンのIDを設定します\n",
    "    if tokenizer.sep_token_id is None:\n",
    "        tokenizer.sep_token_id = tokenizer.convert_tokens_to_ids('[SEP]')  # '[SEP]'トークンのIDを設定します\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')  # '[PAD]'トークンのIDを設定します\n",
    "        \n",
    "    def process(row):\n",
    "        max_len = max_length - 2  # セパレータトークン2つ分を考慮した最大長を計算します\n",
    "        # プロンプトをトークナイズします\n",
    "        prompt_tokens = tokenizer(row['prompt'], truncation=True, max_length=max_len//4)['input_ids']\n",
    "        remaining_length = max_len - len(prompt_tokens)  # 残りの長さを計算します\n",
    "\n",
    "        # レスポンスAをトークナイズします\n",
    "        response_a_tokens = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2)['input_ids']\n",
    "        remaining_length -= len(response_a_tokens)  # 残りの長さを更新します\n",
    "\n",
    "        # レスポンスBをトークナイズします\n",
    "        response_b_tokens = tokenizer(row['response_b'], truncation=True, max_length=remaining_length//2)['input_ids']\n",
    "\n",
    "        # トークンを追加します\n",
    "        input_ids = [tokenizer.cls_token_id] + prompt_tokens + [tokenizer.sep_token_id] + response_a_tokens + [tokenizer.sep_token_id] + response_b_tokens\n",
    "        token_type_ids = [0] * (len(prompt_tokens) + 2) + [1] * (len(response_a_tokens) + 1) + [2] * len(response_b_tokens)\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # パディングを追加します\n",
    "        padding_length = max_length - len(input_ids)\n",
    "        if padding_length > 0:\n",
    "            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length  # パディングトークンで埋めます\n",
    "            token_type_ids = token_type_ids + [0] * padding_length\n",
    "            attention_mask = attention_mask + [0] * padding_length\n",
    "\n",
    "        input_ids = input_ids[:max_length]  # 最大長を超える場合は切り詰めます\n",
    "        token_type_ids = token_type_ids[:max_length]\n",
    "        attention_mask = attention_mask[:max_length]\n",
    "        \n",
    "        return input_ids, token_type_ids, attention_mask\n",
    "    \n",
    "    # 各行に対してprocess関数を適用し、新しいトークン関連の列を作成します\n",
    "    df[['input_ids', 'token_type_ids', 'attention_mask']] = df.apply(lambda row: pd.Series(process(row)), axis=1)\n",
    "#     tokenized = df.apply(lambda row: pd.Series(process(row)), axis=1)  # コメントアウトされたコード\n",
    "#     df.loc[:, ['input_ids', 'token_type_ids', 'attention_mask']] = tokenized  # コメントアウトされたコード\n",
    "#     return df  # コメントアウトされたコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:50.088603Z",
     "iopub.status.busy": "2024-08-03T08:03:50.08816Z",
     "iopub.status.idle": "2024-08-03T08:03:50.09929Z",
     "shell.execute_reply": "2024-08-03T08:03:50.098205Z",
     "shell.execute_reply.started": "2024-08-03T08:03:50.08856Z"
    }
   },
   "outputs": [],
   "source": [
    "# ラベルをカテゴリカル形式に変換します\n",
    "labels = tf.keras.utils.to_categorical(train['label'], num_classes=3)  # 3つのクラスでカテゴリカルラベルを生成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:03:50.100788Z",
     "iopub.status.busy": "2024-08-03T08:03:50.100454Z",
     "iopub.status.idle": "2024-08-03T08:06:58.52437Z",
     "shell.execute_reply": "2024-08-03T08:06:58.523577Z",
     "shell.execute_reply.started": "2024-08-03T08:03:50.100757Z"
    }
   },
   "outputs": [],
   "source": [
    "# trainデータフレームに対して、トークナイゼーションを実施します\n",
    "tokenize_df(train, tokenizer)  # tokenize_df関数を呼び出して、トークン関連の列を生成します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:06:58.525707Z",
     "iopub.status.busy": "2024-08-03T08:06:58.525433Z",
     "iopub.status.idle": "2024-08-03T08:07:07.039745Z",
     "shell.execute_reply": "2024-08-03T08:07:07.038886Z",
     "shell.execute_reply.started": "2024-08-03T08:06:58.525683Z"
    }
   },
   "outputs": [],
   "source": [
    "# トレーニングのためのデータを準備します\n",
    "input_ids = train['input_ids']  # 'input_ids'列を取得します\n",
    "attention_mask = train['attention_mask']  # 'attention_mask'列を取得します\n",
    "\n",
    "# input_idsとattention_maskを最大長に合わせてパディングします\n",
    "X_train = sq.pad_sequences(input_ids, maxlen=max_length)  # 入力IDにパディングを適用します\n",
    "X_train_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)  # 注意マスクにパディングを適用します\n",
    "\n",
    "y_train = labels  # ラベルをy_trainに格納します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bfbab0",
   "metadata": {},
   "source": [
    "## モデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:26.889671Z",
     "iopub.status.busy": "2024-08-03T08:10:26.889305Z",
     "iopub.status.idle": "2024-08-03T08:10:26.901293Z",
     "shell.execute_reply": "2024-08-03T08:10:26.900295Z",
     "shell.execute_reply.started": "2024-08-03T08:10:26.88964Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D, Masking  # 必要なKerasレイヤーをインポートします\n",
    "\n",
    "# CNNモデルを定義します\n",
    "def create_cnn_model(vocab_size, embedding_dim, max_length):\n",
    "    model = Sequential([\n",
    "        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します\n",
    "        Conv1D(filters=256, kernel_size=5, activation='relu'),  # 畳み込み層を追加します\n",
    "        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します\n",
    "        Dense(128, activation='relu'),  # 全結合層を追加します\n",
    "        Dropout(0.5),  # ドロップアウト層を追加して過学習を防ぎます\n",
    "        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします\n",
    "    return model\n",
    "\n",
    "# LSTMモデルを定義します\n",
    "def create_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    model = Sequential([\n",
    "        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します\n",
    "        LSTM(256, return_sequences=True),  # LSTM層を追加します\n",
    "        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します\n",
    "        Dense(128, activation='relu'),  # 全結合層を追加します\n",
    "        Dropout(0.5),  # ドロップアウト層を追加します\n",
    "        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします\n",
    "    return model\n",
    "\n",
    "# CNN LSTMモデルを定義します\n",
    "def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):\n",
    "    model = Sequential([\n",
    "        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します\n",
    "        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します\n",
    "        Conv1D(filters=128, kernel_size=5, activation='relu'),  # 畳み込み層を追加します\n",
    "        LSTM(128, return_sequences=True),  # LSTM層を追加します\n",
    "        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します\n",
    "        Dense(64, activation='relu'),  # 全結合層を追加します\n",
    "        Dropout(0.5),  # ドロップアウト層を追加して過学習を防ぎます\n",
    "        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:28.674752Z",
     "iopub.status.busy": "2024-08-03T08:10:28.674272Z",
     "iopub.status.idle": "2024-08-03T08:10:28.766639Z",
     "shell.execute_reply": "2024-08-03T08:10:28.765718Z",
     "shell.execute_reply.started": "2024-08-03T08:10:28.674703Z"
    }
   },
   "outputs": [],
   "source": [
    "# パラメータを設定します\n",
    "vocab_size = tokenizer.vocab_size  # トークナイザーから語彙サイズを取得します\n",
    "# vocab_size = max_length  # コメントアウトされたコード\n",
    "embedding_dim = 100  # 埋め込み次元を設定します\n",
    "max_length = max_length  # 最大長を設定します\n",
    "max_features = tokenizer.vocab_size  # 最大特徴量を語彙サイズで設定します\n",
    "# max_features = max_length * 2  # コメントアウトされたコード\n",
    "max_len = max_length  # 最大長をmax_lenに再設定します\n",
    "maxlen = max_len  # maxlenにその値を設定します\n",
    "batch_size = 16  # バッチサイズを設定します\n",
    "embedding_dims = 100  # 埋め込み次元を設定します\n",
    "nb_filter = 150  # フィルター数を設定します\n",
    "filter_length = 3  # フィルターの長さを設定します\n",
    "hidden_dims = 100  # 隠れ層の次元を設定します\n",
    "nb_epoch = 100  # エポック数を設定します\n",
    "\n",
    "# モデルを作成します\n",
    "# model = create_lstm_model(vocab_size, embedding_dim, max_length)  # LSTMモデルを作成するためのコメントアウトされたコード\n",
    "model = create_cnn_lstm_model(vocab_size, embedding_dim, max_length)  # CNN LSTMモデルを作成します\n",
    "model.summary()  # モデルの要約を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:36.064883Z",
     "iopub.status.busy": "2024-08-03T08:10:36.064528Z",
     "iopub.status.idle": "2024-08-03T08:10:36.073974Z",
     "shell.execute_reply": "2024-08-03T08:10:36.072978Z",
     "shell.execute_reply.started": "2024-08-03T08:10:36.064855Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function  # 将来のバージョンとの互換性のためのインポート\n",
    "import numpy as np  # 数値計算のためのnumpyライブラリをインポートします\n",
    "\n",
    "from keras.preprocessing import sequence  # シーケンス処理のためのKerasのpreprocessingモジュールをインポートします\n",
    "from keras.models import Sequential  # Kerasの順次モデルをインポートします\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda  # 様々なKerasレイヤーをインポートします\n",
    "from keras.layers import Embedding  # 埋め込み層をインポートします\n",
    "from keras.layers import Convolution1D, LSTM  # 1次元畳み込み層とLSTMをインポートします\n",
    "from keras.datasets import imdb  # IMDbデータセットをインポートします\n",
    "from keras import backend as K  # Kerasバックエンドをインポートします\n",
    "from keras.optimizers import Adadelta, Adamax  # 最適化アルゴリズムをインポートします\n",
    "from keras.preprocessing import sequence as sq  # シーケンス処理のためのKerasのpreprocessingモジュールを再インポートします\n",
    "\n",
    "from keras.layers import Dense, Dropout, Activation, Lambda, Input, TimeDistributed, Flatten  # 複数のKerasレイヤーをインポートします\n",
    "from keras.models import Model  # Kerasのモデルクラスをインポートします\n",
    "from keras.callbacks import ModelCheckpoint  # モデルチェックポイント用のコールバックをインポートします"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:36.076014Z",
     "iopub.status.busy": "2024-08-03T08:10:36.075682Z",
     "iopub.status.idle": "2024-08-03T08:10:36.081868Z",
     "shell.execute_reply": "2024-08-03T08:10:36.080998Z",
     "shell.execute_reply.started": "2024-08-03T08:10:36.075991Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# 使用されていないインポートのコメントアウトされたコード\n",
    "# from tensorflow.keras.layers import Layer  # Kerasレイヤーをインポートします\n",
    "# from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D  # 複数のKerasレイヤーをインポートします\n",
    "# from keras.layers import GlobalMaxPooling1D  # グローバルマックスプーリング層をインポートします\n",
    "# import tensorflow as tf  # TensorFlowライブラリをインポートします\n",
    "\n",
    "# ApplyAttentionMaskクラスを定義します\n",
    "# class ApplyAttentionMask(Layer):\n",
    "#     def call(self, inputs):\n",
    "#         embeddings, attention_mask = inputs  # 入力の分離\n",
    "#         return embeddings * tf.expand_dims(attention_mask, -1)  # アテンションマスクを適用します\n",
    "\n",
    "# 入力層を定義します\n",
    "# input_layer = Input(shape=(max_length,), dtype='int32', name='main_input')  # 主入力層を定義します\n",
    "# attention_masks = Input(shape=(max_length,), dtype='float32', name=\"attention_masks\")  # アテンションマスク用の入力層を定義します\n",
    "\n",
    "# 埋め込み層を定義します\n",
    "# emb_layer = Embedding(max_features,\n",
    "#                       embedding_dims,\n",
    "#                       input_length=max_len\n",
    "#                       )(input_layer)  # 埋め込み層を定義します\n",
    "\n",
    "# アテンションマスクを適用した埋め込みを定義します\n",
    "# masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])\n",
    "\n",
    "# LSTMブランチを定義します\n",
    "# lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)  # 最初のLSTM層を追加します\n",
    "# lstm_out = LSTM(64, return_sequences=True)(lstm_out)  # 2つ目のLSTM層を追加します\n",
    "# lstm_out = LSTM(32)(lstm_out)  # 3つ目のLSTM層を追加します\n",
    "# lstm_out = BatchNormalization()(lstm_out)  # バッチ正規化を適用します\n",
    "# lstm_out = Dropout(0.5)(lstm_out)  # ドロップアウトを適用します\n",
    "# lstm_out = GlobalMaxPooling1D()(lstm_out)  # グローバルマックスプーリングを適用します\n",
    "\n",
    "# CNN層ブランチを定義します\n",
    "# cnn_out = Conv1D(128, 5, activation='relu')(masked_embeddings)  # 畳み込み層を追加します\n",
    "# cnn_out = Conv1D(64, 5, activation='relu')(cnn_out)  # 2つ目の畳み込み層を追加します\n",
    "# cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)  # 3つ目の畳み込み層を追加します\n",
    "# cnn_out = BatchNormalization()(cnn_out)  # バッチ正規化を適用します\n",
    "# cnn_out = Dropout(0.5)(cnn_out)  # ドロップアウトを適用します\n",
    "# cnn_out = GlobalMaxPooling1D()(cnn_out)  # グローバルマックスプーリングを適用します\n",
    "\n",
    "# LSTMとCNNの出力を連結します\n",
    "# merged = concatenate([lstm_out, cnn_out])  # 出力を連結します\n",
    "# merged = Dense(32, activation='sigmoid')(merged)  # 全結合層を追加します\n",
    "# merged = BatchNormalization()(merged)  # バッチ正規化を適用します\n",
    "# merged = Dropout(0.5)(merged)  # ドロップアウトを適用します\n",
    "# pred = Dense(3, activation='softmax')(merged)  # 最終出力層を追加します\n",
    "\n",
    "# モデルを構築します\n",
    "# model = Model(inputs=[input_layer, attention_masks], outputs=[pred])  # モデルを構築します\n",
    "# adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)  # Adadeltaオプティマイザの設定です\n",
    "# adamax = Adamax(learning_rate=0.001)  # Adamaxオプティマイザの設定です\n",
    "# model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])  # モデルをコンパイルします\n",
    "# model.summary()  # モデルの要約を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:36.083267Z",
     "iopub.status.busy": "2024-08-03T08:10:36.082972Z",
     "iopub.status.idle": "2024-08-03T08:10:36.096475Z",
     "shell.execute_reply": "2024-08-03T08:10:36.095652Z",
     "shell.execute_reply.started": "2024-08-03T08:10:36.083245Z"
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf  # TensorFlowライブラリをインポートします\n",
    "# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Masking  # Kerasレイヤーをインポートします\n",
    "# from tensorflow.keras.models import Model  # Kerasのモデルクラスをインポートします\n",
    "# from transformers import DebertaTokenizer, AutoModel  # Hugging Faceのトランスフォーマーモデルをインポートします\n",
    "\n",
    "# ハイブリッドCNN-LSTMモデルを作成する関数を定義します\n",
    "# def create_cnn_lstm_hybrid_model(base_model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes):\n",
    "#     # 事前学習済みBERTモデルをロードします\n",
    "#     model = AutoModel.from_pretrained(base_model_name)  # 指定されたモデル名からモデルをロードします\n",
    "    \n",
    "#     # 入力を定義します\n",
    "#     input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')  # input_ids用の入力層\n",
    "#     attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')  # attention_mask用の入力層\n",
    "\n",
    "#     # BERTの出力を取得します\n",
    "#     outputs = model(input_ids, attention_mask=attention_mask)  # モデルによって出力を取得します\n",
    "#     seq_output = outputs.last_hidden_state  # seq_outputの形状: (バッチサイズ, シーケンス長, 隠れサイズ)\n",
    "\n",
    "#     # CNNを適用します\n",
    "#     cnn_output = Conv1D(filters=cnn_output_channels, kernel_size=cnn_kernel_size, padding='same', activation='relu')(seq_output)  # 畳み込み層を適用します\n",
    "\n",
    "#     # パディングされたシーケンスを処理するためにマスキングを適用します\n",
    "#     masked_cnn_output = Masking()(cnn_output)  # マスキングを適用します\n",
    "    \n",
    "#     # LSTMを適用します\n",
    "#     rnn_output = LSTM(hidden_dim)(masked_cnn_output)  # LSTM層を適用します\n",
    "\n",
    "#     # クラス分類用の層を定義します\n",
    "#     logits = Dense(num_classes, activation='softmax')(rnn_output)  # クラス数に応じた出力層を設定します\n",
    "\n",
    "#     # モデルを作成します\n",
    "#     model = Model(inputs=[input_ids, attention_mask], outputs=logits)  # モデルを構築します\n",
    "\n",
    "#     return model  # モデルを返します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ハイパーパラメータを定義します\n",
    "# bert_model_name = '/kaggle/input/deberta_v3/keras/deberta_v3_large_en/2'  # 使用するBERTモデルのパス\n",
    "# bert_model_name = '/kaggle/input/deberta-v3/pytorch/large/1'  # 別のBERTモデルのパス\n",
    "# bert_model_name = 'deberta_v3_large_en'  # 使用するBERTモデル名\n",
    "# model_name = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'  # 使用するQwenモデルのパス\n",
    "# cnn_output_channels = 128  # CNNの出力チャンネル数\n",
    "# cnn_kernel_size = 5  # CNNのカーネルサイズ\n",
    "# hidden_dim = 256  # 隠れ層の次元\n",
    "# num_classes = 3  # クラス数\n",
    "\n",
    "# # モデルを初期化します\n",
    "# model = create_cnn_lstm_hybrid_model(model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes)  # ハイブリッドモデルを作成します\n",
    "# model.summary()  # モデルの要約を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-03T08:10:51.105324Z",
     "iopub.status.busy": "2024-08-03T08:10:51.104735Z",
     "iopub.status.idle": "2024-08-03T08:11:52.190361Z",
     "shell.execute_reply": "2024-08-03T08:11:52.188502Z",
     "shell.execute_reply.started": "2024-08-03T08:10:51.105291Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping  # 早期終了用のコールバックをインポートします\n",
    "\n",
    "# モデルをトレーニングします\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)  # バリデーション損失を監視し、8エポックで改善がない場合にトレーニングを停止します\n",
    "\n",
    "# モデルのフィッティングを行います\n",
    "history = model.fit([X_train, X_train_attention_mask], y_train, epochs=20, batch_size=32, validation_split=0.2,  # バリデーションデータを分割して使用します\n",
    "                    callbacks=[early_stopping])  # 早期終了のコールバックを適用します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567287c",
   "metadata": {},
   "source": [
    "## テスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-03T08:11:52.191401Z",
     "iopub.status.idle": "2024-08-03T08:11:52.191823Z",
     "shell.execute_reply": "2024-08-03T08:11:52.19166Z",
     "shell.execute_reply.started": "2024-08-03T08:11:52.191643Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータをエンコードします\n",
    "combine_text(test)  # テストデータに対してテキストを結合します\n",
    "tokenize_df(test, tokenizer)  # テストデータに対してトークナイゼーションを実施します\n",
    "\n",
    "input_ids = test['input_ids']  # 'input_ids'列を取得します\n",
    "attention_mask = test['attention_mask']  # 'attention_mask'列を取得します\n",
    "\n",
    "# input_idsとattention_maskを最大長に合わせてパディングします\n",
    "X_test = sq.pad_sequences(input_ids, maxlen=max_length)  # テストデータの入力IDにパディングを適用します\n",
    "X_test_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)  # テストデータの注意マスクにパディングを適用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-03T08:11:52.19495Z",
     "iopub.status.idle": "2024-08-03T08:11:52.195296Z",
     "shell.execute_reply": "2024-08-03T08:11:52.195116Z",
     "shell.execute_reply.started": "2024-08-03T08:11:52.195104Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータに対して予測を行います\n",
    "predictions = model.predict([X_test, X_test_attention_mask])  # モデルを使用して予測を行います\n",
    "predictions  # 予測結果を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-08-03T08:11:52.196009Z",
     "iopub.status.idle": "2024-08-03T08:11:52.196355Z",
     "shell.execute_reply": "2024-08-03T08:11:52.1962Z",
     "shell.execute_reply.started": "2024-08-03T08:11:52.196186Z"
    }
   },
   "outputs": [],
   "source": [
    "# 予測結果をデータフレームに変換します\n",
    "winner = pd.DataFrame(predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # 予測結果をデータフレームに格納します\n",
    "result = pd.concat([test['id'], winner], axis=1)  # テストデータのIDと予測結果を結合します\n",
    "\n",
    "# 結果をCSVファイルに保存します\n",
    "result.to_csv('submission.csv', index=False)  # 提出用のCSVファイルを作成します\n",
    "result  # 結果を表示します"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 60131,
     "modelInstanceId": 43529,
     "sourceId": 51747,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 71342,
     "modelInstanceId": 51944,
     "sourceId": 62188,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 2820,
     "modelInstanceId": 4687,
     "sourceId": 6066,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
