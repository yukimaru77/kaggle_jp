{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "325d1178",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、LMSYS - Chatbot Arena コンペティションにおける人間の好み予測モデルの構築を目的としています。具体的には、異なる大規模言語モデル（LLM）の応答がどちらがユーザーに好まれるかを予測するための強化学習を活用した選好モデルのファインチューニングと推論を行っています。\n",
    "\n",
    "### 主な内容と問題設定\n",
    "- **問題**: モデルは、ユーザーの選好を予測するために、異なるLLMが生成した応答を受け取り、どちらの応答がより好まれるか（または同点）を予測することを目的としています。具体的には、ユーザーから提供されたプロンプトに対する応答Aと応答Bを比較します。\n",
    "  \n",
    "### 使用する手法やライブラリ\n",
    "1. **ライブラリのインストール**:\n",
    "   - `bitsandbytes`, `transformers`, `tokenizers`, `peft` などの必須ライブラリをインストールしています。\n",
    "\n",
    "2. **データ準備**:\n",
    "   - Kaggleからテストデータを読み込み、プロンプトと応答をトークン化する関数`tokenize`を定義し、データセットを適切な長さに調整しています。最大シーケンス長は2400に設定。\n",
    "\n",
    "3. **モデルの構築**:\n",
    "   - `Llama3ForSFT`クラスを定義し、事前訓練されたLLaMAモデルをベースとしたモデルを構築しています。これにより、入力データを受け取り、隠れ状態からロジットを計算します。\n",
    "\n",
    "4. **推論**:\n",
    "   - 複数のGPUを活用して推論を行い、各応答（A, B）の勝率を計算します。また、結果をDataFrameに格納し、最終的な提出ファイル（submission.csv）を作成しています。\n",
    "\n",
    "5. **マルチスレッド処理**:\n",
    "   - `ThreadPoolExecutor`を使用して、効率的に推論を実行し、計算時間を短縮しています。\n",
    "\n",
    "### 結果の提出\n",
    "- モデルの推論結果は`submission.csv`として保存され、各IDに対する応答の勝率（winner_model_a, winner_model_b, winner_tie）を含む形式で生成されています。\n",
    "\n",
    "このノートブックは、現実の人間の選好を機械学習技術を用いて予測するための基盤として、特にLLMのトレーニングと評価に焦点を当てています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb95e54",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、初心者が『LMSYS - Chatbot Arena 人間による好み予測チャレンジ』のノートブックを読んでいて、理解に苦しむ可能性がある専門用語の解説です。\n",
    "\n",
    "1. **PEFT（Parameter-Efficient Fine-Tuning）**:\n",
    "   - モデルの全てのパラメータを調整することなく、一部のパラメータのみを調整することで、モデルのパフォーマンスを向上させる技術です。この技術は、特に大規模なモデルを効率的にファインチューニングする際に有効です。\n",
    "\n",
    "2. **LoRA（Low-Rank Adaptation）**:\n",
    "   - PGFTと同様に、既存の重みを固定し、補助的な低ランクの重みを追加することでモデルの適応を行う手法です。これにより、少ないパラメータで効率的にファインチューニングが可能となります。\n",
    "\n",
    "3. **Causal Language Model (CausalLM)**:\n",
    "   - 自己回帰型言語モデルで、シーケンスの次の単語を予測するために、これまでの単語の情報のみを利用します。例えば、文の最初から最後までの単語を順に生成するモデルです。\n",
    "\n",
    "4. **ノイズがないトークナイザー**:\n",
    "   - 通常のトークナイザーは、テキストに対して特殊なトークン（例えば、[CLS]や[SEP]など）を追加しますが、ノイズのないトークナイザーは追加トークンなしで純粋にトークン化のみを行います。文脈の理解に影響を与えないことが期待されます。\n",
    "\n",
    "5. **アテンションマスク**:\n",
    "   - トランスフォーマモデルにおいて、入力の一部を無視するためのマスクです。通常、パディングされた部分を無視させるために使われ、アテンション演算を効率化します。\n",
    "\n",
    "6. **エクストラ・プロンプト**:\n",
    "   - モデルに与える追加の情報や指示です。通常のプロンプトではカバーできない詳細な指示を提供することで、モデルの出力の品質を向上させることが可能です。\n",
    "\n",
    "7. **torch.cuda.amp**:\n",
    "   - 自動混合精度（Automatic Mixed Precision）のこと。計算を行う際に、必要に応じて32ビットと16ビットの浮動小数点を使うことにより、メモリ使用量を削減し、計算速度を向上させる技術です。\n",
    "\n",
    "8. **モデル並列（Model Parallelism）**:\n",
    "   - モデルが大きすぎて1つのGPUに収まらない場合、異なる部分のモデルを複数のGPUにわけて実行する手法です。\n",
    "\n",
    "9. **ロジット（Logits）**:\n",
    "   - モデルの出力で、クラスごとのスコアや値です。通常、カテゴリ分け問題においてソフトマックス関数を通して各クラスの確率に変換されます。\n",
    "\n",
    "10. **グラディエント計算**:\n",
    "    - ネットワークの出力に対する誤差の寄与度を計算し、パラメータの更新に利用するプロセスです。これによりモデルの学習が行われます。\n",
    "\n",
    "11. **重みのキー（Weight Keys）**:\n",
    "    - モデルの重みにアクセスする際の識別子。特定の重みを指定し、それらを一括で更新したり参照するために用いられます。\n",
    "\n",
    "12. **特定のトークン**:\n",
    "    - モデルによって定義された特別なトークン。例えば、ラベルのトークンIDは特定の応答を示すために使用されることが多いです。\n",
    "\n",
    "これらの用語についての理解が 進むことで、Jupyter Notebookの内容がより明確に理解できるようになるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6c8cfb",
   "metadata": {},
   "source": [
    "# 注意\n",
    "- [トレーニングスクリプト](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T19:17:37.872397Z",
     "iopub.status.busy": "2024-08-01T19:17:37.871598Z"
    },
    "papermill": {
     "duration": 53.686843,
     "end_time": "2024-07-01T02:57:31.530755",
     "exception": false,
     "start_time": "2024-07-01T02:56:37.843912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 21.138547,
     "end_time": "2024-07-01T02:57:52.676238",
     "exception": false,
     "start_time": "2024-07-01T02:57:31.537691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "\n",
    "# CUDAのメモリ効率の良いSDPを有効化します\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "# CUDAのフラッシュSDPを有効化します\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "\n",
    "# GPUデバイスが2つ存在することを確認します\n",
    "assert torch.cuda.device_count() == 2, \"すみません - マルチGPUが必要です！\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-07-01T02:57:52.720706",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.705889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'  # 使用するモデルのパス\n",
    "WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'  # 学習済みモデルの重みのパス\n",
    "MAX_LENGTH = 2400  # 最大シーケンス長\n",
    "BATCH_SIZE = 2  # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # CUDAデバイスを使用する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810badc7",
   "metadata": {},
   "source": [
    "# データの準備\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040947,
     "end_time": "2024-07-01T02:57:52.781962",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.741015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込みます\n",
    "\n",
    "# トークナイザーでトークン化する関数\n",
    "def tokenize(example, tokenizer):\n",
    "    prompts = tokenizer(eval(example['prompt'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]  # プロンプトのトークン化\n",
    "    responses_a = tokenizer(eval(example['response_a'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]  # 応答Aのトークン化\n",
    "    responses_b = tokenizer(eval(example['response_b'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]  # 応答Bのトークン化\n",
    "    \n",
    "    # プロンプト、応答A、応答Bの長さが一致することを確認\n",
    "    assert len(prompts) == len(responses_a) == len(responses_b), \"プロンプト、応答A、応答Bの長さが一致しません\"\n",
    "    \n",
    "    # プロンプト、応答を逆順にします\n",
    "    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]\n",
    "    \n",
    "    prompt, response_a, response_b = [], [], []\n",
    "    p_len, a_len, b_len = 0, 0, 0\n",
    "    \n",
    "    # それぞれのプロンプトと応答の長さを計算します\n",
    "    for p, a, b in zip(prompts, responses_a, responses_b):\n",
    "        prompt.append(p)\n",
    "        response_a.append(a)\n",
    "        response_b.append(b)\n",
    "        p_len += len(p)\n",
    "        a_len += len(a)\n",
    "        b_len += len(b)\n",
    "        # 最大シーケンス長を超える場合は停止します\n",
    "        if p_len + a_len + b_len > MAX_LENGTH:\n",
    "            break\n",
    "\n",
    "    # リストをフラット化して逆順に戻します\n",
    "    prompt = [item for sublist in reversed(prompt) for item in sublist]\n",
    "    response_a = [item for sublist in reversed(response_a) for item in sublist]\n",
    "    response_b = [item for sublist in reversed(response_b) for item in sublist]\n",
    "    \n",
    "    # トークン数が最大長を超える場合はカットします\n",
    "    p_a_b_len = len(prompt) + len(response_a) + len(response_b)\n",
    "    cut_len = p_a_b_len - MAX_LENGTH\n",
    "    if cut_len > 0:\n",
    "        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]\n",
    "        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]\n",
    "        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]\n",
    "    \n",
    "    # トークンを追加します\n",
    "    prompt = tokenizer('<prompt>: ', add_special_tokens=False)[\"input_ids\"] + prompt\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ', add_special_tokens=False)[\"input_ids\"] + response_a\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ', add_special_tokens=False)[\"input_ids\"] + response_b\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nどちらがプロンプトに対してより良い応答ですか？ a または b または tie ?\\n\\n回答: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    label_token_id = [128250]  # ラベル用トークンID\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]  # 入力IDを構築\n",
    "    attention_mask = len(input_ids)*[1]  # アテンションマスクを構築\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]  # ラベルを構築\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a6d10",
   "metadata": {},
   "source": [
    "# トークン化\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.596117,
     "end_time": "2024-07-01T02:57:53.425111",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.828994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーを読み込みます\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]  # ラベルのトークンIDを取得します\n",
    "\n",
    "# データを読み込む関数\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)  # パンダのデータフレームからデータセットを作成します\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        # remove_columns=raw_datasets.column_names,  # 列を削除するオプション（コメントアウトされています）\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "test_ds = load_data(test, tokenizer)  # データをトークン化します\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-07-01T02:57:53.472007",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.456043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = test_ds.to_pandas()  # データセットをパンダのデータフレームに変換します\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)  # 各入力の最大長を計算します\n",
    "data[:3]  # データの最初の3行を表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input_ids'][0]  # 最初の入力IDを表示します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 最初の入力IDをデコードして表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2809326",
   "metadata": {},
   "source": [
    "# モデルの読み込み \n",
    "各GPUに1つのモデルを読み込みます。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # モデルに関連する重みのキー\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)  # Llamaモデルを初期化\n",
    "        self.vocab_size = config.vocab_size  # ボキャブラリのサイズを取得\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # ロジットを計算するための線形層\n",
    "        self.post_init()  # モデルの初期化後の処理\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ状態を取得\n",
    "        if self.config.pretraining_tp > 1:  # モデル並列が必要な場合\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)  # 重みを分割\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]  # ロジットを計算\n",
    "            logits = torch.cat(logits, dim=-1)  # 結果を結合\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # 通常のロジット計算\n",
    "\n",
    "        logits = logits.float()  # ロジットをfloat型に変換\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:  # ラベルがある場合\n",
    "            # トークンをシフトしてnを予測する\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # トークンをフラット化\n",
    "            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピーロスを定義\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラット化\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルをフラット化\n",
    "            # モデル並列を有効にする\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250], device=shift_labels.device)  # 偽ラベルのトークンID\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルのトークンID\n",
    "            \n",
    "            # true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 本物のラベルの取得（コメントアウトされています）\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:, label_tokens_ids]  # 本物のロジットを取得\n",
    "\n",
    "        return CausalLMOutputWithPast(  # 結果を返します\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 105.076557,
     "end_time": "2024-07-01T02:59:38.570536",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.493979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU 0にベースモデルを読み込みます\n",
    "device0 = torch.device('cuda:0')\n",
    "base_model_0 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "# GPU 1にベースモデルを読み込みます\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:1',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b15793",
   "metadata": {},
   "source": [
    "# 重みの読み込み \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.701042,
     "end_time": "2024-07-01T02:59:52.320278",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.619236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PEFTを取得します\n",
    "model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0)  # GPU 0にモデルを移動します\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)  # GPU 1にモデルを移動します\n",
    "model_1.eval()  # 評価モードに設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb72a42",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021078,
     "end_time": "2024-07-01T02:59:52.402973",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.381895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配計算を無効にします\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を有効にします\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    a_win, b_win, tie = [], [], []  # 各応答の勝率を格納するリスト\n",
    "\n",
    "    model.eval()  # 評価モードに設定\n",
    "    for start_idx in range(0, len(df), batch_size):  # バッチ単位でループ\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックス\n",
    "        tmp = df.iloc[start_idx:end_idx]  # 一時データフレームを作成\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストとして取得\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストとして取得\n",
    "        labels = tmp[\"labels\"].to_list()  # ラベルをリストとして取得\n",
    "        \n",
    "        # パディングを行い、テンソルに変換します\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)  # デバイスに移動\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)  # デバイスに移動\n",
    "        \n",
    "        pad_labels = []  # パディングラベルのリストを初期化\n",
    "        for label in labels:\n",
    "            # ラベルを入力の長さに合わせてパディングします\n",
    "            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0] - label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルに入力を渡して出力を取得\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ソフトマックスを適用して確率を取得\n",
    "        \n",
    "        # 各応答の勝率をリストに追加します\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df['winner_model_a'] = a_win  # 応答Aの勝率\n",
    "    df['winner_model_b'] = b_win  # 応答Bの勝率\n",
    "    df['winner_tie'] = tie  # 同点の勝率\n",
    "    return df  # 結果を返します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.316613,
     "end_time": "2024-07-01T02:59:55.727834",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.411221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 時間計測の開始\n",
    "\n",
    "# データを最大長でソートします\n",
    "data = data.sort_values(\"max_len\", ascending=False)\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータを取得\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータを取得\n",
    "\n",
    "# マルチスレッドで推論を実行します\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 確率を取得\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.755381,
     "end_time": "2024-07-01T02:59:57.492377",
     "exception": false,
     "start_time": "2024-07-01T02:59:55.736996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝率を結果データフレームに保存\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝率を結果データフレームに保存\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # 同点の勝率を結果データフレームに保存\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルとして保存\n",
    "display(submission_df)  # 提出データフレームを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c900b6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# コメント \n",
    "\n",
    "> ## Songling\n",
    "> \n",
    "> このファイルが見つかりません。誰か助けてくれませんか？\n",
    "> \n",
    "> 警告: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスか、特定のスキームが欠如しています。\n",
    "> \n",
    "> エラー: 要件bitsandbytesを満たすバージョンが見つかりません（バージョン: なし）\n",
    "> \n",
    "> エラー: peftに対する一致する配布が見つかりません\n",
    "> \n",
    "> 警告: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスか、特定のスキームが欠如しています。\n",
    "> \n",
    "> 警告: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスか、特定のスキームが欠如しています。\n",
    "> \n",
    "> 警告: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスか、特定のスキームが欠如しています。\n",
    "> \n",
    "> エラー: 要件peftを満たすバージョンが見つかりません（バージョン: なし）\n",
    "> \n",
    "> エラー: 一致する配布が見つかりませんでした\n",
    "\n",
    "---\n",
    "\n",
    "> ## Shimei\n",
    "> \n",
    "> こんにちは、この素晴らしいノートブックを共有してくれてありがとう！\n",
    "> \n",
    "> しかし、トレーニングノートブックのファインチューニングの後、推論時に次のエラーが発生します。\n",
    "> \n",
    "> OSError: /kaggle/input/llama3-sft/checkpoint-2800 に config.jsonというファイルが見当たりません。'https://huggingface.co//kaggle/input/llama3-sft/checkpoint-2800/tree/None' を確認して、利用可能なファイルを確認してください。\n",
    "\n",
    "> ## Romanov_Alex\n",
    "> > 同じです。修正方法を試してみましたか？\n",
    "\n",
    "> ## duncangao\n",
    "> > 私も同じです。修正方法を知りたいです。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Huang Jing Stark\n",
    "> \n",
    "> MAX_LENGTHを2400に設定した具体的な理由はありますか？\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > モデルにより多くの情報を与えるためです。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Rabbit\n",
    "> \n",
    "> モデルをHuggingFaceからKaggleにどうやってロードしますか？\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > 新しいノートブックを作成し、インターネットを開き、スナップショットダウンロードを使用してモデルをダウンロードし、このノートブックを新しいデータセットとして使用します。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Dlond Mike\n",
    "> \n",
    "> でも、君は本当に英雄だ。\n",
    "\n",
    "---\n",
    "\n",
    "> ## PaulRRR\n",
    "> \n",
    "> llama3.1を使用していますが、次のエラーが出ます: ValueError: rope_scalingは二つのフィールド、typeとfactorを持つ辞書である必要があります。{'factor': 8.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}を取得しました。\n",
    "\n",
    "> ## zhudong1949\n",
    "> > transformersのバージョンを更新してください。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Aaryan Gupta\n",
    "> \n",
    "> こんにちは、このノートブックのデータセットはプライベートですか？コードがパスが見つからないエラーを出しています。\n",
    "\n",
    "---\n",
    "\n",
    "> ## 박민욱peterminpark\n",
    "> \n",
    "> 待ってください。\n",
    "> \n",
    "> モデル /kaggle/input/sft-llama3-lora-9231 は公開されていますか？\n",
    "> \n",
    "> また、トレーニングコードを使用してFold0-Fold4モデルを取得し、テストしましたが、0.94 - 0.95範囲で止まっています。\n",
    "\n",
    "> ## PaulRRR\n",
    "> > こんにちは、この推論コードを実行できますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## Lorry Zou\n",
    "> \n",
    "> Huggingfaceからリポジトリ全体をダウンロードし、推論用のデータセットを作成しましたが、読み込み時に次のエラーが出ました:\n",
    "> \n",
    "> Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index または flax_model.msgpackが、ディレクトリ/kaggle/input/llama-3-8b-instruct-bnb-4bit-1に見つかりませんでした。 \n",
    "> 誰か同じ問題に遭遇した人はいますか？解決方法を知っている人はいますか？\n",
    "\n",
    "> ## Lorry Zou\n",
    "> > 今、別のエラーが発生しました:\n",
    "> > 不正な path_or_model_id: '/kaggle/input/llama-3-8b-instruct-bnb-4bit'。ローカルフォルダへのパスまたはHub上のモデルのrepo_idを指定してください。 \n",
    "> > 誰か助けてください。\n",
    "\n",
    "---\n",
    "\n",
    "> ## OHIRA\n",
    "> \n",
    "> 素晴らしい作業に感謝します!!!\n",
    "> \n",
    "> もし8bit LoRAを使用する場合、時間制限内に推論できますか？\n",
    "> \n",
    "> どのくらいの時間ですか？\n",
    "> \n",
    "> 知っているなら教えてください！\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > 4ビットでも同様です。\n",
    "\n",
    "> ## hn\n",
    "> > こんにちは、gemma2-9bを使おうとしましたが、何故か8ビットを使用し、合計の長さが[256+640+640] = [1536]以上になると常に例外が発生します。以前に経験したことがありますか？4ビットに減らしたら、[256+512+512]で推論が実行されました。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Qihang Wang\n",
    "> \n",
    "> こんにちは、label_token_id = [128250] に設定した理由をお聞きしてもいいですか？\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > [128250]はllama3の特殊トークンで、意味はありません。ただラベル位置を取得し、他の通常トークンを避けます。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Dlond Mike\n",
    "> \n",
    "> こんにちは、この新しいノートブックの新しい点を簡単に説明してもらえますか？\n",
    "\n",
    "---\n",
    "\n",
    "> ## YEI0907\n",
    "> \n",
    "> 推論時間はどのくらいかかりましたか？自回帰アーキテクチャを使用しましたが、推論時間が大幅に増加しました。\n",
    "\n",
    "---\n",
    "\n",
    "> ## YEI0907\n",
    "> \n",
    "> 老哥，你的这个架构推理时间花了多久呀？我也用了自回归模型进行推理，但是推理时间比直接分类长了很多\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > max_len=2400 -> 6h -> 0.935\n",
    "> > max_len=1024 -> 3h -> 0.938\n",
    "\n",
    "---\n",
    "\n",
    "> ## Rabbit\n",
    "> \n",
    "> あなたの方法を試しましたが、cv0.927だけどlbは0.960です。\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > 悲しい、過剰適合かもしれませんか？\n",
    "\n",
    "> ## PaulRRR\n",
    "> > こんにちは、この推論コードを実行できますか？\n",
    "\n",
    "> ## Rabbit\n",
    "> > コードをファインチューニングしました。\n",
    "\n",
    "---\n",
    "\n",
    "> ## lllleeeo\n",
    "> \n",
    "> これは本当に素晴らしい作業です！分類ヘッドとのスコアを比較しましたか？\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > もちろん、この方法はほぼ同じパラメータでより良く機能します。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Octavio Grau\n",
    "> \n",
    "> このノートブックは私にエラーを出します（0.935バージョン）。誰かヒントがありますか？ [@shelterw](https://www.kaggle.com/shelterw) 公開してくれてありがとう！\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > どんなエラーですか？\n",
    "\n",
    "> ## Songling\n",
    "> > [@shelterw](https://www.kaggle.com/shelterw) \n",
    "> > 警告: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスか、特定のスキームが欠如しています。\n",
    "> > エラー: 要件bitsandbytesを満たすバージョンが見つかりません（バージョン: なし）\n",
    "> > エラー: 要件peftを満たすバージョンが見つかりません（バージョン: なし）\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > 申し訳ありません、更新しました。\n",
    "\n",
    "---\n",
    "\n",
    "> ## Nikhil Tumbde\n",
    "> \n",
    "> ノートブックをありがとう！私はLLMのunslothバージョンを実装しようとしていましたが、できませんでした。これは本当に助かります。 \n",
    "> スコア計算にどのくらい時間がかかりますか？\n",
    "\n",
    "> ## ShelterWTopic Author\n",
    "> > 約6時間です。\n",
    "> > max_len = 1024に設定すると、3時間で0.938のスコアになります。\n",
    "\n",
    "> ## Nikhil Tumbde\n",
    "> > ありがとう！\n",
    "> > 最後の質問をしてもいいですか（オープンエンドかもしれません、申し訳ありません）。私はNLP分野の初心者で、unslothからシーケンシャル分類への変換中に多くのエラーに直面しました。各関数の期待される出力と入力を知る方法はありますか？chatgptを試しましたが、あまり役立ちませんでした。例えば、\n",
    "> > Llama3ForSFTはCausalLMOutputWithPastを返す必要があります。\n",
    "> > loss=loss,\n",
    "> > logits=true_logits,\n",
    "> > のように。\n",
    "\n",
    "> > トークナイズの方は、次のように返す必要があります\n",
    "> > {\n",
    "> > \"input_ids\": input_ids,\n",
    "> > \"attention_mask\": attention_mask,\n",
    "> > \"labels\": labels\n",
    "> > }\n",
    "> > ご時間をいただきありがとうございます！"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5418635,
     "sourceId": 8995950,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5439897,
     "sourceId": 9026149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5439960,
     "sourceId": 9026233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5452248,
     "sourceId": 9043661,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.513308,
   "end_time": "2024-07-01T03:00:01.146998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-01T02:56:34.63369",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b341b06afc40599e50c9c1ce88be20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07273d2112d649ffbea2991a6a79df98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "142be9e5949c44fabd0370c6df1203d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d31276fcd44350a50d1c561c13e3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c",
       "placeholder": "​",
       "style": "IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1df41ee456cd4fa78a23f7ea2fded110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28cb0aaaf9d24d3d857d224850f62f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd",
       "value": 4
      }
     },
     "3974a6c7f61d4f4582a8e4fdf4c9976c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3daebd55184a4a9982813dc1ac948f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "411187a29c544ebbb425a06b0dfae7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "475dd481f05a46c1908b9f781ae1afa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075",
        "IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b",
        "IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"
       ],
       "layout": "IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"
      }
     },
     "74d46aef6d8949c584024a6e7bb4f06c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c5128ae1d114334b30f2e1013062b26",
       "placeholder": "​",
       "style": "IPY_MODEL_03b341b06afc40599e50c9c1ce88be20",
       "value": " 4/4 [01:30&lt;00:00, 18.30s/it]"
      }
     },
     "846776c5dc1f44bc9cf2e3d394e8ba48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e8e3620620b445eb1d0286befa13278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4",
        "IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe",
        "IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"
       ],
       "layout": "IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"
      }
     },
     "9c5128ae1d114334b30f2e1013062b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7c6588ad13549ae958237ca8e3af9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c23e125e5c3e4cee844bd057453c7aca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b",
       "placeholder": "​",
       "style": "IPY_MODEL_07273d2112d649ffbea2991a6a79df98",
       "value": " 4/4 [00:13&lt;00:00,  2.76s/it]"
      }
     },
     "cb2874dbe9904c07a254eb33d6f0ecfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0043cb27ac54061b85f9b3886954314",
       "value": 4
      }
     },
     "d0043cb27ac54061b85f9b3886954314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd4aa3639e4a46e7a3861fa3dbd5a31b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7361623d5e1491c888080ee4fb8bfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0cc9ad10c3d4a63bd03716995531022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f43e003b1f2e4367800ba2bad74c7075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db",
       "placeholder": "​",
       "style": "IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
