{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5e20744",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、KaggleのLMSYS - Chatbot Arenaコンペティションにおいて、大規模言語モデル（LLM）の応答の好ましさを予測するタスクに取り組んでいます。特に、LLMの応答がどちらのモデル（モデルAまたはモデルB）がより好まれるかを判定するための推論を行っています。\n",
    "\n",
    "### 主要な取り組みと手法\n",
    "\n",
    "1. **高速化された推論**:\n",
    "   - 元のスクリプトに対して38%の推論時間短縮を達成しました。具体的には、トレーニングセットの最初の10,000サンプルにおける推論時間を65分から40分に短縮しました。\n",
    "   - これを実現するために、二つの主要な技術を導入しました：\n",
    "     - **動的パディング**: 各ミニバッチの最長シーケンスに合わせてオンザフライでパディングを行う手法。\n",
    "     - **テストデータ長によるソート**: 入力の長さでソートすることで各ミニバッチ内のトークン数を揃え、余分なパディングを避けています。\n",
    "\n",
    "2. **長い入力シーケンスの扱い**: \n",
    "   - `max_length`を1024から1280に変更することで、モデルのパフォーマンスが向上しました。\n",
    "\n",
    "3. **使用ライブラリ**:\n",
    "   - **PyTorch**、**Transformers**、**Tokenizers**、および**PEFT**（Parameter-Efficient Fine-Tuning）などの機械学習用のライブラリを使用しています。\n",
    "   - 特に、Transformersライブラリを使ってLLMのトークナイゼーションやモデルの初期化を行っています。\n",
    "\n",
    "4. **推論プロセスの実装**:\n",
    "   - 複数のGPUを使用し、並列処理により推論を行うことで効率を向上させています。\n",
    "   - モデルからの出力を用いて、各モデルの勝利の確率を計算し、最終的な結果を出力します。\n",
    "\n",
    "5. **提出用ファイルの作成**:\n",
    "   - 出力された確率をもとに「submission.csv」という形式でCSVファイルを生成し、Kaggleに提出可能なフォーマットに適合させています。\n",
    "\n",
    "このノートブックは、効率的な推論手法を取り入れ、特にモデルのパフォーマンスを維持しつつ迅速に結果を生成することに焦点を当てています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369d8dcd",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、ノートブック特有のドメイン知識や実務で馴染みの少ない専門用語の簡単な解説です。\n",
    "\n",
    "### 動的パディング\n",
    "- **解説**: トレーニングデータ中の各ミニバッチにおいて、最長のシーケンスに合わせてその都度パディングを行う技術。これにより、固定長の入力に対する無駄な計算を減らし、推論時間を短縮できる。\n",
    "\n",
    "### テスト時間の拡張 (TTA)\n",
    "- **解説**: テスト時間の拡張（Test Time Augmentation）は、同じサンプルに対して複数の推論を行い、その結果を組み合わせて最終的な予測を行う手法。具体的には、応答の順序を変えたモデル呼び出しのことで、モデルの堅牢性を向上させるために行う。\n",
    "\n",
    "### ロジット\n",
    "- **解説**: モデルの出力層から得られる生のスコアで、通常は分類タスクにおいて各クラスに対する非正規化の確率を表す。スコアはソフトマックス関数を通して確率に変換される。\n",
    "\n",
    "### アテンションマスク\n",
    "- **解説**: トークンがどの部分で重要であるかを示す二値配列。通常、パディングされた部分に対して0、重要なトークンに対して1を設定して、モデルが無視すべき部分を明示する。\n",
    "\n",
    "### PEFT (Parameter-Efficient Fine-Tuning)\n",
    "- **解説**: より少ないパラメータでモデルを微調整する手法。これは、フルモデルを再訓練することなく任意のタスクに適応させたり、効率的にリソースを使用したりするために利用される。\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "- **解説**: モデルの低ランク表示を利用して、パラメータを効率的に調整する技術。これにより、少ないデータでモデルのパフォーマンスを向上させることが可能になる。\n",
    "\n",
    "### スパースドット製品 (SDP)\n",
    "- **解説**: スパース（まばら）な行列に対して効率的にドット積（内積）を計算するための手法。特に、メモリ使用量と計算コストを削減するために、特定の条件下での計算を最適化する。\n",
    "\n",
    "この解説が、初心者がつまずく可能性のある用語を理解する助けになれば幸いです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfbd9d",
   "metadata": {},
   "source": [
    "## 🦙🦙🦙 このノートブックについて\n",
    "このノートブックは、@kishanvavdaraによる [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) を元に作成されています。リンクされたノートブックをまだ確認していない方は、ぜひチェックして高評価をつけることをお勧めします。\n",
    "私は、@kishanvavdaraの作品に対していくつかの改善を加えました：\n",
    "\n",
    "### 38%高速化された推論\n",
    "トレーニングセットの最初の10,000サンプルを使用した推論時間は、このスクリプトを使うと40分かかります（TTAなし）。一方、元のスクリプトでは65分かかるため、精度に劣化がないまま38%高速化されました。私が主に追加したのは2つの機能です：\n",
    "\n",
    "#### 1. 動的パディング\n",
    "すべての入力を事前に固定長にパディングするのではなく、各ミニバッチの最長のシーケンスに合わせてオンザフライでパディングを適用します。\n",
    "\n",
    "#### 2. テストデータを入力の長さでソート\n",
    "動的パディングの利点を最大限に活用するために、テストデータは入力の長さでソートされます。こうすることで、各ミニバッチ内の入力がほぼ同じ長さになり、不要なパディングが減ります。\n",
    "\n",
    "### より長い入力シーケンス\n",
    "トレーニングデータの99%は1024トークン以内に収まっていますが、残りの1%はそれを超えています。また、テストセットにはさらに長いシーケンスが存在する可能性があるため、`max_length`をできるだけ長く設定する方が安全だと考えました。\n",
    "`max_length`を1024から1280に変更すると、LBは0.989から0.983に改善されました。\n",
    "\n",
    "## 試したが効果がなかったこと\n",
    "\n",
    "### テスト時間の拡張 (TTA)\n",
    "私は、response_aとresponse_bの順序を入れ替える簡単なTTAを試みました。この方法では、サンプルごとにモデルが2回呼び出されるため、推論時間が2倍に増加することに注意してください。\n",
    "2つのソフトマックス確率を平均化するか、2つのロジットを平均してからソフトマックス確率を計算することができます。どちらのアプローチもLBを改善しませんでしたが、ソフトマックスを平均化する方がパフォーマンスが良かったです。\n",
    "TTAは推論時間を2倍に増加させるため、サンプルごとにモデルを2回呼び出します。効率的な推論のおかげで、`max_length=1280`でTTAを有効にした状態でも、9時間以内に提出が完了しました。\n",
    "\n",
    "### 各入力の切り捨て\n",
    "元の実装では、プロンプト + response_a + response_bとして連結されたシーケンスを切り捨てています。切り捨てを単純に適用すると、一部の（稀ではありますが）プロンプトが1280トークンを超えるため、プロンプトのみの入力が生成され、モデルは勝者をランダムに推測するしかなくなります。\n",
    "私は各入力を固定長にまず切り捨て、その後3つを連結する方法を試しましたが、LBは改善されませんでした。\n",
    "\n",
    "# ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-03T04:41:47.903328Z",
     "iopub.status.busy": "2024-07-03T04:41:47.90305Z",
     "iopub.status.idle": "2024-07-03T04:42:39.631135Z",
     "shell.execute_reply": "2024-07-03T04:42:39.629914Z",
     "shell.execute_reply.started": "2024-07-03T04:41:47.903302Z"
    },
    "papermill": {
     "duration": 53.686843,
     "end_time": "2024-07-01T02:57:31.530755",
     "exception": false,
     "start_time": "2024-07-01T02:56:37.843912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bitsandbytesライブラリを最新バージョンにインストールします。 \n",
    "# -qオプションは出力を抑制し、--no-indexオプションはPyPIインデックスを無視してローカルのリンクを使用します。\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "\n",
    "# transformersライブラリを最新バージョンにインストールします。\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "\n",
    "# tokenizersライブラリを最新バージョンにインストールします。\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "\n",
    "# peftライブラリを最新バージョンにインストールします。\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:11.621254Z",
     "iopub.status.busy": "2024-07-02T03:14:11.62089Z",
     "iopub.status.idle": "2024-07-02T03:14:30.392808Z",
     "shell.execute_reply": "2024-07-02T03:14:30.391997Z",
     "shell.execute_reply.started": "2024-07-02T03:14:11.621219Z"
    },
    "papermill": {
     "duration": 21.138547,
     "end_time": "2024-07-01T02:57:52.676238",
     "exception": false,
     "start_time": "2024-07-01T02:57:31.537691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 必要なライブラリをインポートします\n",
    "\n",
    "import time  # 時間計測のためのライブラリ\n",
    "from dataclasses import dataclass  # データクラスを使用するためのライブラリ\n",
    "from concurrent.futures import ThreadPoolExecutor  # スレッドプールを使用して並行処理を行うためのライブラリ\n",
    "\n",
    "import torch  # PyTorchライブラリ\n",
    "import sklearn  # 機械学習ライブラリ\n",
    "import numpy as np  # 数値計算ライブラリ\n",
    "import pandas as pd  # データ操作ライブラリ\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig  # Hugging FaceのTransformersライブラリから必要なクラスをインポート\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning  # パディングを行う関数をインポート\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # PEFT関連のクラスや関数をインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:30.394366Z",
     "iopub.status.busy": "2024-07-02T03:14:30.393853Z",
     "iopub.status.idle": "2024-07-02T03:14:30.401305Z",
     "shell.execute_reply": "2024-07-02T03:14:30.400241Z",
     "shell.execute_reply.started": "2024-07-02T03:14:30.39434Z"
    },
    "papermill": {
     "duration": 0.014661,
     "end_time": "2024-07-01T02:57:52.698559",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.683898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用可能なGPUの数が2つであることを確認します。\n",
    "# 要求された条件を満たさない場合、エラーメッセージが表示されます。\n",
    "assert torch.cuda.device_count() == 2, \"申し訳ありませんが、マルチGPUが必要です！\"\n",
    "\n",
    "# CUDAのメモリ効率的なスパースドット製品（SDP）を無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "\n",
    "# CUDAのフラッシュスパースドット製品（SDP）を無効にします。\n",
    "torch.backends.cuda.enable_flash_sdp(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:30.403467Z",
     "iopub.status.busy": "2024-07-02T03:14:30.403058Z",
     "iopub.status.idle": "2024-07-02T03:14:30.429871Z",
     "shell.execute_reply": "2024-07-02T03:14:30.429026Z",
     "shell.execute_reply.started": "2024-07-02T03:14:30.403417Z"
    },
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-07-01T02:57:52.720706",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.705889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # 使用するモデルの名前（またはファイルパス）を指定します。\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    \n",
    "    # モデルの重みの保存場所を指定します。\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    \n",
    "    # 最大入力シーケンスの長さを指定します。\n",
    "    max_length = 1280\n",
    "    \n",
    "    # バッチサイズを指定します。1回の処理で扱うサンプルの数です。\n",
    "    batch_size = 8\n",
    "    \n",
    "    # 使用するデバイスを指定します。ここではCUDA（GPU）を指定しています。\n",
    "    device = torch.device(\"cuda\")    \n",
    "    \n",
    "    # テスト時間の拡張（TTA）を使用するかどうかを指定します。\n",
    "    # <prompt>-<model-bの応答>-<model-aの応答>の形式で使用されます。\n",
    "    tta = False  \n",
    "    \n",
    "    # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するかを指定します。\n",
    "    spread_max_length = False  \n",
    "\n",
    "# Configクラスのインスタンスを作成します。\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d13432",
   "metadata": {},
   "source": [
    "# データの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:30.432769Z",
     "iopub.status.busy": "2024-07-02T03:14:30.432475Z",
     "iopub.status.idle": "2024-07-02T03:14:30.471168Z",
     "shell.execute_reply": "2024-07-02T03:14:30.470159Z",
     "shell.execute_reply.started": "2024-07-02T03:14:30.432745Z"
    },
    "papermill": {
     "duration": 0.040947,
     "end_time": "2024-07-01T02:57:52.781962",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.741015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# テストデータをCSVファイルから読み込みます。\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# 文字列のリストを連結する関数を定義します。\n",
    "def process(input_str):\n",
    "    # 引数として渡された文字列の両端のブラケットを取り除きます。\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # 各文を取り出し、ダブルクオーテーションを取り除いてリストを作成します。\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # リスト内の文を空白で結合して1つの文字列にします。\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "# 'prompt'カラム、'response_a'カラム、'response_b'カラムに対してprocess関数を適用します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# 処理したデータを表示します。最初の5行を表示します。\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9165e",
   "metadata": {},
   "source": [
    "# トークナイズ（単語分割）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:30.472869Z",
     "iopub.status.busy": "2024-07-02T03:14:30.472522Z",
     "iopub.status.idle": "2024-07-02T03:14:30.483247Z",
     "shell.execute_reply": "2024-07-02T03:14:30.482262Z",
     "shell.execute_reply.started": "2024-07-02T03:14:30.472842Z"
    },
    "papermill": {
     "duration": 0.017982,
     "end_time": "2024-07-01T02:57:52.821282",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.8033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# トークナイザーを使用してテキストをトークンに変換する関数を定義します。\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    # ユーザーのプロンプトに接頭辞を追加します。\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]\n",
    "    # モデルAの応答に接頭辞を追加します。\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "    # モデルBの応答に接頭辞を追加します。\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    \n",
    "    if spread_max_length:\n",
    "        # max_lengthを3で割った値を使用して個々のトークンの長さを設定します。\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        \n",
    "        # プロンプトと応答のトークンを結合してinput_idsを作成します。\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        # attention_maskを作成します。各トークンの有効性を示します。\n",
    "        attention_mask = [[1] * len(i) for i in input_ids]\n",
    "    else:\n",
    "        # プロンプトと応答を結合して1つのテキストリストを作成します。\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        # 一括でトークナイザーを使ってトークン化します。\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        # input_idsとattention_maskを取得します。\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    \n",
    "    return input_ids, attention_mask  # トークンIDとアテンションマスクを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:30.485344Z",
     "iopub.status.busy": "2024-07-02T03:14:30.484646Z",
     "iopub.status.idle": "2024-07-02T03:14:31.052319Z",
     "shell.execute_reply": "2024-07-02T03:14:31.051301Z",
     "shell.execute_reply.started": "2024-07-02T03:14:30.485309Z"
    },
    "papermill": {
     "duration": 0.596117,
     "end_time": "2024-07-01T02:57:53.425111",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.828994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time  # このセルの実行時間を計測します。\n",
    "\n",
    "# トークナイザーを事前学習済みモデルからロードします。\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# データフレームを初期化します。\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]  # テストデータのIDを追加します。\n",
    "# トークナイズした情報をdataフレームに追加します。\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # 各入力の長さを計算します。\n",
    "\n",
    "# 拡張データフレームを初期化します。\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]  # テストデータのIDを追加します。\n",
    "# response_aとresponse_bを入れ替えたトークナイズを行います。\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # 各入力の長さを計算します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:31.053896Z",
     "iopub.status.busy": "2024-07-02T03:14:31.053586Z",
     "iopub.status.idle": "2024-07-02T03:14:31.060126Z",
     "shell.execute_reply": "2024-07-02T03:14:31.059158Z",
     "shell.execute_reply.started": "2024-07-02T03:14:31.053869Z"
    },
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-07-01T02:57:53.448552",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.432565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# トークンIDからデコードして、最初の入力を人間が読める形式に変換します。\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:31.061655Z",
     "iopub.status.busy": "2024-07-02T03:14:31.061348Z",
     "iopub.status.idle": "2024-07-02T03:14:31.069545Z",
     "shell.execute_reply": "2024-07-02T03:14:31.068581Z",
     "shell.execute_reply.started": "2024-07-02T03:14:31.061631Z"
    },
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-07-01T02:57:53.472007",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.456043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 拡張データフレームにおける最初の入力をトークンIDからデコードし、人間が読める形式に変換します。\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c6475b",
   "metadata": {},
   "source": [
    "# モデルの読み込み\n",
    "各GPUに1つのモデルを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:14:31.071051Z",
     "iopub.status.busy": "2024-07-02T03:14:31.070767Z",
     "iopub.status.idle": "2024-07-02T03:16:20.333489Z",
     "shell.execute_reply": "2024-07-02T03:16:20.332491Z",
     "shell.execute_reply.started": "2024-07-02T03:14:31.071027Z"
    },
    "papermill": {
     "duration": 105.076557,
     "end_time": "2024-07-01T02:59:38.570536",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.493979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesの設定を行います。\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットでモデルをロードします。\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 計算時のデータ型をfloat16に設定します。\n",
    "    bnb_8bit_use_double_quant=False,  # ダブル量子化を使用しない設定です。\n",
    ")\n",
    "\n",
    "# GPU 0にベースモデルをロードします。\n",
    "device_0 = torch.device('cuda:0')  # 使用するデバイスを指定します。\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,  # モデルの名前を指定します。\n",
    "    num_labels=3,  # 出力ラベルの数を指定します。\n",
    "    torch_dtype=torch.float16,  # トークンのデータ型をfloat16に設定します。\n",
    "    quantization_config=bnb_config,  # 量子化構成を設定します。\n",
    "    device_map='cuda:0'  # モデルをCUDAデバイス0にマッピングします。\n",
    ")\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定します。\n",
    "\n",
    "# GPU 1にベースモデルをロードします。\n",
    "device_1 = torch.device('cuda:1')  # 使用するデバイスを指定します。\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,  # モデルの名前を指定します。\n",
    "    num_labels=3,  # 出力ラベルの数を指定します。\n",
    "    torch_dtype=torch.float16,  # トークンのデータ型をfloat16に設定します。\n",
    "    quantization_config=bnb_config,  # 量子化構成を設定します。\n",
    "    device_map='cuda:1'  # モデルをCUDAデバイス1にマッピングします。\n",
    ")\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a505174",
   "metadata": {},
   "source": [
    "# 重みのロード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:20.335814Z",
     "iopub.status.busy": "2024-07-02T03:16:20.334929Z",
     "iopub.status.idle": "2024-07-02T03:16:20.342069Z",
     "shell.execute_reply": "2024-07-02T03:16:20.340677Z",
     "shell.execute_reply.started": "2024-07-02T03:16:20.335778Z"
    },
    "papermill": {
     "duration": 0.0162,
     "end_time": "2024-07-01T02:59:38.610906",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.594706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRAの設定を行います。\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # LoRAのランクを指定します。\n",
    "    lora_alpha=32,  # LoRAのアルファ値を設定します。\n",
    "    lora_dropout=0.10,  # LoRAに使用するドロップアウト率を設定します。\n",
    "    bias='none',  # バイアスの設定を指定します。\n",
    "    inference_mode=True,  # 推論モードを有効にします。\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクのタイプをシーケンス分類に設定します。\n",
    "    target_modules=['o_proj', 'v_proj']  # 対象モジュールを指定します。\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:20.344016Z",
     "iopub.status.busy": "2024-07-02T03:16:20.343599Z",
     "iopub.status.idle": "2024-07-02T03:16:34.706184Z",
     "shell.execute_reply": "2024-07-02T03:16:34.705179Z",
     "shell.execute_reply.started": "2024-07-02T03:16:20.343979Z"
    },
    "papermill": {
     "duration": 13.701042,
     "end_time": "2024-07-01T02:59:52.320278",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.619236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PEFTモデルを取得します。\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # base_model_0にPEFT設定を適用し、デバイス0に移動します。\n",
    "# 重みをロードします。\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを指定されたパスからロードします。\n",
    "model_0.eval()  # モデルを評価モードに設定します。\n",
    "\n",
    "# PEFTモデルを取得します。\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # base_model_1にPEFT設定を適用し、デバイス1に移動します。\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを指定されたパスからロードします。\n",
    "model_1.eval()  # モデルを評価モードに設定します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:34.70787Z",
     "iopub.status.busy": "2024-07-02T03:16:34.707509Z",
     "iopub.status.idle": "2024-07-02T03:16:34.721779Z",
     "shell.execute_reply": "2024-07-02T03:16:34.720734Z",
     "shell.execute_reply.started": "2024-07-02T03:16:34.707843Z"
    },
    "papermill": {
     "duration": 0.026729,
     "end_time": "2024-07-01T02:59:52.356012",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.329283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# モデルの学習可能なパラメータを表示します。\n",
    "model_0.print_trainable_parameters()  # model_0の学習可能なパラメータを表示します。\n",
    "model_1.print_trainable_parameters()  # model_1の学習可能なパラメータを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee13126a",
   "metadata": {},
   "source": [
    "# 推論"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:34.725899Z",
     "iopub.status.busy": "2024-07-02T03:16:34.725601Z",
     "iopub.status.idle": "2024-07-02T03:16:34.735894Z",
     "shell.execute_reply": "2024-07-02T03:16:34.734863Z",
     "shell.execute_reply.started": "2024-07-02T03:16:34.725874Z"
    },
    "papermill": {
     "duration": 0.021078,
     "end_time": "2024-07-01T02:59:52.402973",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.381895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 推論を行う関数を定義します。\n",
    "@torch.no_grad()  # 勾配計算を無効にします。推論時は必要ないため効率的です。\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を使用して計算を高速化します。\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝者の確率を格納するリストを初期化します。\n",
    "    \n",
    "    # データフレームをバッチサイズごとに処理します。\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを計算します。\n",
    "        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチのデータを取得します。\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストに変換します。\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストに変換します。\n",
    "        \n",
    "        # パディングを行い、モデルの入力形式に変換します。\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=True,\n",
    "            max_length=max_length,\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchテンソルとして返します。\n",
    "        )\n",
    "        \n",
    "        # モデルを使って推論を行います。\n",
    "        outputs = model(**inputs.to(device))  # デバイスに入力を移動させてモデルに渡します。\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットにソフトマックスを適用し、確率を計算します。\n",
    "\n",
    "        # 各モデルの勝者の確率をリストに追加します。\n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝者の確率を追加します。\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝者の確率を追加します。\n",
    "        tie.extend(proba[:, 2].tolist())    # 引き分けの確率を追加します。\n",
    "    \n",
    "    # データフレームに勝者の情報を追加します。\n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df  # 更新されたデータフレームを返します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:34.737581Z",
     "iopub.status.busy": "2024-07-02T03:16:34.737213Z",
     "iopub.status.idle": "2024-07-02T03:16:38.026999Z",
     "shell.execute_reply": "2024-07-02T03:16:38.02593Z",
     "shell.execute_reply.started": "2024-07-02T03:16:34.737553Z"
    },
    "papermill": {
     "duration": 3.316613,
     "end_time": "2024-07-01T02:59:55.727834",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.411221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 時間計測を開始します。\n",
    "st = time.time()\n",
    "\n",
    "# 動的パディングを最大限活用するために入力の長さでソートします。\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# サブセット1とサブセット2ではトークンの総数がほぼ同じである必要があります。\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピーします。\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピーします。\n",
    "\n",
    "# スレッドプールエグゼキュータを使用して並行処理を行います。\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    # inference関数をサブセット1とサブセット2に対してモデル0とモデル1で実行します。\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "# 結果をデータフレームとして結合します。\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝者の確率を抽出します。\n",
    "\n",
    "# 経過時間を表示します。\n",
    "print(f\"経過時間: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:38.028866Z",
     "iopub.status.busy": "2024-07-02T03:16:38.028453Z",
     "iopub.status.idle": "2024-07-02T03:16:39.822255Z",
     "shell.execute_reply": "2024-07-02T03:16:39.821134Z",
     "shell.execute_reply.started": "2024-07-02T03:16:38.028828Z"
    },
    "papermill": {
     "duration": 1.755381,
     "end_time": "2024-07-01T02:59:57.492377",
     "exception": false,
     "start_time": "2024-07-01T02:59:55.736996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 時間計測を開始します。\n",
    "st = time.time()\n",
    "\n",
    "# テスト時間の拡張（TTA）が有効な場合の処理です。\n",
    "if cfg.tta:\n",
    "    # 入力の長さでソートし、処理速度を向上させます。\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)\n",
    "    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピーします。\n",
    "    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピーします。\n",
    "\n",
    "    # スレッドプールエグゼキュータを使用して並行処理を行います。\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        # inference関数をサブセット1とサブセット2に対してモデル0とモデル1で実行します。\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    # 結果をデータフレームとして結合します。\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # TTAの順序が反転していることを考慮します。\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  \n",
    "    # 元の結果とTTA結果を平均化します。\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "# 経過時間を表示します。\n",
    "print(f\"経過時間: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T03:16:39.823929Z",
     "iopub.status.busy": "2024-07-02T03:16:39.823604Z",
     "iopub.status.idle": "2024-07-02T03:16:39.842742Z",
     "shell.execute_reply": "2024-07-02T03:16:39.841784Z",
     "shell.execute_reply.started": "2024-07-02T03:16:39.823902Z"
    },
    "papermill": {
     "duration": 0.03061,
     "end_time": "2024-07-01T02:59:57.532498",
     "exception": false,
     "start_time": "2024-07-01T02:59:57.501888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 結果データフレームに、勝者の確率を更新します。\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝者の確率を追加します。\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝者の確率を追加します。\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]      # 引き分けの確率を追加します。\n",
    "\n",
    "# 提出用データフレームを作成します。\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  \n",
    "# 提出用データフレームをCSVファイルとして保存します。\n",
    "submission_df.to_csv('submission.csv', index=False)  \n",
    "# 提出用データフレームを表示します。\n",
    "display(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.008889,
     "end_time": "2024-07-01T02:59:57.551154",
     "exception": false,
     "start_time": "2024-07-01T02:59:57.542265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.513308,
   "end_time": "2024-07-01T03:00:01.146998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-01T02:56:34.63369",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b341b06afc40599e50c9c1ce88be20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07273d2112d649ffbea2991a6a79df98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "142be9e5949c44fabd0370c6df1203d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d31276fcd44350a50d1c561c13e3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c",
       "placeholder": "​",
       "style": "IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1df41ee456cd4fa78a23f7ea2fded110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28cb0aaaf9d24d3d857d224850f62f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd",
       "value": 4
      }
     },
     "3974a6c7f61d4f4582a8e4fdf4c9976c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3daebd55184a4a9982813dc1ac948f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "411187a29c544ebbb425a06b0dfae7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "475dd481f05a46c1908b9f781ae1afa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075",
        "IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b",
        "IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"
       ],
       "layout": "IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"
      }
     },
     "74d46aef6d8949c584024a6e7bb4f06c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c5128ae1d114334b30f2e1013062b26",
       "placeholder": "​",
       "style": "IPY_MODEL_03b341b06afc40599e50c9c1ce88be20",
       "value": " 4/4 [01:30&lt;00:00, 18.30s/it]"
      }
     },
     "846776c5dc1f44bc9cf2e3d394e8ba48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e8e3620620b445eb1d0286befa13278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4",
        "IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe",
        "IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"
       ],
       "layout": "IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"
      }
     },
     "9c5128ae1d114334b30f2e1013062b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7c6588ad13549ae958237ca8e3af9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c23e125e5c3e4cee844bd057453c7aca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b",
       "placeholder": "​",
       "style": "IPY_MODEL_07273d2112d649ffbea2991a6a79df98",
       "value": " 4/4 [00:13&lt;00:00,  2.76s/it]"
      }
     },
     "cb2874dbe9904c07a254eb33d6f0ecfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0043cb27ac54061b85f9b3886954314",
       "value": 4
      }
     },
     "d0043cb27ac54061b85f9b3886954314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd4aa3639e4a46e7a3861fa3dbd5a31b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7361623d5e1491c888080ee4fb8bfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0cc9ad10c3d4a63bd03716995531022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f43e003b1f2e4367800ba2bad74c7075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db",
       "placeholder": "​",
       "style": "IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
