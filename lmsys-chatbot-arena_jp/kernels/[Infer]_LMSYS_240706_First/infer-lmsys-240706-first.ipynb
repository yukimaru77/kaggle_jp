{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e632c6",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸLlama-3 8bãƒ¢ãƒ‡ãƒ«ã‚’åˆ©ç”¨ã—ã¦ã€å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆï¼ˆ25,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã«å¯¾ã™ã‚‹æ¨è«–ã‚’è¡Œã†ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ä½¿ç”¨ã™ã‚‹ç’°å¢ƒã¯ã€T4 GPUã‚’2å°ä½¿ç”¨ã—ã¦ãŠã‚Šã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦ã§ã™ã€‚\n",
    "\n",
    "### å–ã‚Šçµ„ã‚€å•é¡Œ\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ç‰¹ã«å¤§è¦æ¨¡ãªãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã—ã¦å„å¿œç­”ãƒ¢ãƒ‡ãƒ«ã®å¥½ã¿ã‚’äºˆæ¸¬ã—ã€ã©ã®ãƒ¢ãƒ‡ãƒ«ãŒé¸ã°ã‚Œã‚‹ã‹ã‚’åˆ¤æ–­ã™ã‚‹ã“ã¨ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã—ã¦ãƒ¢ãƒ‡ãƒ«Aã¨ãƒ¢ãƒ‡ãƒ«Bã®å¿œç­”ã‚’æ¯”è¼ƒã—ã€ãã®é¸å¥½ã‚’äºˆæ¸¬ã—ã¾ã™ã€‚\n",
    "\n",
    "### æ‰‹æ³•\n",
    "1. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**:\n",
    "   - `torch`, `sklearn`, `numpy`, `pandas`ãªã©ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã€‚\n",
    "   - Hugging Faceã®`transformers`ãŠã‚ˆã³`peft`ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®Llamaãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ã¨æ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "2. **ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™**:\n",
    "   - ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰å¿…è¦ãªã‚«ãƒ©ãƒ ã‚’èª­ã¿è¾¼ã¿ã€æ–‡å­—åˆ—ã‚’å‡¦ç†ã—ã¦é©åˆ‡ãªå½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "3. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º**:\n",
    "   - `AutoTokenizer`ã‚’ç”¨ã„ã¦å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€PyTorchãƒ†ãƒ³ã‚½ãƒ«å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "4. **ãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰**:\n",
    "   - 2å°ã®GPUãã‚Œãã‚Œã«ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’é…ç½®ã—ã€`AutoModelForSequenceClassification`ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "\n",
    "5. **LoRaè¨­å®š**:\n",
    "   - æ¸›å°‘ã•ã‚ŒãŸãƒ¡ãƒ¢ãƒªãƒ•ãƒƒãƒˆãƒ—ãƒªãƒ³ãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†ãŸã‚ã«ã€LoRaï¼ˆLow-Rank Adaptationï¼‰ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "\n",
    "6. **æ¨è«–**:\n",
    "   - ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒãƒƒãƒå‡¦ç†ã—ãªãŒã‚‰ã€ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹æ¨è«–ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ãã—ã¦ã€ãƒ¢ãƒ‡ãƒ«Aã‚„Bã€å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "\n",
    "7. **ãƒãƒ«ãƒã‚¹ãƒ¬ãƒƒãƒ‰æ¨è«–**:\n",
    "   - çµæœã‚’ä¸¦è¡Œã—ã¦è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€Pythonã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã€2ã¤ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’å„ãƒ¢ãƒ‡ãƒ«ã§å‡¦ç†ã—ã¾ã™ã€‚\n",
    "\n",
    "8. **çµæœã®çµ±åˆ**:\n",
    "   - æœ€çµ‚çš„ã«ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’çµ±åˆã—ã€æå‡ºç”¨ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "æ¨è«–ã¯ç´„4.5æ™‚é–“ã§å®Œäº†ã—ã¾ã™ãŒã€ã•ã‚‰ãªã‚‹æ”¹å–„ã®ä½™åœ°ãŒã‚ã‚‹ãŸã‚ã€ç•°ãªã‚‹å¾Œå‡¦ç†æ–¹æ³•ã®æ¤œè¨ãŒææ¡ˆã•ã‚Œã¦ã„ã¾ã™ã€‚å…¨ä½“ã¨ã—ã¦ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸãƒ¦ãƒ¼ã‚¶ãƒ¼é¸å¥½äºˆæ¸¬ã®å®Ÿè£…ä¾‹ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32235089",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã¯ã€Jupyter Notebookã®å†…å®¹ã«åŸºã¥ã„ã¦ã€åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªå°‚é–€ç”¨èªã®è§£èª¬ã§ã™ã€‚\n",
    "\n",
    "1. **llama-3**:\n",
    "   - OpenAIãŒé–‹ç™ºã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ä¸€ç¨®ã§ã€ç‰¹ã«ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ï¼ˆã“ã®å ´åˆã¯ä¸€å¯¾ã®å¿œç­”ã®æ¯”è¼ƒï¼‰ã«å¯¾ã™ã‚‹æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚\n",
    "\n",
    "2. **ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°**:\n",
    "   - æ—¢å­˜ã®ãƒ¢ãƒ‡ãƒ«ã«è¿½åŠ ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã£ã¦ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å‘ä¸Šã•ã›ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–ã™ã‚‹ã“ã¨ã«ã‚ˆã£ã¦ã€ç‰¹å®šã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã™ã‚‹é©å¿œæ€§ã‚’æŒãŸã›ã‚‹ã€‚\n",
    "\n",
    "3. **AutoTokenizer**:\n",
    "   - Hugging Faceã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ä¸€éƒ¨ã§ã€è‡ªå‹•çš„ã«ãƒ¢ãƒ‡ãƒ«ã«é©ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’å–å¾—ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã€‚ã“ã®ã‚¯ãƒ©ã‚¹ã¯ã€å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ï¼ˆæ•°å€¤ã®é…åˆ—ï¼‰ã«å¤‰æ›ã™ã‚‹æ©Ÿèƒ½ã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "4. **BitsAndBytesConfig**:\n",
    "   - è¨ˆç®—é‡ã‚’å‰Šæ¸›ã—ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’åŠ¹ç‡åŒ–ã™ã‚‹ãŸã‚ã®è¨­å®šã‚¯ãƒ©ã‚¹ã€‚ç‰¹ã«ã€å¤§è¦æ¨¡ãªãƒ¢ãƒ‡ãƒ«ã‚’è»½é‡åŒ–ã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã®ä¸€ã¤ï¼ˆä¾‹ï¼šé‡å­åŒ–ï¼‰ã‚’å®Ÿç¾ã™ã‚‹ãŸã‚ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "5. **é‡å­åŒ–**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½ç²¾åº¦ã®å½¢å¼ï¼ˆä¾‹ãˆã°ã€8ãƒ“ãƒƒãƒˆã¾ãŸã¯4ãƒ“ãƒƒãƒˆï¼‰ã§è¡¨ç¾ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¡ãƒ¢ãƒª footprint ã‚’å‰Šæ¸›ã—ã¦æ¨è«–é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "6. **LoRa (Low-Rank Adaptation)**:\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®åŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã®ä¸€ã¤ã§ã™ã€‚ç‰¹ã«ã€å…¨çµåˆå±¤ã®é‡ã¿ã‚’ä½ãƒ©ãƒ³ã‚¯ã«è¿‘ä¼¼ã™ã‚‹ã“ã¨ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’æ¸›ã‚‰ã—ã¾ã™ã€‚\n",
    "\n",
    "7. **LoraConfig**:\n",
    "   - LoRaã®è¨­å®šã‚’å®šç¾©ã™ã‚‹ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã§ã€ç‰¹å®šã®ãƒ­ãƒ¼ãƒ¬ãƒ™ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆä¾‹ãˆã°ã€ãƒ©ãƒ³ã‚¯ã‚„ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡ï¼‰ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n",
    "\n",
    "8. **PeftModel**:\n",
    "   - Parameter Efficient Fine-Tuningã®ç•¥ã§ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã†éš›ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’åŠ¹ç‡è‰¯ãç®¡ç†ã™ã‚‹ãŸã‚ã®ãƒ¢ãƒ‡ãƒ«ã‚¯ãƒ©ã‚¹ã€‚\n",
    "\n",
    "9. **autocast**:\n",
    "   - PyTorchã®æ©Ÿèƒ½ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã™ã‚‹éš›ã«ã€è‡ªå‹•çš„ã«æ··åˆç²¾åº¦è¨ˆç®—ã‚’è¡Œã†ã“ã¨ã§æ€§èƒ½ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€é€Ÿåº¦ã¨ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒæ”¹å–„ã•ã‚Œã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "10. **ã‚¹ãƒ¬ãƒƒãƒ‰**:\n",
    "    - ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã®å®Ÿè¡Œå˜ä½ã§ã‚ã‚Šã€è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’ç”¨ã„ã¦åŒæ™‚ã«å‡¦ç†ã‚’è¡Œã†ã“ã¨ã§ã€è¨ˆç®—ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è¤‡æ•°ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã§ä¸¦è¡Œã—ã¦æ‰±ã£ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "11. **attention masks**:\n",
    "    - ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸå…¥åŠ›ãŒãƒ¢ãƒ‡ãƒ«ã«ä¸ãˆã‚‰ã‚Œã‚‹éš›ã€ã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ãŒå®Ÿéš›ã®å…¥åŠ›ã«è©²å½“ã™ã‚‹ã®ã‹ã‚’ç¤ºã™ãƒã‚¹ã‚¯ã§ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç„¡è¦–ã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã¾ã™ã€‚\n",
    "\n",
    "12. **ãƒãƒƒãƒã‚µã‚¤ã‚º (BATCH_SIZE)**:\n",
    "    - æ¨è«–ã‚„ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®éš›ã«ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«æ•°ã®ã“ã¨ã€‚å¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚ºã¯è¨ˆç®—åŠ¹ç‡ã‚’é«˜ã‚ã¾ã™ãŒã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¢—ãˆã‚‹ãŸã‚ã€é©åˆ‡ãªã‚µã‚¤ã‚ºè¨­å®šãŒé‡è¦ã§ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªã«é–¢ã™ã‚‹ç†è§£ãŒæ·±ã¾ã‚‹ã“ã¨ã§ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯å†…ã§ã®ã‚³ãƒ¼ãƒ‰ã®æ„å‘³ãŒã‚ˆã‚Šæ˜ç¢ºã«ãªã‚Šã€å…¨ä½“ã®æµã‚Œã‚’ç†è§£ã™ã‚‹æ‰‹åŠ©ã‘ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534231d0",
   "metadata": {},
   "source": [
    "# æ¨è«– - llama-3 8b è¶…é«˜é€Ÿ ğŸš€\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€T4 GPUã‚’2å°ä½¿ã£ã¦ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ã®llama-3 8bãƒ¢ãƒ‡ãƒ«ã‚’ç”¨ã„ãŸæ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½œæˆã—ãŸç†ç”±ã¯ã€ãƒ†ã‚¹ãƒˆã‚µã‚¤ã‚ºãŒéå¸¸ã«å¤§ãã„ï¼ˆ25,000ã‚µãƒ³ãƒ—ãƒ«ï¼‰ã‹ã‚‰ã§ã™ã€‚\n",
    "\n",
    "å‰ææ¡ä»¶: Llama-3ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹ãŒå¿…è¦ã§ã™ã€‚ã‚‚ã—åŒæ„æ›¸ã«è¨˜å…¥ã—ã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](https://www.kaggle.com/models/metaresearch/llama-3)ã«è¡Œãã€åŒæ„æ›¸ã«è¨˜å…¥ã—ã¦Llama-3ã«ã‚¢ã‚¯ã‚»ã‚¹ã—ã¦ãã ã•ã„ã€‚\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯[ã“ã¡ã‚‰](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)ã«ã‚ã‚Šã¾ã™ã€‚\n",
    "\n",
    "å½¹ç«‹ã¤ã¨æ„Ÿã˜ãŸã‚‰ã€ãœã²ã‚¢ãƒƒãƒ—ãƒœãƒ¼ãƒˆã—ã¦ãã ã•ã„ï¼\n",
    "\n",
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:02.957881Z",
     "iopub.status.busy": "2024-07-08T02:27:02.957535Z",
     "iopub.status.idle": "2024-07-08T02:27:55.475584Z",
     "shell.execute_reply": "2024-07-08T02:27:55.474311Z",
     "shell.execute_reply.started": "2024-07-08T02:27:02.957851Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:55.477832Z",
     "iopub.status.busy": "2024-07-08T02:27:55.477514Z",
     "iopub.status.idle": "2024-07-08T02:28:01.180228Z",
     "shell.execute_reply": "2024-07-08T02:28:01.179278Z",
     "shell.execute_reply.started": "2024-07-08T02:27:55.477802Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€GPUãŒå¿…è¦ã§ã™ï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.181789Z",
     "iopub.status.busy": "2024-07-08T02:28:01.181357Z",
     "iopub.status.idle": "2024-07-08T02:28:01.186354Z",
     "shell.execute_reply": "2024-07-08T02:28:01.185425Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.181764Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aed9ee",
   "metadata": {},
   "source": [
    "# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.189204Z",
     "iopub.status.busy": "2024-07-08T02:28:01.188861Z",
     "iopub.status.idle": "2024-07-08T02:28:01.23147Z",
     "shell.execute_reply": "2024-07-08T02:28:01.230662Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.189175Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# ãƒªã‚¹ãƒˆå†…ã®æ–‡å­—åˆ—ã‚’é€£çµã™ã‚‹é–¢æ•°\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å„åˆ—ã«å¯¾ã—ã¦processé–¢æ•°ã‚’é©ç”¨\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.232988Z",
     "iopub.status.busy": "2024-07-08T02:28:01.232654Z",
     "iopub.status.idle": "2024-07-08T02:28:01.240511Z",
     "shell.execute_reply": "2024-07-08T02:28:01.239621Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.232956Z"
    }
   },
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’æº–å‚™\n",
    "test['text'] = 'ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ: ' + test['prompt'] +  '\\n\\nãƒ¢ãƒ‡ãƒ« A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nãƒ¢ãƒ‡ãƒ« B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6c04b",
   "metadata": {},
   "source": [
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.24295Z",
     "iopub.status.busy": "2024-07-08T02:28:01.242086Z",
     "iopub.status.idle": "2024-07-08T02:28:02.030199Z",
     "shell.execute_reply": "2024-07-08T02:28:02.029332Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.242902Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç”Ÿæˆã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã€æœ€å¤§é•·ã•ã«ãƒˆãƒ©ãƒ³ã‚¯ã‚·ãƒ§ãƒ³ã—ã€PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# ãƒ†ãƒ³ã‚½ãƒ«ã‚’CPUã«ç§»å‹•ã—ã€ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041df94",
   "metadata": {},
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ \n",
    "ä¸€å°ã®GPUã«1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:02.03184Z",
     "iopub.status.busy": "2024-07-08T02:28:02.03147Z",
     "iopub.status.idle": "2024-07-08T02:29:45.864969Z",
     "shell.execute_reply": "2024-07-08T02:29:45.863989Z",
     "shell.execute_reply.started": "2024-07-08T02:28:02.031806Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesã®è¨­å®š\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "# GPU 0ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:0',\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# GPU 1ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:1',\n",
    "    use_cache=False,\n",
    ")\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a0973",
   "metadata": {},
   "source": [
    "ã“ã‚Œã§ã€å„GPUã«1ã¤ã®ãƒ¢ãƒ‡ãƒ«ã‚’æ­£å¸¸ã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã—ãŸï¼\n",
    "\n",
    "# é‡ã¿ã®ãƒ­ãƒ¼ãƒ‰ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:45.866326Z",
     "iopub.status.busy": "2024-07-08T02:29:45.866042Z",
     "iopub.status.idle": "2024-07-08T02:29:45.871399Z",
     "shell.execute_reply": "2024-07-08T02:29:45.870364Z",
     "shell.execute_reply.started": "2024-07-08T02:29:45.8663Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRaã®è¨­å®š\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    inference_mode=True,\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:58.204581Z",
     "iopub.status.busy": "2024-07-08T02:29:58.203873Z",
     "iopub.status.idle": "2024-07-08T02:30:24.990536Z",
     "shell.execute_reply": "2024-07-08T02:30:24.989591Z",
     "shell.execute_reply.started": "2024-07-08T02:29:58.204542Z"
    }
   },
   "outputs": [],
   "source": [
    "# PEFTãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—\n",
    "# model_0 = get_peft_model(base_model_0, lora_config).to(device0) \n",
    "model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)\n",
    "# model_0 = model_0.merge_and_unload()\n",
    "# model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_0.config.pretraining_tp = 1\n",
    "model_0.eval()\n",
    "\n",
    "# model_1 = get_peft_model(base_model_1, lora_config).to(device1) \n",
    "model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)\n",
    "# model_1 = model_1.merge_and_unload()\n",
    "# model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_1.config.pretraining_tp = 1\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.487776Z",
     "iopub.status.busy": "2024-07-08T02:30:48.487408Z",
     "iopub.status.idle": "2024-07-08T02:30:48.506346Z",
     "shell.execute_reply": "2024-07-08T02:30:48.505415Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.487748Z"
    }
   },
   "outputs": [],
   "source": [
    "# å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043d12e",
   "metadata": {},
   "source": [
    "# æ¨è«–\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.831515Z",
     "iopub.status.busy": "2024-07-08T02:30:48.830751Z",
     "iopub.status.idle": "2024-07-08T02:30:49.013585Z",
     "shell.execute_reply": "2024-07-08T02:30:49.012589Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.831485Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.015875Z",
     "iopub.status.busy": "2024-07-08T02:30:49.015444Z",
     "iopub.status.idle": "2024-07-08T02:30:49.025481Z",
     "shell.execute_reply": "2024-07-08T02:30:49.024531Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.015843Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "\n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒãƒƒãƒã«åˆ†å‰²ã—ã¦æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])  # ãƒ¢ãƒ‡ãƒ« A ã®ç¢ºç‡\n",
    "        generated_class_b.extend(probabilities[:, 1])  # ãƒ¢ãƒ‡ãƒ« B ã®ç¢ºç‡\n",
    "        generated_class_c.extend(probabilities[:, 2])  # å¼•ãåˆ†ã‘ã®ç¢ºç‡\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.168703Z",
     "iopub.status.busy": "2024-07-08T02:30:49.168372Z",
     "iopub.status.idle": "2024-07-08T02:31:00.511295Z",
     "shell.execute_reply": "2024-07-08T02:31:00.510349Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.168678Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚’2ã¤ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã«åˆ†å‰²\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# ã‚¹ãƒ¬ãƒƒãƒ‰ã§æ¨è«–ã‚’å®Ÿè¡Œã™ã‚‹ãŸã‚ã®é–¢æ•°\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰ã®çµæœã‚’æ ¼ç´ã™ã‚‹è¾æ›¸\n",
    "results = {}\n",
    "\n",
    "# ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’é–‹å§‹\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# ã™ã¹ã¦ã®ã‚¹ãƒ¬ãƒƒãƒ‰ãŒçµ‚äº†ã™ã‚‹ã®ã‚’å¾…ã¤\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# çµæœã‚’å…ƒã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµ±åˆ\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"å‡¦ç†å®Œäº†ã€‚ç·æ™‚é–“: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.513694Z",
     "iopub.status.busy": "2024-07-08T02:31:00.513202Z",
     "iopub.status.idle": "2024-07-08T02:31:00.532115Z",
     "shell.execute_reply": "2024-07-08T02:31:00.530886Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.513657Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "display(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.534677Z",
     "iopub.status.busy": "2024-07-08T02:31:00.533529Z",
     "iopub.status.idle": "2024-07-08T02:31:00.551799Z",
     "shell.execute_reply": "2024-07-08T02:31:00.550761Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.534638Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46876b6",
   "metadata": {},
   "source": [
    "æ¨è«–ã¯ç´„4.5æ™‚é–“ã§å®Œäº†ã—ã¾ã™ãŒã€ã¾ã æ”¹å–„ã™ã¹ãç‚¹ãŒã‚ã‚Šã¾ã™ã€‚ç•°ãªã‚‹å¾Œå‡¦ç†ã‚’è©¦ã—ã¦ã¿ã¦ã€ãœã²å…±æœ‰ã—ã¦ãã ã•ã„ã€‚Kaggleã®ã‚„ã‚Šæ–¹ã§ã™ :)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351301,
     "sourceId": 8917867,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186059439,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
