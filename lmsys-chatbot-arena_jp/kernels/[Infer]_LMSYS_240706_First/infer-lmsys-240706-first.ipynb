{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e632c6",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、ファインチューニングされたLlama-3 8bモデルを利用して、大規模なデータセット（25,000サンプル）に対する推論を行うことを目的としています。使用する環境は、T4 GPUを2台使用しており、トレーニング済みのモデルへのアクセスが必要です。\n",
    "\n",
    "### 取り組む問題\n",
    "ノートブックは、特に大規模なテストセットに対して各応答モデルの好みを予測し、どのモデルが選ばれるかを判断することに焦点を当てています。具体的には、ユーザーのプロンプトに対してモデルAとモデルBの応答を比較し、その選好を予測します。\n",
    "\n",
    "### 手法\n",
    "1. **ライブラリのインポート**:\n",
    "   - `torch`, `sklearn`, `numpy`, `pandas`などのライブラリを使用。\n",
    "   - Hugging Faceの`transformers`および`peft`ライブラリを使用して、事前学習済みのLlamaモデルのトークナイゼーションと推論を行います。\n",
    "\n",
    "2. **データの準備**:\n",
    "   - テストデータセットから必要なカラムを読み込み、文字列を処理して適切な形式に変換します。\n",
    "\n",
    "3. **トークナイズ**:\n",
    "   - `AutoTokenizer`を用いて入力テキストをトークン化し、PyTorchテンソル形式に変換します。\n",
    "\n",
    "4. **モデルのロード**:\n",
    "   - 2台のGPUそれぞれに異なるモデルを配置し、`AutoModelForSequenceClassification`を用いてモデルを読み込みます。\n",
    "\n",
    "5. **LoRa設定**:\n",
    "   - 減少されたメモリフットプリントでファインチューニングを行うために、LoRa（Low-Rank Adaptation）を設定します。\n",
    "\n",
    "6. **推論**:\n",
    "   - データフレームをバッチ処理しながら、モデルに対する推論を実行します。そして、モデルAやB、引き分けの確率を計算します。\n",
    "\n",
    "7. **マルチスレッド推論**:\n",
    "   - 結果を並行して計算するために、Pythonのスレッドを使用し、2つのサブセットに分割したデータを各モデルで処理します。\n",
    "\n",
    "8. **結果の統合**:\n",
    "   - 最終的にモデルの出力を統合し、提出用のCSVファイルに保存します。\n",
    "\n",
    "推論は約4.5時間で完了しますが、さらなる改善の余地があるため、異なる後処理方法の検討が提案されています。全体として、このノートブックは大規模言語モデルを用いたユーザー選好予測の実装例を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32235089",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づいて、初心者がつまずきそうな専門用語の解説です。\n",
    "\n",
    "1. **llama-3**:\n",
    "   - OpenAIが開発した大規模言語モデルの一種で、特に特定のタスク（この場合は一対の応答の比較）に対する性能を向上させるためにファインチューニングされたモデルです。\n",
    "\n",
    "2. **ファインチューニング**:\n",
    "   - 既存のモデルに追加のトレーニングを行って、特定のタスクのパフォーマンスを向上させるプロセス。ベースモデルのパラメータを最適化することによって、特定のデータセットに対する適応性を持たせる。\n",
    "\n",
    "3. **AutoTokenizer**:\n",
    "   - Hugging FaceのTransformersライブラリの一部で、自動的にモデルに適したトークナイザを取得するためのクラス。このクラスは、入力テキストをトークン（数値の配列）に変換する機能を提供します。\n",
    "\n",
    "4. **BitsAndBytesConfig**:\n",
    "   - 計算量を削減しメモリ使用量を効率化するための設定クラス。特に、大規模なモデルを軽量化するための手法の一つ（例：量子化）を実現するために用いられます。\n",
    "\n",
    "5. **量子化**:\n",
    "   - モデルのパラメータを低精度の形式（例えば、8ビットまたは4ビット）で表現するプロセス。これにより、メモリ footprint を削減して推論速度を向上させることができます。\n",
    "\n",
    "6. **LoRa (Low-Rank Adaptation)**:\n",
    "   - モデルの効率的なファインチューニング手法の一つです。特に、全結合層の重みを低ランクに近似することで、トレーニングに必要なパラメータの数を減らします。\n",
    "\n",
    "7. **LoraConfig**:\n",
    "   - LoRaの設定を定義するためのクラスで、特定のローレベルのパラメータ（例えば、ランクやドロップアウト率）を指定します。\n",
    "\n",
    "8. **PeftModel**:\n",
    "   - Parameter Efficient Fine-Tuningの略で、ファインチューニングを行う際に必要なパラメータを効率良く管理するためのモデルクラス。\n",
    "\n",
    "9. **autocast**:\n",
    "   - PyTorchの機能で、モデルの推論やトレーニングを実行する際に、自動的に混合精度計算を行うことで性能を向上させます。これにより、速度とメモリ効率が改善されることがあります。\n",
    "\n",
    "10. **スレッド**:\n",
    "    - プログラムの実行単位であり、複数のスレッドを用いて同時に処理を行うことで、計算の効率を向上させます。このノートブックでは、異なるモデルを複数のスレッドで並行して扱っています。\n",
    "\n",
    "11. **attention masks**:\n",
    "    - トークン化された入力がモデルに与えられる際、どのトークンが実際の入力に該当するのかを示すマスクで、パディングトークンを無視するために使われます。\n",
    "\n",
    "12. **バッチサイズ (BATCH_SIZE)**:\n",
    "    - 推論やトレーニングの際に一度に処理するデータのサンプル数のこと。大きなバッチサイズは計算効率を高めますが、メモリ使用量が増えるため、適切なサイズ設定が重要です。\n",
    "\n",
    "これらの用語に関する理解が深まることで、ノートブック内でのコードの意味がより明確になり、全体の流れを理解する手助けになるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534231d0",
   "metadata": {},
   "source": [
    "# 推論 - llama-3 8b 超高速 🚀\n",
    "このノートブックでは、T4 GPUを2台使ってファインチューニング済みのllama-3 8bモデルを用いた推論を行います。このノートブックを作成した理由は、テストサイズが非常に大きい（25,000サンプル）からです。\n",
    "\n",
    "前提条件: Llama-3へのアクセスが必要です。もし同意書に記入していない場合は、[こちら](https://www.kaggle.com/models/metaresearch/llama-3)に行き、同意書に記入してLlama-3にアクセスしてください。\n",
    "\n",
    "トレーニングノートブックは[こちら](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)にあります。\n",
    "\n",
    "役立つと感じたら、ぜひアップボートしてください！\n",
    "\n",
    "# ライブラリのインポート\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:02.957881Z",
     "iopub.status.busy": "2024-07-08T02:27:02.957535Z",
     "iopub.status.idle": "2024-07-08T02:27:55.475584Z",
     "shell.execute_reply": "2024-07-08T02:27:55.474311Z",
     "shell.execute_reply.started": "2024-07-08T02:27:02.957851Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:27:55.477832Z",
     "iopub.status.busy": "2024-07-08T02:27:55.477514Z",
     "iopub.status.idle": "2024-07-08T02:28:01.180228Z",
     "shell.execute_reply": "2024-07-08T02:28:01.179278Z",
     "shell.execute_reply.started": "2024-07-08T02:27:55.477802Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"申し訳ありませんが、GPUが必要です！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.181789Z",
     "iopub.status.busy": "2024-07-08T02:28:01.181357Z",
     "iopub.status.idle": "2024-07-08T02:28:01.186354Z",
     "shell.execute_reply": "2024-07-08T02:28:01.185425Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.181764Z"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'\n",
    "MAX_LENGTH = 2048\n",
    "BATCH_SIZE = 4\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67aed9ee",
   "metadata": {},
   "source": [
    "# データの準備 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.189204Z",
     "iopub.status.busy": "2024-07-08T02:28:01.188861Z",
     "iopub.status.idle": "2024-07-08T02:28:01.23147Z",
     "shell.execute_reply": "2024-07-08T02:28:01.230662Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.189175Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "# データフレームの各列に対してprocess関数を適用\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.232988Z",
     "iopub.status.busy": "2024-07-08T02:28:01.232654Z",
     "iopub.status.idle": "2024-07-08T02:28:01.240511Z",
     "shell.execute_reply": "2024-07-08T02:28:01.239621Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.232956Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデル用のテキストを準備\n",
    "test['text'] = 'ユーザーのプロンプト: ' + test['prompt'] +  '\\n\\nモデル A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nモデル B:\\n'  + test['response_b']\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d6c04b",
   "metadata": {},
   "source": [
    "# トークナイズ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:01.24295Z",
     "iopub.status.busy": "2024-07-08T02:28:01.242086Z",
     "iopub.status.idle": "2024-07-08T02:28:02.030199Z",
     "shell.execute_reply": "2024-07-08T02:28:02.029332Z",
     "shell.execute_reply.started": "2024-07-08T02:28:01.242902Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# トークンを生成、パディング、最大長さにトランクションし、PyTorchテンソルとして返す\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041df94",
   "metadata": {},
   "source": [
    "# モデルの読み込み \n",
    "一台のGPUに1つのモデルを読み込みます。  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:28:02.03184Z",
     "iopub.status.busy": "2024-07-08T02:28:02.03147Z",
     "iopub.status.idle": "2024-07-08T02:29:45.864969Z",
     "shell.execute_reply": "2024-07-08T02:29:45.863989Z",
     "shell.execute_reply.started": "2024-07-08T02:28:02.031806Z"
    }
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesの設定\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit = True, \n",
    "    bnb_4bit_quant_type = 'nf4',\n",
    "    bnb_4bit_use_double_quant = True, \n",
    "    bnb_4bit_compute_dtype = torch.bfloat16 \n",
    ")\n",
    "\n",
    "# GPU 0にベースモデルをロード\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:0',\n",
    "    use_cache=False,\n",
    ")\n",
    "\n",
    "# パディングトークンIDを設定\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# GPU 1にベースモデルをロード\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "#     quantization_config=quantization_config,\n",
    "    num_labels=3,\n",
    "    device_map='cuda:1',\n",
    "    use_cache=False,\n",
    ")\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62a0973",
   "metadata": {},
   "source": [
    "これで、各GPUに1つのモデルを正常にロードできました！\n",
    "\n",
    "# 重みのロード \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:45.866326Z",
     "iopub.status.busy": "2024-07-08T02:29:45.866042Z",
     "iopub.status.idle": "2024-07-08T02:29:45.871399Z",
     "shell.execute_reply": "2024-07-08T02:29:45.870364Z",
     "shell.execute_reply.started": "2024-07-08T02:29:45.8663Z"
    }
   },
   "outputs": [],
   "source": [
    "# LoRaの設定\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 16, \n",
    "    lora_alpha = 8,\n",
    "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
    "    lora_dropout = 0.05, \n",
    "    bias = 'none',\n",
    "    inference_mode=True,\n",
    "    task_type = 'SEQ_CLS'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:29:58.204581Z",
     "iopub.status.busy": "2024-07-08T02:29:58.203873Z",
     "iopub.status.idle": "2024-07-08T02:30:24.990536Z",
     "shell.execute_reply": "2024-07-08T02:30:24.989591Z",
     "shell.execute_reply.started": "2024-07-08T02:29:58.204542Z"
    }
   },
   "outputs": [],
   "source": [
    "# PEFTモデルを取得\n",
    "# model_0 = get_peft_model(base_model_0, lora_config).to(device0) \n",
    "model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)\n",
    "# model_0 = model_0.merge_and_unload()\n",
    "# model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_0.config.pretraining_tp = 1\n",
    "model_0.eval()\n",
    "\n",
    "# model_1 = get_peft_model(base_model_1, lora_config).to(device1) \n",
    "model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)\n",
    "# model_1 = model_1.merge_and_unload()\n",
    "# model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model_1.config.pretraining_tp = 1\n",
    "model_1.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.487776Z",
     "iopub.status.busy": "2024-07-08T02:30:48.487408Z",
     "iopub.status.idle": "2024-07-08T02:30:48.506346Z",
     "shell.execute_reply": "2024-07-08T02:30:48.505415Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.487748Z"
    }
   },
   "outputs": [],
   "source": [
    "# 学習可能なパラメータ\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043d12e",
   "metadata": {},
   "source": [
    "# 推論\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:48.831515Z",
     "iopub.status.busy": "2024-07-08T02:30:48.830751Z",
     "iopub.status.idle": "2024-07-08T02:30:49.013585Z",
     "shell.execute_reply": "2024-07-08T02:30:49.012589Z",
     "shell.execute_reply.started": "2024-07-08T02:30:48.831485Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.015875Z",
     "iopub.status.busy": "2024-07-08T02:30:49.015444Z",
     "iopub.status.idle": "2024-07-08T02:30:49.025481Z",
     "shell.execute_reply": "2024-07-08T02:30:49.024531Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.015843Z"
    }
   },
   "outputs": [],
   "source": [
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "\n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    # データフレームをバッチに分割して推論を実行\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])  # モデル A の確率\n",
    "        generated_class_b.extend(probabilities[:, 1])  # モデル B の確率\n",
    "        generated_class_c.extend(probabilities[:, 2])  # 引き分けの確率\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:30:49.168703Z",
     "iopub.status.busy": "2024-07-08T02:30:49.168372Z",
     "iopub.status.idle": "2024-07-08T02:31:00.511295Z",
     "shell.execute_reply": "2024-07-08T02:31:00.510349Z",
     "shell.execute_reply.started": "2024-07-08T02:30:49.168678Z"
    }
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# データを2つのサブセットに分割\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# スレッドで推論を実行するための関数\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# スレッドからの結果を格納する辞書\n",
    "results = {}\n",
    "\n",
    "# スレッドを開始\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# すべてのスレッドが終了するのを待つ\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のデータフレームに統合\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"処理完了。総時間: {time.time() - st}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.513694Z",
     "iopub.status.busy": "2024-07-08T02:31:00.513202Z",
     "iopub.status.idle": "2024-07-08T02:31:00.532115Z",
     "shell.execute_reply": "2024-07-08T02:31:00.530886Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.513657Z"
    }
   },
   "outputs": [],
   "source": [
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "display(sample_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-08T02:31:00.534677Z",
     "iopub.status.busy": "2024-07-08T02:31:00.533529Z",
     "iopub.status.idle": "2024-07-08T02:31:00.551799Z",
     "shell.execute_reply": "2024-07-08T02:31:00.550761Z",
     "shell.execute_reply.started": "2024-07-08T02:31:00.534638Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_sub.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46876b6",
   "metadata": {},
   "source": [
    "推論は約4.5時間で完了しますが、まだ改善すべき点があります。異なる後処理を試してみて、ぜひ共有してください。Kaggleのやり方です :)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5351301,
     "sourceId": 8917867,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 186059439,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
