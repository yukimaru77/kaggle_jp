{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0d35449",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "### Jupyter Notebook 要約\n",
    "\n",
    "このJupyter Notebookは、「LMSYS - Chatbot Arena」コンペティションにおいて、大規模言語モデル（LLM）であるLlama 3を使用し、ユーザーの好みを予測する問題に取り組んでいます。具体的には、異なるモデルの応答の優劣を評価し、その結果に基づいて予測を行うプロセスを含むノートブックです。\n",
    "\n",
    "#### 主要な問題:\n",
    "- ユーザーから提供されたプロンプトに対する2つのチャットボット（モデルAとモデルB）の応答を比較し、どちらが好まれるかを予測すること。\n",
    "\n",
    "#### 使用した手法:\n",
    "1. **ライブラリとフレームワーク**:\n",
    "   - `transformers`: Llamaモデルのトークナイジングやシーケンス分類のために使用。\n",
    "   - `peft`: 軽量なファインチューニングを可能にするために使用され、LoRa設定を通じてモデルの重みを管理。\n",
    "   - `torch`: PyTorchを用いて深層学習モデルを構築、GPUでの計算を行う。\n",
    "\n",
    "2. **データの処理**:\n",
    "   - Pandasを用いてデータを読み込み、トークナイズすることで、モデルが処理しやすい形式に変換。\n",
    "   - 特徴量を作成するために、プロンプトや応答の文字列情報を結合して特長抽出を実施。\n",
    "\n",
    "3. **モデルの構築と推論**:\n",
    "   - Llamaモデルを2つの異なるGPU（デバイス）に読み込み、それぞれのモデルで予測を行う。\n",
    "   - データの半分を異なるスレッドで処理することで、推論の効率を向上。\n",
    "\n",
    "4. **LightGBMとのハイブリッドアプローチ**:\n",
    "   - LightGBMを使って別途予測を実施し、最終的な予測はLlamaによる予測とLightGBMによる予測の加重平均で得られる。\n",
    "\n",
    "5. **出力**:\n",
    "   - 最終的に得られた予測結果をCSVファイル（`submission.csv`）として保存。\n",
    "\n",
    "このNotebookは、高度な機械学習技術を活用し、ユーザーの好みをより正確に予測するための基盤を提供することを目的としています。特に、異なるモデルの応答を効果的に比較する手法が強調されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3ad88e",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に基づいて、初心者がつまずきそうな専門用語の簡単な解説をリストアップします。\n",
    "\n",
    "### 専門用語解説\n",
    "\n",
    "1. **LoRa (Low-Rank Adaptation)**:\n",
    "   - LoRaは、事前学習済みモデルの適応を効率的に行う技術で、パラメータの更新を低ランク行列に圧縮し、計算コストとメモリ使用量を削減します。これにより、大規模モデルを効率よく調整できるようになります。具体的には、選択的に更新するパラメータの数を減らすことで、少ないリソースでモデルを学習させる手法です。\n",
    "\n",
    "2. **量子化 (Quantization)**:\n",
    "   - 量子化は、モデルの数値を低精度な型（例：float32からfloat16）に変換してストレージと処理にかかるコストを削減する技術です。これにより、モデルのサイズが小さくなり、GPUメモリの使用効率が向上します。\n",
    "\n",
    "3. **アテンションマスク (Attention Mask)**:\n",
    "   - アテンションマスクは、モデルが入力シーケンスのどの部分に注意を向けるかを制御するための構造です。特にパディングされた部分を無視するために使用され、モデルが無意味な情報を学習しないようにします。\n",
    "\n",
    "4. **自動混合精度 (Automatic Mixed Precision)**:\n",
    "   - より少ない計算資源で訓練を行うために、異なる精度のデータを自動で混合して使用する技術です。通常、訓練時に計算が重たくなりがちですが、これにより計算速度が向上し、トレーニングが効率化されます。\n",
    "\n",
    "5. **トークナイザー (Tokenizer)**:\n",
    "   - トークナイザーは、テキストデータをモデルが理解できる数値データ（トークン）に変換するためのツールです。この変換は、自然言語処理において不可欠なステップで、文章を単語や文の単位に分割します。\n",
    "\n",
    "6. **ガーベジコレクション (Garbage Collection)**:\n",
    "   - プログラミングにおけるメモリ管理の一環で、不要になったオブジェクトや変数を自動的に解放してメモリの効率を改善するプロセスです。Pythonでは、不要なメモリを自動で回収する仕組みがあります。\n",
    "\n",
    "7. **バッチ処理 (Batch Processing)**:\n",
    "   - データを小さな単位（バッチ）に分けて処理する方法で、メモリ効率を向上させ、計算速度を早めるために使われる技術です。特に大きなデータセットやモデルでこの方法が重宝されます。\n",
    "\n",
    "8. **フォールド (Fold)**:\n",
    "   - クロスバリデーションにおいてデータセットを分割する単位です。フォールドはモデルの訓練と検証を繰り返し行い、モデルの性能を評価するために用いられます。\n",
    "\n",
    "9. **アンサンブル (Ensemble)**:\n",
    "   - 複数の異なるモデルの予測を組み合わせて、最終的な予測を決定する手法です。これにより、予測精度を向上させることができます。\n",
    "\n",
    "これらの説明が、Jupyter Notebookの内容を理解する際に役立つことを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6498b5",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## llama3-8b\n",
    "\n",
    "shout out to:\n",
    "https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## llama3-8b\n",
    "\n",
    "作成者への感謝:\n",
    "https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51887469",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:22:58.329854Z",
     "iopub.status.busy": "2024-07-05T03:22:58.329426Z",
     "iopub.status.idle": "2024-07-05T03:23:53.200284Z",
     "shell.execute_reply": "2024-07-05T03:23:53.198969Z",
     "shell.execute_reply.started": "2024-07-05T03:22:58.329828Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92c88e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "MAX_LENGTH = 1024\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\")    \n",
    "\n",
    "# # Prepare Data \n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])\n",
    "\n",
    "# # Tokenize\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]\n",
    "\n",
    "# # Load model \n",
    "# We load 1 model on each gpu.  \n",
    "\n",
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Now, we have sucessfully loaded one model on each GPU!\n",
    "\n",
    "# # Load weights \n",
    "\n",
    "# LoRa configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj'])\n",
    "\n",
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "# Load weights\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()\n",
    "\n",
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "\n",
    "# # Inference\n",
    "# \n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df\n",
    "\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}\n",
    "\n",
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")\n",
    "\n",
    "# Inference completes in ~4.5 hrs, there are still stuff to improve upon this. I would encourage to try out different post-processing and share. Kaggle way :) \n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを有効にする\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にする\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"申し訳ありませんが、GPUが必要です！\")  # GPUが利用できない場合のメッセージ\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのパス\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'  # 重みのパス\n",
    "MAX_LENGTH = 1024  # トークンの最大長\n",
    "BATCH_SIZE = 8  # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # デバイスをCUDAに設定\n",
    "\n",
    "# # データの準備\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出データを読み込む\n",
    "\n",
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文章を分割してクォーテーションを削除\n",
    "    return  ' '.join(sentences)  # 文章をスペースで連結して返す\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプトを処理\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答を処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答を処理\n",
    "\n",
    "display(sample_sub)  # サンプル提出データを表示\n",
    "display(test.head(5))  # テストデータの最初の5行を表示\n",
    "\n",
    "# モデル用のテキストを準備\n",
    "test['text'] = 'ユーザーのプロンプト: ' + test['prompt'] +  '\\n\\nモデルA :\\n' + test['response_a'] +'\\n\\n--------\\n\\nモデルB:\\n'  + test['response_b']\n",
    "print(test['text'][0])  # 最初のテキストを表示\n",
    "\n",
    "# # トークナイズ\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーの読み込み\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')  # テキストをトークナイズ\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)  # 入力IDをデバイスに転送\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)  # アテンションマスクをデバイスに転送\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()  # 新しいデータフレームを作成\n",
    "data['INPUT_IDS'] = input_ids_cpu  # 入力IDを格納\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu  # アテンションマスクを格納\n",
    "data[:2]  # 最初の2行を表示\n",
    "\n",
    "# # モデルをロード\n",
    "# 各GPUに1つのモデルを読み込む\n",
    "\n",
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットで読み込む\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 8ビット計算のデータ型を指定\n",
    "    bnb_8bit_use_double_quant=False)  # 二重量子化を無効にする\n",
    "\n",
    "# GPU 0にベースモデルを読み込む\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # ラベルの数\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')  # モデルをGPU 0にマップ\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# GPU 1にベースモデルを読み込む\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')  # モデルをGPU 1にマップ\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# これで、各GPUにモデルが正常に読み込まれました！\n",
    "\n",
    "# # 重みを読み込む\n",
    "\n",
    "# LoRa設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # LoRaのパラメータ\n",
    "    lora_alpha=32,  # LoRaのアルファパラメータ\n",
    "    lora_dropout=0.10,  # LoRaのドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    "    target_modules=['o_proj', 'v_proj'])  # 対象モジュール\n",
    "\n",
    "# PEFTを取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0)  # モデルをGPUに転送\n",
    "# 重みを読み込む\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)  # モデルをGPUに転送\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み\n",
    "model_1.eval()  # 評価モードに設定\n",
    "\n",
    "# 学習可能なパラメータの表示\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "\n",
    "# # 推論\n",
    "# \n",
    "\n",
    "import gc\n",
    "gc.collect()  # ガーベジコレクションを実行\n",
    "\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルに変換\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # アテンションマスクをテンソルに変換\n",
    "    \n",
    "    generated_class_a = []  # モデルAの結果を格納するリスト\n",
    "    generated_class_b = []  # モデルBの結果を格納するリスト\n",
    "    generated_class_c = []  # 同点を格納するリスト\n",
    "\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):  # バッチごとに繰り返す\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算を無効にする\n",
    "            with autocast():  # 自動混合精度を使用\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # 出力の確率を計算\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])  # モデルAの結果を追加\n",
    "        generated_class_b.extend(probabilities[:, 1])  # モデルBの結果を追加\n",
    "        generated_class_c.extend(probabilities[:, 2])  # 同点の結果を追加\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a  # データフレームにモデルAの結果を保存\n",
    "    df['winner_model_b'] = generated_class_b  # データフレームにモデルBの結果を保存\n",
    "    df['winner_tie'] = generated_class_c  # データフレームに同点の結果を保存\n",
    "\n",
    "    torch.cuda.empty_cache()  # CUDAのメモリを空にする\n",
    "\n",
    "    return df  # 処理したデータフレームを返す\n",
    "\n",
    "st = time.time()  # 処理開始時間を記録\n",
    "\n",
    "N_SAMPLES = len(data)  # データのサンプル数\n",
    "\n",
    "# データを二つのサブセットに分割\n",
    "half = round(N_SAMPLES / 2)  # サンプルの半分のサイズを計算\n",
    "sub1 = data.iloc[0:half].copy()  # 最初のサブセット\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 2つ目のサブセット\n",
    "\n",
    "# スレッドで推論を実行する関数\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 結果を保存\n",
    "\n",
    "# スレッドからの結果を保存するための辞書\n",
    "results = {}\n",
    "\n",
    "# スレッドを開始\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # モデルAを使用するスレッド\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # モデルBを使用するスレッド\n",
    "\n",
    "t0.start()  # スレッドの開始\n",
    "t1.start()  # スレッドの開始\n",
    "\n",
    "# すべてのスレッドが終了するのを待つ\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のデータフレームに統合\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"処理が完了しました。合計時間: {time.time() - st}\")  # 処理時間を表示\n",
    "\n",
    "# 推論が約4.5時間で完了しました。改善の余地があるため、異なる後処理を試して共有することをお勧めします。Kaggleのやり方 :)\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測のターゲット列\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]  # サンプル提出に結果を追加\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:23:53.202855Z",
     "iopub.status.busy": "2024-07-05T03:23:53.202545Z",
     "iopub.status.idle": "2024-07-05T03:26:11.922417Z",
     "shell.execute_reply": "2024-07-05T03:26:11.921452Z",
     "shell.execute_reply.started": "2024-07-05T03:23:53.202827Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを有効にする\n",
    "torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にする\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"申し訳ありませんが、GPUが必要です！\")  # GPUが利用できない場合のメッセージ\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのパス\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'  # 重みのパス\n",
    "MAX_LENGTH = 1024  # トークンの最大長\n",
    "BATCH_SIZE = 8  # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # デバイスをCUDAに設定\n",
    "\n",
    "# # データの準備\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出データを読み込む\n",
    "\n",
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文章を分割してクォーテーションを削除\n",
    "    return  ' '.join(sentences)  # 文章をスペースで連結して返す\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプトを処理\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答を処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答を処理\n",
    "\n",
    "display(sample_sub)  # サンプル提出データを表示\n",
    "display(test.head(5))  # テストデータの最初の5行を表示\n",
    "\n",
    "# モデル用のテキストを準備\n",
    "test['text'] = 'ユーザーのプロンプト: ' + test['prompt'] +  '\\n\\nモデルA :\\n' + test['response_a'] +'\\n\\n--------\\n\\nモデルB:\\n'  + test['response_b']\n",
    "print(test['text'][0])  # 最初のテキストを表示\n",
    "\n",
    "# # トークナイズ\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーの読み込み\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')  # テキストをトークナイズ\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)  # 入力IDをデバイスに転送\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)  # アテンションマスクをデバイスに転送\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()  # 新しいデータフレームを作成\n",
    "data['INPUT_IDS'] = input_ids_cpu  # 入力IDを格納\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu  # アテンションマスクを格納\n",
    "data[:2]  # 最初の2行を表示\n",
    "\n",
    "# # モデルをロード\n",
    "# 各GPUに1つのモデルを読み込む\n",
    "\n",
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # 8ビットで読み込む\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 8ビット計算のデータ型を指定\n",
    "    bnb_8bit_use_double_quant=False)  # 二重量子化を無効にする\n",
    "\n",
    "# GPU 0にベースモデルを読み込む\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,  # ラベルの数\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')  # モデルをGPU 0にマップ\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# GPU 1にベースモデルを読み込む\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')  # モデルをGPU 1にマップ\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# これで、各GPUにモデルが正常に読み込まれました！\n",
    "\n",
    "# # 重みを読み込む\n",
    "\n",
    "# LoRa設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # LoRaのパラメータ\n",
    "    lora_alpha=32,  # LoRaのアルファパラメータ\n",
    "    lora_dropout=0.10,  # LoRaのドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    "    target_modules=['o_proj', 'v_proj'])  # 対象モジュール\n",
    "\n",
    "# PEFTを取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0)  # モデルをGPUに転送\n",
    "# 重みを読み込む\n",
    "model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)  # モデルをGPUに転送\n",
    "model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み\n",
    "model_1.eval()  # 評価モードに設定\n",
    "\n",
    "# 学習可能なパラメータの表示\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "\n",
    "# # 推論\n",
    "# \n",
    "\n",
    "import gc\n",
    "gc.collect()  # ガーベジコレクションを実行\n",
    "\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルに変換\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # アテンションマスクをテンソルに変換\n",
    "    \n",
    "    generated_class_a = []  # モデルAの結果を格納するリスト\n",
    "    generated_class_b = []  # モデルBの結果を格納するリスト\n",
    "    generated_class_c = []  # 同点を格納するリスト\n",
    "\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):  # バッチごとに繰り返す\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算を無効にする\n",
    "            with autocast():  # 自動混合精度を使用\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # 出力の確率を計算\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])  # モデルAの結果を追加\n",
    "        generated_class_b.extend(probabilities[:, 1])  # モデルBの結果を追加\n",
    "        generated_class_c.extend(probabilities[:, 2])  # 同点の結果を追加\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a  # データフレームにモデルAの結果を保存\n",
    "    df['winner_model_b'] = generated_class_b  # データフレームにモデルBの結果を保存\n",
    "    df['winner_tie'] = generated_class_c  # データフレームに同点の結果を保存\n",
    "\n",
    "    torch.cuda.empty_cache()  # CUDAのメモリを空にする\n",
    "\n",
    "    return df  # 処理したデータフレームを返す\n",
    "\n",
    "st = time.time()  # 処理開始時間を記録\n",
    "\n",
    "N_SAMPLES = len(data)  # データのサンプル数\n",
    "\n",
    "# データを二つのサブセットに分割\n",
    "half = round(N_SAMPLES / 2)  # サンプルの半分のサイズを計算\n",
    "sub1 = data.iloc[0:half].copy()  # 最初のサブセット\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 2つ目のサブセット\n",
    "\n",
    "# スレッドで推論を実行する関数\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 結果を保存\n",
    "\n",
    "# スレッドからの結果を保存するための辞書\n",
    "results = {}\n",
    "\n",
    "# スレッドを開始\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # モデルAを使用するスレッド\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # モデルBを使用するスレッド\n",
    "\n",
    "t0.start()  # スレッドの開始\n",
    "t1.start()  # スレッドの開始\n",
    "\n",
    "# すべてのスレッドが終了するのを待つ\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のデータフレームに統合\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"処理が完了しました。合計時間: {time.time() - st}\")  # 処理時間を表示\n",
    "\n",
    "# 推論が約4.5時間で完了しました。改善の余地があるため、異なる後処理を試して共有することをお勧めします。Kaggleのやり方 :)\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測のターゲット列\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]  # サンプル提出に結果を追加"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2599a7c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "llama_preds = data[TARGETS].values\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "llama_preds = data[TARGETS].values  # ターゲットの値を取得\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:11.928099Z",
     "iopub.status.busy": "2024-07-05T03:26:11.927672Z",
     "iopub.status.idle": "2024-07-05T03:26:11.933799Z",
     "shell.execute_reply": "2024-07-05T03:26:11.932872Z",
     "shell.execute_reply.started": "2024-07-05T03:26:11.928064Z"
    }
   },
   "outputs": [],
   "source": [
    "llama_preds = data[TARGETS].values  # ターゲットの値を取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e6a73a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## LGBM + tfidf\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## LGBM + tfidf\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386a5bf3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "TAG = 'lmsys-chatbot-arena'\n",
    "\n",
    "import os\n",
    "RUNPOD = os.path.exists('/workspace/')\n",
    "KAGGLE = not RUNPOD\n",
    "if KAGGLE: print('kaggle')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ\n",
    "\n",
    "import os\n",
    "RUNPOD = os.path.exists('/workspace/')  # 実行環境がWORKSPACEかどうか確認\n",
    "KAGGLE = not RUNPOD  # Kaggleでの実行かどうか確認\n",
    "if KAGGLE: print('kaggle')  # Kaggleでの実行メッセージ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:11.935398Z",
     "iopub.status.busy": "2024-07-05T03:26:11.935092Z",
     "iopub.status.idle": "2024-07-05T03:26:12.001814Z",
     "shell.execute_reply": "2024-07-05T03:26:12.000853Z",
     "shell.execute_reply.started": "2024-07-05T03:26:11.935373Z"
    }
   },
   "outputs": [],
   "source": [
    "TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ\n",
    "\n",
    "import os\n",
    "RUNPOD = os.path.exists('/workspace/')  # 実行環境がWORKSPACEかどうか確認\n",
    "KAGGLE = not RUNPOD  # Kaggleでの実行かどうか確認\n",
    "if KAGGLE: print('kaggle')  # Kaggleでの実行メッセージ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee6abd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import pandas as pd\n",
    "except:\n",
    "    !pip install -q kaggle\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm \n",
    "    !pip install -q protobuf \n",
    "    !pip install -q numba\n",
    "    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "try:\n",
    "    import pandas as pd  # pandasをインポート\n",
    "except:\n",
    "    !pip install -q kaggle  # kaggleがインストールされていない場合インストール\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # 必要なパッケージをインストール\n",
    "    !pip install -q protobuf  # protobufをインストール\n",
    "    !pip install -q numba  # numbaをインストール\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.003301Z",
     "iopub.status.busy": "2024-07-05T03:26:12.003018Z",
     "iopub.status.idle": "2024-07-05T03:26:12.013545Z",
     "shell.execute_reply": "2024-07-05T03:26:12.012779Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.003277Z"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import pandas as pd  # pandasをインポート\n",
    "except:\n",
    "    !pip install -q kaggle  # kaggleがインストールされていない場合インストール\n",
    "    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # 必要なパッケージをインストール\n",
    "    !pip install -q protobuf  # protobufをインストール\n",
    "    !pip install -q numba  # numbaをインストール"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81f1b27",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)\n",
    "\n",
    "import os\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):\n",
    "        !mkdir -p ~/.kaggle\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json\n",
    "        !chmod 600 /root/.kaggle/kaggle.json\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):\n",
    "        !kaggle competitions download $TAG -p /workspace/ \n",
    "        \n",
    "    if not os.path.exists('/data/'):\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)  # データのパスを設定\n",
    "\n",
    "import os\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):  # kaggle.jsonが存在しない場合\n",
    "        !mkdir -p ~/.kaggle  # .kaggleディレクトリを作成\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # kaggle.jsonをコピー\n",
    "        !chmod 600 /root/.kaggle/kaggle.json  # permissionsを変更\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):  # ZIPファイルが存在しない場合\n",
    "        !kaggle competitions download $TAG -p /workspace/  # コンペティションデータをダウンロード\n",
    "        \n",
    "    if not os.path.exists('/data/'):  # /data/ディレクトリが存在しない場合\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.014825Z",
     "iopub.status.busy": "2024-07-05T03:26:12.014548Z",
     "iopub.status.idle": "2024-07-05T03:26:12.029324Z",
     "shell.execute_reply": "2024-07-05T03:26:12.028419Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.014803Z"
    }
   },
   "outputs": [],
   "source": [
    "DATA = '/data/' if RUNPOD else 'data/' \\\n",
    "        if not os.path.exists('/kaggle/') \\\n",
    "            else '/kaggle/input/{}/'.format(TAG)  # データのパスを設定\n",
    "\n",
    "import os\n",
    "\n",
    "if RUNPOD:\n",
    "    if not os.path.exists('~/.kaggle/kaggle.json'):  # kaggle.jsonが存在しない場合\n",
    "        !mkdir -p ~/.kaggle  # .kaggleディレクトリを作成\n",
    "        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # kaggle.jsonをコピー\n",
    "        !chmod 600 /root/.kaggle/kaggle.json  # permissionsを変更\n",
    "\n",
    "    if not os.path.exists('/workspace/' + TAG + '.zip'):  # ZIPファイルが存在しない場合\n",
    "        !kaggle competitions download $TAG -p /workspace/  # コンペティションデータをダウンロード\n",
    "        \n",
    "    if not os.path.exists('/data/'):  # /data/ディレクトリが存在しない場合\n",
    "        import zipfile\n",
    "        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bbf7bc",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "INPUT_PATH = '/kaggle/input/'  \n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'\n",
    "# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\n",
    "print(MODEL_PATH)\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "INPUT_PATH = '/kaggle/input/'  # 入力パス\n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'  # モデルとロジットのパス\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'  # モデルパスを設定\n",
    "# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\n",
    "print(MODEL_PATH)  # モデルパスを表示\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスを設定\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.030826Z",
     "iopub.status.busy": "2024-07-05T03:26:12.030511Z",
     "iopub.status.idle": "2024-07-05T03:26:12.040806Z",
     "shell.execute_reply": "2024-07-05T03:26:12.039827Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.030795Z"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_PATH = '/kaggle/input/'  # 入力パス\n",
    "MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'  # モデルとロジットのパス\n",
    "MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \\\n",
    "                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'  # モデルパスを設定\n",
    "# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'\n",
    "print(MODEL_PATH)  # モデルパスを表示\n",
    "\n",
    "CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスを設定\n",
    "SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bc616c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # TOKENIZERSの並列処理を無効にする\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:12.042334Z",
     "iopub.status.busy": "2024-07-05T03:26:12.041989Z",
     "iopub.status.idle": "2024-07-05T03:26:14.692364Z",
     "shell.execute_reply": "2024-07-05T03:26:14.691542Z",
     "shell.execute_reply.started": "2024-07-05T03:26:12.042304Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import gc\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # TOKENIZERSの並列処理を無効にする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0088b380",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')\n",
    "\n",
    "print(len(train), len(test))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # 訓練データを読み込む\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))  # テストデータを読み込む\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込む\n",
    "\n",
    "print(len(train), len(test))  # 訓練データとテストデータの長さを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:14.696304Z",
     "iopub.status.busy": "2024-07-05T03:26:14.695993Z",
     "iopub.status.idle": "2024-07-05T03:26:17.778317Z",
     "shell.execute_reply": "2024-07-05T03:26:17.777217Z",
     "shell.execute_reply.started": "2024-07-05T03:26:14.696278Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # 訓練データを読み込む\n",
    "test = pd.read_csv(open(DATA + 'test.csv', 'r'))  # テストデータを読み込む\n",
    "sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込む\n",
    "\n",
    "print(len(train), len(test))  # 訓練データとテストデータの長さを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ced61",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "params = {}\n",
    "if False:#len(test) < 10: \n",
    "    pass;\n",
    "    params['subsample'] = 30\n",
    "else:\n",
    "    # params['subsample'] = 2\n",
    "    params['fold'] = -1\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1\n",
    "params['n_lgb'] = 1\n",
    "params['model'] = 'microsoft/deberta-v3-small'\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "params = {}  # パラメータの初期化\n",
    "if False:  # もしテストデータの長さが10未満の場合\n",
    "    pass;\n",
    "    params['subsample'] = 30  # サブサンプルの数を設定\n",
    "else:\n",
    "    # params['subsample'] = 2\n",
    "    params['fold'] = -1  # フォールドを設定\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1  # 学習のエポック数\n",
    "params['n_lgb'] = 1  # LightGBMの数\n",
    "params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデル\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.779974Z",
     "iopub.status.busy": "2024-07-05T03:26:17.779658Z",
     "iopub.status.idle": "2024-07-05T03:26:17.785073Z",
     "shell.execute_reply": "2024-07-05T03:26:17.784149Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.779949Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {}  # パラメータの初期化\n",
    "if False:  # もしテストデータの長さが10未満の場合\n",
    "    pass;\n",
    "    params['subsample'] = 30  # サブサンプルの数を設定\n",
    "else:\n",
    "    # params['subsample'] = 2\n",
    "    params['fold'] = -1  # フォールドを設定\n",
    "\n",
    "\n",
    "params['n_epochs'] = 1  # 学習のエポック数\n",
    "params['n_lgb'] = 1  # LightGBMの数\n",
    "params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデル"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc5a964",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0\n",
    "N_FOLDS = int(params.get('n_folds', 3)); \n",
    "FOLD = int(params.get('fold', 0))\n",
    "SEED = int(params.get('seed', 3))\n",
    "SS = int(params.get('subsample', 1))\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0  # 完全に学習するかどうか\n",
    "N_FOLDS = int(params.get('n_folds', 3));  # フォールド数を設定\n",
    "FOLD = int(params.get('fold', 0))  # 現在のフォールドを設定\n",
    "SEED = int(params.get('seed', 3))  # 乱数シードを設定\n",
    "SS = int(params.get('subsample', 1))  # サブサンプルの数を設定\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)  # フォールド数、現在のフォールド、シード、サブサンプル数を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.786808Z",
     "iopub.status.busy": "2024-07-05T03:26:17.786464Z",
     "iopub.status.idle": "2024-07-05T03:26:17.798101Z",
     "shell.execute_reply": "2024-07-05T03:26:17.79707Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.786777Z"
    }
   },
   "outputs": [],
   "source": [
    "# params = {}\n",
    "FULL = params.get('fold', 0) < 0  # 完全に学習するかどうか\n",
    "N_FOLDS = int(params.get('n_folds', 3));  # フォールド数を設定\n",
    "FOLD = int(params.get('fold', 0))  # 現在のフォールドを設定\n",
    "SEED = int(params.get('seed', 3))  # 乱数シードを設定\n",
    "SS = int(params.get('subsample', 1))  # サブサンプルの数を設定\n",
    "\n",
    "print(N_FOLDS, FOLD, SEED, SS)  # フォールド数、現在のフォールド、シード、サブサンプル数を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b502ed3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]\n",
    "if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set() \n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldをインポート\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))  # フォールドを取得\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]  # IDを取得\n",
    "if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]  # サブサンプルを取得\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # 確認\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.799617Z",
     "iopub.status.busy": "2024-07-05T03:26:17.799332Z",
     "iopub.status.idle": "2024-07-05T03:26:17.824537Z",
     "shell.execute_reply": "2024-07-05T03:26:17.823635Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.799593Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldをインポート\n",
    "\n",
    "def get_folds(train): \n",
    "    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\\\n",
    "                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))  # フォールドを取得\n",
    "\n",
    "train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]  # IDを取得\n",
    "if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]  # サブサンプルを取得\n",
    "\n",
    "print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # 確認"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bb4bcf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def join_strings(x, ):\n",
    "    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x\n",
    "    return x\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def join_strings(x, ):\n",
    "    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x  # リスト内の要素を結合\n",
    "    return x  # 結合した文字列を返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.825901Z",
     "iopub.status.busy": "2024-07-05T03:26:17.825663Z",
     "iopub.status.idle": "2024-07-05T03:26:17.830321Z",
     "shell.execute_reply": "2024-07-05T03:26:17.829485Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.825881Z"
    }
   },
   "outputs": [],
   "source": [
    "def join_strings(x, ):\n",
    "    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x  # リスト内の要素を結合\n",
    "    return x  # 結合した文字列を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c7656",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def len_join_strings(x, ):\n",
    "    return len(join_strings(x).split())\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def len_join_strings(x, ):\n",
    "    return len(join_strings(x).split())  # 結合した文字列の長さを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.831869Z",
     "iopub.status.busy": "2024-07-05T03:26:17.831631Z",
     "iopub.status.idle": "2024-07-05T03:26:17.839093Z",
     "shell.execute_reply": "2024-07-05T03:26:17.838221Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.831849Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings(x, ):\n",
    "    return len(join_strings(x).split())  # 結合した文字列の長さを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcff0295",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def len_join_strings_j(x):\n",
    "    x = json.loads(x)\n",
    "    return len_join_strings(x)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def len_join_strings_j(x):\n",
    "    x = json.loads(x)  # JSON文字列をロード\n",
    "    return len_join_strings(x)  # 結合した文字列の長さを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.840214Z",
     "iopub.status.busy": "2024-07-05T03:26:17.839938Z",
     "iopub.status.idle": "2024-07-05T03:26:17.849138Z",
     "shell.execute_reply": "2024-07-05T03:26:17.848218Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.840184Z"
    }
   },
   "outputs": [],
   "source": [
    "def len_join_strings_j(x):\n",
    "    x = json.loads(x)  # JSON文字列をロード\n",
    "    return len_join_strings(x)  # 結合した文字列の長さを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6063f59f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "torch.manual_seed(datetime.datetime.now().microsecond)\n",
    "random.seed(datetime.datetime.now().microsecond)\n",
    "np.random.seed(datetime.datetime.now().microsecond)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchの乱数シードを現在のマイクロ秒で設定\n",
    "random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールの乱数シードを設定\n",
    "np.random.seed(datetime.datetime.now().microsecond)  # NumPyの乱数シードを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.850794Z",
     "iopub.status.busy": "2024-07-05T03:26:17.850445Z",
     "iopub.status.idle": "2024-07-05T03:26:17.859198Z",
     "shell.execute_reply": "2024-07-05T03:26:17.858439Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.850764Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchの乱数シードを現在のマイクロ秒で設定\n",
    "random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールの乱数シードを設定\n",
    "np.random.seed(datetime.datetime.now().microsecond)  # NumPyの乱数シードを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfb48f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# TRAIN = True and not KAGGLE\n",
    "TRAIN = False\n",
    "INFER = True # or KAGGLE \n",
    "SAVE = False\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# TRAIN = True and not KAGGLE\n",
    "TRAIN = False  # 訓練フラグを設定\n",
    "INFER = True  # 推論フラグを設定（KAGGLEの場合も推論）\n",
    "SAVE = False  # 保存フラグを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.860595Z",
     "iopub.status.busy": "2024-07-05T03:26:17.860334Z",
     "iopub.status.idle": "2024-07-05T03:26:17.873495Z",
     "shell.execute_reply": "2024-07-05T03:26:17.872508Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.860573Z"
    }
   },
   "outputs": [],
   "source": [
    "# TRAIN = True and not KAGGLE\n",
    "TRAIN = False  # 訓練フラグを設定\n",
    "INFER = True  # 推論フラグを設定（KAGGLEの場合も推論）\n",
    "SAVE = False  # 保存フラグを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1037f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb  # LightGBMをインポート\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # CountVectorizerをインポート\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:17.875032Z",
     "iopub.status.busy": "2024-07-05T03:26:17.874756Z",
     "iopub.status.idle": "2024-07-05T03:26:20.474174Z",
     "shell.execute_reply": "2024-07-05T03:26:20.473348Z",
     "shell.execute_reply.started": "2024-07-05T03:26:17.875011Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb  # LightGBMをインポート\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # CountVectorizerをインポート"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956821b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "LGB = True\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0\n",
    "INFER_LGB = not TRAIN and LGB\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "LGB = True  # LightGBMフラグを設定\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # LightGBMの訓練フラグを設定\n",
    "INFER_LGB = not TRAIN and LGB  # LightGBMの推論フラグを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:20.475474Z",
     "iopub.status.busy": "2024-07-05T03:26:20.47522Z",
     "iopub.status.idle": "2024-07-05T03:26:20.480687Z",
     "shell.execute_reply": "2024-07-05T03:26:20.479612Z",
     "shell.execute_reply.started": "2024-07-05T03:26:20.47545Z"
    }
   },
   "outputs": [],
   "source": [
    "LGB = True  # LightGBMフラグを設定\n",
    "TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # LightGBMの訓練フラグを設定\n",
    "INFER_LGB = not TRAIN and LGB  # LightGBMの推論フラグを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbb8e82",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # CountVectorizerを読み込み\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # カスタムCountVectorizerを読み込み\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:20.48293Z",
     "iopub.status.busy": "2024-07-05T03:26:20.48218Z",
     "iopub.status.idle": "2024-07-05T03:26:29.993702Z",
     "shell.execute_reply": "2024-07-05T03:26:29.992737Z",
     "shell.execute_reply.started": "2024-07-05T03:26:20.482897Z"
    }
   },
   "outputs": [],
   "source": [
    "cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # CountVectorizerを読み込み\n",
    "ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # カスタムCountVectorizerを読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2875183",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())\n",
    "    x = symlog(x)\n",
    "    return x\n",
    "\n",
    "def get_features(df):\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    v = np.hstack([\n",
    "    # pfeat, \n",
    "          afeat - bfeat, np.abs(afeat - bfeat), \n",
    "    # afeat + bfeat\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)\n",
    "    except: pass\n",
    "\n",
    "    extras = []\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)\n",
    "            \n",
    "    extras.append(df[c].str.len())\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])\n",
    "    return np.hstack([v, extras])\n",
    "    # return v\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # 対称対数変換の関数\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())  # 疎行列を密行列に変換\n",
    "    x = symlog(x)  # 対称対数変換を適用\n",
    "    return x  # 変換した値を返す\n",
    "\n",
    "def get_features(df):\n",
    "    # プロンプトに対する特徴を抽出\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    \n",
    "    # モデルAに対する特徴を抽出\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    # モデルBに対する特徴を抽出\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    # 特徴の組み合わせ\n",
    "    v = np.hstack([\n",
    "          afeat - bfeat, np.abs(afeat - bfeat),  # モデルAとBの差分\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # モデル数で割る（条件付き）\n",
    "    except: pass\n",
    "\n",
    "    extras = []  # 追加の特徴を格納するリスト\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']  # カウントする文字のリスト\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)  # 特徴量としてカウントを追加\n",
    "            \n",
    "    extras.append(df[c].str.len())  # 文字列の長さを追加\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 単語数を追加\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)  # 配列に変換\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 追加特徴の変換\n",
    "    return np.hstack([v, extras])  # 特徴を結合して返す\n",
    "    # return v\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:29.99511Z",
     "iopub.status.busy": "2024-07-05T03:26:29.994843Z",
     "iopub.status.idle": "2024-07-05T03:26:31.218575Z",
     "shell.execute_reply": "2024-07-05T03:26:31.217496Z",
     "shell.execute_reply.started": "2024-07-05T03:26:29.995087Z"
    }
   },
   "outputs": [],
   "source": [
    "def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # 対称対数変換の関数\n",
    "\n",
    "def dense(x):\n",
    "    x = np.asarray(x.astype(np.float32).todense())  # 疎行列を密行列に変換\n",
    "    x = symlog(x)  # 対称対数変換を適用\n",
    "    return x  # 変換した値を返す\n",
    "\n",
    "def get_features(df):\n",
    "    # プロンプトに対する特徴を抽出\n",
    "    pfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for v in [cvec, ccvec]\n",
    "                    for c in ['prompt', ]])\n",
    "    \n",
    "    # モデルAに対する特徴を抽出\n",
    "    afeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_a', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    # モデルBに対する特徴を抽出\n",
    "    bfeat = np.hstack([dense(v.transform(df[c])) \n",
    "                for c in ['response_b', ]\n",
    "                    for v in [cvec, ccvec]\n",
    "                ])\n",
    "    \n",
    "    # 特徴の組み合わせ\n",
    "    v = np.hstack([\n",
    "          afeat - bfeat, np.abs(afeat - bfeat),  # モデルAとBの差分\n",
    "        ])\n",
    "    try: \n",
    "        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # モデル数で割る（条件付き）\n",
    "    except: pass\n",
    "\n",
    "    extras = []  # 追加の特徴を格納するリスト\n",
    "    EXTRAS = ['\\n', '\\n\\n', '.', ' ', '\",\"']  # カウントする文字のリスト\n",
    "    for e in EXTRAS:\n",
    "        for c in ['prompt', 'response_a', 'response_b']:\n",
    "            extras.append(df[c].str.count(e).values)  # 特徴量としてカウントを追加\n",
    "            \n",
    "    extras.append(df[c].str.len())  # 文字列の長さを追加\n",
    "    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 単語数を追加\n",
    "    \n",
    "    extras = np.stack(extras, axis = 1)  # 配列に変換\n",
    "    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 追加特徴の変換\n",
    "    return np.hstack([v, extras])  # 特徴を結合して返す\n",
    "    # return v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e06353d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込み\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.220049Z",
     "iopub.status.busy": "2024-07-05T03:26:31.219725Z",
     "iopub.status.idle": "2024-07-05T03:26:31.348239Z",
     "shell.execute_reply": "2024-07-05T03:26:31.347217Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.220019Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込み"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e88ea6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "if INFER and params.get('n_lgb', 1) > 0:\n",
    "    df = test\n",
    "    yps = []; b = 1000\n",
    "    for i in range(0, len(df), b):\n",
    "        arr = get_features(df.iloc[i: i + b])\n",
    "        ypms = []\n",
    "        for model in lgb_models:\n",
    "            ypms.append(model.predict_proba(arr))\n",
    "        yps.append(np.stack(ypms).mean(0))\n",
    "        # break;\n",
    "        print('.', end = '')\n",
    "        \n",
    "        if len(yps) % 2 == 0:\n",
    "            gc.collect()\n",
    "    print()\n",
    "\n",
    "    yp = np.concatenate(yps)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "if INFER and params.get('n_lgb', 1) > 0:  # 推論が有効で、LightGBMのモデルが存在する場合\n",
    "    df = test  # テストデータを指定\n",
    "    yps = []  # 予測を保存するリスト\n",
    "    b = 1000  # バッチサイズ\n",
    "    for i in range(0, len(df), b):  # データをバッチごとに処理\n",
    "        arr = get_features(df.iloc[i: i + b])  # 特徴を取得\n",
    "        ypms = []  # 各モデルの予測を保存するリスト\n",
    "        for model in lgb_models:  # すべてのモデルに対して予測を実行\n",
    "            ypms.append(model.predict_proba(arr))  # モデルからの予測を追加\n",
    "        yps.append(np.stack(ypms).mean(0))  # モデルの平均を計算してリストに追加\n",
    "        print('.', end = '')  # 進行状況を表示\n",
    "        \n",
    "        if len(yps) % 2 == 0:  # 2つのバッチごとにガーベジコレクションを実行\n",
    "            gc.collect()\n",
    "    print()  # 改行\n",
    "\n",
    "    yp = np.concatenate(yps)  # すべての予測を結合\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.349678Z",
     "iopub.status.busy": "2024-07-05T03:26:31.349385Z",
     "iopub.status.idle": "2024-07-05T03:26:31.398862Z",
     "shell.execute_reply": "2024-07-05T03:26:31.397897Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.349653Z"
    }
   },
   "outputs": [],
   "source": [
    "if INFER and params.get('n_lgb', 1) > 0:  # 推論が有効で、LightGBMのモデルが存在する場合\n",
    "    df = test  # テストデータを指定\n",
    "    yps = []  # 予測を保存するリスト\n",
    "    b = 1000  # バッチサイズ\n",
    "    for i in range(0, len(df), b):  # データをバッチごとに処理\n",
    "        arr = get_features(df.iloc[i: i + b])  # 特徴を取得\n",
    "        ypms = []  # 各モデルの予測を保存するリスト\n",
    "        for model in lgb_models:  # すべてのモデルに対して予測を実行\n",
    "            ypms.append(model.predict_proba(arr))  # モデルからの予測を追加\n",
    "        yps.append(np.stack(ypms).mean(0))  # モデルの平均を計算してリストに追加\n",
    "        print('.', end = '')  # 進行状況を表示\n",
    "        \n",
    "        if len(yps) % 2 == 0:  # 2つのバッチごとにガーベジコレクションを実行\n",
    "            gc.collect()\n",
    "    print()  # 改行\n",
    "\n",
    "    yp = np.concatenate(yps)  # すべての予測を結合"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c05246",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "lgb_preds = yp\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "lgb_preds = yp  # LightGBMの予測を保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.400185Z",
     "iopub.status.busy": "2024-07-05T03:26:31.3999Z",
     "iopub.status.idle": "2024-07-05T03:26:31.404215Z",
     "shell.execute_reply": "2024-07-05T03:26:31.403205Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.400143Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_preds = yp  # LightGBMの予測を保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4382473",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Blend predictions\n",
    "\n",
    "$\\operatorname{preds} = 0.2 \\cdot \\operatorname{lgbm boosting preds} + 0.8 \\cdot \\operatorname{llama preds}$\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 予測のブレンド\n",
    "\n",
    "$\\operatorname{preds} = 0.2 \\cdot \\operatorname{lgbm \\ boosting \\ preds} + 0.8 \\cdot \\operatorname{llama \\ preds}$\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7e83f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "lgb_wt = 0.2 \n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "lgb_wt = 0.2  # LightGBMの重みを設定\n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測をブレンド\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.406004Z",
     "iopub.status.busy": "2024-07-05T03:26:31.405649Z",
     "iopub.status.idle": "2024-07-05T03:26:31.414649Z",
     "shell.execute_reply": "2024-07-05T03:26:31.413643Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.405973Z"
    }
   },
   "outputs": [],
   "source": [
    "lgb_wt = 0.2  # LightGBMの重みを設定\n",
    "preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測をブレンド"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ac9f3f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "out = pd.DataFrame(preds, \n",
    "                index = df.id, \n",
    "                    columns = train.columns[-3:])\n",
    "display(out.head())\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "out = pd.DataFrame(preds, \n",
    "                index = df.id,  # インデックスに元のIDを設定\n",
    "                    columns = train.columns[-3:])  # 予測列の列名を設定\n",
    "display(out.head())  # 結果を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.415752Z",
     "iopub.status.busy": "2024-07-05T03:26:31.41551Z",
     "iopub.status.idle": "2024-07-05T03:26:31.430607Z",
     "shell.execute_reply": "2024-07-05T03:26:31.42974Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.415727Z"
    }
   },
   "outputs": [],
   "source": [
    "out = pd.DataFrame(preds, \n",
    "                index = df.id,  # インデックスに元のIDを設定\n",
    "                    columns = train.columns[-3:])  # 予測列の列名を設定\n",
    "display(out.head())  # 結果を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800cfcfe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "out.to_csv('submission.csv')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "out.to_csv('submission.csv')  # 結果をCSVファイルに保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-05T03:26:31.435887Z",
     "iopub.status.busy": "2024-07-05T03:26:31.435492Z",
     "iopub.status.idle": "2024-07-05T03:26:31.444213Z",
     "shell.execute_reply": "2024-07-05T03:26:31.443233Z",
     "shell.execute_reply.started": "2024-07-05T03:26:31.435861Z"
    }
   },
   "outputs": [],
   "source": [
    "out.to_csv('submission.csv')  # 結果をCSVファイルに保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
