{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a0fc8c",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Llama-3 8bモデルをTPU(Super TPU)でトレーニングすることに焦点を当てています。特に、大規模言語モデル(LLM)のトレーニング過程を詳細に説明し、実際にトレーニングを行うための実装方法を示しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "ノートブックは、Llama-3のLPF（Low-Rank Adaptation、LoRA）と呼ばれる技術を使用して、TPU上でのモデルのトレーニングを効率化する問題に取り組んでいます。具体的には、チャットボットの応答に対するユーザーの好みを予測するためのモデルをトレーニングし、ユーザインタラクションの改善を目指しています。\n",
    "\n",
    "### 使用している手法とライブラリ\n",
    "- **ライブラリ**：\n",
    "  - `transformers`: Hugging Faceのライブラリを用いて、LLMのモデルやトークナイザーを扱っています。\n",
    "  - `torch`: PyTorchを使用して、モデルのトレーニングとテストを行います。\n",
    "  - `torch_xla`: TPUでの計算をサポートするために、XLA（Accelerated Linear Algebra）関連の機能を利用しています。\n",
    "  - `peft`: 特にLoRAに関連するパラメータの効率的な調整を行います。\n",
    "\n",
    "- **手法**:\n",
    "  - **LoRA (Low-Rank Adaptation)**: 低ランク行列の近似を使用して、モデルのトレーニングを効率的に行い、パラメータの数を削減します。\n",
    "  - **クロスエントロピー損失関数**: モデルの損失を評価するために使用されます。\n",
    "  - **コサイン学習率スケジュール**: 学習率を調整しつつトレーニングを進めるために用いられます。\n",
    "  \n",
    "ノートブックは、トレーニングデータを読み込み、データ前処理を行い、トークナイザーを使ってデータをトークナイズします。その後、モデルを定義し、TPU環境におけるデータのシャーディングとパラメータの設定を行います。最終的に、トレーニングループ内でモデルを訓練し、損失の履歴をプロットして視覚的に訓練状況を確認します。\n",
    "\n",
    "### 結論\n",
    "全体として、このノートブックはLlama-3モデルをTPUでトレーニングするための具体的な手順と実装を示し、さらなるデータやハイパーパラメータの調整によるトレーニングの最適化の可能性についても言及しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f4bb6",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連する初心者がつまずきそうな専門用語の簡単な解説を示します。\n",
    "\n",
    "1. **TPU (Tensor Processing Unit)**:\n",
    "   - Googleが開発した専用のハードウェアで、特に機械学習のトレーニングと推論を効率的に行うために設計されています。CPUやGPUよりも高速にディープラーニングモデルを処理することができます。\n",
    "\n",
    "2. **BFloat16**:\n",
    "   - GoogleのTPUで使用される数値表現の一つで、16ビットの浮動小数点数です。BFloat16は、効率的な計算を可能にしつつ、深層学習の精度を維持するために使用されます。\n",
    "\n",
    "3. **LoRA (Low-Rank Adaptation)**:\n",
    "   - 事前トレーニングされたモデルに対して、パラメータの数を抑えて微調整を行う手法です。基本的なモデルに対して追加の低ランクの行列を学習させることで、リソースを節約しつつ高い性能を実現します。\n",
    "\n",
    "4. **SPMD (Single Program Multiple Data)**:\n",
    "   - 同じプログラムが複数のデータに対して並行して実行されるプログラミングモデルです。TPUやGPUの使用において、データパラレルな計算を行う際によく用いられます。\n",
    "\n",
    "5. **シャーディング (Sharding)**:\n",
    "   - 大規模データセットやモデルを複数のデバイスに分散（シャード）して処理する技術です。これにより、計算負荷を軽減し、効率的な学習が可能になります。\n",
    "\n",
    "6. **メッシュ (Mesh)**:\n",
    "   - データプレシジョン（precision）や並列処理のために、TPUやGPUデバイスの構造を定義することに関連する概念です。通常、デバイスの形状や分割方法を決定します。\n",
    "\n",
    "7. **クロスエントロピー損失 (Cross Entropy Loss)**:\n",
    "   - 主に分類問題で使用される損失関数で、予測した確率と実際のラベルの分布間の差異を評価します。出力確率がどれだけ真のラベルに近いかを測ります。\n",
    "\n",
    "8. **アテンションマスク (Attention Mask)**:\n",
    "   - Transformerモデルにおいて、どのトークンが注意の対象であるかを示すためのマスクです。無関係なトークンを考慮しないようにするために使用されます。\n",
    "\n",
    "9. **ウィアモード (Warmup Steps)**:\n",
    "   - 学習率を徐々に増加させるための初期ステップです。急激な学習率の変化を防ぐことで、モデルの収束を安定させる役割があります。\n",
    "\n",
    "10. **トークナイザー (Tokenizer)**:\n",
    "   - テキストをモデルが処理しやすい形式（トークン）に変換するコンポーネントです。トークン化は、自然言語処理の初期ステップとして重要です。\n",
    "\n",
    "これらの用語の理解は、このJupyter Notebookの内容を深く理解し、トレーニングプロセスを効果的に進めるために役立つでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f59398d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "#  Llama-3 8b [TPU Train]\n",
    "\n",
    "Learning to train llms on tpu, Hope this will help you too!\n",
    "\n",
    "Notebook inspired from:\n",
    "\n",
    "* [LLM detect AI comp Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)\n",
    "* [DAIGT Mistral-7B TPU BFloat16 [Train]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)\n",
    "* [LLAMA 2 13B on TPU (Training)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)\n",
    "\n",
    "\n",
    "Prerequisite: Access to using llama-3\n",
    "\n",
    "Note: This is only training notebook, you can find inference notebook [here](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)\n",
    "\n",
    "Please upvote if you learn or find this helpful!\n",
    "\n",
    "# Import libs \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# Llama-3 8b [TPUトレーニング]\n",
    "\n",
    "TPU上でLLMをトレーニングする方法を学ぶことができます。これがあなたにも役立つことを願っています！\n",
    "\n",
    "ノートブックは以下からインスパイアを受けています：\n",
    "\n",
    "* [LLM検出AIコンペ Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)\n",
    "* [DAIGT Mistral-7B TPU BFloat16 [トレーニング]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)\n",
    "* [LLAMA 2 13B on TPU (トレーニング)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)\n",
    "\n",
    "前提条件：Llama-3の使用権限\n",
    "\n",
    "注意：これはトレーニング専用のノートブックです、推論ノートブックは[こちら](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)から見つけることができます。\n",
    "\n",
    "学びや役立つことがあれば、投票していただけると嬉しいです！\n",
    "\n",
    "# ライブラリのインポート\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048d8a13",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Install libs\n",
    "!pip install -qq peft==0.6.0\n",
    "!pip install -qq bitsandbytes==0.41.1\n",
    "!pip install -qq accelerate==0.24.1\n",
    "!pip install -qq transformers==4.35.0\n",
    "!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q \n",
    "!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
    "!pip uninstall -qq tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n",
    "!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# ライブラリをインストール\n",
    "!pip install -qq peft==0.6.0  # PEFTライブラリのインストール\n",
    "!pip install -qq bitsandbytes==0.41.1  # BitsAndBytesライブラリのインストール\n",
    "!pip install -qq accelerate==0.24.1  # Accelerateライブラリのインストール\n",
    "!pip install -qq transformers==4.35.0  # Transformersライブラリのインストール\n",
    "!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q  # CPU用のPyTorchのインストール\n",
    "!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q  # TPU用のTorch XLAのインストール\n",
    "!pip uninstall -qq tensorflow -y # テンソルフローをアンインストールします。これを行わないと、TFがTPUを占有し、PyTorchに対する権限エラーが発生する可能性があります。\n",
    "!cp /kaggle/input/utils-xla/spmd_util.py . # このリポジトリからファイルをコピーします: https://github.com/HeegyuKim/torch-xla-SPMD\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ライブラリをインストール\n",
    "!pip install -qq peft==0.6.0  # PEFTライブラリのインストール\n",
    "!pip install -qq bitsandbytes==0.41.1  # BitsAndBytesライブラリのインストール\n",
    "!pip install -qq accelerate==0.24.1  # Accelerateライブラリのインストール\n",
    "!pip install -qq transformers==4.35.0  # Transformersライブラリのインストール\n",
    "!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q  # CPU用のPyTorchのインストール\n",
    "!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q  # TPU用のTorch XLAのインストール\n",
    "!pip uninstall -qq tensorflow -y # テンソルフローをアンインストールします。これを行わないと、TFがTPUを占有し、PyTorchに対する権限エラーが発生する可能性があります。\n",
    "!cp /kaggle/input/utils-xla/spmd_util.py . # このリポジトリからファイルをコピーします: https://github.com/HeegyuKim/torch-xla-SPMD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0660078",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "from time import time\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.runtime as xr\n",
    "\n",
    "xr.use_spmd()\n",
    "\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "from spmd_util import partition_module\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "print(f'Torch Version: {torch.__version__}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import os  # osモジュールをインポート（オペレーティングシステムに関連する機能を提供）\n",
    "import gc  # gcモジュールをインポート（ガーベジコレクション、不要なメモリの解放を行う）\n",
    "import re  # reモジュールをインポート（正規表現を扱うための機能を提供）\n",
    "from time import time  # timeモジュールからtime関数をインポート（時間計測を行うため）\n",
    "import random  # randomモジュールをインポート（ランダムな数の生成を行う）\n",
    "import warnings  # warningsモジュールをインポート（警告メッセージを表示するため）\n",
    "import numpy as np  # NumPyライブラリをnpというエイリアスでインポート（数値計算を行うため）\n",
    "import pandas as pd  # pandasライブラリをpdというエイリアスでインポート（データ操作と分析のためのライブラリ）\n",
    "import matplotlib.pyplot as plt  # Matplotlibライブラリをpltというエイリアスでインポート（データの可視化を行うため）\n",
    "from tqdm.auto import tqdm  # tqdmライブラリから自動的にプログレスバーをインポート\n",
    "\n",
    "import torch  # PyTorchライブラリをインポート（機械学習のためのライブラリ）\n",
    "import transformers  # Hugging FaceのTransformersライブラリをインポート（NLPモデルを使用するため）\n",
    "from sklearn.metrics import accuracy_score  # scikit-learnから精度計算のための関数をインポート\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification  # 自動トークナイザーとモデルをインポート\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # PEFT関連のモジュールをインポート\n",
    "import torch.nn.functional as F  # PyTorchのニューラルネットワークの関数をFというエイリアスでインポート\n",
    "\n",
    "import torch_xla.debug.profiler as xp  # XLAのプロファイラをインポート（パフォーマンスの分析に使用）\n",
    "import torch_xla.core.xla_model as xm  # XLAモジュールをインポート（TPUのための処理をサポート）\n",
    "import torch_xla.experimental.xla_sharding as xs  # XLAのシャーディング機能をインポート（データ分散のため）\n",
    "import torch_xla.runtime as xr  # XLAの実行時機能をインポート\n",
    "\n",
    "xr.use_spmd()  # SPMD（Single Program Multiple Data）を使用する設定\n",
    "\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor  # XLAシャーディングテンソルをインポート（シャーディングされたテンソルを扱う）\n",
    "from torch_xla.experimental.xla_sharding import Mesh  # メッシュ構造をインポート（データ分散のため）\n",
    "from spmd_util import partition_module  # モジュールをパーティションするための関数をインポート\n",
    "\n",
    "tqdm.pandas()  # pandasの進捗バーを設定\n",
    "\n",
    "print(f'Torch Version: {torch.__version__}')  # 現在のPyTorchのバージョンを出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # osモジュールをインポート（オペレーティングシステムに関連する機能を提供）\n",
    "import gc  # gcモジュールをインポート（ガーベジコレクション、不要なメモリの解放を行う）\n",
    "import re  # reモジュールをインポート（正規表現を扱うための機能を提供）\n",
    "from time import time  # timeモジュールからtime関数をインポート（時間計測を行うため）\n",
    "import random  # randomモジュールをインポート（ランダムな数の生成を行う）\n",
    "import warnings  # warningsモジュールをインポート（警告メッセージを表示するため）\n",
    "import numpy as np  # NumPyライブラリをnpというエイリアスでインポート（数値計算を行うため）\n",
    "import pandas as pd  # pandasライブラリをpdというエイリアスでインポート（データ操作と分析のためのライブラリ）\n",
    "import matplotlib.pyplot as plt  # Matplotlibライブラリをpltというエイリアスでインポート（データの可視化を行うため）\n",
    "from tqdm.auto import tqdm  # tqdmライブラリから自動的にプログレスバーをインポート\n",
    "\n",
    "import torch  # PyTorchライブラリをインポート（機械学習のためのライブラリ）\n",
    "import transformers  # Hugging FaceのTransformersライブラリをインポート（NLPモデルを使用するため）\n",
    "from sklearn.metrics import accuracy_score  # scikit-learnから精度計算のための関数をインポート\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification  # 自動トークナイザーとモデルをインポート\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # PEFT関連のモジュールをインポート\n",
    "import torch.nn.functional as F  # PyTorchのニューラルネットワークの関数をFというエイリアスでインポート\n",
    "\n",
    "import torch_xla.debug.profiler as xp  # XLAのプロファイラをインポート（パフォーマンスの分析に使用）\n",
    "import torch_xla.core.xla_model as xm  # XLAモジュールをインポート（TPUのための処理をサポート）\n",
    "import torch_xla.experimental.xla_sharding as xs  # XLAのシャーディング機能をインポート（データ分散のため）\n",
    "import torch_xla.runtime as xr  # XLAの実行時機能をインポート\n",
    "\n",
    "xr.use_spmd()  # SPMD（Single Program Multiple Data）を使用する設定\n",
    "\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor  # XLAシャーディングテンソルをインポート（シャーディングされたテンソルを扱う）\n",
    "from torch_xla.experimental.xla_sharding import Mesh  # メッシュ構造をインポート（データ分散のため）\n",
    "from spmd_util import partition_module  # モジュールをパーティションするための関数をインポート\n",
    "\n",
    "tqdm.pandas()  # pandasの進捗バーを設定\n",
    "\n",
    "print(f'Torch Version: {torch.__version__}')  # 現在のPyTorchのバージョンを出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3450a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Configs\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 設定\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d70c6e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    NUM_EPOCHS = 1\n",
    "    BATCH_SIZE = 16\n",
    "    DROPOUT = 0.05 \n",
    "    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    SEED = 2024 \n",
    "    MAX_LENGTH = 1024 \n",
    "    NUM_WARMUP_STEPS = 128\n",
    "    LR_MAX = 5e-5 \n",
    "    NUM_LABELS = 3 \n",
    "    LORA_RANK = 4\n",
    "    LORA_ALPHA = 8\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']\n",
    "    \n",
    "DEVICE = xm.xla_device() # Initialize TPU Device\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class CFG:  # 設定を管理するクラス\n",
    "    NUM_EPOCHS = 1  # エポック数（モデルをトレーニングする回数）\n",
    "    BATCH_SIZE = 16  # バッチサイズ（一度に処理するデータの数）\n",
    "    DROPOUT = 0.05  # ドロップアウト率（過学習を防ぐための手法）\n",
    "    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # 使用するモデルのパス\n",
    "    SEED = 2024  # 再現性を確保するための乱数の種\n",
    "    MAX_LENGTH = 1024  # 最大シーケンス長（モデルに入力する最大の長さ）\n",
    "    NUM_WARMUP_STEPS = 128  # ウォームアップステップの数（学習率を調整するための初期ステップ）\n",
    "    LR_MAX = 5e-5  # 最大学習率（モデルをトレーニングする際の最大の学習率）\n",
    "    NUM_LABELS = 3  # ラベルの数（分類するクラスの数）\n",
    "    LORA_RANK = 4  # LoRAのランク（低ランク近似のためのパラメーター）\n",
    "    LORA_ALPHA = 8  # LoRAのアルファ値（スケーリングファクター）\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモジュールのリスト\n",
    "    \n",
    "DEVICE = xm.xla_device()  # TPUデバイスを初期化する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:  # 設定を管理するクラス\n",
    "    NUM_EPOCHS = 1  # エポック数（モデルをトレーニングする回数）\n",
    "    BATCH_SIZE = 16  # バッチサイズ（一度に処理するデータの数）\n",
    "    DROPOUT = 0.05  # ドロップアウト率（過学習を防ぐための手法）\n",
    "    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # 使用するモデルのパス\n",
    "    SEED = 2024  # 再現性を確保するための乱数の種\n",
    "    MAX_LENGTH = 1024  # 最大シーケンス長（モデルに入力する最大の長さ）\n",
    "    NUM_WARMUP_STEPS = 128  # ウォームアップステップの数（学習率を調整するための初期ステップ）\n",
    "    LR_MAX = 5e-5  # 最大学習率（モデルをトレーニングする際の最大の学習率）\n",
    "    NUM_LABELS = 3  # ラベルの数（分類するクラスの数）\n",
    "    LORA_RANK = 4  # LoRAのランク（低ランク近似のためのパラメーター）\n",
    "    LORA_ALPHA = 8  # LoRAのアルファ値（スケーリングファクター）\n",
    "    LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモジュールのリスト\n",
    "    \n",
    "DEVICE = xm.xla_device()  # TPUデバイスを初期化する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44c5a8a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def set_seeds(seed):\n",
    "    \"\"\"Set seeds for reproducibility \"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    # Set seed for all TPU cores\n",
    "    xm.set_rng_state(seed, device=xm.xla_device())  \n",
    "\n",
    "set_seeds(seed=CFG.SEED)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def set_seeds(seed):  # 再現性を確保するための乱数の種を設定する関数\n",
    "    \"\"\"再現性を確保するために種を設定します\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # Pythonのハッシュシードを設定\n",
    "    random.seed(seed)  # randomモジュールの種を設定\n",
    "    np.random.seed(seed)  # NumPyの乱数生成器の種を設定\n",
    "    torch.manual_seed(seed)  # PyTorchの手動シードを設定\n",
    "    if torch.cuda.is_available():  # CUDAが利用可能な場合\n",
    "        torch.cuda.manual_seed(seed)  # GPUのシードを設定\n",
    "        torch.cuda.manual_seed_all(seed)  # 全てのGPUのシードを設定\n",
    "        \n",
    "    # すべてのTPUコアに対してシードを設定\n",
    "    xm.set_rng_state(seed, device=xm.xla_device())  \n",
    "\n",
    "set_seeds(seed=CFG.SEED)  # 定義した関数を呼び出し、設定したシードを使用して乱数の種を設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed):  # 再現性を確保するための乱数の種を設定する関数\n",
    "    \"\"\"再現性を確保するために種を設定します\"\"\"\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # Pythonのハッシュシードを設定\n",
    "    random.seed(seed)  # randomモジュールの種を設定\n",
    "    np.random.seed(seed)  # NumPyの乱数生成器の種を設定\n",
    "    torch.manual_seed(seed)  # PyTorchの手動シードを設定\n",
    "    if torch.cuda.is_available():  # CUDAが利用可能な場合\n",
    "        torch.cuda.manual_seed(seed)  # GPUのシードを設定\n",
    "        torch.cuda.manual_seed_all(seed)  # 全てのGPUのシードを設定\n",
    "        \n",
    "    # すべてのTPUコアに対してシードを設定\n",
    "    xm.set_rng_state(seed, device=xm.xla_device())  \n",
    "\n",
    "set_seeds(seed=CFG.SEED)  # 定義した関数を呼び出し、設定したシードを使用して乱数の種を設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f70e909",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Tokenizer\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トークナイザー\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d21dc7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = 'right'\n",
    "tokenizer.add_eos_token = True\n",
    "\n",
    "# save tokenizer to load offline during inference\n",
    "tokenizer.save_pretrained('tokenizer')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)  # 設定したモデル名から事前トレーニングされたトークナイザーをロード\n",
    "tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークン（eos_token）に設定\n",
    "tokenizer.padding_side = 'right'  # パディングを右側に追加\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加する設定\n",
    "\n",
    "# 推論時にオフラインでロードできるようにトークナイザーを保存\n",
    "tokenizer.save_pretrained('tokenizer')  # トークナイザーを指定したディレクトリに保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)  # 設定したモデル名から事前トレーニングされたトークナイザーをロード\n",
    "tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークン（eos_token）に設定\n",
    "tokenizer.padding_side = 'right'  # パディングを右側に追加\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加する設定\n",
    "\n",
    "# 推論時にオフラインでロードできるようにトークナイザーを保存\n",
    "tokenizer.save_pretrained('tokenizer')  # トークナイザーを指定したディレクトリに保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2389f787",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Utility function giving token length\n",
    "def get_token_lengths(texts):\n",
    "    # tokenize and receive input_ids for reach text\n",
    "    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']\n",
    "    # return length of inputs_ids for each text\n",
    "    return [len(t) for t in input_ids]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# テキストのトークン長を取得するユーティリティ関数\n",
    "def get_token_lengths(texts):  # テキストのリストを引数に取る関数\n",
    "    # テキストをトークナイズし、各テキストに対するinput_idsを取得\n",
    "    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']  \n",
    "    # 各テキストのinput_idsの長さを返す\n",
    "    return [len(t) for t in input_ids]  # 各トークンの長さをリストとして返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# テキストのトークン長を取得するユーティリティ関数\n",
    "def get_token_lengths(texts):  # テキストのリストを引数に取る関数\n",
    "    # テキストをトークナイズし、各テキストに対するinput_idsを取得\n",
    "    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']  \n",
    "    # 各テキストのinput_idsの長さを返す\n",
    "    return [len(t) for t in input_ids]  # 各トークンの長さをリストとして返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced86c2e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Prepare train\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トレーニングの準備\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f963a28",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "# Drop 'Null' for training\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")\n",
    "print('Total train samples: ', len(train))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込む\n",
    "\n",
    "def process(input_str):  # 入力文字列を処理する関数\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を取り除く\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 各文をトリミングし、リストに分割\n",
    "    return ' '.join(sentences)  # 文をスペースで結合して返す\n",
    "\n",
    "# 各列に対してprocess関数を適用\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)  # 'prompt'列を処理\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)  # 'response_a'列を処理\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)  # 'response_b'列を処理\n",
    "\n",
    "# トレーニングのために'Null'をドロップ\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 'response_a'と'response_b'が両方'null'の行のインデックスを取得\n",
    "train.drop(indexes, inplace=True)  # 取得したインデックスの行を削除\n",
    "train.reset_index(inplace=True, drop=True)  # インデックスをリセット\n",
    "\n",
    "print(f\"合計 {len(indexes)} 行のNullレスポンスが削除されました\")\n",
    "print('トレーニングサンプルの合計: ', len(train))  # トレーニングサンプルの数を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込む\n",
    "\n",
    "def process(input_str):  # 入力文字列を処理する関数\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を取り除く\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 各文をトリミングし、リストに分割\n",
    "    return ' '.join(sentences)  # 文をスペースで結合して返す\n",
    "\n",
    "# 各列に対してprocess関数を適用\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)  # 'prompt'列を処理\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)  # 'response_a'列を処理\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)  # 'response_b'列を処理\n",
    "\n",
    "# トレーニングのために'Null'をドロップ\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 'response_a'と'response_b'が両方'null'の行のインデックスを取得\n",
    "train.drop(indexes, inplace=True)  # 取得したインデックスの行を削除\n",
    "train.reset_index(inplace=True, drop=True)  # インデックスをリセット\n",
    "\n",
    "print(f\"合計 {len(indexes)} 行のNullレスポンスが削除されました\")\n",
    "print('トレーニングサンプルの合計: ', len(train))  # トレーニングサンプルの数を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40730628",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train.head(5)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train.head(5)  # トレーニングデータの最初の5行を表示する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(5)  # トレーニングデータの最初の5行を表示する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d594eb59",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']\n",
    "print(train['text'][4])\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']  \n",
    "# トレーニング用のテキストを形成するために、各列のデータを結合して新しい'text'列を作成\n",
    "\n",
    "print(train['text'][4])  # 'text'列の5行目（インデックスは4）の内容を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['text'] = 'User prompt: ' + train['prompt'] +  '\\n\\nModel A :\\n' + train['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + train['response_b']  \n",
    "# トレーニング用のテキストを形成するために、各列のデータを結合して新しい'text'列を作成\n",
    "\n",
    "print(train['text'][4])  # 'text'列の5行目（インデックスは4）の内容を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a011fab",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Train with only take 50% train dataset\n",
    "train = train[:int(len(train) * 0.5)]\n",
    "\n",
    "train.loc[:, 'token_count'] = get_token_lengths(train['text'])\n",
    "\n",
    "# prepare label for model\n",
    "train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)\n",
    "\n",
    "# Display data\n",
    "display(train.head())\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# トレーニングにはデータセットの50％のみを使用\n",
    "train = train[:int(len(train) * 0.5)]  # トレーニングデータの最初の50％を選択\n",
    "\n",
    "train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # 'text'列のトークン数を計算し、'token_count'列に保存\n",
    "\n",
    "# モデル用のラベルを準備\n",
    "train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # 勝者モデルのインデックスをラベルとして設定\n",
    "\n",
    "# データを表示\n",
    "display(train.head())  # トレーニングデータの最初の数行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングにはデータセットの50％のみを使用\n",
    "train = train[:int(len(train) * 0.5)]  # トレーニングデータの最初の50％を選択\n",
    "\n",
    "train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # 'text'列のトークン数を計算し、'token_count'列に保存\n",
    "\n",
    "# モデル用のラベルを準備\n",
    "train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # 勝者モデルのインデックスをラベルとして設定\n",
    "\n",
    "# データを表示\n",
    "display(train.head())  # トレーニングデータの最初の数行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6b3d3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train.label.value_counts()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train.label.value_counts()  # 各ラベルの出現頻度をカウントして表示する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.label.value_counts()  # 各ラベルの出現頻度をカウントして表示する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fd8e0c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# token Count\n",
    "display(train['token_count'].describe().to_frame().astype(int))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# トークン数の概要を表示\n",
    "display(train['token_count'].describe().to_frame().astype(int))  # 'token_count'列の統計情報を整数型のデータフレームとして表示する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トークン数の概要を表示\n",
    "display(train['token_count'].describe().to_frame().astype(int))  # 'token_count'列の統計情報を整数型のデータフレームとして表示する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a54016",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# get length of tokens which covers 90% of data, we'll still take 1024 length!\n",
    "np.percentile(train['token_count'], 90)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# データの90％をカバーするトークンの長さを取得しますが、1024の長さを維持します！\n",
    "np.percentile(train['token_count'], 90)  # 'token_count'列の90パーセンタイルを計算して返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データの90％をカバーするトークンの長さを取得しますが、1024の長さを維持します！\n",
    "np.percentile(train['token_count'], 90)  # 'token_count'列の90パーセンタイルを計算して返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756dc520",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トークナイズ\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661b9c9c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Tokenize Data\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(), \n",
    "    padding='max_length', \n",
    "    max_length=CFG.MAX_LENGTH, \n",
    "    truncation=True, \n",
    "    return_tensors='np')\n",
    "\n",
    "# Input IDs are the token IDs\n",
    "INPUT_IDS = tokens['input_ids']\n",
    "# Attention Masks to Ignore Padding Tokens\n",
    "ATTENTION_MASKS = tokens['attention_mask']\n",
    "# Label of Texts\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "\n",
    "print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')\n",
    "print(f'LABELS shape: {LABELS.shape}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# データをトークナイズ\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(),  # トークナイズするテキストのリスト\n",
    "    padding='max_length',  # 最大長にパディング\n",
    "    max_length=CFG.MAX_LENGTH,  # 設定した最大長を使用\n",
    "    truncation=True,  # 超過するトークンを切り捨てる\n",
    "    return_tensors='np'  # NumPy形式で返す\n",
    ")\n",
    "\n",
    "# 入力IDはトークンIDに対応\n",
    "INPUT_IDS = tokens['input_ids']  # トークンIDを取得\n",
    "# パディングトークンを無視するためのアテンションマスク\n",
    "ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得\n",
    "# テキストのラベル\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values  # 各テキストのラベルを取得\n",
    "\n",
    "print(f'入力IDの形状: {INPUT_IDS.shape}, アテンションマスクの形状: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を出力\n",
    "print(f'ラベルの形状: {LABELS.shape}')  # ラベルの形状を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをトークナイズ\n",
    "tokens = tokenizer(\n",
    "    train['text'].tolist(),  # トークナイズするテキストのリスト\n",
    "    padding='max_length',  # 最大長にパディング\n",
    "    max_length=CFG.MAX_LENGTH,  # 設定した最大長を使用\n",
    "    truncation=True,  # 超過するトークンを切り捨てる\n",
    "    return_tensors='np'  # NumPy形式で返す\n",
    ")\n",
    "\n",
    "# 入力IDはトークンIDに対応\n",
    "INPUT_IDS = tokens['input_ids']  # トークンIDを取得\n",
    "# パディングトークンを無視するためのアテンションマスク\n",
    "ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得\n",
    "# テキストのラベル\n",
    "LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values  # 各テキストのラベルを取得\n",
    "\n",
    "print(f'入力IDの形状: {INPUT_IDS.shape}, アテンションマスクの形状: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を出力\n",
    "print(f'ラベルの形状: {LABELS.shape}')  # ラベルの形状を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4b5db5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def train_dataset(batch_size):\n",
    "    N_SAMPLES = LABELS.shape[0]\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))\n",
    "    while True:\n",
    "        # Shuffle Indices\n",
    "        np.random.shuffle(IDXS)\n",
    "        # Iterate Over All Indices Once\n",
    "        for idxs in IDXS.reshape(-1, batch_size):\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)\n",
    "            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # Multi-label output\n",
    "            \n",
    "            # Shard Over TPU Nodes if applicable (you need to define mesh appropriately)\n",
    "            xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "            xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "            xs.mark_sharding(labels, mesh, (0, 1))\n",
    "            \n",
    "            yield input_ids, attention_mask, labels\n",
    "\n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def train_dataset(batch_size):  # トレーニング用データセットを生成する関数\n",
    "    N_SAMPLES = LABELS.shape[0]  # サンプルの数を取得\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))  # バッチサイズで割り切れるインデックスを作成\n",
    "    while True:  # 無限ループでデータを生成\n",
    "        # インデックスをシャッフル\n",
    "        np.random.shuffle(IDXS)  \n",
    "        # すべてのインデックスを一度だけ反復\n",
    "        for idxs in IDXS.reshape(-1, batch_size):  # バッチサイズのインデックスで形を変更\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)  # 入力IDをテンソルにしてデバイスに移動\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)  # アテンションマスクをテンソルにしてデバイスに移動\n",
    "            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # ラベルをテンソルにしてデバイスに移動（マルチラベル出力）\n",
    "            \n",
    "            # TPUノードに対してシャーディングを行う（メッシュを適切に定義する必要があります）\n",
    "            xs.mark_sharding(input_ids, mesh, (0, 1))  # 入力IDのシャーディングをマーク\n",
    "            xs.mark_sharding(attention_mask, mesh, (0, 1))  # アテンションマスクのシャーディングをマーク\n",
    "            xs.mark_sharding(labels, mesh, (0, 1))  # ラベルのシャーディングをマーク\n",
    "            \n",
    "            yield input_ids, attention_mask, labels  # 入力ID、アテンションマスク、ラベルを返す\n",
    "\n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)  # 定義したバッチサイズでトレーニングデータセットを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dataset(batch_size):  # トレーニング用データセットを生成する関数\n",
    "    N_SAMPLES = LABELS.shape[0]  # サンプルの数を取得\n",
    "    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))  # バッチサイズで割り切れるインデックスを作成\n",
    "    while True:  # 無限ループでデータを生成\n",
    "        # インデックスをシャッフル\n",
    "        np.random.shuffle(IDXS)  \n",
    "        # すべてのインデックスを一度だけ反復\n",
    "        for idxs in IDXS.reshape(-1, batch_size):  # バッチサイズのインデックスで形を変更\n",
    "            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)  # 入力IDをテンソルにしてデバイスに移動\n",
    "            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)  # アテンションマスクをテンソルにしてデバイスに移動\n",
    "            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # ラベルをテンソルにしてデバイスに移動（マルチラベル出力）\n",
    "            \n",
    "            # TPUノードに対してシャーディングを行う（メッシュを適切に定義する必要があります）\n",
    "            xs.mark_sharding(input_ids, mesh, (0, 1))  # 入力IDのシャーディングをマーク\n",
    "            xs.mark_sharding(attention_mask, mesh, (0, 1))  # アテンションマスクのシャーディングをマーク\n",
    "            xs.mark_sharding(labels, mesh, (0, 1))  # ラベルのシャーディングをマーク\n",
    "            \n",
    "            yield input_ids, attention_mask, labels  # 入力ID、アテンションマスク、ラベルを返す\n",
    "\n",
    "TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)  # 定義したバッチサイズでトレーニングデータセットを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da447ec6",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load Model\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# モデルをロード\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69da6c2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Load model for classification with 3 target label\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,\n",
    "    num_labels=CFG.NUM_LABELS,\n",
    "    torch_dtype=torch.bfloat16)\n",
    "\n",
    "base_model.config.pretraining_tp = 1 \n",
    "\n",
    "# Assign Padding TOKEN\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 3つのターゲットラベルの分類用モデルをロード\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,  # 設定したモデル名からロード\n",
    "    num_labels=CFG.NUM_LABELS,  # ラベルの数を設定\n",
    "    torch_dtype=torch.bfloat16)  # 使用するデータ型を指定\n",
    "\n",
    "base_model.config.pretraining_tp = 1  # プリトレーニングのTPを設定\n",
    "\n",
    "# パディングトークンのIDを設定\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id  # トークナイザーのパディングトークンIDをモデルに設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3つのターゲットラベルの分類用モデルをロード\n",
    "base_model = LlamaForSequenceClassification.from_pretrained(\n",
    "    CFG.MODEL_NAME,  # 設定したモデル名からロード\n",
    "    num_labels=CFG.NUM_LABELS,  # ラベルの数を設定\n",
    "    torch_dtype=torch.bfloat16)  # 使用するデータ型を指定\n",
    "\n",
    "base_model.config.pretraining_tp = 1  # プリトレーニングのTPを設定\n",
    "\n",
    "# パディングトークンのIDを設定\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id  # トークナイザーのパディングトークンIDをモデルに設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c1ff3b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Low-Rank Adaptation [LORA]\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 低ランク適応 [LORA]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ca9f4",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(\n",
    "    r=CFG.LORA_RANK,  # the dimension of the low-rank matrices\n",
    "    lora_alpha = CFG.LORA_ALPHA, # scaling factor for LoRA activations vs pre-trained weight activations\n",
    "    lora_dropout= CFG.DROPOUT, \n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=CFG.LORA_MODULES ) # Only Use Output and Values Projection\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "lora_config = LoraConfig(  # LoRAの設定を定義\n",
    "    r=CFG.LORA_RANK,  # 低ランクマトリックスの次元\n",
    "    lora_alpha=CFG.LORA_ALPHA,  # LoRAアクティベーションと事前トレーニングされた重みアクティベーションのスケーリングファクター\n",
    "    lora_dropout=CFG.DROPOUT,  # ドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=False,  # 推論モードの設定\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類（シーケンス分類）\n",
    "    target_modules=CFG.LORA_MODULES  # 出力と値の投影のみを使用\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(  # LoRAの設定を定義\n",
    "    r=CFG.LORA_RANK,  # 低ランクマトリックスの次元\n",
    "    lora_alpha=CFG.LORA_ALPHA,  # LoRAアクティベーションと事前トレーニングされた重みアクティベーションのスケーリングファクター\n",
    "    lora_dropout=CFG.DROPOUT,  # ドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=False,  # 推論モードの設定\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類（シーケンス分類）\n",
    "    target_modules=CFG.LORA_MODULES  # 出力と値の投影のみを使用\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2034d394",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Create LoRa Model\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "# Trainable Parameters\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# LoRAモデルを作成\n",
    "model = get_peft_model(base_model, lora_config)  # 基本モデルからLoRAモデルを取得\n",
    "# 訓練可能なパラメーターを表示\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメーターの数を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRAモデルを作成\n",
    "model = get_peft_model(base_model, lora_config)  # 基本モデルからLoRAモデルを取得\n",
    "# 訓練可能なパラメーターを表示\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメーターの数を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebc634",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Number of TPU Nodes\n",
    "num_devices = xr.global_runtime_device_count()\n",
    "mesh_shape = (1, num_devices, 1)\n",
    "device_ids = np.array(range(num_devices))\n",
    "mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
    "# distribute model\n",
    "partition_module(model, mesh)\n",
    "\n",
    "print(f'num_devices: {num_devices}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# TPUノードの数を取得\n",
    "num_devices = xr.global_runtime_device_count()  # 使用可能なTPUデバイスの数を取得\n",
    "mesh_shape = (1, num_devices, 1)  # メッシュの形状を定義\n",
    "device_ids = np.array(range(num_devices))  # デバイスIDの配列を作成\n",
    "mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))  # メッシュを作成（データ並列、フルモデルの分散、モデル並列）\n",
    "\n",
    "# モデルを分散させる\n",
    "partition_module(model, mesh)  # モジュールをメッシュに基づいてパーティショニング\n",
    "\n",
    "print(f'デバイスの数: {num_devices}')  # 使用するデバイスの数を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPUノードの数を取得\n",
    "num_devices = xr.global_runtime_device_count()  # 使用可能なTPUデバイスの数を取得\n",
    "mesh_shape = (1, num_devices, 1)  # メッシュの形状を定義\n",
    "device_ids = np.array(range(num_devices))  # デバイスIDの配列を作成\n",
    "mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))  # メッシュを作成（データ並列、フルモデルの分散、モデル並列）\n",
    "\n",
    "# モデルを分散させる\n",
    "partition_module(model, mesh)  # モジュールをメッシュに基づいてパーティショニング\n",
    "\n",
    "print(f'デバイスの数: {num_devices}')  # 使用するデバイスの数を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d8e86",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Verfy The Trainable Layers\n",
    "MODEL_LAYERS_ROWS = []\n",
    "TRAINABLE_PARAMS = []\n",
    "N_TRAINABLE_PARAMS = 0\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    # Layer Parameter Count\n",
    "    n_parameters = int(torch.prod(torch.tensor(param.shape)))\n",
    "    # Only Trainable Layers\n",
    "    if param.requires_grad:\n",
    "        # Add Layer Information\n",
    "        MODEL_LAYERS_ROWS.append({\n",
    "            'param': n_parameters,\n",
    "            'name': name,\n",
    "            'dtype': param.data.dtype,\n",
    "        })\n",
    "        # Append Trainable Parameter\n",
    "        TRAINABLE_PARAMS.append({ 'params': param })\n",
    "        # Add Number Of Trainable Parameters\"\n",
    "        N_TRAINABLE_PARAMS += n_parameters\n",
    "        \n",
    "display(pd.DataFrame(MODEL_LAYERS_ROWS))\n",
    "\n",
    "print(f\"\"\"\n",
    "===============================\n",
    "N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}\n",
    "N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}\n",
    "===============================\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 学習可能なレイヤーを確認\n",
    "MODEL_LAYERS_ROWS = []  # モデルのレイヤー情報を格納するリスト\n",
    "TRAINABLE_PARAMS = []  # 学習可能なパラメータを格納するリスト\n",
    "N_TRAINABLE_PARAMS = 0  # 学習可能なパラメータの合計数\n",
    "\n",
    "for name, param in model.named_parameters():  # モデルの各パラメータについてループ\n",
    "    # レイヤーのパラメータ数を計算\n",
    "    n_parameters = int(torch.prod(torch.tensor(param.shape)))  \n",
    "    # 学習可能なレイヤーのみ確認\n",
    "    if param.requires_grad:  \n",
    "        # レイヤー情報を追加\n",
    "        MODEL_LAYERS_ROWS.append({\n",
    "            'param': n_parameters,  # パラメータ数\n",
    "            'name': name,  # レイヤー名\n",
    "            'dtype': param.data.dtype,  # データ型\n",
    "        })\n",
    "        # 学習可能なパラメータを追加\n",
    "        TRAINABLE_PARAMS.append({'params': param})  \n",
    "        # 学習可能なパラメータの数を増加\n",
    "        N_TRAINABLE_PARAMS += n_parameters\n",
    "        \n",
    "display(pd.DataFrame(MODEL_LAYERS_ROWS))  # レイヤー情報のデータフレームを表示\n",
    "\n",
    "print(f\"\"\"\n",
    "===============================\n",
    "学習可能なパラメータの数: {N_TRAINABLE_PARAMS:,}\n",
    "学習可能なレイヤーの数: {len(TRAINABLE_PARAMS)}\n",
    "===============================\n",
    "\"\"\")  # 学習可能なパラメータの総数とレイヤーの数を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習可能なレイヤーを確認\n",
    "MODEL_LAYERS_ROWS = []  # モデルのレイヤー情報を格納するリスト\n",
    "TRAINABLE_PARAMS = []  # 学習可能なパラメータを格納するリスト\n",
    "N_TRAINABLE_PARAMS = 0  # 学習可能なパラメータの合計数\n",
    "\n",
    "for name, param in model.named_parameters():  # モデルの各パラメータについてループ\n",
    "    # レイヤーのパラメータ数を計算\n",
    "    n_parameters = int(torch.prod(torch.tensor(param.shape)))  \n",
    "    # 学習可能なレイヤーのみ確認\n",
    "    if param.requires_grad:  \n",
    "        # レイヤー情報を追加\n",
    "        MODEL_LAYERS_ROWS.append({\n",
    "            'param': n_parameters,  # パラメータ数\n",
    "            'name': name,  # レイヤー名\n",
    "            'dtype': param.data.dtype,  # データ型\n",
    "        })\n",
    "        # 学習可能なパラメータを追加\n",
    "        TRAINABLE_PARAMS.append({'params': param})  \n",
    "        # 学習可能なパラメータの数を増加\n",
    "        N_TRAINABLE_PARAMS += n_parameters\n",
    "        \n",
    "display(pd.DataFrame(MODEL_LAYERS_ROWS))  # レイヤー情報のデータフレームを表示\n",
    "\n",
    "print(f\"\"\"\n",
    "===============================\n",
    "学習可能なパラメータの数: {N_TRAINABLE_PARAMS:,}\n",
    "学習可能なレイヤーの数: {len(TRAINABLE_PARAMS)}\n",
    "===============================\n",
    "\"\"\")  # 学習可能なパラメータの総数とレイヤーの数を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053ac2d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Training\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トレーニング\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872d20c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# LR & Optimizer\n",
    "N_SAMPLES = len(train)\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE\n",
    "\n",
    "OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)\n",
    "\n",
    "# Cosine Learning Rate With Warmup\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)\n",
    "\n",
    "print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 学習率 & オプティマイザーの設定\n",
    "N_SAMPLES = len(train)  # トレーニングサンプルの数を取得\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE  # エポックごとのステップ数を計算\n",
    "\n",
    "OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)  # AdamWオプティマイザーを設定\n",
    "\n",
    "# コサイン学習率スケジュール（ウォームアップ付き）\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,  # 使用するオプティマイザー\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,  # ウォームアップのステップ数\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)  # 総トレーニングステップ数\n",
    "\n",
    "print(f'バッチサイズ: {CFG.BATCH_SIZE}, サンプル数: {N_SAMPLES}, エポックごとのステップ数: {STEPS_PER_EPOCH}')  # バッチサイズ、サンプル数、エポックごとのステップ数を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習率 & オプティマイザーの設定\n",
    "N_SAMPLES = len(train)  # トレーニングサンプルの数を取得\n",
    "STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE  # エポックごとのステップ数を計算\n",
    "\n",
    "OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)  # AdamWオプティマイザーを設定\n",
    "\n",
    "# コサイン学習率スケジュール（ウォームアップ付き）\n",
    "lr_scheduler = transformers.get_cosine_schedule_with_warmup(\n",
    "    optimizer=OPTIMIZER,  # 使用するオプティマイザー\n",
    "    num_warmup_steps=CFG.NUM_WARMUP_STEPS,  # ウォームアップのステップ数\n",
    "    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)  # 総トレーニングステップ数\n",
    "\n",
    "print(f'バッチサイズ: {CFG.BATCH_SIZE}, サンプル数: {N_SAMPLES}, エポックごとのステップ数: {STEPS_PER_EPOCH}')  # バッチサイズ、サンプル数、エポックごとのステップ数を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2070049c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Set the data type for the optimizer's state (e.g., momentum buffers)\n",
    "for state in OPTIMIZER.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:\n",
    "            state[v] = v.to(dtype=torch.float32)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# オプティマイザーの状態（例：モーメンタムバッファ）に対するデータ型を設定\n",
    "for state in OPTIMIZER.state.values():  # オプティマイザーのすべての状態についてループ\n",
    "    for k, v in state.items():  # 各状態のキーと値についてループ\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:  # テンソルであり、データ型がfloat32でない場合\n",
    "            state[v] = v.to(dtype=torch.float32)  # テンソルのデータ型をfloat32に変換\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# オプティマイザーの状態（例：モーメンタムバッファ）に対するデータ型を設定\n",
    "for state in OPTIMIZER.state.values():  # オプティマイザーのすべての状態についてループ\n",
    "    for k, v in state.items():  # 各状態のキーと値についてループ\n",
    "        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:  # テンソルであり、データ型がfloat32でない場合\n",
    "            state[v] = v.to(dtype=torch.float32)  # テンソルのデータ型をfloat32に変換"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418eff0d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "\n",
    "print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')\n",
    "print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')\n",
    "print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "input_ids, attention_mask, labels = next(TRAIN_DATASET)  # トレーニングデータセットから次のバッチを取得\n",
    "\n",
    "print(f'入力IDの形状: {input_ids.shape}, データ型: {input_ids.dtype}')  # 入力IDの形状とデータ型を出力\n",
    "print(f'アテンションマスクの形状: {attention_mask.shape}, データ型: {attention_mask.dtype}')  # アテンションマスクの形状とデータ型を出力\n",
    "print(f'ラベルの形状: {labels.shape}, データ型: {labels.dtype}')  # ラベルの形状とデータ型を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask, labels = next(TRAIN_DATASET)  # トレーニングデータセットから次のバッチを取得\n",
    "\n",
    "print(f'入力IDの形状: {input_ids.shape}, データ型: {input_ids.dtype}')  # 入力IDの形状とデータ型を出力\n",
    "print(f'アテンションマスクの形状: {attention_mask.shape}, データ型: {attention_mask.dtype}')  # アテンションマスクの形状とデータ型を出力\n",
    "print(f'ラベルの形状: {labels.shape}, データ型: {labels.dtype}')  # ラベルの形状とデータ型を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed189206",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "# Dummy Prediction\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    \n",
    "print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "# ダミー予測\n",
    "with torch.no_grad():  # 勾配計算を無効にする\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # モデルの出力を取得\n",
    "    \n",
    "print(f'ロジット: {outputs.logits}, データ型: {outputs.logits.dtype}')  # ロジットの値とデータ型を出力\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# ダミー予測\n",
    "with torch.no_grad():  # 勾配計算を無効にする\n",
    "    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # モデルの出力を取得\n",
    "    \n",
    "print(f'ロジット: {outputs.logits}, データ型: {outputs.logits.dtype}')  # ロジットの値とデータ型を出力"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07420a7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Put Model In Train Mode\n",
    "model.train()\n",
    "\n",
    "# Loss Function, Cross Entropy\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# モデルをトレーニングモードに設定\n",
    "model.train()  # モデルをトレーニングモードに切り替え\n",
    "\n",
    "# 損失関数、クロスエントロピー\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)  # クロスエントロピー損失を定義し、データ型をfloat32に設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルをトレーニングモードに設定\n",
    "model.train()  # モデルをトレーニングモードに切り替え\n",
    "\n",
    "# 損失関数、クロスエントロピー\n",
    "LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)  # クロスエントロピー損失を定義し、データ型をfloat32に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebb64ff",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time()\n",
    "warnings.filterwarnings(\"error\")\n",
    "METRICS = {\n",
    "    'loss': [],\n",
    "    'accuracy': {'y_true': [], 'y_pred': [] }}\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCHS)):\n",
    "    ste = time()\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Zero Out Gradients\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        # Get Batch\n",
    "        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "        \n",
    "        # Forward Pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "       \n",
    "        # Logits Float32\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "        \n",
    "        # Backward Pass\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))\n",
    "        loss.backward()\n",
    "        \n",
    "        # optimizer step\n",
    "        OPTIMIZER.step()\n",
    "        xm.mark_step()\n",
    "        \n",
    "        # Update Learning Rate Scheduler\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Update Metrics And Progress Bar\n",
    "        METRICS['loss'].append(float(loss))\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()\n",
    "        \n",
    "        if (step + 1) % 200 == 0:  \n",
    "            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])\n",
    "            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n",
    "                                                               METRICS['accuracy']['y_pred']))\n",
    "            lr = OPTIMIZER.param_groups[0]['lr']\n",
    "            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            print(f'\\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')\n",
    "    \n",
    "    print(f'\\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )\n",
    "\n",
    "    # If stopped, and to continue training in future on tpu we save model and optimizer\n",
    "    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')\n",
    "    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')    \n",
    "    \n",
    "    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "st = time()  # 開始時刻を記録\n",
    "warnings.filterwarnings(\"error\")  # 警告をエラーとして扱う\n",
    "METRICS = {\n",
    "    'loss': [],  # 損失を記録するリスト\n",
    "    'accuracy': {'y_true': [], 'y_pred': [] }  # 正解ラベルと予測ラベルを記録する辞書\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCHS)):  # エポック数だけループ\n",
    "    ste = time()  # 各エポックの開始時刻を記録\n",
    "    for step in range(STEPS_PER_EPOCH):  # 各エポック内のステップ数だけループ\n",
    "        # 勾配をゼロに設定\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        # バッチを取得\n",
    "        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "        \n",
    "        # フォワードパス\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "       \n",
    "        # ロジットをfloat32に変換\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "        \n",
    "        # バックワードパス\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))  # 損失を計算\n",
    "        loss.backward()  # バックワードパスの実行\n",
    "        \n",
    "        # オプティマイザーのステップ\n",
    "        OPTIMIZER.step()  # オプティマイザーを更新\n",
    "        xm.mark_step()  # TPUデバイスのステップをマーク\n",
    "        \n",
    "        # 学習率スケジューラーを更新\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # メトリクスと進行状況バーを更新\n",
    "        METRICS['loss'].append(float(loss))  # 損失を記録\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()  # 正解ラベルを記録\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()  # 予測ラベルを記録\n",
    "        \n",
    "        if (step + 1) % 200 == 0:  # 200ステップごとに進捗を表示\n",
    "            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))  # 平均損失を計算\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])  # 最後の損失を表示\n",
    "            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n",
    "                                                               METRICS['accuracy']['y_pred']))  # 正確度を計算\n",
    "            lr = OPTIMIZER.param_groups[0]['lr']  # 現在の学習率を取得\n",
    "            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            print(f'\\nエポック内のステップ: {step+1} 完了 | 経過時間: {time()- st}')  # 経過時間を表示\n",
    "    \n",
    "    print(f'\\nエポック {epoch+1} 完了 | エポック全体の経過時間: {time() - ste} ' )\n",
    "\n",
    "    # 停止した場合、将来再トレーニングを行うためにモデルとオプティマイザーを保存\n",
    "    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')  # モデルを保存\n",
    "    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')  # オプティマイザーを保存    \n",
    "    \n",
    "    print(f'エポック {epoch+1} でモデルが保存されました | 経過時間: {time() - st} ')\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time()  # 開始時刻を記録\n",
    "warnings.filterwarnings(\"error\")  # 警告をエラーとして扱う\n",
    "METRICS = {\n",
    "    'loss': [],  # 損失を記録するリスト\n",
    "    'accuracy': {'y_true': [], 'y_pred': [] }  # 正解ラベルと予測ラベルを記録する辞書\n",
    "}\n",
    "\n",
    "for epoch in tqdm(range(CFG.NUM_EPOCHS)):  # エポック数だけループ\n",
    "    ste = time()  # 各エポックの開始時刻を記録\n",
    "    for step in range(STEPS_PER_EPOCH):  # 各エポック内のステップ数だけループ\n",
    "        # 勾配をゼロに設定\n",
    "        OPTIMIZER.zero_grad()\n",
    "        \n",
    "        # バッチを取得\n",
    "        input_ids, attention_mask, labels = next(TRAIN_DATASET)\n",
    "        \n",
    "        # フォワードパス\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "       \n",
    "        # ロジットをfloat32に変換\n",
    "        logits = outputs.logits.to(dtype=torch.float32)\n",
    "        \n",
    "        # バックワードパス\n",
    "        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))  # 損失を計算\n",
    "        loss.backward()  # バックワードパスの実行\n",
    "        \n",
    "        # オプティマイザーのステップ\n",
    "        OPTIMIZER.step()  # オプティマイザーを更新\n",
    "        xm.mark_step()  # TPUデバイスのステップをマーク\n",
    "        \n",
    "        # 学習率スケジューラーを更新\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # メトリクスと進行状況バーを更新\n",
    "        METRICS['loss'].append(float(loss))  # 損失を記録\n",
    "        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()  # 正解ラベルを記録\n",
    "        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()  # 予測ラベルを記録\n",
    "        \n",
    "        if (step + 1) % 200 == 0:  # 200ステップごとに進捗を表示\n",
    "            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))  # 平均損失を計算\n",
    "            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])  # 最後の損失を表示\n",
    "            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \\\n",
    "                                                               METRICS['accuracy']['y_pred']))  # 正確度を計算\n",
    "            lr = OPTIMIZER.param_groups[0]['lr']  # 現在の学習率を取得\n",
    "            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')\n",
    "            print(f'\\nエポック内のステップ: {step+1} 完了 | 経過時間: {time()- st}')  # 経過時間を表示\n",
    "    \n",
    "    print(f'\\nエポック {epoch+1} 完了 | エポック全体の経過時間: {time() - ste} ' )\n",
    "\n",
    "    # 停止した場合、将来再トレーニングを行うためにモデルとオプティマイザーを保存\n",
    "    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')  # モデルを保存\n",
    "    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')  # オプティマイザーを保存    \n",
    "    \n",
    "    print(f'エポック {epoch+1} でモデルが保存されました | 経過時間: {time() - st} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645cf01",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(METRICS['loss'])    \n",
    "plt.xlabel('Step per epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot step per epoch')    \n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(15, 6))  # グラフのサイズを設定\n",
    "plt.plot(METRICS['loss'])  # 損失の変化をプロット\n",
    "plt.xlabel('エポック内のステップ')  # x軸のラベル\n",
    "plt.ylabel('損失')  # y軸のラベル\n",
    "plt.title('エポック内のステップに対する損失プロット')  # グラフのタイトル\n",
    "plt.show()  # グラフを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))  # グラフのサイズを設定\n",
    "plt.plot(METRICS['loss'])  # 損失の変化をプロット\n",
    "plt.xlabel('エポック内のステップ')  # x軸のラベル\n",
    "plt.ylabel('損失')  # y軸のラベル\n",
    "plt.title('エポック内のステップに対する損失プロット')  # グラフのタイトル\n",
    "plt.show()  # グラフを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621c012",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Save Model\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# モデルを保存\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b6c7b1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "model = model.cpu()\n",
    "torch.save(dict([(k,v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "model = model.cpu()  # モデルをCPUに移動\n",
    "torch.save(dict([(k, v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')  \n",
    "# 学習可能なパラメータのみを保存し、'llama_3_finetuned_model.pth'というファイル名で保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cpu()  # モデルをCPUに移動\n",
    "torch.save(dict([(k, v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')  \n",
    "# 学習可能なパラメータのみを保存し、'llama_3_finetuned_model.pth'というファイル名で保存"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8ff967",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Conclusion \n",
    "\n",
    "There is still alot of room to speed up and optimize training! Try out more data, different batch size, lr... All the best!\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 結論\n",
    "\n",
    "トレーニングを加速させ、最適化する余地がまだたくさんあります！もっと多くのデータ、異なるバッチサイズ、学習率などを試してみてください。成功を祈っています！\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30734,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
