{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56fb9600",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションのために、チャットボットの応答に対する人間の好みを予測するモデルを開発することを目的としています。具体的には、LLM（大規模言語モデル）を使用し、「winner_model_a」「winner_model_b」「winner_tie」といった分類を予測するために、ロジスティック回帰的な手法を用いています。\n",
    "\n",
    "### 主な問題\n",
    "- ユーザーが2つのモデルによる応答の中からどちらを好むかを予測する問題に取り組んでいます。データからは、各応答がどれだけ好まれるかを評価するためにログ損失（log loss）をメトリクスとして使用しています。\n",
    "\n",
    "### 使用手法とライブラリ\n",
    "- **Hugging Face Transformers**: モデルのトレーニングや推論に使用され、特に`LlamaModel`や`AutoTokenizer`が利用されています。\n",
    "- **Pandas、NumPy**: データ処理に使用されており、データの読み込みや変換、数値計算を行っています。\n",
    "- **PEFT (Parameter-Efficient Fine-Tuning)**: パラメータの効率的な調整を行うために、Low-Rank Adaptation（LoRA）を使用しています。これにより、モデルの性能を保ちながら、必要なメモリを削減しています。\n",
    "- **Scikit-learn**: 評価指標としてのログロスと精度を計算するためにインポートされています。\n",
    "\n",
    "### 処理の流れ\n",
    "1. 必要なライブラリのインストールとインポート。\n",
    "2. 訓練データの読み込みと前処理。各応答のラベル設定を行います。\n",
    "3. トークナイザーの設定を行い、データセットをトークン化。\n",
    "4. モデルの準備（`Llama3ForSFT`クラス）、LoRA設定を行い、事前学習されたモデルを読み込みます。\n",
    "5. トレーニングの引数を設定し、トレーニングを実行。\n",
    "\n",
    "最終的に、提案されたアプローチによって、与えられたプロンプトに対して最適な応答を予測する能力を持つモデルが訓練されています。評価結果として、ログロス値が0.9231、リーダーボードでのスコアが0.936となっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971ae4ad",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、機械学習・深層学習の初心者がつまずきそうな専門用語の解説です。特に、ノートブック特有のドメイン知識や、実務を経験していないと馴染みのないものに焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **ログロス (Log Loss)**: \n",
    "   確率的な予測と実際のラベルとの間の差異を測定する評価指標です。特に二項分類や多項分類の問題で使われ、低い値が高い予測性能を示します。予測確率が正解ラベルに近づくほど、ログロスは小さくなります。\n",
    "\n",
    "2. **アテンションマスク (Attention Mask)**:\n",
    "   トランスフォーマーモデルにおいて、モデルがどの部分に注意を向けるべきかを示すものです。通常、入力がパディングされた場合、そのパディング部分に注意を向けないように1と0の値を持ちます（1は注意する部分、0は無視する部分）。\n",
    "\n",
    "3. **トークナイザー (Tokenizer)**:\n",
    "   テキストデータを機械が理解できる形式に変換するツールです。単語やサブワードのトークンに分割し、それに対するインデックス（ID）を付与します。事前訓練済みのトークナイザーは、大規模な言語モデルに特化したものが多いです。\n",
    "\n",
    "4. **データコレータ (Data Collator)**:\n",
    "   各バッチに対するデータの前処理を行うクラスです。例えば、各サンプルの長さを揃えるためのパディングを行うことなどが含まれます。特にバッチサイズが異なるサンプルが混在している場合には重要です。\n",
    "\n",
    "5. **勾配蓄積 (Gradient Accumulation)**:\n",
    "   メモリ制約で大きなバッチサイズを使えない場合、小さなバッチサイズの勾配を複数回計算し、それらを累積してから重みの更新を行う手法です。これにより、実質的に大きなバッチサイズで訓練した時と同等の効果が得られます。\n",
    "\n",
    "6. **PEFT (Parameter-Efficient Fine-Tuning)**:\n",
    "   訓練済みのモデルパラメータを効率的に微調整する手法のことです。特に、大規模モデルのパラメータ数が多い場合に、一部のパラメータのみを更新することで、計算資源を節約しながら高精度なモデルを得ることを目的としています。\n",
    "\n",
    "7. **ロジット (Logits)**:\n",
    "   ニューラルネットワークの出力で、クラスに対する生の信号やスコアを指します。ロジットは通常、ソフトマックス関数を通じて確率に変換され、確率分布を形成します。\n",
    "\n",
    "8. **因果言語モデル (Causal Language Model)**:\n",
    "   次の単語を予測するために、前の単語の情報のみを使用するモデルのことです。文の左側から右側に向けて条件付けが行われるため、過去の単語に基づいて次の単語を生成します。\n",
    "\n",
    "9. **デバイス (Device)**:\n",
    "   モデルやデータが置かれるコンピュータハードウェアのこと。一般にGPU（グラフィックス処理ユニット）を使用することが多く、計算速度を飛躍的に向上させます。`torch.device('cuda')`のようにして指定します。\n",
    "\n",
    "10. **事前学習済みモデル (Pre-trained Model)**:\n",
    "    大量のデータで訓練されたモデルのこと。特定のタスクに対して再訓練（ファインチューニング）することで、少ないデータで効果的なモデルを構築するために利用されます。\n",
    "\n",
    "これらの用語は、特にノートブックの機械学習や深層学習の流れにおいて、理解を深めるために重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea281481",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Result\n",
    "- [Inference Code](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    \n",
    "\n",
    "- [Base Model: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)\n",
    "\n",
    "| subset | log loss |\n",
    "| - | - |\n",
    "| Eval | 0.9231|\n",
    "| LB | 0.936 |\n",
    "\n",
    "## Note\n",
    "If you want to reproduce the code, please note the following:\n",
    "- use all data\n",
    "- set per_device_train_batch_size=4\n",
    "- 1 epoch using A10 took ~15h\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 結果\n",
    "- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    \n",
    "\n",
    "- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)\n",
    "\n",
    "| サブセット | ログロス |\n",
    "| - | - |\n",
    "| 評価 | 0.9231|\n",
    "| LB | 0.936 |\n",
    "\n",
    "## 注意事項\n",
    "コードを再現したい場合は、以下の点に注意してください:\n",
    "- すべてのデータを使用\n",
    "- `per_device_train_batch_size=4`を設定\n",
    "- A10を使用して1エポックで約15時間かかります\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b4cb41",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install git+https://github.com/huggingface/transformers\n",
    "!pip install -U bitsandbytes accelerate peft\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install git+https://github.com/huggingface/transformers  # Hugging FaceのTransformersライブラリをインストールします。\n",
    "!pip install -U bitsandbytes accelerate peft  # bitsandbytes、accelerate、peftを最新バージョンにアップグレードしてインストールします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-22T15:49:39.661073Z",
     "iopub.status.busy": "2024-07-22T15:49:39.660337Z",
     "iopub.status.idle": "2024-07-22T15:50:50.77581Z",
     "shell.execute_reply": "2024-07-22T15:50:50.774819Z",
     "shell.execute_reply.started": "2024-07-22T15:49:39.661018Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers  # Hugging FaceのTransformersライブラリをインストールします。\n",
    "!pip install -U bitsandbytes accelerate peft  # bitsandbytes、accelerate、peftを最新バージョンにアップグレードしてインストールします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c02474",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import os\n",
    "import copy\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from scipy.special import softmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModel\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import os  # オペレーティングシステムに関連する機能をインポートします。\n",
    "import copy  # オブジェクトのコピーをサポートする機能をインポートします。\n",
    "from dataclasses import dataclass  # データクラスを作成するためのデコレーターをインポートします。\n",
    "\n",
    "import torch  # PyTorchの基本ライブラリをインポートします。\n",
    "import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポートします。\n",
    "import torch.nn.functional as F  # ニューラルネットワーク用の関数をインポートします。\n",
    "import pandas as pd  # データ処理用のPandasライブラリをインポートします。\n",
    "import numpy as np  # 数値計算用のNumPyライブラリをインポートします。\n",
    "from datasets import Dataset  # データセットを扱うためのクラスをインポートします。\n",
    "from scipy.special import softmax  # softmax関数をインポートします。\n",
    "from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングを行うためのクラスをインポートします。\n",
    "from transformers import (  # Transformersライブラリから必要なクラスをインポートします。\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModel\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast  # 過去の出力を含む因果言語モデルの出力をインポートします。\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTに関するクラスや関数をインポートします。\n",
    "from sklearn.metrics import log_loss, accuracy_score  # 評価指標としてのログロスと精度をインポートします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:21.237862Z",
     "iopub.status.busy": "2024-07-22T15:51:21.237499Z",
     "iopub.status.idle": "2024-07-22T15:51:21.245144Z",
     "shell.execute_reply": "2024-07-22T15:51:21.243844Z",
     "shell.execute_reply.started": "2024-07-22T15:51:21.237833Z"
    }
   },
   "outputs": [],
   "source": [
    "import os  # オペレーティングシステムに関連する機能をインポートします。\n",
    "import copy  # オブジェクトのコピーをサポートする機能をインポートします。\n",
    "from dataclasses import dataclass  # データクラスを作成するためのデコレーターをインポートします。\n",
    "\n",
    "import torch  # PyTorchの基本ライブラリをインポートします。\n",
    "import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポートします。\n",
    "import torch.nn.functional as F  # ニューラルネットワーク用の関数をインポートします。\n",
    "import pandas as pd  # データ処理用のPandasライブラリをインポートします。\n",
    "import numpy as np  # 数値計算用のNumPyライブラリをインポートします。\n",
    "from datasets import Dataset  # データセットを扱うためのクラスをインポートします。\n",
    "from scipy.special import softmax  # softmax関数をインポートします。\n",
    "from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングを行うためのクラスをインポートします。\n",
    "from transformers import (  # Transformersライブラリから必要なクラスをインポートします。\n",
    "    BitsAndBytesConfig,\n",
    "    LlamaModel,\n",
    "    AutoTokenizer,\n",
    "    PreTrainedTokenizerBase, \n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    AutoModel\n",
    ")\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast  # 過去の出力を含む因果言語モデルの出力をインポートします。\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTに関するクラスや関数をインポートします。\n",
    "from sklearn.metrics import log_loss, accuracy_score  # 評価指標としてのログロスと精度をインポートします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc21130",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Configurations\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### 設定\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2f6a66",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"\n",
    "model_path = \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "MAX_LENGTH = 1024\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)\n",
    "train = train.head(100)\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"  # 訓練データのCSVファイルのパスを指定します。\n",
    "model_path = \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\"  # 使用するモデルのパスを指定します。\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能かどうかを確認してデバイスを設定します。\n",
    "MAX_LENGTH = 1024  # 最大入力長を設定します。\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測対象の列を指定します。\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]  # ベクトル化する列を指定します。\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)  # CSVファイルから訓練データを読み込みます。\n",
    "train = train.head(100)  # 最初の100件を取得します。\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行のラベルを決定します。\n",
    "label_encoder = LabelEncoder()  # ラベルエンコーダを初期化します。\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコードします。\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要な列だけを保持します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:23.395038Z",
     "iopub.status.busy": "2024-07-22T15:51:23.394401Z",
     "iopub.status.idle": "2024-07-22T15:51:26.676636Z",
     "shell.execute_reply": "2024-07-22T15:51:26.67564Z",
     "shell.execute_reply.started": "2024-07-22T15:51:23.395007Z"
    }
   },
   "outputs": [],
   "source": [
    "TRAIN_CSV = \"/kaggle/input/lmsys-chatbot-arena/train.csv\"  # 訓練データのCSVファイルのパスを指定します。\n",
    "model_path = \"unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit\"  # 使用するモデルのパスを指定します。\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能かどうかを確認してデバイスを設定します。\n",
    "MAX_LENGTH = 1024  # 最大入力長を設定します。\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測対象の列を指定します。\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]  # ベクトル化する列を指定します。\n",
    "\n",
    "train = pd.read_csv(TRAIN_CSV)  # CSVファイルから訓練データを読み込みます。\n",
    "train = train.head(100)  # 最初の100件を取得します。\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行のラベルを決定します。\n",
    "label_encoder = LabelEncoder()  # ラベルエンコーダを初期化します。\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコードします。\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要な列だけを保持します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbb5b7",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Tokenizer and prepare dataset, metrics\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### トークナイザーとデータセットの準備、メトリクス\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2ef328",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, force_download = True )\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, force_download=True)  # 事前学習済みのトークナイザーをロードします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:26.679294Z",
     "iopub.status.busy": "2024-07-22T15:51:26.678789Z",
     "iopub.status.idle": "2024-07-22T15:51:27.827263Z",
     "shell.execute_reply": "2024-07-22T15:51:27.826405Z",
     "shell.execute_reply.started": "2024-07-22T15:51:26.679258Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, force_download=True)  # 事前学習済みのトークナイザーをロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8519dbb8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "tokenizer.add_eos_token = True\n",
    "tokenizer.padding_side = 'right'\n",
    "\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "tokenizer.add_eos_token = True  # 終了トークンを追加します。\n",
    "tokenizer.padding_side = 'right'  # パディングを右側に設定します。\n",
    "\n",
    "# ラベルIDsをトークナイザーを使って作成します。\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    # プロンプト、応答a、および応答bをトークン化します。\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    # 入力が最大長を超える場合は入力を切り詰めます。\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # ラベルに対応するトークンIDを取得します。\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]  # アテンションマスクを作成します。\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    # トークナイズされたデータを返します。\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:27.828991Z",
     "iopub.status.busy": "2024-07-22T15:51:27.828631Z",
     "iopub.status.idle": "2024-07-22T15:51:27.842816Z",
     "shell.execute_reply": "2024-07-22T15:51:27.84195Z",
     "shell.execute_reply.started": "2024-07-22T15:51:27.828959Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.add_eos_token = True  # 終了トークンを追加します。\n",
    "tokenizer.padding_side = 'right'  # パディングを右側に設定します。\n",
    "\n",
    "# ラベルIDsをトークナイザーを使って作成します。\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    # プロンプト、応答a、および応答bをトークン化します。\n",
    "    prompt = tokenizer('<prompt>: ' + \" \".join(eval(example['prompt'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ' + \" \".join(eval(example['response_a'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ' + \" \".join(eval(example['response_b'], {\"null\": \"\"})), add_special_tokens=False)[\"input_ids\"]\n",
    "    # 入力が最大長を超える場合は入力を切り詰めます。\n",
    "    if len(prompt+response_a+response_b) > MAX_LENGTH:\n",
    "        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:256]\n",
    "        response_a = tokenizer('\\n\\n<response_a>: ' + eval(example['response_a'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "        response_b = tokenizer('\\n\\n<response_b>: ' + eval(example['response_b'], {\"null\": \"\"})[-1], add_special_tokens=False)[\"input_ids\"][:512]\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "\n",
    "    # ラベルに対応するトークンIDを取得します。\n",
    "    label_token_id = LABEL_IDS[int(example['label'])]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]  # アテンションマスクを作成します。\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]\n",
    "    # トークナイズされたデータを返します。\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de2ce9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    label_tokens_ids = np.array(LABEL_IDS)\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    probs = softmax(logits, axis=-1)\n",
    "    log_loss_ = log_loss(labels, probs)\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}\n",
    "\n",
    "n_splits = 5\n",
    "fold_idx = 0\n",
    "ds = load_data(train, tokenizer)\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def load_data(df, tokenizer):\n",
    "    # DataFrameをデータセットに変換します。\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}  # トークナイザーを引数として渡します。\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred  # 予測とラベルを取得します。\n",
    "    preds = logits.argmax(axis=-1)  # 最大のロジット値を持つインデックスを取得します。\n",
    "    label_tokens_ids = np.array(LABEL_IDS)  # ラベルIDsを配列に変換します。\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]  # ラベルがラベルIDのいずれかに含まれている場合にフィルタリングします。\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])  # マッピングを適用してラベルを変換します。\n",
    "    acc = accuracy_score(labels, preds)  # 精度を計算します。\n",
    "    probs = softmax(logits, axis=-1)  # ソフトマックスを適用して確率を計算します。\n",
    "    log_loss_ = log_loss(labels, probs)  # ログロスを計算します。\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログロスを返します。\n",
    "\n",
    "n_splits = 5  # データセットを分割するためのスプリット数を設定します。\n",
    "fold_idx = 0  # フォールドインデックスを初期化します。\n",
    "ds = load_data(train, tokenizer)  # データセットをロードします。\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],  # 訓練データのインデックス\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]  # 評価データのインデックス\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)  # 各フォールドのインデックスに対して反復処理します。\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]  # 訓練インデックスと評価インデックスを取得します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:27.845152Z",
     "iopub.status.busy": "2024-07-22T15:51:27.844872Z",
     "iopub.status.idle": "2024-07-22T15:51:28.468006Z",
     "shell.execute_reply": "2024-07-22T15:51:28.467008Z",
     "shell.execute_reply.started": "2024-07-22T15:51:27.845129Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_data(df, tokenizer):\n",
    "    # DataFrameをデータセットに変換します。\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer}  # トークナイザーを引数として渡します。\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    logits, labels = pred  # 予測とラベルを取得します。\n",
    "    preds = logits.argmax(axis=-1)  # 最大のロジット値を持つインデックスを取得します。\n",
    "    label_tokens_ids = np.array(LABEL_IDS)  # ラベルIDsを配列に変換します。\n",
    "    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。\n",
    "    labels = labels[np.isin(labels, label_tokens_ids)]  # ラベルがラベルIDのいずれかに含まれている場合にフィルタリングします。\n",
    "    labels = np.array([index_mapping[label.item()] for label in labels])  # マッピングを適用してラベルを変換します。\n",
    "    acc = accuracy_score(labels, preds)  # 精度を計算します。\n",
    "    probs = softmax(logits, axis=-1)  # ソフトマックスを適用して確率を計算します。\n",
    "    log_loss_ = log_loss(labels, probs)  # ログロスを計算します。\n",
    "    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログロスを返します。\n",
    "\n",
    "n_splits = 5  # データセットを分割するためのスプリット数を設定します。\n",
    "fold_idx = 0  # フォールドインデックスを初期化します。\n",
    "ds = load_data(train, tokenizer)  # データセットをロードします。\n",
    "folds = [\n",
    "    (\n",
    "        [i for i in range(len(ds)) if i % n_splits != fold_idx],  # 訓練データのインデックス\n",
    "        [i for i in range(len(ds)) if i % n_splits == fold_idx]  # 評価データのインデックス\n",
    "    ) \n",
    "    for fold_idx in range(n_splits)  # 各フォールドのインデックスに対して反復処理します。\n",
    "]\n",
    "train_idx, eval_idx = folds[fold_idx]  # 訓練インデックスと評価インデックスを取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08526a39",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Model\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### モデル\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576f75e6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel, MistralPreTrainedModel, MistralModel\n",
    "class Llama3ForSFT(MistralPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = MistralModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "#         if self.config.pretraining_tp > 1:\n",
    "#             lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "#             logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "#             logits = torch.cat(logits, dim=-1)\n",
    "#         else:\n",
    "        logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:,label_tokens_ids]\n",
    "            loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from transformers import AutoModel, MistralPreTrainedModel, MistralModel  # モデル関連のクラスをインポートします。\n",
    "class Llama3ForSFT(MistralPreTrainedModel):  # MistralPreTrainedModelを拡張したLlama3ForSFTクラスを定義します。\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みの結合キーを定義します。\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # 親クラスの初期化を行います。\n",
    "        self.model = MistralModel(config)  # Mistralモデルを初期化します。\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズを取得します。\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # ロジット層を初期化します。\n",
    "        self.post_init()  # モデルの初期化を完了します。\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,  # 入力IDを渡します。\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ状態を取得します。\n",
    "        logits = self.lm_head(hidden_states)  # 隠れ状態からロジットを計算します。\n",
    "        logits = logits.float()  # ロジットを浮動小数点型に変換します。\n",
    "\n",
    "        loss = None  # 初期化\n",
    "        if labels is not None:\n",
    "            # ラベルがある場合の処理を行います。\n",
    "            # シフトしてトークンがn未満を予測します。\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフトします。\n",
    "            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフトします。\n",
    "            # トークンをフラット化します。\n",
    "            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失関数を定義します。\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットを再成形します。\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルを再成形します。\n",
    "            # モデル並行処理を有効にします。\n",
    "            shift_labels = shift_labels.to(shift_logits.device)  # デバイスにラベルを移動します。\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルIDsをテンソルに変換します。\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 正しいラベルを取得します。\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)  # マッピングを適用します。\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 正しいロジットを取得します。\n",
    "            loss = loss_fct(true_logits, true_labels)  # 損失を計算します。\n",
    "\n",
    "        return CausalLMOutputWithPast(  # 結果を返します。\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:28.469916Z",
     "iopub.status.busy": "2024-07-22T15:51:28.469605Z",
     "iopub.status.idle": "2024-07-22T15:51:28.487629Z",
     "shell.execute_reply": "2024-07-22T15:51:28.486741Z",
     "shell.execute_reply.started": "2024-07-22T15:51:28.469889Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModel, MistralPreTrainedModel, MistralModel  # モデル関連のクラスをインポートします。\n",
    "class Llama3ForSFT(MistralPreTrainedModel):  # MistralPreTrainedModelを拡張したLlama3ForSFTクラスを定義します。\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みの結合キーを定義します。\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # 親クラスの初期化を行います。\n",
    "        self.model = MistralModel(config)  # Mistralモデルを初期化します。\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズを取得します。\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # ロジット層を初期化します。\n",
    "        self.post_init()  # モデルの初期化を完了します。\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,  # 入力IDを渡します。\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ状態を取得します。\n",
    "        logits = self.lm_head(hidden_states)  # 隠れ状態からロジットを計算します。\n",
    "        logits = logits.float()  # ロジットを浮動小数点型に変換します。\n",
    "\n",
    "        loss = None  # 初期化\n",
    "        if labels is not None:\n",
    "            # ラベルがある場合の処理を行います。\n",
    "            # シフトしてトークンがn未満を予測します。\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフトします。\n",
    "            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフトします。\n",
    "            # トークンをフラット化します。\n",
    "            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失関数を定義します。\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットを再成形します。\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルを再成形します。\n",
    "            # モデル並行処理を有効にします。\n",
    "            shift_labels = shift_labels.to(shift_logits.device)  # デバイスにラベルを移動します。\n",
    "\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルIDsをテンソルに変換します。\n",
    "            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。\n",
    "            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 正しいラベルを取得します。\n",
    "            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)  # マッピングを適用します。\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 正しいロジットを取得します。\n",
    "            loss = loss_fct(true_logits, true_labels)  # 損失を計算します。\n",
    "\n",
    "        return CausalLMOutputWithPast(  # 結果を返します。\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72712e0e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    inference_mode=False,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj',], \n",
    ")\n",
    "\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16, \n",
    ")\n",
    "model.config.use_cache = False\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model)\n",
    "model.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "peft_config = LoraConfig(  # PEFT設定を作成します。\n",
    "    r=16,  # ランクを設定します。\n",
    "    lora_alpha=32,  # alpha値を設定します。\n",
    "    lora_dropout=0.05,  # ドロップアウト率を設定します。\n",
    "    bias='none',  # バイアスの設定を行います。\n",
    "    inference_mode=False,  # 推論モードをオフにします。\n",
    "    task_type=TaskType.CAUSAL_LM,  # タスクのタイプを因果言語モデルに設定します。\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj'],  # 対象となるモジュールを設定します。\n",
    ")\n",
    "\n",
    "# 事前学習済みのモデルをロードし、kビット訓練の準備をします。\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定します。\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを無効にします。\n",
    "model = prepare_model_for_kbit_training(model)  # kビット訓練のための準備をします。\n",
    "model = get_peft_model(model, peft_config)  # PEFTモデルを取得します。\n",
    "print(model)  # モデルの情報を表示します。\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメータを表示します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:51:28.489272Z",
     "iopub.status.busy": "2024-07-22T15:51:28.488896Z",
     "iopub.status.idle": "2024-07-22T15:52:16.419305Z",
     "shell.execute_reply": "2024-07-22T15:52:16.418243Z",
     "shell.execute_reply.started": "2024-07-22T15:51:28.48924Z"
    }
   },
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(  # PEFT設定を作成します。\n",
    "    r=16,  # ランクを設定します。\n",
    "    lora_alpha=32,  # alpha値を設定します。\n",
    "    lora_dropout=0.05,  # ドロップアウト率を設定します。\n",
    "    bias='none',  # バイアスの設定を行います。\n",
    "    inference_mode=False,  # 推論モードをオフにします。\n",
    "    task_type=TaskType.CAUSAL_LM,  # タスクのタイプを因果言語モデルに設定します。\n",
    "    target_modules=['q_proj', 'k_proj', 'v_proj'],  # 対象となるモジュールを設定します。\n",
    ")\n",
    "\n",
    "# 事前学習済みのモデルをロードし、kビット訓練の準備をします。\n",
    "model = Llama3ForSFT.from_pretrained(\n",
    "    model_path, \n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定します。\n",
    ")\n",
    "model.config.use_cache = False  # キャッシュを無効にします。\n",
    "model = prepare_model_for_kbit_training(model)  # kビット訓練のための準備をします。\n",
    "model = get_peft_model(model, peft_config)  # PEFTモデルを取得します。\n",
    "print(model)  # モデルの情報を表示します。\n",
    "model.print_trainable_parameters()  # 訓練可能なパラメータを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6675baf",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "#### Training Arguments\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "#### トレーニング引数\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c000dd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "args = TrainingArguments(\n",
    "    output_dir='output',\n",
    "    overwrite_output_dir = True,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"steps\",\n",
    "    save_steps=200,\n",
    "    save_total_limit=1,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    warmup_steps=20,\n",
    "    optim=\"adamw_8bit\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=3,\n",
    "    num_train_epochs=1,\n",
    "    fp16=True,\n",
    "    metric_for_best_model=\"log_loss\",\n",
    "    greater_is_better = False,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "args = TrainingArguments(  # トレーニング引数を設定します。\n",
    "    output_dir='output',  # 出力ディレクトリを指定します。\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書きします。\n",
    "    evaluation_strategy=\"epoch\",  # 評価戦略をエポックごとに設定します。\n",
    "    save_strategy=\"steps\",  # 保存戦略をステップごとに設定します。\n",
    "    save_steps=200,  # 200ステップごとに保存します。\n",
    "    save_total_limit=1,  # 保存するファイル数の制限を設定します。\n",
    "    logging_strategy=\"steps\",  # ロギング戦略をステップごとに設定します。\n",
    "    logging_steps=10,  # 10ステップごとにロギングします。\n",
    "    warmup_steps=20,  # ウォームアップステップを設定します。\n",
    "    optim=\"adamw_8bit\",  # 最適化手法を設定します。\n",
    "    learning_rate=2e-4,  # 学習率を設定します。\n",
    "    per_device_train_batch_size=1,  # デバイスごとの訓練バッチサイズを設定します。\n",
    "    per_device_eval_batch_size=1,  # デバイスごとの評価バッチサイズを設定します。\n",
    "    gradient_accumulation_steps=3,  # 勾配蓄積ステップを設定します。\n",
    "    num_train_epochs=1,  # 訓練エポック数を設定します。\n",
    "    fp16=True,  # 半精度浮動小数点を使用します。\n",
    "    metric_for_best_model=\"log_loss\",  # 最良モデルのメトリックをログロスに設定します。\n",
    "    greater_is_better=False,  # より良いのは小さい方の損失とします。\n",
    "    report_to=\"none\",  # ロギングの報告先を指定しません。\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:52:16.421287Z",
     "iopub.status.busy": "2024-07-22T15:52:16.420829Z",
     "iopub.status.idle": "2024-07-22T15:52:16.455288Z",
     "shell.execute_reply": "2024-07-22T15:52:16.45432Z",
     "shell.execute_reply.started": "2024-07-22T15:52:16.421242Z"
    }
   },
   "outputs": [],
   "source": [
    "args = TrainingArguments(  # トレーニング引数を設定します。\n",
    "    output_dir='output',  # 出力ディレクトリを指定します。\n",
    "    overwrite_output_dir=True,  # 出力ディレクトリを上書きします。\n",
    "    evaluation_strategy=\"epoch\",  # 評価戦略をエポックごとに設定します。\n",
    "    save_strategy=\"steps\",  # 保存戦略をステップごとに設定します。\n",
    "    save_steps=200,  # 200ステップごとに保存します。\n",
    "    save_total_limit=1,  # 保存するファイル数の制限を設定します。\n",
    "    logging_strategy=\"steps\",  # ロギング戦略をステップごとに設定します。\n",
    "    logging_steps=10,  # 10ステップごとにロギングします。\n",
    "    warmup_steps=20,  # ウォームアップステップを設定します。\n",
    "    optim=\"adamw_8bit\",  # 最適化手法を設定します。\n",
    "    learning_rate=2e-4,  # 学習率を設定します。\n",
    "    per_device_train_batch_size=1,  # デバイスごとの訓練バッチサイズを設定します。\n",
    "    per_device_eval_batch_size=1,  # デバイスごとの評価バッチサイズを設定します。\n",
    "    gradient_accumulation_steps=3,  # 勾配蓄積ステップを設定します。\n",
    "    num_train_epochs=1,  # 訓練エポック数を設定します。\n",
    "    fp16=True,  # 半精度浮動小数点を使用します。\n",
    "    metric_for_best_model=\"log_loss\",  # 最良モデルのメトリックをログロスに設定します。\n",
    "    greater_is_better=False,  # より良いのは小さい方の損失とします。\n",
    "    report_to=\"none\",  # ロギングの報告先を指定しません。\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b325e96",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Training !\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### トレーニング !\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0cf8b7",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import transformers \n",
    "transformers.__version__\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import transformers  # Transformersライブラリをインポートします。\n",
    "transformers.__version__  # 現在のTransformersのバージョンを表示します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:52:16.456729Z",
     "iopub.status.busy": "2024-07-22T15:52:16.456422Z",
     "iopub.status.idle": "2024-07-22T15:52:21.442937Z",
     "shell.execute_reply": "2024-07-22T15:52:21.441472Z",
     "shell.execute_reply.started": "2024-07-22T15:52:16.456704Z"
    }
   },
   "outputs": [],
   "source": [
    "import transformers  # Transformersライブラリをインポートします。\n",
    "transformers.__version__  # 現在のTransformersのバージョンを表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e061a16",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "trainer = Trainer(\n",
    "    args=args,\n",
    "    model=model,\n",
    "    train_dataset=ds.select(train_idx),\n",
    "    eval_dataset=ds.select(eval_idx),\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "trainer = Trainer(  # Trainerインスタンスを作成します。\n",
    "    args=args,  # トレーニング引数を渡します。\n",
    "    model=model,  # 使用するモデルを指定します。\n",
    "    train_dataset=ds.select(train_idx),  # 訓練データセットを指定します。\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセットを指定します。\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータを指定します。\n",
    "    compute_metrics=compute_metrics,  # メトリクス計算を指定します。\n",
    ")\n",
    "trainer.train()  # 訓練を開始します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-22T15:52:21.444971Z",
     "iopub.status.busy": "2024-07-22T15:52:21.444534Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(  # Trainerインスタンスを作成します。\n",
    "    args=args,  # トレーニング引数を渡します。\n",
    "    model=model,  # 使用するモデルを指定します。\n",
    "    train_dataset=ds.select(train_idx),  # 訓練データセットを指定します。\n",
    "    eval_dataset=ds.select(eval_idx),  # 評価データセットを指定します。\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータを指定します。\n",
    "    compute_metrics=compute_metrics,  # メトリクス計算を指定します。\n",
    ")\n",
    "trainer.train()  # 訓練を開始します。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
