{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8ed5557",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するためのモデルを構築するプロセスを示しています。Notebookは、様々なデータ処理と機械学習手法を用いて、与えられたプロンプトに対する異なるチャットボットの応答の好みを予測する問題に取り組んでいます。\n",
    "\n",
    "### 主な内容と手法\n",
    "\n",
    "1. **データの読み込みと前処理**:\n",
    "   - データは`pandas`を用いてCSVファイルから読み込まれ、必要に応じてトレーニングデータの一部をサブセットとして利用します。\n",
    "   - 特定の文字列処理を行い、プロンプトや応答から余分な文字を取り除く`process`関数が定義されています。\n",
    "\n",
    "2. **特徴量エンジニアリング**:\n",
    "   - `Preprocessor`クラスでは、テキスト間のコサイン類似度やジャッカード類似度の計算、n-グラムの生成、改行や引用符のカウント、トークン化など、さまざまなテキスト特徴量を計算します。\n",
    "   - 各応答とプロンプトの類似性を数値化することに焦点をあて、生成した特徴量を基に予測モデルを訓練します。\n",
    "\n",
    "3. **モデルの構築と評価**:\n",
    "   - モデルには`XGBoost`ライブラリを使用し、多クラス分類器を訓練します。\n",
    "   - `StratifiedKFold`を用いてクロスバリデーションを行い、各フォールドの対数損失（log loss）を評価指標として使用します。\n",
    "\n",
    "4. **結果の可視化と提出ファイルの作成**:\n",
    "   - 各特徴量の重要度を計算し、`matplotlib`および`seaborn`を使って重要度のバープロットを作成します。\n",
    "   - 最後に、テストデータに基づいた予測結果を`submission.csv`としてエクスポートし、コンペティションへの提出を可能にします。\n",
    "\n",
    "### 使用ライブラリ\n",
    "- `numpy`, `pandas`: データ操作\n",
    "- `nltk`: テキスト処理\n",
    "- `matplotlib`, `seaborn`: データ可視化\n",
    "- `xgboost`: 機械学習モデル\n",
    "- `sklearn`: モデル評価とデータ前処理\n",
    "\n",
    "このNotebookは、機械学習における特徴量エンジニアリングの重要性を示すとともに、フレームワークを利用して有効な予測モデルを構築する一連の流れを具体化したものとなっています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766918ac",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookに登場する専門用語で、機械学習・深層学習の初心者がつまずきそうなものの簡単な解説を示します。普段あまり使用されない用語や、特にこのノートブックに特有の概念に焦点を当てています。\n",
    "\n",
    "1. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - テキストデータの重要度を測る指標。特定の単語が文書内でどのくらい頻繁に出てくるか（Term Frequency）と、その単語が文書全体でどれくらいの頻度で出現するか（Inverse Document Frequency）を組み合わせて計算する。テキスト分類や情報検索の際に、多く使われる。\n",
    "\n",
    "2. **コサイン類似度 (Cosine Similarity)**:\n",
    "   - 2つのベクトル間の類似度を計算する指標です。コサインの角度を用い、ベクトルの向き（意味的な類似性）を比較する。特にテキストデータの類似性を測定する際に有用。\n",
    "\n",
    "3. **ジャッカード類似度 (Jaccard Similarity)**:\n",
    "   - 2つの集合の間の類似度を測る指標で、積集合のサイズを和集合のサイズで割ったもの。異なるテキスト間での共通語の割合を測る際に使われ、単語の重複を確認するのに役立つ。\n",
    "\n",
    "4. **n-グラム (n-gram)**:\n",
    "   - 連続するn個の要素（通常は単語や文字）から構成される部分列。言語モデルやテキスト処理のタスクにおいて、文脈依存性を持つ情報を捉えるために使用される。\n",
    "\n",
    "5. **ストラティファイドKフォールド交差検定 (Stratified K-Fold Cross Validation)**:\n",
    "   - データセットをK個のサブセットに分割し、各サブセットが元のデータセットのクラス分布を保持するような交差検定の手法。特にクラスの不均衡がある場合に有用。\n",
    "\n",
    "6. **early stopping (早期停止)**:\n",
    "   - モデル訓練中にValidationの性能が向上しなくなった場合、訓練を自動的に終了させる手法。過学習を防ぐために使用される。\n",
    "\n",
    "7. **弱学習器 (Weak Learner)**:\n",
    "   - パフォーマンスがランダムな予測よりもわずかに優れているが、それ自体が強力ではないモデル。バギングやブースティングのアルゴリズムで多く使用される。\n",
    "\n",
    "8. **対数損失 (Log Loss)**:\n",
    "   - 分類問題で用いる評価指標。予測の確率に基づいてモデルの性能を測る。0から無限大までの値を取るが、低いほど良い評価を意味する。\n",
    "\n",
    "9. **フィーチャーインポータンス (Feature Importance)**:\n",
    "   - モデルがどの特徴量（フィーチャー）を重視しているかを示す指標。XGBoostなどのツリー系モデルでは、各特徴がモデルの予測にどれほど貢献したかを確認できる。\n",
    "\n",
    "10. **バイグラム (Bigram) / トライグラム (Trigram)**:\n",
    "    - それぞれ2つまたは3つの連続する単語からなるn-グラム。例えば、「自然言語処理」というフレーズでは、「自然言語」「言語処理」というバイグラムが得られる。\n",
    "\n",
    "これらの用語は、初心者には難解であるが、ノートブック内で効果的にデータ処理とモデル構築をするために重要な概念です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "530ca32d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-18T18:15:59.049623Z",
     "iopub.status.busy": "2024-07-18T18:15:59.048859Z",
     "iopub.status.idle": "2024-07-18T18:16:02.217493Z",
     "shell.execute_reply": "2024-07-18T18:16:02.216508Z",
     "shell.execute_reply.started": "2024-07-18T18:15:59.049591Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f60518",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")  # トレーニングデータのパス\n",
    "    test_path = os.path.join(root, \"test.csv\")    # テストデータのパス\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")  # サンプル提出データのパス\n",
    "    seed = 42  # 乱数シードを42に設定\n",
    "    n_splits = 10  # クロスバリデーションの分割数を10に設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T18:16:09.090238Z",
     "iopub.status.busy": "2024-07-18T18:16:09.089262Z",
     "iopub.status.idle": "2024-07-18T18:16:09.095249Z",
     "shell.execute_reply": "2024-07-18T18:16:09.094153Z",
     "shell.execute_reply.started": "2024-07-18T18:16:09.090203Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")  # トレーニングデータのパス\n",
    "    test_path = os.path.join(root, \"test.csv\")    # テストデータのパス\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")  # サンプル提出データのパス\n",
    "    seed = 42  # 乱数シードを42に設定\n",
    "    n_splits = 10  # クロスバリデーションの分割数を10に設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9013e40",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)\n",
    "test = pd.read_csv(config.test_path)\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)  # トレーニングデータを読み込む\n",
    "test = pd.read_csv(config.test_path)    # テストデータを読み込む\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込む\n",
    "\n",
    "if test.shape[0] < 10:  # テストデータのサンプル数が10未満の場合\n",
    "    train = train.iloc[:10000]  # 最初の10000行のみをトレーニングデータに使用\n",
    "    \n",
    "def process(input_str):  # 入力文字列を処理する関数\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を除去\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 行を分割し、不要なダブルクオートを除去\n",
    "    return ' '.join(sentences)  # 処理した文を空白区切りで結合して返す\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)  # \"prompt\"列を処理\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)  # \"response_a\"列を処理\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)  # \"response_b\"列を処理\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)  # テストデータの\"prompt\"列を処理\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)  # テストデータの\"response_a\"列を処理\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)  # テストデータの\"response_b\"列を処理\n",
    "\n",
    "print(f\"train shape: {train.shape}\")  # トレーニングデータの形状を表示\n",
    "print(f\"test shape: {test.shape}\")  # テストデータの形状を表示\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")  # トレーニングデータの欠損値を表示\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")  # テストデータの欠損値を表示\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # トレーニングデータの先頭を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T18:16:25.39013Z",
     "iopub.status.busy": "2024-07-18T18:16:25.389745Z",
     "iopub.status.idle": "2024-07-18T18:16:28.741512Z",
     "shell.execute_reply": "2024-07-18T18:16:28.74062Z",
     "shell.execute_reply.started": "2024-07-18T18:16:25.390099Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.train_path)  # トレーニングデータを読み込む\n",
    "test = pd.read_csv(config.test_path)    # テストデータを読み込む\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込む\n",
    "\n",
    "if test.shape[0] < 10:  # テストデータのサンプル数が10未満の場合\n",
    "    train = train.iloc[:10000]  # 最初の10000行のみをトレーニングデータに使用\n",
    "    \n",
    "def process(input_str):  # 入力文字列を処理する関数\n",
    "    stripped_str = input_str.strip('[]')  # 角括弧を除去\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 行を分割し、不要なダブルクオートを除去\n",
    "    return ' '.join(sentences)  # 処理した文を空白区切りで結合して返す\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)  # \"prompt\"列を処理\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)  # \"response_a\"列を処理\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)  # \"response_b\"列を処理\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)  # テストデータの\"prompt\"列を処理\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)  # テストデータの\"response_a\"列を処理\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)  # テストデータの\"response_b\"列を処理\n",
    "\n",
    "print(f\"train shape: {train.shape}\")  # トレーニングデータの形状を表示\n",
    "print(f\"test shape: {test.shape}\")  # テストデータの形状を表示\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")  # トレーニングデータの欠損値を表示\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")  # テストデータの欠損値を表示\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # トレーニングデータの先頭を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8856b058",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "            vectorizer.fit([text1, text2])\n",
    "            output = vectorizer.transform([text1, text2]).toarray()\n",
    "            cos_sim = cosine_similarity(output)\n",
    "            return cos_sim[0][1]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())\n",
    "        set2 = set(text2.split())\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def count_new_lines(self, text: str) -> int:\n",
    "        return text.count('\\\\n') \n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"\n",
    "        double_quote_pattern = r'\"(.*?)\"'\n",
    "        single_quotes = re.findall(single_quote_pattern, text)\n",
    "        double_quotes = re.findall(double_quote_pattern, text)\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)\n",
    "        return len(single_quotes) + len(double_quotes)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)\n",
    "            ngrams2 = self.generate_ngrams(text2, n)\n",
    "            counter1 = Counter(ngrams1)\n",
    "            counter2 = Counter(ngrams2)\n",
    "            overlap = counter1 & counter2\n",
    "            overlap_count = sum(overlap.values())\n",
    "            return overlap_count\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        \n",
    "        data[\"respa_new_lines\"] = data[\"response_a\"].apply(lambda x: self.count_new_lines(x))\n",
    "        data[\"respb_new_lines\"] = data[\"response_b\"].apply(lambda x: self.count_new_lines(x))\n",
    "        data[\"prompt_new_lines\"] = data[\"prompt\"].apply(lambda x: self.count_new_lines(x))\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_len_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_len_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_len_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_len_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_len_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_len_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"overlap_unigram_diff\"] = data[\"respa_prompt_overlap_unigram\"] - data[\"respb_prompt_overlap_unigram\"]\n",
    "        data[\"overlap_bigram_diff\"] = data[\"respa_prompt_overlap_bigram\"] - data[\"respb_prompt_overlap_bigram\"]\n",
    "        data[\"overlap_trigram_diff\"] = data[\"respa_prompt_overlap_trigram\"] - data[\"respb_prompt_overlap_trigram\"]\n",
    "        \n",
    "        data[\"overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"respa_prompt_overlap_unigram\"] \n",
    "        data[\"overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"respa_prompt_overlap_bigram\"] \n",
    "        data[\"overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"respa_prompt_overlap_trigram\"] \n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"jaccard_sim_diff\"] = data[\"respa_prompt_jaccard_sim\"] - data[\"respb_prompt_jaccard_sim\"]\n",
    "        data[\"jaccard_sim_ratio\"] = data[\"respb_prompt_jaccard_sim\"] / data[\"respa_prompt_jaccard_sim\"]\n",
    "        \n",
    "        return data\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):  # コサイン類似度を計算するメソッド\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # TF-IDFベクトル化器を作成（1～3グラム）\n",
    "            vectorizer.fit([text1, text2])  # テキストをフィッティング\n",
    "            output = vectorizer.transform([text1, text2]).toarray()  # TF-IDF行列を生成\n",
    "            cos_sim = cosine_similarity(output)  # コサイン類似度を計算\n",
    "            return cos_sim[0][1]  # 二つのテキスト間のコサイン類似度を返す\n",
    "        except:\n",
    "            return np.nan  # エラーが発生した場合はNaNを返す\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):  # ジャッカード類似度を計算するメソッド\n",
    "        set1 = set(text1.split())  # text1を単語に分割しセットに変換\n",
    "        set2 = set(text2.split())  # text2を単語に分割しセットに変換\n",
    "        intersection = set1.intersection(set2)  # 二つのセットの積集合を計算\n",
    "        union = set1.union(set2)  # 二つのセットの和集合を計算\n",
    "        return len(intersection) / len(union)  # ジャッカード類似度を計算して返す\n",
    "    \n",
    "    def count_new_lines(self, text: str) -> int:  # テキスト内の改行数をカウントするメソッド\n",
    "        return text.count('\\\\n')  # 改行の数を返す\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:  # テキスト内の引用符の数をカウントするメソッド\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # シングルクォートのパターン\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # ダブルクォートのパターン\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # シングルクォートを探す\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # ダブルクォートを探す\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用符の数をカウント\n",
    "        return len(single_quotes) + len(double_quotes)  # 合計の引用符の数を返す\n",
    "\n",
    "    def tokenize(self, text: str):  # テキストをトークン化するメソッド\n",
    "        return nltk.word_tokenize(text.lower())  # 小文字に変換してトークン化して返す\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):  # n-グラムを生成するメソッド\n",
    "        tokens = self.tokenize(text)  # テキストをトークン化\n",
    "        return list(ngrams(tokens, n))  # n-グラムのリストを生成して返す\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:  # n-グラムの重複数をカウントするメソッド\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # text1からn-グラムを生成\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # text2からn-グラムを生成\n",
    "            counter1 = Counter(ngrams1)  # text1のn-グラムをカウント\n",
    "            counter2 = Counter(ngrams2)  # text2のn-グラムをカウント\n",
    "            overlap = counter1 & counter2  # カウンターの共通部分を計算\n",
    "            overlap_count = sum(overlap.values())  # 重複の合計数を計算\n",
    "            return overlap_count  # 重複の数を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:  # データを処理するメソッド\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)  # response_aとresponse_bのユニグラム重複を計算\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)  # response_aとresponse_bのバイグラム重複を計算\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)  # response_aとresponse_bのトライグラム重複を計算\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)  # response_aとpromptのユニグラム重複を計算\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)  # response_aとpromptのバイグラム重複を計算\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)  # response_aとpromptのトライグラム重複を計算\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)  # response_bとpromptのユニグラム重複を計算\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)  # response_bとpromptのバイグラム重複を計算\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)  # response_bとpromptのトライグラム重複を計算\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # response_aのトークン数を計算\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # response_bのトークン数を計算\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # promptのトークン数を計算\n",
    "        \n",
    "        data[\"respa_new_lines\"] = data[\"response_a\"].apply(lambda x: self.count_new_lines(x))  # response_aの改行数を数える\n",
    "        data[\"respb_new_lines\"] = data[\"response_b\"].apply(lambda x: self.count_new_lines(x))  # response_bの改行数を数える\n",
    "        data[\"prompt_new_lines\"] = data[\"prompt\"].apply(lambda x: self.count_new_lines(x))  # promptの改行数を数える\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # response_aの長さとpromptの比率を計算\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # response_bの長さとpromptの比率を計算\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # response_aとresponse_bの長さの比率を計算\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # response_aとresponse_bの長さの差を計算\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # response_aとpromptの長さの差を計算\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # response_bとpromptの長さの差を計算\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_len_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # ユニグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respa_prompt_overlap_bigram_len_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # バイグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respa_prompt_overlap_trigram_len_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # トライグラム重複とpromptの長さの比率を計算\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_len_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # ユニグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respb_prompt_overlap_bigram_len_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # バイグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respb_prompt_overlap_trigram_len_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # トライグラム重複とpromptの長さの比率を計算\n",
    "        \n",
    "        data[\"overlap_unigram_diff\"] = data[\"respa_prompt_overlap_unigram\"] - data[\"respb_prompt_overlap_unigram\"]  # ユニグラム重複の差を計算\n",
    "        data[\"overlap_bigram_diff\"] = data[\"respa_prompt_overlap_bigram\"] - data[\"respb_prompt_overlap_bigram\"]  # バイグラム重複の差を計算\n",
    "        data[\"overlap_trigram_diff\"] = data[\"respa_prompt_overlap_trigram\"] - data[\"respb_prompt_overlap_trigram\"]  # トライグラム重複の差を計算\n",
    "        \n",
    "        data[\"overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"respa_prompt_overlap_unigram\"]  # ユニグラム重複の比率を計算\n",
    "        data[\"overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"respa_prompt_overlap_bigram\"]  # バイグラム重複の比率を計算\n",
    "        data[\"overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"respa_prompt_overlap_trigram\"]  # トライグラム重複の比率を計算 \n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # response_aの引用符の数をカウント\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # response_bの引用符の数をカウント\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # promptの引用符の数をカウント\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)  # response_aとresponse_bのコサイン類似度を計算\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)  # response_aとresponse_bのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)  # response_aとpromptのコサイン類似度を計算\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)  # response_aとpromptのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)  # response_bとpromptのコサイン類似度を計算\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)  # response_bとpromptのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"jaccard_sim_diff\"] = data[\"respa_prompt_jaccard_sim\"] - data[\"respb_prompt_jaccard_sim\"]  # ジャッカード類似度の差を計算\n",
    "        data[\"jaccard_sim_ratio\"] = data[\"respb_prompt_jaccard_sim\"] / data[\"respa_prompt_jaccard_sim\"]  # ジャッカード類似度の比率を計算\n",
    "        \n",
    "        return data  # 処理後のデータを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T18:17:00.086969Z",
     "iopub.status.busy": "2024-07-18T18:17:00.086606Z",
     "iopub.status.idle": "2024-07-18T18:17:00.120027Z",
     "shell.execute_reply": "2024-07-18T18:17:00.119037Z",
     "shell.execute_reply.started": "2024-07-18T18:17:00.086939Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):  # コサイン類似度を計算するメソッド\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # TF-IDFベクトル化器を作成（1～3グラム）\n",
    "            vectorizer.fit([text1, text2])  # テキストをフィッティング\n",
    "            output = vectorizer.transform([text1, text2]).toarray()  # TF-IDF行列を生成\n",
    "            cos_sim = cosine_similarity(output)  # コサイン類似度を計算\n",
    "            return cos_sim[0][1]  # 二つのテキスト間のコサイン類似度を返す\n",
    "        except:\n",
    "            return np.nan  # エラーが発生した場合はNaNを返す\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):  # ジャッカード類似度を計算するメソッド\n",
    "        set1 = set(text1.split())  # text1を単語に分割しセットに変換\n",
    "        set2 = set(text2.split())  # text2を単語に分割しセットに変換\n",
    "        intersection = set1.intersection(set2)  # 二つのセットの積集合を計算\n",
    "        union = set1.union(set2)  # 二つのセットの和集合を計算\n",
    "        return len(intersection) / len(union)  # ジャッカード類似度を計算して返す\n",
    "    \n",
    "    def count_new_lines(self, text: str) -> int:  # テキスト内の改行数をカウントするメソッド\n",
    "        return text.count('\\\\n')  # 改行の数を返す\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:  # テキスト内の引用符の数をカウントするメソッド\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # シングルクォートのパターン\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # ダブルクォートのパターン\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # シングルクォートを探す\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # ダブルクォートを探す\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用符の数をカウント\n",
    "        return len(single_quotes) + len(double_quotes)  # 合計の引用符の数を返す\n",
    "\n",
    "    def tokenize(self, text: str):  # テキストをトークン化するメソッド\n",
    "        return nltk.word_tokenize(text.lower())  # 小文字に変換してトークン化して返す\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):  # n-グラムを生成するメソッド\n",
    "        tokens = self.tokenize(text)  # テキストをトークン化\n",
    "        return list(ngrams(tokens, n))  # n-グラムのリストを生成して返す\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:  # n-グラムの重複数をカウントするメソッド\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # text1からn-グラムを生成\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # text2からn-グラムを生成\n",
    "            counter1 = Counter(ngrams1)  # text1のn-グラムをカウント\n",
    "            counter2 = Counter(ngrams2)  # text2のn-グラムをカウント\n",
    "            overlap = counter1 & counter2  # カウンターの共通部分を計算\n",
    "            overlap_count = sum(overlap.values())  # 重複の合計数を計算\n",
    "            return overlap_count  # 重複の数を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:  # データを処理するメソッド\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)  # response_aとresponse_bのユニグラム重複を計算\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)  # response_aとresponse_bのバイグラム重複を計算\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)  # response_aとresponse_bのトライグラム重複を計算\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)  # response_aとpromptのユニグラム重複を計算\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)  # response_aとpromptのバイグラム重複を計算\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)  # response_aとpromptのトライグラム重複を計算\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)  # response_bとpromptのユニグラム重複を計算\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)  # response_bとpromptのバイグラム重複を計算\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)  # response_bとpromptのトライグラム重複を計算\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # response_aのトークン数を計算\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # response_bのトークン数を計算\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # promptのトークン数を計算\n",
    "        \n",
    "        data[\"respa_new_lines\"] = data[\"response_a\"].apply(lambda x: self.count_new_lines(x))  # response_aの改行数を数える\n",
    "        data[\"respb_new_lines\"] = data[\"response_b\"].apply(lambda x: self.count_new_lines(x))  # response_bの改行数を数える\n",
    "        data[\"prompt_new_lines\"] = data[\"prompt\"].apply(lambda x: self.count_new_lines(x))  # promptの改行数を数える\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # response_aの長さとpromptの比率を計算\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # response_bの長さとpromptの比率を計算\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # response_aとresponse_bの長さの比率を計算\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # response_aとresponse_bの長さの差を計算\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # response_aとpromptの長さの差を計算\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # response_bとpromptの長さの差を計算\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_len_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # ユニグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respa_prompt_overlap_bigram_len_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # バイグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respa_prompt_overlap_trigram_len_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # トライグラム重複とpromptの長さの比率を計算\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_len_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]  # ユニグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respb_prompt_overlap_bigram_len_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]  # バイグラム重複とpromptの長さの比率を計算\n",
    "        data[\"respb_prompt_overlap_trigram_len_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]  # トライグラム重複とpromptの長さの比率を計算\n",
    "        \n",
    "        data[\"overlap_unigram_diff\"] = data[\"respa_prompt_overlap_unigram\"] - data[\"respb_prompt_overlap_unigram\"]  # ユニグラム重複の差を計算\n",
    "        data[\"overlap_bigram_diff\"] = data[\"respa_prompt_overlap_bigram\"] - data[\"respb_prompt_overlap_bigram\"]  # バイグラム重複の差を計算\n",
    "        data[\"overlap_trigram_diff\"] = data[\"respa_prompt_overlap_trigram\"] - data[\"respb_prompt_overlap_trigram\"]  # トライグラム重複の差を計算\n",
    "        \n",
    "        data[\"overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"respa_prompt_overlap_unigram\"]  # ユニグラム重複の比率を計算\n",
    "        data[\"overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"respa_prompt_overlap_bigram\"]  # バイグラム重複の比率を計算\n",
    "        data[\"overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"respa_prompt_overlap_trigram\"]  # トライグラム重複の比率を計算 \n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # response_aの引用符の数をカウント\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # response_bの引用符の数をカウント\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # promptの引用符の数をカウント\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)  # response_aとresponse_bのコサイン類似度を計算\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)  # response_aとresponse_bのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)  # response_aとpromptのコサイン類似度を計算\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)  # response_aとpromptのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)  # response_bとpromptのコサイン類似度を計算\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)  # response_bとpromptのジャッカード類似度を計算\n",
    "        \n",
    "        data[\"jaccard_sim_diff\"] = data[\"respa_prompt_jaccard_sim\"] - data[\"respb_prompt_jaccard_sim\"]  # ジャッカード類似度の差を計算\n",
    "        data[\"jaccard_sim_ratio\"] = data[\"respb_prompt_jaccard_sim\"] / data[\"respa_prompt_jaccard_sim\"]  # ジャッカード類似度の比率を計算\n",
    "        \n",
    "        return data  # 処理後のデータを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a1d455",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "preprocessor = Preprocessor()\n",
    "train = preprocessor.run(train)\n",
    "test = preprocessor.run(test)\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "preprocessor = Preprocessor()  # Preprocessorのインスタンスを生成\n",
    "train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行\n",
    "test = preprocessor.run(test)  # テストデータに対して前処理を実行\n",
    "train.head()  # トレーニングデータの先頭を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T18:17:09.501873Z",
     "iopub.status.busy": "2024-07-18T18:17:09.501527Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "preprocessor = Preprocessor()  # Preprocessorのインスタンスを生成\n",
    "train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行\n",
    "test = preprocessor.run(test)  # テストデータに対して前処理を実行\n",
    "train.head()  # トレーニングデータの先頭を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e38618d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = np.nan\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx\n",
    "train[target] = train[target].astype(\"int32\")\n",
    "    \n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # 除外する列のリスト\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ターゲットとなる列のリスト\n",
    "target = \"target\"  # ターゲットの名前\n",
    "\n",
    "train[target] = np.nan  # ターゲット列をNaNで初期化\n",
    "for idx, t in enumerate(target_cols):  # ターゲット列をループ\n",
    "    train.loc[train[t] == 1, target] = idx  # 勝者モデルをターゲットとして設定\n",
    "train[target] = train[target].astype(\"int32\")  # ターゲット列の型を整数型に変換\n",
    "    \n",
    "train.head()  # トレーニングデータの先頭を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # 除外する列のリスト\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ターゲットとなる列のリスト\n",
    "target = \"target\"  # ターゲットの名前\n",
    "\n",
    "train[target] = np.nan  # ターゲット列をNaNで初期化\n",
    "for idx, t in enumerate(target_cols):  # ターゲット列をループ\n",
    "    train.loc[train[t] == 1, target] = idx  # 勝者モデルをターゲットとして設定\n",
    "train[target] = train[target].astype(\"int32\")  # ターゲット列の型を整数型に変換\n",
    "    \n",
    "train.head()  # トレーニングデータの先頭を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a41aa06",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)\n",
    "y = train[target]\n",
    "X_test = test.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols + drop_cols + [target] + [\"model_a\", \"model_b\"], axis=1)  # 特徴量Xを定義（不要な列を除外）\n",
    "y = train[target]  # ターゲットyを定義\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # テストデータから特徴量X_testを定義\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)  # 特徴量Xの無限値をNaNに置き換え\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)  # テストデータの無限値をNaNに置き換え\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=target_cols + drop_cols + [target] + [\"model_a\", \"model_b\"], axis=1)  # 特徴量Xを定義（不要な列を除外）\n",
    "y = train[target]  # ターゲットyを定義\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # テストデータから特徴量X_testを定義\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)  # 特徴量Xの無限値をNaNに置き換え\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)  # テストデータの無限値をNaNに置き換え"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cca92a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))\n",
    "cv_scores = list()\n",
    "\n",
    "features = X.columns.tolist()\n",
    "feat_imp_df = pd.DataFrame({\"feature\": features})\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n",
    "    print(f\"| Fold {idx+1} |\".center(90, \"=\"))\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]\n",
    "    X_val, y_val = X.loc[val_idx], y.loc[val_idx]\n",
    "\n",
    "    print(f'train: {X_train.shape}')\n",
    "    print(f'val: {X_val.shape}')\n",
    "    \n",
    "    model = xgb.XGBClassifier(\n",
    "        objective='multi:softprob',\n",
    "        num_class=3,\n",
    "        eval_metric='mlogloss',\n",
    "        subsample=0.8,\n",
    "        n_estimators=650,\n",
    "        learning_rate=0.045,\n",
    "        max_depth=5,\n",
    "        random_state=config.seed\n",
    "    )\n",
    "    \n",
    "    model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "        early_stopping_rounds=75,\n",
    "        verbose=75\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict_proba(X_val)\n",
    "    val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")\n",
    "    print(f\"val log loss: {val_log_loss:.5f}\")\n",
    "    cv_scores.append(val_log_loss)\n",
    "    \n",
    "    test_preds += model.predict_proba(X_test) / cv.get_n_splits()\n",
    "    \n",
    "    feat_imp_df = feat_imp_df.merge(\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": features,\n",
    "                f\"fold_{idx+1}_feat_imp\": model.feature_importances_,\n",
    "            }\n",
    "        ),\n",
    "        on=[\"feature\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "print(\"=\"*90)\n",
    "print(f\"CV: {np.mean(cv_scores):.5f}\")\n",
    "\n",
    "feat_imp_df[\"avg_importance\"] = feat_imp_df.iloc[:, 1:].mean(axis=1)\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.barplot(\n",
    "    data=feat_imp_df.sort_values(by=\"avg_importance\", ascending=False).iloc[\n",
    "        :50\n",
    "    ],\n",
    "    x=\"avg_importance\",\n",
    "    y=\"feature\",\n",
    "    color=\"royalblue\",\n",
    "    width=0.75,\n",
    ")\n",
    "plt.title(\"Average Feature Importances of All Folds\", size=12)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # ストラティファイドKフォールドの設定\n",
    "test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テスト予測の初期化\n",
    "cv_scores = list()  # クロスバリデーションスコアのリスト\n",
    "\n",
    "features = X.columns.tolist()  # 特徴量のリストを取得\n",
    "feat_imp_df = pd.DataFrame({\"feature\": features})  # 特徴量の重要度を格納するデータフレームを作成\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):  # Kフォールド分割に基づいて訓練インデックスと検証インデックスを取得\n",
    "    print(f\"| Fold {idx+1} |\".center(90, \"=\"))  # 現在のフォールドの表示\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # 訓練データとラベルの取得\n",
    "    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # 検証データとラベルの取得\n",
    "\n",
    "    print(f'train: {X_train.shape}')  # 訓練データの形状を表示\n",
    "    print(f'val: {X_val.shape}')  # 検証データの形状を表示\n",
    "    \n",
    "    model = xgb.XGBClassifier(  # XGBoostの分類器を定義\n",
    "        objective='multi:softprob',  # 目的関数を多クラスの確率に設定\n",
    "        num_class=3,  # クラス数を3に設定\n",
    "        eval_metric='mlogloss',  # 評価指標を対数損失に設定\n",
    "        subsample=0.8,  # サンプリング率を0.8に設定\n",
    "        n_estimators=650,  # 弱学習器の数を650に設定\n",
    "        learning_rate=0.045,  # 学習率を0.045に設定\n",
    "        max_depth=5,  # 木の最大深さを5に設定\n",
    "        random_state=config.seed  # 乱数シードを設定\n",
    "    )\n",
    "    \n",
    "    model.fit(  # モデルの訓練\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],  # 評価セットを訓練データと検証データに設定\n",
    "        early_stopping_rounds=75,  # 75ラウンドで早期停止\n",
    "        verbose=75  # 75回ごとに出力\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict_proba(X_val)  # 検証データに対する確率予測\n",
    "    val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # 検証データに対する対数損失を計算\n",
    "    print(f\"val log loss: {val_log_loss:.5f}\")  # 検証データの対数損失を表示\n",
    "    cv_scores.append(val_log_loss)  # クロスバリデーションスコアを追加\n",
    "    \n",
    "    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # 各フォールドのテスト予測を累積\n",
    "    \n",
    "    feat_imp_df = feat_imp_df.merge(  # 特徴量の重要度をデータフレームに追加\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": features,  # 特徴量名\n",
    "                f\"fold_{idx+1}_feat_imp\": model.feature_importances_,  # 各フォールドの特徴量重要度\n",
    "            }\n",
    "        ),\n",
    "        on=[\"feature\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "print(\"=\"*90)  # 区切り線を表示\n",
    "print(f\"CV: {np.mean(cv_scores):.5f}\")  # クロスバリデーションの平均スコアを表示\n",
    "\n",
    "feat_imp_df[\"avg_importance\"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴量の平均重要度を計算\n",
    "plt.figure(figsize=(12, 10))  # プロットサイズを設定\n",
    "sns.barplot(  # バープロットを作成\n",
    "    data=feat_imp_df.sort_values(by=\"avg_importance\", ascending=False).iloc[:50],  # 重要度が高い50の特徴量を表示\n",
    "    x=\"avg_importance\",\n",
    "    y=\"feature\",\n",
    "    color=\"royalblue\",\n",
    "    width=0.75,\n",
    ")\n",
    "plt.title(\"Average Feature Importances of All Folds\", size=12)  # タイトルを設定\n",
    "plt.show()  # プロットを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # ストラティファイドKフォールドの設定\n",
    "test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テスト予測の初期化\n",
    "cv_scores = list()  # クロスバリデーションスコアのリスト\n",
    "\n",
    "features = X.columns.tolist()  # 特徴量のリストを取得\n",
    "feat_imp_df = pd.DataFrame({\"feature\": features})  # 特徴量の重要度を格納するデータフレームを作成\n",
    "\n",
    "for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):  # Kフォールド分割に基づいて訓練インデックスと検証インデックスを取得\n",
    "    print(f\"| Fold {idx+1} |\".center(90, \"=\"))  # 現在のフォールドの表示\n",
    "    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # 訓練データとラベルの取得\n",
    "    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # 検証データとラベルの取得\n",
    "\n",
    "    print(f'train: {X_train.shape}')  # 訓練データの形状を表示\n",
    "    print(f'val: {X_val.shape}')  # 検証データの形状を表示\n",
    "    \n",
    "    model = xgb.XGBClassifier(  # XGBoostの分類器を定義\n",
    "        objective='multi:softprob',  # 目的関数を多クラスの確率に設定\n",
    "        num_class=3,  # クラス数を3に設定\n",
    "        eval_metric='mlogloss',  # 評価指標を対数損失に設定\n",
    "        subsample=0.8,  # サンプリング率を0.8に設定\n",
    "        n_estimators=650,  # 弱学習器の数を650に設定\n",
    "        learning_rate=0.045,  # 学習率を0.045に設定\n",
    "        max_depth=5,  # 木の最大深さを5に設定\n",
    "        random_state=config.seed  # 乱数シードを設定\n",
    "    )\n",
    "    \n",
    "    model.fit(  # モデルの訓練\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_val, y_val)],  # 評価セットを訓練データと検証データに設定\n",
    "        early_stopping_rounds=75,  # 75ラウンドで早期停止\n",
    "        verbose=75  # 75回ごとに出力\n",
    "    )\n",
    "    \n",
    "    val_preds = model.predict_proba(X_val)  # 検証データに対する確率予測\n",
    "    val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # 検証データに対する対数損失を計算\n",
    "    print(f\"val log loss: {val_log_loss:.5f}\")  # 検証データの対数損失を表示\n",
    "    cv_scores.append(val_log_loss)  # クロスバリデーションスコアを追加\n",
    "    \n",
    "    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # 各フォールドのテスト予測を累積\n",
    "    \n",
    "    feat_imp_df = feat_imp_df.merge(  # 特徴量の重要度をデータフレームに追加\n",
    "        pd.DataFrame(\n",
    "            {\n",
    "                \"feature\": features,  # 特徴量名\n",
    "                f\"fold_{idx+1}_feat_imp\": model.feature_importances_,  # 各フォールドの特徴量重要度\n",
    "            }\n",
    "        ),\n",
    "        on=[\"feature\"],\n",
    "        how=\"left\",\n",
    "    )\n",
    "\n",
    "print(\"=\"*90)  # 区切り線を表示\n",
    "print(f\"CV: {np.mean(cv_scores):.5f}\")  # クロスバリデーションの平均スコアを表示\n",
    "\n",
    "feat_imp_df[\"avg_importance\"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴量の平均重要度を計算\n",
    "plt.figure(figsize=(12, 10))  # プロットサイズを設定\n",
    "sns.barplot(  # バープロットを作成\n",
    "    data=feat_imp_df.sort_values(by=\"avg_importance\", ascending=False).iloc[:50],  # 重要度が高い50の特徴量を表示\n",
    "    x=\"avg_importance\",\n",
    "    y=\"feature\",\n",
    "    color=\"royalblue\",\n",
    "    width=0.75,\n",
    ")\n",
    "plt.title(\"Average Feature Importances of All Folds\", size=12)  # タイトルを設定\n",
    "plt.show()  # プロットを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c84a18",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]\n",
    "sample_submission.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):  # ターゲット列をループ\n",
    "    sample_submission[t] = test_preds[:, idx]  # ターゲット列にテスト予測を格納\n",
    "sample_submission.head()  # サンプル提出データの先頭を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):  # ターゲット列をループ\n",
    "    sample_submission[t] = test_preds[:, idx]  # ターゲット列にテスト予測を格納\n",
    "sample_submission.head()  # サンプル提出データの先頭を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c944f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)  # 提出ファイルとしてCSVに保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)  # 提出ファイルとしてCSVに保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
