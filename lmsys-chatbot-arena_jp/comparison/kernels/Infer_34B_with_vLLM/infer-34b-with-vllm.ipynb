{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a47cf11",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」における、LLM（大規模言語モデル）34Bを用いた問題解決に取り組んでいます。具体的には、モデルの推論を5時間で完了させる方法を示しています。以下に、ノートブックの主な内容と使用手法について要約します。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "- **迅速なモデル提出**: 大規模言語モデルを短時間で提出する方法を求めています。具体的には、LLM 34Bモデルを用いて、提出の際に求められるレスポンスを迅速に生成することを目的としています。\n",
    "\n",
    "### 使用手法・ライブラリ\n",
    "1. **vLLM**: 高速なLLM推論ライブラリであり、推論速度を向上させるために使用されています。環境によってはエラーが出るため、再インストールが推奨されています。\n",
    "\n",
    "2. **AWQ（Adaptive Weight Quantization）**: 4ビット量子化を用いることで、GPUのVRAMを効率的に使用する手法です。これにより、大きなモデルをメモリ制約のある環境で実行可能にしています。\n",
    "\n",
    "3. **トークン制限**: \n",
    "   - **入力サイズ**: 最大1024トークンに制限し、速度を向上させます。\n",
    "   - **出力サイズ**: 生成される応答のトークンを1トークンに制限することで、処理速度を加速させています。\n",
    "\n",
    "4. **トークナイザー**: モデルに最適化されたトークナイザーを使用し、入力を整形します。\n",
    "\n",
    "5. **プロンプト工夫**: モデルが出力するトークンを制御し、特定のトークン(A、B、tie)に対して予測を強化するためのロジットプロセッサーを定義しています。これにより、ユーザーが選択するべき応答を促進します。\n",
    "\n",
    "6. **CSVファイルの生成**: 提出用のCSVファイルを作成するプロセスが含まれており、推論結果を適切な形式で出力します。\n",
    "\n",
    "7. **CVスコア計算**: 提出を通じて、交差検証スコア（CVスコア）を計算し、結果の性能を評価します。\n",
    "\n",
    "このノートブックは、モデルのトレーニングや微調整に加え、推論速度や精度を管理するための細やかな配慮がなされており、特にリソースの限られた環境での実用的なアプローチを提示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64672513",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、初心者がつまずきそうな専門用語の簡単な解説です。特に、実務経験が少ない場合や、この特定のノートブックに関連した知識が必要な用語に焦点を当てています。\n",
    "\n",
    "1. **vLLM**:\n",
    "   - *Very Large Language Model*の略。大規模な言語モデルの推論のために設計された非常に高速なライブラリ。このライブラリを使用することで、サンプリングやトークン生成が効率的に行えます。\n",
    "\n",
    "2. **AWQ (Adaptive Weight Quantization)**:\n",
    "   - 重みを量子化する手法。モデルのストレージや推論時のメモリ消費を削減するために、モデルの重みをより少ないビットで表現します。この手法は特に4ビット量子化で使用され、性能とメモリ利用のバランスを取るのに役立ちます。\n",
    "\n",
    "3. **トークン**:\n",
    "   - モデルが処理する基本的な単位で、通常は単語や文字の断片。入力テキストをトークンに分割し、これを基にしてモデルが予測を行います。\n",
    "\n",
    "4. **ロジットプロセッサ (LogitsProcessor)**:\n",
    "   - 出力のスコア（ロジット）を操作するためのクラス。このクラスを通じて、モデルの出力トークンの選擇を制御したり、特定のトークンを優先させたりできます。\n",
    "\n",
    "5. **グラデーションチェックポイント (Gradient Checkpointing)**:\n",
    "   - メモリを節約するための手法で、バックプロパゲーション時に全ての中間結果を保存せず、いくつかの重要な中間結果だけを保存します。この手法により、より大きなモデルの訓練が可能になりますが、計算コストが増加します。\n",
    "\n",
    "6. **信頼されたリモートコード (trust_remote_code)**:\n",
    "   - リモートのコードを実行する際の設定。これを`True`に設定すると、リモートのソースから取得したコードを実行することを許可します。この設定は、セキュリティ上のリスクを伴うことがあります。\n",
    "\n",
    "7. **半精度 (half precision)**:\n",
    "   - 浮動小数点数の表現方法の一つで、32ビット（single precision）よりも少ない16ビットで数値を表現します。これにより、計算速度が向上し、メモリ利用量も減少するため、特に深層学習でよく使用されます。\n",
    "\n",
    "8. **トップP (top-p sampling)**:\n",
    "   - 確率的なトークンサンプリング手法の一つで、選択肢を累積確率に基づいて制限します。例えば、累積確率が95%になるまでのトークン群を考慮し、その中からランダムにサンプリングすることで、多様性を持った出力が得られます。\n",
    "\n",
    "9. **温度 (temperature)**:\n",
    "   - サンプリングの際に、出力の多様性を調整するパラメータ。低い温度は決定的な出力をもたらし、高い温度はより多様でランダムな出力を生成します。\n",
    "\n",
    "10. **CVスコア (Cross-Validation Score)**:\n",
    "   - モデルの一般化性能を評価するために、異なるデータセットで訓練と評価を繰り返した際のスコア。通常は、様々なトレーニングデータの分割を使用して、モデルがデータにどれだけ適応できるかを計測します。\n",
    "\n",
    "これらの解説が、ノートブックの内容を理解する上で役立つことを願っています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f585e08c",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Submit LLM 34B Model in 5 hours!\n",
    "This notebook demonstrates how to submit a LLM 34B model in only 5 hours! Amazing! The key tricks are:\n",
    "* use vLLM (for speed)\n",
    "* use AWQ 4bit quantization (to avoid GPU VRAM OOM)\n",
    "* limit input size to 1024 tokens (for speed)\n",
    "* limit output size to 1 token (for speed)\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LLM 34Bモデルを5時間で提出する方法！\n",
    "このノートブックでは、わずか5時間でLLM 34Bモデルを提出する方法を示します！すごいですね！主なポイントは以下の通りです：\n",
    "* 速度のためにvLLMを使用する\n",
    "* GPUのVRAM不足を避けるためにAWQ 4ビット量子化を使用する\n",
    "* 入力サイズを1024トークンに制限する（速度向上のため）\n",
    "* 出力サイズを1トークンに制限する（速度向上のため）\n",
    "\n",
    "# vLLMのインストール\n",
    "vLLMパッケージは非常に高速なLLM推論ライブラリです！KaggleノートブックにインストールされているvLLMではエラーが発生するため、再インストールが必要です。以下のコードはノートブック[こちら][1]から取得したものです。\n",
    "\n",
    "[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80abad0",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Pip Install vLLM\n",
    "The package vLLM is an incredibly fast LLM inference library! The vLLM that is installed in Kaggle notebooks will produce errors, therefore we need to reinstall vLLM. The code below was taken from notebook [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "import os, math, numpy as np  # osモジュール、mathモジュール、numpyライブラリをインポートします。\n",
    "\n",
    "# 環境変数CUDA_VISIBLE_DEVICESを設定します。\n",
    "# これにより、どのGPUデバイスを使用するかを指定できます。\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"  # 使用するGPUデバイスとして0番と1番を指定します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5712b722",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import os, math, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time  # このセルの実行にかかる時間を計測します。\n",
    "\n",
    "# torchパッケージをアンインストールします。\n",
    "# -yオプションを使用して、確認なしでアンインストールを行います。\n",
    "!pip uninstall -y torch  \n",
    "\n",
    "# vllmパッケージをアップグレードしてインストールします。\n",
    "# --no-indexオプションを使用してPyPIからのインストールを無効にし、\n",
    "# --find-linksオプションで指定したディレクトリからパッケージを探します。\n",
    "!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm  \n",
    "\n",
    "# grpcioの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  \n",
    "\n",
    "# rayの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-17T05:49:10.683275Z",
     "iopub.status.busy": "2024-07-17T05:49:10.682943Z",
     "iopub.status.idle": "2024-07-17T05:52:22.306408Z",
     "shell.execute_reply": "2024-07-17T05:52:22.305139Z",
     "shell.execute_reply.started": "2024-07-17T05:49:10.683249Z"
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time  # このセルの実行にかかる時間を計測します。\n",
    "\n",
    "# torchパッケージをアンインストールします。\n",
    "# -yオプションを使用して、確認なしでアンインストールを行います。\n",
    "!pip uninstall -y torch  \n",
    "\n",
    "# vllmパッケージをアップグレードしてインストールします。\n",
    "# --no-indexオプションを使用してPyPIからのインストールを無効にし、\n",
    "# --find-linksオプションで指定したディレクトリからパッケージを探します。\n",
    "!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm  \n",
    "\n",
    "# grpcioの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  \n",
    "\n",
    "# rayの特定バージョンをアップグレードしてインストールします。\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e0d709",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# vLLMを使って34B量子化モデルをロードする！\n",
    "LLM 34B Bagelモデルを[こちら][1]からロードして使用します。これは非常に強力なモデルです。\n",
    "\n",
    "[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919a31bd",
   "metadata": {},
   "source": [
    "# vLLMを使って34B量子化モデルをロードする！\n",
    "LLM 34B Bagelモデルを[こちら][1]からロードして使用します。これは非常に強力なモデルです。\n",
    "\n",
    "[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce88069",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load 34B Quantized Model with vLLM!\n",
    "We will load and use LLM 34B Bagel [here][1]. This is a strong model.\n",
    "\n",
    "[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "import vllm  # vLLMライブラリをインポートします。\n",
    "\n",
    "# LLMオブジェクトを作成します。\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/bagel-v3-343\",  # モデルのパスを指定します。\n",
    "    quantization=\"awq\",  # AWQ量子化を使用します。\n",
    "    tensor_parallel_size=2,  # テンソル並列処理のサイズを2に設定します。\n",
    "    gpu_memory_utilization=0.95,  # GPUメモリの使用率を95%に設定します。\n",
    "    trust_remote_code=True,  # リモートコードを信頼する設定です。\n",
    "    dtype=\"half\",  # データ型を半精度に設定します（メモリ効率を向上します）。\n",
    "    enforce_eager=True,  # イージー実行を強制します。\n",
    "    max_model_len=1024,  # モデルの最大長を1024トークンに設定します。\n",
    "    #distributed_executor_backend=\"ray\",  #（コメントアウト）分散実行のバックエンドをRayに設定します。\n",
    ")\n",
    "\n",
    "# トークナイザーを取得します。\n",
    "tokenizer = llm.get_tokenizer()  # モデルに関連付けられたトークナイザーを取得します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7a2923",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import vllm\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/bagel-v3-343\",\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=1024,\n",
    "    #distributed_executor_backend=\"ray\",\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# テストデータをロードする\n",
    "**コミット**の際にはCVスコアを計算するために128行のトレーニングデータをロードします。**提出**の際にはテストデータをロードします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1175b726",
   "metadata": {},
   "source": [
    "# テストデータをロードする\n",
    "**コミット**の際にはCVスコアを計算するために128行のトレーニングデータをロードします。**提出**の際にはテストデータをロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a685406d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load Test Data\n",
    "During **commit** we load 128 rows of train to compute CV score. During **submit**, we load the test data.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "import pandas as pd  # pandasライブラリをインポートします。\n",
    "\n",
    "VALIDATE = 128  # バリデーション用の行数を128に設定します。\n",
    "\n",
    "# テストデータをCSVファイルから読み込みます。\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\") \n",
    "\n",
    "# 読み込んだテストデータの行数が3の場合、\n",
    "# トレーニングデータを読み込み、最初の128行を取得します。\n",
    "if len(test) == 3:\n",
    "    test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test = test.iloc[:VALIDATE]  # 最初の128行を取得します。\n",
    "\n",
    "# テストデータの形状を出力します。\n",
    "print(test.shape)  # テストデータの行数と列数を表示します。\n",
    "\n",
    "# テストデータの最初の1行を表示します。\n",
    "test.head(1)  # テストデータの最初の1行を表示します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a69c8d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "VALIDATE = 128\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\") \n",
    "if len(test)==3:\n",
    "    test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test = test.iloc[:VALIDATE]\n",
    "print( test.shape )\n",
    "test.head(1)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# プロンプトの工夫\n",
    "ゼロショットLLMを提出したい場合、CVスコアを改善するためにさまざまなシステムプロンプトを試す必要があります。モデルをファインチューニングする場合、システムプロンプトはそれほど重要ではなくなります。なぜなら、モデルはターゲットから何をすべきかを学ぶため、どのシステムプロンプトを使用しても影響を受けないからです。\n",
    "\n",
    "ロジットプロセッサを使用して、モデルが私たちが興味を持っている3つのトークンを出力するよう強制します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2b2fde",
   "metadata": {},
   "source": [
    "# プロンプトの工夫\n",
    "ゼロショットLLMを提出したい場合、CVスコアを改善するためにさまざまなシステムプロンプトを試す必要があります。モデルをファインチューニングする場合、システムプロンプトはそれほど重要ではなくなります。なぜなら、モデルはターゲットから何をすべきかを学ぶため、どのシステムプロンプトを使用しても影響を受けないからです。\n",
    "\n",
    "ロジットプロセッサを使用して、モデルが私たちが興味を持っている3つのトークンを出力するよう強制します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f1c42e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Engineer Prompt\n",
    "If we want to submit zero shot LLM, we need to experiment with different system prompts to improve CV score. If we finetune the model, then system is not as important because the model will learn from the targets what to do regardless of which system prompt we use.\n",
    "\n",
    "We use a logits processor to force the model to output the 3 tokens we are interested in.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "from typing import Any, Dict, List  # 必要な型をインポートします。\n",
    "from transformers import LogitsProcessor  # LogitsProcessorクラスをインポートします。\n",
    "import torch  # PyTorchライブラリをインポートします。\n",
    "\n",
    "choices = [\"A\", \"B\", \"tie\"]  # 選択肢を定義します。\n",
    "\n",
    "KEEP = []  # 出力トークンとして保持するトークンIDのリストを初期化します。\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x, add_special_tokens=False)[0]  # トークンをエンコードし、リストから最初の要素を取得します。\n",
    "    KEEP.append(c)  # 取得したトークンIDをKEEPリストに追加します。\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")  # どのトークンを強制するか出力します。\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP  # 許可されたトークンIDを初期化します。\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100  # 許可されたトークンIDのスコアを100増加させます。\n",
    "        return scores  # 修正されたスコアを返します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea2ffe3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "\n",
    "choices = [\"A\",\"B\",\"tie\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "sys_prompt = \"\"\"以下のプロンプトと2つの応答を読み、それぞれの応答がどちらが優れているかを判断してください。\n",
    "もし応答が比較的新しい場合は、「tie」と返信してください。それ以外の場合は、「A」または「B」と応答し、どちらが優れているかを示してください。\"\"\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:54:53.883237Z",
     "iopub.status.busy": "2024-07-17T05:54:53.882323Z",
     "iopub.status.idle": "2024-07-17T05:54:53.890156Z",
     "shell.execute_reply": "2024-07-17T05:54:53.887959Z",
     "shell.execute_reply.started": "2024-07-17T05:54:53.883211Z"
    }
   },
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"以下のプロンプトと2つの応答を読み、それぞれの応答がどちらが優れているかを判断してください。\n",
    "もし応答が比較的新しい場合は、「tie」と返信してください。それ以外の場合は、「A」または「B」と応答し、どちらが優れているかを示してください。\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49282847",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "sys_prompt = \"\"\"Please read the following prompt and two responses. Determine which response is better.\n",
    "If the responses are relatively the same, respond with 'tie'. Otherwise respond with 'A' or 'B' to indicate which is better.\"\"\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "SS = \"#\"*25 + \"\\n\"  # \"#\"を25個連結し、改行を追加した文字列をSSに設定します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SS = \"#\"*25 + \"\\n\"  # \"#\"を25個連結し、改行を追加した文字列をSSに設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8388e9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "SS = \"#\"*25 + \"\\n\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "all_prompts = []  # プロンプトを保存するリストを初期化します。\n",
    "for index, row in test.iterrows():  # テストデータの各行をイテレートします。\n",
    "    \n",
    "    # プロンプト、応答A、応答Bを取得し、nullを空文字に置き換えます。\n",
    "    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))  # プロンプトを評価し、nullを空文字に置き換えた後、スペースで結合します。\n",
    "    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))  # 応答Aを評価・結合します。\n",
    "    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))  # 応答Bを評価・結合します。\n",
    "    \n",
    "    # フォーマットされたプロンプトを作成します。\n",
    "    prompt = f\"{SS}PROMPT: \" + a + f\"\\n\\n{SS}RESPONSE A: \" + b + f\"\\n\\n{SS}RESPONSE B: \" + c + \"\\n\\n\"\n",
    "    \n",
    "    # システムプロンプトとフォーマットされたプロンプトを結合します。\n",
    "    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n",
    "    \n",
    "    # フォーマットされたサンプルをリストに追加します。\n",
    "    all_prompts.append(formatted_sample)  # フォーマットされたサンプルをall_promptsリストに追加します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T05:54:53.918708Z",
     "iopub.status.busy": "2024-07-17T05:54:53.917724Z",
     "iopub.status.idle": "2024-07-17T05:54:54.168859Z",
     "shell.execute_reply": "2024-07-17T05:54:54.166722Z",
     "shell.execute_reply.started": "2024-07-17T05:54:53.918626Z"
    }
   },
   "outputs": [],
   "source": [
    "all_prompts = []  # プロンプトを保存するリストを初期化します。\n",
    "for index, row in test.iterrows():  # テストデータの各行をイテレートします。\n",
    "    \n",
    "    # プロンプト、応答A、応答Bを取得し、nullを空文字に置き換えます。\n",
    "    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))  # プロンプトを評価し、nullを空文字に置き換えた後、スペースで結合します。\n",
    "    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))  # 応答Aを評価・結合します。\n",
    "    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))  # 応答Bを評価・結合します。\n",
    "    \n",
    "    # フォーマットされたプロンプトを作成します。\n",
    "    prompt = f\"{SS}PROMPT: \" + a + f\"\\n\\n{SS}RESPONSE A: \" + b + f\"\\n\\n{SS}RESPONSE B: \" + c + \"\\n\\n\"\n",
    "    \n",
    "    # システムプロンプトとフォーマットされたプロンプトを結合します。\n",
    "    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n",
    "    \n",
    "    # フォーマットされたサンプルをリストに追加します。\n",
    "    all_prompts.append(formatted_sample)  # フォーマットされたサンプルをall_promptsリストに追加します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7022cc1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "all_prompts = []\n",
    "for index,row in test.iterrows():\n",
    "    \n",
    "    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))\n",
    "    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))\n",
    "    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))\n",
    "    \n",
    "    prompt = f\"{SS}PROMPT: \"+a+f\"\\n\\n{SS}RESPONSE A: \"+b+f\"\\n\\n{SS}RESPONSE B: \"+c+\"\\n\\n\"\n",
    "    \n",
    "    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n",
    "    \n",
    "    all_prompts.append( formatted_sample )\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# テストを推論する\n",
    "高速なvLLMを使用してテストを推論します。vLLMに対し、最初のトークンで予測される上位5つのトークンの確率を出力するように依頼します。また、推論速度を向上させるために、予測を1トークンに制限します。\n",
    "\n",
    "128のトレーニングサンプルを推論するのにかかる時間に基づいて、25,000のテストサンプルを推論するのにかかる時間を推測できます。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c860bfb",
   "metadata": {},
   "source": [
    "# テストを推論する\n",
    "高速なvLLMを使用してテストを推論します。vLLMに対し、最初のトークンで予測される上位5つのトークンの確率を出力するように依頼します。また、推論速度を向上させるために、予測を1トークンに制限します。\n",
    "\n",
    "128のトレーニングサンプルを推論するのにかかる時間に基づいて、25,000のテストサンプルを推論するのにかかる時間を推測できます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacb0ba5",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Infer Test\n",
    "We infer test using fast vLLM. We ask vLLM to output probabilties of the top 5 tokens considered to be predicted in the first token. We also limit prediction to 1 token to increase inference speed.\n",
    "\n",
    "Based on the speed it takes to infer 128 train samples, we can deduce how long inferring 25,000 test samples will take.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "%%time  # このセルの実行にかかる時間を計測します。\n",
    "\n",
    "from time import time  # timeモジュールからtime関数をインポートします。\n",
    "start = time()  # 実行開始時刻を記録します。\n",
    "\n",
    "# ロジットプロセッサを定義します。\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "# vLLMを使用して応答を生成します。\n",
    "responses = llm.generate(\n",
    "    all_prompts,  # すべてのプロンプトを指定します。\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # 各プロンプトに対して返す出力シーケンスの数。\n",
    "        top_p=0.9,  # 上位トークンを考慮する際の累積確率を制御する浮動小数点数。\n",
    "        temperature=0,  # サンプリングのランダム性。\n",
    "        seed=777,  # 再現性のためのシード。\n",
    "        skip_special_tokens=True,  # 出力で特殊トークンをスキップするかどうか。\n",
    "        max_tokens=1,  # 各出力シーケンスに生成する最大トークン数。\n",
    "        logits_processors=logits_processors,  # 使用するロジットプロセッサ。\n",
    "        logprobs=5  # 上位5つのトークン確率を出力。\n",
    "    ),\n",
    "    use_tqdm=True  # プログレスバーを表示するためのオプション。\n",
    ")\n",
    "\n",
    "end = time()  # 実行終了時刻を記録します。\n",
    "elapsed = (end - start) / 60.  # 経過時間を分単位で計算します。\n",
    "print(f\"{VALIDATE}サンプルの推論に{elapsed}分かかりました！\")  # 推論にかかった時間を出力します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb84a3ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 5\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")\n",
    "\n",
    "end = time()\n",
    "elapsed = (end-start)/60. #minutes\n",
    "print(f\"Inference of {VALIDATE} samples took {elapsed} minutes!\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "submit = 25_000 / 128 * elapsed / 60  # 25,000サンプルの推論にかかる時間を計算します。\n",
    "print(f\"提出には{submit}時間かかります\")  # 推論にかかる時間を出力します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:02:13.41828Z",
     "iopub.status.busy": "2024-07-17T06:02:13.417593Z",
     "iopub.status.idle": "2024-07-17T06:02:13.423213Z",
     "shell.execute_reply": "2024-07-17T06:02:13.422183Z",
     "shell.execute_reply.started": "2024-07-17T06:02:13.418247Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = 25_000 / 128 * elapsed / 60  # 25,000サンプルの推論にかかる時間を計算します。\n",
    "print(f\"提出には{submit}時間かかります\")  # 推論にかかる時間を出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e259c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "submit = 25_000 / 128 * elapsed / 60\n",
    "print(f\"Submit will take {submit} hours\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 推論確率の抽出\n",
    "これからvLLMの予測から「A」、「B」、「tie」の確率を抽出します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264f7fe",
   "metadata": {},
   "source": [
    "# 推論確率の抽出\n",
    "これからvLLMの予測から「A」、「B」、「tie」の確率を抽出します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc16d4",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Extract Inference Probabilites\n",
    "We now extract the probabilties of \"A\", \"B\", \"tie\" from the vLLM predictions.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "results = []  # 推論結果を格納するリストを初期化します。\n",
    "errors = 0  # エラーのカウンタを初期化します。\n",
    "\n",
    "for i, response in enumerate(responses):  # レスポンスの各要素をイテレートします。\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]  # 最初の出力のロジットを取得します。\n",
    "        logprobs = []  # ログ確率を格納するリストを初期化します。\n",
    "        for k in KEEP:  # KEEPに含まれる各トークンIDについて\n",
    "            if k in x:  # トークンIDがログ確率に存在する場合\n",
    "                logprobs.append(math.exp(x[k].logprob))  # ログ確率の指数を計算して追加します。\n",
    "            else:\n",
    "                logprobs.append(0)  # トークンIDが存在しない場合は0を追加します。\n",
    "                print(f\"bad logits {i}\")  # 不正なロジットが見つかったことを記録します。\n",
    "        logprobs = np.array(logprobs)  # ログ確率リストをNumPy配列に変換します。\n",
    "        logprobs /= logprobs.sum()  # 確率を正規化します。\n",
    "        results.append(logprobs)  # 結果リストに追加します。\n",
    "    except:  # エラーが発生した場合\n",
    "        # print(f\"error {i}\")  # エラーを記録（コメントアウトされています）。\n",
    "        results.append(np.array([1/3., 1/3., 1/3.]))  # 一様分布の確率を追加します。\n",
    "        errors += 1  # エラーカウンタを増やします。\n",
    "        \n",
    "print(f\"{i+1}回の推論のうちエラーは{errors}回発生しました。\")  # エラーの総数を出力します。\n",
    "results = np.vstack(results)  # 結果を垂直スタックして2次元配列を作成します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f871dfb6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "results = []\n",
    "errors = 0\n",
    "\n",
    "for i,response in enumerate(responses):\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append( math.exp(x[k].logprob) )\n",
    "            else:\n",
    "                logprobs.append( 0 )\n",
    "                print(f\"bad logits {i}\")\n",
    "        logprobs = np.array( logprobs )\n",
    "        logprobs /= logprobs.sum()\n",
    "        results.append( logprobs )\n",
    "    except:\n",
    "        #print(f\"error {i}\")\n",
    "        results.append( np.array([1/3., 1/3., 1/3.]) )\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "results = np.vstack(results)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 提出用CSVを作成する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d166e",
   "metadata": {},
   "source": [
    "# 提出用CSVを作成する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2713779f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Create Submission CSV\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "sub = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # サンプル提出CSVファイルを読み込みます。\n",
    "\n",
    "# テストデータの長さがVALIDATEでない場合、結果をサブミッション用データフレームに代入します。\n",
    "if len(test) != VALIDATE:\n",
    "    sub[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]] = results  # 結果を対応する列に割り当てます。\n",
    "    \n",
    "sub.to_csv(\"submission.csv\", index=False)  # 提出用CSVを作成します。インデックスは出力しません。\n",
    "sub.head()  # サブミッションデータフレームの最初の数行を表示します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c52144",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "sub = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")\n",
    "\n",
    "if len(test)!=VALIDATE:\n",
    "    sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = results\n",
    "    \n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "sub.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# CVスコアを計算する\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0d3a55",
   "metadata": {},
   "source": [
    "# CVスコアを計算する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19d030a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Compute CV Score\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合\n",
    "    true = test[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # 真のラベルを取得します。\n",
    "    print(true.shape)  # 真のラベルの形状を出力します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36589f6f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "if len(test)==VALIDATE:\n",
    "    true = test[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "    print(true.shape)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合\n",
    "    from sklearn.metrics import log_loss  # log_loss関数をインポートします。\n",
    "    print(f\"CV loglossは {log_loss(true, results)} です\")  # CVのログ損失を計算して出力します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-17T06:03:23.265602Z",
     "iopub.status.busy": "2024-07-17T06:03:23.264976Z",
     "iopub.status.idle": "2024-07-17T06:03:23.273439Z",
     "shell.execute_reply": "2024-07-17T06:03:23.272475Z",
     "shell.execute_reply.started": "2024-07-17T06:03:23.265556Z"
    }
   },
   "outputs": [],
   "source": [
    "if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合\n",
    "    from sklearn.metrics import log_loss  # log_loss関数をインポートします。\n",
    "    print(f\"CV loglossは {log_loss(true, results)} です\")  # CVのログ損失を計算して出力します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "526de640",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "if len(test)==VALIDATE:\n",
    "    from sklearn.metrics import log_loss\n",
    "    print(f\"CV loglosss is {log_loss(true,results)}\" )\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# コメント\n",
    "\n",
    "> ## Cody_Null\n",
    "> \n",
    "> すごい仕事ですね、クリス。vLLMのパフォーマンス低下について何か知っていますか？私は何も知らないので、他のノートブックでこれを使う際の期待値を知りたいと思っています。\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > vLLMによるパフォーマンス低下はないと思います。vLLMパッケージは単に私たちのモデルを実行するだけです。選択するオプション（vllm.SamplingParams、量子化の有無、量子化の種類、max_model_length、トークナイザの切り捨てなど）がパフォーマンスに影響を与えます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Cody_Null\n",
    "> > > \n",
    "> > > すばらしい！ONNXに似ているのか、正確に私が必要としているものなのかわからなかったです（笑）。\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > モデルの変換（ONNXのような）は必要ありません。Hugging Faceから非量子化モデルと量子化モデルの両方を直接取得し、推論を実行できます。（つまり、モデルは全く変更されません）。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## SeshuRaju 🧘‍♂️\n",
    "> \n",
    "> [@cdeotte](https://www.kaggle.com/cdeotte) ありがとうございます。\n",
    "> \n",
    "> このモデルはSFTまたはDPOでファインチューニングされていますか？\n",
    "> \n",
    "> ファインチューニングにはどれほどのGPUが必要ですか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはSFTでファインチューニングされています。選択したLoRAランクパラメータr、max_model_len、バッチサイズ、およびトレーニングデータの量に応じて、任意のGPU数でファインチューニングできます。\n",
    "> > \n",
    "> > ファインチューニング中に、34Bを4ビットに量子化し、サイズを20GBに削減します。したがって、要件は、GPUの総VRAMがトレーニングに十分な20GBを超えることです。Kaggleの2xT4（合計VRAM 32GB）上で、上記のパラメータを減らせばファインチューニングできると思います。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## SeshuRaju 🧘‍♂️\n",
    "> > > \n",
    "> > > 限られたKaggle GPU時間のため、16GB VRAMで訓練しようとしています。バッチ=1、r=3、max_model_len=1024で訓練する他の方法を探しています。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Chris Deotte（トピック作成者）\n",
    "> > > > \n",
    "> > > > 16GB VRAMで34B LLMをファインチューニングする方法はわかりません。最小要件は32GB VRAMに近いと思います。効率的なファインチューニングに関する詳細を説明した素敵なYouTubeビデオがあります [こちら](https://www.youtube.com/watch?v=XpoKB3usmKc) をご覧ください。ファインチューニング中には、次のメモリが必要です：\n",
    "> > > > \n",
    "> > > > - モデルの重み\n",
    "> > > > \n",
    "> > > > - 勾配\n",
    "> > > > \n",
    "> > > > - オプティマイザ\n",
    "> > > > \n",
    "> > > > メモリを最も削減するには、4ビット量子化されたモデルをロード（QLoRAを使用してモデル重みのメモリ使用量を削減）し、llm_int8_enable_fp32_cpu_offloadを使用してGPUとCPUメモリ間でモデルの重みをスワップできるようにし、PEFTを使用して少ないrパラメータ（QLoRA）でファインチューニングし、勾配チェックポイントを使用して（勾配メモリ使用量を削減）、paged_adam_8bitオプティマイザを使用します（小さなバッチと小さな最大トークン長で）。このオプティマイザは、必要に応じてGPUからCPUに保存された変数をスワップし、オプティマイザ変数のサイズを削減するために8ビットを使用します。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## ano\n",
    "> \n",
    "> この素晴らしいモデルを共有していただきありがとうございます！トレーニングに使用したデータセットについて教えていただけますか？トレーニングデータセット全体を使用しましたか、それともその一部ですか？または外部データセットを使用しましたか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはコンペティションデータの80%でファインチューニングしました。5行ごとにすべてのデータを除外しました。したがって、正しい検証は以下のデータを使用することです pd.read_csv(\"train.csv\").iloc[0::5]。現在、このノートブックでは pd.read_csv(\"train.csv\").iloc[:128] を使用していますが、より正確な迅速な検証は \n",
    "> > \n",
    "> > ```\n",
    "> > VALIDATE = 128\n",
    "> > test = test.iloc[0:VALIDATE*5:5]\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > その後、最初のインデックス % 5 == 0 のサンプル128を使用します。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## JM\n",
    "> \n",
    "> 提出段階で実際に5時間かかっていますか？私のはずっとオーバーしています。\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > いいえ、私は5時間かかっていません。私のは8から9時間かかっています。このノートブックの別のバージョンをリリースするときに、コードセル#10の時間推定コードを修正します（「推論エラー」を削除することが含まれます）。また、導入文を「5時間」ではなく「9時間未満」と更新します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## ano\n",
    "> > > \n",
    "> > > 推論時間の推定（5時間）が実際のもの（8〜9時間）と異なる理由に心当たりはありますか？私は1000サンプルの推論を試してみましたが、25000サンプルのための推定時間（5-6時間）は似たようなものでした。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> こんにちは、GPTQの量子化結果がAWQとどう比較されるか、またKaggleでの可行性についてお尋ねしますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > GPTQを使用するためには、vLLMパラメータで量子化=\"gptq\"を設定し、事前にモデルをGPTQとして保存します。私はKaggleで試したことはありませんが、過去にT4 GPUでAWQとGPTQをどちらも成功裏に利用してきました。それぞれの精度は基本的に似ています。また、推論時間も似ています。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Akhila datta dola\n",
    "> > > \n",
    "> > > 共有していただきありがとうございます！\n",
    "> > > \n",
    "> > > vLLMはbitsandbytesのようなオンザフライ4ビット量子化をサポートしていますか？一般的にgptqとawqと比較してどうですか？\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> すばらしい共有をありがとうございます!!!\n",
    "> \n",
    "> 推論プロセスで「128回中33回の推論エラー」が発生した理由を教えていただけますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > 128の入力テキストのうち33回は、モデルが生成テキストを予測できませんでした。したがって、これらの33回には確率を抽出するためのトークンがありません。私たちのtry/exceptコードブロックがこれをキャッチし、これらのケースでは[1/3, 1/3, 1/3]を予測します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Luan Ngo Dinh\n",
    "> > > \n",
    "> > > モデルはファインチューニングされていても、トークン「a」、「b」、「tie」を生成しなかったという意味ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > はい。問題は、モデルをmax_model_len=1024に切り捨てたため（速度のため）、入力テキストが1024を超えるとvLLMが1024に切り捨てられ、生成されたテキストを出力するための余地がなくなるからです。よりスマートなプロンプトエンジニアリングまたは切り捨て戦略を使用することで、これらの推論エラーを回避できます。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > プロンプトエンジニアリングまたは切り捨て戦略に取り組んでいます。正しい方向に進んでいるかもしれないと聞いてうれしいです;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Xinian Guo\n",
    "> \n",
    "> こんにちは、このモデルはコンペティションデータでファインチューニングされていますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > はい。このモデル（ノートブックバージョン8）はコンペデータでファインチューニングされています。ノートブックバージョン6 [こちら](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm?scriptVersionId=188642633) はファインチューニングなしのゼロショットです。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## yechenzhi1\n",
    "> > > \n",
    "> > > こんにちは、ファインチューニングされたバージョンのLBスコアは何ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > 私はそのノートブックで0.972を得ました。頑張ってください;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Qihang Wang\n",
    "> \n",
    "> こんにちは、クリス、あなたのプロセスを確認したいです：\n",
    "> \n",
    "> CMIIW\n",
    "> \n",
    "> qloraファインチューニング4ビットモデル？\n",
    "> \n",
    "> qloraをモデルにマージ？\n",
    "> \n",
    "> 4ビットを変換？\n",
    "> \n",
    "> AWQ量子化\n",
    "> \n",
    "> vLLMを使用して推論\n",
    "> \n",
    "> 私は最初の3つのステップにあまり慣れていないので、これがあなたのやり方かどうかは確信がありません。\n",
    "> \n",
    "> もう少し詳しく説明していただけますか？\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df67311",
   "metadata": {},
   "source": [
    "# コメント\n",
    "\n",
    "> ## Cody_Null\n",
    "> \n",
    "> すごい仕事ですね、クリス。vLLMのパフォーマンス低下について何か知っていますか？私は何も知らないので、他のノートブックでこれを使う際の期待値を知りたいと思っています。\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > vLLMによるパフォーマンス低下はないと思います。vLLMパッケージは単に私たちのモデルを実行するだけです。選択するオプション（vllm.SamplingParams、量子化の有無、量子化の種類、max_model_length、トークナイザの切り捨てなど）がパフォーマンスに影響を与えます。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Cody_Null\n",
    "> > > \n",
    "> > > すばらしい！ONNXに似ているのか、正確に私が必要としているものなのかわからなかったです（笑）。\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > モデルの変換（ONNXのような）は必要ありません。Hugging Faceから非量子化モデルと量子化モデルの両方を直接取得し、推論を実行できます。（つまり、モデルは全く変更されません）。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## SeshuRaju 🧘‍♂️\n",
    "> \n",
    "> [@cdeotte](https://www.kaggle.com/cdeotte) ありがとうございます。\n",
    "> \n",
    "> このモデルはSFTまたはDPOでファインチューニングされていますか？\n",
    "> \n",
    "> ファインチューニングにはどれほどのGPUが必要ですか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはSFTでファインチューニングされています。選択したLoRAランクパラメータr、max_model_len、バッチサイズ、およびトレーニングデータの量に応じて、任意のGPU数でファインチューニングできます。\n",
    "> > \n",
    "> > ファインチューニング中に、34Bを4ビットに量子化し、サイズを20GBに削減します。したがって、要件は、GPUの総VRAMがトレーニングに十分な20GBを超えることです。Kaggleの2xT4（合計VRAM 32GB）上で、上記のパラメータを減らせばファインチューニングできると思います。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## SeshuRaju 🧘‍♂️\n",
    "> > > \n",
    "> > > 限られたKaggle GPU時間のため、16GB VRAMで訓練しようとしています。バッチ=1、r=3、max_model_len=1024で訓練する他の方法を探しています。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## Chris Deotte（トピック作成者）\n",
    "> > > > \n",
    "> > > > 16GB VRAMで34B LLMをファインチューニングする方法はわかりません。最小要件は32GB VRAMに近いと思います。効率的なファインチューニングに関する詳細を説明した素敵なYouTubeビデオがあります [こちら](https://www.youtube.com/watch?v=XpoKB3usmKc) をご覧ください。ファインチューニング中には、次のメモリが必要です：\n",
    "> > > > \n",
    "> > > > - モデルの重み\n",
    "> > > > \n",
    "> > > > - 勾配\n",
    "> > > > \n",
    "> > > > - オプティマイザ\n",
    "> > > > \n",
    "> > > > メモリを最も削減するには、4ビット量子化されたモデルをロード（QLoRAを使用してモデル重みのメモリ使用量を削減）し、llm_int8_enable_fp32_cpu_offloadを使用してGPUとCPUメモリ間でモデルの重みをスワップできるようにし、PEFTを使用して少ないrパラメータ（QLoRA）でファインチューニングし、勾配チェックポイントを使用して（勾配メモリ使用量を削減）、paged_adam_8bitオプティマイザを使用します（小さなバッチと小さな最大トークン長で）。このオプティマイザは、必要に応じてGPUからCPUに保存された変数をスワップし、オプティマイザ変数のサイズを削減するために8ビットを使用します。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## ano\n",
    "> \n",
    "> この素晴らしいモデルを共有していただきありがとうございます！トレーニングに使用したデータセットについて教えていただけますか？トレーニングデータセット全体を使用しましたか、それともその一部ですか？または外部データセットを使用しましたか？\n",
    "> \n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > このモデルはコンペティションデータの80%でファインチューニングしました。5行ごとにすべてのデータを除外しました。したがって、正しい検証は以下のデータを使用することです pd.read_csv(\"train.csv\").iloc[0::5]。現在、このノートブックでは pd.read_csv(\"train.csv\").iloc[:128] を使用していますが、より正確な迅速な検証は \n",
    "> > \n",
    "> > ```\n",
    "> > VALIDATE = 128\n",
    "> > test = test.iloc[0:VALIDATE*5:5]\n",
    "> > \n",
    "> > ```\n",
    "> > \n",
    "> > その後、最初のインデックス % 5 == 0 のサンプル128を使用します。\n",
    "> > \n",
    "> > \n",
    "\n",
    "---\n",
    "\n",
    "> ## JM\n",
    "> \n",
    "> 提出段階で実際に5時間かかっていますか？私のはずっとオーバーしています。\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > いいえ、私は5時間かかっていません。私のは8から9時間かかっています。このノートブックの別のバージョンをリリースするときに、コードセル#10の時間推定コードを修正します（「推論エラー」を削除することが含まれます）。また、導入文を「5時間」ではなく「9時間未満」と更新します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## ano\n",
    "> > > \n",
    "> > > 推論時間の推定（5時間）が実際のもの（8〜9時間）と異なる理由に心当たりはありますか？私は1000サンプルの推論を試してみましたが、25000サンプルのための推定時間（5-6時間）は似たようなものでした。\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> こんにちは、GPTQの量子化結果がAWQとどう比較されるか、またKaggleでの可行性についてお尋ねしますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > GPTQを使用するためには、vLLMパラメータで量子化=\"gptq\"を設定し、事前にモデルをGPTQとして保存します。私はKaggleで試したことはありませんが、過去にT4 GPUでAWQとGPTQをどちらも成功裏に利用してきました。それぞれの精度は基本的に似ています。また、推論時間も似ています。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Akhila datta dola\n",
    "> > > \n",
    "> > > 共有していただきありがとうございます！\n",
    "> > > \n",
    "> > > vLLMはbitsandbytesのようなオンザフライ4ビット量子化をサポートしていますか？一般的にgptqとawqと比較してどうですか？\n",
    "> > > \n",
    "> > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Luan Ngo Dinh\n",
    "> \n",
    "> すばらしい共有をありがとうございます!!!\n",
    "> \n",
    "> 推論プロセスで「128回中33回の推論エラー」が発生した理由を教えていただけますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > 128の入力テキストのうち33回は、モデルが生成テキストを予測できませんでした。したがって、これらの33回には確率を抽出するためのトークンがありません。私たちのtry/exceptコードブロックがこれをキャッチし、これらのケースでは[1/3, 1/3, 1/3]を予測します。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## Luan Ngo Dinh\n",
    "> > > \n",
    "> > > モデルはファインチューニングされていても、トークン「a」、「b」、「tie」を生成しなかったという意味ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > \n",
    "> > > ## Chris Deotte（トピック作成者）\n",
    "> > > \n",
    "> > > はい。問題は、モデルをmax_model_len=1024に切り捨てたため（速度のため）、入力テキストが1024を超えるとvLLMが1024に切り捨てられ、生成されたテキストを出力するための余地がなくなるからです。よりスマートなプロンプトエンジニアリングまたは切り捨て戦略を使用することで、これらの推論エラーを回避できます。\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > プロンプトエンジニアリングまたは切り捨て戦略に取り組んでいます。正しい方向に進んでいるかもしれないと聞いてうれしいです;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Xinian Guo\n",
    "> \n",
    "> こんにちは、このモデルはコンペティションデータでファインチューニングされていますか？\n",
    "> \n",
    "> > ## Chris Deotte（トピック作成者）\n",
    "> > \n",
    "> > はい。このモデル（ノートブックバージョン8）はコンペデータでファインチューニングされています。ノートブックバージョン6 [こちら](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm?scriptVersionId=188642633) はファインチューニングなしのゼロショットです。\n",
    "> > \n",
    "> > \n",
    "> > \n",
    "> > > ## yechenzhi1\n",
    "> > > \n",
    "> > > こんにちは、ファインチューニングされたバージョンのLBスコアは何ですか？\n",
    "> > > \n",
    "> > > \n",
    "> > > > ## floriandev\n",
    "> > > > \n",
    "> > > > 私はそのノートブックで0.972を得ました。頑張ってください;-)\n",
    "> > > > \n",
    "\n",
    "---\n",
    "\n",
    "> ## Qihang Wang\n",
    "> \n",
    "> こんにちは、クリス、あなたのプロセスを確認したいです：\n",
    "> \n",
    "> CMIIW\n",
    "> \n",
    "> qloraファインチューニング4ビットモデル？\n",
    "> \n",
    "> qloraをモデルにマージ？\n",
    "> \n",
    "> 4ビットを変換？\n",
    "> \n",
    "> AWQ量子化\n",
    "> \n",
    "> vLLMを使用して推論\n",
    "> \n",
    "> 私は最初の3つのステップにあまり慣れていないので、これがあなたのやり方かどうかは確信がありません。\n",
    "> \n",
    "> もう少し詳しく説明していただけますか？"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5409557,
     "sourceId": 8982890,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
