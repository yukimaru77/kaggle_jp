{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a373c1",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookでは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、人間による好みの予測に挑戦しています。具体的には、異なる大規模言語モデル（LLM）が生成した応答の中から、どちらがユーザーに好まれるかを予測するための機械学習モデルを構築しています。\n",
    "\n",
    "### 問題に対するアプローチ\n",
    "Notebookは以下の主要なステップで構成されています：\n",
    "\n",
    "1. **ライブラリのインポート**: pandas、numpy、nltkなどのデータ処理および自然言語処理ライブラリに加えて、XGBoost、LightGBM、CatBoostなどの機械学習ライブラリをインポートしています。\n",
    "\n",
    "2. **データの読み込みおよび前処理**:\n",
    "   - トレーニングデータとテストデータをCSVファイルから読み込み、不要な文字を取り除くなどの前処理を行います。\n",
    "   - トークン化や特徴量の生成を行う`Preprocessor`クラスが定義され、コサイン類似度やジャッカード類似度の計算、n-gramの生成などが実装されています。\n",
    "\n",
    "3. **特徴選択**: ANOVA F値に基づいて、最も有用な特徴量を25個選択します。\n",
    "\n",
    "4. **モデルのトレーニングと評価**:\n",
    "   - RandomForest、GradientBoosting、SVM、XGBoost、CatBoost、VotingClassifierなど、複数の機械学習モデルを使用します。\n",
    "   - クロスバリデーションによってモデルのパフォーマンスを評価し、平均CVログロスを計算します。\n",
    "\n",
    "5. **結果の報告**: 各モデルのCVログロスを比較し、最良のモデルを特定するとともに、特徴の重要度を示すDataFrameを生成します。\n",
    "\n",
    "6. **提出ファイルの作成**: 最後に、テストデータに対する予測結果を用いて、提出用のCSVファイル（`submission.csv`）を出力します。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **データ処理**: pandas、numpy、nltk\n",
    "- **特徴量生成**: コサイン類似度、ジャッカード類似度、n-gram、引用数カウント、トークン化などの手法を用いた特徴量の生成。\n",
    "- **モデル**: RandomForest、GradientBoosting、SVM、XGBoost、CatBoost、VotingClassifier\n",
    "- **評価手法**: StratifiedKFoldによるクロスバリデーション、log_lossによる評価\n",
    "\n",
    "このNotebookは、テキストデータを用いた機械学習タスクに対する徹底した前処理、特徴量エンジニアリング、モデル評価のプロセスを示しており、選好予測の向上を目指しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e39764",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookに含まれる専門用語の解説です。初心者向けに必要な域外の用語や少しマイナーなものに焦点を当てています。\n",
    "\n",
    "1. **ガーベジコレクション(GC)**:\n",
    "   自動的にメモリを管理し、不要なオブジェクトを削除する機能。メモリリークを避けるのに役立つ。`gc`モジュールを使用して、明示的にガーベジコレクションを起動できる。\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   文書内における単語の重要度を測るための指標。頻繁に出現する単語はあまり重要性がないとみなされ、逆に文書全体に広まっていない単語は重要と評価される。\n",
    "\n",
    "3. **コサイン類似度 (Cosine Similarity)**:\n",
    "   2つのベクトル間の角度を測定する指標で、特にテキストの類似度を測るためによく使われる。数値的には、内積をベクトルのノルムで割ったものと定義される。\n",
    "\n",
    "4. **ジャッカード類似度 (Jaccard Similarity)**:\n",
    "   2つの集合の類似度を測る方法。交差部分のサイズを合併部分のサイズで割ることで計算される。特に、テキスト間の重複した単語の割合を知るのに役立つ。\n",
    "\n",
    "5. **n-gram**:\n",
    "   テキストを「n」個の連続した項目に分けたもの。たとえば、バイグラムは2つの単語の組み合わせ、トライグラムは3つの単語の組み合わせとなる。\n",
    "\n",
    "6. **トークン化(Tokenization)**:\n",
    "   テキストデータを個々の単語やフレーズに分解するプロセス。自然言語処理において基本的なステップ。\n",
    "\n",
    "7. **層化K分割交差検証 (Stratified K-Fold Cross-Validation)**:\n",
    "   データセットをK個の部分に分ける方法で、各部分がターゲット変数の分布を保つようになっている。これにより、モデル評価がより信頼性の高いものになる。\n",
    "\n",
    "8. **ANOVA F値 (Analysis of Variance F-Value)**:\n",
    "   異なるグループ間の平均の違いを測る統計的方法。特徴選択で、各特徴がターゲット変数に対してどれだけ情報を持っているかを評価する。\n",
    "\n",
    "9. **アーリーストップ(Early Stopping)**:\n",
    "   モデルのトレーニング中に検証データの性能が向上しなくなった段階でトレーニングを早期に終了する仕組み。過学習を防ぐために有効。\n",
    "\n",
    "これらの用語は、機械学習や自然言語処理の領域でよく使われるため、知識を深めるうえで重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73481",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# 📚 Importing Libraries\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 📚 ライブラリのインポート\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a94ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# gcモジュール: Pythonのガーベジコレクションを提供\n",
    "# osモジュール: オペレーティングシステムの機能を利用\n",
    "# reモジュール: 正規表現による文字列操作\n",
    "# numpy: 数値計算用のライブラリ\n",
    "# pandas: データ操作用のライブラリ\n",
    "# nltk: 自然言語処理用のライブラリ\n",
    "# matplotlib.pyplot: データ可視化用の2Dプロットライブラリ\n",
    "# seaborn: より洗練されたデータ可視化用のライブラリ\n",
    "# xgboost: 勾配ブースティングアルゴリズムの実装\n",
    "# lightgbm: LightGBMアルゴリズムの実装\n",
    "# catboost: CatBoostアルゴリズムの実装\n",
    "# sklearn: 機械学習用の多数のツールを提供\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:33.526535Z",
     "iopub.status.busy": "2024-07-11T07:20:33.526078Z",
     "iopub.status.idle": "2024-07-11T07:20:38.692753Z",
     "shell.execute_reply": "2024-07-11T07:20:38.691507Z",
     "shell.execute_reply.started": "2024-07-11T07:20:33.526503Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# gcモジュール: Pythonのガーベジコレクションを提供\n",
    "# osモジュール: オペレーティングシステムの機能を利用\n",
    "# reモジュール: 正規表現による文字列操作\n",
    "# numpy: 数値計算用のライブラリ\n",
    "# pandas: データ操作用のライブラリ\n",
    "# nltk: 自然言語処理用のライブラリ\n",
    "# matplotlib.pyplot: データ可視化用の2Dプロットライブラリ\n",
    "# seaborn: より洗練されたデータ可視化用のライブラリ\n",
    "# xgboost: 勾配ブースティングアルゴリズムの実装\n",
    "# lightgbm: LightGBMアルゴリズムの実装\n",
    "# catboost: CatBoostアルゴリズムの実装\n",
    "# sklearn: 機械学習用の多数のツールを提供"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb7f61",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# ⚙️ Configuration Class\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ⚙️ 設定クラス\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbaeb6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.695283Z",
     "iopub.status.busy": "2024-07-11T07:20:38.694735Z",
     "iopub.status.idle": "2024-07-11T07:20:38.701258Z",
     "shell.execute_reply": "2024-07-11T07:20:38.699693Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.695252Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e7dd8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "\n",
    "# 📊 Loading and Processing Data\n",
    "\n",
    "we will load our training and test datasets, and apply some preprocessing. This includes:\n",
    "\n",
    "\n",
    "1. **Loading Data**: Read the CSV files into pandas DataFrames.\n",
    "2. **Subsampling**: If the test dataset has less than 10 rows, subsample the training dataset to 10,000 rows for quicker processing.\n",
    "3. **Processing Strings**: Clean and process the string columns (`prompt`, `response_a`, `response_b`) by removing unwanted characters.\n",
    "4. **Shape and Missing Values**: Print the shape of the datasets and count missing values to understand the data structure and quality.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "\n",
    "# 📊 データの読み込みと処理\n",
    "\n",
    "トレーニングデータセットとテストデータセットを読み込み、一部の前処理を適用します。これには以下が含まれます:\n",
    "\n",
    "1. **データの読み込み**: CSVファイルをpandasのDataFrameに読み込みます。\n",
    "2. **サブサンプリング**: テストデータセットの行数が10未満の場合は、トレーニングデータセットから10,000行サブサンプリングして迅速に処理します。\n",
    "3. **文字列の処理**: 不要な文字を削除して、文字列列（`prompt`, `response_a`, `response_b`）をクリーンアップし処理します。\n",
    "4. **データの形状と欠損値**: データセットの形状を出力し、欠損値をカウントしてデータ構造と品質を理解します。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2acaf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)\n",
    "test = pd.read_csv(config.test_path)\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)  # 訓練データを読み込む\n",
    "test = pd.read_csv(config.test_path)    # テストデータを読み込む\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # 提出用のサンプルデータを読み込む\n",
    "\n",
    "# テストデータの行数が10未満であれば、トレーニングデータの最初の10,000行を使用する\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 文字列の前後のブラケットを削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文字列を分割してクォーテーションを削除\n",
    "    return  ' '.join(sentences)  # 結果をスペースで繋げて返す\n",
    "\n",
    "# 各列にprocess関数を適用して文字列をクリーンアップ\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "# データセットの形状を出力\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "# 欠損値の計算\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # トレーニングデータの最初の5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.703195Z",
     "iopub.status.busy": "2024-07-11T07:20:38.702888Z",
     "iopub.status.idle": "2024-07-11T07:20:42.781785Z",
     "shell.execute_reply": "2024-07-11T07:20:42.780465Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.703171Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.train_path)  # 訓練データを読み込む\n",
    "test = pd.read_csv(config.test_path)    # テストデータを読み込む\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # 提出用のサンプルデータを読み込む\n",
    "\n",
    "# テストデータの行数が10未満であれば、トレーニングデータの最初の10,000行を使用する\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 文字列の前後のブラケットを削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文字列を分割してクォーテーションを削除\n",
    "    return  ' '.join(sentences)  # 結果をスペースで繋げて返す\n",
    "\n",
    "# 各列にprocess関数を適用して文字列をクリーンアップ\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "# データセットの形状を出力\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "# 欠損値の計算\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # トレーニングデータの最初の5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc9dc6",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# 🛠️ Preprocessing Class Definition\n",
    "\n",
    "This class, `Preprocessor`, contains several methods to process and feature-engineer the text data. Here’s a breakdown of its functionalities:\n",
    "\n",
    "#### Cosine Similarity\n",
    "- **Formula**: \n",
    "  $$\\text{cosine_similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **Description**: Cosine similarity measures the cosine of the angle between two vectors, providing a metric of how similar the texts are.\n",
    "\n",
    "#### Jaccard Similarity\n",
    "- **Formula**:\n",
    "\n",
    "  $$\\text{Jaccard_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- **Description**: Jaccard similarity measures the similarity between two sets by comparing their intersection and union.\n",
    "\n",
    "#### Count Quotes\n",
    "- **Description**: This method identifies and counts both single and double quoted texts in a string, giving an idea of how many quotations are present.\n",
    "\n",
    "#### Tokenize\n",
    "- **Description**: This method splits the text into individual words (tokens), which can be used for further analysis like generating n-grams or calculating overlaps.\n",
    "\n",
    "#### Generate N-grams\n",
    "- **Description**: N-grams are contiguous sequences of 'n' items from a given text. This method helps in analyzing the text at different levels of granularity (unigrams, bigrams, trigrams, etc.).\n",
    "\n",
    "#### Count N-gram Overlaps\n",
    "- **Description**: This method calculates how many n-grams are common between two texts, helping to measure their similarity.\n",
    "\n",
    "#### Run\n",
    "- **Description**: This method processes the entire dataset, generating new features based on the above calculations, which can be used for training machine learning models.\n",
    "\n",
    "\n",
    "$\\frac{n!}{k!(n-k)!} = \\binom{n}{k}$\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 🛠️ 前処理クラス定義\n",
    "\n",
    "`Preprocessor`クラスは、テキストデータを処理し特徴量エンジニアリングを行ういくつかのメソッドを含んでいます。以下にその機能の説明を示します：\n",
    "\n",
    "#### コサイン類似度\n",
    "- **数式**: \n",
    "  $$\\text{cosine_similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **説明**: コサイン類似度は2つのベクトルの間の角度のコサインを測定し、テキストの類似度の指標を提供します。\n",
    "\n",
    "#### ジャッカード類似度\n",
    "- **数式**:\n",
    "\n",
    "  $$\\text{Jaccard_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- **説明**: ジャッカード類似度は、2つの集合の交差部分と合併部分を比較することで類似度を測定します。\n",
    "\n",
    "#### 引用のカウント\n",
    "- **説明**: このメソッドは、文字列内の単一および二重引用されたテキストを特定し、カウントすることで、引用の数を把握します。\n",
    "\n",
    "#### トークン化\n",
    "- **説明**: このメソッドは、テキストを個々の単語（トークン）に分割し、n-gramを生成したり、重複を計算するためのさらなる分析に使用できます。\n",
    "\n",
    "#### N-gramの生成\n",
    "- **説明**: N-gramは、与えられたテキストからの'n'項目の連続したシーケンスです。このメソッドは、異なる粒度（単語、2-gram、3-gramなど）でテキストを分析するのに役立ちます。\n",
    "\n",
    "#### N-gramの重複カウント\n",
    "- **説明**: このメソッドは、2つのテキスト間の共通のn-gramの数を計算し、類似度を測定するのに役立ちます。\n",
    "\n",
    "#### 実行\n",
    "- **説明**: このメソッドは、上記の計算に基づいて新しい特徴量を生成し、機械学習モデルのトレーニングに使用できる全データセットを処理します。\n",
    "\n",
    "$\\frac{n!}{k!(n-k)!} = \\binom{n}{k}$\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5dc84",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "            vectors = vectorizer.toarray()\n",
    "            cos_sim = cosine_similarity(vectors)\n",
    "            return cos_sim[0][1]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())\n",
    "        set2 = set(text2.split())\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"\n",
    "        double_quote_pattern = r'\"(.*?)\"'\n",
    "        single_quotes = re.findall(single_quote_pattern, text)\n",
    "        double_quotes = re.findall(double_quote_pattern, text)\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)\n",
    "        return len(single_quotes) + len(double_quotes)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)\n",
    "            ngrams2 = self.generate_ngrams(text2, n)\n",
    "            counter1 = Counter(ngrams1)\n",
    "            counter2 = Counter(ngrams2)\n",
    "            overlap = counter1 & counter2\n",
    "            overlap_count = sum(overlap.values())\n",
    "            return overlap_count\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])  # テキストのTF-IDFベクトルを作成\n",
    "            vectors = vectorizer.toarray()  # ベクトルを配列に変換\n",
    "            cos_sim = cosine_similarity(vectors)  # コサイン類似度を計算\n",
    "            return cos_sim[0][1]  # コサイン類似度の結果を返す\n",
    "        except:\n",
    "            return np.nan  # エラーが発生した場合はNaNを返す\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # テキスト1を単語の集合に変換\n",
    "        set2 = set(text2.split())  # テキスト2を単語の集合に変換\n",
    "        intersection = set1.intersection(set2)  # 交差部分を計算\n",
    "        union = set1.union(set2)  # 合併部分を計算\n",
    "        return len(intersection) / len(union)  # ジャッカード類似度を返す\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # 単一引用のパターン\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # 二重引用のパターン\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用を抽出\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用を抽出\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用数\n",
    "        return total_quotes  # 合計の引用数を返す\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())  # テキストを小文字にしトークン化\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # トークン化されたテキストを取得\n",
    "        return list(ngrams(tokens, n))  # n-gramを生成\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # テキスト1からn-gramを生成\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # テキスト2からn-gramを生成\n",
    "            counter1 = Counter(ngrams1)  # テキスト1のn-gramのカウント\n",
    "            counter2 = Counter(ngrams2)  # テキスト2のn-gramのカウント\n",
    "            overlap = counter1 & counter2  # 共通のn-gramを計算\n",
    "            overlap_count = sum(overlap.values())  # 重複の合計を計算\n",
    "            return overlap_count  # 重複数を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # それぞれの応答間でのn-gramオーバーラップを計算\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        # 各応答とプロンプト間でのn-gramオーバーラップを計算\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        # 各応答とプロンプトの長さを計算\n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # 応答Aの長さ\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # 応答Bの長さ\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # プロンプトの長さ\n",
    "        \n",
    "        # 異なる長さの比率や差を計算\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # 応答Aとプロンプトの長さの比率\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # 応答Bとプロンプトの長さの比率\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # 応答Aと応答Bの長さの比率\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # 長さの差\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # 応答Aとプロンプトの長さの差\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # 応答Bとプロンプトの長さの差\n",
    "        \n",
    "        # n-gramオーバーラップの比率を計算\n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        # 引用のカウントを計算\n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # 応答Aの引用数\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # 応答Bの引用数\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # プロンプトの引用数\n",
    "        \n",
    "        # コサイン類似度を計算\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data  # 処理後のデータを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.785579Z",
     "iopub.status.busy": "2024-07-11T07:20:42.784505Z",
     "iopub.status.idle": "2024-07-11T07:20:42.810829Z",
     "shell.execute_reply": "2024-07-11T07:20:42.809399Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.785544Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])  # テキストのTF-IDFベクトルを作成\n",
    "            vectors = vectorizer.toarray()  # ベクトルを配列に変換\n",
    "            cos_sim = cosine_similarity(vectors)  # コサイン類似度を計算\n",
    "            return cos_sim[0][1]  # コサイン類似度の結果を返す\n",
    "        except:\n",
    "            return np.nan  # エラーが発生した場合はNaNを返す\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # テキスト1を単語の集合に変換\n",
    "        set2 = set(text2.split())  # テキスト2を単語の集合に変換\n",
    "        intersection = set1.intersection(set2)  # 交差部分を計算\n",
    "        union = set1.union(set2)  # 合併部分を計算\n",
    "        return len(intersection) / len(union)  # ジャッカード類似度を返す\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # 単一引用のパターン\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # 二重引用のパターン\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用を抽出\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用を抽出\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用数\n",
    "        return total_quotes  # 合計の引用数を返す\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())  # テキストを小文字にしトークン化\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # トークン化されたテキストを取得\n",
    "        return list(ngrams(tokens, n))  # n-gramを生成\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # テキスト1からn-gramを生成\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # テキスト2からn-gramを生成\n",
    "            counter1 = Counter(ngrams1)  # テキスト1のn-gramのカウント\n",
    "            counter2 = Counter(ngrams2)  # テキスト2のn-gramのカウント\n",
    "            overlap = counter1 & counter2  # 共通のn-gramを計算\n",
    "            overlap_count = sum(overlap.values())  # 重複の合計を計算\n",
    "            return overlap_count  # 重複数を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # それぞれの応答間でのn-gramオーバーラップを計算\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        # 各応答とプロンプト間でのn-gramオーバーラップを計算\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        # 各応答とプロンプトの長さを計算\n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # 応答Aの長さ\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # 応答Bの長さ\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # プロンプトの長さ\n",
    "        \n",
    "        # 異なる長さの比率や差を計算\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # 応答Aとプロンプトの長さの比率\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # 応答Bとプロンプトの長さの比率\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # 応答Aと応答Bの長さの比率\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # 長さの差\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # 応答Aとプロンプトの長さの差\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # 応答Bとプロンプトの長さの差\n",
    "        \n",
    "        # n-gramオーバーラップの比率を計算\n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        # 引用のカウントを計算\n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # 応答Aの引用数\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # 応答Bの引用数\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # プロンプトの引用数\n",
    "        \n",
    "        # コサイン類似度を計算\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data  # 処理後のデータを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04121949",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "train = preprocessor.run(train)\n",
    "test = preprocessor.run(test)\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()  # Preprocessorクラスのインスタンスを作成\n",
    "train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行\n",
    "test = preprocessor.run(test)  # テストデータに対して前処理を実行\n",
    "train.head()  # トレーニングデータの最初の5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.812893Z",
     "iopub.status.busy": "2024-07-11T07:20:42.812433Z",
     "iopub.status.idle": "2024-07-11T07:27:19.005204Z",
     "shell.execute_reply": "2024-07-11T07:27:19.003828Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.812855Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()  # Preprocessorクラスのインスタンスを作成\n",
    "train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行\n",
    "test = preprocessor.run(test)  # テストデータに対して前処理を実行\n",
    "train.head()  # トレーニングデータの最初の5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b5632",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### データ準備\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dd705",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = np.nan\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx\n",
    "train[target] = train[target].astype(\"int32\")\n",
    "    \n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # 削除する列のリスト\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ターゲット列のリスト\n",
    "target = \"target\"  # ターゲット変数\n",
    "\n",
    "train[target] = np.nan  # ターゲット変数の初期化\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx  # ターゲット列に基づいてインデックスを設定\n",
    "train[target] = train[target].astype(\"int32\")  # ターゲット変数を整数型に変換\n",
    "    \n",
    "train.head()  # トレーニングデータの最初の5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.007225Z",
     "iopub.status.busy": "2024-07-11T07:27:19.006777Z",
     "iopub.status.idle": "2024-07-11T07:27:19.041533Z",
     "shell.execute_reply": "2024-07-11T07:27:19.040457Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.007186Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # 削除する列のリスト\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ターゲット列のリスト\n",
    "target = \"target\"  # ターゲット変数\n",
    "\n",
    "train[target] = np.nan  # ターゲット変数の初期化\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx  # ターゲット列に基づいてインデックスを設定\n",
    "train[target] = train[target].astype(\"int32\")  # ターゲット変数を整数型に変換\n",
    "    \n",
    "train.head()  # トレーニングデータの最初の5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55728508",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)\n",
    "y = train[target]\n",
    "X_test = test.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)  # 特徴量マトリックスXを作成\n",
    "y = train[target]  # ターゲット変数yを設定\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # テストデータの特徴量マトリックスX_testを作成\n",
    "\n",
    "# 無限大や負無限大をNaNに置き換える\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.04354Z",
     "iopub.status.busy": "2024-07-11T07:27:19.043159Z",
     "iopub.status.idle": "2024-07-11T07:27:19.071594Z",
     "shell.execute_reply": "2024-07-11T07:27:19.070564Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.043511Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)  # 特徴量マトリックスXを作成\n",
    "y = train[target]  # ターゲット変数yを設定\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # テストデータの特徴量マトリックスX_testを作成\n",
    "\n",
    "# 無限大や負無限大をNaNに置き換える\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7be369",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 欠損値の処理\n",
    "imputer = SimpleImputer(strategy='mean')  # 平均で欠損値を補完するためのインプッターを作成\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # トレーニングデータの欠損値を補完\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)  # テストデータの欠損値を補完\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.073423Z",
     "iopub.status.busy": "2024-07-11T07:27:19.072999Z",
     "iopub.status.idle": "2024-07-11T07:27:19.095853Z",
     "shell.execute_reply": "2024-07-11T07:27:19.094538Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.073387Z"
    }
   },
   "outputs": [],
   "source": [
    "# 欠損値の処理\n",
    "imputer = SimpleImputer(strategy='mean')  # 平均で欠損値を補完するためのインプッターを作成\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # トレーニングデータの欠損値を補完\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)  # テストデータの欠損値を補完"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9912f1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Feature Selection\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 特徴選択\n",
    "selector = SelectKBest(f_classif, k=25)  # 最良のk個の特徴量を選択\n",
    "X_new = selector.fit_transform(X, y)  # トレーニングデータに対して特徴選択を適用\n",
    "X_test_new = selector.transform(X_test)  # テストデータに対して同様に適用\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.097367Z",
     "iopub.status.busy": "2024-07-11T07:27:19.096993Z",
     "iopub.status.idle": "2024-07-11T07:27:19.122964Z",
     "shell.execute_reply": "2024-07-11T07:27:19.121688Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.09734Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特徴選択\n",
    "selector = SelectKBest(f_classif, k=25)  # 最良のk個の特徴量を選択\n",
    "X_new = selector.fit_transform(X, y)  # トレーニングデータに対して特徴選択を適用\n",
    "X_test_new = selector.transform(X_test)  # テストデータに対して同様に適用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d910368",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# 🧩 Model Training and Evaluation\n",
    "\n",
    "### Model Definitions\n",
    "\n",
    "Defines several machine learning models including Random Forest, Gradient Boosting, SVM, XGBoost, CatBoost, and a Voting Classifier.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Uses SelectKBest to select 25 best features based on ANOVA F-value between feature and target.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Uses Stratified K-Fold cross-validation for model evaluation.\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "Iterates through each model, trains it using the training data, evaluates using cross-validation, and calculates the mean CV Log Loss.\n",
    "\n",
    "### Feature Importances\n",
    "\n",
    "Calculates and stores feature importances for applicable models (Random Forest, Gradient Boosting, XGBoost, CatBoost).\n",
    "\n",
    "### Best Model Identification\n",
    "\n",
    "Identifies the best performing model based on the lowest CV Log Loss.\n",
    "\n",
    "### Results Display\n",
    "\n",
    "Displays the results in a DataFrame showing the CV Log Loss for each model and, if applicable, feature importances.\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 🧩 モデルのトレーニングと評価\n",
    "\n",
    "### モデルの定義\n",
    "\n",
    "ランダムフォレスト、勾配ブースティング、SVM、XGBoost、CatBoost、および投票分類器を含むいくつかの機械学習モデルを定義します。\n",
    "\n",
    "### 特徴選択\n",
    "\n",
    "ANOVA F値に基づいて、SelectKBestを使用して25の最良特徴を選択します。\n",
    "\n",
    "### クロスバリデーション\n",
    "\n",
    "モデル評価のために層化K分割交差検証を使用します。\n",
    "\n",
    "### トレーニングと評価\n",
    "\n",
    "各モデルを反復処理し、トレーニングデータを使ってトレーニングし、クロスバリデーションを使用して評価し、平均CVログロスを計算します。\n",
    "\n",
    "### 特徴の重要度\n",
    "\n",
    "適用可能なモデル（ランダムフォレスト、勾配ブースティング、XGBoost、CatBoost）について、特徴の重要度を計算して保存します。\n",
    "\n",
    "### 最良モデルの特定\n",
    "\n",
    "最も低いCVログロスに基づいて、最も良いパフォーマンスのモデルを特定します。\n",
    "\n",
    "### 結果の表示\n",
    "\n",
    "各モデルのCVログロスを示し、適用可能な場合は特徴の重要度を表示するDataFrameを表示します。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2effc08",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "# Define models and their configurations\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # GPU acceleration if available\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # Use GPU if available\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select features using SelectKBest\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# Dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"Training model: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))\n",
    "    cv_scores = []\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train\n",
    "            )\n",
    "\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")\n",
    "            cv_scores.append(val_log_loss)\n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})\n",
    "        print(f\"Mean CV Log Loss: {mean_cv_log_loss:.5f}\")\n",
    "\n",
    "# Store feature importances if applicable\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best model\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]\n",
    "print(f\"\\nBest Model:\\n{best_model}\")\n",
    "\n",
    "# Display results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df)\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# モデルとその設定を定義\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # グラフィックデバイスが利用可能ならばGPU加速を使用\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # GPUが利用可能ならば使用\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# SelectKBestを使用して特徴を選択\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)  # 特徴量マトリックスXを新しい特徴に変換\n",
    "X_test_new = selector.transform(X_test)  # テストデータに対しても新しい特徴を適用\n",
    "\n",
    "# クロスバリデーションの設定\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# 結果を格納するDataFrame\n",
    "results = []\n",
    "\n",
    "# モデルを反復処理\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"モデルのトレーニング: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))  # テスト予測の配列を初期化\n",
    "    cv_scores = []  # CVスコアを格納するリスト\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]  # トレーニングデータとラベル\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]  # 検証データとラベル\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)  # 投票モデルをトレーニング\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,  # 早期停止\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)  # その他のモデルをトレーニング\n",
    "\n",
    "        # 検証データで予測を行う\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)  # 検証データの確率予測\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # ログロスを計算\n",
    "            cv_scores.append(val_log_loss)  # スコアをリストに追加\n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()  # テスト予測を集積\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)  # 平均CVログロスを計算\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})  # 結果を格納\n",
    "        print(f\"平均CVログロス: {mean_cv_log_loss:.5f}\")  # 結果を表示\n",
    "\n",
    "# 特徴重要度を格納（適用可能なモデルに対して）\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()  # 選択された特徴\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})  # 特徴重要度のDataFrameを作成\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0  # 初期化\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]  # 検証データ\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()  # 重みを集計\n",
    "\n",
    "    results_df = pd.DataFrame(results)  # 結果のDataFrameを作成\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)  # 特徴重要度を結合\n",
    "\n",
    "# 結果をDataFrameに変換\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 最良モデルの特定\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]  # 最小のCVログロスを持つモデルを特定\n",
    "print(f\"\\n最良モデル:\\n{best_model}\")\n",
    "\n",
    "# 結果のDataFrameを表示\n",
    "print(\"\\n結果のDataFrame:\")\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.154308Z",
     "iopub.status.busy": "2024-07-11T07:27:19.153925Z",
     "iopub.status.idle": "2024-07-11T07:53:41.875009Z",
     "shell.execute_reply": "2024-07-11T07:53:41.873898Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.154277Z"
    }
   },
   "outputs": [],
   "source": [
    "# モデルとその設定を定義\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # グラフィックデバイスが利用可能ならばGPU加速を使用\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # GPUが利用可能ならば使用\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# SelectKBestを使用して特徴を選択\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)  # 特徴量マトリックスXを新しい特徴に変換\n",
    "X_test_new = selector.transform(X_test)  # テストデータに対しても新しい特徴を適用\n",
    "\n",
    "# クロスバリデーションの設定\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# 結果を格納するDataFrame\n",
    "results = []\n",
    "\n",
    "# モデルを反復処理\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"モデルのトレーニング: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))  # テスト予測の配列を初期化\n",
    "    cv_scores = []  # CVスコアを格納するリスト\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]  # トレーニングデータとラベル\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]  # 検証データとラベル\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)  # 投票モデルをトレーニング\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,  # 早期停止\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)  # その他のモデルをトレーニング\n",
    "\n",
    "        # 検証データで予測を行う\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)  # 検証データの確率予測\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # ログロスを計算\n",
    "            cv_scores.append(val_log_loss)  # スコアをリストに追加\n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()  # テスト予測を集積\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)  # 平均CVログロスを計算\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})  # 結果を格納\n",
    "        print(f\"平均CVログロス: {mean_cv_log_loss:.5f}\")  # 結果を表示\n",
    "\n",
    "# 特徴重要度を格納（適用可能なモデルに対して）\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()  # 選択された特徴\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})  # 特徴重要度のDataFrameを作成\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0  # 初期化\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]  # 検証データ\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()  # 重みを集計\n",
    "\n",
    "    results_df = pd.DataFrame(results)  # 結果のDataFrameを作成\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)  # 特徴重要度を結合\n",
    "\n",
    "# 結果をDataFrameに変換\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# 最良モデルの特定\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]  # 最小のCVログロスを持つモデルを特定\n",
    "print(f\"\\n最良モデル:\\n{best_model}\")\n",
    "\n",
    "# 結果のDataFrameを表示\n",
    "print(\"\\n結果のDataFrame:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e98e84",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]\n",
    "sample_submission.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]  # 各ターゲット列にテスト予測を格納\n",
    "sample_submission.head()  # 提出用DataFrameの最初の5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:53:41.877465Z",
     "iopub.status.busy": "2024-07-11T07:53:41.876688Z",
     "iopub.status.idle": "2024-07-11T07:53:41.890065Z",
     "shell.execute_reply": "2024-07-11T07:53:41.888965Z",
     "shell.execute_reply.started": "2024-07-11T07:53:41.877426Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]  # 各ターゲット列にテスト予測を格納\n",
    "sample_submission.head()  # 提出用DataFrameの最初の5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080f48",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)  # 提出ファイルをCSVとして保存\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:36:46.168878Z",
     "iopub.status.busy": "2024-07-11T08:36:46.168386Z",
     "iopub.status.idle": "2024-07-11T08:36:46.17975Z",
     "shell.execute_reply": "2024-07-11T08:36:46.178638Z",
     "shell.execute_reply.started": "2024-07-11T08:36:46.168847Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)  # 提出ファイルをCSVとして保存"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
