{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2a373c1",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyter Notebookã§ã¯ã€Kaggleã®ã€ŒLMSYS - Chatbot Arenaã€ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã«ãŠã„ã¦ã€äººé–“ã«ã‚ˆã‚‹å¥½ã¿ã®äºˆæ¸¬ã«æŒ‘æˆ¦ã—ã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€ç•°ãªã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ãŒç”Ÿæˆã—ãŸå¿œç­”ã®ä¸­ã‹ã‚‰ã€ã©ã¡ã‚‰ãŒãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¥½ã¾ã‚Œã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å•é¡Œã«å¯¾ã™ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ\n",
    "Notebookã¯ä»¥ä¸‹ã®ä¸»è¦ãªã‚¹ãƒ†ãƒƒãƒ—ã§æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ï¼š\n",
    "\n",
    "1. **ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ**: pandasã€numpyã€nltkãªã©ã®ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãŠã‚ˆã³è‡ªç„¶è¨€èªå‡¦ç†ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã«åŠ ãˆã¦ã€XGBoostã€LightGBMã€CatBoostãªã©ã®æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "2. **ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ãŠã‚ˆã³å‰å‡¦ç†**:\n",
    "   - ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰èª­ã¿è¾¼ã¿ã€ä¸è¦ãªæ–‡å­—ã‚’å–ã‚Šé™¤ããªã©ã®å‰å‡¦ç†ã‚’è¡Œã„ã¾ã™ã€‚\n",
    "   - ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã‚„ç‰¹å¾´é‡ã®ç”Ÿæˆã‚’è¡Œã†`Preprocessor`ã‚¯ãƒ©ã‚¹ãŒå®šç¾©ã•ã‚Œã€ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚„ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ã®è¨ˆç®—ã€n-gramã®ç”Ÿæˆãªã©ãŒå®Ÿè£…ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "3. **ç‰¹å¾´é¸æŠ**: ANOVA Få€¤ã«åŸºã¥ã„ã¦ã€æœ€ã‚‚æœ‰ç”¨ãªç‰¹å¾´é‡ã‚’25å€‹é¸æŠã—ã¾ã™ã€‚\n",
    "\n",
    "4. **ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡**:\n",
    "   - RandomForestã€GradientBoostingã€SVMã€XGBoostã€CatBoostã€VotingClassifierãªã©ã€è¤‡æ•°ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "   - ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã«ã‚ˆã£ã¦ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã—ã€å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "\n",
    "5. **çµæœã®å ±å‘Š**: å„ãƒ¢ãƒ‡ãƒ«ã®CVãƒ­ã‚°ãƒ­ã‚¹ã‚’æ¯”è¼ƒã—ã€æœ€è‰¯ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã™ã‚‹ã¨ã¨ã‚‚ã«ã€ç‰¹å¾´ã®é‡è¦åº¦ã‚’ç¤ºã™DataFrameã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n",
    "\n",
    "6. **æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ**: æœ€å¾Œã«ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹äºˆæ¸¬çµæœã‚’ç”¨ã„ã¦ã€æå‡ºç”¨ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆ`submission.csv`ï¼‰ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚\n",
    "\n",
    "### ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "- **ãƒ‡ãƒ¼ã‚¿å‡¦ç†**: pandasã€numpyã€nltk\n",
    "- **ç‰¹å¾´é‡ç”Ÿæˆ**: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã€ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ã€n-gramã€å¼•ç”¨æ•°ã‚«ã‚¦ãƒ³ãƒˆã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ãªã©ã®æ‰‹æ³•ã‚’ç”¨ã„ãŸç‰¹å¾´é‡ã®ç”Ÿæˆã€‚\n",
    "- **ãƒ¢ãƒ‡ãƒ«**: RandomForestã€GradientBoostingã€SVMã€XGBoostã€CatBoostã€VotingClassifier\n",
    "- **è©•ä¾¡æ‰‹æ³•**: StratifiedKFoldã«ã‚ˆã‚‹ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã€log_lossã«ã‚ˆã‚‹è©•ä¾¡\n",
    "\n",
    "ã“ã®Notebookã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ãŸæ©Ÿæ¢°å­¦ç¿’ã‚¿ã‚¹ã‚¯ã«å¯¾ã™ã‚‹å¾¹åº•ã—ãŸå‰å‡¦ç†ã€ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã€ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ãƒ—ãƒ­ã‚»ã‚¹ã‚’ç¤ºã—ã¦ãŠã‚Šã€é¸å¥½äºˆæ¸¬ã®å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e39764",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã¯ã€Jupyter Notebookã«å«ã¾ã‚Œã‚‹å°‚é–€ç”¨èªã®è§£èª¬ã§ã™ã€‚åˆå¿ƒè€…å‘ã‘ã«å¿…è¦ãªåŸŸå¤–ã®ç”¨èªã‚„å°‘ã—ãƒã‚¤ãƒŠãƒ¼ãªã‚‚ã®ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "1. **ã‚¬ãƒ¼ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³(GC)**:\n",
    "   è‡ªå‹•çš„ã«ãƒ¡ãƒ¢ãƒªã‚’ç®¡ç†ã—ã€ä¸è¦ãªã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å‰Šé™¤ã™ã‚‹æ©Ÿèƒ½ã€‚ãƒ¡ãƒ¢ãƒªãƒªãƒ¼ã‚¯ã‚’é¿ã‘ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚`gc`ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã€æ˜ç¤ºçš„ã«ã‚¬ãƒ¼ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’èµ·å‹•ã§ãã‚‹ã€‚\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   æ–‡æ›¸å†…ã«ãŠã‘ã‚‹å˜èªã®é‡è¦åº¦ã‚’æ¸¬ã‚‹ãŸã‚ã®æŒ‡æ¨™ã€‚é »ç¹ã«å‡ºç¾ã™ã‚‹å˜èªã¯ã‚ã¾ã‚Šé‡è¦æ€§ãŒãªã„ã¨ã¿ãªã•ã‚Œã€é€†ã«æ–‡æ›¸å…¨ä½“ã«åºƒã¾ã£ã¦ã„ãªã„å˜èªã¯é‡è¦ã¨è©•ä¾¡ã•ã‚Œã‚‹ã€‚\n",
    "\n",
    "3. **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ (Cosine Similarity)**:\n",
    "   2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è§’åº¦ã‚’æ¸¬å®šã™ã‚‹æŒ‡æ¨™ã§ã€ç‰¹ã«ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹ãŸã‚ã«ã‚ˆãä½¿ã‚ã‚Œã‚‹ã€‚æ•°å€¤çš„ã«ã¯ã€å†…ç©ã‚’ãƒ™ã‚¯ãƒˆãƒ«ã®ãƒãƒ«ãƒ ã§å‰²ã£ãŸã‚‚ã®ã¨å®šç¾©ã•ã‚Œã‚‹ã€‚\n",
    "\n",
    "4. **ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ (Jaccard Similarity)**:\n",
    "   2ã¤ã®é›†åˆã®é¡ä¼¼åº¦ã‚’æ¸¬ã‚‹æ–¹æ³•ã€‚äº¤å·®éƒ¨åˆ†ã®ã‚µã‚¤ã‚ºã‚’åˆä½µéƒ¨åˆ†ã®ã‚µã‚¤ã‚ºã§å‰²ã‚‹ã“ã¨ã§è¨ˆç®—ã•ã‚Œã‚‹ã€‚ç‰¹ã«ã€ãƒ†ã‚­ã‚¹ãƒˆé–“ã®é‡è¤‡ã—ãŸå˜èªã®å‰²åˆã‚’çŸ¥ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚\n",
    "\n",
    "5. **n-gram**:\n",
    "   ãƒ†ã‚­ã‚¹ãƒˆã‚’ã€Œnã€å€‹ã®é€£ç¶šã—ãŸé …ç›®ã«åˆ†ã‘ãŸã‚‚ã®ã€‚ãŸã¨ãˆã°ã€ãƒã‚¤ã‚°ãƒ©ãƒ ã¯2ã¤ã®å˜èªã®çµ„ã¿åˆã‚ã›ã€ãƒˆãƒ©ã‚¤ã‚°ãƒ©ãƒ ã¯3ã¤ã®å˜èªã®çµ„ã¿åˆã‚ã›ã¨ãªã‚‹ã€‚\n",
    "\n",
    "6. **ãƒˆãƒ¼ã‚¯ãƒ³åŒ–(Tokenization)**:\n",
    "   ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å€‹ã€…ã®å˜èªã‚„ãƒ•ãƒ¬ãƒ¼ã‚ºã«åˆ†è§£ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã€‚è‡ªç„¶è¨€èªå‡¦ç†ã«ãŠã„ã¦åŸºæœ¬çš„ãªã‚¹ãƒ†ãƒƒãƒ—ã€‚\n",
    "\n",
    "7. **å±¤åŒ–Kåˆ†å‰²äº¤å·®æ¤œè¨¼ (Stratified K-Fold Cross-Validation)**:\n",
    "   ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’Kå€‹ã®éƒ¨åˆ†ã«åˆ†ã‘ã‚‹æ–¹æ³•ã§ã€å„éƒ¨åˆ†ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆ†å¸ƒã‚’ä¿ã¤ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ãŒã‚ˆã‚Šä¿¡é ¼æ€§ã®é«˜ã„ã‚‚ã®ã«ãªã‚‹ã€‚\n",
    "\n",
    "8. **ANOVA Få€¤ (Analysis of Variance F-Value)**:\n",
    "   ç•°ãªã‚‹ã‚°ãƒ«ãƒ¼ãƒ—é–“ã®å¹³å‡ã®é•ã„ã‚’æ¸¬ã‚‹çµ±è¨ˆçš„æ–¹æ³•ã€‚ç‰¹å¾´é¸æŠã§ã€å„ç‰¹å¾´ãŒã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã«å¯¾ã—ã¦ã©ã‚Œã ã‘æƒ…å ±ã‚’æŒã£ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã™ã‚‹ã€‚\n",
    "\n",
    "9. **ã‚¢ãƒ¼ãƒªãƒ¼ã‚¹ãƒˆãƒƒãƒ—(Early Stopping)**:\n",
    "   ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®æ€§èƒ½ãŒå‘ä¸Šã—ãªããªã£ãŸæ®µéšã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’æ—©æœŸã«çµ‚äº†ã™ã‚‹ä»•çµ„ã¿ã€‚éå­¦ç¿’ã‚’é˜²ããŸã‚ã«æœ‰åŠ¹ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªã¯ã€æ©Ÿæ¢°å­¦ç¿’ã‚„è‡ªç„¶è¨€èªå‡¦ç†ã®é ˜åŸŸã§ã‚ˆãä½¿ã‚ã‚Œã‚‹ãŸã‚ã€çŸ¥è­˜ã‚’æ·±ã‚ã‚‹ã†ãˆã§é‡è¦ã§ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d73481",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# ğŸ“š Importing Libraries\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ğŸ“š ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286a94ef",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# gcãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: Pythonã®ã‚¬ãƒ¼ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æä¾›\n",
    "# osãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½ã‚’åˆ©ç”¨\n",
    "# reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹æ–‡å­—åˆ—æ“ä½œ\n",
    "# numpy: æ•°å€¤è¨ˆç®—ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# pandas: ãƒ‡ãƒ¼ã‚¿æ“ä½œç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# nltk: è‡ªç„¶è¨€èªå‡¦ç†ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# matplotlib.pyplot: ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ç”¨ã®2Dãƒ—ãƒ­ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# seaborn: ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# xgboost: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# lightgbm: LightGBMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# catboost: CatBoostã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# sklearn: æ©Ÿæ¢°å­¦ç¿’ç”¨ã®å¤šæ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’æä¾›\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:33.526535Z",
     "iopub.status.busy": "2024-07-11T07:20:33.526078Z",
     "iopub.status.idle": "2024-07-11T07:20:38.692753Z",
     "shell.execute_reply": "2024-07-11T07:20:38.691507Z",
     "shell.execute_reply.started": "2024-07-11T07:20:33.526503Z"
    }
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# gcãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: Pythonã®ã‚¬ãƒ¼ãƒ™ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’æä¾›\n",
    "# osãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: ã‚ªãƒšãƒ¬ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã®æ©Ÿèƒ½ã‚’åˆ©ç”¨\n",
    "# reãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«: æ­£è¦è¡¨ç¾ã«ã‚ˆã‚‹æ–‡å­—åˆ—æ“ä½œ\n",
    "# numpy: æ•°å€¤è¨ˆç®—ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# pandas: ãƒ‡ãƒ¼ã‚¿æ“ä½œç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# nltk: è‡ªç„¶è¨€èªå‡¦ç†ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# matplotlib.pyplot: ãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ç”¨ã®2Dãƒ—ãƒ­ãƒƒãƒˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# seaborn: ã‚ˆã‚Šæ´—ç·´ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿å¯è¦–åŒ–ç”¨ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "# xgboost: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# lightgbm: LightGBMã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# catboost: CatBoostã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Ÿè£…\n",
    "# sklearn: æ©Ÿæ¢°å­¦ç¿’ç”¨ã®å¤šæ•°ã®ãƒ„ãƒ¼ãƒ«ã‚’æä¾›"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb7f61",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# âš™ï¸ Configuration Class\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# âš™ï¸ è¨­å®šã‚¯ãƒ©ã‚¹\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbbaeb6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.695283Z",
     "iopub.status.busy": "2024-07-11T07:20:38.694735Z",
     "iopub.status.idle": "2024-07-11T07:20:38.701258Z",
     "shell.execute_reply": "2024-07-11T07:20:38.699693Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.695252Z"
    }
   },
   "outputs": [],
   "source": [
    "class config:\n",
    "    root = \"/kaggle/input/lmsys-chatbot-arena/\"\n",
    "    train_path = os.path.join(root, \"train.csv\")\n",
    "    test_path = os.path.join(root, \"test.csv\")\n",
    "    sample_submission_path = os.path.join(root, \"sample_submission.csv\")\n",
    "    seed = 42\n",
    "    n_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108e7dd8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "\n",
    "# ğŸ“Š Loading and Processing Data\n",
    "\n",
    "we will load our training and test datasets, and apply some preprocessing. This includes:\n",
    "\n",
    "\n",
    "1. **Loading Data**: Read the CSV files into pandas DataFrames.\n",
    "2. **Subsampling**: If the test dataset has less than 10 rows, subsample the training dataset to 10,000 rows for quicker processing.\n",
    "3. **Processing Strings**: Clean and process the string columns (`prompt`, `response_a`, `response_b`) by removing unwanted characters.\n",
    "4. **Shape and Missing Values**: Print the shape of the datasets and count missing values to understand the data structure and quality.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "\n",
    "# ğŸ“Š ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨å‡¦ç†\n",
    "\n",
    "ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã€ä¸€éƒ¨ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™:\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿**: CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’pandasã®DataFrameã«èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n",
    "2. **ã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¡Œæ•°ãŒ10æœªæº€ã®å ´åˆã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰10,000è¡Œã‚µãƒ–ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦è¿…é€Ÿã«å‡¦ç†ã—ã¾ã™ã€‚\n",
    "3. **æ–‡å­—åˆ—ã®å‡¦ç†**: ä¸è¦ãªæ–‡å­—ã‚’å‰Šé™¤ã—ã¦ã€æ–‡å­—åˆ—åˆ—ï¼ˆ`prompt`, `response_a`, `response_b`ï¼‰ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—å‡¦ç†ã—ã¾ã™ã€‚\n",
    "4. **ãƒ‡ãƒ¼ã‚¿ã®å½¢çŠ¶ã¨æ¬ æå€¤**: ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢çŠ¶ã‚’å‡ºåŠ›ã—ã€æ¬ æå€¤ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã¨å“è³ªã‚’ç†è§£ã—ã¾ã™ã€‚\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c2acaf",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)\n",
    "test = pd.read_csv(config.test_path)\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(config.train_path)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "test = pd.read_csv(config.test_path)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # æå‡ºç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ãŒ10æœªæº€ã§ã‚ã‚Œã°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®10,000è¡Œã‚’ä½¿ç”¨ã™ã‚‹\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # æ–‡å­—åˆ—ã®å‰å¾Œã®ãƒ–ãƒ©ã‚±ãƒƒãƒˆã‚’å‰Šé™¤\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤\n",
    "    return  ' '.join(sentences)  # çµæœã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§ç¹‹ã’ã¦è¿”ã™\n",
    "\n",
    "# å„åˆ—ã«processé–¢æ•°ã‚’é©ç”¨ã—ã¦æ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢çŠ¶ã‚’å‡ºåŠ›\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "# æ¬ æå€¤ã®è¨ˆç®—\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:38.703195Z",
     "iopub.status.busy": "2024-07-11T07:20:38.702888Z",
     "iopub.status.idle": "2024-07-11T07:20:42.781785Z",
     "shell.execute_reply": "2024-07-11T07:20:42.780465Z",
     "shell.execute_reply.started": "2024-07-11T07:20:38.703171Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(config.train_path)  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "test = pd.read_csv(config.test_path)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "sample_submission = pd.read_csv(config.sample_submission_path)  # æå‡ºç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®è¡Œæ•°ãŒ10æœªæº€ã§ã‚ã‚Œã°ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®10,000è¡Œã‚’ä½¿ç”¨ã™ã‚‹\n",
    "if test.shape[0] < 10:\n",
    "    train = train.iloc[:10000]\n",
    "    \n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # æ–‡å­—åˆ—ã®å‰å¾Œã®ãƒ–ãƒ©ã‚±ãƒƒãƒˆã‚’å‰Šé™¤\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦ã‚¯ã‚©ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‰Šé™¤\n",
    "    return  ' '.join(sentences)  # çµæœã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§ç¹‹ã’ã¦è¿”ã™\n",
    "\n",
    "# å„åˆ—ã«processé–¢æ•°ã‚’é©ç”¨ã—ã¦æ–‡å­—åˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—\n",
    "train[\"prompt\"] = train[\"prompt\"].apply(process)\n",
    "train[\"response_a\"] = train[\"response_a\"].apply(process)\n",
    "train[\"response_b\"] = train[\"response_b\"].apply(process)\n",
    "\n",
    "test[\"prompt\"] = test[\"prompt\"].apply(process)\n",
    "test[\"response_a\"] = test[\"response_a\"].apply(process)\n",
    "test[\"response_b\"] = test[\"response_b\"].apply(process)\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å½¢çŠ¶ã‚’å‡ºåŠ›\n",
    "print(f\"train shape: {train.shape}\")\n",
    "print(f\"test shape: {test.shape}\")\n",
    "print(\"-\"*90)\n",
    "# æ¬ æå€¤ã®è¨ˆç®—\n",
    "print(f\"train missing values: {train.isnull().sum().sum()}\")\n",
    "print(f\"test missing values: {test.isnull().sum().sum()}\")\n",
    "print(\"-\"*90)\n",
    "\n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cc9dc6",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# ğŸ› ï¸ Preprocessing Class Definition\n",
    "\n",
    "This class, `Preprocessor`, contains several methods to process and feature-engineer the text data. Hereâ€™s a breakdown of its functionalities:\n",
    "\n",
    "#### Cosine Similarity\n",
    "- **Formula**: \n",
    "  $$\\text{cosine_similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **Description**: Cosine similarity measures the cosine of the angle between two vectors, providing a metric of how similar the texts are.\n",
    "\n",
    "#### Jaccard Similarity\n",
    "- **Formula**:\n",
    "\n",
    "  $$\\text{Jaccard_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- **Description**: Jaccard similarity measures the similarity between two sets by comparing their intersection and union.\n",
    "\n",
    "#### Count Quotes\n",
    "- **Description**: This method identifies and counts both single and double quoted texts in a string, giving an idea of how many quotations are present.\n",
    "\n",
    "#### Tokenize\n",
    "- **Description**: This method splits the text into individual words (tokens), which can be used for further analysis like generating n-grams or calculating overlaps.\n",
    "\n",
    "#### Generate N-grams\n",
    "- **Description**: N-grams are contiguous sequences of 'n' items from a given text. This method helps in analyzing the text at different levels of granularity (unigrams, bigrams, trigrams, etc.).\n",
    "\n",
    "#### Count N-gram Overlaps\n",
    "- **Description**: This method calculates how many n-grams are common between two texts, helping to measure their similarity.\n",
    "\n",
    "#### Run\n",
    "- **Description**: This method processes the entire dataset, generating new features based on the above calculations, which can be used for training machine learning models.\n",
    "\n",
    "\n",
    "$\\frac{n!}{k!(n-k)!} = \\binom{n}{k}$\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ğŸ› ï¸ å‰å‡¦ç†ã‚¯ãƒ©ã‚¹å®šç¾©\n",
    "\n",
    "`Preprocessor`ã‚¯ãƒ©ã‚¹ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ç‰¹å¾´é‡ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ã‚’è¡Œã†ã„ãã¤ã‹ã®ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚ä»¥ä¸‹ã«ãã®æ©Ÿèƒ½ã®èª¬æ˜ã‚’ç¤ºã—ã¾ã™ï¼š\n",
    "\n",
    "#### ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "- **æ•°å¼**: \n",
    "  $$\\text{cosine_similarity} = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **èª¬æ˜**: ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã¯2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«ã®é–“ã®è§’åº¦ã®ã‚³ã‚µã‚¤ãƒ³ã‚’æ¸¬å®šã—ã€ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦ã®æŒ‡æ¨™ã‚’æä¾›ã—ã¾ã™ã€‚\n",
    "\n",
    "#### ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦\n",
    "- **æ•°å¼**:\n",
    "\n",
    "  $$\\text{Jaccard_similarity} = \\frac{|A \\cap B|}{|A \\cup B|}$$\n",
    "\n",
    "- **èª¬æ˜**: ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ã¯ã€2ã¤ã®é›†åˆã®äº¤å·®éƒ¨åˆ†ã¨åˆä½µéƒ¨åˆ†ã‚’æ¯”è¼ƒã™ã‚‹ã“ã¨ã§é¡ä¼¼åº¦ã‚’æ¸¬å®šã—ã¾ã™ã€‚\n",
    "\n",
    "#### å¼•ç”¨ã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "- **èª¬æ˜**: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€æ–‡å­—åˆ—å†…ã®å˜ä¸€ãŠã‚ˆã³äºŒé‡å¼•ç”¨ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’ç‰¹å®šã—ã€ã‚«ã‚¦ãƒ³ãƒˆã™ã‚‹ã“ã¨ã§ã€å¼•ç”¨ã®æ•°ã‚’æŠŠæ¡ã—ã¾ã™ã€‚\n",
    "\n",
    "#### ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "- **èª¬æ˜**: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆã‚’å€‹ã€…ã®å˜èªï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«åˆ†å‰²ã—ã€n-gramã‚’ç”Ÿæˆã—ãŸã‚Šã€é‡è¤‡ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã®ã•ã‚‰ãªã‚‹åˆ†æã«ä½¿ç”¨ã§ãã¾ã™ã€‚\n",
    "\n",
    "#### N-gramã®ç”Ÿæˆ\n",
    "- **èª¬æ˜**: N-gramã¯ã€ä¸ãˆã‚‰ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã®'n'é …ç›®ã®é€£ç¶šã—ãŸã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã™ã€‚ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ç•°ãªã‚‹ç²’åº¦ï¼ˆå˜èªã€2-gramã€3-gramãªã©ï¼‰ã§ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†æã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "#### N-gramã®é‡è¤‡ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "- **èª¬æ˜**: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€2ã¤ã®ãƒ†ã‚­ã‚¹ãƒˆé–“ã®å…±é€šã®n-gramã®æ•°ã‚’è¨ˆç®—ã—ã€é¡ä¼¼åº¦ã‚’æ¸¬å®šã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "#### å®Ÿè¡Œ\n",
    "- **èª¬æ˜**: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã€ä¸Šè¨˜ã®è¨ˆç®—ã«åŸºã¥ã„ã¦æ–°ã—ã„ç‰¹å¾´é‡ã‚’ç”Ÿæˆã—ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ä½¿ç”¨ã§ãã‚‹å…¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‡¦ç†ã—ã¾ã™ã€‚\n",
    "\n",
    "$\\frac{n!}{k!(n-k)!} = \\binom{n}{k}$\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca5dc84",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])\n",
    "            vectors = vectorizer.toarray()\n",
    "            cos_sim = cosine_similarity(vectors)\n",
    "            return cos_sim[0][1]\n",
    "        except:\n",
    "            return np.nan\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())\n",
    "        set2 = set(text2.split())\n",
    "        intersection = set1.intersection(set2)\n",
    "        union = set1.union(set2)\n",
    "        return len(intersection) / len(union)\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"\n",
    "        double_quote_pattern = r'\"(.*?)\"'\n",
    "        single_quotes = re.findall(single_quote_pattern, text)\n",
    "        double_quotes = re.findall(double_quote_pattern, text)\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)\n",
    "        return len(single_quotes) + len(double_quotes)\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)\n",
    "        return list(ngrams(tokens, n))\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)\n",
    "            ngrams2 = self.generate_ngrams(text2, n)\n",
    "            counter1 = Counter(ngrams1)\n",
    "            counter2 = Counter(ngrams2)\n",
    "            overlap = counter1 & counter2\n",
    "            overlap_count = sum(overlap.values())\n",
    "            return overlap_count\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))\n",
    "        \n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))\n",
    "        \n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])  # ãƒ†ã‚­ã‚¹ãƒˆã®TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ\n",
    "            vectors = vectorizer.toarray()  # ãƒ™ã‚¯ãƒˆãƒ«ã‚’é…åˆ—ã«å¤‰æ›\n",
    "            cos_sim = cosine_similarity(vectors)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "            return cos_sim[0][1]  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®çµæœã‚’è¿”ã™\n",
    "        except:\n",
    "            return np.nan  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯NaNã‚’è¿”ã™\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # ãƒ†ã‚­ã‚¹ãƒˆ1ã‚’å˜èªã®é›†åˆã«å¤‰æ›\n",
    "        set2 = set(text2.split())  # ãƒ†ã‚­ã‚¹ãƒˆ2ã‚’å˜èªã®é›†åˆã«å¤‰æ›\n",
    "        intersection = set1.intersection(set2)  # äº¤å·®éƒ¨åˆ†ã‚’è¨ˆç®—\n",
    "        union = set1.union(set2)  # åˆä½µéƒ¨åˆ†ã‚’è¨ˆç®—\n",
    "        return len(intersection) / len(union)  # ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ã‚’è¿”ã™\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # å˜ä¸€å¼•ç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # äºŒé‡å¼•ç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # å˜ä¸€å¼•ç”¨ã‚’æŠ½å‡º\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # äºŒé‡å¼•ç”¨ã‚’æŠ½å‡º\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # åˆè¨ˆã®å¼•ç”¨æ•°\n",
    "        return total_quotes  # åˆè¨ˆã®å¼•ç”¨æ•°ã‚’è¿”ã™\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())  # ãƒ†ã‚­ã‚¹ãƒˆã‚’å°æ–‡å­—ã«ã—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "        return list(ngrams(tokens, n))  # n-gramã‚’ç”Ÿæˆ\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # ãƒ†ã‚­ã‚¹ãƒˆ1ã‹ã‚‰n-gramã‚’ç”Ÿæˆ\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # ãƒ†ã‚­ã‚¹ãƒˆ2ã‹ã‚‰n-gramã‚’ç”Ÿæˆ\n",
    "            counter1 = Counter(ngrams1)  # ãƒ†ã‚­ã‚¹ãƒˆ1ã®n-gramã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            counter2 = Counter(ngrams2)  # ãƒ†ã‚­ã‚¹ãƒˆ2ã®n-gramã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            overlap = counter1 & counter2  # å…±é€šã®n-gramã‚’è¨ˆç®—\n",
    "            overlap_count = sum(overlap.values())  # é‡è¤‡ã®åˆè¨ˆã‚’è¨ˆç®—\n",
    "            return overlap_count  # é‡è¤‡æ•°ã‚’è¿”ã™\n",
    "        except:\n",
    "            return 0  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯0ã‚’è¿”ã™\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # ãã‚Œãã‚Œã®å¿œç­”é–“ã§ã®n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¨ˆç®—\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        # å„å¿œç­”ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–“ã§ã®n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        # å„å¿œç­”ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã‚’è¨ˆç®—\n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # å¿œç­”Aã®é•·ã•\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # å¿œç­”Bã®é•·ã•\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•\n",
    "        \n",
    "        # ç•°ãªã‚‹é•·ã•ã®æ¯”ç‡ã‚„å·®ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # å¿œç­”Aã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®æ¯”ç‡\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # å¿œç­”Bã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®æ¯”ç‡\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # å¿œç­”Aã¨å¿œç­”Bã®é•·ã•ã®æ¯”ç‡\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # é•·ã•ã®å·®\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # å¿œç­”Aã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®å·®\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # å¿œç­”Bã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®å·®\n",
    "        \n",
    "        # n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        # å¼•ç”¨ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # å¿œç­”Aã®å¼•ç”¨æ•°\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # å¿œç­”Bã®å¼•ç”¨æ•°\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼•ç”¨æ•°\n",
    "        \n",
    "        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data  # å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.785579Z",
     "iopub.status.busy": "2024-07-11T07:20:42.784505Z",
     "iopub.status.idle": "2024-07-11T07:20:42.810829Z",
     "shell.execute_reply": "2024-07-11T07:20:42.809399Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.785544Z"
    }
   },
   "outputs": [],
   "source": [
    "class Preprocessor:\n",
    "\n",
    "    def cosine_sim(self, text1: str, text2: str):\n",
    "        try:\n",
    "            vectorizer = TfidfVectorizer().fit_transform([text1, text2])  # ãƒ†ã‚­ã‚¹ãƒˆã®TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã‚’ä½œæˆ\n",
    "            vectors = vectorizer.toarray()  # ãƒ™ã‚¯ãƒˆãƒ«ã‚’é…åˆ—ã«å¤‰æ›\n",
    "            cos_sim = cosine_similarity(vectors)  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "            return cos_sim[0][1]  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®çµæœã‚’è¿”ã™\n",
    "        except:\n",
    "            return np.nan  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯NaNã‚’è¿”ã™\n",
    "\n",
    "    def jaccard_sim(self, text1: str, text2: str):\n",
    "        set1 = set(text1.split())  # ãƒ†ã‚­ã‚¹ãƒˆ1ã‚’å˜èªã®é›†åˆã«å¤‰æ›\n",
    "        set2 = set(text2.split())  # ãƒ†ã‚­ã‚¹ãƒˆ2ã‚’å˜èªã®é›†åˆã«å¤‰æ›\n",
    "        intersection = set1.intersection(set2)  # äº¤å·®éƒ¨åˆ†ã‚’è¨ˆç®—\n",
    "        union = set1.union(set2)  # åˆä½µéƒ¨åˆ†ã‚’è¨ˆç®—\n",
    "        return len(intersection) / len(union)  # ã‚¸ãƒ£ãƒƒã‚«ãƒ¼ãƒ‰é¡ä¼¼åº¦ã‚’è¿”ã™\n",
    "    \n",
    "    def count_quotes(self, text: str) -> int:\n",
    "        single_quote_pattern = r\"'(.*?)'\"  # å˜ä¸€å¼•ç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        double_quote_pattern = r'\"(.*?)\"'  # äºŒé‡å¼•ç”¨ã®ãƒ‘ã‚¿ãƒ¼ãƒ³\n",
    "        single_quotes = re.findall(single_quote_pattern, text)  # å˜ä¸€å¼•ç”¨ã‚’æŠ½å‡º\n",
    "        double_quotes = re.findall(double_quote_pattern, text)  # äºŒé‡å¼•ç”¨ã‚’æŠ½å‡º\n",
    "        total_quotes = len(single_quotes) + len(double_quotes)  # åˆè¨ˆã®å¼•ç”¨æ•°\n",
    "        return total_quotes  # åˆè¨ˆã®å¼•ç”¨æ•°ã‚’è¿”ã™\n",
    "\n",
    "    def tokenize(self, text: str):\n",
    "        return nltk.word_tokenize(text.lower())  # ãƒ†ã‚­ã‚¹ãƒˆã‚’å°æ–‡å­—ã«ã—ãƒˆãƒ¼ã‚¯ãƒ³åŒ–\n",
    "\n",
    "    def generate_ngrams(self, text: str, n: int):\n",
    "        tokens = self.tokenize(text)  # ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—\n",
    "        return list(ngrams(tokens, n))  # n-gramã‚’ç”Ÿæˆ\n",
    "\n",
    "    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:\n",
    "        try:\n",
    "            ngrams1 = self.generate_ngrams(text1, n)  # ãƒ†ã‚­ã‚¹ãƒˆ1ã‹ã‚‰n-gramã‚’ç”Ÿæˆ\n",
    "            ngrams2 = self.generate_ngrams(text2, n)  # ãƒ†ã‚­ã‚¹ãƒˆ2ã‹ã‚‰n-gramã‚’ç”Ÿæˆ\n",
    "            counter1 = Counter(ngrams1)  # ãƒ†ã‚­ã‚¹ãƒˆ1ã®n-gramã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            counter2 = Counter(ngrams2)  # ãƒ†ã‚­ã‚¹ãƒˆ2ã®n-gramã®ã‚«ã‚¦ãƒ³ãƒˆ\n",
    "            overlap = counter1 & counter2  # å…±é€šã®n-gramã‚’è¨ˆç®—\n",
    "            overlap_count = sum(overlap.values())  # é‡è¤‡ã®åˆè¨ˆã‚’è¨ˆç®—\n",
    "            return overlap_count  # é‡è¤‡æ•°ã‚’è¿”ã™\n",
    "        except:\n",
    "            return 0  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯0ã‚’è¿”ã™\n",
    "        \n",
    "    def run(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \n",
    "        # ãã‚Œãã‚Œã®å¿œç­”é–“ã§ã®n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¨ˆç®—\n",
    "        data[\"respa_respb_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 1), axis=1)\n",
    "        data[\"respa_respb_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 2), axis=1)\n",
    "        data[\"respa_respb_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"response_b\"], 3), axis=1)\n",
    "\n",
    "        # å„å¿œç­”ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé–“ã§ã®n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respa_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respa_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_a\"], x[\"prompt\"], 3), axis=1)\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 1), axis=1)\n",
    "        data[\"respb_prompt_overlap_bigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 2), axis=1)\n",
    "        data[\"respb_prompt_overlap_trigram\"] = data.apply(lambda x: self.count_ngram_overlaps(x[\"response_b\"], x[\"prompt\"], 3), axis=1)\n",
    "        \n",
    "        # å„å¿œç­”ã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã‚’è¨ˆç®—\n",
    "        data[\"respa_len\"] = data[\"response_a\"].apply(lambda x: len(self.tokenize(x)))  # å¿œç­”Aã®é•·ã•\n",
    "        data[\"respb_len\"] = data[\"response_b\"].apply(lambda x: len(self.tokenize(x)))  # å¿œç­”Bã®é•·ã•\n",
    "        data[\"prompt_len\"] = data[\"prompt\"].apply(lambda x: len(self.tokenize(x)))  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•\n",
    "        \n",
    "        # ç•°ãªã‚‹é•·ã•ã®æ¯”ç‡ã‚„å·®ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_len_ratio\"] = data[\"respa_len\"] / data[\"prompt_len\"]  # å¿œç­”Aã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®æ¯”ç‡\n",
    "        data[\"respb_prompt_len_ratio\"] = data[\"respb_len\"] / data[\"prompt_len\"]  # å¿œç­”Bã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®æ¯”ç‡\n",
    "        data[\"respa_respb_len_ratio\"] = data[\"respa_len\"] / data[\"respb_len\"]  # å¿œç­”Aã¨å¿œç­”Bã®é•·ã•ã®æ¯”ç‡\n",
    "        \n",
    "        data[\"respa_respb_len_diff\"] = data[\"respa_len\"] - data[\"respb_len\"]  # é•·ã•ã®å·®\n",
    "        data[\"respa_prompt_len_diff\"] = data[\"respa_len\"] - data[\"prompt_len\"]  # å¿œç­”Aã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®å·®\n",
    "        data[\"respb_prompt_len_diff\"] = data[\"respb_len\"] - data[\"prompt_len\"]  # å¿œç­”Bã¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®é•·ã•ã®å·®\n",
    "        \n",
    "        # n-gramã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—ã®æ¯”ç‡ã‚’è¨ˆç®—\n",
    "        data[\"respa_prompt_overlap_unigram_ratio\"] = data[\"respa_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_bigram_ratio\"] = data[\"respa_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respa_prompt_overlap_trigram_ratio\"] = data[\"respa_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "\n",
    "        data[\"respb_prompt_overlap_unigram_ratio\"] = data[\"respb_prompt_overlap_unigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_bigram_ratio\"] = data[\"respb_prompt_overlap_bigram\"] / data[\"prompt_len\"]\n",
    "        data[\"respb_prompt_overlap_trigram_ratio\"] = data[\"respb_prompt_overlap_trigram\"] / data[\"prompt_len\"]\n",
    "        \n",
    "        # å¼•ç”¨ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "        data[\"respa_quotes\"] = data[\"response_a\"].apply(lambda x: self.count_quotes(x))  # å¿œç­”Aã®å¼•ç”¨æ•°\n",
    "        data[\"respb_quotes\"] = data[\"response_b\"].apply(lambda x: self.count_quotes(x))  # å¿œç­”Bã®å¼•ç”¨æ•°\n",
    "        data[\"prompt_quotes\"] = data[\"prompt\"].apply(lambda x: self.count_quotes(x))  # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å¼•ç”¨æ•°\n",
    "        \n",
    "        # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "        data[\"respa_respb_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        data[\"respa_respb_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"response_b\"]), axis=1)\n",
    "        \n",
    "        data[\"respa_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respa_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_a\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        data[\"respb_prompt_cosine_sim\"] = data.apply(lambda x: self.cosine_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        data[\"respb_prompt_jaccard_sim\"] = data.apply(lambda x: self.jaccard_sim(x[\"response_b\"], x[\"prompt\"]), axis=1)\n",
    "        \n",
    "        return data  # å‡¦ç†å¾Œã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04121949",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()\n",
    "train = preprocessor.run(train)\n",
    "test = preprocessor.run(test)\n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()  # Preprocessorã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "train = preprocessor.run(train)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "test = preprocessor.run(test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:20:42.812893Z",
     "iopub.status.busy": "2024-07-11T07:20:42.812433Z",
     "iopub.status.idle": "2024-07-11T07:27:19.005204Z",
     "shell.execute_reply": "2024-07-11T07:27:19.003828Z",
     "shell.execute_reply.started": "2024-07-11T07:20:42.812855Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "preprocessor = Preprocessor()  # Preprocessorã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆ\n",
    "train = preprocessor.run(train)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "test = preprocessor.run(test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦å‰å‡¦ç†ã‚’å®Ÿè¡Œ\n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29b5632",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Data Preparation\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "### ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59dd705",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "target = \"target\"\n",
    "\n",
    "train[target] = np.nan\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx\n",
    "train[target] = train[target].astype(\"int32\")\n",
    "    \n",
    "train.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # å‰Šé™¤ã™ã‚‹åˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "target = \"target\"  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°\n",
    "\n",
    "train[target] = np.nan  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆæœŸåŒ–\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã«åŸºã¥ã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨­å®š\n",
    "train[target] = train[target].astype(\"int32\")  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’æ•´æ•°å‹ã«å¤‰æ›\n",
    "    \n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.007225Z",
     "iopub.status.busy": "2024-07-11T07:27:19.006777Z",
     "iopub.status.idle": "2024-07-11T07:27:19.041533Z",
     "shell.execute_reply": "2024-07-11T07:27:19.040457Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.007186Z"
    }
   },
   "outputs": [],
   "source": [
    "drop_cols = [\"id\", \"response_a\", \"response_b\", \"prompt\"]  # å‰Šé™¤ã™ã‚‹åˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "target_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã®ãƒªã‚¹ãƒˆ\n",
    "target = \"target\"  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°\n",
    "\n",
    "train[target] = np.nan  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã®åˆæœŸåŒ–\n",
    "for idx, t in enumerate(target_cols):\n",
    "    train.loc[train[t] == 1, target] = idx  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã«åŸºã¥ã„ã¦ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨­å®š\n",
    "train[target] = train[target].astype(\"int32\")  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°ã‚’æ•´æ•°å‹ã«å¤‰æ›\n",
    "    \n",
    "train.head()  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55728508",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)\n",
    "y = train[target]\n",
    "X_test = test.drop(columns=drop_cols, axis=1)\n",
    "\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)  # ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹Xã‚’ä½œæˆ\n",
    "y = train[target]  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°yã‚’è¨­å®š\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹X_testã‚’ä½œæˆ\n",
    "\n",
    "# ç„¡é™å¤§ã‚„è² ç„¡é™å¤§ã‚’NaNã«ç½®ãæ›ãˆã‚‹\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.04354Z",
     "iopub.status.busy": "2024-07-11T07:27:19.043159Z",
     "iopub.status.idle": "2024-07-11T07:27:19.071594Z",
     "shell.execute_reply": "2024-07-11T07:27:19.070564Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.043511Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop(columns=target_cols+drop_cols+[target]+[\"model_a\", \"model_b\"], axis=1)  # ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹Xã‚’ä½œæˆ\n",
    "y = train[target]  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆå¤‰æ•°yã‚’è¨­å®š\n",
    "X_test = test.drop(columns=drop_cols, axis=1)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹X_testã‚’ä½œæˆ\n",
    "\n",
    "# ç„¡é™å¤§ã‚„è² ç„¡é™å¤§ã‚’NaNã«ç½®ãæ›ãˆã‚‹\n",
    "X = X.replace([-np.inf, np.inf], np.nan)\n",
    "X_test = X_test.replace([-np.inf, np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7be369",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# æ¬ æå€¤ã®å‡¦ç†\n",
    "imputer = SimpleImputer(strategy='mean')  # å¹³å‡ã§æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ—ãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’è£œå®Œ\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’è£œå®Œ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.073423Z",
     "iopub.status.busy": "2024-07-11T07:27:19.072999Z",
     "iopub.status.idle": "2024-07-11T07:27:19.095853Z",
     "shell.execute_reply": "2024-07-11T07:27:19.094538Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.073387Z"
    }
   },
   "outputs": [],
   "source": [
    "# æ¬ æå€¤ã®å‡¦ç†\n",
    "imputer = SimpleImputer(strategy='mean')  # å¹³å‡ã§æ¬ æå€¤ã‚’è£œå®Œã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ—ãƒƒã‚¿ãƒ¼ã‚’ä½œæˆ\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’è£œå®Œ\n",
    "X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æ¬ æå€¤ã‚’è£œå®Œ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9912f1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Feature Selection\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ç‰¹å¾´é¸æŠ\n",
    "selector = SelectKBest(f_classif, k=25)  # æœ€è‰¯ã®kå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "X_new = selector.fit_transform(X, y)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ç‰¹å¾´é¸æŠã‚’é©ç”¨\n",
    "X_test_new = selector.transform(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦åŒæ§˜ã«é©ç”¨\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.097367Z",
     "iopub.status.busy": "2024-07-11T07:27:19.096993Z",
     "iopub.status.idle": "2024-07-11T07:27:19.122964Z",
     "shell.execute_reply": "2024-07-11T07:27:19.121688Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.09734Z"
    }
   },
   "outputs": [],
   "source": [
    "# ç‰¹å¾´é¸æŠ\n",
    "selector = SelectKBest(f_classif, k=25)  # æœ€è‰¯ã®kå€‹ã®ç‰¹å¾´é‡ã‚’é¸æŠ\n",
    "X_new = selector.fit_transform(X, y)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ç‰¹å¾´é¸æŠã‚’é©ç”¨\n",
    "X_test_new = selector.transform(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦åŒæ§˜ã«é©ç”¨"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d910368",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# ğŸ§© Model Training and Evaluation\n",
    "\n",
    "### Model Definitions\n",
    "\n",
    "Defines several machine learning models including Random Forest, Gradient Boosting, SVM, XGBoost, CatBoost, and a Voting Classifier.\n",
    "\n",
    "### Feature Selection\n",
    "\n",
    "Uses SelectKBest to select 25 best features based on ANOVA F-value between feature and target.\n",
    "\n",
    "### Cross-validation\n",
    "\n",
    "Uses Stratified K-Fold cross-validation for model evaluation.\n",
    "\n",
    "### Training and Evaluation\n",
    "\n",
    "Iterates through each model, trains it using the training data, evaluates using cross-validation, and calculates the mean CV Log Loss.\n",
    "\n",
    "### Feature Importances\n",
    "\n",
    "Calculates and stores feature importances for applicable models (Random Forest, Gradient Boosting, XGBoost, CatBoost).\n",
    "\n",
    "### Best Model Identification\n",
    "\n",
    "Identifies the best performing model based on the lowest CV Log Loss.\n",
    "\n",
    "### Results Display\n",
    "\n",
    "Displays the results in a DataFrame showing the CV Log Loss for each model and, if applicable, feature importances.\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ğŸ§© ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡\n",
    "\n",
    "### ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©\n",
    "\n",
    "ãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã€SVMã€XGBoostã€CatBoostã€ãŠã‚ˆã³æŠ•ç¥¨åˆ†é¡å™¨ã‚’å«ã‚€ã„ãã¤ã‹ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚\n",
    "\n",
    "### ç‰¹å¾´é¸æŠ\n",
    "\n",
    "ANOVA Få€¤ã«åŸºã¥ã„ã¦ã€SelectKBestã‚’ä½¿ç”¨ã—ã¦25ã®æœ€è‰¯ç‰¹å¾´ã‚’é¸æŠã—ã¾ã™ã€‚\n",
    "\n",
    "### ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "\n",
    "ãƒ¢ãƒ‡ãƒ«è©•ä¾¡ã®ãŸã‚ã«å±¤åŒ–Kåˆ†å‰²äº¤å·®æ¤œè¨¼ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "\n",
    "### ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨è©•ä¾¡\n",
    "\n",
    "å„ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©å‡¦ç†ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã€ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦è©•ä¾¡ã—ã€å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "\n",
    "### ç‰¹å¾´ã®é‡è¦åº¦\n",
    "\n",
    "é©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ãƒ•ã‚©ãƒ¬ã‚¹ãƒˆã€å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã€XGBoostã€CatBoostï¼‰ã«ã¤ã„ã¦ã€ç‰¹å¾´ã®é‡è¦åº¦ã‚’è¨ˆç®—ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "### æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š\n",
    "\n",
    "æœ€ã‚‚ä½ã„CVãƒ­ã‚°ãƒ­ã‚¹ã«åŸºã¥ã„ã¦ã€æœ€ã‚‚è‰¯ã„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã—ã¾ã™ã€‚\n",
    "\n",
    "### çµæœã®è¡¨ç¤º\n",
    "\n",
    "å„ãƒ¢ãƒ‡ãƒ«ã®CVãƒ­ã‚°ãƒ­ã‚¹ã‚’ç¤ºã—ã€é©ç”¨å¯èƒ½ãªå ´åˆã¯ç‰¹å¾´ã®é‡è¦åº¦ã‚’è¡¨ç¤ºã™ã‚‹DataFrameã‚’è¡¨ç¤ºã—ã¾ã™ã€‚\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2effc08",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "# Define models and their configurations\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # GPU acceleration if available\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # Use GPU if available\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Select features using SelectKBest\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "X_test_new = selector.transform(X_test)\n",
    "\n",
    "# Cross-validation setup\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# Dataframe to store results\n",
    "results = []\n",
    "\n",
    "# Iterate over models\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"Training model: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))\n",
    "    cv_scores = []\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train\n",
    "            )\n",
    "\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")\n",
    "            cv_scores.append(val_log_loss)\n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})\n",
    "        print(f\"Mean CV Log Loss: {mean_cv_log_loss:.5f}\")\n",
    "\n",
    "# Store feature importances if applicable\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)\n",
    "\n",
    "# Convert results to DataFrame\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Identify the best model\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]\n",
    "print(f\"\\nBest Model:\\n{best_model}\")\n",
    "\n",
    "# Display results DataFrame\n",
    "print(\"\\nResults DataFrame:\")\n",
    "print(results_df)\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãã®è¨­å®šã‚’å®šç¾©\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ã°GPUåŠ é€Ÿã‚’ä½¿ç”¨\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ã°ä½¿ç”¨\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# SelectKBestã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´ã‚’é¸æŠ\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)  # ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹Xã‚’æ–°ã—ã„ç‰¹å¾´ã«å¤‰æ›\n",
    "X_test_new = selector.transform(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚æ–°ã—ã„ç‰¹å¾´ã‚’é©ç”¨\n",
    "\n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# çµæœã‚’æ ¼ç´ã™ã‚‹DataFrame\n",
    "results = []\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©å‡¦ç†\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®é…åˆ—ã‚’åˆæœŸåŒ–\n",
    "    cv_scores = []  # CVã‚¹ã‚³ã‚¢ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)  # æŠ•ç¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,  # æ—©æœŸåœæ­¢\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)  # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’è¡Œã†\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡äºˆæ¸¬\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # ãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—\n",
    "            cv_scores.append(val_log_loss)  # ã‚¹ã‚³ã‚¢ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’é›†ç©\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)  # å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})  # çµæœã‚’æ ¼ç´\n",
    "        print(f\"å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹: {mean_cv_log_loss:.5f}\")  # çµæœã‚’è¡¨ç¤º\n",
    "\n",
    "# ç‰¹å¾´é‡è¦åº¦ã‚’æ ¼ç´ï¼ˆé©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ï¼‰\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()  # é¸æŠã•ã‚ŒãŸç‰¹å¾´\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})  # ç‰¹å¾´é‡è¦åº¦ã®DataFrameã‚’ä½œæˆ\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0  # åˆæœŸåŒ–\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()  # é‡ã¿ã‚’é›†è¨ˆ\n",
    "\n",
    "    results_df = pd.DataFrame(results)  # çµæœã®DataFrameã‚’ä½œæˆ\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)  # ç‰¹å¾´é‡è¦åº¦ã‚’çµåˆ\n",
    "\n",
    "# çµæœã‚’DataFrameã«å¤‰æ›\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]  # æœ€å°ã®CVãƒ­ã‚°ãƒ­ã‚¹ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š\n",
    "print(f\"\\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«:\\n{best_model}\")\n",
    "\n",
    "# çµæœã®DataFrameã‚’è¡¨ç¤º\n",
    "print(\"\\nçµæœã®DataFrame:\")\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:27:19.154308Z",
     "iopub.status.busy": "2024-07-11T07:27:19.153925Z",
     "iopub.status.idle": "2024-07-11T07:53:41.875009Z",
     "shell.execute_reply": "2024-07-11T07:53:41.873898Z",
     "shell.execute_reply.started": "2024-07-11T07:27:19.154277Z"
    }
   },
   "outputs": [],
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«ã¨ãã®è¨­å®šã‚’å®šç¾©\n",
    "models = {\n",
    "    'random_forest': {\n",
    "        'model': RandomForestClassifier(\n",
    "            n_estimators=100,\n",
    "            max_depth=10,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'gradient_boosting': {\n",
    "        'model': GradientBoostingClassifier(\n",
    "            n_estimators=300,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            subsample=0.8,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'svm': {\n",
    "        'model': SVC(\n",
    "            kernel='rbf',\n",
    "            C=1.0,\n",
    "            gamma='scale',\n",
    "            probability=True,\n",
    "            random_state=config.seed\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'model': xgb.XGBClassifier(\n",
    "            objective='multi:softprob',\n",
    "            num_class=3,\n",
    "            eval_metric='mlogloss',\n",
    "            subsample=0.8,\n",
    "            n_estimators=650,\n",
    "            learning_rate=0.045,\n",
    "            max_depth=5,\n",
    "            random_state=config.seed,\n",
    "#             tree_method='gpu_hist'  # ã‚°ãƒ©ãƒ•ã‚£ãƒƒã‚¯ãƒ‡ãƒã‚¤ã‚¹ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ã°GPUåŠ é€Ÿã‚’ä½¿ç”¨\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'catboost': {\n",
    "        'model': cb.CatBoostClassifier(\n",
    "            loss_function='MultiClass',\n",
    "            iterations=650,\n",
    "            learning_rate=0.045,\n",
    "            depth=5,\n",
    "            random_seed=config.seed,\n",
    "#             task_type=\"GPU\",  # GPUãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ã°ä½¿ç”¨\n",
    "            verbose=75\n",
    "        ),\n",
    "        'params': {}\n",
    "    },\n",
    "    'voting': {\n",
    "        'model': VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),\n",
    "                ('svc', SVC(probability=True)),\n",
    "                ('knn', KNeighborsClassifier(n_neighbors=5))\n",
    "            ],\n",
    "            voting='soft'\n",
    "        ),\n",
    "        'params': {}\n",
    "    }\n",
    "}\n",
    "\n",
    "# SelectKBestã‚’ä½¿ç”¨ã—ã¦ç‰¹å¾´ã‚’é¸æŠ\n",
    "selector = SelectKBest(f_classif, k=25)\n",
    "X_new = selector.fit_transform(X, y)  # ç‰¹å¾´é‡ãƒãƒˆãƒªãƒƒã‚¯ã‚¹Xã‚’æ–°ã—ã„ç‰¹å¾´ã«å¤‰æ›\n",
    "X_test_new = selector.transform(X_test)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ã‚‚æ–°ã—ã„ç‰¹å¾´ã‚’é©ç”¨\n",
    "\n",
    "# ã‚¯ãƒ­ã‚¹ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®è¨­å®š\n",
    "cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)\n",
    "\n",
    "# çµæœã‚’æ ¼ç´ã™ã‚‹DataFrame\n",
    "results = []\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã‚’åå¾©å‡¦ç†\n",
    "for model_name, model_data in models.items():\n",
    "    model = model_data['model']\n",
    "    print(f\"ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°: {model_name}\")\n",
    "\n",
    "    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®é…åˆ—ã‚’åˆæœŸåŒ–\n",
    "    cv_scores = []  # CVã‚¹ã‚³ã‚¢ã‚’æ ¼ç´ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "\n",
    "    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_train, y_train = X_new[train_idx], y[train_idx]  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«\n",
    "        X_val, y_val = X_new[val_idx], y[val_idx]  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã¨ãƒ©ãƒ™ãƒ«\n",
    "\n",
    "        if model_name == 'voting':\n",
    "            model.fit(X_train, y_train)  # æŠ•ç¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "        elif model_name == 'catboost':\n",
    "            model.fit(\n",
    "                X_train,\n",
    "                y_train,\n",
    "                eval_set=[(X_train, y_train), (X_val, y_val)],\n",
    "                early_stopping_rounds=75,  # æ—©æœŸåœæ­¢\n",
    "                verbose=75\n",
    "            )\n",
    "        else:\n",
    "            model.fit(X_train, y_train)  # ãã®ä»–ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "\n",
    "        # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã§äºˆæ¸¬ã‚’è¡Œã†\n",
    "        if model_name != 'voting':\n",
    "            val_preds = model.predict_proba(X_val)  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®ç¢ºç‡äºˆæ¸¬\n",
    "            val_log_loss = log_loss(y_val, val_preds, eps=\"auto\")  # ãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—\n",
    "            cv_scores.append(val_log_loss)  # ã‚¹ã‚³ã‚¢ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\n",
    "            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()  # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’é›†ç©\n",
    "\n",
    "    if model_name != 'voting':\n",
    "        mean_cv_log_loss = np.mean(cv_scores)  # å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹ã‚’è¨ˆç®—\n",
    "        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})  # çµæœã‚’æ ¼ç´\n",
    "        print(f\"å¹³å‡CVãƒ­ã‚°ãƒ­ã‚¹: {mean_cv_log_loss:.5f}\")  # çµæœã‚’è¡¨ç¤º\n",
    "\n",
    "# ç‰¹å¾´é‡è¦åº¦ã‚’æ ¼ç´ï¼ˆé©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã«å¯¾ã—ã¦ï¼‰\n",
    "if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:\n",
    "    features = X.columns[selector.get_support()].tolist()  # é¸æŠã•ã‚ŒãŸç‰¹å¾´\n",
    "    feat_imp_df = pd.DataFrame({\"feature\": features})  # ç‰¹å¾´é‡è¦åº¦ã®DataFrameã‚’ä½œæˆ\n",
    "    feat_imp_df[f\"{model_name}_avg_importance\"] = 0  # åˆæœŸåŒ–\n",
    "\n",
    "    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):\n",
    "        X_val, _ = X_new[val_idx], y[val_idx]  # æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿\n",
    "        feat_imp_df[f\"{model_name}_avg_importance\"] += model.feature_importances_ / cv.get_n_splits()  # é‡ã¿ã‚’é›†è¨ˆ\n",
    "\n",
    "    results_df = pd.DataFrame(results)  # çµæœã®DataFrameã‚’ä½œæˆ\n",
    "    results_df = pd.concat([results_df, feat_imp_df], axis=1)  # ç‰¹å¾´é‡è¦åº¦ã‚’çµåˆ\n",
    "\n",
    "# çµæœã‚’DataFrameã«å¤‰æ›\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# æœ€è‰¯ãƒ¢ãƒ‡ãƒ«ã®ç‰¹å®š\n",
    "best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]  # æœ€å°ã®CVãƒ­ã‚°ãƒ­ã‚¹ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®š\n",
    "print(f\"\\næœ€è‰¯ãƒ¢ãƒ‡ãƒ«:\\n{best_model}\")\n",
    "\n",
    "# çµæœã®DataFrameã‚’è¡¨ç¤º\n",
    "print(\"\\nçµæœã®DataFrame:\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e98e84",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]\n",
    "sample_submission.head()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]  # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã«ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’æ ¼ç´\n",
    "sample_submission.head()  # æå‡ºç”¨DataFrameã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T07:53:41.877465Z",
     "iopub.status.busy": "2024-07-11T07:53:41.876688Z",
     "iopub.status.idle": "2024-07-11T07:53:41.890065Z",
     "shell.execute_reply": "2024-07-11T07:53:41.888965Z",
     "shell.execute_reply.started": "2024-07-11T07:53:41.877426Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, t in enumerate(target_cols):\n",
    "    sample_submission[t] = test_preds[:, idx]  # å„ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã«ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’æ ¼ç´\n",
    "sample_submission.head()  # æå‡ºç”¨DataFrameã®æœ€åˆã®5è¡Œã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77080f48",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "sample_submission.to_csv(\"submission.csv\", index=False)  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVã¨ã—ã¦ä¿å­˜\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-11T08:36:46.168878Z",
     "iopub.status.busy": "2024-07-11T08:36:46.168386Z",
     "iopub.status.idle": "2024-07-11T08:36:46.17975Z",
     "shell.execute_reply": "2024-07-11T08:36:46.178638Z",
     "shell.execute_reply.started": "2024-07-11T08:36:46.168847Z"
    }
   },
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"submission.csv\", index=False)  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVã¨ã—ã¦ä¿å­˜"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30732,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
