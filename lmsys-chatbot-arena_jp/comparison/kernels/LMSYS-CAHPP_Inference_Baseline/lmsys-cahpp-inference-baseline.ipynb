{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc5b0ea",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arena コンペティションにおいて、ユーザーが選好する応答を予測するための機械学習モデルの構築に取り組んでいます。具体的には、巨大な言語モデル（LLM）であるQwen2を用いて、与えられたプロンプトに対する2つのモデルからの応答のどちらが好まれるかを予測します。\n",
    "\n",
    "### 主要な問題\n",
    "- ユーザーのプロンプトに対して、2つの匿名化された応答から、どちらがより好まれるかを明らかにするための予測モデルを構築します。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "1. **ライブラリのインストールとインポート**：\n",
    "   - `bitsandbytes`, `transformers`, `tokenizers`, `peft` などのライブラリを用いて、最新のLLMを利用可能にします。\n",
    "   - PyTorch、scikit-learn、NumPy、Pandasなど、データ処理およびモデル構築に必要なライブラリがインポートされています。\n",
    "\n",
    "2. **データの準備**：\n",
    "   - トレーニングデータとテストデータが CSV ファイルから読み込まれ、応答Aと応答Bの処理が行われます。\n",
    "   - 入力テキストは、ユーザーのプロンプトとそれに対するモデルの応答を包含するように整形されます。\n",
    "\n",
    "3. **トークナイゼーション**：\n",
    "   - `AutoTokenizer`を使用して、テキストデータをトークン化し、モデルの入力として適切な形式に変換しています。\n",
    "\n",
    "4. **モデルのロード**：\n",
    "   - Qwen2モデルが初期化され、4ビット量子化を利用して効率的にGPUに配置されています。これにより、計算リソースの使用が最適化されます。\n",
    "   - LoRa (Low-Rank Adaptation) によるパラメータ効率の良いファインチューニングの設定が行われています。\n",
    "\n",
    "5. **推論**：\n",
    "   - 推論には、2つのスレッドを用いてモデルの予測を効率的に行う関数が実装されています。各モデルによる出力が確率として計算され、最終的にそれぞれのモデルA、B、およびタイの結果が生成されます。\n",
    "\n",
    "6. **出力処理**：\n",
    "   - 生成された予測結果は、提出フォーマットに合う形で整形され、最終的には \"submission.csv\" として保存されます。\n",
    "\n",
    "このノートブック全体を通じて、複数のLLMを活用し、複雑なデータ処理や予測を実現するための工夫が凝らされています。基本的に、二つのモデルからの応答を比較し、ユーザーの選好を予測するために、最先端の機械学習技術が利用されている点が特徴です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d355c2e",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、提供されたJupyter Notebookの内容に関連して、機械学習・深層学習の初心者がつまずきそうな専門用語の解説を列挙します。\n",
    "\n",
    "1. **bitsandbytes**:\n",
    "   - 高効率なメモリ使用を実現するためのライブラリで、特に8ビットや4ビットの量子化を用いて大規模言語モデル（LLM）を効率的に動作させるのに役立ちます。このライブラリは、GPUメモリの使用を最適化するために設計されています。\n",
    "\n",
    "2. **PEFT (Parameter Efficient Fine-Tuning)**:\n",
    "   - モデル全体を訓練するのではなく、一部のパラメータ（たとえば、LoRAやAdapterなど）だけを訓練する手法で、少ないデータや計算資源で効果的にモデルの性能を向上させることができます。\n",
    "\n",
    "3. **LoRa (Low-Rank Adaptation)**:\n",
    "   - PEFTの一手法で、ニューラルネットワークの重みを低ランクで調整することで、効率的にファインチューニングを行うことができます。これにより、全体のパラメータの数を減らしながらも性能を保つことが可能です。\n",
    "\n",
    "4. **自動混合精度 (Automatic Mixed Precision, AMP)**:\n",
    "   - 実行中のモデルの計算精度を自動で調整し、計算を高速化しつつメモリ使用量を削減するための技術です。特に深層学習において、計算効率を向上させるために よく用いられます。\n",
    "\n",
    "5. **スレッド処理 (Threading)**:\n",
    "   - プログラムを同時に複数の流れで実行するための技術です。このノートブックでは、異なるGPUにモデルを分散してロードし、同時に推論を行うためにスレッドが用いられています。\n",
    "\n",
    "6. **量子化 (Quantization)**:\n",
    "   - モデルの数値を、実際に使用される数値（たとえば、浮動小数点数から整数）に変換するプロセスです。これにより、メモリ使用を削減し、計算を高速化することができます。特に、8ビットや4ビット量子化が一般的に用いられます。\n",
    "\n",
    "7. **ログ損失 (Log Loss)**:\n",
    "   - モデルの予測と実際の結果との間の誤差を評価する指標で、確率的にモデルがどれだけ的確に予測を行っているかを示します。特に2クラス分類や多クラス分類の問題において用いられることが多いです。\n",
    "\n",
    "8. **アテンションマスク (Attention Mask)**:\n",
    "   - Transformerモデルにおいて、どのトークンが計算に含まれるかを示すためのマスクです。特に、パディングされているトークンを無視するために使用されます。\n",
    "\n",
    "9. **デバイスマップ (Device Map)**:\n",
    "   - モデルがどのデバイス（CPU/GPU/TPUなど）で実行されるかを指定するためのマップで、複数のGPU環境での効率的な処理に役立ちます。\n",
    "\n",
    "これらの解説が、ノートブックを理解するための助けとなることを期待しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649d774a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# bitsandbytesライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "# transformersライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "# tokenizersライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "# peftライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:08:19.41193Z",
     "iopub.status.busy": "2024-07-04T02:08:19.41131Z",
     "iopub.status.idle": "2024-07-04T02:09:08.666863Z",
     "shell.execute_reply": "2024-07-04T02:09:08.665859Z",
     "shell.execute_reply.started": "2024-07-04T02:08:19.411898Z"
    }
   },
   "outputs": [],
   "source": [
    "# bitsandbytesライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "# transformersライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "# tokenizersライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "# peftライブラリを最新のバージョンにインストールします。\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfcaabe",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "The work in this notebook is inspired by these notebooks:\n",
    "* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n",
    "* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "このノートブックでの作業は、以下のノートブックからインスピレーションを受けています：\n",
    "* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf\n",
    "* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b\n",
    "\n",
    "## ライブラリのインポート\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca8c9c5",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Importing Libraries\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 必要なライブラリをインポートします。\n",
    "import torch  # PyTorchライブラリをインポート\n",
    "import sklearn  # scikit-learnライブラリをインポート\n",
    "import numpy as np  # NumPyライブラリをインポート\n",
    "import pandas as pd  # Pandasライブラリをインポート\n",
    "import time  # 時間操作のためのライブラリをインポート\n",
    "\n",
    "# Transformersライブラリから必要なモデルとトークナイザーをインポート\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, Qwen2ForSequenceClassification, BitsAndBytesConfig\n",
    "# PEFT（Parameter Efficient Fine-Tuning）のための設定をインポート\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast  # 自動混合精度のための機能をインポート\n",
    "from threading import Thread  # スレッド処理のためのライブラリをインポート\n",
    "\n",
    "# ガベージコレクションをインポート\n",
    "import gc\n",
    "import os  # OS関連の機能をインポート\n",
    "import io  # 入出力操作のためのライブラリをインポート\n",
    "import time  # 時間操作のためのライブラリをインポート（再度インポート、必要であれば削除可能）\n",
    "import json  # JSONデータを扱うためのライブラリをインポート\n",
    "import random  # ランダム操作のためのライブラリをインポート\n",
    "import pickle  # Pythonオブジェクトをバイナリ形式で保存・読み込みするためのライブラリをインポート\n",
    "import zipfile  # ZIPファイル操作のためのライブラリをインポート\n",
    "import datetime  # 日付と時間を操作するためのライブラリをインポート\n",
    "import matplotlib.pyplot as plt  # グラフ描画のためのライブラリをインポート\n",
    "from IPython.display import display  # IPythonの表示機能をインポート\n",
    "from collections import Counter  # 要素のカウントを行うためのCounterクラスをインポート\n",
    "from collections import defaultdict  # デフォルト辞書を扱うためのdefaultdictクラスをインポート\n",
    "import torch  # 再度、PyTorchライブラリをインポート\n",
    "from torch import nn  # ニューラルネットワークの基盤を構築するためのライブラリをインポート\n",
    "import torch.nn.functional as F  # ニューラルネットワークの機能的操作を簡単にするためのライブラリをインポート\n",
    "import pytorch_lightning as pl  # PyTorch Lightningライブラリをインポート\n",
    "from torch.utils.data import Dataset, DataLoader  # データセットとデータローダーのためのクラスをインポート\n",
    "from sklearn.metrics import log_loss  # 対数損失を計算するための関数をインポート\n",
    "import tokenizers  # トークナイザーを扱うためのライブラリをインポート\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b2b15",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification,Qwen2ForSequenceClassification, BitsAndBytesConfig\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from torch.cuda.amp import autocast\n",
    "from threading import Thread\n",
    "\n",
    "import gc\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "import pickle\n",
    "import zipfile\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import log_loss\n",
    "import tokenizers\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# メモリ効率の良いスパース分解（SDP）を無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# フラッシュSDPを無効にします。\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# GPUが利用できない場合はメッセージを表示します。\n",
    "if (not torch.cuda.is_available()): \n",
    "    print(\"申し訳ありませんが、GPUが必要です！\")\n",
    "\n",
    "# モデル名のパスを定義します。\n",
    "MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'\n",
    "# 重みのパスを定義します。\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "# 最大入力長を定義します。\n",
    "MAX_LENGTH = 1284\n",
    "# バッチサイズを定義します。\n",
    "BATCH_SIZE = 8\n",
    "# 使用するデバイスをCUDAに設定します（GPU）。\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:09:08.681276Z",
     "iopub.status.busy": "2024-07-04T02:09:08.680953Z",
     "iopub.status.idle": "2024-07-04T02:09:08.690924Z",
     "shell.execute_reply": "2024-07-04T02:09:08.690021Z",
     "shell.execute_reply.started": "2024-07-04T02:09:08.681252Z"
    }
   },
   "outputs": [],
   "source": [
    "# メモリ効率の良いスパース分解（SDP）を無効にします。\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "# フラッシュSDPを無効にします。\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "# GPUが利用できない場合はメッセージを表示します。\n",
    "if (not torch.cuda.is_available()): \n",
    "    print(\"申し訳ありませんが、GPUが必要です！\")\n",
    "\n",
    "# モデル名のパスを定義します。\n",
    "MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'\n",
    "# 重みのパスを定義します。\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "# 最大入力長を定義します。\n",
    "MAX_LENGTH = 1284\n",
    "# バッチサイズを定義します。\n",
    "BATCH_SIZE = 8\n",
    "# 使用するデバイスをCUDAに設定します（GPU）。\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edbaef1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "\n",
    "if (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n",
    "\n",
    "MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'\n",
    "WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'\n",
    "MAX_LENGTH = 1284\n",
    "BATCH_SIZE = 8\n",
    "DEVICE = torch.device(\"cuda\")    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "## データの準備\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14ba7eb",
   "metadata": {},
   "source": [
    "## データの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb73769b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Prepare Data \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# テストデータを読み込みます。\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "# 訓練データを読み込みます。ファイルを読み取りモードで開きます。\n",
    "train = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))\n",
    "# サンプルの提出ファイルを読み込みます。\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f084d8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "train = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))\n",
    "sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 文字列のリストを連結する関数を定義します。\n",
    "def process(input_str):\n",
    "    # 入力文字列の両端の'[]'を取り除きます。\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # 文字列を分割して、各文から余分な引用符を取り除きます。\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # 文をスペースで結合して返します。\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "# テストデータに対してプロンプト、応答A、応答Bを処理します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# サンプルの提出データとテストデータの先頭5行を表示します。\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# モデル用にテキストを準備します。\n",
    "test['text'] = 'ユーザープロンプト: ' + test['prompt'] +  '\\n\\nモデルA :\\n' + test['response_a'] +'\\n\\n--------\\n\\nモデルB:\\n'  + test['response_b']\n",
    "# 処理されたテキストの最初の要素を表示します。\n",
    "print(test['text'][0])\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:09:10.576123Z",
     "iopub.status.busy": "2024-07-04T02:09:10.575733Z",
     "iopub.status.idle": "2024-07-04T02:09:10.599148Z",
     "shell.execute_reply": "2024-07-04T02:09:10.598191Z",
     "shell.execute_reply.started": "2024-07-04T02:09:10.576089Z"
    }
   },
   "outputs": [],
   "source": [
    "# 文字列のリストを連結する関数を定義します。\n",
    "def process(input_str):\n",
    "    # 入力文字列の両端の'[]'を取り除きます。\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    # 文字列を分割して、各文から余分な引用符を取り除きます。\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    # 文をスペースで結合して返します。\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "# テストデータに対してプロンプト、応答A、応答Bを処理します。\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "# サンプルの提出データとテストデータの先頭5行を表示します。\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# モデル用にテキストを準備します。\n",
    "test['text'] = 'ユーザープロンプト: ' + test['prompt'] +  '\\n\\nモデルA :\\n' + test['response_a'] +'\\n\\n--------\\n\\nモデルB:\\n'  + test['response_b']\n",
    "# 処理されたテキストの最初の要素を表示します。\n",
    "print(test['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2413f83",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(sample_sub)\n",
    "display(test.head(5))\n",
    "\n",
    "# Prepare text for model\n",
    "test['text'] = 'User prompt: ' + test['prompt'] +  '\\n\\nModel A :\\n' + test['response_a'] +'\\n\\n--------\\n\\nModel B:\\n'  + test['response_b']\n",
    "print(test['text'][0])\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "## トークナイズ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a664c",
   "metadata": {},
   "source": [
    "## トークナイズ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500b8b8a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Tokenize\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トークナイザーを初期化します。\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "# テキストデータをトークン化します。\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "# トークン化された入力IDとアテンションマスクをデバイス（GPU）に移動させます。\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# テンソルをCPUに移動し、リストに変換します。\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "# 新しいデータフレームを作成し、INPUT_IDSとATTENTION_MASKSを格納します。\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "# データフレームの最初の2行を表示します。\n",
    "data[:2]\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0252d45",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "tokens = tokenizer(test['text'].tolist(), padding='max_length',\n",
    "                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')\n",
    "\n",
    "INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)\n",
    "ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)\n",
    "\n",
    "# Move tensors to CPU and convert them to lists\n",
    "input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]\n",
    "attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data['INPUT_IDS'] = input_ids_cpu\n",
    "data['ATTENTION_MASKS'] = attention_masks_cpu\n",
    "data[:2]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "## モデルのロード\n",
    "> 各GPUに1つのモデルをロードします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a9b2fe",
   "metadata": {},
   "source": [
    "## モデルのロード\n",
    "> 各GPUに1つのモデルをロードします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95015ec5",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Load model \n",
    "> We load 1 model on each gpu.  \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# BitsAndBytesの設定を行います。\n",
    "# 8ビットでの読み込みを設定する場合（コメントアウトされています）\n",
    "# bnb_config =  BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     bnb_8bit_compute_dtype=torch.float16,\n",
    "#     bnb_8bit_use_double_quant=False)\n",
    "\n",
    "# 4ビットでの読み込みを設定します。\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 4ビットで読み込み\n",
    "    bnb_4bit_quant_type=\"nf4\",  # 量子化のタイプを\"nf4\"に設定\n",
    "    bnb_4bit_use_double_quant=True,  # 二重量子化を使用\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,  # 計算のデータ型をbfloat16に設定\n",
    ")\n",
    "\n",
    "# GPU 0に基礎モデルをロードします。\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,  # モデル名を指定\n",
    "    num_labels=3,  # ラベルの数を指定\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を指定\n",
    "    device_map='cuda:0'  # モデルをGPU 0に配置\n",
    ")\n",
    "\n",
    "# パディングトークンIDをトークナイザーから設定します。\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce564ec",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# BitsAndBytes configuration\n",
    "# bnb_config =  BitsAndBytesConfig(\n",
    "#     load_in_8bit=True,\n",
    "#     bnb_8bit_compute_dtype=torch.float16,\n",
    "#     bnb_8bit_use_double_quant=False)\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "\n",
    "base_model_0 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# GPU 1に基礎モデルをロードします。\n",
    "device1 = torch.device('cuda:1')\n",
    "\n",
    "base_model_1 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,  # モデル名を指定\n",
    "    num_labels=3,  # ラベルの数を指定\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を指定\n",
    "    device_map='cuda:1'  # モデルをGPU 1に配置\n",
    ")\n",
    "\n",
    "# パディングトークンIDをトークナイザーから設定します。\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:10:42.859611Z",
     "iopub.status.busy": "2024-07-04T02:10:42.859332Z",
     "iopub.status.idle": "2024-07-04T02:12:16.527811Z",
     "shell.execute_reply": "2024-07-04T02:12:16.527017Z",
     "shell.execute_reply.started": "2024-07-04T02:10:42.859586Z"
    }
   },
   "outputs": [],
   "source": [
    "# GPU 1に基礎モデルをロードします。\n",
    "device1 = torch.device('cuda:1')\n",
    "\n",
    "base_model_1 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,  # モデル名を指定\n",
    "    num_labels=3,  # ラベルの数を指定\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を指定\n",
    "    device_map='cuda:1'  # モデルをGPU 1に配置\n",
    ")\n",
    "\n",
    "# パディングトークンIDをトークナイザーから設定します。\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862e5ece",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = Qwen2ForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "## 重みのロード\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7388d1db",
   "metadata": {},
   "source": [
    "## 重みのロード"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0073b112",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Load weights \n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LoRa（Low-Rank Adaptation）の設定を行います。\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # 決定係数レイテンシを指定\n",
    "    lora_alpha=32,  # LoRaでのスケーリングファクターを指定\n",
    "    lora_dropout=0.10,  # ドロップアウト率を指定\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクのタイプをシーケンス分類に設定\n",
    "    target_modules=['o_proj', 'v_proj']  # ターゲットモジュールを指定\n",
    ")\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e354a5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# LoRa configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj'])\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# PEFTモデルを取得し、GPU 0に配置します。\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "\n",
    "# 重みをロードします（コメントアウトされています）\n",
    "# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "# モデルを評価モードに設定します。\n",
    "model_0.eval()\n",
    "\n",
    "# PEFTモデルを取得し、GPU 1に配置します。\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "\n",
    "# 重みをロードします（コメントアウトされています）\n",
    "# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "# モデルを評価モードに設定します。\n",
    "model_1.eval()\n",
    "\n",
    "# 学習可能なパラメータを表示します。\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:16.53639Z",
     "iopub.status.busy": "2024-07-04T02:12:16.536064Z",
     "iopub.status.idle": "2024-07-04T02:12:24.092142Z",
     "shell.execute_reply": "2024-07-04T02:12:24.091243Z",
     "shell.execute_reply.started": "2024-07-04T02:12:16.536352Z"
    }
   },
   "outputs": [],
   "source": [
    "# PEFTモデルを取得し、GPU 0に配置します。\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "\n",
    "# 重みをロードします（コメントアウトされています）\n",
    "# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "# モデルを評価モードに設定します。\n",
    "model_0.eval()\n",
    "\n",
    "# PEFTモデルを取得し、GPU 1に配置します。\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "\n",
    "# 重みをロードします（コメントアウトされています）\n",
    "# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "# モデルを評価モードに設定します。\n",
    "model_1.eval()\n",
    "\n",
    "# 学習可能なパラメータを表示します。\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d85d86",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device0) \n",
    "#Load weights\n",
    "# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device1)\n",
    "# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)\n",
    "model_1.eval()\n",
    "\n",
    "#Trainable Parameters\n",
    "model_0.print_trainable_parameters(), model_1.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# ガベージコレクションを実行して、不要なメモリを解放します。\n",
    "gc.collect()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:24.093939Z",
     "iopub.status.busy": "2024-07-04T02:12:24.093493Z",
     "iopub.status.idle": "2024-07-04T02:12:24.402154Z",
     "shell.execute_reply": "2024-07-04T02:12:24.401243Z",
     "shell.execute_reply.started": "2024-07-04T02:12:24.093906Z"
    }
   },
   "outputs": [],
   "source": [
    "# ガベージコレクションを実行して、不要なメモリを解放します。\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde0c4b5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "gc.collect()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "## 推論\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e6074",
   "metadata": {},
   "source": [
    "## 推論"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10135950",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Inference\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 推論を行う関数を定義します。\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    # DataFrameから入力IDとアテンションマスクを張り込むテンソルを作成します。\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    # 各クラスの生成された確率を格納するリストを初期化します。\n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()  # モデルを評価モードに設定します。\n",
    "    \n",
    "    # バッチサイズごとにデータを処理します。\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算します。\n",
    "        \n",
    "        # バッチの入力IDとアテンションマスクを取得します。\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():  # 勾配計算を無効にします。\n",
    "            with autocast():  # 自動混合精度を使用します。\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,  # モデルに入力IDを渡します。\n",
    "                    attention_mask=batch_attention_mask  # 注意マスクを渡します。\n",
    "                )\n",
    "        \n",
    "        # 出力のロジットをソフトマックス関数で確率に変換します。\n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        # 各クラスの確率をリストに追加します。\n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    # DataFrameに生成されたクラスの確率を追加します。\n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    # GPUのキャッシュをクリアします。\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df  # 処理されたDataFrameを返します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808238df",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE):\n",
    "    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)\n",
    "    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)\n",
    "    \n",
    "    generated_class_a = []\n",
    "    generated_class_b = []\n",
    "    generated_class_c = []\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        batch_input_ids = input_ids[start_idx:end_idx].to(device)\n",
    "        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            with autocast():\n",
    "                outputs = model(\n",
    "                    input_ids=batch_input_ids,\n",
    "                    attention_mask=batch_attention_mask\n",
    "                )\n",
    "        \n",
    "        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        \n",
    "        generated_class_a.extend(probabilities[:, 0])\n",
    "        generated_class_b.extend(probabilities[:, 1])\n",
    "        generated_class_c.extend(probabilities[:, 2])\n",
    "    \n",
    "    df['winner_model_a'] = generated_class_a\n",
    "    df['winner_model_b'] = generated_class_b\n",
    "    df['winner_tie'] = generated_class_c\n",
    "\n",
    "    torch.cuda.empty_cache()  \n",
    "\n",
    "    return df\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 処理時間の計測を開始します。\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)  # データサンプルの総数を取得します。\n",
    "\n",
    "# データを2つのサブセットに分割します。\n",
    "half = round(N_SAMPLES / 2)  # サンプル数の半分を計算します。\n",
    "sub1 = data.iloc[0:half].copy()  # 最初の半分のデータを取得します。\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りの半分のデータを取得します。\n",
    "\n",
    "# スレッド内で推論を実行するための関数を定義します。\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 推論結果を辞書に格納します。\n",
    "\n",
    "# スレッドからの結果を保存するための辞書を初期化します。\n",
    "results = {}\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:24.416736Z",
     "iopub.status.busy": "2024-07-04T02:12:24.416431Z",
     "iopub.status.idle": "2024-07-04T02:12:24.431371Z",
     "shell.execute_reply": "2024-07-04T02:12:24.430582Z",
     "shell.execute_reply.started": "2024-07-04T02:12:24.416711Z"
    }
   },
   "outputs": [],
   "source": [
    "# 処理時間の計測を開始します。\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)  # データサンプルの総数を取得します。\n",
    "\n",
    "# データを2つのサブセットに分割します。\n",
    "half = round(N_SAMPLES / 2)  # サンプル数の半分を計算します。\n",
    "sub1 = data.iloc[0:half].copy()  # 最初の半分のデータを取得します。\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りの半分のデータを取得します。\n",
    "\n",
    "# スレッド内で推論を実行するための関数を定義します。\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)  # 推論結果を辞書に格納します。\n",
    "\n",
    "# スレッドからの結果を保存するための辞書を初期化します。\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc3e95",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "N_SAMPLES = len(data)\n",
    "\n",
    "# Split the data into two subsets\n",
    "half = round(N_SAMPLES / 2)\n",
    "sub1 = data.iloc[0:half].copy()\n",
    "sub2 = data.iloc[half:N_SAMPLES].copy()\n",
    "\n",
    "# Function to run inference in a thread\n",
    "def run_inference(df, model, device, results, index):\n",
    "    results[index] = inference(df, model, device)\n",
    "\n",
    "# Dictionary to store results from threads\n",
    "results = {}\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# スレッドを開始します。\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッドt0を作成\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッドt1を作成\n",
    "\n",
    "t0.start()  # スレッドt0を開始\n",
    "t1.start()  # スレッドt1を開始\n",
    "\n",
    "# すべてのスレッドの終了を待ちます。\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のDataFrameに結合します。\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "# 処理が完了したことを表示します。\n",
    "print(f\"処理が完了しました。合計時間: {time.time() - st}\")\n",
    "\n",
    "# ターゲット列を定義します。\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "# サンプルの提出データフレームに結果を追加します。\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:24.435578Z",
     "iopub.status.busy": "2024-07-04T02:12:24.435211Z",
     "iopub.status.idle": "2024-07-04T02:12:27.427665Z",
     "shell.execute_reply": "2024-07-04T02:12:27.426669Z",
     "shell.execute_reply.started": "2024-07-04T02:12:24.435554Z"
    }
   },
   "outputs": [],
   "source": [
    "# スレッドを開始します。\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッドt0を作成\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッドt1を作成\n",
    "\n",
    "t0.start()  # スレッドt0を開始\n",
    "t1.start()  # スレッドt1を開始\n",
    "\n",
    "# すべてのスレッドの終了を待ちます。\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# 結果を元のDataFrameに結合します。\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "# 処理が完了したことを表示します。\n",
    "print(f\"処理が完了しました。合計時間: {time.time() - st}\")\n",
    "\n",
    "# ターゲット列を定義します。\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "# サンプルの提出データフレームに結果を追加します。\n",
    "sample_sub[TARGETS] = data[TARGETS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bf2ea6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# start threads\n",
    "t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))\n",
    "t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))\n",
    "\n",
    "t0.start()\n",
    "t1.start()\n",
    "\n",
    "# Wait for all threads to finish\n",
    "t0.join()\n",
    "t1.join()\n",
    "\n",
    "# Combine results back into the original DataFrame\n",
    "data = pd.concat([results[0], results[1]], axis=0)\n",
    "\n",
    "print(f\"Processing complete. Total time: {time.time() - st}\")\n",
    "\n",
    "TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "sample_sub[TARGETS] = data[TARGETS]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# ターゲット列の値を取得し、予測結果を変数に格納します。\n",
    "llama_preds = data[TARGETS].values\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:27.432543Z",
     "iopub.status.busy": "2024-07-04T02:12:27.432162Z",
     "iopub.status.idle": "2024-07-04T02:12:27.438717Z",
     "shell.execute_reply": "2024-07-04T02:12:27.437781Z",
     "shell.execute_reply.started": "2024-07-04T02:12:27.432504Z"
    }
   },
   "outputs": [],
   "source": [
    "# ターゲット列の値を取得し、予測結果を変数に格納します。\n",
    "llama_preds = data[TARGETS].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dd2b2b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "llama_preds = data[TARGETS].values\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 予測結果をDataFrameに変換し、元のテストデータのIDをインデックスとして設定します。\n",
    "out = pd.DataFrame(llama_preds, \n",
    "                index=test.id,  # テストデータのIDをインデックスに設定\n",
    "                columns=train.columns[-3:])  # 直近の3つの列をカラムとして設定\n",
    "# 結果の先頭5行を表示します。\n",
    "display(out.head())\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:27.440117Z",
     "iopub.status.busy": "2024-07-04T02:12:27.43983Z",
     "iopub.status.idle": "2024-07-04T02:12:27.453037Z",
     "shell.execute_reply": "2024-07-04T02:12:27.452123Z",
     "shell.execute_reply.started": "2024-07-04T02:12:27.440094Z"
    }
   },
   "outputs": [],
   "source": [
    "# 予測結果をDataFrameに変換し、元のテストデータのIDをインデックスとして設定します。\n",
    "out = pd.DataFrame(llama_preds, \n",
    "                index=test.id,  # テストデータのIDをインデックスに設定\n",
    "                columns=train.columns[-3:])  # 直近の3つの列をカラムとして設定\n",
    "# 結果の先頭5行を表示します。\n",
    "display(out.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47d7811",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "out = pd.DataFrame(llama_preds, \n",
    "                index = test.id, \n",
    "                    columns = train.columns[-3:])\n",
    "display(out.head())\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 予測結果をCSVファイルとして保存します。\n",
    "out.to_csv('submission.csv')\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-04T02:12:27.454408Z",
     "iopub.status.busy": "2024-07-04T02:12:27.454149Z",
     "iopub.status.idle": "2024-07-04T02:12:27.463274Z",
     "shell.execute_reply": "2024-07-04T02:12:27.462504Z",
     "shell.execute_reply.started": "2024-07-04T02:12:27.454386Z"
    }
   },
   "outputs": [],
   "source": [
    "# 予測結果をCSVファイルとして保存します。\n",
    "out.to_csv('submission.csv')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4946449,
     "sourceId": 8330401,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 51944,
     "sourceId": 62188,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelInstanceId": 52038,
     "sourceId": 62308,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
