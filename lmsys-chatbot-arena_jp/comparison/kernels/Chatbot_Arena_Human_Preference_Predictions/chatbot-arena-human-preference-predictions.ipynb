{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46b80f81",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena Human Preference Predictions」コンペティションにおいて、ユーザーが好むチャットボットの応答を予測するための機械学習モデルの構築に取り組んでいます。具体的には、与えられたプロンプトに対して生成された複数の応答の中から、どの応答が好まれるかを予測するモデルを構築しています。\n",
    "\n",
    "### 使用ライブラリと手法\n",
    "- **PandasおよびNumPy**: データ処理や数値計算に使用。\n",
    "- **Scikit-learn**: 特徴量のベクトル化およびモデルの学習データの分割に用いる。特に`LabelEncoder`と`HashingVectorizer`を使用しています。\n",
    "- **XGBoost**: モデルの構築にはXGBoost（eXtreme Gradient Boosting）を使用し、特にマルチクラスの確率を予測するための設定が行われています。\n",
    "- **SciPy**: スパース行列の操作に使用。\n",
    "\n",
    "### 主な手順\n",
    "1. **データの読み込み**: トレーニングデータをCSVファイルから読み込む。\n",
    "2. **特徴量ベクトルの作成**: プロンプトや応答をHashingVectorizerを使用してベクトル化し、スパース行列として結合。モデル識別子はLabelEncoderを使用してエンコーディングされます。\n",
    "3. **データの分割**: 学習データセットとテストデータセットに分割。\n",
    "4. **XGBoostモデルの訓練**: 複数クラスのソフトマックス出力を形成する目的でXGBoostモデルを訓練。\n",
    "5. **予測と評価**: テストデータセットに対して予測を行い、その精度を計算して表示。\n",
    "\n",
    "最後に、テストデータに対して得られた予測確率を含むDataFrameを作成し、指定された形式でCSVファイルとして保存しています。これにより、コンペティションの提出要件に従った出力が得られています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6fb3ca",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容に関連する初心者がつまずきそうな専門用語の簡単な解説を示します。\n",
    "\n",
    "1. **HashingVectorizer**:\n",
    "   - テキストデータを数値ベクトルに変換するツール。単語を固定長のベクトルにマッピングするため、情報を圧縮することができる。ハッシュ関数を使用するため、メモリの消費が少ないが、情報の衝突（異なる単語が同じベクトルにマッピングされること）が発生することがある。\n",
    "\n",
    "2. **LabelEncoder**:\n",
    "   - カテゴリカルデータを数値に変換するためのツール。特に機械学習モデルに入力するために、文字列ラベルを整数にエンコードする際に使用される。例えば、「モデルA」を0、「モデルB」を1というように変換する。\n",
    "\n",
    "3. **hstack**:\n",
    "   - NumPyやSciPyの関数で、複数の配列や行列を水平方向に結合するために使用される。例えば、異なる特徴量を保持する複数の行列を一つの行列に結合する際に使う。\n",
    "\n",
    "4. **csr_matrix**:\n",
    "   - Compressed Sparse Row形式の行列を表すSciPyのデータ構造。スパース行列（ほとんどの要素がゼロの行列）を効率的に保存し、計算を高速化するために使用される。この形式は特に大規模データの処理に有効。\n",
    "\n",
    "5. **DMatrix**:\n",
    "   - XGBoostで使用されるデータ構造。データとラベルを効率的に格納し、モデルの学習や予測に使用される。特に大規模データセット向けに最適化されている。\n",
    "\n",
    "6. **multi:softprob**:\n",
    "   - XGBoostの目的関数の一つで、複数のクラスに対する確率を予測するために使用される。各クラスに対して0と1の間の確率を出力し、どのクラスが最も可能性が高いかを判断する。\n",
    "\n",
    "7. **mlogloss**:\n",
    "   - 多クラスのロジスティック回帰における損失関数。「対数損失」とも呼ばれ、予測確率が正解とどれだけ乖離しているかを測る指標。値が小さいほどモデルの性能が良いとされる。\n",
    "\n",
    "8. **num_boost_round**:\n",
    "   - XGBoostモデルを学習する際のブーストラウンドの数を指定するパラメータ。ブースティングは弱いモデルを多数組み合わせて堅牢なモデルを作る技法で、ラウンド数が多いほど学習が深くなるが、過学習のリスクもある。\n",
    "\n",
    "9. **device**:\n",
    "   - モデルを学習させる際に使用されるデバイスを指定するパラメータ。`'cuda'`はNVIDIAのGPUを使用することを意味し、大規模な計算を高速に行うことが可能。\n",
    "\n",
    "10. **pred_probs**:\n",
    "    - モデルが出力する予測確率の配列。クラスごとに確率を持ち、これを基に最終的な分類結果を取得する。確率が高い方のクラスが選ばれる。\n",
    "\n",
    "これらの用語は、特に初心者が機械学習や深層学習を学習する際に理解に苦しむことが多い内容です。理解を深めることで、よりスムーズにコンペティションに参加できるようになると思います。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07a57e9",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# このPython 3環境には、多くの役立つ分析ライブラリがインストールされています\n",
    "# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python\n",
    "# 例えば、こちらにはいくつかの便利なパッケージをロードする方法が示されています\n",
    "\n",
    "import numpy as np # 線形代数のためのライブラリ\n",
    "import pandas as pd # データ処理、CSVファイルの入出力 (例: pd.read_csv)\n",
    "\n",
    "# 入力データファイルは読み取り専用の\"../input/\"ディレクトリにあります\n",
    "# 例えば、これを実行すると (クリックするか、Shift + Enterを押して) 入力ディレクトリ内のすべてのファイルがリストされます\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'): # ディレクトリを探索する\n",
    "    for filename in filenames: # 発見されたファイルについて\n",
    "        print(os.path.join(dirname, filename)) # ファイルのパスを表示する\n",
    "\n",
    "# 現在のディレクトリ (/kaggle/working/) に最大20GBのデータを書き込むことができ、これは「すべて保存して実行」を使用してバージョンを作成するときに出力として保持されます\n",
    "# 一時ファイルを/kaggle/temp/に書き込むこともできますが、それらは現在のセッションの外には保存されません\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-06T03:14:50.573897Z",
     "iopub.status.busy": "2024-07-06T03:14:50.573076Z",
     "iopub.status.idle": "2024-07-06T03:14:51.815862Z",
     "shell.execute_reply": "2024-07-06T03:14:51.81499Z",
     "shell.execute_reply.started": "2024-07-06T03:14:50.573865Z"
    }
   },
   "outputs": [],
   "source": [
    "# このPython 3環境には、多くの役立つ分析ライブラリがインストールされています\n",
    "# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python\n",
    "# 例えば、こちらにはいくつかの便利なパッケージをロードする方法が示されています\n",
    "\n",
    "import numpy as np # 線形代数のためのライブラリ\n",
    "import pandas as pd # データ処理、CSVファイルの入出力 (例: pd.read_csv)\n",
    "\n",
    "# 入力データファイルは読み取り専用の\"../input/\"ディレクトリにあります\n",
    "# 例えば、これを実行すると (クリックするか、Shift + Enterを押して) 入力ディレクトリ内のすべてのファイルがリストされます\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'): # ディレクトリを探索する\n",
    "    for filename in filenames: # 発見されたファイルについて\n",
    "        print(os.path.join(dirname, filename)) # ファイルのパスを表示する\n",
    "\n",
    "# 現在のディレクトリ (/kaggle/working/) に最大20GBのデータを書き込むことができ、これは「すべて保存して実行」を使用してバージョンを作成するときに出力として保持されます\n",
    "# 一時ファイルを/kaggle/temp/に書き込むこともできますが、それらは現在のセッションの外には保存されません"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba45b3b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# Load data\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "\n",
    "# Initialize vectorizers and label encoders\n",
    "n_features = 2**10\n",
    "vectorizers = {\n",
    "    'prompt': HashingVectorizer(n_features=n_features),\n",
    "    'response_a': HashingVectorizer(n_features=n_features),\n",
    "    'response_b': HashingVectorizer(n_features=n_features)\n",
    "}\n",
    "model_encoder = LabelEncoder()\n",
    "\n",
    "# Encode model identifiers\n",
    "train['model_a_encoded'] = model_encoder.fit_transform(train['model_a'])\n",
    "train['model_b_encoded'] = model_encoder.transform(train['model_b'])\n",
    "\n",
    "# Process text data into vectors\n",
    "def process_and_concat_features(data, vectorizers):\n",
    "    features_list = []\n",
    "    for column, vectorizer in vectorizers.items():\n",
    "        print(f\"Vectorizing '{column}'...\")\n",
    "        transformed_data = vectorizer.transform(data[column])\n",
    "        features_list.append(transformed_data)\n",
    "    final_features = hstack(features_list)  # Keep as sparse matrix\n",
    "    return final_features\n",
    "\n",
    "train_features = process_and_concat_features(train, vectorizers)\n",
    "\n",
    "# Combine model identifiers with text features\n",
    "model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']])\n",
    "X_combined = hstack([model_features, train_features])\n",
    "\n",
    "# Encode target variable\n",
    "train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(train['winner'])\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set up parameters\n",
    "params = {\n",
    "    'objective': 'multi:softprob',\n",
    "    'num_class': len(np.unique(y_encoded)),\n",
    "    'eval_metric': 'mlogloss',\n",
    "    'tree_method':'hist',\n",
    "    'device':'cuda'\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_boost_round = 100\n",
    "bst = xgb.train(params, dtrain, num_boost_round)\n",
    "\n",
    "# Predict the probabilities\n",
    "pred_probs = bst.predict(dtest)\n",
    "predictions = np.argmax(pred_probs, axis=1)\n",
    "\n",
    "# Evaluate the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(y_test, predictions)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# データを読み込む\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "\n",
    "# ベクトライザーとラベルエンコーダの初期化\n",
    "n_features = 2**10 # ベクトルの次元数を設定\n",
    "vectorizers = {\n",
    "    'prompt': HashingVectorizer(n_features=n_features), # プロンプト用のベクトライザー\n",
    "    'response_a': HashingVectorizer(n_features=n_features), # 応答A用のベクトライザー\n",
    "    'response_b': HashingVectorizer(n_features=n_features) # 応答B用のベクトライザー\n",
    "}\n",
    "model_encoder = LabelEncoder() # モデル識別子用のラベルエンコーダを初期化\n",
    "\n",
    "# モデル識別子をエンコードする\n",
    "train['model_a_encoded'] = model_encoder.fit_transform(train['model_a']) # モデルAのエンコード\n",
    "train['model_b_encoded'] = model_encoder.transform(train['model_b']) # モデルBのエンコード\n",
    "\n",
    "# テキストデータをベクトルに変換して結合する関数\n",
    "def process_and_concat_features(data, vectorizers):\n",
    "    features_list = [] # 特徴量のリストを初期化\n",
    "    for column, vectorizer in vectorizers.items():\n",
    "        print(f\"Vectorizing '{column}'...\") # ベクトライジングの進捗を表示\n",
    "        transformed_data = vectorizer.transform(data[column]) # ベクトルに変換\n",
    "        features_list.append(transformed_data) # リストに追加\n",
    "    final_features = hstack(features_list)  # スパース行列として保持\n",
    "    return final_features\n",
    "\n",
    "train_features = process_and_concat_features(train, vectorizers) # 特徴量を処理して結合\n",
    "\n",
    "# モデル識別子とテキスト特徴を組み合わせる\n",
    "model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']]) # スパース行列を使用\n",
    "X_combined = hstack([model_features, train_features]) # 結合する\n",
    "\n",
    "# 目的変数をエンコードする\n",
    "train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1) # 勝者の決定\n",
    "label_encoder = LabelEncoder() # ラベルエンコーダを初期化\n",
    "y_encoded = label_encoder.fit_transform(train['winner']) # エンコードされたラベル\n",
    "\n",
    "# データを分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42) # 学習データとテストデータに分割\n",
    "\n",
    "# データをDMatrixに変換する\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train) # 学習用データ\n",
    "dtest = xgb.DMatrix(X_test, label=y_test) # テスト用データ\n",
    "\n",
    "# パラメータの設定\n",
    "params = {\n",
    "    'objective': 'multi:softprob', # 複数クラスの確率を予測\n",
    "    'num_class': len(np.unique(y_encoded)), # クラスの数\n",
    "    'eval_metric': 'mlogloss', # 損失関数\n",
    "    'tree_method':'hist', # 木構造の学習方法\n",
    "    'device':'cuda' # GPUを使用する\n",
    "}\n",
    "\n",
    "# モデルの学習\n",
    "num_boost_round = 100 # ブーストラウンドの数\n",
    "bst = xgb.train(params, dtrain, num_boost_round) # モデルの学習\n",
    "\n",
    "# 確率を予測する\n",
    "pred_probs = bst.predict(dtest) # 確率を予測\n",
    "predictions = np.argmax(pred_probs, axis=1) # 予測されたクラスの取得\n",
    "\n",
    "# モデルを評価する\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(y_test, predictions) # テストデータの精度を計算\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\") # 精度を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:14:55.616829Z",
     "iopub.status.busy": "2024-07-06T03:14:55.616391Z",
     "iopub.status.idle": "2024-07-06T03:15:47.339503Z",
     "shell.execute_reply": "2024-07-06T03:15:47.338606Z",
     "shell.execute_reply.started": "2024-07-06T03:14:55.616803Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "\n",
    "# データを読み込む\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')\n",
    "\n",
    "# ベクトライザーとラベルエンコーダの初期化\n",
    "n_features = 2**10 # ベクトルの次元数を設定\n",
    "vectorizers = {\n",
    "    'prompt': HashingVectorizer(n_features=n_features), # プロンプト用のベクトライザー\n",
    "    'response_a': HashingVectorizer(n_features=n_features), # 応答A用のベクトライザー\n",
    "    'response_b': HashingVectorizer(n_features=n_features) # 応答B用のベクトライザー\n",
    "}\n",
    "model_encoder = LabelEncoder() # モデル識別子用のラベルエンコーダを初期化\n",
    "\n",
    "# モデル識別子をエンコードする\n",
    "train['model_a_encoded'] = model_encoder.fit_transform(train['model_a']) # モデルAのエンコード\n",
    "train['model_b_encoded'] = model_encoder.transform(train['model_b']) # モデルBのエンコード\n",
    "\n",
    "# テキストデータをベクトルに変換して結合する関数\n",
    "def process_and_concat_features(data, vectorizers):\n",
    "    features_list = [] # 特徴量のリストを初期化\n",
    "    for column, vectorizer in vectorizers.items():\n",
    "        print(f\"Vectorizing '{column}'...\") # ベクトライジングの進捗を表示\n",
    "        transformed_data = vectorizer.transform(data[column]) # ベクトルに変換\n",
    "        features_list.append(transformed_data) # リストに追加\n",
    "    final_features = hstack(features_list)  # スパース行列として保持\n",
    "    return final_features\n",
    "\n",
    "train_features = process_and_concat_features(train, vectorizers) # 特徴量を処理して結合\n",
    "\n",
    "# モデル識別子とテキスト特徴を組み合わせる\n",
    "model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']]) # スパース行列を使用\n",
    "X_combined = hstack([model_features, train_features]) # 結合する\n",
    "\n",
    "# 目的変数をエンコードする\n",
    "train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1) # 勝者の決定\n",
    "label_encoder = LabelEncoder() # ラベルエンコーダを初期化\n",
    "y_encoded = label_encoder.fit_transform(train['winner']) # エンコードされたラベル\n",
    "\n",
    "# データを分割する\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42) # 学習データとテストデータに分割\n",
    "\n",
    "# データをDMatrixに変換する\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train) # 学習用データ\n",
    "dtest = xgb.DMatrix(X_test, label=y_test) # テスト用データ\n",
    "\n",
    "# パラメータの設定\n",
    "params = {\n",
    "    'objective': 'multi:softprob', # 複数クラスの確率を予測\n",
    "    'num_class': len(np.unique(y_encoded)), # クラスの数\n",
    "    'eval_metric': 'mlogloss', # 損失関数\n",
    "    'tree_method':'hist', # 木構造の学習方法\n",
    "    'device':'cuda' # GPUを使用する\n",
    "}\n",
    "\n",
    "# モデルの学習\n",
    "num_boost_round = 100 # ブーストラウンドの数\n",
    "bst = xgb.train(params, dtrain, num_boost_round) # モデルの学習\n",
    "\n",
    "# 確率を予測する\n",
    "pred_probs = bst.predict(dtest) # 確率を予測\n",
    "predictions = np.argmax(pred_probs, axis=1) # 予測されたクラスの取得\n",
    "\n",
    "# モデルを評価する\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_accuracy = accuracy_score(y_test, predictions) # テストデータの精度を計算\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\") # 精度を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001168cc",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Load data\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "test_features = process_and_concat_features(test, vectorizers)\n",
    "\n",
    "dtest = xgb.DMatrix(test_features)\n",
    "\n",
    "# Predict the probabilities\n",
    "pred_probs = bst.predict(dtest)\n",
    "\n",
    "# Create DataFrame with the prediction probabilities for each class\n",
    "df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_)\n",
    "df_submission.insert(0, 'id', test['id'])\n",
    "df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "\n",
    "# Save predictions to CSV\n",
    "df_submission.to_csv('submission.csv', index=False)\n",
    "print(df_submission.head())\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# データを読み込む\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv') # テストデータの読み込み\n",
    "test_features = process_and_concat_features(test, vectorizers) # テストデータの特徴量を処理して結合\n",
    "\n",
    "dtest = xgb.DMatrix(test_features) # DMatrix形式に変換\n",
    "\n",
    "# 確率を予測する\n",
    "pred_probs = bst.predict(dtest) # 確率を予測\n",
    "\n",
    "# 各クラスの予測確率を持つDataFrameを作成\n",
    "df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_) # 確率をDataFrameに変換\n",
    "df_submission.insert(0, 'id', test['id']) # 'id'列を最初に挿入\n",
    "df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie'] # 列名を設定\n",
    "\n",
    "# 予測をCSVに保存\n",
    "df_submission.to_csv('submission.csv', index=False) # インデックスなしでCSVに保存\n",
    "print(df_submission.head()) # 保存したデータの最初の5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-06T03:33:31.839561Z",
     "iopub.status.busy": "2024-07-06T03:33:31.836315Z",
     "iopub.status.idle": "2024-07-06T03:33:31.880061Z",
     "shell.execute_reply": "2024-07-06T03:33:31.879042Z",
     "shell.execute_reply.started": "2024-07-06T03:33:31.839526Z"
    }
   },
   "outputs": [],
   "source": [
    "# データを読み込む\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv') # テストデータの読み込み\n",
    "test_features = process_and_concat_features(test, vectorizers) # テストデータの特徴量を処理して結合\n",
    "\n",
    "dtest = xgb.DMatrix(test_features) # DMatrix形式に変換\n",
    "\n",
    "# 確率を予測する\n",
    "pred_probs = bst.predict(dtest) # 確率を予測\n",
    "\n",
    "# 各クラスの予測確率を持つDataFrameを作成\n",
    "df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_) # 確率をDataFrameに変換\n",
    "df_submission.insert(0, 'id', test['id']) # 'id'列を最初に挿入\n",
    "df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie'] # 列名を設定\n",
    "\n",
    "# 予測をCSVに保存\n",
    "df_submission.to_csv('submission.csv', index=False) # インデックスなしでCSVに保存\n",
    "print(df_submission.head()) # 保存したデータの最初の5行を表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30733,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
