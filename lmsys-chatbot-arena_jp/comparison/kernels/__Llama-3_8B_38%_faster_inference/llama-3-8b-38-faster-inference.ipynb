{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed8b55b",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€Kaggleã®ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ã€ŒLMSYS - Chatbot Arenaã€ã«ãŠã„ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å¥½ã¿ã«åŸºã¥ããƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆå¿œç­”ã®å„ªåŠ£ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹ã®æœ€é©åŒ–ã‚’ç›®æŒ‡ã—ãŸã‚‚ã®ã§ã™ã€‚å…·ä½“çš„ã«ã¯ã€LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ã§ã‚ã‚‹Llamaã‚’ä½¿ç”¨ã—ã€å¿œç­”ç¢ºç‡ã‚’è¨ˆç®—ã™ã‚‹éš›ã®æ¨è«–é€Ÿåº¦ã‚’38%å‘ä¸Šã•ã›ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å–ã‚Šçµ„ã‚“ã§ã„ã‚‹å•é¡Œ\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ä¸»ã«æ¬¡ã®å•é¡Œã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™:\n",
    "1. **æ¨è«–æ™‚é–“ã®çŸ­ç¸®**: å…ƒã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã®65åˆ†ã‹ã‚‰ã€40åˆ†ã«æ¨è«–æ™‚é–“ã‚’çŸ­ç¸®ã™ã‚‹ã“ã¨ã«æˆåŠŸã€‚\n",
    "2. **å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®å®Ÿè£…**: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®é•·ã•ã«åŸºã¥ã„ã¦ãƒãƒƒãƒå†…ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’å‹•çš„ã«é©ç”¨ã—ã€å†—é•·ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¸›å°‘ã•ã›ã¦ã„ã¾ã™ã€‚\n",
    "3. **å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®æœ€å¤§é•·ã®è¨­å®š**: ãƒ¢ãƒ‡ãƒ«ãŒé•·ã„å…¥åŠ›ã«å¯¾å‡¦ã§ãã‚‹ã‚ˆã†ã«ã€`max_length`ã‚’1024ã‹ã‚‰2048ã«æ‹¡å¼µã€‚\n",
    "\n",
    "### ä½¿ç”¨ã—ã¦ã„ã‚‹æ‰‹æ³•ãƒ»ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "- **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯**: PyTorchãŒä½¿ç”¨ã•ã‚Œã€ç‰¹ã«GPUãƒªã‚½ãƒ¼ã‚¹ã‚’æ´»ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ãŒè¡Œã‚ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "- **Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒª**: Hugging Faceã®Transformersãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’åˆ©ç”¨ã—ã¦LLMã‚’èª­ã¿è¾¼ã¿ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ã‚’å®Ÿæ–½ã€‚\n",
    "- **ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ æ‹¡å¼µï¼ˆTTAï¼‰**: ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€ç•°ãªã‚‹å¿œç­”ã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‹æ‰‹æ³•ã‚‚è©¦ã•ã‚Œã¦ã„ã¾ã™ãŒã€åŠ¹æœã¯é™å®šçš„ã§ã—ãŸã€‚\n",
    "- **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„æ³¨æ„æ©Ÿæ§‹**: ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’å‰Šæ¸›ã™ã‚‹ãŸã‚ã«ã€Memory-Efficient Attentionã‚’æœ‰åŠ¹åŒ–ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å®Ÿè£…ã®æµã‚Œ\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã—ã€æ•´å½¢ã—ã¾ã™ã€‚\n",
    "2. **ãƒ¢ãƒ‡ãƒ«ã®è¨­å®šã¨èª­ã¿è¾¼ã¿**: è¤‡æ•°ã®GPUã«ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€LoRAï¼ˆLow-Rank Adaptationï¼‰ã«ã‚ˆã‚‹å¾®èª¿æ•´ã‚’è¨­å®šã—ã¾ã™ã€‚\n",
    "3. **æ¨è«–ãƒ—ãƒ­ã‚»ã‚¹**: ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ãƒãƒƒãƒå‡¦ç†ã—ã€å„ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å‡ºåŠ›ã‚’è¨ˆç®—ã—ã€çµæœã‚’çµåˆã—ã¦æœ€çµ‚çš„ãªç¢ºç‡ã‚’ç®—å‡ºã—ã¾ã™ã€‚\n",
    "4. **çµæœã®ä¿å­˜**: ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹å‹è€…ã®ç¢ºç‡ã‚’æå‡ºç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®åŠ¹ç‡çš„ãªæ¨è«–æ‰‹æ³•ã®å®Ÿè·µçš„ãªã‚±ãƒ¼ã‚¹ã‚¹ã‚¿ãƒ‡ã‚£ã‚’ç¤ºã—ã¦ãŠã‚Šã€ãã®ä¸­ã§æœ€å–„ã®ãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹ãŒç´¹ä»‹ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccaa45",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã«ã€Jupyter Notebookã®å†…å®¹ã‹ã‚‰åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªå°‚é–€ç”¨èªã«ã¤ã„ã¦ç°¡å˜ã«è§£èª¬ã—ã¾ã™ã€‚ç‰¹ã«ã€ã‚ã¾ã‚Šä¸€èˆ¬çš„ã§ãªã„ç”¨èªã‚„ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç‰¹æœ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦èª¬æ˜ã—ã¾ã™ã€‚\n",
    "\n",
    "1. **å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°**\n",
    "   - ã™ã¹ã¦ã®å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’åŒã˜é•·ã•ã«ã™ã‚‹ãŸã‚ã«ã€çŸ­ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«ã‚¼ãƒ­ãªã©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚å›ºå®šé•·ã§ã¯ãªãã€å„ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®æœ€é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã«åˆã‚ã›ã¦ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€ç„¡é§„ãªè¨ˆç®—ã‚’æ¸›ã‚‰ã—ã€åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ã“ã¨ãŒå¯èƒ½ã§ã™ã€‚\n",
    "\n",
    "2. **ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ æ‹¡å¼µï¼ˆTTA: Test Time Augmentationï¼‰**\n",
    "   - ãƒ†ã‚¹ãƒˆæ®µéšã§ãƒ‡ãƒ¼ã‚¿ã‚’å¢—ã‚„ã™æ‰‹æ³•ã§ã€ãƒ¢ãƒ‡ãƒ«ã®äºˆæ¸¬ã‚’æ”¹å–„ã™ã‚‹ãŸã‚ã«ã€åŒä¸€ã‚µãƒ³ãƒ—ãƒ«ã‚’ç•°ãªã‚‹æ–¹æ³•ï¼ˆã“ã“ã§ã¯å¿œç­”ã®é †åºã®å…¥ã‚Œæ›¿ãˆï¼‰ã§å‡¦ç†ã™ã‚‹ã“ã¨ã«ã‚ˆã‚Šã€ã‚ˆã‚Šå¤šæ§˜ãªäºˆæ¸¬ã‚’å¾—ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã™ã€‚\n",
    "\n",
    "3. **ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹**\n",
    "   - å¤šã‚¯ãƒ©ã‚¹åˆ†é¡å•é¡Œã§ä½¿ã†æ´»æ€§åŒ–é–¢æ•°ã§ã€å‡ºåŠ›ã‚’ç¢ºç‡ã¨ã—ã¦è§£é‡ˆã§ãã‚‹ã‚ˆã†ã«å¤‰æ›ã—ã¾ã™ã€‚ä¸€ã¤ã®å‡ºåŠ›ã‚’1ã«ã—ã€ä»–ã®å‡ºåŠ›ã‚’0ã‹ã‚‰1ã®é–“ã®æ•°å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚ã“ã®ãƒãƒ¼ãƒˆã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å¾—ã‚‰ã‚ŒãŸãƒ­ã‚¸ãƒƒãƒˆï¼ˆraw scoresï¼‰ã«å¯¾ã—ã¦é©ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "4. **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„æ³¨æ„æ©Ÿæ§‹**\n",
    "   - å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã‚’æ‰±ã†éš›ã«ãƒ¡ãƒ¢ãƒªæ¶ˆè²»ã‚’æŠ‘ãˆã‚‹ãŸã‚ã®ç‰¹åˆ¥ãªè¨­è¨ˆã®æ³¨æ„æ©Ÿæ§‹ã§ã™ã€‚é€šå¸¸ã®æ³¨æ„æ©Ÿæ§‹ã¯è¨ˆç®—ãŒé‡ããªã‚‹ãŸã‚ã€ç‰¹ã«å¤§è¦æ¨¡ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«å¯¾ã—ã¦åŠ¹ç‡çš„ãªæ–¹æ³•ãŒæ±‚ã‚ã‚‰ã‚Œã¾ã™ã€‚ã“ã®æ©Ÿæ§‹ã§ã¯ã€ãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨ã‚’æœ€å°é™ã«æŠ‘ãˆã€è¨ˆç®—æ™‚é–“ã‚’çŸ­ç¸®ã—ã¾ã™ã€‚\n",
    "\n",
    "5. **LoRAï¼ˆLow-Rank Adaptationï¼‰**\n",
    "   - ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã€å…¨ä½“ã®é‡ã¿ã‚’å¤‰æ›´ã™ã‚‹ã®ã§ã¯ãªãã€å°‘æ•°ã®è¿½åŠ ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã§ã™ã€‚å…ƒã®ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãŒå›ºå®šã•ã‚Œã€æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã«ç‰¹åŒ–ã—ãŸè»½é‡ã®é©å¿œã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "6. **ãƒã‚¤ã‚¢ã‚¹**\n",
    "   - æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã§ã€å­¦ç¿’ã‚„æ¨è«–ãŒç‰¹å®šã®æ–¹å‘ã«åã‚‹ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚ã“ã“ã§ã¯ãƒã‚¤ã‚¢ã‚¹ã®è¨­å®šã‚’æŒ‡ã—ã¦ãŠã‚Šã€ç‰¹å®šã®å­¦ç¿’æ¡ä»¶ã‚„ç›®çš„ã«åŸºã¥ã„ã¦èª¿æ•´ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "7. **ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º**\n",
    "   - ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹å½¢å¼ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã«å¤‰æ›ã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã¯å˜èªã‚„ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ãªã©ã€æ„å‘³ã®ã‚ã‚‹å˜ä½ã§ã‚ã‚Šã€ãã‚Œã«ã‚ˆã‚Šè‡ªç„¶è¨€èªå‡¦ç†ï¼ˆNLPï¼‰ãƒ¢ãƒ‡ãƒ«ãŒãƒ†ã‚­ã‚¹ãƒˆã‚’å‡¦ç†ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n",
    "\n",
    "8. **æœ€é•·ã®é•·ã•ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆpadding=\"longest\"ï¼‰**\n",
    "   - ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®æœ€ã‚‚é•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã«å¿œã˜ã¦ä»–ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã§ã™ã€‚ãƒãƒƒãƒå…¨ä½“ã®åŠ¹ç‡ã‚’å‘ä¸Šã•ã›ã‚‹ãŸã‚ã«é¸ã°ã‚Œã‚‹ã“ã¨ãŒå¤šã„ã§ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®èª¬æ˜ã¯ã€åˆå¿ƒè€…ãŒè¸ã¾ãˆã¦ãŠãã¹ãç”¨èªã®ç†è§£ã‚’åŠ©ã‘ã€ç‰¹ã«ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«ãŠã‘ã‚‹é‡è¦ãªæ¦‚å¿µãŒè‰¯ãåˆ†ã‹ã‚‹åŠ©ã‘ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a18f27",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## ğŸ¦™ğŸ¦™ğŸ¦™ What this notebook is\n",
    "This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.\n",
    "I made a few improvements upon @kishanvavdara's work:\n",
    "\n",
    "### 38% faster inference\n",
    "Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:\n",
    "\n",
    "#### 1. Dynamic padding\n",
    "Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.\n",
    "\n",
    "#### 2. Sort the test data by input length\n",
    "To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.\n",
    "\n",
    "### Longer input sequence\n",
    "Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.\n",
    "Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.\n",
    "\n",
    "## Things I have tried but didn't work\n",
    "\n",
    "### Test Time Augmentation (TTA)\n",
    "I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.\n",
    "We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.\n",
    "TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.\n",
    "\n",
    "### Truncate each input\n",
    "The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.\n",
    "I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.\n",
    "\n",
    "## ğŸ†• Update in version 4\n",
    "The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.\n",
    "In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.\n",
    "This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.\n",
    "We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.\n",
    "\n",
    "# Import libs\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "## ğŸ¦™ğŸ¦™ğŸ¦™ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å†…å®¹\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€@kishanvavdaraã«ã‚ˆã‚‹[Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)ã‚’åŸºã«ä½œæˆã•ã‚Œã¦ã„ã¾ã™ã€‚ãƒªãƒ³ã‚¯å…ˆã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ã¾ã ç¢ºèªã—ã¦ã„ãªã„å ´åˆã¯ã€ãœã²ãƒã‚§ãƒƒã‚¯ã—ã¦è©•ä¾¡ã‚’ãŠé¡˜ã„ã—ã¾ã™ã€‚\n",
    "ç§ã¯ã€@kishanvavdaraã®ä½œæ¥­ã‚’ã‚‚ã¨ã«ã„ãã¤ã‹ã®æ”¹å–„ã‚’åŠ ãˆã¾ã—ãŸï¼š\n",
    "\n",
    "### 38%é€Ÿã„æ¨è«–\n",
    "ã“ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ä½¿ã£ã¦ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã®æœ€åˆã®10,000ã‚µãƒ³ãƒ—ãƒ«ã«å¯¾ã™ã‚‹æ¨è«–æ™‚é–“ã¯40åˆ†ã§ã€ä¸€æ–¹ã§å…ƒã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯65åˆ†ã‹ã‹ã‚‹ãŸã‚ã€ç²¾åº¦ã«å½±éŸ¿ã‚’ä¸ãˆãšã«38%é€Ÿããªã£ã¦ã„ã¾ã™ã€‚ä¸»ã«æ¬¡ã®2ã¤ã‚’è¿½åŠ ã—ã¾ã—ãŸï¼š\n",
    "\n",
    "#### 1. å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "ã™ã¹ã¦ã®å…¥åŠ›ã‚’äº‹å‰ã«å›ºå®šé•·ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ä»£ã‚ã‚Šã«ã€å„ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®æœ€é•·ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã¾ã§å‹•çš„ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "#### 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆ\n",
    "å‹•çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã®åˆ©ç‚¹ã‚’æœ€å¤§é™ã«æ´»ã‹ã™ãŸã‚ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆã•ã‚Œã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å„ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®å…¥åŠ›ãŒã»ã¼åŒã˜é•·ã•ã«ãªã‚Šã€å†—é•·ãªãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "### ã‚ˆã‚Šé•·ã„å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹\n",
    "è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã®99%ã¯1024ä»¥å†…ã«åã¾ã£ã¦ã„ã¾ã™ãŒã€æ®‹ã‚Šã®1%ã¯åã¾ã£ã¦ã„ã¾ã›ã‚“ã€‚ã•ã‚‰ã«ã€ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«ã¯ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒå«ã¾ã‚Œã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€`max_length`ã¯å¯èƒ½ãªé™ã‚Šé•·ãè¨­å®šã™ã‚‹æ–¹ãŒå®‰å…¨ã ã¨æ€ã„ã¾ã™ã€‚\n",
    "`max_length`ã‚’1024ã‹ã‚‰1280ã«å¤‰æ›´ã™ã‚‹ã“ã¨ã§LBãŒ0.989ã‹ã‚‰0.983ã«æ”¹å–„ã•ã‚Œã¾ã—ãŸã€‚\n",
    "\n",
    "## è©¦ã—ã¦ã¿ãŸãŒåŠ¹æœãŒãªã‹ã£ãŸã“ã¨\n",
    "\n",
    "### ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ æ‹¡å¼µï¼ˆTTAï¼‰\n",
    "ç§ã¯ã€response_aã¨response_bã®é †åºã‚’å…¥ã‚Œæ›¿ãˆã‚‹ã‚·ãƒ³ãƒ—ãƒ«ãªTTAã‚’è©¦ã—ã¾ã—ãŸã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ãŒ2å›å‘¼ã³å‡ºã•ã‚Œã‚‹ãŸã‚æ¨è«–æ™‚é–“ãŒ2å€ã«ãªã‚Šã¾ã™ã€‚\n",
    "äºŒã¤ã®ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã‚’å¹³å‡åŒ–ã™ã‚‹ã‹ã€äºŒã¤ã®ãƒ­ã‚¸ãƒƒãƒˆã‚’å¹³å‡åŒ–ã—ã¦ã‹ã‚‰ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ç¢ºç‡ã‚’è¨ˆç®—ã§ãã¾ã™ã€‚ä¸¡æ–¹ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ã¯LBãŒæ”¹å–„ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸãŒã€ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹ã®å¹³å‡åŒ–ãŒã‚ˆã‚Šè‰¯ã„çµæœã‚’ç¤ºã—ã¾ã—ãŸã€‚\n",
    "TTAã¯ã‚µãƒ³ãƒ—ãƒ«ã”ã¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’2å›å‘¼ã³å‡ºã™ãŸã‚ã€æ¨è«–æ™‚é–“ãŒ2å€ã«ãªã‚Šã¾ã™ã€‚`max_length=1280`ã¨TTAã‚’æœ‰åŠ¹ã«ã—ã¦æå‡ºãŒå®Œäº†ã—ã¾ã—ãŸãŒã€åŠ¹ç‡çš„ãªæ¨è«–ã«ã‚ˆã‚Š9æ™‚é–“ä»¥å†…ã«åã¾ã‚Šã¾ã—ãŸã€‚\n",
    "\n",
    "### å„å…¥åŠ›ã‚’åˆ‡ã‚Šè©°ã‚ã‚‹\n",
    "å…ƒã®å®Ÿè£…ã§ã¯ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ + response_a + response_bã¨ã„ã†é€£çµã‚·ãƒ¼ã‚±ãƒ³ã‚¹ãŒåˆ‡ã‚Šè©°ã‚ã‚‰ã‚Œã¾ã™ã€‚ç›´æˆªçš„ãªåˆ‡ã‚Šè©°ã‚ã‚’é©ç”¨ã™ã‚‹ã¨ã€ä¸€éƒ¨ã®ï¼ˆç¨€ã§ã¯ã‚ã‚Šã¾ã™ãŒï¼‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒ1280ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¶…ãˆã‚‹ãŸã‚ã€ãƒ¢ãƒ‡ãƒ«ã¯å‹è€…ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«æ¨æ¸¬ã™ã‚‹ã—ã‹ãªããªã‚Šã¾ã™ã€‚\n",
    "ç§ã¯ã€æœ€åˆã«å„å…¥åŠ›ã‚’å›ºå®šé•·ã«åˆ‡ã‚Šè©°ã‚ã¦ã‹ã‚‰ã€ä¸‰ã¤ã‚’é€£çµã—ã‚ˆã†ã¨ã—ã¾ã—ãŸãŒã€LBã¯æ”¹å–„ã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n",
    "\n",
    "## ğŸ†• ãƒãƒ¼ã‚¸ãƒ§ãƒ³4ã®æ›´æ–°\n",
    "åŠ¹æœçš„ãªæ¨è«–ã®ãŠã‹ã’ã§å…¥åŠ›ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã®é•·ã•ã‚’å¢—ã‚„ã™æ™‚é–“ãŒååˆ†ã«ã‚ã‚‹ãŸã‚ã€`max_length`ã‚’2048ã«å¤‰æ›´ã—ã¾ã—ãŸã€‚ã¾ãŸã€ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã¯8ã‹ã‚‰4ã«æ¸›å°‘ã•ã›ã¾ã—ãŸã€‚\n",
    "ã•ã‚‰ã«ã€ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã‚’æ¸›ã‚‰ã™ãŸã‚ã«[Memory-Efficient Attention](https://github.com/facebookresearch/xformers)ã‚’æœ‰åŠ¹ã«ã—ã¾ã—ãŸã€‚\n",
    "ã“ã‚Œã«ã‚ˆã‚ŠLBãŒ0.983ã‹ã‚‰0.979ã«æ”¹å–„ã•ã‚Œã€æå‡ºã¯TTAãªã—ã§4æ™‚é–“æœªæº€ã§æ¸ˆã¿ã¾ã—ãŸã€‚\n",
    "ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’1ã«æ¸›ã‚‰ã™ã“ã¨ã§ã•ã‚‰ã«é•·ãã§ãã¾ã™ãŒã€ã¾ã ãƒ†ã‚¹ãƒˆã—ã¦ã„ã¾ã›ã‚“ã€‚\n",
    "\n",
    "# ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb856f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:21:10.180351Z",
     "iopub.status.busy": "2024-07-09T04:21:10.17999Z",
     "iopub.status.idle": "2024-07-09T04:22:04.09703Z",
     "shell.execute_reply": "2024-07-09T04:22:04.095937Z",
     "shell.execute_reply.started": "2024-07-09T04:21:10.180321Z"
    },
    "papermill": {
     "duration": 53.686843,
     "end_time": "2024-07-01T02:57:31.530755",
     "exception": false,
     "start_time": "2024-07-01T02:56:37.843912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a214c03",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:04.099619Z",
     "iopub.status.busy": "2024-07-09T04:22:04.099318Z",
     "iopub.status.idle": "2024-07-09T04:22:21.661753Z",
     "shell.execute_reply": "2024-07-09T04:22:21.660958Z",
     "shell.execute_reply.started": "2024-07-09T04:22:04.099589Z"
    },
    "papermill": {
     "duration": 21.138547,
     "end_time": "2024-07-01T02:57:52.676238",
     "exception": false,
     "start_time": "2024-07-01T02:57:31.537691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1c134",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:\n",
    "1. Flash Attention\n",
    "2. Memory Efficient Attention\n",
    "3. A PyTorch (naive) implementation\n",
    "\n",
    "By default, all of those are enabled but we can also manually enable/disable certain backends.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "PyTorchã®[ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention)ã«ã‚ˆã‚‹ã¨ã€`scaled_dot_product_attention`ã¯ã€è‡ªå‹•çš„ã«æœ€ã‚‚æœ€é©ãªå®Ÿè£…ã‚’é¸æŠã—ã¾ã™ï¼š\n",
    "1. Flash Attention\n",
    "2. ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„æ³¨æ„æ©Ÿæ§‹\n",
    "3. PyTorchã®ï¼ˆãƒŠã‚¤ãƒ¼ãƒ–ãªï¼‰å®Ÿè£…\n",
    "\n",
    "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã¯ã€ã“ã‚Œã‚‰ã™ã¹ã¦ãŒæœ‰åŠ¹ã§ã™ãŒã€ç‰¹å®šã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’æ‰‹å‹•ã§æœ‰åŠ¹åŒ–/ç„¡åŠ¹åŒ–ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e6fbe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "assert torch.cuda.device_count() == 2, \"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒãƒ«ãƒGPUãŒå¿…è¦ã§ã™ï¼\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Flash Attentionã¯T4/P100ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€åŠ¹æœã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.663342Z",
     "iopub.status.busy": "2024-07-09T04:22:21.662808Z",
     "iopub.status.idle": "2024-07-09T04:22:21.668153Z",
     "shell.execute_reply": "2024-07-09T04:22:21.667165Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.663315Z"
    },
    "papermill": {
     "duration": 0.014661,
     "end_time": "2024-07-01T02:57:52.698559",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.683898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2, \"ç”³ã—è¨³ã‚ã‚Šã¾ã›ã‚“ãŒã€ãƒãƒ«ãƒGPUãŒå¿…è¦ã§ã™ï¼\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Flash Attentionã¯T4/P100ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„ãŸã‚ã€åŠ¹æœã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ad0dd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # ãƒ¢ãƒ‡ãƒ«ã®åå‰\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'  # é‡ã¿ã®ãƒ‘ã‚¹\n",
    "    max_length = 2048  # æœ€å¤§é•·\n",
    "    batch_size = 4  # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    device = torch.device(\"cuda\")  # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "    tta = False  # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ æ‹¡å¼µã€‚<prompt>-<model-bã®å¿œç­”>-<model-aã®å¿œç­”>\n",
    "    spread_max_length = False  # max_length//3ã‚’å„å…¥åŠ›ã«é©ç”¨ã™ã‚‹ã‹ã€é€£çµå…¥åŠ›ã«max_lengthã‚’é©ç”¨ã™ã‚‹ã‹\n",
    "\n",
    "cfg = Config()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.669917Z",
     "iopub.status.busy": "2024-07-09T04:22:21.669556Z",
     "iopub.status.idle": "2024-07-09T04:22:21.695408Z",
     "shell.execute_reply": "2024-07-09T04:22:21.694637Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.669883Z"
    },
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-07-01T02:57:52.720706",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.705889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # ãƒ¢ãƒ‡ãƒ«ã®åå‰\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'  # é‡ã¿ã®ãƒ‘ã‚¹\n",
    "    max_length = 2048  # æœ€å¤§é•·\n",
    "    batch_size = 4  # ãƒãƒƒãƒã‚µã‚¤ã‚º\n",
    "    device = torch.device(\"cuda\")  # ä½¿ç”¨ã™ã‚‹ãƒ‡ãƒã‚¤ã‚¹\n",
    "    tta = False  # ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒ æ‹¡å¼µã€‚<prompt>-<model-bã®å¿œç­”>-<model-aã®å¿œç­”>\n",
    "    spread_max_length = False  # max_length//3ã‚’å„å…¥åŠ›ã«é©ç”¨ã™ã‚‹ã‹ã€é€£çµå…¥åŠ›ã«max_lengthã‚’é©ç”¨ã™ã‚‹ã‹\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cf628",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Prepare Data \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™ \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddb7f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(test.head(5))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# ãƒªã‚¹ãƒˆå†…ã®æ–‡å­—åˆ—ã‚’é€£çµã™ã‚‹é–¢æ•°\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # æ–‡å­—åˆ—ã®å…ˆé ­ã¨æœ«å°¾ã®è§’æ‹¬å¼§ã‚’å–ã‚Šé™¤ã\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦ãƒˆãƒªãƒŸãƒ³ã‚°\n",
    "    return  ' '.join(sentences)  # æ–‡ã‚’çµåˆã—ã¦è¿”ã™\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'åˆ—ã‚’å‡¦ç†\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'åˆ—ã‚’å‡¦ç†\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'åˆ—ã‚’å‡¦ç†\n",
    "\n",
    "display(test.head(5))  # ä¸Šä½5è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.698075Z",
     "iopub.status.busy": "2024-07-09T04:22:21.697797Z",
     "iopub.status.idle": "2024-07-09T04:22:21.73015Z",
     "shell.execute_reply": "2024-07-09T04:22:21.729264Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.698051Z"
    },
    "papermill": {
     "duration": 0.040947,
     "end_time": "2024-07-01T02:57:52.781962",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.741015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# ãƒªã‚¹ãƒˆå†…ã®æ–‡å­—åˆ—ã‚’é€£çµã™ã‚‹é–¢æ•°\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # æ–‡å­—åˆ—ã®å…ˆé ­ã¨æœ«å°¾ã®è§’æ‹¬å¼§ã‚’å–ã‚Šé™¤ã\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # æ–‡å­—åˆ—ã‚’åˆ†å‰²ã—ã¦ãƒˆãƒªãƒŸãƒ³ã‚°\n",
    "    return  ' '.join(sentences)  # æ–‡ã‚’çµåˆã—ã¦è¿”ã™\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'åˆ—ã‚’å‡¦ç†\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'åˆ—ã‚’å‡¦ç†\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'åˆ—ã‚’å‡¦ç†\n",
    "\n",
    "display(test.head(5))  # ä¸Šä½5è¡Œã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1a122",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4717a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ•´å½¢\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]  # Model Aã®å¿œç­”ã‚’æ•´å½¢\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]  # Model Bã®å¿œç­”ã‚’æ•´å½¢\n",
    "    if spread_max_length:\n",
    "        # max_lengthã‚’å„å…¥åŠ›ã«å‡ç­‰ã«åˆ†é…ã™ã‚‹å ´åˆ\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # å…¥åŠ›IDã®ç”Ÿæˆ\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # æ³¨æ„ãƒã‚¹ã‚¯ã®ç”Ÿæˆ\n",
    "    else:\n",
    "        # max_lengthã‚’å…¨ä½“ã®å…¥åŠ›ã«é©ç”¨ã™ã‚‹å ´åˆ\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "        input_ids = tokenized.input_ids  # å…¥åŠ›IDã‚’å–å¾—\n",
    "        attention_mask = tokenized.attention_mask  # æ³¨æ„ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    return input_ids, attention_mask  # å…¥åŠ›IDã¨æ³¨æ„ãƒã‚¹ã‚¯ã‚’è¿”ã™\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.731569Z",
     "iopub.status.busy": "2024-07-09T04:22:21.731292Z",
     "iopub.status.idle": "2024-07-09T04:22:21.741316Z",
     "shell.execute_reply": "2024-07-09T04:22:21.740313Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.731544Z"
    },
    "papermill": {
     "duration": 0.017982,
     "end_time": "2024-07-01T02:57:52.821282",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.8033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]  # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’æ•´å½¢\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]  # Model Aã®å¿œç­”ã‚’æ•´å½¢\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]  # Model Bã®å¿œç­”ã‚’æ•´å½¢\n",
    "    if spread_max_length:\n",
    "        # max_lengthã‚’å„å…¥åŠ›ã«å‡ç­‰ã«åˆ†é…ã™ã‚‹å ´åˆ\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # å…¥åŠ›IDã®ç”Ÿæˆ\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # æ³¨æ„ãƒã‚¹ã‚¯ã®ç”Ÿæˆ\n",
    "    else:\n",
    "        # max_lengthã‚’å…¨ä½“ã®å…¥åŠ›ã«é©ç”¨ã™ã‚‹å ´åˆ\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã§ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚º\n",
    "        input_ids = tokenized.input_ids  # å…¥åŠ›IDã‚’å–å¾—\n",
    "        attention_mask = tokenized.attention_mask  # æ³¨æ„ãƒã‚¹ã‚¯ã‚’å–å¾—\n",
    "    return input_ids, attention_mask  # å…¥åŠ›IDã¨æ³¨æ„ãƒã‚¹ã‚¯ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ed075",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]  # IDåˆ—ã‚’ä½œæˆ\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’é©ç”¨\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # å…¥åŠ›IDã®é•·ã•ã‚’è¨ˆç®—\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]  # IDåˆ—ã‚’ä½œæˆ\n",
    "# response_aã¨response_bã‚’å…¥ã‚Œæ›¿ãˆã‚‹\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # å…¥åŠ›IDã®é•·ã•ã‚’è¨ˆç®—\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.742873Z",
     "iopub.status.busy": "2024-07-09T04:22:21.742567Z",
     "iopub.status.idle": "2024-07-09T04:22:22.300441Z",
     "shell.execute_reply": "2024-07-09T04:22:22.299465Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.742847Z"
    },
    "papermill": {
     "duration": 0.596117,
     "end_time": "2024-07-01T02:57:53.425111",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.828994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’èª­ã¿è¾¼ã¿\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]  # IDåˆ—ã‚’ä½œæˆ\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã‚’é©ç”¨\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # å…¥åŠ›IDã®é•·ã•ã‚’è¨ˆç®—\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]  # IDåˆ—ã‚’ä½œæˆ\n",
    "# response_aã¨response_bã‚’å…¥ã‚Œæ›¿ãˆã‚‹\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # å…¥åŠ›IDã®é•·ã•ã‚’è¨ˆç®—"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fadcc6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # æœ€åˆã®å…¥åŠ›IDã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.302245Z",
     "iopub.status.busy": "2024-07-09T04:22:22.301819Z",
     "iopub.status.idle": "2024-07-09T04:22:22.308508Z",
     "shell.execute_reply": "2024-07-09T04:22:22.307457Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.302186Z"
    },
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-07-01T02:57:53.448552",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.432565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # æœ€åˆã®å…¥åŠ›IDã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41727d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  # æœ€åˆã®å…¥ã‚Œæ›¿ãˆãŸå…¥åŠ›IDã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.310334Z",
     "iopub.status.busy": "2024-07-09T04:22:22.309948Z",
     "iopub.status.idle": "2024-07-09T04:22:22.319687Z",
     "shell.execute_reply": "2024-07-09T04:22:22.318833Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.310297Z"
    },
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-07-01T02:57:53.472007",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.456043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  # æœ€åˆã®å…¥ã‚Œæ›¿ãˆãŸå…¥åŠ›IDã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb2cd8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load model \n",
    "We load 1 model on each gpu.  \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ \n",
    "å„GPUã«1ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚  \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb82e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# BitsAndBytesã®è¨­å®š\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # ãƒ¢ãƒ‡ãƒ«ã‚’8ãƒ“ãƒƒãƒˆã§èª­ã¿è¾¼ã‚€è¨­å®š\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # è¨ˆç®—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    bnb_8bit_use_double_quant=False,  # äºŒé‡é‡å­åŒ–ã‚’ç„¡åŠ¹ã«ã™ã‚‹è¨­å®š\n",
    ")\n",
    "\n",
    "# GPU 0ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    quantization_config=bnb_config,  # é‡å­åŒ–è¨­å®šã‚’é©ç”¨\n",
    "    device_map='cuda:0')  # GPU 0ã«å‰²ã‚Šå½“ã¦\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š\n",
    "\n",
    "# GPU 1ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    quantization_config=bnb_config,  # é‡å­åŒ–è¨­å®šã‚’é©ç”¨\n",
    "    device_map='cuda:1')  # GPU 1ã«å‰²ã‚Šå½“ã¦\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.320895Z",
     "iopub.status.busy": "2024-07-09T04:22:22.320635Z",
     "iopub.status.idle": "2024-07-09T04:24:16.456934Z",
     "shell.execute_reply": "2024-07-09T04:24:16.456022Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.320873Z"
    },
    "papermill": {
     "duration": 105.076557,
     "end_time": "2024-07-01T02:59:38.570536",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.493979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesã®è¨­å®š\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # ãƒ¢ãƒ‡ãƒ«ã‚’8ãƒ“ãƒƒãƒˆã§èª­ã¿è¾¼ã‚€è¨­å®š\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # è¨ˆç®—ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    bnb_8bit_use_double_quant=False,  # äºŒé‡é‡å­åŒ–ã‚’ç„¡åŠ¹ã«ã™ã‚‹è¨­å®š\n",
    ")\n",
    "\n",
    "# GPU 0ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    quantization_config=bnb_config,  # é‡å­åŒ–è¨­å®šã‚’é©ç”¨\n",
    "    device_map='cuda:0')  # GPU 0ã«å‰²ã‚Šå½“ã¦\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š\n",
    "\n",
    "# GPU 1ã«ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚€\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # ãƒ¢ãƒ‡ãƒ«ã®ãƒ‡ãƒ¼ã‚¿å‹ã‚’float16ã«è¨­å®š\n",
    "    quantization_config=bnb_config,  # é‡å­åŒ–è¨­å®šã‚’é©ç”¨\n",
    "    device_map='cuda:1')  # GPU 1ã«å‰²ã‚Šå½“ã¦\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³IDã‚’è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f01e3",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load weights \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# é‡ã¿ã®èª­ã¿è¾¼ã¿ \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e6f0c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj']\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# LoRAã®è¨­å®š\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # ç¸®å°æ¬¡å…ƒ\n",
    "    lora_alpha=32,  # LoRAã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "    lora_dropout=0.10,  # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
    "    bias='none',  # ãƒã‚¤ã‚¢ã‚¹ã®è¨­å®š\n",
    "    inference_mode=True,  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "    task_type=TaskType.SEQ_CLS,  # ã‚¿ã‚¹ã‚¯ã®ç¨®é¡\n",
    "    target_modules=['o_proj', 'v_proj']  # å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:16.458567Z",
     "iopub.status.busy": "2024-07-09T04:24:16.458232Z",
     "iopub.status.idle": "2024-07-09T04:24:16.463705Z",
     "shell.execute_reply": "2024-07-09T04:24:16.462773Z",
     "shell.execute_reply.started": "2024-07-09T04:24:16.45854Z"
    },
    "papermill": {
     "duration": 0.0162,
     "end_time": "2024-07-01T02:59:38.610906",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.594706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRAã®è¨­å®š\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # ç¸®å°æ¬¡å…ƒ\n",
    "    lora_alpha=32,  # LoRAã®ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°ä¿‚æ•°\n",
    "    lora_dropout=0.10,  # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆç‡\n",
    "    bias='none',  # ãƒã‚¤ã‚¢ã‚¹ã®è¨­å®š\n",
    "    inference_mode=True,  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹ã«ã™ã‚‹\n",
    "    task_type=TaskType.SEQ_CLS,  # ã‚¿ã‚¹ã‚¯ã®ç¨®é¡\n",
    "    target_modules=['o_proj', 'v_proj']  # å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e967fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0) \n",
    "# Load weights\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_1.eval()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# PEFTã®å–å¾—\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦GPU 0ã«é…ç½®\n",
    "# é‡ã¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦GPU 1ã«é…ç½®\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # é‡ã¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "model_1.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:16.465891Z",
     "iopub.status.busy": "2024-07-09T04:24:16.464949Z",
     "iopub.status.idle": "2024-07-09T04:24:29.793371Z",
     "shell.execute_reply": "2024-07-09T04:24:29.792506Z",
     "shell.execute_reply.started": "2024-07-09T04:24:16.465854Z"
    },
    "papermill": {
     "duration": 13.701042,
     "end_time": "2024-07-01T02:59:52.320278",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.619236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PEFTã®å–å¾—\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦GPU 0ã«é…ç½®\n",
    "# é‡ã¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—ã—ã¦GPU 1ã«é…ç½®\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # é‡ã¿ã‚’èª­ã¿è¾¼ã‚€\n",
    "model_1.eval()  # è©•ä¾¡ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6867db",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters()\n",
    "model_1.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "model_0.print_trainable_parameters()  # ãƒ¢ãƒ‡ãƒ«0ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
    "model_1.print_trainable_parameters()  # ãƒ¢ãƒ‡ãƒ«1ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.794793Z",
     "iopub.status.busy": "2024-07-09T04:24:29.794518Z",
     "iopub.status.idle": "2024-07-09T04:24:29.807888Z",
     "shell.execute_reply": "2024-07-09T04:24:29.806996Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.79477Z"
    },
    "papermill": {
     "duration": 0.026729,
     "end_time": "2024-07-01T02:59:52.356012",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.329283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "model_0.print_trainable_parameters()  # ãƒ¢ãƒ‡ãƒ«0ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
    "model_1.print_trainable_parameters()  # ãƒ¢ãƒ‡ãƒ«1ã®å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1048ba",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Inference\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# æ¨è«–\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b04b69",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # å„ãƒ¢ãƒ‡ãƒ«ã®å‹ã¡ã¨å¼•ãåˆ†ã‘ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "    \n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦æ¨è«–ã‚’è¡Œã†\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # ãƒãƒƒãƒã®çµ‚ã‚ã‚Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
    "        tmp = df.iloc[start_idx:end_idx]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # å…¥åŠ›IDã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # æ³¨æ„ãƒã‚¹ã‚¯ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # æœ€é•·ã®é•·ã•ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã‚’æ¸¡ã—ã¦å‡ºåŠ›ã‚’è¨ˆç®—\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ãƒ­ã‚¸ãƒƒãƒˆã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã§å‡¦ç†\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())  # Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "        b_win.extend(proba[:, 1].tolist())  # Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "        tie.extend(proba[:, 2].tolist())  # å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    df[\"winner_model_b\"] = b_win  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    df[\"winner_tie\"] = tie  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    \n",
    "    return df  # çµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.811288Z",
     "iopub.status.busy": "2024-07-09T04:24:29.810984Z",
     "iopub.status.idle": "2024-07-09T04:24:29.821659Z",
     "shell.execute_reply": "2024-07-09T04:24:29.820866Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.811264Z"
    },
    "papermill": {
     "duration": 0.021078,
     "end_time": "2024-07-01T02:59:52.402973",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.381895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # å„ãƒ¢ãƒ‡ãƒ«ã®å‹ã¡ã¨å¼•ãåˆ†ã‘ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’åˆæœŸåŒ–\n",
    "    \n",
    "    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã«åŸºã¥ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†å‰²ã—ã¦æ¨è«–ã‚’è¡Œã†\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # ãƒãƒƒãƒã®çµ‚ã‚ã‚Šã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ±ºå®š\n",
    "        tmp = df.iloc[start_idx:end_idx]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’å–å¾—\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # å…¥åŠ›IDã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # æ³¨æ„ãƒã‚¹ã‚¯ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # æœ€é•·ã®é•·ã•ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã™\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # ãƒ¢ãƒ‡ãƒ«ã«å…¥åŠ›ã‚’æ¸¡ã—ã¦å‡ºåŠ›ã‚’è¨ˆç®—\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ãƒ­ã‚¸ãƒƒãƒˆã‚’ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã§å‡¦ç†\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())  # Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "        b_win.extend(proba[:, 1].tolist())  # Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "        tie.extend(proba[:, 2].tolist())  # å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    df[\"winner_model_b\"] = b_win  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    df[\"winner_tie\"] = tie  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "    \n",
    "    return df  # çµæœã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316608cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "# åŠ¨çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã«å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆ\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# sub_1ã¨sub_2ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒã»ã¼åŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
    "sub_1 = data.iloc[0::2].copy()  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "sub_2 = data.iloc[1::2].copy()  # å¥‡æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ä¸¦åˆ—å‡¦ç†\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # ä¸¦åˆ—ã«æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # çµæœã‚’çµåˆ\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # å‹è€…ã®ç¢ºç‡ã‚’å–å¾—\n",
    "\n",
    "print(f\"çµŒéæ™‚é–“: {time.time() - st}\")  # çµŒéæ™‚é–“ã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.822956Z",
     "iopub.status.busy": "2024-07-09T04:24:29.822692Z",
     "iopub.status.idle": "2024-07-09T04:24:33.127581Z",
     "shell.execute_reply": "2024-07-09T04:24:33.126609Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.822935Z"
    },
    "papermill": {
     "duration": 3.316613,
     "end_time": "2024-07-01T02:59:55.727834",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.411221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "# åŠ¨çš„ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’æœ€å¤§é™ã«æ´»ç”¨ã™ã‚‹ãŸã‚ã«å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆ\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# sub_1ã¨sub_2ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ãŒã»ã¼åŒã˜ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n",
    "sub_1 = data.iloc[0::2].copy()  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "sub_2 = data.iloc[1::2].copy()  # å¥‡æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ä¸¦åˆ—å‡¦ç†\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # ä¸¦åˆ—ã«æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # çµæœã‚’çµåˆ\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # å‹è€…ã®ç¢ºç‡ã‚’å–å¾—\n",
    "\n",
    "print(f\"çµŒéæ™‚é–“: {time.time() - st}\")  # çµŒéæ™‚é–“ã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c623f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆã—ã¦é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "    sub_1 = data.iloc[0::2].copy()  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "    sub_2 = data.iloc[1::2].copy()  # å¥‡æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ä¸¦åˆ—å‡¦ç†\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # ä¸¦åˆ—ã«æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # çµæœã‚’çµåˆ\n",
    "    # TTAã®çµæœã¯é †åºãŒé€†ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  # TTAã®ç¢ºç‡ã‚’å–å¾—\n",
    "    # å…ƒã®çµæœã¨TTAã®çµæœã‚’å¹³å‡åŒ–ã™ã‚‹\n",
    "    proba = (proba + tta_proba) / 2  # ç¢ºç‡ã‚’å¹³å‡åŒ–\n",
    "\n",
    "print(f\"çµŒéæ™‚é–“: {time.time() - st}\")  # çµŒéæ™‚é–“ã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:33.129319Z",
     "iopub.status.busy": "2024-07-09T04:24:33.128932Z",
     "iopub.status.idle": "2024-07-09T04:24:33.137663Z",
     "shell.execute_reply": "2024-07-09T04:24:33.136587Z",
     "shell.execute_reply.started": "2024-07-09T04:24:33.129289Z"
    },
    "papermill": {
     "duration": 1.755381,
     "end_time": "2024-07-01T02:59:57.492377",
     "exception": false,
     "start_time": "2024-07-01T02:59:55.736996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # å…¥åŠ›é•·ã§ã‚½ãƒ¼ãƒˆã—ã¦é€Ÿåº¦ã‚’å‘ä¸Šã•ã›ã‚‹\n",
    "    sub_1 = data.iloc[0::2].copy()  # å¶æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "    sub_2 = data.iloc[1::2].copy()  # å¥‡æ•°ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚³ãƒ”ãƒ¼\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # ã‚¹ãƒ¬ãƒƒãƒ‰ãƒ—ãƒ¼ãƒ«ã‚’ä½¿ã£ã¦ä¸¦åˆ—å‡¦ç†\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # ä¸¦åˆ—ã«æ¨è«–ã‚’å®Ÿè¡Œ\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # çµæœã‚’çµåˆ\n",
    "    # TTAã®çµæœã¯é †åºãŒé€†ã§ã‚ã‚‹ã“ã¨ã«æ³¨æ„\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  # TTAã®ç¢ºç‡ã‚’å–å¾—\n",
    "    # å…ƒã®çµæœã¨TTAã®çµæœã‚’å¹³å‡åŒ–ã™ã‚‹\n",
    "    proba = (proba + tta_proba) / 2  # ç¢ºç‡ã‚’å¹³å‡åŒ–\n",
    "\n",
    "print(f\"çµŒéæ™‚é–“: {time.time() - st}\")  # çµŒéæ™‚é–“ã‚’è¡¨ç¤º"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef779c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "display(submission_df)  # æå‡ºãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:33.139089Z",
     "iopub.status.busy": "2024-07-09T04:24:33.138799Z",
     "iopub.status.idle": "2024-07-09T04:24:33.163375Z",
     "shell.execute_reply": "2024-07-09T04:24:33.162496Z",
     "shell.execute_reply.started": "2024-07-09T04:24:33.139066Z"
    },
    "papermill": {
     "duration": 0.03061,
     "end_time": "2024-07-01T02:59:57.532498",
     "exception": false,
     "start_time": "2024-07-01T02:59:57.501888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Aã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«Model Bã®å‹ã¡ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«å¼•ãåˆ†ã‘ã®ç¢ºç‡ã‚’è¿½åŠ \n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—\n",
    "display(submission_df)  # æå‡ºãƒ‡ãƒ¼ã‚¿ã‚’è¡¨ç¤º"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.513308,
   "end_time": "2024-07-01T03:00:01.146998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-01T02:56:34.63369",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b341b06afc40599e50c9c1ce88be20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07273d2112d649ffbea2991a6a79df98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "142be9e5949c44fabd0370c6df1203d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d31276fcd44350a50d1c561c13e3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1df41ee456cd4fa78a23f7ea2fded110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28cb0aaaf9d24d3d857d224850f62f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd",
       "value": 4
      }
     },
     "3974a6c7f61d4f4582a8e4fdf4c9976c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3daebd55184a4a9982813dc1ac948f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "411187a29c544ebbb425a06b0dfae7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "475dd481f05a46c1908b9f781ae1afa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075",
        "IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b",
        "IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"
       ],
       "layout": "IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"
      }
     },
     "74d46aef6d8949c584024a6e7bb4f06c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c5128ae1d114334b30f2e1013062b26",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_03b341b06afc40599e50c9c1ce88be20",
       "value": " 4/4 [01:30&lt;00:00, 18.30s/it]"
      }
     },
     "846776c5dc1f44bc9cf2e3d394e8ba48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e8e3620620b445eb1d0286befa13278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4",
        "IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe",
        "IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"
       ],
       "layout": "IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"
      }
     },
     "9c5128ae1d114334b30f2e1013062b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7c6588ad13549ae958237ca8e3af9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c23e125e5c3e4cee844bd057453c7aca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_07273d2112d649ffbea2991a6a79df98",
       "value": " 4/4 [00:13&lt;00:00,  2.76s/it]"
      }
     },
     "cb2874dbe9904c07a254eb33d6f0ecfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0043cb27ac54061b85f9b3886954314",
       "value": 4
      }
     },
     "d0043cb27ac54061b85f9b3886954314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd4aa3639e4a46e7a3861fa3dbd5a31b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7361623d5e1491c888080ee4fb8bfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0cc9ad10c3d4a63bd03716995531022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f43e003b1f2e4367800ba2bad74c7075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db",
       "placeholder": "â€‹",
       "style": "IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
