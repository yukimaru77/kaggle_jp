{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fed8b55b",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、Kaggleのコンペティション「LMSYS - Chatbot Arena」において、ユーザーの好みに基づくチャットボット応答の優劣を予測するための推論プロセスの最適化を目指したものです。具体的には、LLM（大規模言語モデル）であるLlamaを使用し、応答確率を計算する際の推論速度を38%向上させています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "ノートブックは、主に次の問題に焦点を当てています:\n",
    "1. **推論時間の短縮**: 元のスクリプトの65分から、40分に推論時間を短縮することに成功。\n",
    "2. **動的パディングの実装**: 入力データの長さに基づいてバッチ内でパディングを動的に適用し、冗長なパディングを減少させています。\n",
    "3. **入力シーケンスの最大長の設定**: モデルが長い入力に対処できるように、`max_length`を1024から2048に拡張。\n",
    "\n",
    "### 使用している手法・ライブラリ\n",
    "- **フレームワーク**: PyTorchが使用され、特にGPUリソースを活用したモデルの推論が行われています。\n",
    "- **Transformersライブラリ**: Hugging FaceのTransformersライブラリを利用してLLMを読み込み、トークナイズ、モデルの推論を実施。\n",
    "- **テストタイム拡張（TTA）**: モデルの出力を改善するために、異なる応答の順序を入れ替える手法も試されていますが、効果は限定的でした。\n",
    "- **メモリ効率の良い注意機構**: メモリ使用量を削減するために、Memory-Efficient Attentionを有効化しています。\n",
    "\n",
    "### 実装の流れ\n",
    "1. **データの準備**: テストデータをトークナイズし、整形します。\n",
    "2. **モデルの設定と読み込み**: 複数のGPUにモデルを読み込み、LoRA（Low-Rank Adaptation）による微調整を設定します。\n",
    "3. **推論プロセス**: データフレームをバッチ処理し、各モデルによる出力を計算し、結果を結合して最終的な確率を算出します。\n",
    "4. **結果の保存**: モデルによる勝者の確率を提出用CSVファイルに保存します。\n",
    "\n",
    "このノートブックは、チャットボットの応答を評価するための効率的な推論手法の実践的なケーススタディを示しており、その中で最善のプラクティスが紹介されています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ccaa45",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebookの内容から初心者がつまずきそうな専門用語について簡単に解説します。特に、あまり一般的でない用語やこのノートブック特有のドメイン知識に焦点を当てて説明します。\n",
    "\n",
    "1. **動的パディング**\n",
    "   - すべての入力シーケンスを同じ長さにするために、短いシーケンスにゼロなどのトークンを追加するプロセスです。固定長ではなく、各ミニバッチ内の最長シーケンスの長さに合わせてパディングすることで、無駄な計算を減らし、効率を向上させることが可能です。\n",
    "\n",
    "2. **テストタイム拡張（TTA: Test Time Augmentation）**\n",
    "   - テスト段階でデータを増やす手法で、モデルの予測を改善するために、同一サンプルを異なる方法（ここでは応答の順序の入れ替え）で処理することにより、より多様な予測を得るアプローチです。\n",
    "\n",
    "3. **ソフトマックス**\n",
    "   - 多クラス分類問題で使う活性化関数で、出力を確率として解釈できるように変換します。一つの出力を1にし、他の出力を0から1の間の数値に変換します。このノートでは、モデルから得られたロジット（raw scores）に対して適用されています。\n",
    "\n",
    "4. **メモリ効率の良い注意機構**\n",
    "   - 大規模モデルを扱う際にメモリ消費を抑えるための特別な設計の注意機構です。通常の注意機構は計算が重くなるため、特に大規模トークン数に対して効率的な方法が求められます。この機構では、メモリの使用を最小限に抑え、計算時間を短縮します。\n",
    "\n",
    "5. **LoRA（Low-Rank Adaptation）**\n",
    "   - モデルのパラメータをカスタマイズするための手法で、全体の重みを変更するのではなく、少数の追加のパラメータを用いてモデルをファインチューニングする方法です。元のモデルのパラメータが固定され、新しいタスクに特化した軽量の適応を可能にします。\n",
    "\n",
    "6. **バイアス**\n",
    "   - 機械学習モデルで、学習や推論が特定の方向に偏ることを指します。ここではバイアスの設定を指しており、特定の学習条件や目的に基づいて調整されます。\n",
    "\n",
    "7. **トークナイズ**\n",
    "   - テキストデータをモデルが理解できる形式（トークン）に変換するプロセスです。トークンは単語やサブワードなど、意味のある単位であり、それにより自然言語処理（NLP）モデルがテキストを処理できるようになります。\n",
    "\n",
    "8. **最長の長さでパディング（padding=\"longest\"）**\n",
    "   - ミニバッチ内の最も長いシーケンスに応じて他のシーケンスをパディングする方法です。バッチ全体の効率を向上させるために選ばれることが多いです。\n",
    "\n",
    "これらの説明は、初心者が踏まえておくべき用語の理解を助け、特にこのノートブックにおける重要な概念が良く分かる助けになるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a18f27",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## 🦙🦙🦙 What this notebook is\n",
    "This notebook is made upon [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) by @kishanvavdara. If you haven't checked the linked notebook I highly recommend you to check and upvote.\n",
    "I made a few improvements upon @kishanvavdara's work:\n",
    "\n",
    "### 38% faster inference\n",
    "Inference time using the first 10k samples in the training set takes 40 mins using this script (without TTA) while the original script takes 65 mins, which is 38% faster without any degradation in accuracy. I mainly added two things:\n",
    "\n",
    "#### 1. Dynamic padding\n",
    "Instead of padding all the inputs to a fixed length in advance, padding is applied on-the-fly up to the longest sequence in each mini-batch.\n",
    "\n",
    "#### 2. Sort the test data by input length\n",
    "To take full advantage of dynamic padding, the test data is sorted by input length. This way, inputs in each mini-batch have more or less same length to reduce the redundant padding.\n",
    "\n",
    "### Longer input sequence\n",
    "Although 99% of the training data falls within 1024, the rest 1% are not. Besides, test set may have more long sequences, so I suppose it's safer to make `max_length` as long as possible.\n",
    "Changing `max_length` from 1024 to 1280 improved LB from 0.989 to 0.983.\n",
    "\n",
    "## Things I have tried but didn't work\n",
    "\n",
    "### Test Time Augmentation (TTA)\n",
    "I tried a simple TTA which swaps the order of response_a and response_b. Note that this will increase the inference time by 2x as model is called twice per sample.\n",
    "We can average the two softmax probabilities or average the two logits and then compute softmax probability. Alghouth both approaches didn't improve LB, averaging softmax performed better.\n",
    "TTA will increase the inference time 2x as model is called twice per sample. Submission finished within 9 hours with `max_length=1280` and TTA enabled thanks to the efficient inference.\n",
    "\n",
    "### Truncate each input\n",
    "The original implementation truncates the concatenated sequence i.e. prompt + response_a + response_b. Naively applying truncation may end up producing prompt only input as some (though rare) prompt is longer than 1280 tokens, then the model has no way but randomly guessing the winner.\n",
    "I tried to truncate each input to a fixed length first and then concatenate the three. But it didn't improve LB.\n",
    "\n",
    "## 🆕 Update in version 4\n",
    "The efficient inference gives us enough time to increase the input sequence length, so I changed `max_length` to 2048 while mini-batch size is reduced to 4 from 8.\n",
    "In addition, I enabled [Memory-Efficient Attention](https://github.com/facebookresearch/xformers) to reduce memory usage.\n",
    "This improved LB from 0.983 to 0.979 and submission still takes less then 4 hours without TTA.\n",
    "We can go even longer by reducing mini-batch size to 1 but I haven't tested yet.\n",
    "\n",
    "# Import libs\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 🦙🦙🦙 このノートブックの内容\n",
    "このノートブックは、@kishanvavdaraによる[Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)を基に作成されています。リンク先のノートブックをまだ確認していない場合は、ぜひチェックして評価をお願いします。\n",
    "私は、@kishanvavdaraの作業をもとにいくつかの改善を加えました：\n",
    "\n",
    "### 38%速い推論\n",
    "このスクリプトを使って、トレーニングセットの最初の10,000サンプルに対する推論時間は40分で、一方で元のスクリプトは65分かかるため、精度に影響を与えずに38%速くなっています。主に次の2つを追加しました：\n",
    "\n",
    "#### 1. 動的パディング\n",
    "すべての入力を事前に固定長にパディングする代わりに、各ミニバッチ内の最長シーケンスまで動的にパディングが適用されます。\n",
    "\n",
    "#### 2. テストデータを入力長でソート\n",
    "動的パディングの利点を最大限に活かすため、テストデータは入力長でソートされます。これにより、各ミニバッチ内の入力がほぼ同じ長さになり、冗長なパディングを減らすことができます。\n",
    "\n",
    "### より長い入力シーケンス\n",
    "訓練データの99%は1024以内に収まっていますが、残りの1%は収まっていません。さらに、テストセットにはより長いシーケンスが含まれる可能性があるため、`max_length`は可能な限り長く設定する方が安全だと思います。\n",
    "`max_length`を1024から1280に変更することでLBが0.989から0.983に改善されました。\n",
    "\n",
    "## 試してみたが効果がなかったこと\n",
    "\n",
    "### テストタイム拡張（TTA）\n",
    "私は、response_aとresponse_bの順序を入れ替えるシンプルなTTAを試しました。これにより、サンプルごとにモデルが2回呼び出されるため推論時間が2倍になります。\n",
    "二つのソフトマックス確率を平均化するか、二つのロジットを平均化してからソフトマックス確率を計算できます。両方のアプローチではLBが改善されませんでしたが、ソフトマックスの平均化がより良い結果を示しました。\n",
    "TTAはサンプルごとにモデルを2回呼び出すため、推論時間が2倍になります。`max_length=1280`とTTAを有効にして提出が完了しましたが、効率的な推論により9時間以内に収まりました。\n",
    "\n",
    "### 各入力を切り詰める\n",
    "元の実装では、プロンプト + response_a + response_bという連結シーケンスが切り詰められます。直截的な切り詰めを適用すると、一部の（稀ではありますが）プロンプトが1280トークンを超えるため、モデルは勝者をランダムに推測するしかなくなります。\n",
    "私は、最初に各入力を固定長に切り詰めてから、三つを連結しようとしましたが、LBは改善されませんでした。\n",
    "\n",
    "## 🆕 バージョン4の更新\n",
    "効果的な推論のおかげで入力シーケンスの長さを増やす時間が十分にあるため、`max_length`を2048に変更しました。また、ミニバッチサイズは8から4に減少させました。\n",
    "さらに、メモリ使用量を減らすために[Memory-Efficient Attention](https://github.com/facebookresearch/xformers)を有効にしました。\n",
    "これによりLBが0.983から0.979に改善され、提出はTTAなしで4時間未満で済みました。\n",
    "ミニバッチサイズを1に減らすことでさらに長くできますが、まだテストしていません。\n",
    "\n",
    "# ライブラリのインポート\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb856f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:21:10.180351Z",
     "iopub.status.busy": "2024-07-09T04:21:10.17999Z",
     "iopub.status.idle": "2024-07-09T04:22:04.09703Z",
     "shell.execute_reply": "2024-07-09T04:22:04.095937Z",
     "shell.execute_reply.started": "2024-07-09T04:21:10.180321Z"
    },
    "papermill": {
     "duration": 53.686843,
     "end_time": "2024-07-01T02:57:31.530755",
     "exception": false,
     "start_time": "2024-07-01T02:56:37.843912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/\n",
    "!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a214c03",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:04.099619Z",
     "iopub.status.busy": "2024-07-09T04:22:04.099318Z",
     "iopub.status.idle": "2024-07-09T04:22:21.661753Z",
     "shell.execute_reply": "2024-07-09T04:22:21.660958Z",
     "shell.execute_reply.started": "2024-07-09T04:22:04.099589Z"
    },
    "papermill": {
     "duration": 21.138547,
     "end_time": "2024-07-01T02:57:52.676238",
     "exception": false,
     "start_time": "2024-07-01T02:57:31.537691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d1c134",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "According to the pytorch [documentation](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention), `scaled_dot_product_attention` automatically select the most optimal implementation from:\n",
    "1. Flash Attention\n",
    "2. Memory Efficient Attention\n",
    "3. A PyTorch (naive) implementation\n",
    "\n",
    "By default, all of those are enabled but we can also manually enable/disable certain backends.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "PyTorchの[ドキュメント](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention)によると、`scaled_dot_product_attention`は、自動的に最も最適な実装を選択します：\n",
    "1. Flash Attention\n",
    "2. メモリ効率の良い注意機構\n",
    "3. PyTorchの（ナイーブな）実装\n",
    "\n",
    "デフォルトでは、これらすべてが有効ですが、特定のバックエンドを手動で有効化/無効化することもできます。\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25e6fbe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Doesn't have any effect as Flash Attention does not support T4/P100\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "assert torch.cuda.device_count() == 2, \"申し訳ありませんが、マルチGPUが必要です！\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Flash AttentionはT4/P100をサポートしていないため、効果はありません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.663342Z",
     "iopub.status.busy": "2024-07-09T04:22:21.662808Z",
     "iopub.status.idle": "2024-07-09T04:22:21.668153Z",
     "shell.execute_reply": "2024-07-09T04:22:21.667165Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.663315Z"
    },
    "papermill": {
     "duration": 0.014661,
     "end_time": "2024-07-01T02:57:52.698559",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.683898",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "assert torch.cuda.device_count() == 2, \"申し訳ありませんが、マルチGPUが必要です！\"\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)  # Flash AttentionはT4/P100をサポートしていないため、効果はありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9ad0dd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'\n",
    "    max_length = 2048\n",
    "    batch_size = 4\n",
    "    device = torch.device(\"cuda\")    \n",
    "    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>\n",
    "    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input\n",
    "\n",
    "cfg = Config()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルの名前\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'  # 重みのパス\n",
    "    max_length = 2048  # 最大長\n",
    "    batch_size = 4  # バッチサイズ\n",
    "    device = torch.device(\"cuda\")  # 使用するデバイス\n",
    "    tta = False  # テストタイム拡張。<prompt>-<model-bの応答>-<model-aの応答>\n",
    "    spread_max_length = False  # max_length//3を各入力に適用するか、連結入力にmax_lengthを適用するか\n",
    "\n",
    "cfg = Config()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.669917Z",
     "iopub.status.busy": "2024-07-09T04:22:21.669556Z",
     "iopub.status.idle": "2024-07-09T04:22:21.695408Z",
     "shell.execute_reply": "2024-07-09T04:22:21.694637Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.669883Z"
    },
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-07-01T02:57:52.720706",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.705889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルの名前\n",
    "    weights_path = '/kaggle/input/lmsys-model/model'  # 重みのパス\n",
    "    max_length = 2048  # 最大長\n",
    "    batch_size = 4  # バッチサイズ\n",
    "    device = torch.device(\"cuda\")  # 使用するデバイス\n",
    "    tta = False  # テストタイム拡張。<prompt>-<model-bの応答>-<model-aの応答>\n",
    "    spread_max_length = False  # max_length//3を各入力に適用するか、連結入力にmax_lengthを適用するか\n",
    "\n",
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2cf628",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Prepare Data \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# データの準備 \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ddb7f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# concatenate strings in list\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "\n",
    "display(test.head(5))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 文字列の先頭と末尾の角括弧を取り除く\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文字列を分割してトリミング\n",
    "    return  ' '.join(sentences)  # 文を結合して返す\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'列を処理\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'列を処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'列を処理\n",
    "\n",
    "display(test.head(5))  # 上位5行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.698075Z",
     "iopub.status.busy": "2024-07-09T04:22:21.697797Z",
     "iopub.status.idle": "2024-07-09T04:22:21.73015Z",
     "shell.execute_reply": "2024-07-09T04:22:21.729264Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.698051Z"
    },
    "papermill": {
     "duration": 0.040947,
     "end_time": "2024-07-01T02:57:52.781962",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.741015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "\n",
    "# リスト内の文字列を連結する関数\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 文字列の先頭と末尾の角括弧を取り除く\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 文字列を分割してトリミング\n",
    "    return  ' '.join(sentences)  # 文を結合して返す\n",
    "\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'列を処理\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'列を処理\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'列を処理\n",
    "\n",
    "display(test.head(5))  # 上位5行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a1a122",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トークナイズ\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f4717a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]\n",
    "    if spread_max_length:\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]\n",
    "    else:\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)\n",
    "        input_ids = tokenized.input_ids\n",
    "        attention_mask = tokenized.attention_mask\n",
    "    return input_ids, attention_mask\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]  # ユーザーのプロンプトを整形\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]  # Model Aの応答を整形\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]  # Model Bの応答を整形\n",
    "    if spread_max_length:\n",
    "        # max_lengthを各入力に均等に分配する場合\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 入力IDの生成\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # 注意マスクの生成\n",
    "    else:\n",
    "        # max_lengthを全体の入力に適用する場合\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイザでトークナイズ\n",
    "        input_ids = tokenized.input_ids  # 入力IDを取得\n",
    "        attention_mask = tokenized.attention_mask  # 注意マスクを取得\n",
    "    return input_ids, attention_mask  # 入力IDと注意マスクを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.731569Z",
     "iopub.status.busy": "2024-07-09T04:22:21.731292Z",
     "iopub.status.idle": "2024-07-09T04:22:21.741316Z",
     "shell.execute_reply": "2024-07-09T04:22:21.740313Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.731544Z"
    },
    "papermill": {
     "duration": 0.017982,
     "end_time": "2024-07-01T02:57:52.821282",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.8033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(\n",
    "    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length\n",
    "):\n",
    "    prompt = [\"User prompt: \" + p for p in prompt]  # ユーザーのプロンプトを整形\n",
    "    response_a = [\"\\n\\nModel A :\\n\" + r_a for r_a in response_a]  # Model Aの応答を整形\n",
    "    response_b = [\"\\n\\n--------\\n\\nModel B:\\n\" + r_b for r_b in response_b]  # Model Bの応答を整形\n",
    "    if spread_max_length:\n",
    "        # max_lengthを各入力に均等に分配する場合\n",
    "        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids\n",
    "        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 入力IDの生成\n",
    "        attention_mask = [[1]* len(i) for i in input_ids]  # 注意マスクの生成\n",
    "    else:\n",
    "        # max_lengthを全体の入力に適用する場合\n",
    "        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]\n",
    "        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイザでトークナイズ\n",
    "        input_ids = tokenized.input_ids  # 入力IDを取得\n",
    "        attention_mask = tokenized.attention_mask  # 注意マスクを取得\n",
    "    return input_ids, attention_mask  # 入力IDと注意マスクを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6ed075",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]\n",
    "# swap response_a & response_b\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーを読み込み\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]  # ID列を作成\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # トークナイズを適用\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # 入力IDの長さを計算\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]  # ID列を作成\n",
    "# response_aとresponse_bを入れ替える\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # 入力IDの長さを計算\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:21.742873Z",
     "iopub.status.busy": "2024-07-09T04:22:21.742567Z",
     "iopub.status.idle": "2024-07-09T04:22:22.300441Z",
     "shell.execute_reply": "2024-07-09T04:22:22.299465Z",
     "shell.execute_reply.started": "2024-07-09T04:22:21.742847Z"
    },
    "papermill": {
     "duration": 0.596117,
     "end_time": "2024-07-01T02:57:53.425111",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.828994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーを読み込み\n",
    "\n",
    "data = pd.DataFrame()\n",
    "data[\"id\"] = test[\"id\"]  # ID列を作成\n",
    "data[\"input_ids\"], data[\"attention_mask\"] = tokenize(tokenizer, test[\"prompt\"], test[\"response_a\"], test[\"response_b\"])  # トークナイズを適用\n",
    "data[\"length\"] = data[\"input_ids\"].apply(len)  # 入力IDの長さを計算\n",
    "\n",
    "aug_data = pd.DataFrame()\n",
    "aug_data[\"id\"] = test[\"id\"]  # ID列を作成\n",
    "# response_aとresponse_bを入れ替える\n",
    "aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test[\"prompt\"], test[\"response_b\"], test[\"response_a\"])\n",
    "aug_data[\"length\"] = aug_data[\"input_ids\"].apply(len)  # 入力IDの長さを計算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fadcc6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 最初の入力IDをデコードして表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.302245Z",
     "iopub.status.busy": "2024-07-09T04:22:22.301819Z",
     "iopub.status.idle": "2024-07-09T04:22:22.308508Z",
     "shell.execute_reply": "2024-07-09T04:22:22.307457Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.302186Z"
    },
    "papermill": {
     "duration": 0.015987,
     "end_time": "2024-07-01T02:57:53.448552",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.432565",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 最初の入力IDをデコードして表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e41727d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  # 最初の入れ替えた入力IDをデコードして表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.310334Z",
     "iopub.status.busy": "2024-07-09T04:22:22.309948Z",
     "iopub.status.idle": "2024-07-09T04:22:22.319687Z",
     "shell.execute_reply": "2024-07-09T04:22:22.318833Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.310297Z"
    },
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-07-01T02:57:53.472007",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.456043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(tokenizer.decode(aug_data[\"input_ids\"][0]))  # 最初の入れ替えた入力IDをデコードして表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb2cd8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load model \n",
    "We load 1 model on each gpu.  \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# モデルの読み込み \n",
    "各GPUに1モデルを読み込みます。  \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb82e0",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# BitsAndBytes configuration\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    bnb_8bit_compute_dtype=torch.float16,\n",
    "    bnb_8bit_use_double_quant=False,\n",
    ")\n",
    "\n",
    "# Load base model on GPU 0\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:0')\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Load base model on GPU 1\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map='cuda:1')\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # モデルを8ビットで読み込む設定\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定\n",
    "    bnb_8bit_use_double_quant=False,  # 二重量子化を無効にする設定\n",
    ")\n",
    "\n",
    "# GPU 0にベースモデルを読み込む\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:0')  # GPU 0に割り当て\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# GPU 1にベースモデルを読み込む\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:1')  # GPU 1に割り当て\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:22:22.320895Z",
     "iopub.status.busy": "2024-07-09T04:22:22.320635Z",
     "iopub.status.idle": "2024-07-09T04:24:16.456934Z",
     "shell.execute_reply": "2024-07-09T04:24:16.456022Z",
     "shell.execute_reply.started": "2024-07-09T04:22:22.320873Z"
    },
    "papermill": {
     "duration": 105.076557,
     "end_time": "2024-07-01T02:59:38.570536",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.493979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# BitsAndBytesの設定\n",
    "bnb_config =  BitsAndBytesConfig(\n",
    "    load_in_8bit=True,  # モデルを8ビットで読み込む設定\n",
    "    bnb_8bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定\n",
    "    bnb_8bit_use_double_quant=False,  # 二重量子化を無効にする設定\n",
    ")\n",
    "\n",
    "# GPU 0にベースモデルを読み込む\n",
    "device_0 = torch.device('cuda:0')\n",
    "base_model_0 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:0')  # GPU 0に割り当て\n",
    "base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定\n",
    "\n",
    "# GPU 1にベースモデルを読み込む\n",
    "device_1 = torch.device('cuda:1')\n",
    "base_model_1 = LlamaForSequenceClassification.from_pretrained(\n",
    "    cfg.model_name,\n",
    "    num_labels=3,\n",
    "    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定\n",
    "    quantization_config=bnb_config,  # 量子化設定を適用\n",
    "    device_map='cuda:1')  # GPU 1に割り当て\n",
    "base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88f01e3",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load weights \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 重みの読み込み \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70e6f0c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.10,\n",
    "    bias='none',\n",
    "    inference_mode=True,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules=['o_proj', 'v_proj']\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# LoRAの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # 縮小次元\n",
    "    lora_alpha=32,  # LoRAのスケーリング係数\n",
    "    lora_dropout=0.10,  # ドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    "    target_modules=['o_proj', 'v_proj']  # 対象モジュール\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:16.458567Z",
     "iopub.status.busy": "2024-07-09T04:24:16.458232Z",
     "iopub.status.idle": "2024-07-09T04:24:16.463705Z",
     "shell.execute_reply": "2024-07-09T04:24:16.462773Z",
     "shell.execute_reply.started": "2024-07-09T04:24:16.45854Z"
    },
    "papermill": {
     "duration": 0.0162,
     "end_time": "2024-07-01T02:59:38.610906",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.594706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# LoRAの設定\n",
    "peft_config = LoraConfig(\n",
    "    r=16,  # 縮小次元\n",
    "    lora_alpha=32,  # LoRAのスケーリング係数\n",
    "    lora_dropout=0.10,  # ドロップアウト率\n",
    "    bias='none',  # バイアスの設定\n",
    "    inference_mode=True,  # 推論モードを有効にする\n",
    "    task_type=TaskType.SEQ_CLS,  # タスクの種類\n",
    "    target_modules=['o_proj', 'v_proj']  # 対象モジュール\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e967fe",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Get peft\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0) \n",
    "# Load weights\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_1.eval()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# PEFTの取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTモデルを取得してGPU 0に配置\n",
    "# 重みを読み込む\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTモデルを取得してGPU 1に配置\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを読み込む\n",
    "model_1.eval()  # 評価モードに設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:16.465891Z",
     "iopub.status.busy": "2024-07-09T04:24:16.464949Z",
     "iopub.status.idle": "2024-07-09T04:24:29.793371Z",
     "shell.execute_reply": "2024-07-09T04:24:29.792506Z",
     "shell.execute_reply.started": "2024-07-09T04:24:16.465854Z"
    },
    "papermill": {
     "duration": 13.701042,
     "end_time": "2024-07-01T02:59:52.320278",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.619236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PEFTの取得\n",
    "model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTモデルを取得してGPU 0に配置\n",
    "# 重みを読み込む\n",
    "model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)\n",
    "model_0.eval()  # 評価モードに設定\n",
    "\n",
    "model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTモデルを取得してGPU 1に配置\n",
    "model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを読み込む\n",
    "model_1.eval()  # 評価モードに設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6867db",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Trainable Parameters\n",
    "model_0.print_trainable_parameters()\n",
    "model_1.print_trainable_parameters()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 学習可能なパラメータ\n",
    "model_0.print_trainable_parameters()  # モデル0の学習可能なパラメータを表示\n",
    "model_1.print_trainable_parameters()  # モデル1の学習可能なパラメータを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.794793Z",
     "iopub.status.busy": "2024-07-09T04:24:29.794518Z",
     "iopub.status.idle": "2024-07-09T04:24:29.807888Z",
     "shell.execute_reply": "2024-07-09T04:24:29.806996Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.79477Z"
    },
    "papermill": {
     "duration": 0.026729,
     "end_time": "2024-07-01T02:59:52.356012",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.329283",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 学習可能なパラメータ\n",
    "model_0.print_trainable_parameters()  # モデル0の学習可能なパラメータを表示\n",
    "model_1.print_trainable_parameters()  # モデル1の学習可能なパラメータを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1048ba",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Inference\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 推論\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b04b69",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))\n",
    "        proba = outputs.logits.softmax(-1).cpu()\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝ちと引き分けのカウントを初期化\n",
    "    \n",
    "    # バッチサイズに基づいてデータを分割して推論を行う\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを決定\n",
    "        tmp = df.iloc[start_idx:end_idx]  # データフレームのサブセットを取得\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストに変換\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # 注意マスクをリストに変換\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # 最長の長さでパディング\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchテンソルとして返す\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # モデルに入力を渡して出力を計算\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックス関数で処理\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())  # Model Aの勝ちの確率を追加\n",
    "        b_win.extend(proba[:, 1].tolist())  # Model Bの勝ちの確率を追加\n",
    "        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # データフレームにModel Aの勝ちの確率を追加\n",
    "    df[\"winner_model_b\"] = b_win  # データフレームにModel Bの勝ちの確率を追加\n",
    "    df[\"winner_tie\"] = tie  # データフレームに引き分けの確率を追加\n",
    "    \n",
    "    return df  # 結果のデータフレームを返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.811288Z",
     "iopub.status.busy": "2024-07-09T04:24:29.810984Z",
     "iopub.status.idle": "2024-07-09T04:24:29.821659Z",
     "shell.execute_reply": "2024-07-09T04:24:29.820866Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.811264Z"
    },
    "papermill": {
     "duration": 0.021078,
     "end_time": "2024-07-01T02:59:52.402973",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.381895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝ちと引き分けのカウントを初期化\n",
    "    \n",
    "    # バッチサイズに基づいてデータを分割して推論を行う\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを決定\n",
    "        tmp = df.iloc[start_idx:end_idx]  # データフレームのサブセットを取得\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # 入力IDをリストに変換\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # 注意マスクをリストに変換\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",  # 最長の長さでパディング\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",  # PyTorchテンソルとして返す\n",
    "        )\n",
    "        outputs = model(**inputs.to(device))  # モデルに入力を渡して出力を計算\n",
    "        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックス関数で処理\n",
    "        \n",
    "        a_win.extend(proba[:, 0].tolist())  # Model Aの勝ちの確率を追加\n",
    "        b_win.extend(proba[:, 1].tolist())  # Model Bの勝ちの確率を追加\n",
    "        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加\n",
    "    \n",
    "    df[\"winner_model_a\"] = a_win  # データフレームにModel Aの勝ちの確率を追加\n",
    "    df[\"winner_model_b\"] = b_win  # データフレームにModel Bの勝ちの確率を追加\n",
    "    df[\"winner_tie\"] = tie  # データフレームに引き分けの確率を追加\n",
    "    \n",
    "    return df  # 結果のデータフレームを返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316608cd",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "# sort by input length to fully leverage dynaminc padding\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# the total #tokens in sub_1 and sub_2 should be more or less the same\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "# 动的パディングを最大限に活用するために入力長でソート\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# sub_1とsub_2のトークン数がほぼ同じである必要があります\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝者の確率を取得\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:29.822956Z",
     "iopub.status.busy": "2024-07-09T04:24:29.822692Z",
     "iopub.status.idle": "2024-07-09T04:24:33.127581Z",
     "shell.execute_reply": "2024-07-09T04:24:33.126609Z",
     "shell.execute_reply.started": "2024-07-09T04:24:29.822935Z"
    },
    "papermill": {
     "duration": 3.316613,
     "end_time": "2024-07-01T02:59:55.727834",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.411221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "# 动的パディングを最大限に活用するために入力長でソート\n",
    "data = data.sort_values(\"length\", ascending=False)\n",
    "# sub_1とsub_2のトークン数がほぼ同じである必要があります\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝者の確率を取得\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1c623f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # sort by input length to boost speed\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)\n",
    "    # recall TTA's order is flipped\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values \n",
    "    # average original result and TTA result.\n",
    "    proba = (proba + tta_proba) / 2\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # 入力長でソートして速度を向上させる\n",
    "    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー\n",
    "    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "    # TTAの結果は順序が逆であることに注意\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  # TTAの確率を取得\n",
    "    # 元の結果とTTAの結果を平均化する\n",
    "    proba = (proba + tta_proba) / 2  # 確率を平均化\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:33.129319Z",
     "iopub.status.busy": "2024-07-09T04:24:33.128932Z",
     "iopub.status.idle": "2024-07-09T04:24:33.137663Z",
     "shell.execute_reply": "2024-07-09T04:24:33.136587Z",
     "shell.execute_reply.started": "2024-07-09T04:24:33.129289Z"
    },
    "papermill": {
     "duration": 1.755381,
     "end_time": "2024-07-01T02:59:57.492377",
     "exception": false,
     "start_time": "2024-07-01T02:59:55.736996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "\n",
    "if cfg.tta:\n",
    "    data = aug_data.sort_values(\"length\", ascending=False)  # 入力長でソートして速度を向上させる\n",
    "    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー\n",
    "    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理\n",
    "        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行\n",
    "\n",
    "    tta_result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "    # TTAの結果は順序が逆であることに注意\n",
    "    tta_proba = tta_result_df[[\"winner_model_b\", \"winner_model_a\", \"winner_tie\"]].values  # TTAの確率を取得\n",
    "    # 元の結果とTTAの結果を平均化する\n",
    "    proba = (proba + tta_proba) / 2  # 確率を平均化\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbef779c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # データフレームにModel Aの勝ちの確率を追加\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # データフレームにModel Bの勝ちの確率を追加\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # データフレームに引き分けの確率を追加\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルに書き出し\n",
    "display(submission_df)  # 提出データを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-09T04:24:33.139089Z",
     "iopub.status.busy": "2024-07-09T04:24:33.138799Z",
     "iopub.status.idle": "2024-07-09T04:24:33.163375Z",
     "shell.execute_reply": "2024-07-09T04:24:33.162496Z",
     "shell.execute_reply.started": "2024-07-09T04:24:33.139066Z"
    },
    "papermill": {
     "duration": 0.03061,
     "end_time": "2024-07-01T02:59:57.532498",
     "exception": false,
     "start_time": "2024-07-01T02:59:57.501888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # データフレームにModel Aの勝ちの確率を追加\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # データフレームにModel Bの勝ちの確率を追加\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # データフレームに引き分けの確率を追加\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルに書き出し\n",
    "display(submission_df)  # 提出データを表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5034873,
     "sourceId": 8449074,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 148861315,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 28083,
     "sourceId": 33551,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.513308,
   "end_time": "2024-07-01T03:00:01.146998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-01T02:56:34.63369",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b341b06afc40599e50c9c1ce88be20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07273d2112d649ffbea2991a6a79df98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "142be9e5949c44fabd0370c6df1203d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d31276fcd44350a50d1c561c13e3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c",
       "placeholder": "​",
       "style": "IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1df41ee456cd4fa78a23f7ea2fded110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28cb0aaaf9d24d3d857d224850f62f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd",
       "value": 4
      }
     },
     "3974a6c7f61d4f4582a8e4fdf4c9976c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3daebd55184a4a9982813dc1ac948f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "411187a29c544ebbb425a06b0dfae7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "475dd481f05a46c1908b9f781ae1afa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075",
        "IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b",
        "IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"
       ],
       "layout": "IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"
      }
     },
     "74d46aef6d8949c584024a6e7bb4f06c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c5128ae1d114334b30f2e1013062b26",
       "placeholder": "​",
       "style": "IPY_MODEL_03b341b06afc40599e50c9c1ce88be20",
       "value": " 4/4 [01:30&lt;00:00, 18.30s/it]"
      }
     },
     "846776c5dc1f44bc9cf2e3d394e8ba48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e8e3620620b445eb1d0286befa13278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4",
        "IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe",
        "IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"
       ],
       "layout": "IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"
      }
     },
     "9c5128ae1d114334b30f2e1013062b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7c6588ad13549ae958237ca8e3af9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c23e125e5c3e4cee844bd057453c7aca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b",
       "placeholder": "​",
       "style": "IPY_MODEL_07273d2112d649ffbea2991a6a79df98",
       "value": " 4/4 [00:13&lt;00:00,  2.76s/it]"
      }
     },
     "cb2874dbe9904c07a254eb33d6f0ecfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0043cb27ac54061b85f9b3886954314",
       "value": 4
      }
     },
     "d0043cb27ac54061b85f9b3886954314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd4aa3639e4a46e7a3861fa3dbd5a31b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7361623d5e1491c888080ee4fb8bfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0cc9ad10c3d4a63bd03716995531022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f43e003b1f2e4367800ba2bad74c7075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db",
       "placeholder": "​",
       "style": "IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
