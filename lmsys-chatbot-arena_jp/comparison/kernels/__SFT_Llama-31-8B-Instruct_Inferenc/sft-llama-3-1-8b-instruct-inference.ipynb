{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa79a600",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、人間の好みを予測するためのモデルを訓練し、その推論を行うプロセスを示しています。\n",
    "\n",
    "### 取り組んでいる問題\n",
    "Notebookは、異なる大規模言語モデル (LLM) によって生成された応答がユーザーによってどのように評価されるかを予測することに取り組んでいます。具体的には、2つのチャットボットの応答のいずれがユーザーに好まれるか（モデルA、モデルB、または引き分け）の確率を予測するためのフレームワークを構築しています。\n",
    "\n",
    "### 使用されている手法\n",
    "1. **トークン化**: ユーザーのプロンプトとチャットボットの応答をトークン化して、モデルが理解できる形式に変換しています。`AutoTokenizer`を使用して、応答を適切にトークン化し、長さ制限を管理します。\n",
    "\n",
    "2. **モデルの設定**: `LlamaForCausalLM`と、特定のタスク用にカスタマイズされた`Llama3ForSFT`というメソッドを利用して、大規模言語モデルのインスタンスを作成しています。\n",
    "\n",
    "3. **並列処理**: GPUを2つ使用して、トレーニングと推論を効率的に行うために、`ThreadPoolExecutor`を使用して推論の並列処理を行っています。\n",
    "\n",
    "4. **ソフトマックス関数の使用**: モデル出力のロジットをソフトマックス関数で正規化し、各モデルの勝確率を得ています。\n",
    "\n",
    "### 使用されているライブラリ\n",
    "- **Transformers**: モデルの構築やトークナイザー管理のために使用されています。\n",
    "- **Datasets**: データフレームを統合し、モデルへの入力データセットを構築するために使用されています。\n",
    "- **Torch**: 深層学習モデルの訓練と推論のために使用されています。特に、`torch.cuda.amp`で自動混合精度を有効化し、GPUメモリを効率的に活用しています。\n",
    "- **Pandas**: データの操作や処理に使用されています。\n",
    "\n",
    "最後に、Notebookは、予測結果を含むCSVファイル（`submission.csv`）として保存し、提出用に整形しています。このノートブックは、事前トレーニングされたモデルを用いた構築・推論手法を示しており、Kaggleのコンペティションルールに従ったデータ提出を実現しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103f8b2",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、Jupyter Notebookの内容に基づく、初心者がつまずきそうな専門用語の簡単な解説です。このリストは、実務経験が少ないが学部レベルの知識を持っている初心者を意識しており、特にマイナーなものや特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **PEFT (Parameter-Efficient Fine-Tuning)**: 大規模なモデルの微調整を、全パラメータの更新を行うことなく、少数のパラメータのみを更新する手法。これにより、モデルの学習時間やメモリ使用量を削減し、高速な適応を目指す。\n",
    "\n",
    "2. **ロジット (Logits)**: モデルの出力であり、通常はソフトマックス関数を介して確率に変換される値。分類問題でのクラスのスコアを表し、最終的な確率を決定する。\n",
    "\n",
    "3. **トークナイザー (Tokenizer)**: テキストを数値表現に変換する役割を持つコンポーネント。モデルが理解できる形式に変換するため、単語や文をトークンに分割する。\n",
    "\n",
    "4. **自動混合精度 (Automatic Mixed Precision)**: モデルの訓練を行なう際に、計算の精度を自動的に変更する手法で、GPUリソースを効率的に使用し、訓練速度を向上させることができる。\n",
    "\n",
    "5. **注意マスク (Attention Mask)**: モデルが処理する際に、どのトークンに注意を払うべきかを制御するためのマスク。通常、パディングされたトークンに対しては注意を払わないよう指定される。\n",
    "\n",
    "6. **アテンション (Attention)**: 自然言語処理における技術で、モデルが文中の特定の単語やフレーズに着目し、それに応じた重みを持たせることで、情報の関連性を高める。\n",
    "\n",
    "7. **Causal Language Model**: 過去のコンテキストから次のトークンを予測するタイプの言語モデル。例えば、自回帰モデルとも呼ばれ、単語の順序に依存して生成する。\n",
    "\n",
    "8. **スパースデータパラメータ (Sparse Data Parameters)**: 特定のニューロンや接続が無効化され、それを求めないことで全体の計算を効率化する方法。モデルの構造がスパース（疎）になることで、メモリや計算資源を節約する。\n",
    "\n",
    "9. **トレーニングスクリプト (Training Script)**: 機械学習モデルをトレーニングするためのコードスニペット。特にパラメータ設定やデータライブラリの使用、手法の選択を含み、実際の学習プロセスを実行する。\n",
    "\n",
    "10. **データコラレーター (Data Collator)**: バッチ処理のためにデータを整形するコンポーネントで、入力データのパディングや整列を自動で処理する。これにより、異なる長さのトークン列を持つデータを正しくバッチにまとめることができる。\n",
    "\n",
    "11. **フラッシュSDP (Flash SDP)**: CUDAライブラリの機能の1つで、メモリ使用を最適化し、計算性能を向上させるために設計されている。\n",
    "\n",
    "これらの用語は、初心者が特にノートブックのコードや手法を理解する際に、壁となり得るものです。リストが参考になれば幸いです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98741b80",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Note\n",
    "- [Training script](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)\n",
    "\n",
    "# Import\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 注意\n",
    "- [トレーニングスクリプト](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a466b573",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-01T19:17:37.872397Z",
     "iopub.status.busy": "2024-08-01T19:17:37.871598Z"
    },
    "papermill": {
     "duration": 53.686843,
     "end_time": "2024-07-01T02:57:31.530755",
     "exception": false,
     "start_time": "2024-07-01T02:56:37.843912",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727\n",
    "!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be436b2d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "assert torch.cuda.device_count() == 2, \"Sorry - multi-GPU required!\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "\n",
    "# メモリ効率の良いSDP（スパースデータパラメータ）を有効化\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "# フラッシュSDPを有効化\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "# 使用するGPUの数が2であることを確認\n",
    "assert torch.cuda.device_count() == 2, \"申し訳ありませんが、マルチGPUが必要です！\"\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 21.138547,
     "end_time": "2024-07-01T02:57:52.676238",
     "exception": false,
     "start_time": "2024-07-01T02:57:31.537691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from threading import Thread\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification\n",
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType \n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from transformers import set_seed\n",
    "\n",
    "# メモリ効率の良いSDP（スパースデータパラメータ）を有効化\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(True)\n",
    "# フラッシュSDPを有効化\n",
    "torch.backends.cuda.enable_flash_sdp(True)\n",
    "# 使用するGPUの数が2であることを確認\n",
    "assert torch.cuda.device_count() == 2, \"申し訳ありませんが、マルチGPUが必要です！\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77ec285",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'\n",
    "WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'\n",
    "MAX_LENGTH = 2400\n",
    "BATCH_SIZE = 2\n",
    "DEVICE = torch.device(\"cuda\")    \n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'  # 使用するモデルの名前\n",
    "WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'          # 重みのパス\n",
    "MAX_LENGTH = 2400   # 最大長さ\n",
    "BATCH_SIZE = 2      # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # デバイスをGPUに設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.014817,
     "end_time": "2024-07-01T02:57:52.720706",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.705889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'  # 使用するモデルの名前\n",
    "WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'          # 重みのパス\n",
    "MAX_LENGTH = 2400   # 最大長さ\n",
    "BATCH_SIZE = 2      # バッチサイズ\n",
    "DEVICE = torch.device(\"cuda\")  # デバイスをGPUに設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef22d1e8",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Prepare Data \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# データの準備\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44fccac",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')\n",
    "def tokenize(example, tokenizer):\n",
    "    prompts = tokenizer(eval(example['prompt'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_a = tokenizer(eval(example['response_a'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_b = tokenizer(eval(example['response_b'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    assert len(prompts) == len(responses_a) == len(responses_b), \"Lengths of prompts, responses_a, and responses_b do not match\"\n",
    "    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]\n",
    "    prompt, response_a, response_b = [], [], []\n",
    "    p_len, a_len, b_len = 0, 0, 0\n",
    "    for p, a, b in zip(prompts, responses_a, responses_b):\n",
    "        prompt.append(p)\n",
    "        response_a.append(a)\n",
    "        response_b.append(b)\n",
    "        p_len += len(p)\n",
    "        a_len += len(a)\n",
    "        a_len += len(b)\n",
    "        if p_len+a_len+b_len > MAX_LENGTH:\n",
    "            break\n",
    "    prompt = [item for sublist in reversed(prompt) for item in sublist]\n",
    "    response_a = [item for sublist in reversed(response_a) for item in sublist]\n",
    "    response_b = [item for sublist in reversed(response_b) for item in sublist]\n",
    "    p_a_b_len = len(prompt) + len(response_a) + len(response_b)\n",
    "    cut_len = p_a_b_len - MAX_LENGTH\n",
    "    if cut_len>0:\n",
    "        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]\n",
    "        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]\n",
    "        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]\n",
    "    prompt = tokenizer('<prompt>: ', add_special_tokens=False)[\"input_ids\"] + prompt\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ', add_special_tokens=False)[\"input_ids\"] + response_a\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ', add_special_tokens=False)[\"input_ids\"] + response_b\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nWhich is the better response for the prompt ? a or b or tie ?\\n\\nAnswer: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    label_token_id = [128250]\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids)*[1]\n",
    "    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVから読み込む\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    # プロンプトと応答をトークン化\n",
    "    prompts = tokenizer(eval(example['prompt'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_a = tokenizer(eval(example['response_a'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_b = tokenizer(eval(example['response_b'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    # プロンプトと応答の長さが一致することを確認\n",
    "    assert len(prompts) == len(responses_a) == len(responses_b), \"プロンプト、応答A、応答Bの長さが一致しません\"\n",
    "\n",
    "    # プロンプトと応答を逆順にする\n",
    "    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]\n",
    "\n",
    "    prompt, response_a, response_b = [], [], []\n",
    "    p_len, a_len, b_len = 0, 0, 0\n",
    "\n",
    "    # プロンプトと応答を長さ制限に基づいて追加\n",
    "    for p, a, b in zip(prompts, responses_a, responses_b):\n",
    "        prompt.append(p)\n",
    "        response_a.append(a)\n",
    "        response_b.append(b)\n",
    "        p_len += len(p)\n",
    "        a_len += len(a)\n",
    "        b_len += len(b)\n",
    "        \n",
    "        if p_len + a_len + b_len > MAX_LENGTH:  # 制限を超えた場合\n",
    "            break\n",
    "\n",
    "    # 順序を戻す\n",
    "    prompt = [item for sublist in reversed(prompt) for item in sublist]\n",
    "    response_a = [item for sublist in reversed(response_a) for item in sublist]\n",
    "    response_b = [item for sublist in reversed(response_b) for item in sublist]\n",
    "\n",
    "    # トークン数を計算\n",
    "    p_a_b_len = len(prompt) + len(response_a) + len(response_b)\n",
    "    cut_len = p_a_b_len - MAX_LENGTH\n",
    "    \n",
    "    # 制限を超えた場合、トークンを削除\n",
    "    if cut_len > 0:\n",
    "        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]\n",
    "        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]\n",
    "        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]\n",
    "\n",
    "    # 特殊トークンを付加\n",
    "    prompt = tokenizer('<prompt>: ', add_special_tokens=False)[\"input_ids\"] + prompt\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ', add_special_tokens=False)[\"input_ids\"] + response_a\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ', add_special_tokens=False)[\"input_ids\"] + response_b\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nどちらの応答がプロンプトに対して良いか？ a、b、またはtie？\\n\\n回答: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    label_token_id = [128250]  # ラベルのトークンID\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.040947,
     "end_time": "2024-07-01T02:57:52.781962",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.741015",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVから読み込む\n",
    "\n",
    "def tokenize(example, tokenizer):\n",
    "    # プロンプトと応答をトークン化\n",
    "    prompts = tokenizer(eval(example['prompt'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_a = tokenizer(eval(example['response_a'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    responses_b = tokenizer(eval(example['response_b'], {\"null\": \"\"}), add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    # プロンプトと応答の長さが一致することを確認\n",
    "    assert len(prompts) == len(responses_a) == len(responses_b), \"プロンプト、応答A、応答Bの長さが一致しません\"\n",
    "\n",
    "    # プロンプトと応答を逆順にする\n",
    "    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]\n",
    "\n",
    "    prompt, response_a, response_b = [], [], []\n",
    "    p_len, a_len, b_len = 0, 0, 0\n",
    "\n",
    "    # プロンプトと応答を長さ制限に基づいて追加\n",
    "    for p, a, b in zip(prompts, responses_a, responses_b):\n",
    "        prompt.append(p)\n",
    "        response_a.append(a)\n",
    "        response_b.append(b)\n",
    "        p_len += len(p)\n",
    "        a_len += len(a)\n",
    "        b_len += len(b)\n",
    "        \n",
    "        if p_len + a_len + b_len > MAX_LENGTH:  # 制限を超えた場合\n",
    "            break\n",
    "\n",
    "    # 順序を戻す\n",
    "    prompt = [item for sublist in reversed(prompt) for item in sublist]\n",
    "    response_a = [item for sublist in reversed(response_a) for item in sublist]\n",
    "    response_b = [item for sublist in reversed(response_b) for item in sublist]\n",
    "\n",
    "    # トークン数を計算\n",
    "    p_a_b_len = len(prompt) + len(response_a) + len(response_b)\n",
    "    cut_len = p_a_b_len - MAX_LENGTH\n",
    "    \n",
    "    # 制限を超えた場合、トークンを削除\n",
    "    if cut_len > 0:\n",
    "        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]\n",
    "        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]\n",
    "        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]\n",
    "\n",
    "    # 特殊トークンを付加\n",
    "    prompt = tokenizer('<prompt>: ', add_special_tokens=False)[\"input_ids\"] + prompt\n",
    "    response_a = tokenizer('\\n\\n<response_a>: ', add_special_tokens=False)[\"input_ids\"] + response_a\n",
    "    response_b = tokenizer('\\n\\n<response_b>: ', add_special_tokens=False)[\"input_ids\"] + response_b\n",
    "    extra_prompt = tokenizer('\\n\\n---------\\nどちらの応答がプロンプトに対して良いか？ a、b、またはtie？\\n\\n回答: ', add_special_tokens=False)[\"input_ids\"]\n",
    "    \n",
    "    label_token_id = [128250]  # ラベルのトークンID\n",
    "    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]\n",
    "    attention_mask = len(input_ids) * [1]\n",
    "    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8e4eee",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Tokenize\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# トークン化\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d8e31",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        # remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "test_ds = load_data(test, tokenizer)\n",
    "test_ds\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーをロード\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]  # ラベルIDを取得\n",
    "\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)  # データフレームからデータセットを作成\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        # remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "test_ds = load_data(test, tokenizer)  # トークン化されたデータセットをロード\n",
    "test_ds\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.596117,
     "end_time": "2024-07-01T02:57:53.425111",
     "exception": false,
     "start_time": "2024-07-01T02:57:52.828994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーをロード\n",
    "LABEL_IDS = [tokenizer(i, add_special_tokens=False)[\"input_ids\"][0] for i in ['a', 'b', 'tie']]  # ラベルIDを取得\n",
    "\n",
    "def load_data(df, tokenizer):\n",
    "    raw_datasets = Dataset.from_pandas(df)  # データフレームからデータセットを作成\n",
    "    tokenized_datasets = raw_datasets.map(\n",
    "        tokenize, \n",
    "        # remove_columns=raw_datasets.column_names,\n",
    "        fn_kwargs={'tokenizer': tokenizer},\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "test_ds = load_data(test, tokenizer)  # トークン化されたデータセットをロード\n",
    "test_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b1bf38",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "data = test_ds.to_pandas()\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)\n",
    "data[:3]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "data = test_ds.to_pandas()  # トークン化されたデータセットをPandasデータフレームに変換\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)  # 各インプットIDの長さを計算\n",
    "data[:3]  # 最初の3行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.015964,
     "end_time": "2024-07-01T02:57:53.472007",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.456043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = test_ds.to_pandas()  # トークン化されたデータセットをPandasデータフレームに変換\n",
    "data[\"max_len\"] = data[\"input_ids\"].apply(len)  # 各インプットIDの長さを計算\n",
    "data[:3]  # 最初の3行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cd3a1c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "data['input_ids'][0]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "data['input_ids'][0]  # 最初のインプットIDを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input_ids'][0]  # 最初のインプットIDを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa109ee",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 最初のインプットIDをデコードして表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(data[\"input_ids\"][0]))  # 最初のインプットIDをデコードして表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71c2b33",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load model \n",
    "We load 1 model on each gpu.  \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# モデルをロード\n",
    "各GPUにモデルを1つずつロードします。\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec1be8e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.model = LlamaModel(config)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids= None,\n",
    "        attention_mask= None,\n",
    "        position_ids = None,\n",
    "        past_key_values= None,\n",
    "        inputs_embeds= None,\n",
    "        labels= None,\n",
    "        use_cache= None,\n",
    "        output_attentions= None,\n",
    "        output_hidden_states = None,\n",
    "        return_dict= None,\n",
    "        cache_position = None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)\n",
    "        logits = logits.float()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n",
    "            shift_labels = shift_labels.view(-1)\n",
    "            # Enable model parallelism\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250],device=shift_labels.device)\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS,device=shift_labels.device)\n",
    "#             index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "#             true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "#             true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:,label_tokens_ids]\n",
    "#             loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みのキー\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # 親クラスの初期化\n",
    "        self.model = LlamaModel(config)  # Llamaモデルのインスタンス化\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズの設定\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 線形層の設定\n",
    "        self.post_init()  # 初期化後の処理\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ層の状態を取得\n",
    "        if self.config.pretraining_tp > 1:  # モデルパラレルが必要な場合\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # 隠れ層の状態からロジットを計算\n",
    "\n",
    "        logits = logits.float()  # ロジットをfloat型に変換\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:  # ラベルが提供されている場合\n",
    "            # トークンをシフトして、n未満のトークンがnを予測するようにする\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフト\n",
    "            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフト\n",
    "            # トークンをフラットにする\n",
    "            loss_fct = nn.CrossEntropyLoss()  # 損失関数の設定\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラットにする\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルをフラットにする\n",
    "            # モデルパラレルを有効化\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250], device=shift_labels.device)\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)\n",
    "            # index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            # true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            # true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:, label_tokens_ids]\n",
    "            # loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Llama3ForSFT(LlamaPreTrainedModel):\n",
    "    _tied_weights_keys = [\"lm_head.weight\"]  # 重みのキー\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)  # 親クラスの初期化\n",
    "        self.model = LlamaModel(config)  # Llamaモデルのインスタンス化\n",
    "        self.vocab_size = config.vocab_size  # 語彙サイズの設定\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 線形層の設定\n",
    "        self.post_init()  # 初期化後の処理\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        past_key_values=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        cache_position=None,\n",
    "    ):\n",
    "        outputs = self.model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            cache_position=cache_position,\n",
    "        )\n",
    "        hidden_states = outputs[0]  # 隠れ層の状態を取得\n",
    "        if self.config.pretraining_tp > 1:  # モデルパラレルが必要な場合\n",
    "            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)\n",
    "            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]\n",
    "            logits = torch.cat(logits, dim=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(hidden_states)  # 隠れ層の状態からロジットを計算\n",
    "\n",
    "        logits = logits.float()  # ロジットをfloat型に変換\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:  # ラベルが提供されている場合\n",
    "            # トークンをシフトして、n未満のトークンがnを予測するようにする\n",
    "            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフト\n",
    "            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフト\n",
    "            # トークンをフラットにする\n",
    "            loss_fct = nn.CrossEntropyLoss()  # 損失関数の設定\n",
    "            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラットにする\n",
    "            shift_labels = shift_labels.view(-1)  # ラベルをフラットにする\n",
    "            # モデルパラレルを有効化\n",
    "            shift_labels = shift_labels.to(shift_logits.device)\n",
    "\n",
    "            fake_label_tokens_ids = torch.tensor([128250], device=shift_labels.device)\n",
    "            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)\n",
    "            # index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}\n",
    "            # true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]\n",
    "            # true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)\n",
    "            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:, label_tokens_ids]\n",
    "            # loss = loss_fct(true_logits, true_labels)\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=true_logits,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39fe01b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Load base model on GPU 0\n",
    "device0 = torch.device('cuda:0')\n",
    "base_model_0 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:0',\n",
    ")\n",
    "# Load base model on GPU 1\n",
    "device1 = torch.device('cuda:1')\n",
    "base_model_1 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:1',\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# GPU 0に基本モデルをロード\n",
    "device0 = torch.device('cuda:0')  # デバイスの設定\n",
    "base_model_0 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:0',\n",
    ")  # モデルを読み込む\n",
    "# GPU 1に基本モデルをロード\n",
    "device1 = torch.device('cuda:1')  # デバイスの設定\n",
    "base_model_1 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:1',\n",
    ")  # モデルを読み込む\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 105.076557,
     "end_time": "2024-07-01T02:59:38.570536",
     "exception": false,
     "start_time": "2024-07-01T02:57:53.493979",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GPU 0に基本モデルをロード\n",
    "device0 = torch.device('cuda:0')  # デバイスの設定\n",
    "base_model_0 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:0',\n",
    ")  # モデルを読み込む\n",
    "# GPU 1に基本モデルをロード\n",
    "device1 = torch.device('cuda:1')  # デバイスの設定\n",
    "base_model_1 = Llama3ForSFT.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    use_cache=False,\n",
    "    device_map='cuda:1',\n",
    ")  # モデルを読み込む"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e28b01b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Load weights \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 重みをロード\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b28e",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Get peft\n",
    "model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0) \n",
    "model_0.eval()\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)\n",
    "model_1.eval()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# PEFTを取得\n",
    "model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0)  # モデルをPEFTから読み込む\n",
    "model_0.eval()  # モデルを評価モードにする\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)  # モデルをPEFTから読み込む\n",
    "model_1.eval()  # モデルを評価モードにする\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 13.701042,
     "end_time": "2024-07-01T02:59:52.320278",
     "exception": false,
     "start_time": "2024-07-01T02:59:38.619236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PEFTを取得\n",
    "model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0)  # モデルをPEFTから読み込む\n",
    "model_0.eval()  # モデルを評価モードにする\n",
    "\n",
    "model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)  # モデルをPEFTから読み込む\n",
    "model_1.eval()  # モデルを評価モードにする"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9204f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Inference\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 推論\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878ca1b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "\n",
    "    model.eval()\n",
    "    for start_idx in range(0, len(df), batch_size):\n",
    "        end_idx = min(start_idx + batch_size, len(df))\n",
    "        tmp = df.iloc[start_idx:end_idx]\n",
    "        input_ids = tmp[\"input_ids\"].to_list()\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()\n",
    "        labels = tmp[\"labels\"].to_list()\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)\n",
    "        pad_labels=[]\n",
    "        for label in labels:\n",
    "            label = list(label) + [tokenizer.pad_token_id]*(input_ids[0].shape[0]-label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "        labels = torch.tensor(pad_labels).to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()\n",
    "        a_win.extend(proba[:, 0].tolist())\n",
    "        b_win.extend(proba[:, 1].tolist())\n",
    "        tie.extend(proba[:, 2].tolist())\n",
    "    df['winner_model_a'] = a_win\n",
    "    df['winner_model_b'] = b_win\n",
    "    df['winner_tie'] = tie\n",
    "    return df\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "@torch.no_grad()  # 勾配計算を無効化\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を使用\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝利数を初期化\n",
    "\n",
    "    model.eval()  # モデルを評価モードにする\n",
    "    for start_idx in range(0, len(df), batch_size):  # データフレームをバッチサイズで処理\n",
    "        end_idx = min(start_idx + batch_size, len(df))  \n",
    "        tmp = df.iloc[start_idx:end_idx]  # 一時的なデータフレームを作成\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # インプットIDをリストに変換\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストに変換\n",
    "        labels = tmp[\"labels\"].to_list()  # ラベルをリストに変換\n",
    "        \n",
    "        # 入力をパディング\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)  # デバイスに移動\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)  # デバイスに移動\n",
    "        \n",
    "        pad_labels = []\n",
    "        for label in labels:\n",
    "            # ラベルをパディング\n",
    "            label = list(label) + [tokenizer.pad_token_id] * (input_ids[0].shape[0] - label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "\n",
    "        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルを順伝播\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ロジットをソフトマックスで正規化\n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率を保存\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率を保存\n",
    "        tie.extend(proba[:, 2].tolist())  # タイの勝率を保存\n",
    "        \n",
    "    df['winner_model_a'] = a_win  # データフレームに勝率を追加\n",
    "    df['winner_model_b'] = b_win  # データフレームに勝率を追加\n",
    "    df['winner_tie'] = tie  # データフレームに勝率を追加\n",
    "    return df  # 結果を返す\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.021078,
     "end_time": "2024-07-01T02:59:52.402973",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.381895",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()  # 勾配計算を無効化\n",
    "@torch.cuda.amp.autocast()  # 自動混合精度を使用\n",
    "def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):\n",
    "    a_win, b_win, tie = [], [], []  # 各モデルの勝利数を初期化\n",
    "\n",
    "    model.eval()  # モデルを評価モードにする\n",
    "    for start_idx in range(0, len(df), batch_size):  # データフレームをバッチサイズで処理\n",
    "        end_idx = min(start_idx + batch_size, len(df))  \n",
    "        tmp = df.iloc[start_idx:end_idx]  # 一時的なデータフレームを作成\n",
    "        input_ids = tmp[\"input_ids\"].to_list()  # インプットIDをリストに変換\n",
    "        attention_mask = tmp[\"attention_mask\"].to_list()  # アテンションマスクをリストに変換\n",
    "        labels = tmp[\"labels\"].to_list()  # ラベルをリストに変換\n",
    "        \n",
    "        # 入力をパディング\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": input_ids, \"attention_mask\": attention_mask},\n",
    "            padding=\"longest\",\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        input_ids = inputs[\"input_ids\"].to(device)  # デバイスに移動\n",
    "        attention_mask = inputs[\"attention_mask\"].to(device)  # デバイスに移動\n",
    "        \n",
    "        pad_labels = []\n",
    "        for label in labels:\n",
    "            # ラベルをパディング\n",
    "            label = list(label) + [tokenizer.pad_token_id] * (input_ids[0].shape[0] - label.shape[0])\n",
    "            pad_labels.append(label)\n",
    "\n",
    "        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルを順伝播\n",
    "        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ロジットをソフトマックスで正規化\n",
    "        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率を保存\n",
    "        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率を保存\n",
    "        tie.extend(proba[:, 2].tolist())  # タイの勝率を保存\n",
    "        \n",
    "    df['winner_model_a'] = a_win  # データフレームに勝率を追加\n",
    "    df['winner_model_b'] = b_win  # データフレームに勝率を追加\n",
    "    df['winner_tie'] = tie  # データフレームに勝率を追加\n",
    "    return df  # 結果を返す"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8c784f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "st = time.time()\n",
    "\n",
    "data = data.sort_values(\"max_len\", ascending=False)\n",
    "sub_1 = data.iloc[0::2].copy()\n",
    "sub_2 = data.iloc[1::2].copy()\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values\n",
    "\n",
    "print(f\"elapsed time: {time.time() - st}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "st = time.time()  # 開始時間を記録\n",
    "\n",
    "data = data.sort_values(\"max_len\", ascending=False)  # 最大長さでソート\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのサブセット\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのサブセット\n",
    "\n",
    "# 並列で推論を実行\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝率を抽出\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.316613,
     "end_time": "2024-07-01T02:59:55.727834",
     "exception": false,
     "start_time": "2024-07-01T02:59:52.411221",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "st = time.time()  # 開始時間を記録\n",
    "\n",
    "data = data.sort_values(\"max_len\", ascending=False)  # 最大長さでソート\n",
    "sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのサブセット\n",
    "sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのサブセット\n",
    "\n",
    "# 並列で推論を実行\n",
    "with ThreadPoolExecutor(max_workers=2) as executor:\n",
    "    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))\n",
    "\n",
    "result_df = pd.concat(list(results), axis=0)  # 結果を結合\n",
    "proba = result_df[[\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]].values  # 勝率を抽出\n",
    "\n",
    "print(f\"経過時間: {time.time() - st}\")  # 経過時間を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3448cb",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "display(submission_df)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝率を結果データフレームに追加\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝率を結果データフレームに追加\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # タイの勝率を結果データフレームに追加\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルとして保存\n",
    "display(submission_df)  # 提出データフレームを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1.755381,
     "end_time": "2024-07-01T02:59:57.492377",
     "exception": false,
     "start_time": "2024-07-01T02:59:55.736996",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "result_df.loc[:, \"winner_model_a\"] = proba[:, 0]  # モデルAの勝率を結果データフレームに追加\n",
    "result_df.loc[:, \"winner_model_b\"] = proba[:, 1]  # モデルBの勝率を結果データフレームに追加\n",
    "result_df.loc[:, \"winner_tie\"] = proba[:, 2]  # タイの勝率を結果データフレームに追加\n",
    "submission_df = result_df[[\"id\", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成\n",
    "submission_df.to_csv('submission.csv', index=False)  # CSVファイルとして保存\n",
    "display(submission_df)  # 提出データフレームを表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5418635,
     "sourceId": 8995950,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5439897,
     "sourceId": 9026149,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5439960,
     "sourceId": 9026233,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5452248,
     "sourceId": 9043661,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 206.513308,
   "end_time": "2024-07-01T03:00:01.146998",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-01T02:56:34.63369",
   "version": "2.5.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "03b341b06afc40599e50c9c1ce88be20": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "07273d2112d649ffbea2991a6a79df98": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "142be9e5949c44fabd0370c6df1203d6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "15d31276fcd44350a50d1c561c13e3a4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_3974a6c7f61d4f4582a8e4fdf4c9976c",
       "placeholder": "​",
       "style": "IPY_MODEL_3daebd55184a4a9982813dc1ac948f2e",
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "1df41ee456cd4fa78a23f7ea2fded110": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "28cb0aaaf9d24d3d857d224850f62f5b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_846776c5dc1f44bc9cf2e3d394e8ba48",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e7361623d5e1491c888080ee4fb8bfdd",
       "value": 4
      }
     },
     "3974a6c7f61d4f4582a8e4fdf4c9976c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3daebd55184a4a9982813dc1ac948f2e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "411187a29c544ebbb425a06b0dfae7a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "475dd481f05a46c1908b9f781ae1afa8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f43e003b1f2e4367800ba2bad74c7075",
        "IPY_MODEL_28cb0aaaf9d24d3d857d224850f62f5b",
        "IPY_MODEL_74d46aef6d8949c584024a6e7bb4f06c"
       ],
       "layout": "IPY_MODEL_1df41ee456cd4fa78a23f7ea2fded110"
      }
     },
     "74d46aef6d8949c584024a6e7bb4f06c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_9c5128ae1d114334b30f2e1013062b26",
       "placeholder": "​",
       "style": "IPY_MODEL_03b341b06afc40599e50c9c1ce88be20",
       "value": " 4/4 [01:30&lt;00:00, 18.30s/it]"
      }
     },
     "846776c5dc1f44bc9cf2e3d394e8ba48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8e8e3620620b445eb1d0286befa13278": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_15d31276fcd44350a50d1c561c13e3a4",
        "IPY_MODEL_cb2874dbe9904c07a254eb33d6f0ecfe",
        "IPY_MODEL_c23e125e5c3e4cee844bd057453c7aca"
       ],
       "layout": "IPY_MODEL_142be9e5949c44fabd0370c6df1203d6"
      }
     },
     "9c5128ae1d114334b30f2e1013062b26": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b7c6588ad13549ae958237ca8e3af9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c23e125e5c3e4cee844bd057453c7aca": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_dd4aa3639e4a46e7a3861fa3dbd5a31b",
       "placeholder": "​",
       "style": "IPY_MODEL_07273d2112d649ffbea2991a6a79df98",
       "value": " 4/4 [00:13&lt;00:00,  2.76s/it]"
      }
     },
     "cb2874dbe9904c07a254eb33d6f0ecfe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_411187a29c544ebbb425a06b0dfae7a4",
       "max": 4,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_d0043cb27ac54061b85f9b3886954314",
       "value": 4
      }
     },
     "d0043cb27ac54061b85f9b3886954314": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "dd4aa3639e4a46e7a3861fa3dbd5a31b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e7361623d5e1491c888080ee4fb8bfdd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f0cc9ad10c3d4a63bd03716995531022": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "f43e003b1f2e4367800ba2bad74c7075": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b7c6588ad13549ae958237ca8e3af9db",
       "placeholder": "​",
       "style": "IPY_MODEL_f0cc9ad10c3d4a63bd03716995531022",
       "value": "Loading checkpoint shards: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
