{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099d11f7",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arena コンペティションにおいて、ユーザーが好む応答を予測するための機械学習モデルを構築することを目的としています。特に、チャットボットの応答の中から、どちらがユーザーに好まれるかを判定するための特徴量を抽出し、予測モデルを訓練する方法を示しています。\n",
    "\n",
    "### 主な問題:\n",
    "- コンペティションのデータを用いて、二者間の選好（どちらの応答が好まれるか）を予測する。\n",
    "\n",
    "### 使用している手法:\n",
    "1. **データの前処理**:\n",
    "   - プロンプトや応答をクリーニングし、'null'応答を持つ行を削除する。\n",
    "\n",
    "2. **特徴量エンジニアリング**:\n",
    "   - **TF-IDF**ベクトル化（単語と文字レベル）を用いてテキストデータを数値化し、各応答の特徴を抽出。\n",
    "   - 応答の長さに基づく特徴を作成し、応答の長さの差や比も考慮。\n",
    "   - パープレキシティを算出することで、応答の自然さを測定し、IFD（Input-Output Frequency Discrepancy）に基づいた特徴量を生成。\n",
    "\n",
    "3. **モデルの構築**:\n",
    "   - LightGBMを用いた多クラス分類器を使用、一対多の選好予測を行う。交差検証を通じて汎用性を向上させ、評価指標として対数損失（Log Loss）を使用してモデルの性能を測定。\n",
    "\n",
    "### 使用されているライブラリ:\n",
    "- **numpy, pandas**: データの操作と分析。\n",
    "- **regex**: テキスト処理のための正規表現。\n",
    "- **scikit-learn**: 特徴量エンジニアリングやモデル評価。\n",
    "- **lightgbm**: 高速で効率的なブースティング決定木。\n",
    "- **transformers**: DeBERTaとGPTモデルを活用し、自然言語の理解を深めるためのパープレキシティ計算。\n",
    "- **tqdm**: 進捗バーの表示。\n",
    "\n",
    "このノートブックは、ユーザーの選好を予測するために、データの前処理からモデル構築、予測までの一連のステップを網羅しており、特にTF-IDFやパープレキシティなどのテキスト特徴量を活用している点が特徴です。最終的に、予測された結果はCSV形式で出力され、コンペティションに提出可能な形式が整えられています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad90c37",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下は、提供されたJupyter Notebookに関連する機械学習・深層学習の専門用語の簡単な解説です。特に、初心者には馴染みが薄い可能性がある用語やこのノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **IFD (Instructive Feedback Dynamics)**\n",
    "   - モデルが生成した応答の質を評価するための指標。具体的には、ある質問に対する応答が、他の応答に対するパープレキシティ（PPL）とどう関連しているかを示す指標です。小さいほど良い応答とされ、ユーザーから見てどれほど好ましいかの参考になります。\n",
    "\n",
    "2. **パープレキシティ (Perplexity)**\n",
    "   - 言語モデルの性能を測るための指標。モデルが新しいデータをどれだけうまく予測できるかを示し、値が低いほど良い。わかりやすく言うと、モデルが与えられたテキストを理解し、次に来る単語を予測できる程度を示す。\n",
    "\n",
    "3. **TF-IDF (Term Frequency-Inverse Document Frequency)**\n",
    "   - 文書内の用語の重要性を評価するための統計的手法。特定の用語が文書内でどれだけ頻繁に使われているか（Term Frequency）と、全体の文書集でその用語がどれだけ珍しいか（Inverse Document Frequency）を組み合わせることで、文書の特徴を抽出します。\n",
    "\n",
    "4. **特徴量ベクトル (Feature Vector)**\n",
    "   - 機械学習モデルにおける入力データの表現。複数の特徴量（変数）を数値のリストとして表現したもので、モデルが予測を行うための基盤となります。\n",
    "\n",
    "5. **バッチ処理 (Batch Processing)**\n",
    "   - データを小さなグループ（バッチ）に分割して処理する方法。これは、メモリの消費を抑えたり、処理効率を高めるために使用されます。\n",
    "\n",
    "6. **交差検証 (Cross Validation)**\n",
    "   - モデルの汎化能力を評価するためにデータを分割して繰り返しトレーニングとテストを行う手法。これにより、特定のデータセットに対する過剰適合を防ぎます。\n",
    "\n",
    "7. **LightGBM (Light Gradient Boosting Machine)**\n",
    "   - 勾配ブースティングフレームワークの一つで、大規模なデータセットで高速に学習できるように最適化されています。特に、ツリー構造の学習において効率的で、メモリ使用量を抑える設計がされています。\n",
    "\n",
    "8. **スパース行列 (Sparse Matrix)**\n",
    "   - 値がほとんどゼロである行列のこと。計算効率を高めるために、ゼロ以外の値だけを格納するデータ構造を使用します。\n",
    "\n",
    "9. **ハイパーパラメータ (Hyperparameter)**\n",
    "   - モデルの構造や学習プロセスにおいて手動で設定するパラメータ。たとえば、決定木の深さやサンプリング率などがあります。これらは訓練データを通じて学習されるパラメータとは異なり、モデルの学習前に設定されます。\n",
    "\n",
    "10. **エラー処理 (Error Handling)**\n",
    "    - コードが実行中に発生しうるエラーに対して、プログラムがどのように対処するかを定義するプロセス。ここでは、発生したエラーに基づいて、適切な戻り値（例えば、0など）を返すような処理について言及しています。\n",
    "\n",
    "これらの解説はノートブック全体を通じて初心者がつまずく可能性のあるポイントを考慮して作成しました。理解を深めるためにこれらの用語を活用してください。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec0d67",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "## Refer: [IFD](https://github.com/tianyi-lab/Superfiltering)\n",
    "$$\\mathrm{IFD}_\\theta(Q,A)=\\frac{\\mathrm{PPL}_\\theta(A|Q)}{\\mathrm{PPL}_\\theta(A)}$$\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "## 参照: [IFD](https://github.com/tianyi-lab/Superfiltering)\n",
    "$$\\mathrm{IFD}_\\theta(Q,A)=\\frac{\\mathrm{PPL}_\\theta(A|Q)}{\\mathrm{PPL}_\\theta(A)}$$\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952fde74",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import itertools\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import joblib\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import itertools\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import joblib\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.328936Z",
     "iopub.status.busy": "2024-07-02T11:20:07.328566Z",
     "iopub.status.idle": "2024-07-02T11:20:07.337874Z",
     "shell.execute_reply": "2024-07-02T11:20:07.33688Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.328911Z"
    },
    "papermill": {
     "duration": 4.432239,
     "end_time": "2024-05-07T00:30:21.264459",
     "exception": false,
     "start_time": "2024-05-07T00:30:16.83222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import regex as re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy.sparse import csr_matrix, save_npz, load_npz, hstack\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import itertools\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import Word2Vec\n",
    "from transformers import DebertaV2Tokenizer, DebertaV2Model\n",
    "from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import joblib\n",
    "import unicodedata\n",
    "import re\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a4701",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "MAX_LENGTH = 1024\n",
    "deberta_path = \"/kaggle/input/debertav3base\"\n",
    "gpt_path = \"/kaggle/input/qwen2-1-5b-instruct\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "MAX_LENGTH = 1024  # モデルの最大入力長を設定\n",
    "deberta_path = \"/kaggle/input/debertav3base\"  # DeBERTaモデルのパス\n",
    "gpt_path = \"/kaggle/input/qwen2-1-5b-instruct\"  # GPTモデルのパス\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能か確認し、デバイスを設定\n",
    "SEED = 42  # 乱数シードを設定\n",
    "set_seed(SEED)  # 乱数シードを設定する関数を呼び出し\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.340504Z",
     "iopub.status.busy": "2024-07-02T11:20:07.340064Z",
     "iopub.status.idle": "2024-07-02T11:20:07.352823Z",
     "shell.execute_reply": "2024-07-02T11:20:07.351793Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.340472Z"
    }
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 1024  # モデルの最大入力長を設定\n",
    "deberta_path = \"/kaggle/input/debertav3base\"  # DeBERTaモデルのパス\n",
    "gpt_path = \"/kaggle/input/qwen2-1-5b-instruct\"  # GPTモデルのパス\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能か確認し、デバイスを設定\n",
    "SEED = 42  # 乱数シードを設定\n",
    "set_seed(SEED)  # 乱数シードを設定する関数を呼び出し"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49b57b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "vectorize_on_train_and_test = True\n",
    "#quick_test for training on small part of train data (and not using bunch of GPU on submit)\n",
    "#(if this is on - saved models won't be fully trained)\n",
    "quick_test = True\n",
    "quick_test_items = 1000\n",
    "#automatically disable quick_test if we detect actual test data... (assures full training when scoring)\n",
    "if (len(test)) > 3:quick_test = False\n",
    "if quick_test: train = train.head(quick_test_items)\n",
    "\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]\n",
    "    return  ' '.join(sentences)\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index\n",
    "train.drop(indexes, inplace=True)\n",
    "train.reset_index(inplace=True, drop=True)\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")\n",
    "print('Total train samples: ', len(train))\n",
    "\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]\n",
    "train['label'] = train[target_columns].idxmax(axis=1) \n",
    "label_encoder = LabelEncoder()\n",
    "train['label'] = label_encoder.fit_transform(train['label'])\n",
    "train = train[columns_to_vectorize + ['label']]\n",
    "train.head(3)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # トレーニングデータを読み込む\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")  # テストデータを読み込む\n",
    "vectorize_on_train_and_test = True  # トレーニングデータとテストデータのベクトル化を実行するかのフラグ\n",
    "# トレーニングデータの小さい部分でのクイックテスト用（提出時に多くのGPUを使用しないように）\n",
    "# (これがオンの場合、保存されたモデルは完全には訓練されない)\n",
    "quick_test = True\n",
    "quick_test_items = 1000  # クイックテストで使用するデータの数\n",
    "# 実際のテストデータが検出される場合、自動的にクイックテストを無効にする...（スコアリング時に完全なトレーニングを保証する）\n",
    "if (len(test)) > 3: quick_test = False\n",
    "if quick_test: train = train.head(quick_test_items)  # クイックテストの場合、トレーニングデータを制限する\n",
    "\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 入力文字列の前後の中括弧を削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 各文をリストに分割し、余分な引用符を削除\n",
    "    return ' '.join(sentences)  # 文を結合して一つの文字列にする\n",
    "\n",
    "# テストデータのプロンプト、応答A、応答Bを処理\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "# トレーニングデータのプロンプト、応答A、応答Bを処理\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 応答Aと応答Bが両方'null'のインデックスを取得\n",
    "train.drop(indexes, inplace=True)  # 'null'のインデックスをトレーニングデータから削除\n",
    "train.reset_index(inplace=True, drop=True)  # インデックスをリセット\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")  # 削除された行数を表示\n",
    "print('Total train samples: ', len(train))  # トレーニングデータのサンプル数を表示\n",
    "\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなるカラムを定義\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]  # ベクトル化するカラムを定義\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行の勝者モデルをラベルとして設定\n",
    "label_encoder = LabelEncoder()  # ラベルエンコーダを作成\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコード\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要なカラムのみを保持\n",
    "train.head(3)  # トレーニングデータの最初の3行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:07.354644Z",
     "iopub.status.busy": "2024-07-02T11:20:07.354153Z",
     "iopub.status.idle": "2024-07-02T11:20:09.08421Z",
     "shell.execute_reply": "2024-07-02T11:20:09.083316Z",
     "shell.execute_reply.started": "2024-07-02T11:20:07.354612Z"
    },
    "papermill": {
     "duration": 4.40411,
     "end_time": "2024-05-07T00:30:25.698148",
     "exception": false,
     "start_time": "2024-05-07T00:30:21.294038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # トレーニングデータを読み込む\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")  # テストデータを読み込む\n",
    "vectorize_on_train_and_test = True  # トレーニングデータとテストデータのベクトル化を実行するかのフラグ\n",
    "# トレーニングデータの小さい部分でのクイックテスト用（提出時に多くのGPUを使用しないように）\n",
    "# (これがオンの場合、保存されたモデルは完全には訓練されない)\n",
    "quick_test = True\n",
    "quick_test_items = 1000  # クイックテストで使用するデータの数\n",
    "# 実際のテストデータが検出される場合、自動的にクイックテストを無効にする...（スコアリング時に完全なトレーニングを保証する）\n",
    "if (len(test)) > 3: quick_test = False\n",
    "if quick_test: train = train.head(quick_test_items)  # クイックテストの場合、トレーニングデータを制限する\n",
    "\n",
    "def process(input_str):\n",
    "    stripped_str = input_str.strip('[]')  # 入力文字列の前後の中括弧を削除\n",
    "    sentences = [s.strip('\"') for s in stripped_str.split('\",\"')]  # 各文をリストに分割し、余分な引用符を削除\n",
    "    return ' '.join(sentences)  # 文を結合して一つの文字列にする\n",
    "\n",
    "# テストデータのプロンプト、応答A、応答Bを処理\n",
    "test.loc[:, 'prompt'] = test['prompt'].apply(process)\n",
    "test.loc[:, 'response_a'] = test['response_a'].apply(process)\n",
    "test.loc[:, 'response_b'] = test['response_b'].apply(process)\n",
    "# トレーニングデータのプロンプト、応答A、応答Bを処理\n",
    "train.loc[:, 'prompt'] = train['prompt'].apply(process)\n",
    "train.loc[:, 'response_a'] = train['response_a'].apply(process)\n",
    "train.loc[:, 'response_b'] = train['response_b'].apply(process)\n",
    "\n",
    "indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 応答Aと応答Bが両方'null'のインデックスを取得\n",
    "train.drop(indexes, inplace=True)  # 'null'のインデックスをトレーニングデータから削除\n",
    "train.reset_index(inplace=True, drop=True)  # インデックスをリセット\n",
    "print(f\"Total {len(indexes)} Null response rows dropped\")  # 削除された行数を表示\n",
    "print('Total train samples: ', len(train))  # トレーニングデータのサンプル数を表示\n",
    "\n",
    "target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなるカラムを定義\n",
    "columns_to_vectorize = [\"prompt\", \"response_a\", \"response_b\"]  # ベクトル化するカラムを定義\n",
    "train['label'] = train[target_columns].idxmax(axis=1)  # 各行の勝者モデルをラベルとして設定\n",
    "label_encoder = LabelEncoder()  # ラベルエンコーダを作成\n",
    "train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコード\n",
    "train = train[columns_to_vectorize + ['label']]  # 必要なカラムのみを保持\n",
    "train.head(3)  # トレーニングデータの最初の3行を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df30c6",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### TF-IDF\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25e39f05",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "if vectorize_on_train_and_test:\n",
    "    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)\n",
    "else:\n",
    "    vector_fit_text = train_text\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # トレーニングデータの全テキストを結合\n",
    "test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # テストデータの全テキストを結合\n",
    "\n",
    "if vectorize_on_train_and_test:  # トレーニングとテスト両方をベクトル化する場合\n",
    "    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)  # トレーニングデータとテストデータを連結\n",
    "else:\n",
    "    vector_fit_text = train_text  # そうでなければトレーニングデータのみを使用\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:09.085923Z",
     "iopub.status.busy": "2024-07-02T11:20:09.085638Z",
     "iopub.status.idle": "2024-07-02T11:20:09.105478Z",
     "shell.execute_reply": "2024-07-02T11:20:09.104656Z",
     "shell.execute_reply.started": "2024-07-02T11:20:09.085899Z"
    }
   },
   "outputs": [],
   "source": [
    "train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # トレーニングデータの全テキストを結合\n",
    "test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # テストデータの全テキストを結合\n",
    "\n",
    "if vectorize_on_train_and_test:  # トレーニングとテスト両方をベクトル化する場合\n",
    "    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)  # トレーニングデータとテストデータを連結\n",
    "else:\n",
    "    vector_fit_text = train_text  # そうでなければトレーニングデータのみを使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84556c93",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'[^\\W]+', text)\n",
    "\n",
    "#word-level vectorizer\n",
    "tfidf_word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),\n",
    "    tokenizer=custom_tokenizer,\n",
    "    token_pattern=None,\n",
    "    strip_accents='unicode',\n",
    "    min_df=4,\n",
    "    max_features=300\n",
    ")\n",
    "\n",
    "#char-level vectorizer\n",
    "tfidf_char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',\n",
    "    ngram_range=(1, 5), \n",
    "    max_features=1000, \n",
    "    min_df=4\n",
    ")\n",
    "\n",
    "def batch_process(texts, batch_size):\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        yield texts[i:i + batch_size]\n",
    "\n",
    "#doing in batches so we can see progress\n",
    "batch_size = 1000\n",
    "for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):\n",
    "    if len(batch) >= tfidf_word_vectorizer.min_df:\n",
    "        tfidf_word_vectorizer.fit(batch)\n",
    "    if len(batch) >= tfidf_char_vectorizer.min_df:\n",
    "        tfidf_char_vectorizer.fit(batch)\n",
    "        \n",
    "def get_tfidf_vectors(df):\n",
    "    vectorized_columns = []\n",
    "    for column in columns_to_vectorize:\n",
    "        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))\n",
    "        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))\n",
    "    return hstack(vectorized_columns)\n",
    "\n",
    "tfidf_train_vectors = get_tfidf_vectors(train)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'[^\\W]+', text)  # 特殊文字を除外して単語を抽出するカスタムトークナイザー\n",
    "\n",
    "# word-level vectorizer\n",
    "tfidf_word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),  # n-gramの範囲を設定（1から5）\n",
    "    tokenizer=custom_tokenizer,  # カスタムトークナイザーを指定\n",
    "    token_pattern=None,  # デフォルトのトークンパターンを無効に\n",
    "    strip_accents='unicode',  # アクセントを削除\n",
    "    min_df=4,  # 最小出現回数\n",
    "    max_features=300  # 最大特徴数\n",
    ")\n",
    "\n",
    "# char-level vectorizer\n",
    "tfidf_char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',  # 文字単位のベクトル化を実行\n",
    "    ngram_range=(1, 5),  # n-gramの範囲\n",
    "    max_features=1000,  # 最大特徴数\n",
    "    min_df=4  # 最小出現回数\n",
    ")\n",
    "\n",
    "def batch_process(texts, batch_size):\n",
    "    for i in range(0, len(texts), batch_size):  # バッチサイズごとにテキストを処理\n",
    "        yield texts[i:i + batch_size]  # 各バッチを返す\n",
    "\n",
    "# バッチごとに進行状況を表示しながら処理\n",
    "batch_size = 1000  # バッチサイズを設定\n",
    "for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):  # 進捗バーを表示\n",
    "    if len(batch) >= tfidf_word_vectorizer.min_df:  # 最小出現回数を考慮して単語ベクトルをフィット\n",
    "        tfidf_word_vectorizer.fit(batch)\n",
    "    if len(batch) >= tfidf_char_vectorizer.min_df:  # 最小出現回数を考慮して文字ベクトルをフィット\n",
    "        tfidf_char_vectorizer.fit(batch)\n",
    "        \n",
    "def get_tfidf_vectors(df):\n",
    "    vectorized_columns = []  # ベクトル化されたカラムのリスト\n",
    "    for column in columns_to_vectorize:  # 各列に対して\n",
    "        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))  # 単語ベクトルを付加\n",
    "        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))  # 文字ベクトルを付加\n",
    "    return hstack(vectorized_columns)  # スパース行列として返す\n",
    "\n",
    "tfidf_train_vectors = get_tfidf_vectors(train)  # トレーニングデータのTF-IDFベクトルを取得\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-02T11:20:09.107995Z",
     "iopub.status.busy": "2024-07-02T11:20:09.10771Z"
    }
   },
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "    return re.findall(r'[^\\W]+', text)  # 特殊文字を除外して単語を抽出するカスタムトークナイザー\n",
    "\n",
    "# word-level vectorizer\n",
    "tfidf_word_vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 5),  # n-gramの範囲を設定（1から5）\n",
    "    tokenizer=custom_tokenizer,  # カスタムトークナイザーを指定\n",
    "    token_pattern=None,  # デフォルトのトークンパターンを無効に\n",
    "    strip_accents='unicode',  # アクセントを削除\n",
    "    min_df=4,  # 最小出現回数\n",
    "    max_features=300  # 最大特徴数\n",
    ")\n",
    "\n",
    "# char-level vectorizer\n",
    "tfidf_char_vectorizer = TfidfVectorizer(\n",
    "    analyzer='char',  # 文字単位のベクトル化を実行\n",
    "    ngram_range=(1, 5),  # n-gramの範囲\n",
    "    max_features=1000,  # 最大特徴数\n",
    "    min_df=4  # 最小出現回数\n",
    ")\n",
    "\n",
    "def batch_process(texts, batch_size):\n",
    "    for i in range(0, len(texts), batch_size):  # バッチサイズごとにテキストを処理\n",
    "        yield texts[i:i + batch_size]  # 各バッチを返す\n",
    "\n",
    "# バッチごとに進行状況を表示しながら処理\n",
    "batch_size = 1000  # バッチサイズを設定\n",
    "for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):  # 進捗バーを表示\n",
    "    if len(batch) >= tfidf_word_vectorizer.min_df:  # 最小出現回数を考慮して単語ベクトルをフィット\n",
    "        tfidf_word_vectorizer.fit(batch)\n",
    "    if len(batch) >= tfidf_char_vectorizer.min_df:  # 最小出現回数を考慮して文字ベクトルをフィット\n",
    "        tfidf_char_vectorizer.fit(batch)\n",
    "        \n",
    "def get_tfidf_vectors(df):\n",
    "    vectorized_columns = []  # ベクトル化されたカラムのリスト\n",
    "    for column in columns_to_vectorize:  # 各列に対して\n",
    "        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))  # 単語ベクトルを付加\n",
    "        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))  # 文字ベクトルを付加\n",
    "    return hstack(vectorized_columns)  # スパース行列として返す\n",
    "\n",
    "tfidf_train_vectors = get_tfidf_vectors(train)  # トレーニングデータのTF-IDFベクトルを取得"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739aea73",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### LEN\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### LEN\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa8fb74",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def has_none(vals) -> int:\n",
    "    # some responses contains null and probably they are useful for prediction\n",
    "    return int(any(val is None for val in vals))\n",
    "\n",
    "def str_length(vals) -> int:\n",
    "    length = 0\n",
    "    for val in vals:\n",
    "        if isinstance(val, str):\n",
    "            length += len(val)\n",
    "    return length\n",
    "\n",
    "def get_length_features(data: pd.DataFrame):\n",
    "    length_feature_array = []\n",
    "    length_feature_array.append(data[\"response_a\"].apply(str_length))\n",
    "    length_feature_array.append(data[\"response_b\"].apply(str_length))\n",
    "    length_feature_array.append(length_feature_array[0] - length_feature_array[1])\n",
    "    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)\n",
    "    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none))\n",
    "    length_feature_array.append(data[\"response_b\"].apply(has_none))\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none) - data[\"response_b\"].apply(has_none))\n",
    "    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)\n",
    "    length_feature_array = np.transpose(length_feature_array, (1, 0))\n",
    "    return length_feature_array\n",
    "train_length_features = get_length_features(train)\n",
    "print(train_length_features.shape)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def has_none(vals) -> int:\n",
    "    # 応答にnullが含まれる場合、予測において役立つ可能性がある\n",
    "    return int(any(val is None for val in vals))  # 一つでもnullがあれば1を返す\n",
    "\n",
    "def str_length(vals) -> int:\n",
    "    length = 0  # 文字列の長さを保持する変数\n",
    "    for val in vals:  # 各値に対して\n",
    "        if isinstance(val, str):  # 値が文字列であれば\n",
    "            length += len(val)  # その長さを加算\n",
    "    return length  # 総長を返す\n",
    "\n",
    "def get_length_features(data: pd.DataFrame):\n",
    "    length_feature_array = []  # 長さ特徴の配列を作成\n",
    "    length_feature_array.append(data[\"response_a\"].apply(str_length))  # 応答Aの長さを取得\n",
    "    length_feature_array.append(data[\"response_b\"].apply(str_length))  # 応答Bの長さを取得\n",
    "    length_feature_array.append(length_feature_array[0] - length_feature_array[1])  # 応答Aと応答Bの長さの差\n",
    "    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)  # 応答の平均長\n",
    "    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))  # 応答Aと応答Bの長さの比\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none))  # 応答Aにnullが含まれているか\n",
    "    length_feature_array.append(data[\"response_b\"].apply(has_none))  # 応答Bにnullが含まれているか\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none) - data[\"response_b\"].apply(has_none))  # nullの差\n",
    "    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)  # 配列を整形\n",
    "    length_feature_array = np.transpose(length_feature_array, (1, 0))  # 行と列を入れ替え\n",
    "    return length_feature_array  # 最終結果を返す\n",
    "\n",
    "train_length_features = get_length_features(train)  # トレーニングデータの長さ特徴を取得\n",
    "print(train_length_features.shape)  # 形状を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_none(vals) -> int:\n",
    "    # 応答にnullが含まれる場合、予測において役立つ可能性がある\n",
    "    return int(any(val is None for val in vals))  # 一つでもnullがあれば1を返す\n",
    "\n",
    "def str_length(vals) -> int:\n",
    "    length = 0  # 文字列の長さを保持する変数\n",
    "    for val in vals:  # 各値に対して\n",
    "        if isinstance(val, str):  # 値が文字列であれば\n",
    "            length += len(val)  # その長さを加算\n",
    "    return length  # 総長を返す\n",
    "\n",
    "def get_length_features(data: pd.DataFrame):\n",
    "    length_feature_array = []  # 長さ特徴の配列を作成\n",
    "    length_feature_array.append(data[\"response_a\"].apply(str_length))  # 応答Aの長さを取得\n",
    "    length_feature_array.append(data[\"response_b\"].apply(str_length))  # 応答Bの長さを取得\n",
    "    length_feature_array.append(length_feature_array[0] - length_feature_array[1])  # 応答Aと応答Bの長さの差\n",
    "    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)  # 応答の平均長\n",
    "    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))  # 応答Aと応答Bの長さの比\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none))  # 応答Aにnullが含まれているか\n",
    "    length_feature_array.append(data[\"response_b\"].apply(has_none))  # 応答Bにnullが含まれているか\n",
    "    length_feature_array.append(data[\"response_a\"].apply(has_none) - data[\"response_b\"].apply(has_none))  # nullの差\n",
    "    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)  # 配列を整形\n",
    "    length_feature_array = np.transpose(length_feature_array, (1, 0))  # 行と列を入れ替え\n",
    "    return length_feature_array  # 最終結果を返す\n",
    "\n",
    "train_length_features = get_length_features(train)  # トレーニングデータの長さ特徴を取得\n",
    "print(train_length_features.shape)  # 形状を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42694c2e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### IFD\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### IFD\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1a722",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def get_ifd_features(data: pd.DataFrame):\n",
    "    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gpt_path)\n",
    "    model.eval()\n",
    "    def get_ppl_features(output, instruct=''):\n",
    "        try:\n",
    "            answer_start_index = 0\n",
    "            if instruct != '':\n",
    "                answer_start_index = tokenizer.encode(instruct, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).shape[1]\n",
    "            input_ids = tokenizer.encode(instruct + output, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)\n",
    "            labels = input_ids.clone()\n",
    "            labels[:, :answer_start_index] = -100\n",
    "            with torch.no_grad():\n",
    "                outputs = model(input_ids, labels=labels)\n",
    "            perplexity = torch.exp(outputs.loss)\n",
    "            return perplexity.to('cpu').item()\n",
    "        except:\n",
    "            return 0\n",
    "        \n",
    "    ppl_ifd_feature_array = []\n",
    "    for i in tqdm(range(len(data)), desc=\"Scoring ifd\"):\n",
    "        instruct = data['prompt'][i]\n",
    "        output_a = data['response_a'][i]\n",
    "        output_b = data['response_b'][i]\n",
    "        \n",
    "        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)\n",
    "        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)\n",
    "        try:\n",
    "            ifd_a = ppl_ca_a / ppl_da_a\n",
    "        except ZeroDivisionError:\n",
    "            ifd_a = 0\n",
    "        try:\n",
    "            ifd_b = ppl_ca_b / ppl_da_b\n",
    "        except ZeroDivisionError:\n",
    "            ifd_b = 0\n",
    "        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])\n",
    "    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)\n",
    "    return ppl_ifd_feature_array\n",
    "ppl_ifd_features = get_ifd_features(train)\n",
    "print(ppl_ifd_features.shape)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def get_ifd_features(data: pd.DataFrame):\n",
    "    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)  # GPTモデルを読み込んでデバイスに移動\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gpt_path)  # トークナイザーを読み込む\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    def get_ppl_features(output, instruct=''):\n",
    "        try:\n",
    "            answer_start_index = 0  # 応答の開始位置を初期化\n",
    "            if instruct != '':  # 指示があれば\n",
    "                answer_start_index = tokenizer.encode(instruct, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).shape[1]  # 指示のトークン数を取得\n",
    "            input_ids = tokenizer.encode(instruct + output, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)  # 入力IDを生成\n",
    "            labels = input_ids.clone()  # ラベルをクローン\n",
    "            labels[:, :answer_start_index] = -100  # 指示部分のラベルを無視\n",
    "            with torch.no_grad():  # 勾配計算を無視\n",
    "                outputs = model(input_ids, labels=labels)  # モデルへの入力とラベルを指定して出力を取得\n",
    "            perplexity = torch.exp(outputs.loss)  # パープレキシティを計算\n",
    "            return perplexity.to('cpu').item()  # CPUに戻して値を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    ppl_ifd_feature_array = []  # 特徴の配列を初期化\n",
    "    for i in tqdm(range(len(data)), desc=\"Scoring ifd\"):  # データに対して進捗を表示しながらループ\n",
    "        instruct = data['prompt'][i]  # プロンプトを取得\n",
    "        output_a = data['response_a'][i]  # 応答Aを取得\n",
    "        output_b = data['response_b'][i]  # 応答Bを取得\n",
    "        \n",
    "        # 応答Aのパープレキシティを計算\n",
    "        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)\n",
    "        # 応答Bのパープレキシティを計算\n",
    "        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)\n",
    "        try:\n",
    "            ifd_a = ppl_ca_a / ppl_da_a  # 応答AのIFDを計算\n",
    "        except ZeroDivisionError:\n",
    "            ifd_a = 0  # ゼロ除算が発生した場合は0とする\n",
    "        try:\n",
    "            ifd_b = ppl_ca_b / ppl_da_b  # 応答BのIFDを計算\n",
    "        except ZeroDivisionError:\n",
    "            ifd_b = 0  # ゼロ除算が発生した場合は0とする\n",
    "        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])  # 特徴を追加\n",
    "        \n",
    "    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)  # 特徴の配列を整形\n",
    "    return ppl_ifd_feature_array  # 結果を返す\n",
    "\n",
    "ppl_ifd_features = get_ifd_features(train)  # トレーニングデータのIFD特徴を取得\n",
    "print(ppl_ifd_features.shape)  # 形状を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ifd_features(data: pd.DataFrame):\n",
    "    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)  # GPTモデルを読み込んでデバイスに移動\n",
    "    tokenizer = AutoTokenizer.from_pretrained(gpt_path)  # トークナイザーを読み込む\n",
    "    model.eval()  # 評価モードに設定\n",
    "    \n",
    "    def get_ppl_features(output, instruct=''):\n",
    "        try:\n",
    "            answer_start_index = 0  # 応答の開始位置を初期化\n",
    "            if instruct != '':  # 指示があれば\n",
    "                answer_start_index = tokenizer.encode(instruct, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).shape[1]  # 指示のトークン数を取得\n",
    "            input_ids = tokenizer.encode(instruct + output, return_tensors=\"pt\", truncation=True, max_length=MAX_LENGTH).to(device)  # 入力IDを生成\n",
    "            labels = input_ids.clone()  # ラベルをクローン\n",
    "            labels[:, :answer_start_index] = -100  # 指示部分のラベルを無視\n",
    "            with torch.no_grad():  # 勾配計算を無視\n",
    "                outputs = model(input_ids, labels=labels)  # モデルへの入力とラベルを指定して出力を取得\n",
    "            perplexity = torch.exp(outputs.loss)  # パープレキシティを計算\n",
    "            return perplexity.to('cpu').item()  # CPUに戻して値を返す\n",
    "        except:\n",
    "            return 0  # エラーが発生した場合は0を返す\n",
    "        \n",
    "    ppl_ifd_feature_array = []  # 特徴の配列を初期化\n",
    "    for i in tqdm(range(len(data)), desc=\"Scoring ifd\"):  # データに対して進捗を表示しながらループ\n",
    "        instruct = data['prompt'][i]  # プロンプトを取得\n",
    "        output_a = data['response_a'][i]  # 応答Aを取得\n",
    "        output_b = data['response_b'][i]  # 応答Bを取得\n",
    "        \n",
    "        # 応答Aのパープレキシティを計算\n",
    "        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)\n",
    "        # 応答Bのパープレキシティを計算\n",
    "        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)\n",
    "        try:\n",
    "            ifd_a = ppl_ca_a / ppl_da_a  # 応答AのIFDを計算\n",
    "        except ZeroDivisionError:\n",
    "            ifd_a = 0  # ゼロ除算が発生した場合は0とする\n",
    "        try:\n",
    "            ifd_b = ppl_ca_b / ppl_da_b  # 応答BのIFDを計算\n",
    "        except ZeroDivisionError:\n",
    "            ifd_b = 0  # ゼロ除算が発生した場合は0とする\n",
    "        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])  # 特徴を追加\n",
    "        \n",
    "    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)  # 特徴の配列を整形\n",
    "    return ppl_ifd_feature_array  # 結果を返す\n",
    "\n",
    "ppl_ifd_features = get_ifd_features(train)  # トレーニングデータのIFD特徴を取得\n",
    "print(ppl_ifd_features.shape)  # 形状を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b828dfbd",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "### Combine\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "### Combine\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2227d57",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2\n",
    "ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7\n",
    "train_length_features_csr = csr_matrix(train_length_features)*0.1\n",
    "combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])\n",
    "print(combined_train_vectors.shape)\n",
    "print(\"Vectorizing test text...\")\n",
    "tfidf_test_vectors = get_tfidf_vectors(test)\n",
    "tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2\n",
    "test_ppl_ifd_features = get_ifd_features(test)\n",
    "test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7\n",
    "test_length_features = get_length_features(test)\n",
    "test_length_features_csr = csr_matrix(test_length_features)*0.1\n",
    "combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr]) \n",
    "print(\"Done!\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2  # TF-IDFベクトルをスケーリング\n",
    "ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7  # IFD特徴をスケーリング\n",
    "train_length_features_csr = csr_matrix(train_length_features)*0.1  # 長さ特徴をスケーリング\n",
    "combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])  # ベクトルを結合\n",
    "print(combined_train_vectors.shape)  # 結合後の形状を表示\n",
    "print(\"Vectorizing test text...\")  # テストテキストをベクトル化中...\n",
    "tfidf_test_vectors = get_tfidf_vectors(test)  # テストデータのTF-IDFベクトルを取得\n",
    "tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2  # テストデータのTF-IDFベクトルをスケーリング\n",
    "test_ppl_ifd_features = get_ifd_features(test)  # テストデータのIFD特徴を取得\n",
    "test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7  # テストデータのIFD特徴をスケーリング\n",
    "test_length_features = get_length_features(test)  # テストデータの長さ特徴を取得\n",
    "test_length_features_csr = csr_matrix(test_length_features)*0.1  # テストデータの長さ特徴をスケーリング\n",
    "combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr])  # テストデータのベクトルを結合\n",
    "print(\"Done!\")  # 完了メッセージを表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2  # TF-IDFベクトルをスケーリング\n",
    "ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7  # IFD特徴をスケーリング\n",
    "train_length_features_csr = csr_matrix(train_length_features)*0.1  # 長さ特徴をスケーリング\n",
    "combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])  # ベクトルを結合\n",
    "print(combined_train_vectors.shape)  # 結合後の形状を表示\n",
    "print(\"Vectorizing test text...\")  # テストテキストをベクトル化中...\n",
    "tfidf_test_vectors = get_tfidf_vectors(test)  # テストデータのTF-IDFベクトルを取得\n",
    "tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2  # テストデータのTF-IDFベクトルをスケーリング\n",
    "test_ppl_ifd_features = get_ifd_features(test)  # テストデータのIFD特徴を取得\n",
    "test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7  # テストデータのIFD特徴をスケーリング\n",
    "test_length_features = get_length_features(test)  # テストデータの長さ特徴を取得\n",
    "test_length_features_csr = csr_matrix(test_length_features)*0.1  # テストデータの長さ特徴をスケーリング\n",
    "combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr])  # テストデータのベクトルを結合\n",
    "print(\"Done!\")  # 完了メッセージを表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2582f03d",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "max_estimators = 1000\n",
    "early_stopping_limit = 100\n",
    "\n",
    "# Data preparation\n",
    "X = combined_train_vectors\n",
    "y_encoded = train['label'].values\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'n_estimators': max_estimators,\n",
    "    'max_depth': 4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'objective': 'multiclass',\n",
    "    'num_class': 3,\n",
    "    'metric': 'multi_logloss',\n",
    "    'random_state': 42,\n",
    "    'learning_rate': 0.03,\n",
    "    'verbose': -1  # keep logs quiet\n",
    "}\n",
    "\n",
    "# Create the model\n",
    "model = lgb.LGBMClassifier(**params)\n",
    "\n",
    "# 5-fold cross-validation\n",
    "stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "logloss_scores = []\n",
    "accuracy_scores = []\n",
    "test_pred_list = []\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):\n",
    "    print(f\"\\nFold {fold + 1}\")\n",
    "    X_train_fold, X_val_fold = X[train_indices], X[val_indices]\n",
    "    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]\n",
    "\n",
    "    def callback(env):\n",
    "        if env.iteration % 10 == 0: print (\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,\n",
    "        eval_set=[(X_val_fold, y_val_fold)],\n",
    "        eval_metric='multi_logloss',\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]\n",
    "    )\n",
    "\n",
    "    y_pred_proba_fold = model.predict_proba(X_val_fold)\n",
    "    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)\n",
    "    logloss_scores.append(logloss_fold)\n",
    "    print(f\"Log Loss: {logloss_fold}\")\n",
    "    \n",
    "    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)\n",
    "    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)\n",
    "    accuracy_scores.append(accuracy_fold)\n",
    "    print(f\"Accuracy: {accuracy_fold}\")\n",
    "\n",
    "    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))\n",
    "\n",
    "# Calculate and print average scores\n",
    "average_logloss = np.mean(logloss_scores)\n",
    "average_accuracy = np.mean(accuracy_scores)\n",
    "print(f\"\\nAverage Log Loss: {average_logloss}\")\n",
    "print(f\"Average Accuracy: {average_accuracy}\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "max_estimators = 1000  # 最大推定器の数を設定\n",
    "early_stopping_limit = 100  # 早期停止の制限を設定\n",
    "\n",
    "# データ準備\n",
    "X = combined_train_vectors  # 特徴ベクトル\n",
    "y_encoded = train['label'].values  # ラベルを取得\n",
    "\n",
    "# LightGBMのパラメータ\n",
    "params = {\n",
    "    'n_estimators': max_estimators,  # 推定器の数\n",
    "    'max_depth': 4,  # 最大深さ\n",
    "    'subsample': 0.8,  # サンプリングフラクション\n",
    "    'colsample_bytree': 0.8,  # 特徴のサンプリングフラクション\n",
    "    'objective': 'multiclass',  # 目的関数\n",
    "    'num_class': 3,  # クラス数\n",
    "    'metric': 'multi_logloss',  # 評価指標\n",
    "    'random_state': 42,  # 乱数シード\n",
    "    'learning_rate': 0.03,  # 学習率\n",
    "    'verbose': -1  # ログの出力を抑制\n",
    "}\n",
    "\n",
    "# モデルの作成\n",
    "model = lgb.LGBMClassifier(**params)  # LightGBMクラス分類器を生成\n",
    "\n",
    "# 5フォールドの交差検証\n",
    "stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # StratifiedKFoldを設定\n",
    "logloss_scores = []  # loglossスコアを保持するリスト\n",
    "accuracy_scores = []  # 精度スコアを保持するリスト\n",
    "test_pred_list = []  # テストの予測結果を保持するリスト\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):  # 各フォールドに対して\n",
    "    print(f\"\\nFold {fold + 1}\")  # フォールドの番号を表示\n",
    "    X_train_fold, X_val_fold = X[train_indices], X[val_indices]  # フォールドのトレーニングデータとバリデーションデータ\n",
    "    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]  # フォールドのトレーニングラベルとバリデーションラベル\n",
    "\n",
    "    def callback(env):\n",
    "        if env.iteration % 10 == 0: print(\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])  # ログ出力のコールバック関数\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,  # トレーニングデータとトレーニングラベルでフィット\n",
    "        eval_set=[(X_val_fold, y_val_fold)],  # バリデーションデータを指定\n",
    "        eval_metric='multi_logloss',  # 評価指標を指定\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]  # 早期停止とコールバックを設定\n",
    "    )\n",
    "\n",
    "    y_pred_proba_fold = model.predict_proba(X_val_fold)  # バリデーションデータに対する予測確率\n",
    "    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)  # loglossを計算\n",
    "    logloss_scores.append(logloss_fold)  # スコアを追加\n",
    "    print(f\"Log Loss: {logloss_fold}\")  # loglossを表示\n",
    "    \n",
    "    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)  # 最大の予測確率を持つクラスを取得\n",
    "    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)  # 精度を計算\n",
    "    accuracy_scores.append(accuracy_fold)  # スコアを追加\n",
    "    print(f\"Accuracy: {accuracy_fold}\")  # 精度を表示\n",
    "\n",
    "    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))  # テストデータの予測確率を追加\n",
    "\n",
    "# 平均スコアを計算して表示\n",
    "average_logloss = np.mean(logloss_scores)  # 平均loglossを計算\n",
    "average_accuracy = np.mean(accuracy_scores)  # 平均精度を計算\n",
    "print(f\"\\nAverage Log Loss: {average_logloss}\")  # 平均loglossを表示\n",
    "print(f\"Average Accuracy: {average_accuracy}\")  # 平均精度を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 1122.903557,
     "end_time": "2024-05-07T01:06:29.367194",
     "exception": false,
     "start_time": "2024-05-07T00:47:46.463637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "\n",
    "max_estimators = 1000  # 最大推定器の数を設定\n",
    "early_stopping_limit = 100  # 早期停止の制限を設定\n",
    "\n",
    "# データ準備\n",
    "X = combined_train_vectors  # 特徴ベクトル\n",
    "y_encoded = train['label'].values  # ラベルを取得\n",
    "\n",
    "# LightGBMのパラメータ\n",
    "params = {\n",
    "    'n_estimators': max_estimators,  # 推定器の数\n",
    "    'max_depth': 4,  # 最大深さ\n",
    "    'subsample': 0.8,  # サンプリングフラクション\n",
    "    'colsample_bytree': 0.8,  # 特徴のサンプリングフラクション\n",
    "    'objective': 'multiclass',  # 目的関数\n",
    "    'num_class': 3,  # クラス数\n",
    "    'metric': 'multi_logloss',  # 評価指標\n",
    "    'random_state': 42,  # 乱数シード\n",
    "    'learning_rate': 0.03,  # 学習率\n",
    "    'verbose': -1  # ログの出力を抑制\n",
    "}\n",
    "\n",
    "# モデルの作成\n",
    "model = lgb.LGBMClassifier(**params)  # LightGBMクラス分類器を生成\n",
    "\n",
    "# 5フォールドの交差検証\n",
    "stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # StratifiedKFoldを設定\n",
    "logloss_scores = []  # loglossスコアを保持するリスト\n",
    "accuracy_scores = []  # 精度スコアを保持するリスト\n",
    "test_pred_list = []  # テストの予測結果を保持するリスト\n",
    "\n",
    "for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):  # 各フォールドに対して\n",
    "    print(f\"\\nFold {fold + 1}\")  # フォールドの番号を表示\n",
    "    X_train_fold, X_val_fold = X[train_indices], X[val_indices]  # フォールドのトレーニングデータとバリデーションデータ\n",
    "    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]  # フォールドのトレーニングラベルとバリデーションラベル\n",
    "\n",
    "    def callback(env):\n",
    "        if env.iteration % 10 == 0: print(\"Iteration:\", env.iteration, \"\\tLog Loss:\", env.evaluation_result_list[0][2])  # ログ出力のコールバック関数\n",
    "\n",
    "    model.fit(\n",
    "        X_train_fold, y_train_fold,  # トレーニングデータとトレーニングラベルでフィット\n",
    "        eval_set=[(X_val_fold, y_val_fold)],  # バリデーションデータを指定\n",
    "        eval_metric='multi_logloss',  # 評価指標を指定\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]  # 早期停止とコールバックを設定\n",
    "    )\n",
    "\n",
    "    y_pred_proba_fold = model.predict_proba(X_val_fold)  # バリデーションデータに対する予測確率\n",
    "    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)  # loglossを計算\n",
    "    logloss_scores.append(logloss_fold)  # スコアを追加\n",
    "    print(f\"Log Loss: {logloss_fold}\")  # loglossを表示\n",
    "    \n",
    "    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)  # 最大の予測確率を持つクラスを取得\n",
    "    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)  # 精度を計算\n",
    "    accuracy_scores.append(accuracy_fold)  # スコアを追加\n",
    "    print(f\"Accuracy: {accuracy_fold}\")  # 精度を表示\n",
    "\n",
    "    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))  # テストデータの予測確率を追加\n",
    "\n",
    "# 平均スコアを計算して表示\n",
    "average_logloss = np.mean(logloss_scores)  # 平均loglossを計算\n",
    "average_accuracy = np.mean(accuracy_scores)  # 平均精度を計算\n",
    "print(f\"\\nAverage Log Loss: {average_logloss}\")  # 平均loglossを表示\n",
    "print(f\"Average Accuracy: {average_accuracy}\")  # 平均精度を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fbd95b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "preds_test = np.mean(test_pred_list, axis=0)\n",
    "submission = pd.DataFrame({\n",
    "    'id': test[\"id\"],\n",
    "    'winner_model_a': preds_test[:, 0],\n",
    "    'winner_model_b': preds_test[:, 1], \n",
    "    'winner_tie': preds_test[:, 2]\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "display(submission)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "preds_test = np.mean(test_pred_list, axis=0)  # テスト予測の平均を計算\n",
    "submission = pd.DataFrame({\n",
    "    'id': test[\"id\"],  # テストデータのID\n",
    "    'winner_model_a': preds_test[:, 0],  # モデルAの勝者予測\n",
    "    'winner_model_b': preds_test[:, 1],  # モデルBの勝者予測\n",
    "    'winner_tie': preds_test[:, 2]  # タイの勝者予測\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)  # 提出ファイルをCSVとして保存\n",
    "display(submission)  # 提出内容を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.106303,
     "end_time": "2024-05-07T01:06:34.373427",
     "exception": false,
     "start_time": "2024-05-07T01:06:34.267124",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "preds_test = np.mean(test_pred_list, axis=0)  # テスト予測の平均を計算\n",
    "submission = pd.DataFrame({\n",
    "    'id': test[\"id\"],  # テストデータのID\n",
    "    'winner_model_a': preds_test[:, 0],  # モデルAの勝者予測\n",
    "    'winner_model_b': preds_test[:, 1],  # モデルBの勝者予測\n",
    "    'winner_tie': preds_test[:, 2]  # タイの勝者予測\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)  # 提出ファイルをCSVとして保存\n",
    "display(submission)  # 提出内容を表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 2210196,
     "sourceId": 3693646,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5306724,
     "sourceId": 8821100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5306732,
     "sourceId": 8821109,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5313994,
     "sourceId": 8831552,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5314871,
     "sourceId": 8832702,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2185.862577,
   "end_time": "2024-05-07T01:06:39.269064",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-07T00:30:13.406487",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
