{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3fdec13",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このノートブックは、Hugging FaceのTransformersライブラリを使用して、テキスト分類モデルのロード、前処理、およびトレーニングに関する包括的なガイドを提供しています。具体的には、人間による好み予測に関連するデータセットを扱い、プロンプトに対する応答モデルのパフォーマンスを評価します。以下にノートブックの各セクションの概要を示します。\n",
    "\n",
    "1. **データのロードとサンプリング**:\n",
    "   - CSVファイルからデータを読み込み、全体のデータセットの10%をランダムにサンプリングします。これにより、計算リソースを効率的に使いながら処理を迅速化します。\n",
    "\n",
    "2. **データの分割**:\n",
    "   - サンプリングしたデータをトレーニングセットとバリデーションセットに分割し、モデル性能の評価を可能にします。\n",
    "\n",
    "3. **トークナイザーの準備**:\n",
    "   - 小型のRobertaモデルのトークナイザーを使用して、テキストデータをトークン化します。この処理には、パディング、切り捨て、およびPyTorchテンソル形式での戻り値が含まれています。\n",
    "\n",
    "4. **モデル設定とトレーニング**:\n",
    "   - モデルの設定を行い、トレーニング引数を定義した後、トレーニングを開始します。デバイスは、利用可能な場合はGPUを使用します。\n",
    "\n",
    "5. **推論関数の定義**:\n",
    "   - トレーニングされたモデルを使用して、トークン化されたデータに対して推論を行う関数を定義します。これにより、ノートブックは推論時のメモリ管理を最適化します。\n",
    "\n",
    "6. **評価と結果の保存**:\n",
    "   - テストデータをトークン化し、推論を行った結果をCSVファイルに保存します。最終的に、モデルのロジット結果とデータフレームを結合し、提出用のファイルを作成します。\n",
    "\n",
    "このノートブックは、データサンプリング、トークン化、モデル設定、推論、および結果保存という体系的なアプローチを通じて、テキスト分類モデルのトレーニングの流れを示しています。特に、計算リソースの効率的な使用を保ちながらモデルのパフォーマンスを維持することに重点が置かれています。使用されている主要なライブラリは、Pandas、Scikit-learn、およびHugging Face Transformersです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac1e8d0",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyterノートブックで使用されている専門用語や概念の簡単な解説を示します。特に初心者がつまずきそうなものやこのノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "1. **トークナイザー**:\n",
    "   - テキストデータをモデルが理解できる形に変換するためのツールです。具体的には文字や単語を「トークン」と呼ばれる単位に分割し、数値的な表現に変換します。\n",
    "\n",
    "2. **ロバート (RoBERTa)**:\n",
    "   - BERT（Bidirectional Encoder Representations from Transformers）を基にしたモデルで、自然言語処理タスクにおける性能を向上させるために、より多くのデータと計算資源を使用してトレーニングされています。\n",
    "\n",
    "3. **エンコーディング**:\n",
    "   - トークン化されたデータに対して、モデルが理解できる数値表現（通常はベクトル）の形式に変換するプロセスです。これは、トークンに対応する数値を用意することを含みます。\n",
    "\n",
    "4. **バッチサイズ**:\n",
    "   - モデルが一度に処理するデータのサンプル数を指します。大きすぎるとメモリが不足し、小さすぎると効率が悪くなります。\n",
    "\n",
    "5. **ウォームアップステップ**:\n",
    "   - 学習の初期段階で学習率を徐々に上げていく手法です。これにより、最初のエポックでの急激な変化による不安定さを避けます。\n",
    "\n",
    "6. **トレーニング引数 (Training Arguments)**:\n",
    "   - モデルトレーニング時に指定する設定やハイパーパラメータのことです。これにはエポック数、バッチサイズ、ロギングの頻度などが含まれます。\n",
    "\n",
    "7. **ロジット (Logits)**:\n",
    "   - モデルの出力で、各クラスに対するスコアを示す値です。これをsoftmax関数に通すことで、確率に変換されます。ロジットが高いほど、そのクラスが選ばれる可能性が高いとなります。\n",
    "\n",
    "8. **アテンションマスク (Attention Mask)**:\n",
    "   - トークン化されたデータの中で、どのトークンが有効か（例えば、パディングしたトークンを無視するためのマスク）を示すバイナリマスクです。1は有効、0は無効を表します。\n",
    "\n",
    "9. **カスタムデータセットクラス**:\n",
    "   - PyTorchのデータセットクラスを継承して作成したクラスで、トレーニングデータとラベルを格納し、データを効率よく取得できるようにカスタマイズされています。\n",
    "\n",
    "これらの用語や概念は、特に実務経験がない初心者にとって馴染みが少ないかもしれませんが、機械学習プロジェクトを実施する際には重要です。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc0279f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Introduction\n",
    " This notebook provides a comprehensive guide for loading, preprocessing, and training a text classification model using the Hugging Face Transformers library. We will use the Roberta tokenizer and a smaller Roberta model to manage computational resources efficiently. The process includes data loading, sampling, tokenization, and model preparation.\n",
    "\n",
    "# Step 1: Load and Sample Data\n",
    " First, we load the dataset from a CSV file and take a random sample to reduce the dataset size for quicker processing.\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# はじめに\n",
    "このノートブックは、Hugging Face Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する包括的なガイドを提供します。計算リソースを効率的に管理するために、ロバートトークナイザーと小型のロバートモデルを使用します。プロセスには、データのロード、サンプリング、トークナイゼーション、およびモデルの準備が含まれます。\n",
    "\n",
    "# ステップ 1: データのロードとサンプリング\n",
    "最初に、CSVファイルからデータセットをロードし、処理を迅速化するためにデータセットのサイズを減少させるためにランダムサンプルを取得します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f74eb2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "\n",
    "# Sample 10% of the data\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# Check sample data\n",
    "print(df_sample.head())\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データをロードする\n",
    "df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "\n",
    "# データの10%をサンプリングする\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# サンプルデータを確認する\n",
    "print(df_sample.head())  # サンプルデータの最初の5行を表示します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:15.596812Z",
     "iopub.status.busy": "2024-07-28T22:20:15.59644Z",
     "iopub.status.idle": "2024-07-28T22:20:17.317719Z",
     "shell.execute_reply": "2024-07-28T22:20:17.316693Z",
     "shell.execute_reply.started": "2024-07-28T22:20:15.596784Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# データをロードする\n",
    "df = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "\n",
    "# データの10%をサンプリングする\n",
    "df_sample = df.sample(frac=0.1, random_state=42)\n",
    "\n",
    "# サンプルデータを確認する\n",
    "print(df_sample.head())  # サンプルデータの最初の5行を表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486c483f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Step 2: Split Data into Training and Validation Sets\n",
    "We split the data into training and validation sets to evaluate the model's performance.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ステップ 2: データをトレーニングセットとバリデーションセットに分割する\n",
    "データをトレーニングセットとバリデーションセットに分割し、モデルのパフォーマンスを評価します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd5b3b5",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Split data into training and validation sets\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_sample['prompt'].tolist(), \n",
    "    df_sample['winner_model_a'], \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# データをトレーニングセットとバリデーションセットに分割する\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_sample['prompt'].tolist(),  # プロンプト（入力テキスト）のリストを取得します\n",
    "    df_sample['winner_model_a'],    # モデルAの勝者ラベルを取得します\n",
    "    test_size=0.1,                  # データの10%をテストセットとします\n",
    "    random_state=42                 # 再現性のために乱数シードを設定します\n",
    ")\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:17.319584Z",
     "iopub.status.busy": "2024-07-28T22:20:17.319291Z",
     "iopub.status.idle": "2024-07-28T22:20:17.329557Z",
     "shell.execute_reply": "2024-07-28T22:20:17.328559Z",
     "shell.execute_reply.started": "2024-07-28T22:20:17.31956Z"
    }
   },
   "outputs": [],
   "source": [
    "# データをトレーニングセットとバリデーションセットに分割する\n",
    "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
    "    df_sample['prompt'].tolist(),  # プロンプト（入力テキスト）のリストを取得します\n",
    "    df_sample['winner_model_a'],    # モデルAの勝者ラベルを取得します\n",
    "    test_size=0.1,                  # データの10%をテストセットとします\n",
    "    random_state=42                 # 再現性のために乱数シードを設定します\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4094f60",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Step 3: Load and Prepare the Tokenizer\n",
    "We use a smaller Roberta model tokenizer for efficiency and tokenize the text data.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ステップ 3: トークナイザーをロードして準備する\n",
    "効率のために小型のロバートモデルのトークナイザーを使用し、テキストデータをトークン化します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22dbdbc2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "# Tokenize texts\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "val_encodings = tokenize_function(val_texts)\n",
    "\n",
    "# Check tokenized data\n",
    "print(train_encodings)\n",
    "print(val_encodings)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# トークナイザーをロードする\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # ロバートベースモデルのトークナイザーを取得します\n",
    "\n",
    "# トークン化関数\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # トークンのパディングを最大長に設定します\n",
    "        truncation=True,        # テキストが最大長を超えた場合に切り捨てます\n",
    "        max_length=128,        # 最大トークン数を128に設定します\n",
    "        return_tensors='pt'    # PyTorchテンソルとして戻します\n",
    "    )\n",
    "\n",
    "# テキストをトークン化する\n",
    "train_encodings = tokenize_function(train_texts)  # トレーニングデータをトークン化します\n",
    "val_encodings = tokenize_function(val_texts)      # バリデーションデータをトークン化します\n",
    "\n",
    "# トークン化されたデータを確認する\n",
    "print(train_encodings)  # トークン化されたトレーニングデータを表示します\n",
    "print(val_encodings)    # トークン化されたバリデーションデータを表示します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:17.331562Z",
     "iopub.status.busy": "2024-07-28T22:20:17.330852Z",
     "iopub.status.idle": "2024-07-28T22:20:24.153345Z",
     "shell.execute_reply": "2024-07-28T22:20:24.152411Z",
     "shell.execute_reply.started": "2024-07-28T22:20:17.331527Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer\n",
    "\n",
    "# トークナイザーをロードする\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")  # ロバートベースモデルのトークナイザーを取得します\n",
    "\n",
    "# トークン化関数\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(\n",
    "        texts,\n",
    "        padding=\"max_length\",  # トークンのパディングを最大長に設定します\n",
    "        truncation=True,        # テキストが最大長を超えた場合に切り捨てます\n",
    "        max_length=128,        # 最大トークン数を128に設定します\n",
    "        return_tensors='pt'    # PyTorchテンソルとして戻します\n",
    "    )\n",
    "\n",
    "# テキストをトークン化する\n",
    "train_encodings = tokenize_function(train_texts)  # トレーニングデータをトークン化します\n",
    "val_encodings = tokenize_function(val_texts)      # バリデーションデータをトークン化します\n",
    "\n",
    "# トークン化されたデータを確認する\n",
    "print(train_encodings)  # トークン化されたトレーニングデータを表示します\n",
    "print(val_encodings)    # トークン化されたバリデーションデータを表示します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25ceaca",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Step 4: Prepare the Model Configuration\n",
    " Set up the configuration for the model, ensuring it uses the appropriate device (GPU if available).\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ステップ 4: モデルの設定を準備する\n",
    "モデルの設定を行い、適切なデバイス（利用可能な場合はGPU）を使用するようにします。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053302f6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Config class\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.batch_size = 16\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(cfg.device)\n",
    "\n",
    "# CustomDataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = np.array(labels, dtype=int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)\n",
    "\n",
    "# TrainingArguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=cfg.batch_size,\n",
    "    per_device_eval_batch_size=cfg.batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 設定クラス\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUが使用可能であればGPUを、そうでなければCPUを使用します\n",
    "        self.batch_size = 16  # バッチサイズを16に設定します\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# モデルをロードする\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(cfg.device)  # ロバートベースモデルを2つのラベルで分類するためにロードし、指定したデバイスに移動します\n",
    "\n",
    "# カスタムデータセットクラス\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # エンコーディングを格納します\n",
    "        self.labels = np.array(labels, dtype=int)  # ラベルを整数のNumPy配列として格納します\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 指定したインデックスのエンコーディングとラベルを取得します\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}  # エンコーディングをテンソル化します\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # ラベルをテンソル化します\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # データセットの長さを返します\n",
    "\n",
    "# データセットを作成する\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)  # トレーニングデータセットを作成します\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)        # バリデーションデータセットを作成します\n",
    "\n",
    "# トレーニング引数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                # 結果を保存するディレクトリ\n",
    "    num_train_epochs=3,                    # トレーニングを行うエポック数\n",
    "    per_device_train_batch_size=cfg.batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "    per_device_eval_batch_size=cfg.batch_size,   # デバイスごとの評価バッチサイズ\n",
    "    warmup_steps=500,                       # ウォームアップステップ数\n",
    "    weight_decay=0.01,                     # 重み減衰率\n",
    "    logging_dir='./logs',                   # ログを保存するディレクトリ\n",
    "    logging_steps=10,                       # ロギングのステップ頻度\n",
    ")\n",
    "\n",
    "# トレーナーを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,                            # モデル\n",
    "    args=training_args,                     # トレーニング引数\n",
    "    train_dataset=train_dataset,            # トレーニングデータセット\n",
    "    eval_dataset=val_dataset                 # バリデーションデータセット\n",
    ")\n",
    "\n",
    "# モデルをトレーニングする\n",
    "trainer.train()  # モデルのトレーニングを開始します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:20:24.155417Z",
     "iopub.status.busy": "2024-07-28T22:20:24.15514Z",
     "iopub.status.idle": "2024-07-28T22:24:09.126503Z",
     "shell.execute_reply": "2024-07-28T22:24:09.125607Z",
     "shell.execute_reply.started": "2024-07-28T22:20:24.155391Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 設定クラス\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # GPUが使用可能であればGPUを、そうでなければCPUを使用します\n",
    "        self.batch_size = 16  # バッチサイズを16に設定します\n",
    "\n",
    "cfg = Config()\n",
    "\n",
    "# モデルをロードする\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2).to(cfg.device)  # ロバートベースモデルを2つのラベルで分類するためにロードし、指定したデバイスに移動します\n",
    "\n",
    "# カスタムデータセットクラス\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings  # エンコーディングを格納します\n",
    "        self.labels = np.array(labels, dtype=int)  # ラベルを整数のNumPy配列として格納します\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 指定したインデックスのエンコーディングとラベルを取得します\n",
    "        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}  # エンコーディングをテンソル化します\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # ラベルをテンソル化します\n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)  # データセットの長さを返します\n",
    "\n",
    "# データセットを作成する\n",
    "train_dataset = CustomDataset(train_encodings, train_labels)  # トレーニングデータセットを作成します\n",
    "val_dataset = CustomDataset(val_encodings, val_labels)        # バリデーションデータセットを作成します\n",
    "\n",
    "# トレーニング引数\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',                # 結果を保存するディレクトリ\n",
    "    num_train_epochs=3,                    # トレーニングを行うエポック数\n",
    "    per_device_train_batch_size=cfg.batch_size,  # デバイスごとのトレーニングバッチサイズ\n",
    "    per_device_eval_batch_size=cfg.batch_size,   # デバイスごとの評価バッチサイズ\n",
    "    warmup_steps=500,                       # ウォームアップステップ数\n",
    "    weight_decay=0.01,                     # 重み減衰率\n",
    "    logging_dir='./logs',                   # ログを保存するディレクトリ\n",
    "    logging_steps=10,                       # ロギングのステップ頻度\n",
    ")\n",
    "\n",
    "# トレーナーを初期化する\n",
    "trainer = Trainer(\n",
    "    model=model,                            # モデル\n",
    "    args=training_args,                     # トレーニング引数\n",
    "    train_dataset=train_dataset,            # トレーニングデータセット\n",
    "    eval_dataset=val_dataset                 # バリデーションデータセット\n",
    ")\n",
    "\n",
    "# モデルをトレーニングする\n",
    "trainer.train()  # モデルのトレーニングを開始します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25348f0f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Step 5: Inference Function\n",
    "Define a function to perform inference on the tokenized data using the trained model.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ステップ 5: 推論関数\n",
    "トレーニングされたモデルを使用してトークン化されたデータに対して推論を行う関数を定義します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab8eb5c",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):\n",
    "    model.eval()\n",
    "    results = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(input_ids), batch_size):\n",
    "            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)\n",
    "            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)\n",
    "            results.extend(outputs.logits.cpu().numpy())\n",
    "    return np.array(results)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):\n",
    "    model.eval()  # モデルを評価モードに設定します\n",
    "    results = []  # 結果を格納するリストを初期化します\n",
    "    with torch.no_grad():  # 勾配計算を無効にしてメモリを節約します\n",
    "        for i in range(0, len(input_ids), batch_size):  # バッチサイズごとにループします\n",
    "            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)  # バッチの入力IDをデバイスに移動します\n",
    "            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)  # バッチのアテンションマスクをデバイスに移動します\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)  # モデルの出力を取得します\n",
    "            results.extend(outputs.logits.cpu().numpy())  # 出力のロジットをCPUに移動し、NumPy配列に変換して結果に追加します\n",
    "    return np.array(results)  # 結果をNumPy配列として返します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:24:09.128408Z",
     "iopub.status.busy": "2024-07-28T22:24:09.128045Z",
     "iopub.status.idle": "2024-07-28T22:24:09.13874Z",
     "shell.execute_reply": "2024-07-28T22:24:09.137869Z",
     "shell.execute_reply.started": "2024-07-28T22:24:09.128367Z"
    }
   },
   "outputs": [],
   "source": [
    "def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):\n",
    "    model.eval()  # モデルを評価モードに設定します\n",
    "    results = []  # 結果を格納するリストを初期化します\n",
    "    with torch.no_grad():  # 勾配計算を無効にしてメモリを節約します\n",
    "        for i in range(0, len(input_ids), batch_size):  # バッチサイズごとにループします\n",
    "            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)  # バッチの入力IDをデバイスに移動します\n",
    "            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)  # バッチのアテンションマスクをデバイスに移動します\n",
    "            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)  # モデルの出力を取得します\n",
    "            results.extend(outputs.logits.cpu().numpy())  # 出力のロジットをCPUに移動し、NumPy配列に変換して結果に追加します\n",
    "    return np.array(results)  # 結果をNumPy配列として返します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867769b4",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Step 6: Evaluate and Save Results\n",
    "Tokenize the test data, perform inference, and save the results to a CSV file.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# ステップ 6: 評価と結果の保存\n",
    "テストデータをトークン化し、推論を行い、結果をCSVファイルに保存します。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2c27e1",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Tokenize the test data\n",
    "test_texts = df_sample['prompt'].tolist()\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Perform inference\n",
    "results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])\n",
    "\n",
    "# Convert results to DataFrame and save as CSV\n",
    "results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])\n",
    "submission = pd.concat([df_sample[['id']], results_df], axis=1)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# テストデータをトークン化する\n",
    "test_texts = df_sample['prompt'].tolist()  # プロンプトのリストを取得します\n",
    "test_encodings = tokenize_function(test_texts)  # テストデータをトークン化します\n",
    "\n",
    "# 推論を行う\n",
    "results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])  # トークン化されたデータに対して推論を実行します\n",
    "\n",
    "# 結果をDataFrameに変換し、CSVとして保存する\n",
    "results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])  # ロジットの結果をDataFrameに変換します\n",
    "submission = pd.concat([df_sample[['id']], results_df], axis=1)  # IDと結果を結合します\n",
    "submission.to_csv('submission.csv', index=False)  # CSVファイルとして保存します\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T22:24:09.140244Z",
     "iopub.status.busy": "2024-07-28T22:24:09.139927Z",
     "iopub.status.idle": "2024-07-28T22:24:52.526467Z",
     "shell.execute_reply": "2024-07-28T22:24:52.52548Z",
     "shell.execute_reply.started": "2024-07-28T22:24:09.140214Z"
    }
   },
   "outputs": [],
   "source": [
    "# テストデータをトークン化する\n",
    "test_texts = df_sample['prompt'].tolist()  # プロンプトのリストを取得します\n",
    "test_encodings = tokenize_function(test_texts)  # テストデータをトークン化します\n",
    "\n",
    "# 推論を行う\n",
    "results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])  # トークン化されたデータに対して推論を実行します\n",
    "\n",
    "# 結果をDataFrameに変換し、CSVとして保存する\n",
    "results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])  # ロジットの結果をDataFrameに変換します\n",
    "submission = pd.concat([df_sample[['id']], results_df], axis=1)  # IDと結果を結合します\n",
    "submission.to_csv('submission.csv', index=False)  # CSVファイルとして保存します"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764996b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Conclusion\n",
    "This notebook provided a structured approach to loading, preprocessing, and training a text classification model using the Transformers library. The steps included data sampling, tokenization, model configuration, and inference. The final results were saved for further analysis. This methodology ensures efficient use of computational resources while maintaining model performance.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 結論\n",
    "このノートブックでは、Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する体系的なアプローチを提供しました。ステップにはデータのサンプリング、トークン化、モデルの設定、推論が含まれています。最終的な結果はさらなる分析のために保存されました。この方法論は、モデルのパフォーマンスを維持しながら計算リソースの効率的な使用を確保します。\n",
    "\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "modelId": 76277,
     "modelInstanceId": 58238,
     "sourceId": 69788,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
