{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b11889",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションに関連し、特にPrompt/Responseに対してキーワード抽出を行うことに焦点を当てています。具体的には、KeyBERTライブラリを使用して、提供されたテキストデータからキーフレーズやキーワードを抽出し、それをもとにトレーニングとテストデータセットを作成することを目的としています。\n",
    "\n",
    "### 適用されている手法とライブラリ\n",
    "1. **KeyBERT**: Hugging FaceのTransformersライブラリに基づいているキーワード抽出ライブラリです。このノートブックでは、事前学習済みのモデル「distilbert-base-nli-mean-tokens」を利用して、与えられたテキストから有用なキーワードを抽出します。\n",
    "\n",
    "2. **データ処理ライブラリ**:\n",
    "   - **NumPy**: 数値計算のためのライブラリ。\n",
    "   - **Pandas**: データ操作や分析のためのライブラリ。\n",
    "   - **Matplotlib**: データの可視化を行うためのライブラリ。\n",
    "\n",
    "### 処理の流れ\n",
    "- 最初に、KeyBERTライブラリをインストールし、必要なライブラリをインポートします。\n",
    "- トレーニングとテスト用のデータセットをCSVファイルから読み込み、各データセットに新しいカラムを追加して初期化します。\n",
    "- トレーニングデータとテストデータそれぞれの'prompt'、'response_a'、'response_b'各列からキーワードを抽出し、対応するカラムに保存します。\n",
    "- 抽出したキーワードに基づいて、新たに作成されたカラムを組み合わせてコンパイルし、最終的なデータセットを形成します。\n",
    "- 最後に、処理されたトレーニングデータとテストデータをそれぞれCSVファイルとして保存します。\n",
    "\n",
    "このノートブックは、チャットボットの応答の品質を向上させるための前処理ステップとして、キーワード抽出機能を効果的に利用することを目指しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658d2e60",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyterノートブックの内容に基づいて、初心者がつまずきそうな専門用語の解説をまとめました。特に、実務経験がないと馴染みのない領域や、このノートブック特有のドメイン知識に焦点を当てています。\n",
    "\n",
    "### 専門用語の解説\n",
    "\n",
    "1. **KeyBERT**:\n",
    "   - キーワード抽出やキーフレーズ抽出のためのPythonライブラリ。BERT（Bidirectional Encoder Representations from Transformers）を基にしたモデルを使用し、与えられたテキストから重要な単語やフレーズを抽出することができる。\n",
    "\n",
    "2. **Hugging Face Transformers**:\n",
    "   - 自然言語処理（NLP）タスクのためのライブラリで、さまざまな事前学習済みトランスフォーマーモデルが提供されている。BERTやGPTなど、最新のモデルを利用することで、テキスト分析や生成などが簡単に行えるようになる。\n",
    "\n",
    "3. **DistilBERT**:\n",
    "   - BERTモデルを軽量化したもので、性能を維持しつつ計算資源を削減したモデル。トランスフォーマーのアーキテクチャを基にしつつ、サイズが小さく、より高速に推論が行える。\n",
    "\n",
    "4. **トークナイゼーション (Tokenization)**:\n",
    "   - テキストデータを単語やサブワードなどの「トークン」に分割する過程。言語モデルがテキストを理解できる形式に変換するための重要なステップ。\n",
    "\n",
    "5. **キーワード抽出 (Keyword Extraction)**:\n",
    "   - テキストからその内容を要約したり代表するのに重要な単語を特定するプロセス。情報検索やテキスト要約などのタスクで使用される。\n",
    "\n",
    "6. **トップNキーワード (Top-N Keywords)**:\n",
    "   - 特定のテキストから抽出されたキーワードの中で、重要な上位N個を抽出するプロセス。たとえば、`top_n=5`と指定した場合、上位5つのキーワードが出力される。\n",
    "\n",
    "7. **Pandas データフレーム**:\n",
    "   - 構造化データを処理するための2次元のラベル付きデータ構造。行と列からなるテーブル形式でデータを扱うことができるため、データの前処理や分析を容易に行うことができる。\n",
    "\n",
    "8. **CSV (Comma-Separated Values)**:\n",
    "   - データをカンマ区切りで格納したテキストファイル形式。表形式のデータを簡単に保存したり、他のソフトウェアとデータのやり取りを行うために広く使用されている。\n",
    "\n",
    "9. **インデックス (Index)**:\n",
    "   - データフレームや配列における位置を示すラベル。CSVファイルを読み込む際に自動的に作成される場合があり、それが必要ない場合には`index=False`の指定によってインデックスを保存しないようにできる。\n",
    "\n",
    "10. **データの可視化 (Data Visualization)**:\n",
    "    - データをグラフやチャート、マップなどの視覚的手法によって表現すること。データ分析の結果を直感的に理解するための重要な技術。\n",
    "\n",
    "これらの解説が、Jupyterノートブックの理解を助ける一助となれば幸いです。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cd4092",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# LMSYS Prompt/Response Words KeyBERT\n",
    "\n",
    "\n",
    "\n",
    "KeyBERT is a Python library for keyword extraction and keyphrase extraction. It is built on top of the Hugging Face Transformers library and leverages pre-trained transformer models, such as BERT, to extract key phrases or keywords from a given text. KeyBERT is particularly useful for tasks like document summarization, content analysis, and information retrieval.\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LMSYS プロンプト/レスポンス ワーズ KeyBERT\n",
    "\n",
    "KeyBERTは、キーワード抽出およびキーフレーズ抽出のためのPythonライブラリです。これはHugging Face Transformersライブラリの上に構築されており、BERTのような事前学習済みのトランスフォーマーモデルを利用して、与えられたテキストからキーフレーズやキーワードを抽出します。KeyBERTは、文書要約、コンテンツ分析、情報検索などのタスクに特に有用です。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93e87ec",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "!pip install keybert\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "!pip install keybert  # KeyBERTライブラリをインストールするコマンドです。このコマンドを実行することで、キーワード抽出機能を利用できるようになります。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-28T14:33:41.208147Z",
     "iopub.status.busy": "2024-07-28T14:33:41.207826Z",
     "iopub.status.idle": "2024-07-28T14:33:55.646158Z",
     "shell.execute_reply": "2024-07-28T14:33:55.645245Z",
     "shell.execute_reply.started": "2024-07-28T14:33:41.208121Z"
    },
    "papermill": {
     "duration": 0.028693,
     "end_time": "2023-05-15T04:09:36.103904",
     "exception": false,
     "start_time": "2023-05-15T04:09:36.075211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install keybert  # KeyBERTライブラリをインストールするコマンドです。このコマンドを実行することで、キーワード抽出機能を利用できるようになります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa816608",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import random\n",
    "import os\n",
    "from keybert import KeyBERT\n",
    "import matplotlib.pyplot as plt\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import numpy as np  # 数値計算のためのライブラリnumpyをインポートします。\n",
    "import pandas as pd  # データ操作および分析のためのライブラリpandasをインポートします。\n",
    "import random  # ランダム数生成のためのrandomモジュールをインポートします。\n",
    "import os  # オペレーティングシステムとの対話のためのosモジュールをインポートします。\n",
    "from keybert import KeyBERT  # KeyBERTライブラリからキーワード抽出のためのKeyBERTクラスをインポートします。\n",
    "import matplotlib.pyplot as plt  # グラフ作成のためのmatplotlibライブラリをインポートします。このモジュールを用いることでデータの可視化が可能になります。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:33:55.648904Z",
     "iopub.status.busy": "2024-07-28T14:33:55.648601Z",
     "iopub.status.idle": "2024-07-28T14:34:12.862008Z",
     "shell.execute_reply": "2024-07-28T14:34:12.860935Z",
     "shell.execute_reply.started": "2024-07-28T14:33:55.648877Z"
    },
    "papermill": {
     "duration": 5.536922,
     "end_time": "2023-05-15T04:09:41.654588",
     "exception": false,
     "start_time": "2023-05-15T04:09:36.117666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np  # 数値計算のためのライブラリnumpyをインポートします。\n",
    "import pandas as pd  # データ操作および分析のためのライブラリpandasをインポートします。\n",
    "import random  # ランダム数生成のためのrandomモジュールをインポートします。\n",
    "import os  # オペレーティングシステムとの対話のためのosモジュールをインポートします。\n",
    "from keybert import KeyBERT  # KeyBERTライブラリからキーワード抽出のためのKeyBERTクラスをインポートします。\n",
    "import matplotlib.pyplot as plt  # グラフ作成のためのmatplotlibライブラリをインポートします。このモジュールを用いることでデータの可視化が可能になります。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa86ea8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "model = KeyBERT('distilbert-base-nli-mean-tokens')  # 'distilbert-base-nli-mean-tokens'という事前学習済みモデルを用いてKeyBERTのインスタンスを作成します。このモデルは、自然言語処理タスクでよく使用されるDistilBERTを基にしており、キーワードやキーフレーズを抽出するのに役立ちます。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-28T14:34:12.863778Z",
     "iopub.status.busy": "2024-07-28T14:34:12.863198Z",
     "iopub.status.idle": "2024-07-28T14:34:28.004067Z",
     "shell.execute_reply": "2024-07-28T14:34:28.003196Z",
     "shell.execute_reply.started": "2024-07-28T14:34:12.86375Z"
    }
   },
   "outputs": [],
   "source": [
    "model = KeyBERT('distilbert-base-nli-mean-tokens')  # 'distilbert-base-nli-mean-tokens'という事前学習済みモデルを用いてKeyBERTのインスタンスを作成します。このモデルは、自然言語処理タスクでよく使用されるDistilBERTを基にしており、キーワードやキーフレーズを抽出するのに役立ちます。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cfb157",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')#, encoding='iso-8859-1')\n",
    "print(len(train))\n",
    "train['prompt_kw']='-'\n",
    "train['res_a_kw']='-'\n",
    "train['res_b_kw']='-'\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')#, encoding='iso-8859-1')\n",
    "test['prompt_kw']='-'\n",
    "test['res_a_kw']='-'\n",
    "test['res_b_kw']='-'\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます。\n",
    "print(len(train))  # トレーニングデータの行数を出力します。\n",
    "train['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "train['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "train['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。\n",
    "test['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "test['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "test['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:34:28.225539Z",
     "iopub.status.idle": "2024-07-28T14:34:28.226014Z",
     "shell.execute_reply": "2024-07-28T14:34:28.225756Z",
     "shell.execute_reply.started": "2024-07-28T14:34:28.225737Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます。\n",
    "print(len(train))  # トレーニングデータの行数を出力します。\n",
    "train['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "train['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "train['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。\n",
    "test['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "test['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。\n",
    "test['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cd41b6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "kw0 = model.extract_keywords(train['prompt'],top_n=5)\n",
    "kw1 = model.extract_keywords(train['response_a'],top_n=20)\n",
    "kw2 = model.extract_keywords(train['response_b'],top_n=20)\n",
    "\n",
    "tkw0 = model.extract_keywords(test['prompt'],top_n=5)\n",
    "tkw1 = model.extract_keywords(test['response_a'],top_n=20)\n",
    "tkw2 = model.extract_keywords(test['response_b'],top_n=20)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "kw0 = model.extract_keywords(train['prompt'], top_n=5)  # トレーニングデータの'prompt'列から上位5つのキーワードを抽出します。\n",
    "kw1 = model.extract_keywords(train['response_a'], top_n=20)  # トレーニングデータの'response_a'列から上位20のキーワードを抽出します。\n",
    "kw2 = model.extract_keywords(train['response_b'], top_n=20)  # トレーニングデータの'response_b'列から上位20のキーワードを抽出します。\n",
    "\n",
    "tkw0 = model.extract_keywords(test['prompt'], top_n=5)  # テストデータの'prompt'列から上位5つのキーワードを抽出します。\n",
    "tkw1 = model.extract_keywords(test['response_a'], top_n=20)  # テストデータの'response_a'列から上位20のキーワードを抽出します。\n",
    "tkw2 = model.extract_keywords(test['response_b'], top_n=20)  # テストデータの'response_b'列から上位20のキーワードを抽出します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:34:28.227646Z",
     "iopub.status.idle": "2024-07-28T14:34:28.228005Z",
     "shell.execute_reply": "2024-07-28T14:34:28.227856Z",
     "shell.execute_reply.started": "2024-07-28T14:34:28.227839Z"
    }
   },
   "outputs": [],
   "source": [
    "kw0 = model.extract_keywords(train['prompt'], top_n=5)  # トレーニングデータの'prompt'列から上位5つのキーワードを抽出します。\n",
    "kw1 = model.extract_keywords(train['response_a'], top_n=20)  # トレーニングデータの'response_a'列から上位20のキーワードを抽出します。\n",
    "kw2 = model.extract_keywords(train['response_b'], top_n=20)  # トレーニングデータの'response_b'列から上位20のキーワードを抽出します。\n",
    "\n",
    "tkw0 = model.extract_keywords(test['prompt'], top_n=5)  # テストデータの'prompt'列から上位5つのキーワードを抽出します。\n",
    "tkw1 = model.extract_keywords(test['response_a'], top_n=20)  # テストデータの'response_a'列から上位20のキーワードを抽出します。\n",
    "tkw2 = model.extract_keywords(test['response_b'], top_n=20)  # テストデータの'response_b'列から上位20のキーワードを抽出します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3fa13f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "for i,w in enumerate(kw0): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    train.loc[i,'prompt_kw']=' '.join(ws)\n",
    "    \n",
    "for i,w in enumerate(kw1): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    train.loc[i,'res_a_kw']=' '.join(ws)  \n",
    "    \n",
    "for i,w in enumerate(kw2): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    train.loc[i,'res_b_kw']=' '.join(ws)   \n",
    "\n",
    "train['res_a_kw']=train['prompt_kw']+' / '+train['res_a_kw']\n",
    "train['res_b_kw']=train['prompt_kw']+' / '+train['res_b_kw']\n",
    "train=train.iloc[:,6:]\n",
    "display(train)\n",
    "\n",
    "train.to_csv('train_key.csv',index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "for i, w in enumerate(kw0):  # kw0の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiに対して、キーワードのリストをループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、つまり無視するべきでなければ、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'prompt_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(kw1):  # kw1の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_a_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(kw2):  # kw2の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_b_kw'列に格納します。\n",
    "\n",
    "train['res_a_kw'] = train['prompt_kw'] + ' / ' + train['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。\n",
    "train['res_b_kw'] = train['prompt_kw'] + ' / ' + train['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。\n",
    "train = train.iloc[:, 6:]  # トレーニングデータフレームの最初の6列を除外します。\n",
    "display(train)  # トレーニングデータフレームを表示します。\n",
    "\n",
    "train.to_csv('train_key.csv', index=False)  # トレーニングデータをCSVファイルとして保存します。インデックスは保存しません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:34:28.229242Z",
     "iopub.status.idle": "2024-07-28T14:34:28.229689Z",
     "shell.execute_reply": "2024-07-28T14:34:28.229478Z",
     "shell.execute_reply.started": "2024-07-28T14:34:28.22946Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, w in enumerate(kw0):  # kw0の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiに対して、キーワードのリストをループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、つまり無視するべきでなければ、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'prompt_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(kw1):  # kw1の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_a_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(kw2):  # kw2の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    train.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_b_kw'列に格納します。\n",
    "\n",
    "train['res_a_kw'] = train['prompt_kw'] + ' / ' + train['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。\n",
    "train['res_b_kw'] = train['prompt_kw'] + ' / ' + train['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。\n",
    "train = train.iloc[:, 6:]  # トレーニングデータフレームの最初の6列を除外します。\n",
    "display(train)  # トレーニングデータフレームを表示します。\n",
    "\n",
    "train.to_csv('train_key.csv', index=False)  # トレーニングデータをCSVファイルとして保存します。インデックスは保存しません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87155115",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "for i,w in enumerate(tkw0): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    test.loc[i,'prompt_kw']=' '.join(ws)\n",
    "    \n",
    "for i,w in enumerate(tkw1): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    test.loc[i,'res_a_kw']=' '.join(ws)  \n",
    "    \n",
    "for i,w in enumerate(tkw2): \n",
    "    ws=[]\n",
    "    for wi in w:\n",
    "        if '_' not in wi[0]:\n",
    "            ws+=[wi[0]]\n",
    "    test.loc[i,'res_b_kw']=' '.join(ws)   \n",
    "\n",
    "test['res_a_kw']=test['prompt_kw']+' / '+test['res_a_kw']\n",
    "test['res_b_kw']=test['prompt_kw']+' / '+test['res_b_kw']\n",
    "test=test.iloc[:,4:]\n",
    "display(test)\n",
    "\n",
    "test.to_csv('test_key.csv',index=False)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "for i, w in enumerate(tkw0):  # tkw0の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiに対してループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'prompt_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(tkw1):  # tkw1の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_a_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(tkw2):  # tkw2の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_b_kw'列に格納します。\n",
    "\n",
    "test['res_a_kw'] = test['prompt_kw'] + ' / ' + test['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。\n",
    "test['res_b_kw'] = test['prompt_kw'] + ' / ' + test['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。\n",
    "test = test.iloc[:, 4:]  # テストデータフレームの最初の4列を除外します。\n",
    "display(test)  # テストデータフレームを表示します。\n",
    "\n",
    "test.to_csv('test_key.csv', index=False)  # テストデータをCSVファイルとして保存します。インデックスは保存しません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-07-28T14:34:28.231526Z",
     "iopub.status.idle": "2024-07-28T14:34:28.231856Z",
     "shell.execute_reply": "2024-07-28T14:34:28.231711Z",
     "shell.execute_reply.started": "2024-07-28T14:34:28.231698Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, w in enumerate(tkw0):  # tkw0の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiに対してループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'prompt_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(tkw1):  # tkw1の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_a_kw'列に格納します。\n",
    "\n",
    "for i, w in enumerate(tkw2):  # tkw2の各キーワードに対して、インデックスiとキーワードwを取得します。\n",
    "    ws = []  # 有効なキーワードを格納するリストを初期化します。\n",
    "    for wi in w:  # 各キーワードwiについてループします。\n",
    "        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、\n",
    "            ws += [wi[0]]  # 有効なキーワードリストに追加します。\n",
    "    test.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_b_kw'列に格納します。\n",
    "\n",
    "test['res_a_kw'] = test['prompt_kw'] + ' / ' + test['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。\n",
    "test['res_b_kw'] = test['prompt_kw'] + ' / ' + test['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。\n",
    "test = test.iloc[:, 4:]  # テストデータフレームの最初の4列を除外します。\n",
    "display(test)  # テストデータフレームを表示します。\n",
    "\n",
    "test.to_csv('test_key.csv', index=False)  # テストデータをCSVファイルとして保存します。インデックスは保存しません。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
