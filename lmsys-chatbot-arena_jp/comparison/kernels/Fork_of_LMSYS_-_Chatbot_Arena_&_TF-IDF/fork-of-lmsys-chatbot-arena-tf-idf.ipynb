{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924b53f9",
   "metadata": {},
   "source": [
    "# è¦ç´„ \n",
    "ã“ã®Jupyterãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«å¯¾ã™ã‚‹å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å¿œç­”ã®å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®æ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã™ã‚‹ã“ã¨ã‚’ç›®çš„ã¨ã—ã¦ã„ã¾ã™ã€‚ä¸»ã«ã€LightGBMã¨TF-IDFï¼ˆTerm Frequency-Inverse Document Frequencyï¼‰ã‚’ç”¨ã„ã‚‹ã‚¢ãƒ—ãƒ­ãƒ¼ãƒãŒæ¡ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "### å•é¡Œã«å–ã‚Šçµ„ã‚€å†…å®¹\n",
    "ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¯¾ã™ã‚‹äºŒã¤ã®LLMã®å¿œç­”ã®ä¸­ã§ã€ã©ã¡ã‚‰ãŒå¥½ã¾ã‚Œã‚‹ã‹ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®ã‚¿ã‚¹ã‚¯ãŒä¸­å¿ƒã§ã™ã€‚å…·ä½“çš„ã«ã¯ã€æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ãŒå«ã¾ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰**: ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§ã€åŸºæœ¬çš„ãªçµ±è¨ˆæƒ…å ±ã‚„åˆ†å¸ƒã‚’ç¢ºèªã—ã¾ã™ã€‚\n",
    "2. **TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–**: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚ã«TF-IDFã‚’ç”¨ã„ã¾ã™ã€‚ã“ã®æ‰‹æ³•ã¯ã€é‡è¦ãªå˜èªã‚’ç‰¹å®šã—ã€æ–‡æ›¸ã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã«ä½¿ã‚ã‚Œã¾ã™ã€‚\n",
    "3. **ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†**: å„å¿œç­”ã®æƒ…å ±ã‚’å«ã‚€ç‰¹å¾´é‡ã‚’è¿½åŠ ã—ã€é¡ä¼¼æ€§ã‚„è·é›¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "4. **ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´**: LightGBMã‚’ç”¨ã„ã¦ãƒ‡ãƒ¼ã‚¿ã‚’è¨“ç·´ã—ã€äº¤å·®æ¤œè¨¼ã‚’è¡Œã„ãªãŒã‚‰æ€§èƒ½ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "5. **ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–**: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã„ã€çµæœã‚’å…ƒã«æ··åŒè¡Œåˆ—ã‚’ä½œæˆã—è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "6. **æå‡ºç”¨ãƒ•ã‚¡ã‚¤ãƒ«ã®ç”Ÿæˆ**: äºˆæ¸¬çµæœã‚’é©åˆ‡ãªãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã—ã¾ã™ã€‚\n",
    "\n",
    "### ä½¿ç”¨ã—ã¦ã„ã‚‹ä¸»ãªæ‰‹æ³•ã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "- **LightGBM**: å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´ã—ã¾ã™ã€‚ã“ã‚Œã¯å¤§å®¹é‡ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦åŠ¹ç‡çš„ã‹ã¤åŠ¹æœçš„ã«å­¦ç¿’ã‚’è¡Œãˆã‚‹ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã™ã€‚\n",
    "- **TF-IDF**: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€å˜èªã®é‡è¦æ€§ã‚’æ¸¬å®šã™ã‚‹ãŸã‚ã®æ‰‹æ³•ã§ã™ã€‚\n",
    "- **Label Encoding**: è¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’å˜ä¸€ã®ãƒ©ãƒ™ãƒ«ã«çµ±åˆã™ã‚‹ãŸã‚ã®ã‚«ã‚¹ã‚¿ãƒ é–¢æ•°ãŒå®šç¾©ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "- **æ··åŒè¡Œåˆ—**: ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "å…¨ä½“ã¨ã—ã¦ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã®å¿œç­”ã®å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã«é«˜åº¦ãªæ©Ÿæ¢°å­¦ç¿’ã‚’å®Ÿè£…ã—ã¦ãŠã‚Šã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ãƒ‹ãƒ¼ã‚ºã«å¯¾ã—ã¦ã‚ˆã‚Šå„ªã‚ŒãŸå¿œç­”ã‚’ç”Ÿæˆã™ã‚‹ãŸã‚ã®å¼·åŠ›ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88864ad",
   "metadata": {},
   "source": [
    "# ç”¨èªæ¦‚èª¬ \n",
    "ä»¥ä¸‹ã«ã€æ©Ÿæ¢°å­¦ç¿’ãƒ»æ·±å±¤å­¦ç¿’ã®åˆå¿ƒè€…ãŒã¤ã¾ãšããã†ãªå°‚é–€ç”¨èªã®ç°¡å˜ãªè§£èª¬ã‚’æŒ™ã’ã¾ã™ã€‚ç‰¹ã«ã€ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ç‰¹æœ‰ã®ãƒ‰ãƒ¡ã‚¤ãƒ³çŸ¥è­˜ã‚„ã€å®Ÿå‹™ã‚’çµŒé¨“ã—ã¦ã„ãªã„ã¨é¦´æŸ“ã¿ã®ãªã„ã‚‚ã®ã«ç„¦ç‚¹ã‚’å½“ã¦ã¦ã„ã¾ã™ã€‚\n",
    "\n",
    "1. **LightGBM**:\n",
    "   - æ¦‚è¦: Microsoftã«ã‚ˆã£ã¦é–‹ç™ºã•ã‚ŒãŸå‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®ä¸€ç¨®ã§ã€å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§é«˜é€Ÿã«å‹•ä½œã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒè‰¯ã„ã®ãŒç‰¹å¾´ã§ã™ã€‚\n",
    "   - ç‰¹å¾´: æ±ºå®šæœ¨ã®è‘‰ã®åˆ†å‰²ã«ã‚ˆã‚‹å­¦ç¿’ã‚’è¡Œã„ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ™‚é–“ã‚’å¤§å¹…ã«çŸ­ç¸®ã™ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - æ¦‚è¦: ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ•°å€¤åŒ–ã™ã‚‹æ‰‹æ³•ã®ä¸€ã¤ã§ã€ç‰¹å®šã®å˜èªãŒæ–‡æ›¸å†…ã§ã©ã‚Œãã‚‰ã„é‡è¦ã§ã‚ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: TFã¯å˜èªã®å‡ºç¾é »åº¦ã‚’ç¤ºã—ã€IDFã¯å˜èªã®ä¸€èˆ¬çš„ãªé‡è¦æ€§ã‚’ç¤ºã™ãŸã‚ã€ä¸¡è€…ã‚’æ›ã‘ç®—ã™ã‚‹ã“ã¨ã§ã€æ–‡æ›¸ã®å†…å®¹ã«å¿œã˜ãŸé‡è¦ãªå˜èªã‚’ç‰¹å®šã§ãã¾ã™ã€‚\n",
    "\n",
    "3. **n-grams**:\n",
    "   - æ¦‚è¦: ãƒ†ã‚­ã‚¹ãƒˆã«ãŠã‘ã‚‹é€£ç¶šã—ãŸnå€‹ã®å˜èªï¼ˆã¾ãŸã¯ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®çµ„ã¿åˆã‚ã›ã‚’æŒ‡ã—ã¾ã™ã€‚1ã¤ã®å˜èªã‹ã‚‰ãªã‚‹1-gramï¼ˆunigramï¼‰ã€2ã¤ã®å˜èªã‹ã‚‰ãªã‚‹2-gramï¼ˆbigramï¼‰ãªã©ãŒã‚ã‚Šã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: æ–‡è„ˆã‚’æ‰ãˆã‚‹åŠ›ãŒå‘ä¸Šã—ã€å˜èªå˜ä½ã ã‘ã§ã¯æ‰ãˆãã‚Œãªã„æ„å‘³ã‚’æŠ½å‡ºã™ã‚‹ã®ã«å½¹ç«‹ã¤ã€‚\n",
    "\n",
    "4. **StratifiedKFold**:\n",
    "   - æ¦‚è¦: Kåˆ†å‰²äº¤å·®æ¤œè¨¼ã®ä¸€ç¨®ã§ã€å„ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã«ãŠã„ã¦ã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒã‚’ç¶­æŒã—ã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: ã‚¯ãƒ©ã‚¹ä¸å‡è¡¡ãŒã‚ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ãŠã„ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®è©•ä¾¡ã‚’ã‚ˆã‚Šæ­£ç¢ºã«è¡Œã†ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "5. **ãƒˆãƒ©ãƒ³ã‚±ã‚¤ãƒ†ãƒƒãƒ‰ SVD (Truncated Singular Value Decomposition)**:\n",
    "   - æ¦‚è¦: è¡Œåˆ—ã®æ¬¡å…ƒå‰Šæ¸›æŠ€è¡“ã§ã€å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚’å°ã•ãã—ã€é‡è¦ãªæƒ…å ±ã‚’ä¿æŒã—ã¤ã¤ä½™è¨ˆãªæƒ…å ±ã‚’å–ã‚Šé™¤ãã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: TF-IDFã§å¾—ã‚‰ã‚ŒãŸé«˜æ¬¡å…ƒã®ç‰¹å¾´ç©ºé–“ã‚’ã‚ˆã‚Šå°ã•ãªç©ºé–“ã«åœ§ç¸®ã™ã‚‹éš›ã«ç”¨ã„ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "6. **æ··åŒè¡Œåˆ— (Confusion Matrix)**:\n",
    "   - æ¦‚è¦: åˆ†é¡ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®è¡¨ã§ã€å®Ÿéš›ã®ãƒ©ãƒ™ãƒ«ã¨äºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’æ¯”è¼ƒã™ã‚‹ã‚‚ã®ã§ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: æ­£ã—ã„äºˆæ¸¬ã¨èª¤ã£ãŸäºˆæ¸¬ã®æ•°ã‚’ç¤ºã—ã€å„ã‚¯ãƒ©ã‚¹ã®ç²¾åº¦ã‚„å†ç¾ç‡ã‚’è¨ˆç®—ã™ã‚‹éš›ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "7. **ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ (Cosine Similarity)**:\n",
    "   - æ¦‚è¦: 2ã¤ã®ãƒ™ã‚¯ãƒˆãƒ«é–“ã®è§’åº¦ã‚’ä½¿ã£ã¦ã€ãã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹å°ºåº¦ã§ã™ã€‚1ã«è¿‘ã„ã»ã©ã€é¡ä¼¼ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼æ€§ã‚’æ¸¬å®šã™ã‚‹éš›ã«ã‚ˆãä½¿ç”¨ã•ã‚Œã€TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã®æ¯”è¼ƒã«åˆ©ç”¨ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "8. **ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ (Euclidean Distance)**:\n",
    "   - æ¦‚è¦: äºŒç‚¹é–“ã®ç›´ç·šè·é›¢ã‚’è¨ˆæ¸¬ã™ã‚‹æ–¹æ³•ã§ã€å¹³é¢ä¸Šã®ç‚¹åŒå£«ã®æœ€çŸ­è·é›¢ã‚’ç¤ºã—ã¾ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: ç‰¹å¾´ç©ºé–“å†…ã§ã®ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆé–“ã®è·é›¢ã‚’æ¸¬å®šã—ã€åˆ†é¡ã‚„ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "9. **ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ ã‚«ãƒ¼ãƒãƒ« (Laplacian Kernel)**:\n",
    "   - æ¦‚è¦: ãƒ‡ãƒ¼ã‚¿ãƒã‚¤ãƒ³ãƒˆé–“ã®éš£æ¥é–¢ä¿‚ã‚’ãƒ¢ãƒ‡ãƒ«åŒ–ã—ã€ç‰¹ã«ã‚°ãƒ©ãƒ•ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ãŸè·é›¢ã‚’è©•ä¾¡ã™ã‚‹ãŸã‚ã®ã‚«ãƒ¼ãƒãƒ«é–¢æ•°ã§ã™ã€‚\n",
    "   - ãƒ‰ãƒ¡ã‚¤ãƒ³ç‰¹æœ‰ã®ãƒã‚¤ãƒ³ãƒˆ: ç‰¹å¾´é–“ã®éç·šå½¢é–¢ä¿‚ã‚’æ‰ãˆãŸã‚Šã€ãƒ‡ãƒ¼ã‚¿ã®å±€æ‰€çš„ãªæ§‹é€ ã‚’è€ƒæ…®ã—ãŸè·é›¢è¨ˆç®—ã«å½¹ç«‹ã¡ã¾ã™ã€‚\n",
    "\n",
    "ã“ã‚Œã‚‰ã®ç”¨èªã®ç†è§£ãŒé€²ã‚€ã“ã¨ã§ã€ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®å†…å®¹ã‚„æ©Ÿæ¢°å­¦ç¿’ã®å®Ÿå‹™ã«ã‚ˆã‚Šè‡ªä¿¡ã‚’æŒã£ã¦å–ã‚Šçµ„ã‚ã‚‹ã‚ˆã†ã«ãªã‚‹ã§ã—ã‚‡ã†ã€‚\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c038d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Introduction ğŸ“œ\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ã‚¤ãƒ³ãƒˆãƒ­ãƒ€ã‚¯ã‚·ãƒ§ãƒ³ ğŸ“œ\n",
    "\n",
    "âœ”ï¸ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã®ç›®çš„ã¯ä½•ã§ã™ã‹ï¼Ÿ\n",
    "\n",
    "ç›®æ¨™ã¯ã€LightGBMã¨TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®LLMå¿œç­”ã¸ã®å¥½ã¿ã‚’äºˆæ¸¬ã™ã‚‹ãŸã‚ã®å …ç‰¢ã§åŠ¹ç‡çš„ãªã‚½ãƒªãƒ¥ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ã§ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ”ï¸ ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ä½•ãŒæ‰±ã‚ã‚Œã¦ã„ã¾ã™ã‹ï¼Ÿ\n",
    "\n",
    "- `ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ã¨æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰`\n",
    "\n",
    "- `TF-IDFã®ç†è«–`\n",
    "\n",
    "- `ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†`\n",
    "\n",
    "- `ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´`\n",
    "\n",
    "- `ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–`\n",
    "     \n",
    "# ã‚¤ãƒ³ãƒãƒ¼ãƒˆ ğŸ“¦\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df0fec",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "âœ”ï¸ What is the objective of this notebook?\n",
    "\n",
    "The goal is to create a robust and efficient solution to predict users' preference of LLM responses using LightGBM and TF-IDF vectorization.\n",
    "\n",
    "---\n",
    "\n",
    "âœ”ï¸ What does this notebook cover?\n",
    "\n",
    "- `Data Loading & EDA`\n",
    "\n",
    "- `Theory behind TF-IDF`\n",
    "\n",
    "- `Data Preprocessing`\n",
    "\n",
    "- `Model Training`\n",
    "       \n",
    "- `Model Inference`\n",
    "     \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# è­¦å‘Šãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’å‡¦ç†ã™ã‚‹\n",
    "import warnings\n",
    "# è­¦å‘Šã‚’ç„¡è¦–ã™ã‚‹è¨­å®šã‚’è¡Œã„ã¾ã™\n",
    "warnings.filterwarnings('ignore')  # ã“ã‚Œã«ã‚ˆã‚Šã€å®Ÿè¡Œæ™‚ã«è¡¨ç¤ºã•ã‚Œã‚‹è­¦å‘ŠãŒè¡¨ç¤ºã•ã‚Œãªããªã‚Šã¾ã™\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329171a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Imports ğŸ“¦\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†\n",
    "import numpy as np  # NumPyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚æ•°å€¤è¨ˆç®—ã‚„é…åˆ—æ“ä½œã«ä½¿ã„ã¾ã™ã€‚\n",
    "import pandas as pd  # pandasãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ä½œæˆã‚„æ“ä½œã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "from pathlib import Path  # ãƒ‘ã‚¹æ“ä½œç”¨ã®Pathãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿ã®è¦–è¦šåŒ–\n",
    "import plotly.graph_objects as go  # Plotlyãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªã‚°ãƒ©ãƒ•ä½œæˆã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "from sklearn.metrics import confusion_matrix  # æ··åŒè¡Œåˆ—ã®è¨ˆç®—ç”¨ã®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«ã®é–‹ç™º\n",
    "import lightgbm as lgb  # LightGBMãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã€‚å‹¾é…ãƒ–ãƒ¼ã‚¹ãƒ†ã‚£ãƒ³ã‚°ã‚’ç”¨ã„ãŸãƒ¢ãƒ‡ãƒ«ä½œæˆã«ä½¿ç”¨ã—ã¾ã™ã€‚\n",
    "from sklearn.model_selection import StratifiedKFold  # å±¤åŒ–Kãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰äº¤å·®æ¤œè¨¼ç”¨ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "# TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–\n",
    "from sklearn.decomposition import TruncatedSVD  # æ¬¡å…ƒå‰Šæ¸›ç”¨ã®ãƒˆãƒ©ãƒ³ã‚±ã‚¤ãƒ†ãƒƒãƒ‰SVDã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ãŸã‚ã®ã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "# TF-IDFãƒ™ã‚¯ãƒˆãƒ«ã®é¡ä¼¼æ€§/è·é›¢ç‰¹å¾´\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã€ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã€ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«ã®è¨ˆç®—ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee610a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Handle warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# è¨­å®š âš™ï¸\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dc609",
   "metadata": {},
   "source": [
    "# è¨­å®š âš™ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe147a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Data visualization\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Model development\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Similarity/distance features for TF-IDF vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒ‘ã‚¹\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹\n",
    "    \n",
    "    # æ··åŒè¡Œåˆ—ã®ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "    colorscale = \"peach\"  # æ··åŒè¡Œåˆ—ã‚’è¡¨ç¤ºã™ã‚‹éš›ã®ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®š\n",
    "    \n",
    "    # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    components = 32  # æ¬¡å…ƒå‰Šæ¸›ã§ä¿æŒã™ã‚‹æˆåˆ†ã®æ•°\n",
    "    ngrams = (1, 7)  # ä½¿ç”¨ã™ã‚‹n-gramã®ç¯„å›²\n",
    "    max_freq = 0.95  # 95%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–\n",
    "    min_freq = 10    # 10æ–‡æ›¸æœªæº€ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¼•æ•°\n",
    "    num_classes = 3   # åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹ã®æ•°\n",
    "    early_stop = 50   # æ—©æœŸçµ‚äº†ã®ãŸã‚ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    log_steps = 100   # ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    \n",
    "    # LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",  # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’æŒ‡å®š\n",
    "        \"colsample_bytree\": 0.8,     # æœ¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡\n",
    "        \"colsample_bynode\": 0.8,      # ãƒãƒ¼ãƒ‰ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡\n",
    "        \"metric\": \"multiclass\",      # è©•ä¾¡æŒ‡æ¨™ã‚’ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ã«è¨­å®š\n",
    "        \"learning_rate\": 0.02,        # å­¦ç¿’ç‡\n",
    "        \"extra_trees\": True,          # è¿½åŠ ã®æœ¨ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"num_rounds\": 3000,           # å­¦ç¿’ã®ãƒ©ã‚¦ãƒ³ãƒ‰æ•°\n",
    "        \"reg_lambda\": 1.3,            # L2æ­£å‰‡åŒ–\n",
    "        \"num_classes\": 3,             # åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹æ•°\n",
    "        \"num_leaves\": 64,             # è‘‰ã®æ•°\n",
    "        \"reg_alpha\": 0.1,             # L1æ­£å‰‡åŒ–\n",
    "        \"device\": \"cpu\",              # ãƒ‡ãƒã‚¤ã‚¹ã¯CPUã‚’æŒ‡å®š\n",
    "        \"max_depth\": 6,               # æœ¨ã®æœ€å¤§æ·±ã•\n",
    "        \"max_bin\": 128,               # æœ€å¤§ãƒ“ãƒ³ã®æ•°\n",
    "        \"verbose\": -1,                # å‡ºåŠ›ã®è©³ç´°åº¦è¨­å®š\n",
    "        \"seed\": 42                    # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å€¤\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:33:16.95883Z",
     "iopub.status.busy": "2024-06-17T17:33:16.958249Z",
     "iopub.status.idle": "2024-06-17T17:33:16.970032Z",
     "shell.execute_reply": "2024-06-17T17:33:16.968747Z",
     "shell.execute_reply.started": "2024-06-17T17:33:16.95869Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # ã‚³ãƒ³ãƒšãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã¸ã®ãƒ‘ã‚¹\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ãƒ‘ã‚¹\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‘ã‚¹\n",
    "    \n",
    "    # æ··åŒè¡Œåˆ—ã®ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«\n",
    "    colorscale = \"peach\"  # æ··åŒè¡Œåˆ—ã‚’è¡¨ç¤ºã™ã‚‹éš›ã®ã‚«ãƒ©ãƒ¼ã‚¹ã‚±ãƒ¼ãƒ«è¨­å®š\n",
    "    \n",
    "    # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    components = 32  # æ¬¡å…ƒå‰Šæ¸›ã§ä¿æŒã™ã‚‹æˆåˆ†ã®æ•°\n",
    "    ngrams = (1, 7)  # ä½¿ç”¨ã™ã‚‹n-gramã®ç¯„å›²\n",
    "    max_freq = 0.95  # 95%ä»¥ä¸Šã®æ–‡æ›¸ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–\n",
    "    min_freq = 10    # 10æ–‡æ›¸æœªæº€ã«å‡ºç¾ã™ã‚‹å˜èªã¯é™¤å¤–\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®å¼•æ•°\n",
    "    num_classes = 3   # åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹ã®æ•°\n",
    "    early_stop = 50   # æ—©æœŸçµ‚äº†ã®ãŸã‚ã®ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³æ•°\n",
    "    log_steps = 100   # ãƒ­ã‚°ã‚’è¨˜éŒ²ã™ã‚‹ã‚¹ãƒ†ãƒƒãƒ—æ•°\n",
    "    \n",
    "    # LightGBMã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",  # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ã‚’æŒ‡å®š\n",
    "        \"colsample_bytree\": 0.8,     # æœ¨ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡\n",
    "        \"colsample_bynode\": 0.8,      # ãƒãƒ¼ãƒ‰ã§ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ¯”ç‡\n",
    "        \"metric\": \"multiclass\",      # è©•ä¾¡æŒ‡æ¨™ã‚’ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ã«è¨­å®š\n",
    "        \"learning_rate\": 0.02,        # å­¦ç¿’ç‡\n",
    "        \"extra_trees\": True,          # è¿½åŠ ã®æœ¨ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹\n",
    "        \"num_rounds\": 3000,           # å­¦ç¿’ã®ãƒ©ã‚¦ãƒ³ãƒ‰æ•°\n",
    "        \"reg_lambda\": 1.3,            # L2æ­£å‰‡åŒ–\n",
    "        \"num_classes\": 3,             # åˆ†é¡ã™ã‚‹ã‚¯ãƒ©ã‚¹æ•°\n",
    "        \"num_leaves\": 64,             # è‘‰ã®æ•°\n",
    "        \"reg_alpha\": 0.1,             # L1æ­£å‰‡åŒ–\n",
    "        \"device\": \"cpu\",              # ãƒ‡ãƒã‚¤ã‚¹ã¯CPUã‚’æŒ‡å®š\n",
    "        \"max_depth\": 6,               # æœ¨ã®æœ€å¤§æ·±ã•\n",
    "        \"max_bin\": 128,               # æœ€å¤§ãƒ“ãƒ³ã®æ•°\n",
    "        \"verbose\": -1,                # å‡ºåŠ›ã®è©³ç´°åº¦è¨­å®š\n",
    "        \"seed\": 42                    # å†ç¾æ€§ã®ãŸã‚ã®ã‚·ãƒ¼ãƒ‰å€¤\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2973a7d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Configuration âš™ï¸\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æï¼ˆEDAï¼‰ ğŸ—ƒï¸\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e2b98",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # Paths to competition data\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")\n",
    "    \n",
    "    # Colorscale for confusion matrix\n",
    "    colorscale = \"peach\"\n",
    "    \n",
    "    # TF-IDF Vectorization parameters\n",
    "    components = 32\n",
    "    ngrams = (1, 7) \n",
    "    max_freq = 0.95 # Words that occur in more than 95% of the documents are omitted\n",
    "    min_freq = 10   # Words that occur in less than 10 documents are omitted\n",
    "    \n",
    "    # Training arguments\n",
    "    num_classes = 3\n",
    "    early_stop = 50\n",
    "    log_steps = 100\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"colsample_bynode\": 0.8,\n",
    "        \"metric\": \"multiclass\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"extra_trees\": True,\n",
    "        \"num_rounds\": 3000,\n",
    "        \"reg_lambda\": 1.3,\n",
    "        \"num_classes\": 3,\n",
    "        \"num_leaves\": 64,\n",
    "        \"reg_alpha\": 0.1,\n",
    "        \"device\": \"cpu\",\n",
    "        \"max_depth\": 6,\n",
    "        \"max_bin\": 128,\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # æŒ‡å®šã—ãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã¿\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å½¢çŠ¶ã¨æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å½¢çŠ¶ã¯: {df.shape}ã§ã™\")\n",
    "        display(df.head(3))  # ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "        \n",
    "        return df  # èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # å„å‹è€…ã®åˆ—ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),  # ãƒ¢ãƒ‡ãƒ«Aã®å‹ã¡æ•°\n",
    "            'winner_model_b': data['winner_model_b'].sum(),  # ãƒ¢ãƒ‡ãƒ«Bã®å‹ã¡æ•°\n",
    "            'winner_tie': data['winner_tie'].sum()           # å¼•ãåˆ†ã‘ã®æ•°\n",
    "        }\n",
    "\n",
    "        # ã‚«ãƒ©ãƒ¼ã‚’å®šç¾©\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # ã‚¯ãƒªãƒ¼ãƒ ã€ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼ã€ãƒŸãƒ³ãƒˆ\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']  # å„è‰²ã®è­˜åˆ¥å­\n",
    "        \n",
    "        # å††ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # èƒŒæ™¯ã‚’é€æ˜ã«ã—ã€å††ã‚°ãƒ©ãƒ•ã‚’å·¦å¯„ã›ã«ã™ã‚‹ãŸã‚ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # å‡¡ä¾‹ã‚’éè¡¨ç¤ºã«ã™ã‚‹\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()\n",
    "\n",
    "        # ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤º\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['ã‚¯ãƒ©ã‚¹', 'ã‚«ã‚¦ãƒ³ãƒˆ'])\n",
    "        counts_df['è­˜åˆ¥å­'] = identifiers\n",
    "        display(counts_df)  # ã‚«ã‚¦ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # å„å¿œç­”ã®å˜èªæ•°ã‚’è¨ˆç®—\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))  # ãƒ¢ãƒ‡ãƒ«Aã®å˜èªæ•°\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))  # ãƒ¢ãƒ‡ãƒ«Bã®å˜èªæ•°\n",
    "        \n",
    "        # å„å‹è€…ã‚¯ãƒ©ã‚¹ã®å¹³å‡å˜èªæ•°ã‚’è¨ˆç®—\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # ãƒ¢ãƒ‡ãƒ«Aã®å¹³å‡å˜èªæ•°\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # ãƒ¢ãƒ‡ãƒ«Bã®å¹³å‡å˜èªæ•°\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # å¼•ãåˆ†ã‘ã®å¹³å‡å˜èªæ•°\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # ã‚«ã‚¹ã‚¿ãƒ ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        hover_texts = [f\"å˜èªæ•°: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # å‹è€…ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’xè»¸ã«\n",
    "            y=list(word_counts.values()),  # å˜èªæ•°ã‚’yè»¸ã«\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,  # ãƒ›ãƒãƒ¼æ™‚ã®ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # æ£’ã‚’ç¸¦ã«ã™ã‚‹\n",
    "        )])\n",
    "        \n",
    "        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(\n",
    "            title='å‹è€…ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹å¹³å‡å¿œç­”å˜èªæ•°',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='å¹³å‡å¿œç­”å˜èªæ•°',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’éè¡¨ç¤º\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:20.288524Z",
     "iopub.status.busy": "2024-06-17T17:38:20.288118Z",
     "iopub.status.idle": "2024-06-17T17:38:20.308922Z",
     "shell.execute_reply": "2024-06-17T17:38:20.307347Z",
     "shell.execute_reply.started": "2024-06-17T17:38:20.288494Z"
    }
   },
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # æŒ‡å®šã—ãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’èª­ã¿è¾¼ã¿\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å½¢çŠ¶ã¨æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "        print(f\"ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å½¢çŠ¶ã¯: {df.shape}ã§ã™\")\n",
    "        display(df.head(3))  # ãƒ‡ãƒ¼ã‚¿ã®æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "        \n",
    "        return df  # èª­ã¿è¾¼ã‚“ã ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # å„å‹è€…ã®åˆ—ã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),  # ãƒ¢ãƒ‡ãƒ«Aã®å‹ã¡æ•°\n",
    "            'winner_model_b': data['winner_model_b'].sum(),  # ãƒ¢ãƒ‡ãƒ«Bã®å‹ã¡æ•°\n",
    "            'winner_tie': data['winner_tie'].sum()           # å¼•ãåˆ†ã‘ã®æ•°\n",
    "        }\n",
    "\n",
    "        # ã‚«ãƒ©ãƒ¼ã‚’å®šç¾©\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # ã‚¯ãƒªãƒ¼ãƒ ã€ãƒ©ã‚¤ãƒˆãƒ–ãƒ«ãƒ¼ã€ãƒŸãƒ³ãƒˆ\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']  # å„è‰²ã®è­˜åˆ¥å­\n",
    "        \n",
    "        # å††ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # èƒŒæ™¯ã‚’é€æ˜ã«ã—ã€å††ã‚°ãƒ©ãƒ•ã‚’å·¦å¯„ã›ã«ã™ã‚‹ãŸã‚ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # å‡¡ä¾‹ã‚’éè¡¨ç¤ºã«ã™ã‚‹\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()\n",
    "\n",
    "        # ã‚«ã‚¦ãƒ³ãƒˆã‚’ãƒ†ãƒ¼ãƒ–ãƒ«ã¨ã—ã¦è¡¨ç¤º\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['ã‚¯ãƒ©ã‚¹', 'ã‚«ã‚¦ãƒ³ãƒˆ'])\n",
    "        counts_df['è­˜åˆ¥å­'] = identifiers\n",
    "        display(counts_df)  # ã‚«ã‚¦ãƒ³ãƒˆãƒ†ãƒ¼ãƒ–ãƒ«ã‚’è¡¨ç¤º\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’å¤‰æ›´ã—ãªã„ãŸã‚ã«ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®ã‚³ãƒ”ãƒ¼ã‚’ä½œæˆ\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # å„å¿œç­”ã®å˜èªæ•°ã‚’è¨ˆç®—\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))  # ãƒ¢ãƒ‡ãƒ«Aã®å˜èªæ•°\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))  # ãƒ¢ãƒ‡ãƒ«Bã®å˜èªæ•°\n",
    "        \n",
    "        # å„å‹è€…ã‚¯ãƒ©ã‚¹ã®å¹³å‡å˜èªæ•°ã‚’è¨ˆç®—\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # ãƒ¢ãƒ‡ãƒ«Aã®å¹³å‡å˜èªæ•°\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # ãƒ¢ãƒ‡ãƒ«Bã®å¹³å‡å˜èªæ•°\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # å¼•ãåˆ†ã‘ã®å¹³å‡å˜èªæ•°\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # ã‚«ã‚¹ã‚¿ãƒ ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ\n",
    "        hover_texts = [f\"å˜èªæ•°: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # æ£’ã‚°ãƒ©ãƒ•ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # å‹è€…ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«ã‚’xè»¸ã«\n",
    "            y=list(word_counts.values()),  # å˜èªæ•°ã‚’yè»¸ã«\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,  # ãƒ›ãƒãƒ¼æ™‚ã®ãƒ†ã‚­ã‚¹ãƒˆ\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # æ£’ã‚’ç¸¦ã«ã™ã‚‹\n",
    "        )])\n",
    "        \n",
    "        # ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(\n",
    "            title='å‹è€…ã‚¯ãƒ©ã‚¹ã«ã‚ˆã‚‹å¹³å‡å¿œç­”å˜èªæ•°',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='å¹³å‡å¿œç­”å˜èªæ•°',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # xè»¸ã®ãƒ©ãƒ™ãƒ«ã‚’éè¡¨ç¤º\n",
    "        )\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11760f9e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Exploratory Data Analysis (EDA) ğŸ—ƒï¸\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "eda = EDA()  # EDAã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€æ¢ç´¢çš„ãƒ‡ãƒ¼ã‚¿åˆ†æã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de82d0f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # Read dataframe from path\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # Display the shape of the dataframe and the first 3 rows\n",
    "        print(f\"The shape of the dataframe is: {df.shape}\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # Calculate the counts for each winner column\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),\n",
    "            'winner_model_b': data['winner_model_b'].sum(),\n",
    "            'winner_tie': data['winner_tie'].sum()\n",
    "        }\n",
    "\n",
    "        # Define the colors\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # creme, light blue, mint\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']\n",
    "        \n",
    "        # Create the pie chart\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # Update layout for a transparent background and move the pie to the left\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # Hide the legend\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "        # Display the counts as a table\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['Class', 'Count'])\n",
    "        counts_df['Identifier'] = identifiers\n",
    "        display(counts_df)\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # Create a copy of the dataframe to avoid modifying the original data\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Calculate the number of words in each response\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        # Calculate the average word count for each winner class\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Create custom hover text\n",
    "        hover_texts = [f\"Word Count: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # Create the bar chart\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # Winner class labels on x-axis\n",
    "            y=list(word_counts.values()),\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # Ensure bars are vertical\n",
    "        )])\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Average Response Word Count by Winner Class',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='Average Response Word Count',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # Hide x-axis labels\n",
    "        )\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "train_data = eda.read_data(CFG.train_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:25.551342Z",
     "iopub.status.busy": "2024-06-17T17:38:25.550913Z",
     "iopub.status.idle": "2024-06-17T17:38:30.501504Z",
     "shell.execute_reply": "2024-06-17T17:38:30.500243Z",
     "shell.execute_reply.started": "2024-06-17T17:38:25.551312Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = eda.read_data(CFG.train_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d4aa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "eda = EDA()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "test_data = eda.read_data(CFG.test_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:36.653436Z",
     "iopub.status.busy": "2024-06-17T17:38:36.652917Z",
     "iopub.status.idle": "2024-06-17T17:38:36.672035Z",
     "shell.execute_reply": "2024-06-17T17:38:36.670511Z",
     "shell.execute_reply.started": "2024-06-17T17:38:36.653398Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = eda.read_data(CFG.test_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53cb00",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train_data = eda.read_data(CFG.train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "subm_data = eda.read_data(CFG.subm_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰æå‡ºãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:43.898851Z",
     "iopub.status.busy": "2024-06-17T17:38:43.898369Z",
     "iopub.status.idle": "2024-06-17T17:38:43.922869Z",
     "shell.execute_reply": "2024-06-17T17:38:43.921453Z",
     "shell.execute_reply.started": "2024-06-17T17:38:43.898813Z"
    }
   },
   "outputs": [],
   "source": [
    "subm_data = eda.read_data(CFG.subm_data)  # è¨­å®šã•ã‚ŒãŸãƒ‘ã‚¹ã‹ã‚‰æå‡ºãƒ‡ãƒ¼ã‚¿ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å–å¾—ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2eb78",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test_data = eda.read_data(CFG.test_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "print(\"ã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒï¼ˆå‹è€…ï¼‰:\")  # å‹è€…ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "eda.pie_chart(train_data)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—è¡¨ç¤ºã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:48.962237Z",
     "iopub.status.busy": "2024-06-17T17:38:48.961789Z",
     "iopub.status.idle": "2024-06-17T17:38:49.482496Z",
     "shell.execute_reply": "2024-06-17T17:38:49.481275Z",
     "shell.execute_reply.started": "2024-06-17T17:38:48.962204Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"ã‚¯ãƒ©ã‚¹ã®åˆ†å¸ƒï¼ˆå‹è€…ï¼‰:\")  # å‹è€…ã®ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’è¡¨ç¤ºã™ã‚‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸\n",
    "eda.pie_chart(train_data)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ã£ã¦ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã®å††ã‚°ãƒ©ãƒ•ã‚’ä½œæˆã—è¡¨ç¤ºã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7f2ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "subm_data = eda.read_data(CFG.subm_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# å‹è€…ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡å¿œç­”å˜èªæ•°ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹\n",
    "eda.response_length(train_data)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€å„å‹è€…ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã®å¹³å‡å˜èªæ•°ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:59.135398Z",
     "iopub.status.busy": "2024-06-17T17:38:59.134924Z",
     "iopub.status.idle": "2024-06-17T17:39:01.210285Z",
     "shell.execute_reply": "2024-06-17T17:39:01.208513Z",
     "shell.execute_reply.started": "2024-06-17T17:38:59.13536Z"
    }
   },
   "outputs": [],
   "source": [
    "# å‹è€…ãƒ¢ãƒ‡ãƒ«ã”ã¨ã®å¹³å‡å¿œç­”å˜èªæ•°ã‚’ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹\n",
    "eda.response_length(train_data)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ã€å„å‹è€…ãƒ¢ãƒ‡ãƒ«ã®å¿œç­”ã®å¹³å‡å˜èªæ•°ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183cdb2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(\"Distribution of classes (winners):\")\n",
    "eda.pie_chart(train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ç†è«– ğŸ“’\n",
    "\n",
    "âœ”ï¸ **å˜èªé »åº¦ - é€†æ–‡æ›¸é »åº¦**ã€ã¾ãŸã¯ **TF-IDF**ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚„æƒ…å ±æ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã€æ–‡æ›¸å†…ã®å˜èªã®é‡è¦æ€§ã‚’ã‚³ãƒ¼ãƒ‘ã‚¹ã«å¯¾ã—ã¦è©•ä¾¡ã—ã¾ã™ã€‚ã“ã®æŠ€è¡“ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é©ã—ãŸæ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ”ï¸ **TF-IDFã®æ§‹æˆè¦ç´ **\n",
    "\n",
    "1. å˜èªé »åº¦ (TF):\n",
    "\n",
    "   - *å®šç¾©:* æ–‡æ›¸å†…ã®ç‰¹å®šå˜èªã®å‡ºç¾é »åº¦ã‚’æ¸¬å®šã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , ã“ã“ã§ $ f_{t,d} $ ã¯æ–‡æ›¸ $ d $ å†…ã®å˜èª $ t $ ã®å‡ºç¾é »åº¦ã§ã™ã€‚\n",
    "\n",
    "2. é€†æ–‡æ›¸é »åº¦ (IDF):\n",
    "\n",
    "   - *å®šç¾©:* ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã«ãŠã‘ã‚‹ç‰¹å®šå˜èªã®é‡è¦æ€§ã‚’æ¸¬å®šã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , ã“ã“ã§ $ N $ ã¯æ–‡æ›¸ã®ç·æ•°ã€$ n_t $ ã¯å˜èª $ t $ ã‚’å«ã‚€æ–‡æ›¸ã®æ•°ã§ã™ã€‚\n",
    "\n",
    "3. TF-IDFã‚¹ã‚³ã‚¢:\n",
    "\n",
    "   - *å®šç¾©:* TFã¨IDFã‚¹ã‚³ã‚¢ã®ç©ã§ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "âœ”ï¸ ***N-grams* ã®èª¬æ˜**\n",
    "\n",
    "*N-grams* ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹é€£ç¶šã—ãŸ $ n $ é …ç›®ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å€‹ã€…ã®å˜èªã«æ¯”ã¹ã¦è¨€èªæ§‹é€ ã¨æ–‡è„ˆã®ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªè¡¨ç¾ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "*å¼:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*ä¾‹:* `ngrams = (1, 3)` ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸å†…ã®é•·ã•3ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§å¯èƒ½ãªå…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ„ã¿åˆã‚ã›ã‚’è€ƒæ…®ã—ã¾ã™ã€‚å„3ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ„ã¿åˆã‚ã›ã¯ãƒˆãƒ©ã‚¤ã‚°ãƒ©ãƒ ã‚’è¡¨ã—ã¾ã™ã€‚\n",
    "\n",
    "ä¾‹ãˆã°ã€ã€ŒI love coding.ã€ã¨ã„ã†æ–‡ã‚’è€ƒãˆã¾ã™ã€‚\n",
    "\n",
    "`ngrams = (1, 3)`ã®å ´åˆã€ã“ã®æ–‡ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹n-gramsã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ï¼š\n",
    "\n",
    "   * ãƒ¦ãƒ‹ã‚°ãƒ©ãƒ  (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * ãƒã‚¤ã‚°ãƒ©ãƒ  (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * ãƒˆãƒªã‚°ãƒ©ãƒ  (3-grams): [\"I love coding\"]\n",
    "\n",
    "ã“ã®ã‚ˆã†ã«ã€$ N-grams $ ã¯å€‹ã€…ã®å˜èªã ã‘ã§ãªãã€ãƒ•ãƒ¬ãƒ¼ã‚ºã‚„æ–‡ä¸­ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚‚æ‰ãˆã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "   \n",
    "âœ”ï¸ **TF-IDFã®ã‚¹ãƒ†ãƒƒãƒ—**\n",
    "\n",
    "1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–:\n",
    "\n",
    "   - *å®šç¾©:* ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†ã‘ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. æ–‡æ›¸é »åº¦ã®è¨ˆç®—:\n",
    "\n",
    "   - *å®šç¾©:* å„å˜èªã‚’å«ã‚€æ–‡æ›¸ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* \"love\" ã¯1ã¤ã®æ–‡æ›¸ã«å‡ºç¾ã—ã¾ã™ã€‚\n",
    "\n",
    "3. TF-IDFã®è¨ˆç®—:\n",
    "\n",
    "   - *å®šç¾©:* å„æ–‡æ›¸å†…ã®å„å˜èªã®TF-IDFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* ngrams = (1, 3) ã®å ´åˆã€\"love\"ã¯æ–‡æ›¸1ã«å‡ºç¾ã—ã€ãã®TFã¨IDFã«åŸºã¥ã„ã¦TF-IDFã‚¹ã‚³ã‚¢ãŒè¨ˆç®—ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–:\n",
    "\n",
    "   - *å®šç¾©:* å„æ–‡æ›¸ã‚’TF-IDFã‚¹ã‚³ã‚¢ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* å„æ–‡æ›¸ã¯ã€é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã€å„æ¬¡å…ƒã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã¾ãŸã¯n-gramã«å¯¾å¿œã—ã¾ã™ã€‚\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† ğŸ› ï¸\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9337728",
   "metadata": {},
   "source": [
    "# ç†è«– ğŸ“’\n",
    "\n",
    "âœ”ï¸ **å˜èªé »åº¦ - é€†æ–‡æ›¸é »åº¦**ã€ã¾ãŸã¯ **TF-IDF**ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒã‚¤ãƒ‹ãƒ³ã‚°ã‚„æƒ…å ±æ¤œç´¢ã§ä½¿ç”¨ã•ã‚Œã€æ–‡æ›¸å†…ã®å˜èªã®é‡è¦æ€§ã‚’ã‚³ãƒ¼ãƒ‘ã‚¹ã«å¯¾ã—ã¦è©•ä¾¡ã—ã¾ã™ã€‚ã“ã®æŠ€è¡“ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«é©ã—ãŸæ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›ã—ã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "\n",
    "âœ”ï¸ **TF-IDFã®æ§‹æˆè¦ç´ **\n",
    "\n",
    "1. å˜èªé »åº¦ (TF):\n",
    "\n",
    "   - *å®šç¾©:* æ–‡æ›¸å†…ã®ç‰¹å®šå˜èªã®å‡ºç¾é »åº¦ã‚’æ¸¬å®šã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , ã“ã“ã§ $ f_{t,d} $ ã¯æ–‡æ›¸ $ d $ å†…ã®å˜èª $ t $ ã®å‡ºç¾é »åº¦ã§ã™ã€‚\n",
    "\n",
    "2. é€†æ–‡æ›¸é »åº¦ (IDF):\n",
    "\n",
    "   - *å®šç¾©:* ã‚³ãƒ¼ãƒ‘ã‚¹å…¨ä½“ã«ãŠã‘ã‚‹ç‰¹å®šå˜èªã®é‡è¦æ€§ã‚’æ¸¬å®šã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , ã“ã“ã§ $ N $ ã¯æ–‡æ›¸ã®ç·æ•°ã€$ n_t $ ã¯å˜èª $ t $ ã‚’å«ã‚€æ–‡æ›¸ã®æ•°ã§ã™ã€‚\n",
    "\n",
    "3. TF-IDFã‚¹ã‚³ã‚¢:\n",
    "\n",
    "   - *å®šç¾©:* TFã¨IDFã‚¹ã‚³ã‚¢ã®ç©ã§ã™ã€‚\n",
    "   \n",
    "   - *å¼:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "âœ”ï¸ ***N-grams* ã®èª¬æ˜**\n",
    "\n",
    "*N-grams* ã¯ã€ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹é€£ç¶šã—ãŸ $ n $ é …ç›®ï¼ˆãƒˆãƒ¼ã‚¯ãƒ³ï¼‰ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å€‹ã€…ã®å˜èªã«æ¯”ã¹ã¦è¨€èªæ§‹é€ ã¨æ–‡è„ˆã®ã‚ˆã‚ŠåŒ…æ‹¬çš„ãªè¡¨ç¾ãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n",
    "\n",
    "*å¼:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*ä¾‹:* `ngrams = (1, 3)` ã®å ´åˆã€ãƒ†ã‚­ã‚¹ãƒˆæ–‡æ›¸å†…ã®é•·ã•3ã®ã‚¹ãƒ©ã‚¤ãƒ‡ã‚£ãƒ³ã‚°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã§å¯èƒ½ãªå…¨ã¦ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ„ã¿åˆã‚ã›ã‚’è€ƒæ…®ã—ã¾ã™ã€‚å„3ãƒˆãƒ¼ã‚¯ãƒ³ã®çµ„ã¿åˆã‚ã›ã¯ãƒˆãƒ©ã‚¤ã‚°ãƒ©ãƒ ã‚’è¡¨ã—ã¾ã™ã€‚\n",
    "\n",
    "ä¾‹ãˆã°ã€ã€ŒI love coding.ã€ã¨ã„ã†æ–‡ã‚’è€ƒãˆã¾ã™ã€‚\n",
    "\n",
    "`ngrams = (1, 3)`ã®å ´åˆã€ã“ã®æ–‡ã‹ã‚‰æŠ½å‡ºã•ã‚Œã‚‹n-gramsã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™ï¼š\n",
    "\n",
    "   * ãƒ¦ãƒ‹ã‚°ãƒ©ãƒ  (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * ãƒã‚¤ã‚°ãƒ©ãƒ  (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * ãƒˆãƒªã‚°ãƒ©ãƒ  (3-grams): [\"I love coding\"]\n",
    "\n",
    "ã“ã®ã‚ˆã†ã«ã€$ N-grams $ ã¯å€‹ã€…ã®å˜èªã ã‘ã§ãªãã€ãƒ•ãƒ¬ãƒ¼ã‚ºã‚„æ–‡ä¸­ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±ã‚‚æ‰ãˆã¾ã™ã€‚\n",
    "\n",
    "---\n",
    "   \n",
    "âœ”ï¸ **TF-IDFã®ã‚¹ãƒ†ãƒƒãƒ—**\n",
    "\n",
    "1. ãƒˆãƒ¼ã‚¯ãƒ³åŒ–:\n",
    "\n",
    "   - *å®šç¾©:* ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³ã«åˆ†ã‘ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. æ–‡æ›¸é »åº¦ã®è¨ˆç®—:\n",
    "\n",
    "   - *å®šç¾©:* å„å˜èªã‚’å«ã‚€æ–‡æ›¸ã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* \"love\" ã¯1ã¤ã®æ–‡æ›¸ã«å‡ºç¾ã—ã¾ã™ã€‚\n",
    "\n",
    "3. TF-IDFã®è¨ˆç®—:\n",
    "\n",
    "   - *å®šç¾©:* å„æ–‡æ›¸å†…ã®å„å˜èªã®TF-IDFã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* ngrams = (1, 3) ã®å ´åˆã€\"love\"ã¯æ–‡æ›¸1ã«å‡ºç¾ã—ã€ãã®TFã¨IDFã«åŸºã¥ã„ã¦TF-IDFã‚¹ã‚³ã‚¢ãŒè¨ˆç®—ã•ã‚Œã¾ã™ã€‚\n",
    "\n",
    "4. ãƒ™ã‚¯ãƒˆãƒ«åŒ–:\n",
    "\n",
    "   - *å®šç¾©:* å„æ–‡æ›¸ã‚’TF-IDFã‚¹ã‚³ã‚¢ã®ãƒ™ã‚¯ãƒˆãƒ«ã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚\n",
    "   \n",
    "   - *ä¾‹:* å„æ–‡æ›¸ã¯ã€é«˜æ¬¡å…ƒãƒ™ã‚¯ãƒˆãƒ«ã«ãªã‚Šã€å„æ¬¡å…ƒã¯ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã¾ãŸã¯n-gramã«å¯¾å¿œã—ã¾ã™ã€‚\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç† ğŸ› ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87440df",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Plot average response word count per winner model\n",
    "eda.response_length(train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class DataPreprocessing:\n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã«NoneãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))  # 1ã¤ã§ã‚‚NoneãŒã‚ã‚Œã°1ã‚’è¿”ã™\n",
    "\n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã®æ–‡å­—åˆ—ã®åˆè¨ˆé•·ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):  # valãŒæ–‡å­—åˆ—ã§ã‚ã‚Œã°\n",
    "                length += len(val)  # ãã®é•·ã•ã‚’åŠ ç®—\n",
    "        return length\n",
    "    \n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):  # valsãŒæ–‡å­—åˆ—ã®å ´åˆ\n",
    "            return len(set(vals.split()))  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦è¿”ã™\n",
    "        return 0\n",
    "    \n",
    "    # ãƒªã‚¹ãƒˆå†…ã®'None'ã‚’'STR'ã«ç½®ãæ›ãˆã€è¦ç´ ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§çµåˆ\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):  # textãŒãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])  # Noneã‚’'STR'ã«ç½®ãæ›ãˆ\n",
    "            return cleaned_text\n",
    "\n",
    "        return text  # ãã‚Œä»¥å¤–ã¯å…ƒã®textã‚’è¿”ã™\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # å¿œç­”åˆ—ã®é•·ã•ã‚„Noneã®æœ‰ç„¡ã«é–¢é€£ã™ã‚‹ç‰¹å¾´ã‚’è¿½åŠ \n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)  # å¿œç­”Aã®é•·ã•\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)  # å¿œç­”Bã®é•·ã•\n",
    "\n",
    "        # å¿œç­”ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°ã‚’è¨ˆç®—\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)  # å¿œç­”Aã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)  # å¿œç­”Bã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°\n",
    "\n",
    "        # é•·ã•ã®å·®ã€å¹³å‡é•·ã•ã€é•·ã•å·®æ¯”ã‚’è¨ˆç®—\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]  # é•·ã•ã®å·®\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2  # å¹³å‡é•·ã•\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]  # é•·ã•å·®æ¯”\n",
    "\n",
    "        # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å·®ã€å¹³å‡ã€ãŠã‚ˆã³æ¯”ã‚’è¨ˆç®—\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å·®\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å¹³å‡\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°å·®æ¯”\n",
    "\n",
    "        # å¿œç­”åˆ—å†…ã«NoneãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)  # å¿œç­”Aã«NoneãŒã‚ã‚‹ã‹\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)  # å¿œç­”Bã«NoneãŒã‚ã‚‹ã‹\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]  # Noneã®å·®\n",
    "\n",
    "        return data  # åŠ å·¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Aï¼ˆaï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Bï¼ˆbï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb  # é¡ä¼¼åº¦ã‚’è¿”ã™\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”é–“ã®è·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ï¼‰ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Aï¼ˆaï¼‰ã®è·é›¢\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Bï¼ˆbï¼‰ã®è·é›¢\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb  # è·é›¢ã‚’è¿”ã™\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«ãƒ©ã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµåˆ\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦æº–å‚™\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ãŸã‚ã«ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆ\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # TF-IDFãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # æ¬¡å…ƒå‰Šæ¸›ã‚’ãƒˆãƒ©ãƒ³ã‚±ã‚¤ãƒ†ãƒƒãƒ‰SVDã§å®Ÿæ–½\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # çŸ­ç¸®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å¿œç­”Aã€ãŠã‚ˆã³å¿œç­”Béƒ¨åˆ†ã«åˆ†å‰²\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # çŸ­ç¸®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®SVDç‰¹å¾´ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # SVDç‰¹å¾´ã‚’ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¯¾å¿œã™ã‚‹åˆ—ã«å‰²ã‚Šå½“ã¦\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # æ–°ã—ã„ç‰¹å¾´ã‚’å…ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨é€£çµ\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # é¡ä¼¼åº¦ã¨è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test  # åŠ å·¥ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "    \n",
    "    # è¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’å˜ä¸€ã®ãƒ©ãƒ™ãƒ«ã«ãƒãƒ¼ã‚¸\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0  # ãƒ¢ãƒ‡ãƒ«AãŒå‹è€…ã®å ´åˆ\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1  # ãƒ¢ãƒ‡ãƒ«BãŒå‹è€…ã®å ´åˆ\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2  # å¼•ãåˆ†ã‘ã®å ´åˆ\n",
    "        raise ValueError(\"å€¤ãŒç„¡åŠ¹ã§ã™ã€‚\")  # ç„¡åŠ¹ãªå€¤ã®å ´åˆã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T18:16:09.085037Z",
     "iopub.status.busy": "2024-06-17T18:16:09.084595Z",
     "iopub.status.idle": "2024-06-17T18:16:09.127941Z",
     "shell.execute_reply": "2024-06-17T18:16:09.126578Z",
     "shell.execute_reply.started": "2024-06-17T18:16:09.085004Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã«NoneãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))  # 1ã¤ã§ã‚‚NoneãŒã‚ã‚Œã°1ã‚’è¿”ã™\n",
    "\n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã®æ–‡å­—åˆ—ã®åˆè¨ˆé•·ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):  # valãŒæ–‡å­—åˆ—ã§ã‚ã‚Œã°\n",
    "                length += len(val)  # ãã®é•·ã•ã‚’åŠ ç®—\n",
    "        return length\n",
    "    \n",
    "    # å…¥åŠ›ãƒªã‚¹ãƒˆå†…ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®ã‚«ã‚¦ãƒ³ãƒˆã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):  # valsãŒæ–‡å­—åˆ—ã®å ´åˆ\n",
    "            return len(set(vals.split()))  # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªã®æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦è¿”ã™\n",
    "        return 0\n",
    "    \n",
    "    # ãƒªã‚¹ãƒˆå†…ã®'None'ã‚’'STR'ã«ç½®ãæ›ãˆã€è¦ç´ ã‚’ã‚¹ãƒšãƒ¼ã‚¹ã§çµåˆ\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):  # textãŒãƒªã‚¹ãƒˆã®å ´åˆ\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])  # Noneã‚’'STR'ã«ç½®ãæ›ãˆ\n",
    "            return cleaned_text\n",
    "\n",
    "        return text  # ãã‚Œä»¥å¤–ã¯å…ƒã®textã‚’è¿”ã™\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # å¿œç­”åˆ—ã®é•·ã•ã‚„Noneã®æœ‰ç„¡ã«é–¢é€£ã™ã‚‹ç‰¹å¾´ã‚’è¿½åŠ \n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)  # å¿œç­”Aã®é•·ã•\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)  # å¿œç­”Bã®é•·ã•\n",
    "\n",
    "        # å¿œç­”ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå˜èªæ•°ã‚’è¨ˆç®—\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)  # å¿œç­”Aã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)  # å¿œç­”Bã®ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°\n",
    "\n",
    "        # é•·ã•ã®å·®ã€å¹³å‡é•·ã•ã€é•·ã•å·®æ¯”ã‚’è¨ˆç®—\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]  # é•·ã•ã®å·®\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2  # å¹³å‡é•·ã•\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]  # é•·ã•å·®æ¯”\n",
    "\n",
    "        # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å·®ã€å¹³å‡ã€ãŠã‚ˆã³æ¯”ã‚’è¨ˆç®—\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å·®\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°ã®å¹³å‡\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])  # ãƒ¦ãƒ‹ãƒ¼ã‚¯å˜èªæ•°å·®æ¯”\n",
    "\n",
    "        # å¿œç­”åˆ—å†…ã«NoneãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)  # å¿œç­”Aã«NoneãŒã‚ã‚‹ã‹\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)  # å¿œç­”Bã«NoneãŒã‚ã‚‹ã‹\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]  # Noneã®å·®\n",
    "\n",
    "        return data  # åŠ å·¥ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "    \n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”é–“ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Aï¼ˆaï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Bï¼ˆbï¼‰ã®ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb  # é¡ä¼¼åº¦ã‚’è¿”ã™\n",
    "\n",
    "    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å¿œç­”é–“ã®è·é›¢ï¼ˆãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰/ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ï¼‰ã‚’è¨ˆç®—\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Aï¼ˆaï¼‰ã®è·é›¢\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆpï¼‰ã¨å¿œç­”Bï¼ˆbï¼‰ã®è·é›¢\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb  # è·é›¢ã‚’è¿”ã™\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«ãƒ©ã‚¤ã‚¶ã‚’åˆæœŸåŒ–\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’å˜ä¸€ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«çµåˆ\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦æº–å‚™\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # TF-IDFãƒ™ã‚¯ãƒˆãƒ«åŒ–ã®ãŸã‚ã«ã™ã¹ã¦ã®ãƒ†ã‚­ã‚¹ãƒˆåˆ—ã‚’çµåˆ\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # TF-IDFãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # æ¬¡å…ƒå‰Šæ¸›ã‚’ãƒˆãƒ©ãƒ³ã‚±ã‚¤ãƒ†ãƒƒãƒ‰SVDã§å®Ÿæ–½\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # ã‚³ãƒ¼ãƒ‘ã‚¹ã®ç•°ãªã‚‹éƒ¨åˆ†ã‚’åˆ†å‰²ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨ˆç®—\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # çŸ­ç¸®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€å¿œç­”Aã€ãŠã‚ˆã³å¿œç­”Béƒ¨åˆ†ã«åˆ†å‰²\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # çŸ­ç¸®ãƒãƒˆãƒªãƒƒã‚¯ã‚¹ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«åˆ†å‰²\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã®SVDç‰¹å¾´ã‚’ä¿æŒã™ã‚‹ãŸã‚ã®ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½œæˆ\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # SVDç‰¹å¾´ã‚’ç‰¹å¾´ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®å¯¾å¿œã™ã‚‹åˆ—ã«å‰²ã‚Šå½“ã¦\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # æ–°ã—ã„ç‰¹å¾´ã‚’å…ƒã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã¨é€£çµ\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # é¡ä¼¼åº¦ã¨è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ãƒ¦ãƒ¼ã‚¯ãƒªãƒƒãƒ‰è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ãƒ©ãƒ—ãƒ©ã‚·ã‚¢ãƒ³ã‚«ãƒ¼ãƒãƒ«è·é›¢ã®ç‰¹å¾´ã‚’è¨ˆç®—\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test  # åŠ å·¥ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™\n",
    "    \n",
    "    # è¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’å˜ä¸€ã®ãƒ©ãƒ™ãƒ«ã«ãƒãƒ¼ã‚¸\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0  # ãƒ¢ãƒ‡ãƒ«AãŒå‹è€…ã®å ´åˆ\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1  # ãƒ¢ãƒ‡ãƒ«BãŒå‹è€…ã®å ´åˆ\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2  # å¼•ãåˆ†ã‘ã®å ´åˆ\n",
    "        raise ValueError(\"å€¤ãŒç„¡åŠ¹ã§ã™ã€‚\")  # ç„¡åŠ¹ãªå€¤ã®å ´åˆã‚¨ãƒ©ãƒ¼ã‚’ç™ºç”Ÿ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9388c7b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Theory ğŸ“’\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "dp = DataPreprocessing()  # ãƒ‡ãƒ¼ã‚¿å‰å‡¦ç†ã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€ãƒ‡ãƒ¼ã‚¿ã®å‰å‡¦ç†ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f58e0",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "âœ”ï¸ **Term Frequency - Inverse Document Frequency** or **TF-IDF** vectorization is used in text mining and information retrieval to assess the importance of words in a document relative to a corpus. This technique transforms text data into a numerical format suitable for machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "âœ”ï¸ **Components of TF-IDF**\n",
    "\n",
    "1. Term Frequency (TF):\n",
    "\n",
    "   - *Definition:* Measures the frequency of a term in a document.\n",
    "   \n",
    "   - *Formula:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , where $ f_{t,d} $ is the frequency of term $ t $ in document $ d $.\n",
    "\n",
    "2. Inverse Document Frequency (IDF):\n",
    "\n",
    "   - *Definition:* Measures the importance of a term across the entire corpus.\n",
    "   \n",
    "   - *Formula:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , where $ N $ is the total number of documents, and $ n_t $ is the number of documents containing term $ t $.\n",
    "\n",
    "3. TF-IDF Score:\n",
    "\n",
    "   - *Definition:* Product of TF and IDF scores.\n",
    "   \n",
    "   - *Formula:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "âœ”ï¸ ***N-grams* explained**\n",
    "\n",
    "*N-grams* are contiguous sequences of $ n $ items (tokens) extracted from a text document. They provide a more comprehensive representation of the language structure and context compared to individual words.\n",
    "\n",
    "*Formula:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*Example:* For `ngrams = (1, 3)`, it means we are considering all possible combinations of tokens within a sliding window of length 3 in the text document. Each combination of 3 tokens represents a trigram. \n",
    "\n",
    "For instance, consider the sentence: \"I love coding.\"\n",
    "\n",
    "With `ngrams = (1, 3)`, the n-grams extracted from this sentence would include:\n",
    "\n",
    "   * Unigrams (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * Bigrams (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * Trigrams (3-grams): [\"I love coding\"]\n",
    "\n",
    "This way, $ N-grams $ capture not only individual words but also phrases and contextual information within the text.\n",
    "  \n",
    "---\n",
    "   \n",
    "âœ”ï¸ **Steps of TF-IDF**\n",
    "\n",
    "1. Tokenization:\n",
    "\n",
    "   - *Definition:* Breaks text into tokens.\n",
    "   \n",
    "   - *Example:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. Document Frequency Calculation:\n",
    "\n",
    "   - *Definition:* Counts the number of documents containing each term.\n",
    "   \n",
    "   - *Example:* \"love\" appears in 1 document out of 1.\n",
    "\n",
    "3. TF-IDF Calculation:\n",
    "\n",
    "   - *Definition:* Computes the TF-IDF score for each term in each document.\n",
    "   \n",
    "   - *Example:* For ngrams = (1, 3), \"love\" appears in Document 1, the TF-IDF score for \"love\" would be calculated based on its TF and IDF.\n",
    "\n",
    "4. Vectorization:\n",
    "\n",
    "   - *Definition:* Represents each document as a vector of TF-IDF scores.\n",
    "   \n",
    "   - *Example:* Each document becomes a high-dimensional vector where each dimension corresponds to a unique term or n-gram.\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# é•·ã•ã€é¡ä¼¼åº¦ã€ãŠã‚ˆã³è·é›¢ã®ç‰¹å¾´ã‚’è¿½åŠ ã™ã‚‹\n",
    "train_data = dp.add_features(train_data)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´ã‚’è¿½åŠ \n",
    "test_data = dp.add_features(test_data)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«ç‰¹å¾´ã‚’è¿½åŠ \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e1c7f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Data Preprocessing ğŸ› ï¸\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# TF-IDFç‰¹å¾´ã‚’æŠ½å‡ºã—ã€æ¬¡å…ƒå‰Šæ¸›ã‚’å®Ÿæ–½ã™ã‚‹\n",
    "train_data, test_data = dp.create_tfidf_features(train_data,  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "                                                 test_data,   # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨\n",
    "                                                 CFG.ngrams,  # n-gramã®è¨­å®š\n",
    "                                                 CFG.min_freq,  # æœ€å°é »åº¦ã®è¨­å®š\n",
    "                                                 CFG.max_freq,  # æœ€å¤§é »åº¦ã®è¨­å®š\n",
    "                                                 CFG.components)  # ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã®æ•°ã®è¨­å®š\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b238f8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class DataPreprocessing:\n",
    "    # Check if any value in the input list is None\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))\n",
    "\n",
    "    # Calculate the total length of strings in the input list\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):\n",
    "                length += len(val)\n",
    "        return length\n",
    "    \n",
    "    # Calculate the count of unique works in the input list\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):\n",
    "            return len(set(vals.split()))\n",
    "        return 0\n",
    "    \n",
    "    # Replace 'None' in the list with the string 'NONE', and join elements with a space\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])\n",
    "            return cleaned_text\n",
    "\n",
    "        return text\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # Add features related to the length and presence of None values in response columns.\n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)\n",
    "\n",
    "        # Calculate unique word count for responses\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)\n",
    "\n",
    "        # Calculate length difference, mean length, and length difference ratio.\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]\n",
    "\n",
    "        # Calculate unique word count difference, mean, and ratio.\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])\n",
    "\n",
    "        # Check if any value in response columns is None.\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    # Calculate cosine similarity between prompt and responses\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # Cosine similarity between prompt (p) and response_a (a)\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # Cosine similarity between prompt (p) and response_b (b)\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb\n",
    "\n",
    "    # Calculate distances (Euclidean/Laplacian) between prompt and responses\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # Distance between prompt (p) and response_a (a)\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # Distance between prompt (p) and response_b (b)\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # Initialize TF-IDF Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # Combine train and test data into a single DataFrame\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # Clean and prepare the text columns\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # Combine all text columns into a single corpus for TF-IDF vectorization\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # Compute the TF-IDF matrix\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # Perform dimensionality reduction with TruncatedSVD\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Calculate split indices for separating different parts of the corpus\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # Split the reduced matrix into prompts, response_a, and response_b parts\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # Separate the reduced matrix into training and testing sets\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # Create DataFrames to hold the SVD features for train and test sets\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # Assign SVD features to the respective columns in the feature DataFrames\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # Concatenate the new features with the original train and test DataFrames\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # Calculate similarity and distance features\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # Calculate cosine similarity features\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # Calculate Euclidean distance features\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # Calculate Laplacian kernel distance features\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test\n",
    "    \n",
    "    # Merges multiple labels into a single label\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2\n",
    "        raise ValueError(\"The value is invalid.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# è¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’å˜ä¸€ã®ãƒ©ãƒ™ãƒ«ã«ãƒãƒ¼ã‚¸ã™ã‚‹\n",
    "train_data[\"target\"] = train_data[  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ä½œæˆ\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "].apply(lambda x: dp.merge_label(x), axis=1)  # å„è¡Œã«merge_labelé–¢æ•°ã‚’é©ç”¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¤‡æ•°ã®ãƒ©ãƒ™ãƒ«ã‚’å˜ä¸€ã®ãƒ©ãƒ™ãƒ«ã«ãƒãƒ¼ã‚¸ã™ã‚‹\n",
    "train_data[\"target\"] = train_data[  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆåˆ—ã‚’ä½œæˆ\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "].apply(lambda x: dp.merge_label(x), axis=1)  # å„è¡Œã«merge_labelé–¢æ•°ã‚’é©ç”¨ã—ã¦ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚’è¨­å®š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490f0a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "dp = DataPreprocessing()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ãƒ¢ãƒ‡ãƒ«é–‹ç™º ğŸ§ \n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac940c",
   "metadata": {},
   "source": [
    "# ãƒ¢ãƒ‡ãƒ«é–‹ç™º ğŸ§ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e743eff",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Add length, similarity and distance features\n",
    "train_data = dp.add_features(train_data)\n",
    "test_data = dp.add_features(test_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡º\n",
    "        X_train = train_data[feature_cols].values  # ç‰¹å¾´é‡ã®è¡Œåˆ—\n",
    "        X_test = test_data[feature_cols].values    # ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡\n",
    "        Y_train = train_data[\"target\"]              # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«\n",
    "\n",
    "        # äºˆæ¸¬ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # StratifiedKFoldã‚’åˆæœŸåŒ–\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # ç¾åœ¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã«åˆ†å‰²\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # ç¾åœ¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã†\n",
    "            train_preds = model.predict(X_train)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬\n",
    "            test_preds = model.predict(X_test)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬\n",
    "\n",
    "            train_preds_list.append(train_preds)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            test_preds_list.append(test_preds)    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\n",
    "        # äºˆæ¸¬ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "        train_preds = np.mean(train_preds_list, axis=0)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ã®å¹³å‡\n",
    "        test_preds = np.mean(test_preds_list, axis=0)    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®å¹³å‡\n",
    "\n",
    "        return train_preds, test_preds  # äºˆæ¸¬ã‚’è¿”ã™\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ç”¨ã®æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)  # æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—\n",
    "\n",
    "        # ã‚«ã‚¹ã‚¿ãƒ ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ä½œæˆ\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # æ•´æ•°å€¤ã«å¤‰æ›ã—ã¦ã‚«ãƒ³ãƒãªã—ã§è¿”ã™\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"çœŸå®Ÿ: %{y}<br>äºˆæ¸¬: %{x}<br>ã‚«ã‚¦ãƒ³ãƒˆ: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # èƒŒæ™¯ã‚’é€æ˜ã«ã—ã€æ­£æ–¹å½¢ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«è¨­å®šã™ã‚‹ãŸã‚ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"äºˆæ¸¬ãƒ©ãƒ™ãƒ«\",\n",
    "            yaxis_title=\"çœŸå®Ÿã®ãƒ©ãƒ™ãƒ«\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ç‰¹å¾´é‡ã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«ã‚’æŠ½å‡º\n",
    "        X_train = train_data[feature_cols].values  # ç‰¹å¾´é‡ã®è¡Œåˆ—\n",
    "        X_test = test_data[feature_cols].values    # ãƒ†ã‚¹ãƒˆç”¨ç‰¹å¾´é‡\n",
    "        Y_train = train_data[\"target\"]              # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ©ãƒ™ãƒ«\n",
    "\n",
    "        # äºˆæ¸¬ã‚’ä¿å­˜ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # StratifiedKFoldã‚’åˆæœŸåŒ–\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # ç¾åœ¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã®ãŸã‚ã«ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚»ãƒƒãƒˆã«åˆ†å‰²\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã®ãŸã‚ã®LightGBMãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’ä½œæˆ\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # ç¾åœ¨ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒ‰ã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨“ç·´\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆã‚»ãƒƒãƒˆã«å¯¾ã—ã¦äºˆæ¸¬ã‚’è¡Œã†\n",
    "            train_preds = model.predict(X_train)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬\n",
    "            test_preds = model.predict(X_test)    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬\n",
    "\n",
    "            train_preds_list.append(train_preds)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "            test_preds_list.append(test_preds)    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã‚’ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "\n",
    "        # äºˆæ¸¬ã®å¹³å‡ã‚’è¨ˆç®—\n",
    "        train_preds = np.mean(train_preds_list, axis=0)  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°äºˆæ¸¬ã®å¹³å‡\n",
    "        test_preds = np.mean(test_preds_list, axis=0)    # ãƒ†ã‚¹ãƒˆäºˆæ¸¬ã®å¹³å‡\n",
    "\n",
    "        return train_preds, test_preds  # äºˆæ¸¬ã‚’è¿”ã™\n",
    "    \n",
    "    # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿äºˆæ¸¬ç”¨ã®æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)  # æ··åŒè¡Œåˆ—ã‚’è¨ˆç®—\n",
    "\n",
    "        # ã‚«ã‚¹ã‚¿ãƒ ãƒ›ãƒãƒ¼ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚©ãƒ¼ãƒãƒƒã‚¿ã‚’ä½œæˆ\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # æ•´æ•°å€¤ã«å¤‰æ›ã—ã¦ã‚«ãƒ³ãƒãªã—ã§è¿”ã™\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚’ä½œæˆ\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"çœŸå®Ÿ: %{y}<br>äºˆæ¸¬: %{x}<br>ã‚«ã‚¦ãƒ³ãƒˆ: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # èƒŒæ™¯ã‚’é€æ˜ã«ã—ã€æ­£æ–¹å½¢ã®ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ã«è¨­å®šã™ã‚‹ãŸã‚ã«ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æ›´æ–°\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"äºˆæ¸¬ãƒ©ãƒ™ãƒ«\",\n",
    "            yaxis_title=\"çœŸå®Ÿã®ãƒ©ãƒ™ãƒ«\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # ãƒ—ãƒ­ãƒƒãƒˆã‚’è¡¨ç¤º\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4b9d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Extract TF-IDF features and perform dimensionality reduction\n",
    "train_data, test_data = dp.create_tfidf_features(train_data, \n",
    "                                                 test_data, \n",
    "                                                 CFG.ngrams,\n",
    "                                                 CFG.min_freq, \n",
    "                                                 CFG.max_freq, \n",
    "                                                 CFG.components)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "md = ModelDevelopment()  # ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚„è©•ä¾¡ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelDevelopment()  # ãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚¯ãƒ©ã‚¹ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä½œæˆã—ã€ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´ã‚„è©•ä¾¡ã‚’å®Ÿè¡Œã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c255520",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Merge multiple labels into a single label\n",
    "train_data[\"target\"] = train_data[\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "                                 ].apply(lambda x: dp.merge_label(x), axis=1)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ãƒ©ãƒ™ãƒ«åˆ—ã‚’å®šç¾©\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "# é™¤å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œãªã„åˆ—ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ãƒªã‚¹ãƒˆåŒ–\n",
    "features = [col for col in train_data.columns if col not in excluded_features]  # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒ©ãƒ™ãƒ«åˆ—ã‚’å®šç¾©\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰é™¤å¤–ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "# é™¤å¤–ãƒªã‚¹ãƒˆã«å«ã¾ã‚Œãªã„åˆ—ã‚’ç‰¹å¾´é‡ã¨ã—ã¦ãƒªã‚¹ãƒˆåŒ–\n",
    "features = [col for col in train_data.columns if col not in excluded_features]  # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡ã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66b2c0",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Model Development ğŸ§ \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "# LightGBMã‚’è¨“ç·´ã™ã‚‹\n",
    "train_preds, test_preds = md.train_lgb(  # ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ç”¨ã„ã¦LightGBMã‚’è¨“ç·´\n",
    "    train_data, \n",
    "    test_data, \n",
    "    features,  # ä½¿ç”¨ã™ã‚‹ç‰¹å¾´é‡\n",
    "    CFG.params,  # ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    CFG.early_stop,  # æ—©æœŸçµ‚äº†ã®ãŸã‚ã®åŸºæº–\n",
    "    CFG.log_steps  # ãƒ­ã‚°ã‚¹ãƒ†ãƒƒãƒ—ã®è¨­å®š\n",
    ")\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714e556",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # Extract feature values and target labels from the training and testing data\n",
    "        X_train = train_data[feature_cols].values\n",
    "        X_test = test_data[feature_cols].values\n",
    "        Y_train = train_data[\"target\"]\n",
    "\n",
    "        # List to store predictions\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # Initialize StratifiedKFold\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # Split the training data into training and validation sets for the current fold\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # Create LightGBM dataset objects for training and validation\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # Train the model on the current fold\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # Make predictions on the train and test sets\n",
    "            train_preds = model.predict(X_train)\n",
    "            test_preds = model.predict(X_test)\n",
    "\n",
    "            train_preds_list.append(train_preds)\n",
    "            test_preds_list.append(test_preds)\n",
    "\n",
    "        # Average predictions\n",
    "        train_preds = np.mean(train_preds_list, axis=0)\n",
    "        test_preds = np.mean(test_preds_list, axis=0)\n",
    "\n",
    "        return train_preds, test_preds\n",
    "    \n",
    "    # Confusion matrix for train data predictions\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "        # Create a custom hover text formatter\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # Convert to integer without commas or \"k\"\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"True: %{y}<br>Predicted: %{x}<br>Count: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # Update layout for a transparent background and square aspect ratio\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"Predicted Labels\",\n",
    "            yaxis_title=\"True Labels\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ï¼ˆå¹³å‡ï¼‰äºˆæ¸¬ã®æ··åŒè¡Œåˆ—\n",
    "md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’ç”¨ã„ã¦æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã™ã‚‹ï¼ˆå¹³å‡ï¼‰äºˆæ¸¬ã®æ··åŒè¡Œåˆ—\n",
    "md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)  # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã‚’ç”¨ã„ã¦æ··åŒè¡Œåˆ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b0317",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "md = ModelDevelopment()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# äºˆæ¸¬ã‚’æå‡ºã™ã‚‹ ğŸ’¡\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484c33",
   "metadata": {},
   "source": [
    "# äºˆæ¸¬ã‚’æå‡ºã™ã‚‹ ğŸ’¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8c755",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonã‚³ãƒ¼ãƒ‰ã®æ¯”è¼ƒï¼ˆã‚¯ãƒªãƒƒã‚¯ã™ã‚‹ã¨å±•é–‹ã•ã‚Œã¾ã™ï¼‰</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Define label columns\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# Define the list of features to exclude from the training data\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "features = [col for col in train_data.columns if col not in excluded_features]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# æ—¥æœ¬èªè¨³\n",
    "\n",
    "```python\n",
    "# æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«äºˆæ¸¬ã—ãŸãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦\n",
    "subm_data[label_cols] = test_preds  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬çµæœã‚’æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ \n",
    "\n",
    "# æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜ã—ã€æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "subm_data.to_csv(\"submission.csv\", index=False)  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVå½¢å¼ã§ä¿å­˜\n",
    "display(subm_data.head(3))  # æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æå‡ºç”¨ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«äºˆæ¸¬ã—ãŸãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«ã‚’å‰²ã‚Šå½“ã¦\n",
    "subm_data[label_cols] = test_preds  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®äºˆæ¸¬çµæœã‚’æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã«è¿½åŠ \n",
    "\n",
    "# æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä¿å­˜ã—ã€æœ€åˆã®3è¡Œã‚’è¡¨ç¤º\n",
    "subm_data.to_csv(\"submission.csv\", index=False)  # æå‡ºãƒ•ã‚¡ã‚¤ãƒ«ã‚’CSVå½¢å¼ã§ä¿å­˜\n",
    "display(subm_data.head(3))  # æå‡ºãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã®æœ€åˆã®3è¡Œã‚’è¡¨ç¤º"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4959805,
     "sourceId": 8377405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2593.627859,
   "end_time": "2024-06-06T20:41:57.399746",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-06T19:58:43.771887",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
