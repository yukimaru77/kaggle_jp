{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "924b53f9",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyterノートブックは、ユーザーに対する大規模言語モデル（LLM）の応答の好みを予測するための機械学習モデルを構築することを目的としています。主に、LightGBMとTF-IDF（Term Frequency-Inverse Document Frequency）を用いるアプローチが採用されています。\n",
    "\n",
    "### 問題に取り組む内容\n",
    "ノートブックでは、ユーザーからのプロンプトに対する二つのLLMの応答の中で、どちらが好まれるかを予測するためのタスクが中心です。具体的には、次のステップが含まれています。\n",
    "\n",
    "1. **データの読み込みと探索的データ分析（EDA）**: データを読み込んで、基本的な統計情報や分布を確認します。\n",
    "2. **TF-IDFベクトル化**: テキストデータを数値フォーマットに変換するためにTF-IDFを用います。この手法は、重要な単語を特定し、文書の類似性を測定するために使われます。\n",
    "3. **データの前処理**: 各応答の情報を含む特徴量を追加し、類似性や距離を計算します。\n",
    "4. **モデルの訓練**: LightGBMを用いてデータを訓練し、交差検証を行いながら性能を評価します。\n",
    "5. **モデルの推論**: テストデータに対して予測を行い、結果を元に混同行列を作成し評価します。\n",
    "6. **提出用ファイルの生成**: 予測結果を適切なフォーマットでCSVファイルとして保存します。\n",
    "\n",
    "### 使用している主な手法とライブラリ\n",
    "- **LightGBM**: 勾配ブースティングフレームワークを使用してモデルを訓練します。これは大容量のデータセットに対して効率的かつ効果的に学習を行えるライブラリです。\n",
    "- **TF-IDF**: テキストデータを数値ベクトルに変換するために使用されています。具体的には、単語の重要性を測定するための手法です。\n",
    "- **Label Encoding**: 複数のラベルを単一のラベルに統合するためのカスタム関数が定義されています。\n",
    "- **混同行列**: モデルのパフォーマンスを評価するために使用されています。\n",
    "\n",
    "全体として、このノートブックはチャットボットの応答の好みを予測するために高度な機械学習を実装しており、ユーザーのニーズに対してより優れた応答を生成するための強力なフレームワークを提供しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88864ad",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、機械学習・深層学習の初心者がつまずきそうな専門用語の簡単な解説を挙げます。特に、このノートブック特有のドメイン知識や、実務を経験していないと馴染みのないものに焦点を当てています。\n",
    "\n",
    "1. **LightGBM**:\n",
    "   - 概要: Microsoftによって開発された勾配ブースティングアルゴリズムの一種で、大規模なデータセットで高速に動作し、メモリ効率が良いのが特徴です。\n",
    "   - 特徴: 決定木の葉の分割による学習を行い、トレーニング時間を大幅に短縮することができます。\n",
    "\n",
    "2. **TF-IDF (Term Frequency-Inverse Document Frequency)**:\n",
    "   - 概要: テキストデータを数値化する手法の一つで、特定の単語が文書内でどれくらい重要であるかを評価します。\n",
    "   - ドメイン特有のポイント: TFは単語の出現頻度を示し、IDFは単語の一般的な重要性を示すため、両者を掛け算することで、文書の内容に応じた重要な単語を特定できます。\n",
    "\n",
    "3. **n-grams**:\n",
    "   - 概要: テキストにおける連続したn個の単語（またはトークン）の組み合わせを指します。1つの単語からなる1-gram（unigram）、2つの単語からなる2-gram（bigram）などがあります。\n",
    "   - ドメイン特有のポイント: 文脈を捉える力が向上し、単語単位だけでは捉えきれない意味を抽出するのに役立つ。\n",
    "\n",
    "4. **StratifiedKFold**:\n",
    "   - 概要: K分割交差検証の一種で、各フォールドにおいてクラスの分布を維持します。\n",
    "   - ドメイン特有のポイント: クラス不均衡があるデータセットにおいて、モデルの評価をより正確に行うために使用されます。\n",
    "\n",
    "5. **トランケイテッド SVD (Truncated Singular Value Decomposition)**:\n",
    "   - 概要: 行列の次元削減技術で、大きなデータを小さくし、重要な情報を保持しつつ余計な情報を取り除きます。\n",
    "   - ドメイン特有のポイント: TF-IDFで得られた高次元の特徴空間をより小さな空間に圧縮する際に用いられます。\n",
    "\n",
    "6. **混同行列 (Confusion Matrix)**:\n",
    "   - 概要: 分類モデルの性能を評価するための表で、実際のラベルと予測されたラベルを比較するものです。\n",
    "   - ドメイン特有のポイント: 正しい予測と誤った予測の数を示し、各クラスの精度や再現率を計算する際に使用されます。\n",
    "\n",
    "7. **コサイン類似度 (Cosine Similarity)**:\n",
    "   - 概要: 2つのベクトル間の角度を使って、その類似性を測定する尺度です。1に近いほど、類似していることを示します。\n",
    "   - ドメイン特有のポイント: テキストの類似性を測定する際によく使用され、TF-IDFベクトルの比較に利用されます。\n",
    "\n",
    "8. **ユークリッド距離 (Euclidean Distance)**:\n",
    "   - 概要: 二点間の直線距離を計測する方法で、平面上の点同士の最短距離を示します。\n",
    "   - ドメイン特有のポイント: 特徴空間内でのデータポイント間の距離を測定し、分類やクラスタリングに役立ちます。\n",
    "\n",
    "9. **ラプラシアン カーネル (Laplacian Kernel)**:\n",
    "   - 概要: データポイント間の隣接関係をモデル化し、特にグラフデータに基づいた距離を評価するためのカーネル関数です。\n",
    "   - ドメイン特有のポイント: 特徴間の非線形関係を捉えたり、データの局所的な構造を考慮した距離計算に役立ちます。\n",
    "\n",
    "これらの用語の理解が進むことで、ノートブックの内容や機械学習の実務により自信を持って取り組めるようになるでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474c038d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Introduction 📜\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# イントロダクション 📜\n",
    "\n",
    "✔️ このノートブックの目的は何ですか？\n",
    "\n",
    "目標は、LightGBMとTF-IDFベクトル化を使用して、ユーザーのLLM応答への好みを予測するための堅牢で効率的なソリューションを作成することです。\n",
    "\n",
    "---\n",
    "\n",
    "✔️ このノートブックでは何が扱われていますか？\n",
    "\n",
    "- `データの読み込みと探索的データ分析（EDA）`\n",
    "\n",
    "- `TF-IDFの理論`\n",
    "\n",
    "- `データの前処理`\n",
    "\n",
    "- `モデルの訓練`\n",
    "\n",
    "- `モデルの推論`\n",
    "     \n",
    "# インポート 📦\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99df0fec",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "✔️ What is the objective of this notebook?\n",
    "\n",
    "The goal is to create a robust and efficient solution to predict users' preference of LLM responses using LightGBM and TF-IDF vectorization.\n",
    "\n",
    "---\n",
    "\n",
    "✔️ What does this notebook cover?\n",
    "\n",
    "- `Data Loading & EDA`\n",
    "\n",
    "- `Theory behind TF-IDF`\n",
    "\n",
    "- `Data Preprocessing`\n",
    "\n",
    "- `Model Training`\n",
    "       \n",
    "- `Model Inference`\n",
    "     \n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 警告メッセージを処理する\n",
    "import warnings\n",
    "# 警告を無視する設定を行います\n",
    "warnings.filterwarnings('ignore')  # これにより、実行時に表示される警告が表示されなくなります\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3329171a",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Imports 📦\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# データの前処理\n",
    "import numpy as np  # NumPyライブラリをインポート。数値計算や配列操作に使います。\n",
    "import pandas as pd  # pandasライブラリをインポート。データフレーム作成や操作に使用します。\n",
    "from pathlib import Path  # パス操作用のPathライブラリをインポート\n",
    "\n",
    "# データの視覚化\n",
    "import plotly.graph_objects as go  # Plotlyライブラリをインポート。インタラクティブなグラフ作成に使用します。\n",
    "from sklearn.metrics import confusion_matrix  # 混同行列の計算用のメトリクスをインポート\n",
    "\n",
    "# モデルの開発\n",
    "import lightgbm as lgb  # LightGBMライブラリをインポート。勾配ブースティングを用いたモデル作成に使用します。\n",
    "from sklearn.model_selection import StratifiedKFold  # 層化Kフォールド交差検証用のクラスをインポート\n",
    "\n",
    "# TF-IDFベクトル化\n",
    "from sklearn.decomposition import TruncatedSVD  # 次元削減用のトランケイテッドSVDをインポート\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFベクトル化のためのクラスをインポート\n",
    "\n",
    "# TF-IDFベクトルの類似性/距離特徴\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel  # コサイン類似度、ユークリッド距離、ラプラシアンカーネルの計算用メトリクスをインポート\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ee610a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Handle warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 設定 ⚙️\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966dc609",
   "metadata": {},
   "source": [
    "# 設定 ⚙️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe147a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Data preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Data visualization\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Model development\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Similarity/distance features for TF-IDF vectors\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # コンペティションデータへのパス\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # トレーニングデータのパス\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")    # テストデータのパス\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # 提出ファイルのサンプルパス\n",
    "    \n",
    "    # 混同行列のカラースケール\n",
    "    colorscale = \"peach\"  # 混同行列を表示する際のカラースケール設定\n",
    "    \n",
    "    # TF-IDFベクトル化のパラメータ\n",
    "    components = 32  # 次元削減で保持する成分の数\n",
    "    ngrams = (1, 7)  # 使用するn-gramの範囲\n",
    "    max_freq = 0.95  # 95%以上の文書に出現する単語は除外\n",
    "    min_freq = 10    # 10文書未満に出現する単語は除外\n",
    "    \n",
    "    # トレーニングの引数\n",
    "    num_classes = 3   # 分類するクラスの数\n",
    "    early_stop = 50   # 早期終了のためのイテレーション数\n",
    "    log_steps = 100   # ログを記録するステップ数\n",
    "    \n",
    "    # LightGBMのパラメータ\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",  # マルチクラス分類を指定\n",
    "        \"colsample_bytree\": 0.8,     # 木のサンプリング比率\n",
    "        \"colsample_bynode\": 0.8,      # ノードでのサンプリング比率\n",
    "        \"metric\": \"multiclass\",      # 評価指標をマルチクラスに設定\n",
    "        \"learning_rate\": 0.02,        # 学習率\n",
    "        \"extra_trees\": True,          # 追加の木を使用するかどうか\n",
    "        \"num_rounds\": 3000,           # 学習のラウンド数\n",
    "        \"reg_lambda\": 1.3,            # L2正則化\n",
    "        \"num_classes\": 3,             # 分類するクラス数\n",
    "        \"num_leaves\": 64,             # 葉の数\n",
    "        \"reg_alpha\": 0.1,             # L1正則化\n",
    "        \"device\": \"cpu\",              # デバイスはCPUを指定\n",
    "        \"max_depth\": 6,               # 木の最大深さ\n",
    "        \"max_bin\": 128,               # 最大ビンの数\n",
    "        \"verbose\": -1,                # 出力の詳細度設定\n",
    "        \"seed\": 42                    # 再現性のためのシード値\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:33:16.95883Z",
     "iopub.status.busy": "2024-06-17T17:33:16.958249Z",
     "iopub.status.idle": "2024-06-17T17:33:16.970032Z",
     "shell.execute_reply": "2024-06-17T17:33:16.968747Z",
     "shell.execute_reply.started": "2024-06-17T17:33:16.95869Z"
    }
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # コンペティションデータへのパス\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")  # トレーニングデータのパス\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")    # テストデータのパス\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")  # 提出ファイルのサンプルパス\n",
    "    \n",
    "    # 混同行列のカラースケール\n",
    "    colorscale = \"peach\"  # 混同行列を表示する際のカラースケール設定\n",
    "    \n",
    "    # TF-IDFベクトル化のパラメータ\n",
    "    components = 32  # 次元削減で保持する成分の数\n",
    "    ngrams = (1, 7)  # 使用するn-gramの範囲\n",
    "    max_freq = 0.95  # 95%以上の文書に出現する単語は除外\n",
    "    min_freq = 10    # 10文書未満に出現する単語は除外\n",
    "    \n",
    "    # トレーニングの引数\n",
    "    num_classes = 3   # 分類するクラスの数\n",
    "    early_stop = 50   # 早期終了のためのイテレーション数\n",
    "    log_steps = 100   # ログを記録するステップ数\n",
    "    \n",
    "    # LightGBMのパラメータ\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",  # マルチクラス分類を指定\n",
    "        \"colsample_bytree\": 0.8,     # 木のサンプリング比率\n",
    "        \"colsample_bynode\": 0.8,      # ノードでのサンプリング比率\n",
    "        \"metric\": \"multiclass\",      # 評価指標をマルチクラスに設定\n",
    "        \"learning_rate\": 0.02,        # 学習率\n",
    "        \"extra_trees\": True,          # 追加の木を使用するかどうか\n",
    "        \"num_rounds\": 3000,           # 学習のラウンド数\n",
    "        \"reg_lambda\": 1.3,            # L2正則化\n",
    "        \"num_classes\": 3,             # 分類するクラス数\n",
    "        \"num_leaves\": 64,             # 葉の数\n",
    "        \"reg_alpha\": 0.1,             # L1正則化\n",
    "        \"device\": \"cpu\",              # デバイスはCPUを指定\n",
    "        \"max_depth\": 6,               # 木の最大深さ\n",
    "        \"max_bin\": 128,               # 最大ビンの数\n",
    "        \"verbose\": -1,                # 出力の詳細度設定\n",
    "        \"seed\": 42                    # 再現性のためのシード値\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2973a7d",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Configuration ⚙️\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 探索的データ分析（EDA） 🗃️\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e2b98",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class CFG:\n",
    "    # Paths to competition data\n",
    "    train_data = Path(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test_data = Path(\"/kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "    subm_data = Path(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")\n",
    "    \n",
    "    # Colorscale for confusion matrix\n",
    "    colorscale = \"peach\"\n",
    "    \n",
    "    # TF-IDF Vectorization parameters\n",
    "    components = 32\n",
    "    ngrams = (1, 7) \n",
    "    max_freq = 0.95 # Words that occur in more than 95% of the documents are omitted\n",
    "    min_freq = 10   # Words that occur in less than 10 documents are omitted\n",
    "    \n",
    "    # Training arguments\n",
    "    num_classes = 3\n",
    "    early_stop = 50\n",
    "    log_steps = 100\n",
    "    \n",
    "    # LightGBM parameters\n",
    "    params = {\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"colsample_bytree\": 0.8,\n",
    "        \"colsample_bynode\": 0.8,\n",
    "        \"metric\": \"multiclass\",\n",
    "        \"learning_rate\": 0.02,\n",
    "        \"extra_trees\": True,\n",
    "        \"num_rounds\": 3000,\n",
    "        \"reg_lambda\": 1.3,\n",
    "        \"num_classes\": 3,\n",
    "        \"num_leaves\": 64,\n",
    "        \"reg_alpha\": 0.1,\n",
    "        \"device\": \"cpu\",\n",
    "        \"max_depth\": 6,\n",
    "        \"max_bin\": 128,\n",
    "        \"verbose\": -1,\n",
    "        \"seed\": 42\n",
    "    }\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # 指定したパスからデータフレームを読み込み\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # データフレームの形状と最初の3行を表示\n",
    "        print(f\"データフレームの形状は: {df.shape}です\")\n",
    "        display(df.head(3))  # データの最初の3行を表示\n",
    "        \n",
    "        return df  # 読み込んだデータフレームを返す\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # 各勝者の列のカウントを計算\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),  # モデルAの勝ち数\n",
    "            'winner_model_b': data['winner_model_b'].sum(),  # モデルBの勝ち数\n",
    "            'winner_tie': data['winner_tie'].sum()           # 引き分けの数\n",
    "        }\n",
    "\n",
    "        # カラーを定義\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # クリーム、ライトブルー、ミント\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']  # 各色の識別子\n",
    "        \n",
    "        # 円グラフを作成\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # 背景を透明にし、円グラフを左寄せにするためにレイアウトを更新\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # 凡例を非表示にする\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # プロットを表示\n",
    "        fig.show()\n",
    "\n",
    "        # カウントをテーブルとして表示\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['クラス', 'カウント'])\n",
    "        counts_df['識別子'] = identifiers\n",
    "        display(counts_df)  # カウントテーブルを表示\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # 元のデータを変更しないためにデータフレームのコピーを作成\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # 各応答の単語数を計算\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))  # モデルAの単語数\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))  # モデルBの単語数\n",
    "        \n",
    "        # 各勝者クラスの平均単語数を計算\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # モデルAの平均単語数\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # モデルBの平均単語数\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # 引き分けの平均単語数\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # カスタムホバーテキストを作成\n",
    "        hover_texts = [f\"単語数: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # 棒グラフを作成\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # 勝者クラスラベルをx軸に\n",
    "            y=list(word_counts.values()),  # 単語数をy軸に\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,  # ホバー時のテキスト\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # 棒を縦にする\n",
    "        )])\n",
    "        \n",
    "        # レイアウトを更新\n",
    "        fig.update_layout(\n",
    "            title='勝者クラスによる平均応答単語数',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='平均応答単語数',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # x軸のラベルを非表示\n",
    "        )\n",
    "        \n",
    "        # プロットを表示\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:20.288524Z",
     "iopub.status.busy": "2024-06-17T17:38:20.288118Z",
     "iopub.status.idle": "2024-06-17T17:38:20.308922Z",
     "shell.execute_reply": "2024-06-17T17:38:20.307347Z",
     "shell.execute_reply.started": "2024-06-17T17:38:20.288494Z"
    }
   },
   "outputs": [],
   "source": [
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # 指定したパスからデータフレームを読み込み\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # データフレームの形状と最初の3行を表示\n",
    "        print(f\"データフレームの形状は: {df.shape}です\")\n",
    "        display(df.head(3))  # データの最初の3行を表示\n",
    "        \n",
    "        return df  # 読み込んだデータフレームを返す\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # 各勝者の列のカウントを計算\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),  # モデルAの勝ち数\n",
    "            'winner_model_b': data['winner_model_b'].sum(),  # モデルBの勝ち数\n",
    "            'winner_tie': data['winner_tie'].sum()           # 引き分けの数\n",
    "        }\n",
    "\n",
    "        # カラーを定義\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # クリーム、ライトブルー、ミント\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']  # 各色の識別子\n",
    "        \n",
    "        # 円グラフを作成\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # 背景を透明にし、円グラフを左寄せにするためにレイアウトを更新\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # 凡例を非表示にする\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # プロットを表示\n",
    "        fig.show()\n",
    "\n",
    "        # カウントをテーブルとして表示\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['クラス', 'カウント'])\n",
    "        counts_df['識別子'] = identifiers\n",
    "        display(counts_df)  # カウントテーブルを表示\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # 元のデータを変更しないためにデータフレームのコピーを作成\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # 各応答の単語数を計算\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))  # モデルAの単語数\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))  # モデルBの単語数\n",
    "        \n",
    "        # 各勝者クラスの平均単語数を計算\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # モデルAの平均単語数\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # モデルBの平均単語数\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()  # 引き分けの平均単語数\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # カスタムホバーテキストを作成\n",
    "        hover_texts = [f\"単語数: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # 棒グラフを作成\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # 勝者クラスラベルをx軸に\n",
    "            y=list(word_counts.values()),  # 単語数をy軸に\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,  # ホバー時のテキスト\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # 棒を縦にする\n",
    "        )])\n",
    "        \n",
    "        # レイアウトを更新\n",
    "        fig.update_layout(\n",
    "            title='勝者クラスによる平均応答単語数',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='平均応答単語数',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # x軸のラベルを非表示\n",
    "        )\n",
    "        \n",
    "        # プロットを表示\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11760f9e",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Exploratory Data Analysis (EDA) 🗃️\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "eda = EDA()  # EDAクラスのインスタンスを作成し、探索的データ分析を実行できるようにします。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de82d0f",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class EDA:\n",
    "    def read_data(self, path):\n",
    "        # Read dataframe from path\n",
    "        df = pd.read_csv(path)\n",
    "        \n",
    "        # Display the shape of the dataframe and the first 3 rows\n",
    "        print(f\"The shape of the dataframe is: {df.shape}\")\n",
    "        display(df.head(3))\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def pie_chart(self, data):\n",
    "        # Calculate the counts for each winner column\n",
    "        counts = {\n",
    "            'winner_model_a': data['winner_model_a'].sum(),\n",
    "            'winner_model_b': data['winner_model_b'].sum(),\n",
    "            'winner_tie': data['winner_tie'].sum()\n",
    "        }\n",
    "\n",
    "        # Define the colors\n",
    "        colors = ['#a89192', '#8083a8', '#a8c28c']  # creme, light blue, mint\n",
    "        identifiers = ['Creme', 'Light Blue', 'Mint']\n",
    "        \n",
    "        # Create the pie chart\n",
    "        fig = go.Figure(data=[go.Pie(labels=identifiers, \n",
    "                                     values=list(counts.values()), \n",
    "                                     textinfo='percent', \n",
    "                                     hole=0.1,\n",
    "                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])\n",
    "        \n",
    "        # Update layout for a transparent background and move the pie to the left\n",
    "        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', \n",
    "                          paper_bgcolor='rgba(0,0,0,0)', \n",
    "                          margin=dict(l=0, r=0, t=0, b=0))\n",
    "        \n",
    "        # Hide the legend\n",
    "        fig.update_layout(showlegend=False)\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "\n",
    "        # Display the counts as a table\n",
    "        counts_df = pd.DataFrame(list(counts.items()), columns=['Class', 'Count'])\n",
    "        counts_df['Identifier'] = identifiers\n",
    "        display(counts_df)\n",
    "        \n",
    "    def response_length(self, data):\n",
    "        # Create a copy of the dataframe to avoid modifying the original data\n",
    "        data_copy = data.copy()\n",
    "        \n",
    "        # Calculate the number of words in each response\n",
    "        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))\n",
    "        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))\n",
    "        \n",
    "        # Calculate the average word count for each winner class\n",
    "        word_counts = {\n",
    "            'winner_model_a': int(\n",
    "                data_copy[data_copy['winner_model_a'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            ),\n",
    "            \n",
    "            'winner_model_b': int(\n",
    "                data_copy[data_copy['winner_model_b'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            ),\n",
    "            \n",
    "            'winner_tie': int(\n",
    "                data_copy[data_copy['winner_tie'] == 1][\n",
    "                    ['word_count_a', \n",
    "                     'word_count_b']\n",
    "                ].mean().mean()\n",
    "            )\n",
    "        }\n",
    "        \n",
    "        # Create custom hover text\n",
    "        hover_texts = [f\"Word Count: {value}<br>{key}\" for key, value in word_counts.items()]\n",
    "        \n",
    "        # Create the bar chart\n",
    "        fig = go.Figure(data=[go.Bar(\n",
    "            x=list(word_counts.keys()),  # Winner class labels on x-axis\n",
    "            y=list(word_counts.values()),\n",
    "            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),\n",
    "            hovertext=hover_texts,\n",
    "            hoverinfo='text',\n",
    "            orientation='v'  # Ensure bars are vertical\n",
    "        )])\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title='Average Response Word Count by Winner Class',\n",
    "            xaxis_title='',\n",
    "            yaxis_title='Average Response Word Count',\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis=dict(showticklabels=False)  # Hide x-axis labels\n",
    "        )\n",
    "        \n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "train_data = eda.read_data(CFG.train_data)  # 設定されたパスからトレーニングデータを読み込み、データフレームを取得します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:25.551342Z",
     "iopub.status.busy": "2024-06-17T17:38:25.550913Z",
     "iopub.status.idle": "2024-06-17T17:38:30.501504Z",
     "shell.execute_reply": "2024-06-17T17:38:30.500243Z",
     "shell.execute_reply.started": "2024-06-17T17:38:25.551312Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data = eda.read_data(CFG.train_data)  # 設定されたパスからトレーニングデータを読み込み、データフレームを取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f252d4aa",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "eda = EDA()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "test_data = eda.read_data(CFG.test_data)  # 設定されたパスからテストデータを読み込み、データフレームを取得します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:36.653436Z",
     "iopub.status.busy": "2024-06-17T17:38:36.652917Z",
     "iopub.status.idle": "2024-06-17T17:38:36.672035Z",
     "shell.execute_reply": "2024-06-17T17:38:36.670511Z",
     "shell.execute_reply.started": "2024-06-17T17:38:36.653398Z"
    }
   },
   "outputs": [],
   "source": [
    "test_data = eda.read_data(CFG.test_data)  # 設定されたパスからテストデータを読み込み、データフレームを取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f53cb00",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "train_data = eda.read_data(CFG.train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "subm_data = eda.read_data(CFG.subm_data)  # 設定されたパスから提出データのサンプルを読み込み、データフレームを取得します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:43.898851Z",
     "iopub.status.busy": "2024-06-17T17:38:43.898369Z",
     "iopub.status.idle": "2024-06-17T17:38:43.922869Z",
     "shell.execute_reply": "2024-06-17T17:38:43.921453Z",
     "shell.execute_reply.started": "2024-06-17T17:38:43.898813Z"
    }
   },
   "outputs": [],
   "source": [
    "subm_data = eda.read_data(CFG.subm_data)  # 設定されたパスから提出データのサンプルを読み込み、データフレームを取得します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f2eb78",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "test_data = eda.read_data(CFG.test_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "print(\"クラスの分布（勝者）:\")  # 勝者のクラス分布を表示するメッセージ\n",
    "eda.pie_chart(train_data)  # トレーニングデータを使ってクラス分布の円グラフを作成し表示します。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:48.962237Z",
     "iopub.status.busy": "2024-06-17T17:38:48.961789Z",
     "iopub.status.idle": "2024-06-17T17:38:49.482496Z",
     "shell.execute_reply": "2024-06-17T17:38:49.481275Z",
     "shell.execute_reply.started": "2024-06-17T17:38:48.962204Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"クラスの分布（勝者）:\")  # 勝者のクラス分布を表示するメッセージ\n",
    "eda.pie_chart(train_data)  # トレーニングデータを使ってクラス分布の円グラフを作成し表示します。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b7f2ed",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "subm_data = eda.read_data(CFG.subm_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 勝者モデルごとの平均応答単語数をプロットする\n",
    "eda.response_length(train_data)  # トレーニングデータを使用して、各勝者モデルの応答の平均単語数をプロットします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T17:38:59.135398Z",
     "iopub.status.busy": "2024-06-17T17:38:59.134924Z",
     "iopub.status.idle": "2024-06-17T17:39:01.210285Z",
     "shell.execute_reply": "2024-06-17T17:39:01.208513Z",
     "shell.execute_reply.started": "2024-06-17T17:38:59.13536Z"
    }
   },
   "outputs": [],
   "source": [
    "# 勝者モデルごとの平均応答単語数をプロットする\n",
    "eda.response_length(train_data)  # トレーニングデータを使用して、各勝者モデルの応答の平均単語数をプロットします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0183cdb2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "print(\"Distribution of classes (winners):\")\n",
    "eda.pie_chart(train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 理論 📒\n",
    "\n",
    "✔️ **単語頻度 - 逆文書頻度**、または **TF-IDF**ベクトル化は、テキストマイニングや情報検索で使用され、文書内の単語の重要性をコーパスに対して評価します。この技術は、テキストデータを機械学習アルゴリズムに適した数値フォーマットに変換します。\n",
    "\n",
    "---\n",
    "\n",
    "✔️ **TF-IDFの構成要素**\n",
    "\n",
    "1. 単語頻度 (TF):\n",
    "\n",
    "   - *定義:* 文書内の特定単語の出現頻度を測定します。\n",
    "   \n",
    "   - *式:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , ここで $ f_{t,d} $ は文書 $ d $ 内の単語 $ t $ の出現頻度です。\n",
    "\n",
    "2. 逆文書頻度 (IDF):\n",
    "\n",
    "   - *定義:* コーパス全体における特定単語の重要性を測定します。\n",
    "   \n",
    "   - *式:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , ここで $ N $ は文書の総数、$ n_t $ は単語 $ t $ を含む文書の数です。\n",
    "\n",
    "3. TF-IDFスコア:\n",
    "\n",
    "   - *定義:* TFとIDFスコアの積です。\n",
    "   \n",
    "   - *式:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "✔️ ***N-grams* の説明**\n",
    "\n",
    "*N-grams* は、テキスト文書から抽出される連続した $ n $ 項目（トークン）のシーケンスです。これにより、個々の単語に比べて言語構造と文脈のより包括的な表現が得られます。\n",
    "\n",
    "*式:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*例:* `ngrams = (1, 3)` の場合、テキスト文書内の長さ3のスライディングウィンドウで可能な全てのトークンの組み合わせを考慮します。各3トークンの組み合わせはトライグラムを表します。\n",
    "\n",
    "例えば、「I love coding.」という文を考えます。\n",
    "\n",
    "`ngrams = (1, 3)`の場合、この文から抽出されるn-gramsには以下が含まれます：\n",
    "\n",
    "   * ユニグラム (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * バイグラム (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * トリグラム (3-grams): [\"I love coding\"]\n",
    "\n",
    "このように、$ N-grams $ は個々の単語だけでなく、フレーズや文中のコンテキスト情報も捉えます。\n",
    "\n",
    "---\n",
    "   \n",
    "✔️ **TF-IDFのステップ**\n",
    "\n",
    "1. トークン化:\n",
    "\n",
    "   - *定義:* テキストをトークンに分けます。\n",
    "   \n",
    "   - *例:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. 文書頻度の計算:\n",
    "\n",
    "   - *定義:* 各単語を含む文書の数をカウントします。\n",
    "   \n",
    "   - *例:* \"love\" は1つの文書に出現します。\n",
    "\n",
    "3. TF-IDFの計算:\n",
    "\n",
    "   - *定義:* 各文書内の各単語のTF-IDFスコアを計算します。\n",
    "   \n",
    "   - *例:* ngrams = (1, 3) の場合、\"love\"は文書1に出現し、そのTFとIDFに基づいてTF-IDFスコアが計算されます。\n",
    "\n",
    "4. ベクトル化:\n",
    "\n",
    "   - *定義:* 各文書をTF-IDFスコアのベクトルとして表現します。\n",
    "   \n",
    "   - *例:* 各文書は、高次元ベクトルになり、各次元はユニークな単語またはn-gramに対応します。\n",
    "\n",
    "# データ前処理 🛠️\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9337728",
   "metadata": {},
   "source": [
    "# 理論 📒\n",
    "\n",
    "✔️ **単語頻度 - 逆文書頻度**、または **TF-IDF**ベクトル化は、テキストマイニングや情報検索で使用され、文書内の単語の重要性をコーパスに対して評価します。この技術は、テキストデータを機械学習アルゴリズムに適した数値フォーマットに変換します。\n",
    "\n",
    "---\n",
    "\n",
    "✔️ **TF-IDFの構成要素**\n",
    "\n",
    "1. 単語頻度 (TF):\n",
    "\n",
    "   - *定義:* 文書内の特定単語の出現頻度を測定します。\n",
    "   \n",
    "   - *式:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , ここで $ f_{t,d} $ は文書 $ d $ 内の単語 $ t $ の出現頻度です。\n",
    "\n",
    "2. 逆文書頻度 (IDF):\n",
    "\n",
    "   - *定義:* コーパス全体における特定単語の重要性を測定します。\n",
    "   \n",
    "   - *式:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , ここで $ N $ は文書の総数、$ n_t $ は単語 $ t $ を含む文書の数です。\n",
    "\n",
    "3. TF-IDFスコア:\n",
    "\n",
    "   - *定義:* TFとIDFスコアの積です。\n",
    "   \n",
    "   - *式:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "✔️ ***N-grams* の説明**\n",
    "\n",
    "*N-grams* は、テキスト文書から抽出される連続した $ n $ 項目（トークン）のシーケンスです。これにより、個々の単語に比べて言語構造と文脈のより包括的な表現が得られます。\n",
    "\n",
    "*式:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*例:* `ngrams = (1, 3)` の場合、テキスト文書内の長さ3のスライディングウィンドウで可能な全てのトークンの組み合わせを考慮します。各3トークンの組み合わせはトライグラムを表します。\n",
    "\n",
    "例えば、「I love coding.」という文を考えます。\n",
    "\n",
    "`ngrams = (1, 3)`の場合、この文から抽出されるn-gramsには以下が含まれます：\n",
    "\n",
    "   * ユニグラム (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * バイグラム (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * トリグラム (3-grams): [\"I love coding\"]\n",
    "\n",
    "このように、$ N-grams $ は個々の単語だけでなく、フレーズや文中のコンテキスト情報も捉えます。\n",
    "\n",
    "---\n",
    "   \n",
    "✔️ **TF-IDFのステップ**\n",
    "\n",
    "1. トークン化:\n",
    "\n",
    "   - *定義:* テキストをトークンに分けます。\n",
    "   \n",
    "   - *例:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. 文書頻度の計算:\n",
    "\n",
    "   - *定義:* 各単語を含む文書の数をカウントします。\n",
    "   \n",
    "   - *例:* \"love\" は1つの文書に出現します。\n",
    "\n",
    "3. TF-IDFの計算:\n",
    "\n",
    "   - *定義:* 各文書内の各単語のTF-IDFスコアを計算します。\n",
    "   \n",
    "   - *例:* ngrams = (1, 3) の場合、\"love\"は文書1に出現し、そのTFとIDFに基づいてTF-IDFスコアが計算されます。\n",
    "\n",
    "4. ベクトル化:\n",
    "\n",
    "   - *定義:* 各文書をTF-IDFスコアのベクトルとして表現します。\n",
    "   \n",
    "   - *例:* 各文書は、高次元ベクトルになり、各次元はユニークな単語またはn-gramに対応します。\n",
    "\n",
    "# データ前処理 🛠️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87440df",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Plot average response word count per winner model\n",
    "eda.response_length(train_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class DataPreprocessing:\n",
    "    # 入力リスト内にNoneが存在するかチェック\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))  # 1つでもNoneがあれば1を返す\n",
    "\n",
    "    # 入力リスト内の文字列の合計長を計算\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):  # valが文字列であれば\n",
    "                length += len(val)  # その長さを加算\n",
    "        return length\n",
    "    \n",
    "    # 入力リスト内のユニークな単語のカウントを計算\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):  # valsが文字列の場合\n",
    "            return len(set(vals.split()))  # ユニークな単語の数をカウントして返す\n",
    "        return 0\n",
    "    \n",
    "    # リスト内の'None'を'STR'に置き換え、要素をスペースで結合\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):  # textがリストの場合\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])  # Noneを'STR'に置き換え\n",
    "            return cleaned_text\n",
    "\n",
    "        return text  # それ以外は元のtextを返す\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # 応答列の長さやNoneの有無に関連する特徴を追加\n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)  # 応答Aの長さ\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)  # 応答Bの長さ\n",
    "\n",
    "        # 応答のユニークな単語数を計算\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)  # 応答Aのユニーク単語数\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)  # 応答Bのユニーク単語数\n",
    "\n",
    "        # 長さの差、平均長さ、長さ差比を計算\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]  # 長さの差\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2  # 平均長さ\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]  # 長さ差比\n",
    "\n",
    "        # ユニーク単語数の差、平均、および比を計算\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]  # ユニーク単語数の差\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2  # ユニーク単語数の平均\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])  # ユニーク単語数差比\n",
    "\n",
    "        # 応答列内にNoneが含まれているかをチェック\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)  # 応答AにNoneがあるか\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)  # 応答BにNoneがあるか\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]  # Noneの差\n",
    "\n",
    "        return data  # 加工したデータを返す\n",
    "    \n",
    "    # プロンプトと応答間のコサイン類似度を計算\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # プロンプト（p）と応答A（a）のコサイン類似度\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # プロンプト（p）と応答B（b）のコサイン類似度\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb  # 類似度を返す\n",
    "\n",
    "    # プロンプトと応答間の距離（ユークリッド/ラプラシアン）を計算\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # プロンプト（p）と応答A（a）の距離\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # プロンプト（p）と応答B（b）の距離\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb  # 距離を返す\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # TF-IDFベクトルライザを初期化\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # トレーニングデータとテストデータを単一のデータフレームに結合\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # テキスト列をクリーンアップして準備\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # TF-IDFベクトル化のためにすべてのテキスト列を結合\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # TF-IDFマトリックスを計算\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # 次元削減をトランケイテッドSVDで実施\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # コーパスの異なる部分を分割するためのインデックスを計算\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # 短縮マトリックスをプロンプト、応答A、および応答B部分に分割\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # 短縮マトリックスをトレーニングセットとテストセットに分割\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # トレーニングおよびテストセットのSVD特徴を保持するためのデータフレームを作成\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # SVD特徴を特徴データフレームの対応する列に割り当て\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # 新しい特徴を元のトレーニングおよびテストデータフレームと連結\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # 類似度と距離の特徴を計算\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # コサイン類似度の特徴を計算\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ユークリッド距離の特徴を計算\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ラプラシアンカーネル距離の特徴を計算\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test  # 加工したトレーニングデータとテストデータを返す\n",
    "    \n",
    "    # 複数のラベルを単一のラベルにマージ\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0  # モデルAが勝者の場合\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1  # モデルBが勝者の場合\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2  # 引き分けの場合\n",
    "        raise ValueError(\"値が無効です。\")  # 無効な値の場合エラーを発生\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-17T18:16:09.085037Z",
     "iopub.status.busy": "2024-06-17T18:16:09.084595Z",
     "iopub.status.idle": "2024-06-17T18:16:09.127941Z",
     "shell.execute_reply": "2024-06-17T18:16:09.126578Z",
     "shell.execute_reply.started": "2024-06-17T18:16:09.085004Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataPreprocessing:\n",
    "    # 入力リスト内にNoneが存在するかチェック\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))  # 1つでもNoneがあれば1を返す\n",
    "\n",
    "    # 入力リスト内の文字列の合計長を計算\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):  # valが文字列であれば\n",
    "                length += len(val)  # その長さを加算\n",
    "        return length\n",
    "    \n",
    "    # 入力リスト内のユニークな単語のカウントを計算\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):  # valsが文字列の場合\n",
    "            return len(set(vals.split()))  # ユニークな単語の数をカウントして返す\n",
    "        return 0\n",
    "    \n",
    "    # リスト内の'None'を'STR'に置き換え、要素をスペースで結合\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):  # textがリストの場合\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])  # Noneを'STR'に置き換え\n",
    "            return cleaned_text\n",
    "\n",
    "        return text  # それ以外は元のtextを返す\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # 応答列の長さやNoneの有無に関連する特徴を追加\n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)  # 応答Aの長さ\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)  # 応答Bの長さ\n",
    "\n",
    "        # 応答のユニークな単語数を計算\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)  # 応答Aのユニーク単語数\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)  # 応答Bのユニーク単語数\n",
    "\n",
    "        # 長さの差、平均長さ、長さ差比を計算\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]  # 長さの差\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2  # 平均長さ\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]  # 長さ差比\n",
    "\n",
    "        # ユニーク単語数の差、平均、および比を計算\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]  # ユニーク単語数の差\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2  # ユニーク単語数の平均\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])  # ユニーク単語数差比\n",
    "\n",
    "        # 応答列内にNoneが含まれているかをチェック\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)  # 応答AにNoneがあるか\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)  # 応答BにNoneがあるか\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]  # Noneの差\n",
    "\n",
    "        return data  # 加工したデータを返す\n",
    "    \n",
    "    # プロンプトと応答間のコサイン類似度を計算\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # プロンプト（p）と応答A（a）のコサイン類似度\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # プロンプト（p）と応答B（b）のコサイン類似度\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb  # 類似度を返す\n",
    "\n",
    "    # プロンプトと応答間の距離（ユークリッド/ラプラシアン）を計算\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # プロンプト（p）と応答A（a）の距離\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # プロンプト（p）と応答B（b）の距離\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb  # 距離を返す\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # TF-IDFベクトルライザを初期化\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # トレーニングデータとテストデータを単一のデータフレームに結合\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # テキスト列をクリーンアップして準備\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # TF-IDFベクトル化のためにすべてのテキスト列を結合\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # TF-IDFマトリックスを計算\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # 次元削減をトランケイテッドSVDで実施\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # コーパスの異なる部分を分割するためのインデックスを計算\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # 短縮マトリックスをプロンプト、応答A、および応答B部分に分割\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # 短縮マトリックスをトレーニングセットとテストセットに分割\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # トレーニングおよびテストセットのSVD特徴を保持するためのデータフレームを作成\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # SVD特徴を特徴データフレームの対応する列に割り当て\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # 新しい特徴を元のトレーニングおよびテストデータフレームと連結\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # 類似度と距離の特徴を計算\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # コサイン類似度の特徴を計算\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ユークリッド距離の特徴を計算\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # ラプラシアンカーネル距離の特徴を計算\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test  # 加工したトレーニングデータとテストデータを返す\n",
    "    \n",
    "    # 複数のラベルを単一のラベルにマージ\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0  # モデルAが勝者の場合\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1  # モデルBが勝者の場合\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2  # 引き分けの場合\n",
    "        raise ValueError(\"値が無効です。\")  # 無効な値の場合エラーを発生"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9388c7b",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Theory 📒\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "dp = DataPreprocessing()  # データ前処理クラスのインスタンスを作成し、データの前処理を実行できるようにします。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14f58e0",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "✔️ **Term Frequency - Inverse Document Frequency** or **TF-IDF** vectorization is used in text mining and information retrieval to assess the importance of words in a document relative to a corpus. This technique transforms text data into a numerical format suitable for machine learning algorithms.\n",
    "\n",
    "---\n",
    "\n",
    "✔️ **Components of TF-IDF**\n",
    "\n",
    "1. Term Frequency (TF):\n",
    "\n",
    "   - *Definition:* Measures the frequency of a term in a document.\n",
    "   \n",
    "   - *Formula:* $ \\text{TF}(t,d) = \\frac{f_{t,d}}{\\sum\\limits_{t' \\in d} f_{t',d}} $ , where $ f_{t,d} $ is the frequency of term $ t $ in document $ d $.\n",
    "\n",
    "2. Inverse Document Frequency (IDF):\n",
    "\n",
    "   - *Definition:* Measures the importance of a term across the entire corpus.\n",
    "   \n",
    "   - *Formula:* $ \\text{IDF}(t) = \\log \\left( \\frac{N}{1 + n_t} \\right) $ , where $ N $ is the total number of documents, and $ n_t $ is the number of documents containing term $ t $.\n",
    "\n",
    "3. TF-IDF Score:\n",
    "\n",
    "   - *Definition:* Product of TF and IDF scores.\n",
    "   \n",
    "   - *Formula:* $ \\text{TF-IDF}(t,d) = \\text{TF}(t,d) \\times \\text{IDF}(t) $\n",
    "   \n",
    "---\n",
    "\n",
    "✔️ ***N-grams* explained**\n",
    "\n",
    "*N-grams* are contiguous sequences of $ n $ items (tokens) extracted from a text document. They provide a more comprehensive representation of the language structure and context compared to individual words.\n",
    "\n",
    "*Formula:* $ N\\text{-grams} = [t_1, t_2, ..., t_n] $\n",
    "\n",
    "*Example:* For `ngrams = (1, 3)`, it means we are considering all possible combinations of tokens within a sliding window of length 3 in the text document. Each combination of 3 tokens represents a trigram. \n",
    "\n",
    "For instance, consider the sentence: \"I love coding.\"\n",
    "\n",
    "With `ngrams = (1, 3)`, the n-grams extracted from this sentence would include:\n",
    "\n",
    "   * Unigrams (1-grams): [\"I\"], [\"love\"], [\"coding\"]\n",
    "    \n",
    "   * Bigrams (2-grams): [\"I love\"], [\"love coding\"]\n",
    "    \n",
    "   * Trigrams (3-grams): [\"I love coding\"]\n",
    "\n",
    "This way, $ N-grams $ capture not only individual words but also phrases and contextual information within the text.\n",
    "  \n",
    "---\n",
    "   \n",
    "✔️ **Steps of TF-IDF**\n",
    "\n",
    "1. Tokenization:\n",
    "\n",
    "   - *Definition:* Breaks text into tokens.\n",
    "   \n",
    "   - *Example:* \"I love coding\" -> [\"I\", \"love\", \"coding\"]\n",
    "\n",
    "2. Document Frequency Calculation:\n",
    "\n",
    "   - *Definition:* Counts the number of documents containing each term.\n",
    "   \n",
    "   - *Example:* \"love\" appears in 1 document out of 1.\n",
    "\n",
    "3. TF-IDF Calculation:\n",
    "\n",
    "   - *Definition:* Computes the TF-IDF score for each term in each document.\n",
    "   \n",
    "   - *Example:* For ngrams = (1, 3), \"love\" appears in Document 1, the TF-IDF score for \"love\" would be calculated based on its TF and IDF.\n",
    "\n",
    "4. Vectorization:\n",
    "\n",
    "   - *Definition:* Represents each document as a vector of TF-IDF scores.\n",
    "   \n",
    "   - *Example:* Each document becomes a high-dimensional vector where each dimension corresponds to a unique term or n-gram.\n",
    "\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# 長さ、類似度、および距離の特徴を追加する\n",
    "train_data = dp.add_features(train_data)  # トレーニングデータに特徴を追加\n",
    "test_data = dp.add_features(test_data)    # テストデータに特徴を追加\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82e1c7f",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Data Preprocessing 🛠️\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# TF-IDF特徴を抽出し、次元削減を実施する\n",
    "train_data, test_data = dp.create_tfidf_features(train_data,  # トレーニングデータを使用\n",
    "                                                 test_data,   # テストデータを使用\n",
    "                                                 CFG.ngrams,  # n-gramの設定\n",
    "                                                 CFG.min_freq,  # 最小頻度の設定\n",
    "                                                 CFG.max_freq,  # 最大頻度の設定\n",
    "                                                 CFG.components)  # コンポーネントの数の設定\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b238f8",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class DataPreprocessing:\n",
    "    # Check if any value in the input list is None\n",
    "    @staticmethod\n",
    "    def retrieve_none(vals):\n",
    "        return int(any(val is None for val in vals))\n",
    "\n",
    "    # Calculate the total length of strings in the input list\n",
    "    @staticmethod\n",
    "    def retrieve_length(vals):\n",
    "        length = 0\n",
    "        for val in vals:\n",
    "            if isinstance(val, str):\n",
    "                length += len(val)\n",
    "        return length\n",
    "    \n",
    "    # Calculate the count of unique works in the input list\n",
    "    @staticmethod\n",
    "    def retrieve_nuniques(vals):\n",
    "        if isinstance(vals, str):\n",
    "            return len(set(vals.split()))\n",
    "        return 0\n",
    "    \n",
    "    # Replace 'None' in the list with the string 'NONE', and join elements with a space\n",
    "    @staticmethod\n",
    "    def clean_response(text):\n",
    "        if isinstance(text, list):\n",
    "            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])\n",
    "            return cleaned_text\n",
    "\n",
    "        return text\n",
    "\n",
    "    def add_features(self, data):\n",
    "        # Add features related to the length and presence of None values in response columns.\n",
    "        data[f\"response_a_len\"] = data[f\"response_a\"].apply(self.retrieve_length)\n",
    "        data[f\"response_b_len\"] = data[f\"response_b\"].apply(self.retrieve_length)\n",
    "\n",
    "        # Calculate unique word count for responses\n",
    "        data[f\"response_a_unique\"] = data[f\"response_a\"].apply(self.retrieve_nuniques)\n",
    "        data[f\"response_b_unique\"] = data[f\"response_b\"].apply(self.retrieve_nuniques)\n",
    "\n",
    "        # Calculate length difference, mean length, and length difference ratio.\n",
    "        data[\"response_len_diff\"] = data[\"response_a_len\"] - data[\"response_b_len\"]\n",
    "        data[\"response_len_mean\"] = (data[\"response_a_len\"] + data[\"response_b_len\"]) / 2\n",
    "        data[\"response_diff_ratio\"] = data[\"response_len_diff\"] / data[\"response_len_mean\"]\n",
    "\n",
    "        # Calculate unique word count difference, mean, and ratio.\n",
    "        data[\"response_unique_diff\"] = data[\"response_a_unique\"] - data[\"response_b_unique\"]\n",
    "        data[\"response_unique_mean\"] = (data[\"response_a_unique\"] + \n",
    "                                        data[\"response_b_unique\"]) / 2\n",
    "        data[\"response_unique_ratio\"] = (data[\"response_unique_diff\"] / \n",
    "                                         data[\"response_unique_mean\"])\n",
    "\n",
    "        # Check if any value in response columns is None.\n",
    "        data[\"a_has_none\"] = data[\"response_a\"].apply(self.retrieve_none)\n",
    "        data[\"b_has_none\"] = data[\"response_b\"].apply(self.retrieve_none)\n",
    "        data[\"has_none_diff\"] = data[\"a_has_none\"] - data[\"b_has_none\"]\n",
    "\n",
    "        return data\n",
    "    \n",
    "    # Calculate cosine similarity between prompt and responses\n",
    "    @staticmethod\n",
    "    def calculate_cosine_similarity(tfidf_matrix, \n",
    "                                    prompt_idx, \n",
    "                                    response_a_idx, \n",
    "                                    response_b_idx):\n",
    "        \n",
    "        # Cosine similarity between prompt (p) and response_a (a)\n",
    "        similarity_pa = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        # Cosine similarity between prompt (p) and response_b (b)\n",
    "        similarity_pb = cosine_similarity(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "\n",
    "        return similarity_pa, similarity_pb\n",
    "\n",
    "    # Calculate distances (Euclidean/Laplacian) between prompt and responses\n",
    "    @staticmethod\n",
    "    def calculate_distances(tfidf_matrix, \n",
    "                            prompt_idx, \n",
    "                            response_a_idx, \n",
    "                            response_b_idx, \n",
    "                            distance_metric):\n",
    "        \n",
    "        # Distance between prompt (p) and response_a (a)\n",
    "        distance_pa = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1), \n",
    "                tfidf_matrix[response_a_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        # Distance between prompt (p) and response_b (b)\n",
    "        distance_pb = distance_metric(\n",
    "                tfidf_matrix[prompt_idx].reshape(1, -1),\n",
    "                tfidf_matrix[response_b_idx].reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return distance_pa, distance_pb\n",
    "\n",
    "    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):\n",
    "        # Initialize TF-IDF Vectorizer\n",
    "        tfidf_vectorizer = TfidfVectorizer(analyzer='char', \n",
    "                                           ngram_range=ngrams, \n",
    "                                           min_df=min_freq, \n",
    "                                           max_df=max_freq,\n",
    "                                           lowercase=False,\n",
    "                                           sublinear_tf=True)\n",
    "\n",
    "        # Combine train and test data into a single DataFrame\n",
    "        full_data = pd.concat([train, test], ignore_index=True)\n",
    "\n",
    "        # Clean and prepare the text columns\n",
    "        for col in ['prompt', 'response_a', 'response_b']:\n",
    "            full_data[col] = full_data[col].apply(self.clean_response)\n",
    "\n",
    "        # Combine all text columns into a single corpus for TF-IDF vectorization\n",
    "        full_corpus = pd.concat([full_data['prompt'], \n",
    "                                 full_data['response_a'], \n",
    "                                 full_data['response_b']], \n",
    "                                 ignore_index=True)\n",
    "\n",
    "        # Compute the TF-IDF matrix\n",
    "        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)\n",
    "\n",
    "        # Perform dimensionality reduction with TruncatedSVD\n",
    "        svd = TruncatedSVD(n_components=components, random_state=42)\n",
    "        reduced_matrix = svd.fit_transform(tfidf_matrix)\n",
    "\n",
    "        # Calculate split indices for separating different parts of the corpus\n",
    "        len_full = len(full_data)\n",
    "        split_index_01 = len_full\n",
    "        split_index_02 = len_full * 2\n",
    "\n",
    "        # Split the reduced matrix into prompts, response_a, and response_b parts\n",
    "        full_tfidf_prompts = reduced_matrix[:split_index_01]\n",
    "        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]\n",
    "        full_tfidf_response_b = reduced_matrix[split_index_02:]\n",
    "\n",
    "        # Separate the reduced matrix into training and testing sets\n",
    "        len_train = len(train)\n",
    "        train_tfidf_prompts = full_tfidf_prompts[:len_train]\n",
    "        train_tfidf_response_a = full_tfidf_response_a[:len_train]\n",
    "        train_tfidf_response_b = full_tfidf_response_b[:len_train]\n",
    "        test_tfidf_prompts = full_tfidf_prompts[len_train:]\n",
    "        test_tfidf_response_a = full_tfidf_response_a[len_train:]\n",
    "        test_tfidf_response_b = full_tfidf_response_b[len_train:]\n",
    "\n",
    "        # Create DataFrames to hold the SVD features for train and test sets\n",
    "        feature_names = [f'svd_feature_{i}' for i in range(components)]\n",
    "        train_features = pd.DataFrame(index=train.index)\n",
    "        test_features = pd.DataFrame(index=test.index)\n",
    "\n",
    "        # Assign SVD features to the respective columns in the feature DataFrames\n",
    "        for i in range(components):\n",
    "            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]\n",
    "            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]\n",
    "            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]\n",
    "            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]\n",
    "            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]\n",
    "            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]\n",
    "\n",
    "        # Concatenate the new features with the original train and test DataFrames\n",
    "        train = pd.concat([train, train_features], axis=1)\n",
    "        test = pd.concat([test, test_features], axis=1)\n",
    "\n",
    "        # Calculate similarity and distance features\n",
    "        for df, len_df in zip([train, test], [len(train), len(test)]):\n",
    "            prompt_indices = df.index\n",
    "\n",
    "            # Calculate cosine similarity features\n",
    "            df['similarity_pa'], df['similarity_pb'] = zip(*[\n",
    "                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # Calculate Euclidean distance features\n",
    "            df['euclidean_pa'], df['euclidean_pb'] = zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         euclidean_distances)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "            # Calculate Laplacian kernel distance features\n",
    "            df['laplacian_pa'], df['laplacian_pb']= zip(*[\n",
    "                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, \n",
    "                                         laplacian_kernel)\n",
    "                for i in prompt_indices\n",
    "            ])\n",
    "\n",
    "        return train, test\n",
    "    \n",
    "    # Merges multiple labels into a single label\n",
    "    def merge_label(self, row):\n",
    "        if row[\"winner_model_a\"] == 1:\n",
    "            return 0\n",
    "        if row[\"winner_model_b\"] == 1:\n",
    "            return 1\n",
    "        if row[\"winner_tie\"] == 1:\n",
    "            return 2\n",
    "        raise ValueError(\"The value is invalid.\")\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 複数のラベルを単一のラベルにマージする\n",
    "train_data[\"target\"] = train_data[  # ターゲット列を作成\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "].apply(lambda x: dp.merge_label(x), axis=1)  # 各行にmerge_label関数を適用してターゲットを設定\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 複数のラベルを単一のラベルにマージする\n",
    "train_data[\"target\"] = train_data[  # ターゲット列を作成\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "].apply(lambda x: dp.merge_label(x), axis=1)  # 各行にmerge_label関数を適用してターゲットを設定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67490f0a",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "dp = DataPreprocessing()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# モデル開発 🧠\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dac940c",
   "metadata": {},
   "source": [
    "# モデル開発 🧠"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e743eff",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Add length, similarity and distance features\n",
    "train_data = dp.add_features(train_data)\n",
    "test_data = dp.add_features(test_data)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # トレーニングデータとテストデータから特徴量とターゲットラベルを抽出\n",
    "        X_train = train_data[feature_cols].values  # 特徴量の行列\n",
    "        X_test = test_data[feature_cols].values    # テスト用特徴量\n",
    "        Y_train = train_data[\"target\"]              # ターゲットラベル\n",
    "\n",
    "        # 予測を保存するリスト\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # StratifiedKFoldを初期化\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # 現在のフォールドのためにトレーニングデータをトレーニングセットとバリデーションセットに分割\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # トレーニングとバリデーションのためのLightGBMデータセットオブジェクトを作成\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # 現在のフォールドでモデルを訓練\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # トレーニングセットとテストセットに対して予測を行う\n",
    "            train_preds = model.predict(X_train)  # トレーニングデータの予測\n",
    "            test_preds = model.predict(X_test)    # テストデータの予測\n",
    "\n",
    "            train_preds_list.append(train_preds)  # トレーニング予測をリストに追加\n",
    "            test_preds_list.append(test_preds)    # テスト予測をリストに追加\n",
    "\n",
    "        # 予測の平均を計算\n",
    "        train_preds = np.mean(train_preds_list, axis=0)  # トレーニング予測の平均\n",
    "        test_preds = np.mean(test_preds_list, axis=0)    # テスト予測の平均\n",
    "\n",
    "        return train_preds, test_preds  # 予測を返す\n",
    "    \n",
    "    # トレーニングデータ予測用の混同行列をプロット\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)  # 混同行列を計算\n",
    "\n",
    "        # カスタムホバーテキストフォーマッタを作成\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # 整数値に変換してカンマなしで返す\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # ヒートマップを作成\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"真実: %{y}<br>予測: %{x}<br>カウント: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # 背景を透明にし、正方形のアスペクト比に設定するためにレイアウトを更新\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"予測ラベル\",\n",
    "            yaxis_title=\"真実のラベル\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # プロットを表示\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # トレーニングデータとテストデータから特徴量とターゲットラベルを抽出\n",
    "        X_train = train_data[feature_cols].values  # 特徴量の行列\n",
    "        X_test = test_data[feature_cols].values    # テスト用特徴量\n",
    "        Y_train = train_data[\"target\"]              # ターゲットラベル\n",
    "\n",
    "        # 予測を保存するリスト\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # StratifiedKFoldを初期化\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # 現在のフォールドのためにトレーニングデータをトレーニングセットとバリデーションセットに分割\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # トレーニングとバリデーションのためのLightGBMデータセットオブジェクトを作成\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # 現在のフォールドでモデルを訓練\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # トレーニングセットとテストセットに対して予測を行う\n",
    "            train_preds = model.predict(X_train)  # トレーニングデータの予測\n",
    "            test_preds = model.predict(X_test)    # テストデータの予測\n",
    "\n",
    "            train_preds_list.append(train_preds)  # トレーニング予測をリストに追加\n",
    "            test_preds_list.append(test_preds)    # テスト予測をリストに追加\n",
    "\n",
    "        # 予測の平均を計算\n",
    "        train_preds = np.mean(train_preds_list, axis=0)  # トレーニング予測の平均\n",
    "        test_preds = np.mean(test_preds_list, axis=0)    # テスト予測の平均\n",
    "\n",
    "        return train_preds, test_preds  # 予測を返す\n",
    "    \n",
    "    # トレーニングデータ予測用の混同行列をプロット\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)  # 混同行列を計算\n",
    "\n",
    "        # カスタムホバーテキストフォーマッタを作成\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # 整数値に変換してカンマなしで返す\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # ヒートマップを作成\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"真実: %{y}<br>予測: %{x}<br>カウント: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # 背景を透明にし、正方形のアスペクト比に設定するためにレイアウトを更新\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"予測ラベル\",\n",
    "            yaxis_title=\"真実のラベル\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # プロットを表示\n",
    "        fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d4b9d3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Extract TF-IDF features and perform dimensionality reduction\n",
    "train_data, test_data = dp.create_tfidf_features(train_data, \n",
    "                                                 test_data, \n",
    "                                                 CFG.ngrams,\n",
    "                                                 CFG.min_freq, \n",
    "                                                 CFG.max_freq, \n",
    "                                                 CFG.components)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "md = ModelDevelopment()  # モデル開発クラスのインスタンスを作成し、モデルの訓練や評価を実行できるようにします。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = ModelDevelopment()  # モデル開発クラスのインスタンスを作成し、モデルの訓練や評価を実行できるようにします。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c255520",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Merge multiple labels into a single label\n",
    "train_data[\"target\"] = train_data[\n",
    "    [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "                                 ].apply(lambda x: dp.merge_label(x), axis=1)\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# ラベル列を定義\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# トレーニングデータから除外する特徴量のリストを定義\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "# 除外リストに含まれない列を特徴量としてリスト化\n",
    "features = [col for col in train_data.columns if col not in excluded_features]  # 使用する特徴量のリストを作成\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ラベル列を定義\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# トレーニングデータから除外する特徴量のリストを定義\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "# 除外リストに含まれない列を特徴量としてリスト化\n",
    "features = [col for col in train_data.columns if col not in excluded_features]  # 使用する特徴量のリストを作成"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad66b2c0",
   "metadata": {},
   "source": [
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "# Model Development 🧠\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "# LightGBMを訓練する\n",
    "train_preds, test_preds = md.train_lgb(  # トレーニングデータとテストデータを用いてLightGBMを訓練\n",
    "    train_data, \n",
    "    test_data, \n",
    "    features,  # 使用する特徴量\n",
    "    CFG.params,  # モデルパラメータ\n",
    "    CFG.early_stop,  # 早期終了のための基準\n",
    "    CFG.log_steps  # ログステップの設定\n",
    ")\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6714e556",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "class ModelDevelopment:\n",
    "    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):\n",
    "        # Extract feature values and target labels from the training and testing data\n",
    "        X_train = train_data[feature_cols].values\n",
    "        X_test = test_data[feature_cols].values\n",
    "        Y_train = train_data[\"target\"]\n",
    "\n",
    "        # List to store predictions\n",
    "        train_preds_list = []\n",
    "        test_preds_list = []\n",
    "\n",
    "        # Initialize StratifiedKFold\n",
    "        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)\n",
    "        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):\n",
    "            # Split the training data into training and validation sets for the current fold\n",
    "            x_train, x_valid = X_train[train_index], X_train[valid_index]\n",
    "            y_train, y_valid = Y_train[train_index], Y_train[valid_index]\n",
    "\n",
    "            # Create LightGBM dataset objects for training and validation\n",
    "            train = lgb.Dataset(x_train, y_train)\n",
    "            valid = lgb.Dataset(x_valid, y_valid, reference=train)\n",
    "\n",
    "            # Train the model on the current fold\n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train,\n",
    "                valid_sets=[train, valid],\n",
    "                feature_name=feature_cols,\n",
    "                callbacks=[lgb.early_stopping(early_stop),\n",
    "                           lgb.log_evaluation(log_steps)])\n",
    "\n",
    "            # Make predictions on the train and test sets\n",
    "            train_preds = model.predict(X_train)\n",
    "            test_preds = model.predict(X_test)\n",
    "\n",
    "            train_preds_list.append(train_preds)\n",
    "            test_preds_list.append(test_preds)\n",
    "\n",
    "        # Average predictions\n",
    "        train_preds = np.mean(train_preds_list, axis=0)\n",
    "        test_preds = np.mean(test_preds_list, axis=0)\n",
    "\n",
    "        return train_preds, test_preds\n",
    "    \n",
    "    # Confusion matrix for train data predictions\n",
    "    def plot_cm(self, y_true, y_pred, labels, colorscale):\n",
    "        cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "        # Create a custom hover text formatter\n",
    "        def format_hover_text(value):\n",
    "            if value >= 10000:\n",
    "                return str(int(value))  # Convert to integer without commas or \"k\"\n",
    "            else:\n",
    "                return str(value)\n",
    "\n",
    "        # Create the heatmap\n",
    "        fig = go.Figure(data=go.Heatmap(\n",
    "            z=cm,\n",
    "            x=labels,\n",
    "            y=labels,\n",
    "            colorscale=colorscale,\n",
    "            zmin=0,\n",
    "            zmax=20000,\n",
    "            text=cm,\n",
    "            texttemplate=\"%{text:.0f}\",\n",
    "            hovertemplate=\"True: %{y}<br>Predicted: %{x}<br>Count: %{z:,.0f}<extra></extra>\",\n",
    "            customdata=[format_hover_text(value) for value in cm.flatten()]\n",
    "        ))\n",
    "\n",
    "        # Update layout for a transparent background and square aspect ratio\n",
    "        fig.update_layout(\n",
    "            plot_bgcolor='rgba(0,0,0,0)',\n",
    "            paper_bgcolor='rgba(0,0,0,0)',\n",
    "            xaxis_title=\"Predicted Labels\",\n",
    "            yaxis_title=\"True Labels\",\n",
    "            xaxis=dict(constrain='domain'),\n",
    "            yaxis=dict(constrain='domain', scaleanchor='x'),\n",
    "            width=650,  \n",
    "            height=650,  \n",
    "            margin=dict(t=65, b=65, l=65, r=65) \n",
    "        )\n",
    "\n",
    "        # Show the plot\n",
    "        fig.show()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# トレーニングデータに対する（平均）予測の混同行列\n",
    "md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)  # ターゲットと予測ラベルを用いて混同行列をプロット\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# トレーニングデータに対する（平均）予測の混同行列\n",
    "md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)  # ターゲットと予測ラベルを用いて混同行列をプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0b0317",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "md = ModelDevelopment()\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 予測を提出する 💡\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e484c33",
   "metadata": {},
   "source": [
    "# 予測を提出する 💡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8c755",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "# Define label columns\n",
    "label_cols = [\"winner_model_a\", \"winner_model_b\", \"winner_tie\"]\n",
    "\n",
    "# Define the list of features to exclude from the training data\n",
    "excluded_features = ['id', \n",
    "                     'model_a', \n",
    "                     'model_b', \n",
    "                     'prompt', \n",
    "                     'response_a', \n",
    "                     'response_b',\n",
    "                     'winner_model_a', \n",
    "                     'winner_model_b', \n",
    "                     'winner_tie', \n",
    "                     'target', \n",
    "                     'fold_id']\n",
    "\n",
    "features = [col for col in train_data.columns if col not in excluded_features]\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 提出用データフレームに予測したテストラベルを割り当て\n",
    "subm_data[label_cols] = test_preds  # テストデータの予測結果を提出データフレームに追加\n",
    "\n",
    "# 提出データフレームを保存し、最初の3行を表示\n",
    "subm_data.to_csv(\"submission.csv\", index=False)  # 提出ファイルをCSV形式で保存\n",
    "display(subm_data.head(3))  # 提出データフレームの最初の3行を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 提出用データフレームに予測したテストラベルを割り当て\n",
    "subm_data[label_cols] = test_preds  # テストデータの予測結果を提出データフレームに追加\n",
    "\n",
    "# 提出データフレームを保存し、最初の3行を表示\n",
    "subm_data.to_csv(\"submission.csv\", index=False)  # 提出ファイルをCSV形式で保存\n",
    "display(subm_data.head(3))  # 提出データフレームの最初の3行を表示"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4959805,
     "sourceId": 8377405,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30715,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2593.627859,
   "end_time": "2024-06-06T20:41:57.399746",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-06-06T19:58:43.771887",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
