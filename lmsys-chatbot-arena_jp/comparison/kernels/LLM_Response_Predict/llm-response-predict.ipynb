{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741afcd6",
   "metadata": {},
   "source": [
    "# 要約 \n",
    "このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおいて、ユーザーが生成した応答のどちらが好まれるかを予測するための機械学習モデルを構築することに取り組んでいます。具体的には、Siamese LSTM（Long Short-Term Memory）ネットワークを使用して、二つのチャットボットの応答を比較し、どちらのモデルが好まれるかを分類するタスクを行います。\n",
    "\n",
    "### 問題の概要\n",
    "- 目標は、ユーザーがどちらの応答（モデルAまたはモデルB）を選ぶか、または引き分けかを予測することです。この目的のために、提供されたトレーニングデータセットを使用して、3種類の出力（モデルAの勝利、モデルBの勝利、引き分け）を分類します。\n",
    "\n",
    "### 使用されている手法とライブラリ\n",
    "- **トランスフォーマーモデル**: `transformers`ライブラリを使用してBERTトークナイザーを利用しています。このトークナイザーは、テキストをトークン化してモデルに入力する際の前処理を行います。\n",
    "- **データセットの準備**: `datasets`ライブラリを使用して、トレーニングデータセットとテストデータセットの読み込みと処理が行われます。\n",
    "- **Siamese LSTMモデル**: PyTorchを用いてSiameseネットワークを実装し、二つの応答をLSTMで処理し、最終的に全結合層でクラスを予測します。モデルの構成には、埋め込み層、LSTM層、および出力層が含まれます。\n",
    "- **データローダー**: DataLoaderを使用して、バッチ処理を行い、トレーニングデータとテストデータを効率的に処理します。\n",
    "- **損失関数とオプティマイザー**: モデルの学習にはクロスエントロピー損失を用い、Adamオプティマイザーでパラメータを最適化します。\n",
    "\n",
    "### トレーニングプロセス\n",
    "- モデルは指定されたエポック数でトレーニングされ、各エポックで損失が計算され、学習が進む様子が表示されます。\n",
    "- トレーニングの後、テストデータに対して推論を行い、各応答の勝者の確率を予測します。\n",
    "\n",
    "### 結果の保存\n",
    "- 最終的に、テストデータに対する予測結果をCSV形式で保存し、提出フォーマットに従った結果を生成します。予測結果には、各応答に対する確率（モデルAが勝つ、モデルBが勝つ、引き分け）が含まれています。\n",
    "\n",
    "このNotebookは、機械学習モデルの設計、実装、トレーニング、評価、及び提出のための一連の作業を示しています。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a1b7d5",
   "metadata": {},
   "source": [
    "# 用語概説 \n",
    "以下に、Jupyter Notebook内での機械学習・深層学習特有の用語や概念について、初心者がつまずきそうなものの簡単な解説を列挙します。 \n",
    "\n",
    "1. **トランスフォーマー (Transformer)**:\n",
    "   トランスフォーマーは、自然言語処理で用いられる深層学習モデルの一種で、自己注意機構を使用して文脈を捉えます。このアーキテクチャは、従来のリカレントネットワーク（RNN）よりも並列処理が可能で、より大きなデータセットに対応しやすいです。\n",
    "\n",
    "2. **埋め込み層 (Embedding Layer)**:\n",
    "   埋め込み層は、入力データを密なベクトル形式に変換する層です。例えば、単語をベクトル空間にマッピングすることで、意味的に近い単語が互いに近い距離に配置されるようにします。\n",
    "\n",
    "3. **LSTM (Long Short-Term Memory)**:\n",
    "   LSTMは、長期依存性を持つデータを処理するためのリカレントニューラルネットワークの一種です。通常のRNNが抱える勾配消失問題を解決するために、忘却ゲート、入力ゲート、出力ゲートを持っています。\n",
    "\n",
    "4. **Siameseネットワーク (Siamese Network)**:\n",
    "   Siameseネットワークは、2つの同じ構造のサブネットワークを持ち、入力データのペアを同時に処理することで、類似性を比較するために設計されています。この手法は、主に類似度の評価や分類タスクに使われます。\n",
    "\n",
    "5. **アテンションマスク (Attention Mask)**:\n",
    "   アテンションマスクは、トランスフォーマーモデルにおいて、どのトークンに注意を向けるかを示すために用いられます。例えば、パディングされたトークンは無視するようにマスクします。\n",
    "\n",
    "6. **クロスエントロピー損失 (Cross Entropy Loss)**:\n",
    "   クロスエントロピー損失は、分類問題においてよく使用される損失関数で、モデルの予測した確率分布と実際のラベルとの間の不一致を評価します。この値が小さいほど、モデルの予測が正確であることを示します。\n",
    "\n",
    "7. **ソフトマックス関数 (Softmax Function)**:\n",
    "   ソフトマックス関数は、多クラス分類問題の出力層で用いられる関数で、入力された実数のベクトルを確率分布に変換します。これにより、出力の和が1になるように調整されます。\n",
    "\n",
    "8. **バックプロパゲーション (Backpropagation)**:\n",
    "   バックプロパゲーションは、ニューラルネットワークを訓練するためのアルゴリズムで、ネットワークの出力から誤差を逆伝播させて各重みの更新に必要な勾配を計算します。\n",
    "\n",
    "9. **データローダー (DataLoader)**:\n",
    "   データローダーは、データセットからバッチを取り出して訓練中にモデルに入力するための便利なクラスです。データのシャッフルや並列化をサポートする機能を持っています。\n",
    "\n",
    "10. **トークナイザー (Tokenizer)**:\n",
    "    トークナイザーは、テキストデータをトークン（単語やサブワード）に分割し、それぞれに整数IDを割り当てるツールです。この過程は、自然言語処理の前処理の重要なステップです。\n",
    "\n",
    "これらの用語や概念は、初心者が深層学習や機械学習のノートブックを理解する上で重要なポイントとなります。特に、これらの技術がどのように結びついているかを把握することは、実務での応用にも役立つでしょう。\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f017b3",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 特にコードが記載されていないため、翻訳する内容がありません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特にコードが記載されていないため、翻訳する内容がありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8382e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# transformersとdatasetsライブラリをインストールします。\n",
    "# transformersライブラリは、トランスフォーマーベースのモデルの使用を容易にします。\n",
    "# datasetsライブラリは、さまざまなデータセットを簡単にダウンロードして利用できるようにします。\n",
    "pip install transformers datasets\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T13:40:44.464908Z",
     "iopub.status.busy": "2024-07-18T13:40:44.463973Z",
     "iopub.status.idle": "2024-07-18T13:41:20.741215Z",
     "shell.execute_reply": "2024-07-18T13:41:20.739757Z",
     "shell.execute_reply.started": "2024-07-18T13:40:44.464841Z"
    }
   },
   "outputs": [],
   "source": [
    "# transformersとdatasetsライブラリをインストールします。\n",
    "# transformersライブラリは、トランスフォーマーベースのモデルの使用を容易にします。\n",
    "# datasetsライブラリは、さまざまなデータセットを簡単にダウンロードして利用できるようにします。\n",
    "pip install transformers datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff98186",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# Define the SiameseLSTM class\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # Output size 3 for 3 classes: model A wins, model B wins, tie\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.embedding(x)\n",
    "        _, (h, _) = self.lstm(x)\n",
    "        return h[-1]\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        h1 = self.forward_one(x1)\n",
    "        h2 = self.forward_one(x2)\n",
    "        return self.fc(torch.abs(h1 - h2))\n",
    "\n",
    "# Step 1: Load your training dataset\n",
    "df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')\n",
    "\n",
    "# Filter out invalid cases and prepare data\n",
    "data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
    "\n",
    "def determine_label(row):\n",
    "    if row[2] == 1:\n",
    "        return 0  # model A wins\n",
    "    elif row[3] == 1:\n",
    "        return 1  # model B wins\n",
    "    elif row[4] == 1:\n",
    "        return 2  # tie\n",
    "    else:\n",
    "        return -1  # Invalid or unclear case\n",
    "\n",
    "labels = [determine_label(row) for row in data if determine_label(row) != -1]\n",
    "data = [row[:2] for row in data if determine_label(row) != -1]\n",
    "\n",
    "# Step 2: Define a custom Dataset class\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Step 3: Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')\n",
    "max_length = 128  # Adjust according to your dataset\n",
    "\n",
    "# Step 4: Create instances of Dataset and DataLoader for training\n",
    "train_dataset = SiameseDataset(data, labels, tokenizer, max_length)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 5: Define the Siamese network model\n",
    "input_size = len(tokenizer)\n",
    "hidden_size = 300\n",
    "num_layers = 1\n",
    "model = SiameseLSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "# Step 6: Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 7: Training loop\n",
    "num_epochs = 5  # Adjust as needed\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids_a, input_ids_b)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')\n",
    "\n",
    "# Load your test dataset\n",
    "df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')\n",
    "\n",
    "# Define a custom Dataset class for testing\n",
    "class SiameseTestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),\n",
    "        }\n",
    "\n",
    "# Prepare test data\n",
    "test_data = df_test[['response_a', 'response_b']].values.tolist()\n",
    "\n",
    "# Create instance of SiameseTestDataset\n",
    "test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "# Create DataLoader for the test dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load the saved model\n",
    "model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Perform inference on the test data and generate predictions\n",
    "all_preds = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "\n",
    "        outputs = model(input_ids_a, input_ids_b)\n",
    "        probabilities = nn.Softmax(dim=1)(outputs)\n",
    "        all_preds.extend(probabilities.cpu().numpy().tolist())\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "pred_df['id'] = df_test['id']\n",
    "\n",
    "# Reorder columns to match the required format\n",
    "pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "\n",
    "# Save predictions to CSV for submission\n",
    "pred_df.to_csv('submission.csv', index=False)\n",
    "print(pred_df.head())\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# SiameseLSTMクラスを定義します\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # 入力層の埋め込み層\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)  # LSTM層\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # 出力層：3クラス（モデルAが勝つ、モデルBが勝つ、引き分け）\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.embedding(x)  # 入力を埋め込みに変換\n",
    "        _, (h, _) = self.lstm(x)  # LSTMを通して隠れ状態を取得\n",
    "        return h[-1]  # 最後の隠れ状態を返す\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        h1 = self.forward_one(x1)  # 最初の入力の出力を取得\n",
    "        h2 = self.forward_one(x2)  # 二つ目の入力の出力を取得\n",
    "        return self.fc(torch.abs(h1 - h2))  # 二つの隠れ状態の絶対差を計算し、全結合層に渡す\n",
    "\n",
    "# ステップ1：トレーニングデータセットを読み込む\n",
    "df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')\n",
    "\n",
    "# 無効なケースをフィルタリングし、データを準備する\n",
    "data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
    "\n",
    "def determine_label(row):\n",
    "    if row[2] == 1:\n",
    "        return 0  # モデルAが勝つ\n",
    "    elif row[3] == 1:\n",
    "        return 1  # モデルBが勝つ\n",
    "    elif row[4] == 1:\n",
    "        return 2  # 引き分け\n",
    "    else:\n",
    "        return -1  # 無効または不明なケース\n",
    "\n",
    "labels = [determine_label(row) for row in data if determine_label(row) != -1]  # ラベルをリストに格納\n",
    "data = [row[:2] for row in data if determine_label(row) != -1]  # 有効なデータペアを取得\n",
    "\n",
    "# ステップ2：カスタムDatasetクラスを定義します\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # データの長さを返す\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # トークナイザーを使用してテキストをトークン化\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク\n",
    "            'label': torch.tensor(label, dtype=torch.long)  # ラベル\n",
    "        }\n",
    "\n",
    "# ステップ3：BERTトークナイザーを初期化します\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')\n",
    "max_length = 128  # データセットに応じて調整\n",
    "\n",
    "# ステップ4：トレーニング用のDatasetとDataLoaderのインスタンスを作成します\n",
    "train_dataset = SiameseDataset(data, labels, tokenizer, max_length)\n",
    "batch_size = 32  # バッチサイズを32に設定\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # データローダーを作成\n",
    "\n",
    "# ステップ5：Siameseネットワークモデルを定義します\n",
    "input_size = len(tokenizer)  # トークナイザーのサイズを入力サイズに設定\n",
    "hidden_size = 300  # 隠れ層のサイズを300に設定\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "model = SiameseLSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "# ステップ6：損失関数とオプティマイザーを定義します\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adamオプティマイザー\n",
    "\n",
    "# ステップ7：トレーニングループ\n",
    "num_epochs = 5  # 必要に応じて調整\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればGPUを使用\n",
    "model.to(device)  # モデルをデバイスに転送\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        loss.backward()  # バックプロパゲーションで勾配を計算\n",
    "        optimizer.step()  # オプティマイザーで重みを更新\n",
    "\n",
    "        total_loss += loss.item()  # 累積損失を加算\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')  # エポックごとの損失を表示\n",
    "\n",
    "# テストデータセットを読み込む\n",
    "df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')\n",
    "\n",
    "# テスト用のカスタムDatasetクラスを定義します\n",
    "class SiameseTestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # データの長さを返す\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "\n",
    "        # トークナイザーを使用してテキストをトークン化\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク\n",
    "        }\n",
    "\n",
    "# テストデータを準備する\n",
    "test_data = df_test[['response_a', 'response_b']].values.tolist()\n",
    "\n",
    "# SiameseTestDatasetのインスタンスを作成\n",
    "test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "# テストデータセットのデータローダーを作成\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 保存したモデルを読み込む\n",
    "model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))\n",
    "model.eval()  # モデルを評価モードに設定\n",
    "\n",
    "# テストデータに対して推論を行い、予測を生成する\n",
    "all_preds = []\n",
    "with torch.no_grad():  # 勾配計算を無効にする\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "\n",
    "        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得\n",
    "        probabilities = nn.Softmax(dim=1)(outputs)  # ソフトマックス関数を適用して確率を計算\n",
    "        all_preds.extend(probabilities.cpu().numpy().tolist())  # 確率をリストに追加\n",
    "\n",
    "# 予測結果のDataFrameを作成\n",
    "pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "pred_df['id'] = df_test['id']  # テストデータのIDを追加\n",
    "\n",
    "# 必要な形式に列を再配置\n",
    "pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "\n",
    "# 提出用に予測結果をCSVに保存\n",
    "pred_df.to_csv('submission.csv', index=False)\n",
    "print(pred_df.head())  # 予測結果の先頭部分を表示\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-07-18T17:01:34.312889Z",
     "iopub.status.busy": "2024-07-18T17:01:34.312463Z",
     "iopub.status.idle": "2024-07-18T20:21:54.723113Z",
     "shell.execute_reply": "2024-07-18T20:21:54.721672Z",
     "shell.execute_reply.started": "2024-07-18T17:01:34.312857Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# SiameseLSTMクラスを定義します\n",
    "class SiameseLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super(SiameseLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)  # 入力層の埋め込み層\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)  # LSTM層\n",
    "        self.fc = nn.Linear(hidden_size, 3)  # 出力層：3クラス（モデルAが勝つ、モデルBが勝つ、引き分け）\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = self.embedding(x)  # 入力を埋め込みに変換\n",
    "        _, (h, _) = self.lstm(x)  # LSTMを通して隠れ状態を取得\n",
    "        return h[-1]  # 最後の隠れ状態を返す\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        h1 = self.forward_one(x1)  # 最初の入力の出力を取得\n",
    "        h2 = self.forward_one(x2)  # 二つ目の入力の出力を取得\n",
    "        return self.fc(torch.abs(h1 - h2))  # 二つの隠れ状態の絶対差を計算し、全結合層に渡す\n",
    "\n",
    "# ステップ1：トレーニングデータセットを読み込む\n",
    "df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')\n",
    "\n",
    "# 無効なケースをフィルタリングし、データを準備する\n",
    "data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values\n",
    "\n",
    "def determine_label(row):\n",
    "    if row[2] == 1:\n",
    "        return 0  # モデルAが勝つ\n",
    "    elif row[3] == 1:\n",
    "        return 1  # モデルBが勝つ\n",
    "    elif row[4] == 1:\n",
    "        return 2  # 引き分け\n",
    "    else:\n",
    "        return -1  # 無効または不明なケース\n",
    "\n",
    "labels = [determine_label(row) for row in data if determine_label(row) != -1]  # ラベルをリストに格納\n",
    "data = [row[:2] for row in data if determine_label(row) != -1]  # 有効なデータペアを取得\n",
    "\n",
    "# ステップ2：カスタムDatasetクラスを定義します\n",
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, labels, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # データの長さを返す\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # トークナイザーを使用してテキストをトークン化\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク\n",
    "            'label': torch.tensor(label, dtype=torch.long)  # ラベル\n",
    "        }\n",
    "\n",
    "# ステップ3：BERTトークナイザーを初期化します\n",
    "tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')\n",
    "max_length = 128  # データセットに応じて調整\n",
    "\n",
    "# ステップ4：トレーニング用のDatasetとDataLoaderのインスタンスを作成します\n",
    "train_dataset = SiameseDataset(data, labels, tokenizer, max_length)\n",
    "batch_size = 32  # バッチサイズを32に設定\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # データローダーを作成\n",
    "\n",
    "# ステップ5：Siameseネットワークモデルを定義します\n",
    "input_size = len(tokenizer)  # トークナイザーのサイズを入力サイズに設定\n",
    "hidden_size = 300  # 隠れ層のサイズを300に設定\n",
    "num_layers = 1  # LSTMのレイヤー数\n",
    "model = SiameseLSTM(input_size, hidden_size, num_layers)\n",
    "\n",
    "# ステップ6：損失関数とオプティマイザーを定義します\n",
    "criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adamオプティマイザー\n",
    "\n",
    "# ステップ7：トレーニングループ\n",
    "num_epochs = 5  # 必要に応じて調整\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればGPUを使用\n",
    "model.to(device)  # モデルをデバイスに転送\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # モデルを訓練モードに設定\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # 勾配を初期化\n",
    "        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        loss.backward()  # バックプロパゲーションで勾配を計算\n",
    "        optimizer.step()  # オプティマイザーで重みを更新\n",
    "\n",
    "        total_loss += loss.item()  # 累積損失を加算\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')  # エポックごとの損失を表示\n",
    "\n",
    "# テストデータセットを読み込む\n",
    "df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')\n",
    "\n",
    "# テスト用のカスタムDatasetクラスを定義します\n",
    "class SiameseTestDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_length):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)  # データの長さを返す\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pair = self.data[idx]\n",
    "        response_a = pair[0]\n",
    "        response_b = pair[1]\n",
    "\n",
    "        # トークナイザーを使用してテキストをトークン化\n",
    "        tokens_a = self.tokenizer(response_a, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "        tokens_b = self.tokenizer(response_b, padding=\"max_length\", truncation=True, max_length=self.max_length)\n",
    "\n",
    "        return {\n",
    "            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID\n",
    "            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク\n",
    "            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID\n",
    "            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク\n",
    "        }\n",
    "\n",
    "# テストデータを準備する\n",
    "test_data = df_test[['response_a', 'response_b']].values.tolist()\n",
    "\n",
    "# SiameseTestDatasetのインスタンスを作成\n",
    "test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)\n",
    "\n",
    "# テストデータセットのデータローダーを作成\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 保存したモデルを読み込む\n",
    "model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))\n",
    "model.eval()  # モデルを評価モードに設定\n",
    "\n",
    "# テストデータに対して推論を行い、予測を生成する\n",
    "all_preds = []\n",
    "with torch.no_grad():  # 勾配計算を無効にする\n",
    "    for batch in tqdm(test_loader, desc='Testing'):\n",
    "        input_ids_a = batch['input_ids_a'].to(device)\n",
    "        attention_mask_a = batch['attention_mask_a'].to(device)\n",
    "        input_ids_b = batch['input_ids_b'].to(device)\n",
    "        attention_mask_b = batch['attention_mask_b'].to(device)\n",
    "\n",
    "        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得\n",
    "        probabilities = nn.Softmax(dim=1)(outputs)  # ソフトマックス関数を適用して確率を計算\n",
    "        all_preds.extend(probabilities.cpu().numpy().tolist())  # 確率をリストに追加\n",
    "\n",
    "# 予測結果のDataFrameを作成\n",
    "pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])\n",
    "pred_df['id'] = df_test['id']  # テストデータのIDを追加\n",
    "\n",
    "# 必要な形式に列を再配置\n",
    "pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]\n",
    "\n",
    "# 提出用に予測結果をCSVに保存\n",
    "pred_df.to_csv('submission.csv', index=False)\n",
    "print(pred_df.head())  # 予測結果の先頭部分を表示"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352801be",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 特にコードが記載されていないため、翻訳する内容がありません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T02:24:36.879044Z",
     "iopub.status.busy": "2024-07-19T02:24:36.878642Z",
     "iopub.status.idle": "2024-07-19T02:24:36.922822Z",
     "shell.execute_reply": "2024-07-19T02:24:36.920948Z",
     "shell.execute_reply.started": "2024-07-19T02:24:36.879013Z"
    }
   },
   "outputs": [],
   "source": [
    "# 特にコードが記載されていないため、翻訳する内容がありません。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b45803",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary>pythonコードの比較（クリックすると展開されます）</summary>\n",
    "\n",
    "<style>\n",
    ".column-left{\n",
    "  float: left;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-right{\n",
    "  float: right;\n",
    "  width: 47.5%;\n",
    "  text-align: left;\n",
    "}\n",
    ".column-one{\n",
    "  float: left;\n",
    "  width: 100%;\n",
    "  text-align: left;\n",
    "}\n",
    "</style>\n",
    "\n",
    "\n",
    "<div class=\"column-left\">\n",
    "\n",
    "# original\n",
    "\n",
    "```python\n",
    "\n",
    "```\n",
    "\n",
    "</div>\n",
    "<div class=\"column-right\">\n",
    "\n",
    "# 日本語訳\n",
    "\n",
    "```python\n",
    "# 特にコードが記載されていないため、翻訳する内容がありません。\n",
    "```\n",
    "\n",
    "</div>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特にコードが記載されていないため、翻訳する内容がありません。"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 5410430,
     "sourceId": 8984100,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5411525,
     "sourceId": 8985626,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30746,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
