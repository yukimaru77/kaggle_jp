* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:64 @ [Starter Notebook 34B LLM wow! - Nvidia Wins KDD Cup 2024 wow!](./discuss/Starter_Notebook_34B_LLM_wow!_-_Nvidia_Wins_KDD_Cup_2024_woâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Starter_Notebook_34B_LLM_wow!_-_Nvidia_Wins_KDD_Cup_2024_woâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:47 @ [LLM Models that can be used](./discuss/LLM_Models_that_can_be_used.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/LLM_Models_that_can_be_used.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:47 @ [Unstable Deberta Training Results](./discuss/Unstable_Deberta_Training_Results.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Unstable_Deberta_Training_Results.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:39 @ [[???] Crazy lb 0.707 run in just 1h](./discuss/[]_Crazy_lb_0707_run_in_just_1h.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[]_Crazy_lb_0707_run_in_just_1h.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:28 @ [Suspicious Surge of Novice Accounts on the Leaderboard](./discuss/Suspicious_Surge_of_Novice_Accounts_on_the_Leaderboard.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Suspicious_Surge_of_Novice_Accounts_on_the_Leaderboard.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:27 @ [submissions get timeout?](./discuss/submissions_get_timeout.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/submissions_get_timeout.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:20 @ [How long does it tke to be granted access to Llama3 on Kaggle? [Solved: 24 hours]](./discuss/How_long_does_it_tke_to_be_granted_access_to_Llama3_on_Kaggâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_long_does_it_tke_to_be_granted_access_to_Llama3_on_Kaggâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:19 @ [External data - additional 157k human preference ratings ğŸ”¥ğŸ”¥ğŸ”¥](./discuss/External_data_-_additional_157k_human_preference_ratings_ğŸ”¥ğŸ”¥ğŸ”¥.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/External_data_-_additional_157k_human_preference_ratings_ğŸ”¥ğŸ”¥ğŸ”¥.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:18 @ [Llama 3.1 has just been released](./discuss/Llama_31_has_just_been_released.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Llama_31_has_just_been_released.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:16 @ [How crazyï¼Top 1: LB 707 â€”â€”> 663!](./discuss/How_crazyï¼Top_1_LB_707_â€”â€”_663!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_crazyï¼Top_1_LB_707_â€”â€”_663!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:16 @ [What a bad score!!! I am puzzled whether it is reasonable?](./discuss/What_a_bad_score!!!_I_am_puzzled_whether_it_is_reasonable.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_a_bad_score!!!_I_am_puzzled_whether_it_is_reasonable.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:16 @ [Prediction Using Generation Header](./discuss/Prediction_Using_Generation_Header.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Prediction_Using_Generation_Header.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:16 @ [Additional 21k Labelled Conversations ğŸš€](./discuss/Additional_21k_Labelled_Conversations_ğŸš€.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Additional_21k_Labelled_Conversations_ğŸš€.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:15 @ [Gemma 2 has been released](./discuss/Gemma_2_has_been_released.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Gemma_2_has_been_released.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:15 @ [Data Annotation Intuition - why the labels are noisy](./discuss/Data_Annotation_Intuition_-_why_the_labels_are_noisy.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Data_Annotation_Intuition_-_why_the_labels_are_noisy.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:15 @ [Less than 1.3 seconds per inference?](./discuss/Less_than_13_seconds_per_inference.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Less_than_13_seconds_per_inference.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:14 @ [LMSYS Dataset Explorer â€“ Cluster, segment, inspect](./discuss/LMSYS_Dataset_Explorer_â€“_Cluster,_segment,_inspect.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/LMSYS_Dataset_Explorer_â€“_Cluster,_segment,_inspect.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:13 @ [Request for Kaggle Llama 3 access under review for over 6 days [Solved]](./discuss/Request_for_Kaggle_Llama_3_access_under_review_for_over_6_dâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Request_for_Kaggle_Llama_3_access_under_review_for_over_6_dâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:13 @ [Facing "CUDA out of memory" error during fine-tuning Llama3 model](./discuss/Facing_CUDA_out_of_memory_error_during_fine-tuning_Llama3_mâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Facing_CUDA_out_of_memory_error_during_fine-tuning_Llama3_mâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:12 @ [Solution file to be updated the week of May 28th](./discuss/Solution_file_to_be_updated_the_week_of_May_28th.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Solution_file_to_be_updated_the_week_of_May_28th.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:12 @ [Ranking of models estimated from train data.](./discuss/Ranking_of_models_estimated_from_train_data.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Ranking_of_models_estimated_from_train_data.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [Llama 3.1 7b vs Gemma 9b (sft)?](./discuss/Llama_31_7b_vs_Gemma_9b_(sft).md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Llama_31_7b_vs_Gemma_9b_(sft).md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [Feel overwhelmed with this competition](./discuss/Feel_overwhelmed_with_this_competition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Feel_overwhelmed_with_this_competition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [CV vs LBï¼ŒWill there be significant fluctuationsï¼Ÿ](./discuss/CV_vs_LBï¼ŒWill_there_be_significant_fluctuationsï¼Ÿ.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/CV_vs_LBï¼ŒWill_there_be_significant_fluctuationsï¼Ÿ.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [Mistral-NeMo release](./discuss/Mistral-NeMo_release.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Mistral-NeMo_release.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [Gemma2 9b's inference time is much longer that Llama3 8b?](./discuss/Gemma2_9b's_inference_time_is_much_longer_that_Llama3_8b.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Gemma2_9b's_inference_time_is_much_longer_that_Llama3_8b.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:11 @ [DeBERTa is not learning patterns?](./discuss/DeBERTa_is_not_learning_patterns.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/DeBERTa_is_not_learning_patterns.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:10 @ [Combine Gemma-2 9b & Llama-3 8b](./discuss/Combine_Gemma-2_9b_&_Llama-3_8b.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Combine_Gemma-2_9b_&_Llama-3_8b.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:10 @ [Am I overfitting?](./discuss/Am_I_overfitting.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Am_I_overfitting.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:10 @ [CUDA OOM when ensemble Gemma2 and Llama3](./discuss/CUDA_OOM_when_ensemble_Gemma2_and_Llama3.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/CUDA_OOM_when_ensemble_Gemma2_and_Llama3.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:10 @ [Notebook threw exception](./discuss/Notebook_threw_exception.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Notebook_threw_exception.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Expectation for Kaggle Resources?](./discuss/Expectation_for_Kaggle_Resources.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Expectation_for_Kaggle_Resources.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Time out always](./discuss/Time_out_always.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Time_out_always.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Llama3.1 works not as good as expect](./discuss/Llama31_works_not_as_good_as_expect.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Llama31_works_not_as_good_as_expect.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Danube3 (0.5 B & 4B) just dropped!](./discuss/Danube3_(05_B_&_4B)_just_dropped!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Danube3_(05_B_&_4B)_just_dropped!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Why are there suddenly so many new accounts with very similar scores?](./discuss/Why_are_there_suddenly_so_many_new_accounts_with_very_similâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_are_there_suddenly_so_many_new_accounts_with_very_similâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Share my initial experiment](./discuss/Share_my_initial_experiment.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Share_my_initial_experiment.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:9 @ [Why are there winners for the same prompt and response?](./discuss/Why_are_there_winners_for_the_same_prompt_and_response.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_are_there_winners_for_the_same_prompt_and_response.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Get started here!](./discuss/Get_started_here!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Get_started_here!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [The magic behind Gemma2](./discuss/The_magic_behind_Gemma2.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/The_magic_behind_Gemma2.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Is it possible to use gemma-2-27B with vLLM?](./discuss/Is_it_possible_to_use_gemma-2-27B_with_vLLM.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_it_possible_to_use_gemma-2-27B_with_vLLM.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [[need help]How to use DDP to speedup sft ?](./discuss/[need_help]How_to_use_DDP_to_speedup_sft_.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[need_help]How_to_use_DDP_to_speedup_sft_.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Are external data helpful?](./discuss/Are_external_data_helpful.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Are_external_data_helpful.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [How to address the issue of performance loss when transferring trained model weights across different environments and devices?](./discuss/How_to_address_the_issue_of_performance_loss_when_transferrâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_address_the_issue_of_performance_loss_when_transferrâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Does calibration matter?](./discuss/Does_calibration_matter.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Does_calibration_matter.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [How to finetune LLM?](./discuss/How_to_finetune_LLM.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_finetune_LLM.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Can I use model which is not in kaggle ?](./discuss/Can_I_use_model_which_is_not_in_kaggle_.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Can_I_use_model_which_is_not_in_kaggle_.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Training outside kaggle](./discuss/Training_outside_kaggle.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Training_outside_kaggle.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:8 @ [Both the A and B responses are [null]](./discuss/Both_the_A_and_B_responses_are_[null].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Both_the_A_and_B_responses_are_[null].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [This Competition has an Official Discord Channel](./discuss/This_Competition_has_an_Official_Discord_Channel.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/This_Competition_has_an_Official_Discord_Channel.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [0.707 -> 0.663 in 3 hours](./discuss/0707_-_0663_in_3_hours.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/0707_-_0663_in_3_hours.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [question about the quota of GPU](./discuss/question_about_the_quota_of_GPU.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/question_about_the_quota_of_GPU.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [Llama3.1-8B May Release Soon](./discuss/Llama31-8B_May_Release_Soon.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Llama31-8B_May_Release_Soon.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [One-Feature Decision Tree](./discuss/One-Feature_Decision_Tree.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/One-Feature_Decision_Tree.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [Is the Kaggle provided compute enough to join this competition?](./discuss/Is_the_Kaggle_provided_compute_enough_to_join_this_competitâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_the_Kaggle_provided_compute_enough_to_join_this_competitâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:7 @ [Data Understanding: Why prompt is list of strings?](./discuss/Data_Understanding_Why_prompt_is_list_of_strings.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Data_Understanding_Why_prompt_is_list_of_strings.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Holy...did I just see a 0.707 on lb?](./discuss/Holydid_I_just_see_a_0707_on_lb.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Holydid_I_just_see_a_0707_on_lb.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Notebook timeout problem](./discuss/Notebook_timeout_problem.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Notebook_timeout_problem.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [The hidden test dataset has distribution shift](./discuss/The_hidden_test_dataset_has_distribution_shift.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/The_hidden_test_dataset_has_distribution_shift.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Why I did not get the corresponding checkpoint when I continued training from the checkpoint and reset save_steps? My save_steps=5000 before, and now I changed it to 200, but after 200 steps, I did not get the corresponding checkpoint.](./discuss/Why_I_did_not_get_the_corresponding_checkpoint_when_I_contiâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_I_did_not_get_the_corresponding_checkpoint_when_I_contiâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Usage of unsloth](./discuss/Usage_of_unsloth.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Usage_of_unsloth.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Time constraint for private leaderboard [Solved]](./discuss/Time_constraint_for_private_leaderboard_[Solved].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Time_constraint_for_private_leaderboard_[Solved].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Getting error in sumbission.](./discuss/Getting_error_in_sumbission.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Getting_error_in_sumbission.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Can I win only using Kaggle resources?](./discuss/Can_I_win_only_using_Kaggle_resources.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Can_I_win_only_using_Kaggle_resources.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [Unspecified exception, only when submitted (later = Submission Scoring error)](./discuss/Unspecified_exception,_only_when_submitted_(later__Submissiâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Unspecified_exception,_only_when_submitted_(later__Submissiâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [training loss doesnt converge](./discuss/training_loss_doesnt_converge.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/training_loss_doesnt_converge.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [how can I load a pretrained model with internet off](./discuss/how_can_I_load_a_pretrained_model_with_internet_off.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/how_can_I_load_a_pretrained_model_with_internet_off.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:6 @ [How to work with Gemma Keras 1.1_7b instruct _en WITHOUT Google Cloud? On the 1.1_2b_instruct_en No Memory issue.](./discuss/How_to_work_with_Gemma_Keras_11_7b_instruct__en_WITHOUT_Gooâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_work_with_Gemma_Keras_11_7b_instruct__en_WITHOUT_Gooâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Has anyone tried lora variants like lora+, rslora, dora, and the latest lora-ga, lora-pro, and do they work better ?](./discuss/Has_anyone_tried_lora_variants_like_lora+,_rslora,_dora,_anâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Has_anyone_tried_lora_variants_like_lora+,_rslora,_dora,_anâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [How many epochs do people usually choose when fine-tuning?](./discuss/How_many_epochs_do_people_usually_choose_when_fine-tuning.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_many_epochs_do_people_usually_choose_when_fine-tuning.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Does increase the batch size of gemma2 9b in training do some help?](./discuss/Does_increase_the_batch_size_of_gemma2_9b_in_training_do_soâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Does_increase_the_batch_size_of_gemma2_9b_in_training_do_soâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Finetuned --> Quantized vs Quantized--> Finetuned](./discuss/Finetuned_--_Quantized_vs_Quantized--_Finetuned.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Finetuned_--_Quantized_vs_Quantized--_Finetuned.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [How to choose a suitable model](./discuss/How_to_choose_a_suitable_model.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_choose_a_suitable_model.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [External datasets](./discuss/External_datasets.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/External_datasets.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [How much data to train Llama 3?](./discuss/How_much_data_to_train_Llama_3.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_much_data_to_train_Llama_3.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [how to save qlora_trained model weight with pytorch](./discuss/how_to_save_qlora_trained_model_weight_with_pytorch.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/how_to_save_qlora_trained_model_weight_with_pytorch.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Too long waiting time for TPU](./discuss/Too_long_waiting_time_for_TPU.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Too_long_waiting_time_for_TPU.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [7b OOM while 8b works fine, is this strange?](./discuss/7b_OOM_while_8b_works_fine,_is_this_strange.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/7b_OOM_while_8b_works_fine,_is_this_strange.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Replacing response A and response B for Data Augmentation](./discuss/Replacing_response_A_and_response_B_for_Data_Augmentation.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Replacing_response_A_and_response_B_for_Data_Augmentation.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [Is Microsoft phi3 allowed?](./discuss/Is_Microsoft_phi3_allowed.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_Microsoft_phi3_allowed.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:5 @ [CV vs LB thread](./discuss/CV_vs_LB_thread.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/CV_vs_LB_thread.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [1h -> .707! Is the dataset leaked?](./discuss/1h_-_707!_Is_the_dataset_leaked.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/1h_-_707!_Is_the_dataset_leaked.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [About Data Augmentation](./discuss/About_Data_Augmentation.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/About_Data_Augmentation.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [submission scoreing error](./discuss/submission_scoreing_error.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/submission_scoreing_error.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [[Need help] How to custom Head for AutoModelForSequenceClassification with LoRA?](./discuss/[Need_help]_How_to_custom_Head_for_AutoModelForSequenceClasâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Need_help]_How_to_custom_Head_for_AutoModelForSequenceClasâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [Accessing Llama 3 MODEL_NAME in inference notebook](./discuss/Accessing_Llama_3_MODEL_NAME_in_inference_notebook.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Accessing_Llama_3_MODEL_NAME_in_inference_notebook.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [How to avoid wights and bias logging, when I try train the model, it was asking wights and bias token id](./discuss/How_to_avoid_wights_and_bias_logging,_when_I_try_train_the_â€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_avoid_wights_and_bias_logging,_when_I_try_train_the_â€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [What do you think qwen2:0.5b is doing here?](./discuss/What_do_you_think_qwen205b_is_doing_here.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_do_you_think_qwen205b_is_doing_here.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [Why LLAMA3 dominates the leaderboards, not deberta.](./discuss/Why_LLAMA3_dominates_the_leaderboards,_not_deberta.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_LLAMA3_dominates_the_leaderboards,_not_deberta.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [Why is this competition less popular than AES2?](./discuss/Why_is_this_competition_less_popular_than_AES2.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_is_this_competition_less_popular_than_AES2.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [How to get the vector representation of sentences using Llama3?](./discuss/How_to_get_the_vector_representation_of_sentences_using_Llaâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_get_the_vector_representation_of_sentences_using_Llaâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [Submission Scoring Error](./discuss/Submission_Scoring_Error.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Submission_Scoring_Error.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [how many is the perfect score?](./discuss/how_many_is_the_perfect_score.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/how_many_is_the_perfect_score.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [offline use of fine-tuned Llama3](./discuss/offline_use_of_fine-tuned_Llama3.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/offline_use_of_fine-tuned_Llama3.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:4 @ [More information on test dataset](./discuss/More_information_on_test_dataset.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/More_information_on_test_dataset.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Did you try to explode the conversation responses?](./discuss/Did_you_try_to_explode_the_conversation_responses.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Did_you_try_to_explode_the_conversation_responses.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Help! TPU install error(From today)](./discuss/Help!_TPU_install_error(From_today).md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Help!_TPU_install_error(From_today).md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Are We Really on the Right Track?](./discuss/Are_We_Really_on_the_Right_Track.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Are_We_Really_on_the_Right_Track.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Help! Loss function not behaving as expected!](./discuss/Help!_Loss_function_not_behaving_as_expected!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Help!_Loss_function_not_behaving_as_expected!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Larger model, better result?](./discuss/Larger_model,_better_result.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Larger_model,_better_result.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Any ways to see submission runtime?](./discuss/Any_ways_to_see_submission_runtime.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Any_ways_to_see_submission_runtime.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Does post-processing apply to this competition?](./discuss/Does_post-processing_apply_to_this_competition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Does_post-processing_apply_to_this_competition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [[Need Help] Running Gemma 2 9b in Keras on 2xT4s](./discuss/[Need_Help]_Running_Gemma_2_9b_in_Keras_on_2xT4s.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Need_Help]_Running_Gemma_2_9b_in_Keras_on_2xT4s.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [A question about submission](./discuss/A_question_about_submission.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/A_question_about_submission.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Will we no longer be able to use GPUs and TPUs in competitions?](./discuss/Will_we_no_longer_be_able_to_use_GPUs_and_TPUs_in_competitiâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Will_we_no_longer_be_able_to_use_GPUs_and_TPUs_in_competitiâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Training not proceeding for Llama 3](./discuss/Training_not_proceeding_for_Llama_3.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Training_not_proceeding_for_Llama_3.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Is label smoothing beneficial in LLM fine-tuning?](./discuss/Is_label_smoothing_beneficial_in_LLM_fine-tuning.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_label_smoothing_beneficial_in_LLM_fine-tuning.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Notebook throwing Exception](./discuss/Notebook_throwing_Exception.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Notebook_throwing_Exception.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [What is 8b-chat-hf in kaggle's llama3 models?](./discuss/What_is_8b-chat-hf_in_kaggle's_llama3_models.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_is_8b-chat-hf_in_kaggle's_llama3_models.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [this requirement <= 9 hours run-time is for public dataset or private ?](./discuss/this_requirement__9_hours_run-time_is_for_public_dataset_orâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/this_requirement__9_hours_run-time_is_for_public_dataset_orâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [config.json file missing in Llama model](./discuss/configjson_file_missing_in_Llama_model.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/configjson_file_missing_in_Llama_model.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Model licenses](./discuss/Model_licenses.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Model_licenses.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Load 7b Gemma Keras without any memory issue and FAST.](./discuss/Load_7b_Gemma_Keras_without_any_memory_issue_and_FAST.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Load_7b_Gemma_Keras_without_any_memory_issue_and_FAST.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Traing steps cannot match the sample size!](./discuss/Traing_steps_cannot_match_the_sample_size!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Traing_steps_cannot_match_the_sample_size!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [More than 1 response (response_a / response_b)](./discuss/More_than_1_response_(response_a__response_b).md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/More_than_1_response_(response_a__response_b).md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [What is the trick...](./discuss/What_is_the_trick.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_is_the_trick.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [What is the calculation of the loss & log_loss?](./discuss/What_is_the_calculation_of_the_loss_&_log_loss.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_is_the_calculation_of_the_loss_&_log_loss.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Why is the scoring process so time-consuming](./discuss/Why_is_the_scoring_process_so_time-consuming.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_is_the_scoring_process_so_time-consuming.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [What does the #id mean and why they are non-sequential](./discuss/What_does_the_#id_mean_and_why_they_are_non-sequential.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/What_does_the_#id_mean_and_why_they_are_non-sequential.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Question about rule(fine tuning the LLM)](./discuss/Question_about_rule(fine_tuning_the_LLM).md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Question_about_rule(fine_tuning_the_LLM).md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Additional Data Issues to Note](./discuss/Additional_Data_Issues_to_Note.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Additional_Data_Issues_to_Note.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Interpreting the metric & why current baselines are basically guessing](./discuss/Interpreting_the_metric_&_why_current_baselines_are_basicalâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Interpreting_the_metric_&_why_current_baselines_are_basicalâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [predict the model of the response to to a classification?](./discuss/predict_the_model_of_the_response_to_to_a_classification.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/predict_the_model_of_the_response_to_to_a_classification.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [I have started hallucinating because of data :D](./discuss/I_have_started_hallucinating_because_of_data_D.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/I_have_started_hallucinating_because_of_data_D.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Internet access condition](./discuss/Internet_access_condition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Internet_access_condition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Is TPU allowed to be used in this competition](./discuss/Is_TPU_allowed_to_be_used_in_this_competition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_TPU_allowed_to_be_used_in_this_competition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [why are there only 3 test cases in the test.csv?](./discuss/why_are_there_only_3_test_cases_in_the_testcsv.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/why_are_there_only_3_test_cases_in_the_testcsv.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Deberta Baseline - LB 1.075](./discuss/Deberta_Baseline_-_LB_1075.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Deberta_Baseline_-_LB_1075.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Chatbot Arena's Rules](./discuss/Chatbot_Arena's_Rules.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Chatbot_Arena's_Rules.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Why So Little](./discuss/Why_So_Little.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_So_Little.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Prometheus 2 for Evaluating Language Models](./discuss/Prometheus_2_for_Evaluating_Language_Models.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Prometheus_2_for_Evaluating_Language_Models.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [lmsys chat 1m (is Allowed? - dataset consists of user interactions from the ChatBot Arena) [Solved - Allowed]](./discuss/lmsys_chat_1m_(is_Allowed_-_dataset_consists_of_user_interaâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/lmsys_chat_1m_(is_Allowed_-_dataset_consists_of_user_interaâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [Do we need the class "tie"?](./discuss/Do_we_need_the_class_tie.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Do_we_need_the_class_tie.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:3 @ [[New Starter] 3 texts + Huggingface](./discuss/[New_Starter]_3_texts_+_Huggingface.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[New_Starter]_3_texts_+_Huggingface.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [[Insights] on 165k Dataset and Model Performance](./discuss/[Insights]_on_165k_Dataset_and_Model_Performance.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Insights]_on_165k_Dataset_and_Model_Performance.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [postprocess data that have [null] responses](./discuss/postprocess_data_that_have_[null]_responses.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/postprocess_data_that_have_[null]_responses.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Damn Ties!](./discuss/Damn_Ties!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Damn_Ties!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [[Solved] trainer.train("checkpoint-1000") ignores the first samples?: Resuming training in Transformers library.](./discuss/[Solved]_trainertrain(checkpoint-1000)_ignores_the_first_saâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Solved]_trainertrain(checkpoint-1000)_ignores_the_first_saâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Synthetic Closed source API](./discuss/Synthetic_Closed_source_API.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Synthetic_Closed_source_API.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [LMSYS: Research Papers Relevant for this Competition](./discuss/LMSYS_Research_Papers_Relevant_for_this_Competition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/LMSYS_Research_Papers_Relevant_for_this_Competition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [[question]9h time limit](./discuss/[question]9h_time_limit.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[question]9h_time_limit.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Can anyone share some trick about kaggle competitions?](./discuss/Can_anyone_share_some_trick_about_kaggle_competitions.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Can_anyone_share_some_trick_about_kaggle_competitions.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [shake up ?](./discuss/shake_up_.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/shake_up_.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [[Question] Confused with random classification layer weight(score.weight) for LlamaForSequenceClassification](./discuss/[Question]_Confused_with_random_classification_layer_weightâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Question]_Confused_with_random_classification_layer_weightâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Has anyone used Google cloud TPUs ?](./discuss/Has_anyone_used_Google_cloud_TPUs_.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Has_anyone_used_Google_cloud_TPUs_.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [special token Efficiency](./discuss/special_token_Efficiency.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/special_token_Efficiency.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Question about submission](./discuss/Question_about_submission.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Question_about_submission.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Allowing the notebook keep on running at the back on Vast.ai](./discuss/Allowing_the_notebook_keep_on_running_at_the_back_on_Vastai.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Allowing_the_notebook_keep_on_running_at_the_back_on_Vastai.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Error when submit](./discuss/Error_when_submit.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Error_when_submit.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Can we use external data that contain other model's responses excluding  those in  the competition data?](./discuss/Can_we_use_external_data_that_contain_other_model's_responsâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Can_we_use_external_data_that_contain_other_model's_responsâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Using XGBoost and Graident Boosting Techniques](./discuss/Using_XGBoost_and_Graident_Boosting_Techniques.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Using_XGBoost_and_Graident_Boosting_Techniques.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Clarification on the number of chats that need to be processed](./discuss/Clarification_on_the_number_of_chats_that_need_to_be_procesâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Clarification_on_the_number_of_chats_that_need_to_be_procesâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [test data does not have LLM name , while the train data had (columns : 'model_a' and 'model_b').](./discuss/test_data_does_not_have_LLM_name_,_while_the_train_data_hadâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/test_data_does_not_have_LLM_name_,_while_the_train_data_hadâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Download Llama3 Models from Kaggle?](./discuss/Download_Llama3_Models_from_Kaggle.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Download_Llama3_Models_from_Kaggle.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [More Interesting Observations to Share](./discuss/More_Interesting_Observations_to_Share.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/More_Interesting_Observations_to_Share.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [How to run a model on two cards](./discuss/How_to_run_a_model_on_two_cards.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_run_a_model_on_two_cards.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Notebook issue](./discuss/Notebook_issue.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Notebook_issue.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Trying to understand the strength of the models with the highest ties](./discuss/Trying_to_understand_the_strength_of_the_models_with_the_hiâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Trying_to_understand_the_strength_of_the_models_with_the_hiâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [Exciting and Creative](./discuss/Exciting_and_Creative.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Exciting_and_Creative.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [How can i use llama3?](./discuss/How_can_i_use_llama3.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_can_i_use_llama3.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:2 @ [I encountered the following problem while training the model, how should I solve it](./discuss/I_encountered_the_following_problem_while_training_the_modeâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/I_encountered_the_following_problem_while_training_the_modeâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [QA: About Deadline](./discuss/QA_About_Deadline.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/QA_About_Deadline.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Is this competition leaked? Very interesting, there is a huge gap between the first and second place.](./discuss/Is_this_competition_leaked_Very_interesting,_there_is_a_hugâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_this_competition_leaked_Very_interesting,_there_is_a_hugâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [How to tokenize prompts and responses efficiently](./discuss/How_to_tokenize_prompts_and_responses_efficiently.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_tokenize_prompts_and_responses_efficiently.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Llama loading error! HELP!](./discuss/Llama_loading_error!_HELP!.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Llama_loading_error!_HELP!.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [how to fine-tune again?](./discuss/how_to_fine-tune_again.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/how_to_fine-tune_again.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Do I still need to continue training?](./discuss/Do_I_still_need_to_continue_training.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Do_I_still_need_to_continue_training.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [The language](./discuss/The_language.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/The_language.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [P100 and T4*2 GPUs inference results are different](./discuss/P100_and_T42_GPUs_inference_results_are_different.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/P100_and_T42_GPUs_inference_results_are_different.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Clean Text](./discuss/Clean_Text.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Clean_Text.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [How to solve no kernel](./discuss/How_to_solve_no_kernel.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_to_solve_no_kernel.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Should we train our model just for English or for multiple languages ?](./discuss/Should_we_train_our_model_just_for_English_or_for_multiple_â€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Should_we_train_our_model_just_for_English_or_for_multiple_â€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Does the order of the submission file matter?](./discuss/Does_the_order_of_the_submission_file_matter.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Does_the_order_of_the_submission_file_matter.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Which hardware configuration is used in scoring?](./discuss/Which_hardware_configuration_is_used_in_scoring.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Which_hardware_configuration_is_used_in_scoring.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Using template for instruct model?](./discuss/Using_template_for_instruct_model.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Using_template_for_instruct_model.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Criteria and Approaches](./discuss/Criteria_and_Approaches.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Criteria_and_Approaches.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Why do I get "Notebook Threw Exception", and with the same notebook, sometimes, I get a successful submission?](./discuss/Why_do_I_get_Notebook_Threw_Exception,_and_with_the_same_noâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_do_I_get_Notebook_Threw_Exception,_and_with_the_same_noâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [General Question](./discuss/General_Question.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/General_Question.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Questions/Thoughts for building the baseline model for my first submission](./discuss/QuestionsThoughts_for_building_the_baseline_model_for_my_fiâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/QuestionsThoughts_for_building_the_baseline_model_for_my_fiâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [How far can a boosting get you?](./discuss/How_far_can_a_boosting_get_you.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_far_can_a_boosting_get_you.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Data Analysis with Chatbot Arena-like Chat Renderer](./discuss/Data_Analysis_with_Chatbot_Arena-like_Chat_Renderer.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Data_Analysis_with_Chatbot_Arena-like_Chat_Renderer.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Opening move for this competition](./discuss/Opening_move_for_this_competition.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Opening_move_for_this_competition.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [how to submit my predictions](./discuss/how_to_submit_my_predictions.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/how_to_submit_my_predictions.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [LB Experiment: Modify Prediction Temperature](./discuss/LB_Experiment_Modify_Prediction_Temperature.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/LB_Experiment_Modify_Prediction_Temperature.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [is 'Both are bad' = 'Tie' ?](./discuss/is_'Both_are_bad'__'Tie'_.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/is_'Both_are_bad'__'Tie'_.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [Data Understanding: Some response is NULL](./discuss/Data_Understanding_Some_response_is_NULL.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Data_Understanding_Some_response_is_NULL.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [How are we gonna predict probability with no data?](./discuss/How_are_we_gonna_predict_probability_with_no_data.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/How_are_we_gonna_predict_probability_with_no_data.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:1 @ [TF-IDF -> Boosted Trees [LB 1.038]](./discuss/TF-IDF_-_Boosted_Trees_[LB_1038].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/TF-IDF_-_Boosted_Trees_[LB_1038].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Is there verbosity bias?](./discuss/Is_there_verbosity_bias.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Is_there_verbosity_bias.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [An idea to use Reward Model](./discuss/An_idea_to_use_Reward_Model.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/An_idea_to_use_Reward_Model.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [elo modelling?](./discuss/elo_modelling.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/elo_modelling.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Why are cv and lb badly mismatched](./discuss/Why_are_cv_and_lb_badly_mismatched.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Why_are_cv_and_lb_badly_mismatched.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Has anyone tried using an SVR (Support Vector Regression) to replace the original classification header?](./discuss/Has_anyone_tried_using_an_SVR_(Support_Vector_Regression)_tâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Has_anyone_tried_using_an_SVR_(Support_Vector_Regression)_tâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [There is more than 1 prompt in ~10% of the ['prompt'] rows](./discuss/There_is_more_than_1_prompt_in_~10%_of_the_['prompt']_rows.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/There_is_more_than_1_prompt_in_~10%_of_the_['prompt']_rows.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Explanation on the prediction that the model must perform](./discuss/Explanation_on_the_prediction_that_the_model_must_perform.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Explanation_on_the_prediction_that_the_model_must_perform.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Multi GPU support](./discuss/Multi_GPU_support.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Multi_GPU_support.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Converted Ultrafeedback data (External Data)](./discuss/Converted_Ultrafeedback_data_(External_Data).md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Converted_Ultrafeedback_data_(External_Data).md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Train LLMs efficently in multi chip environment [GPU/TPU]](./discuss/Train_LLMs_efficently_in_multi_chip_environment_[GPUTPU].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Train_LLMs_efficently_in_multi_chip_environment_[GPUTPU].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Easily Visualize and Compare Prompt, Response and Winner](./discuss/Easily_Visualize_and_Compare_Prompt,_Response_and_Winner.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Easily_Visualize_and_Compare_Prompt,_Response_and_Winner.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Deberta -> LGBM [1.30 LB]](./discuss/Deberta_-_LGBM_[130_LB].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Deberta_-_LGBM_[130_LB].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [TF-IDF + Word2Vec = FTW [LB 1.026]](./discuss/TF-IDF_+_Word2Vec__FTW_[LB_1026].md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/TF-IDF_+_Word2Vec__FTW_[LB_1026].md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [ChatBot Arena Prompts Can Distinguish Models. The Bradley-Terry model. Elo Ratings on Kaggle.](./discuss/ChatBot_Arena_Prompts_Can_Distinguish_Models_The_Bradley-Teâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/ChatBot_Arena_Prompts_Can_Distinguish_Models_The_Bradley-Teâ€¦.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [[Starter] Deberta-v3 Train & Inference code](./discuss/[Starter]_Deberta-v3_Train_&_Inference_code.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/[Starter]_Deberta-v3_Train_&_Inference_code.md)\]
* ã‚³ãƒ¡ãƒ³ãƒˆæ•°:0 @ [Predictive Human Preference: From Model Ranking to Model Routing (Idea for build a baseline)](./discuss/Predictive_Human_Preference_From_Model_Ranking_to_Model_Rouâ€¦.md)  \[[æ—¥è‹±æ¯”è¼ƒä»˜ãã¯ã“ã¡ã‚‰](./comparison/discuss/Predictive_Human_Preference_From_Model_Ranking_to_Model_Rouâ€¦.md)\]
