** @@@ Jupyter Notebook numver 0, the number of votes :211 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックでは、4ビット量子化された「Gemma-2 9b Instruct」モデルに基づくLoRAアダプターを使用した推論の手法が示されています。主な目的は、Chatbot Arenaコンペティションにおける、2つの言語モデル間のユーザー応答の選好予測を行うことです。

### 問題の扱い
ノートブックは、LoRAアダプターを用いて、量子化による誤差の影響を軽減しつつ、推論を迅速化する方法を探求しています。モデルをマージすることで誤差が生じる可能性があるため、LoRAアダプターを維持した状態での推論を推奨しています。また、モデルの性能は評価セットでの対数損失が0.9371、公開リーダーボードでの対数損失が0.941であると報告しています。

### 使用ライブラリと手法
ノートブックでは、主に以下のライブラリが使用されています:
- **Transformers**: 「Gemma2ForSequenceClassification」と「GemmaTokenizerFast」を使用してモデルの呼び出しやトークン化を実施。
- **Peft**: LoRAアダプターの適用に用いられます。
- **Torch**: GPU利用や自動混合精度計算をサポートし、モデルの推論が行われます。
- **PandasとNumPy**: データ処理と操作のためにデータフレームを使用し、結果の格納に役立てています。

### データ処理と推論
データはCSVファイルから読み込まれ、テキストの前処理を行った後、トークナイズされます。推論は、2つのGPUを用いてバッチごとに行われ、モデルAとモデルBの各々の勝率を計算します。結果として、モデルごとの勝率や引き分け確率をデータフレームに格納し、最終的に提出用のCSVファイルとして成果物を保存します。

全体として、このノートブックは、量子化されたモデルを最適に利用するための手法と細かな設定を提供し、効率的に予測を行うフレームワークを構築しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## このノートブックについて

これは、4ビット量子化された [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) と、私がアップロードしたスクリプトを使用してトレーニングしたLoRAアダプターを利用した推論ノートブックです [ここ](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune)で確認できます。
LoRAアダプターをベースモデルにマージすることで推論を速くすることもできますが、安易にそうすると無視できない量子化誤差が発生する可能性があります。そのため、私はLoRAアダプターをマージせずに維持することにしました。

## 結果

| サブセット | 対数損失 |
| - | - |
| 評価セット | 0.9371 |
| 公開LB | 0.941 |

提出には約4時間かかります。`max_length=2048`でTTAは使用していません。
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # テスト時のデータ拡張。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するか

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データの読み込みと前処理
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 11)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]  # プロンプトに"<prompt>: "を追加
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]  # 応答Aにプレフィックスを追加
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]  # 応答Bにプレフィックスを追加
    if spread_max_length:  # spread_max_lengthがTrueの場合
        # 各要素をmax_length//3でトークナイズ
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 各リストを結合
        attention_mask = [[1]* len(i) for i in input_ids]  # 各入力の長さに応じたアテンションマスクを作成
    else:
        # 各要素を結合してトークン化
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイズ
        input_ids = tokenized.input_ids  # トークンIDを取得
        attention_mask = tokenized.attention_mask  # アテンションマスクを取得
    return input_ids, attention_mask  # トークンIDとアテンションマスクを返す
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)  # Gemmaトークナイザーを読み込む
tokenizer.add_eos_token = True  # 終了トークンを追加
tokenizer.padding_side = "right"  # パディングの位置を右に設定

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークン化した結果をデータフレームに格納
data["length"] = data["input_ids"].apply(len)  # 各入力の長さを計算して追加

aug_data = pd.DataFrame()  # 拡張データ用のデータフレームを作成
aug_data["id"] = test["id"]
# response_aとresponse_bを入れ替える
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])  # トークナイズして格納
aug_data["length"] = aug_data["input_ids"].apply(len)  # 長さを計算
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # トークナイズしたデータの最初の要素をデコードして表示
```

---The following area is a Code cell (cell numver is 14)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  # 拡張データの最初の要素をデコードして表示
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# モデルを読み込む
```

---The following area is a Code cell (cell numver is 16)---
```python
# GPU 0にベースモデルを読み込む
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# GPU 1にベースモデルを読み込む
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
#### LoRAアダプターを読み込む
```

---The following area is a Code cell (cell numver is 18)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0に適用
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1に適用
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()  # 勾配計算を無効にする
@torch.cuda.amp.autocast()  # 自動混合精度を使って演算を行う
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝率と引き分けを記録するリスト
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終端インデックスを計算
        tmp = df.iloc[start_idx:end_idx]  # データフレームからバッチを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDを取得
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクを取得
        # トークナイザーを使ってデータをパディング
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))  # モデルで出力を計算
        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックスで確率に変換
        
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率をリストに追加
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率をリストに追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率をリストに追加
    
    df["winner_model_a"] = a_win  # モデルAの勝率をデータフレームに追加
    df["winner_model_b"] = b_win  # モデルBの勝率をデータフレームに追加
    df["winner_tie"] = tie  # 引き分けの確率をデータフレームに追加
    
    return df  # 更新されたデータフレームを返す
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()  # 処理開始時間を記録

# 入力の長さでソートして動的パディングを最大限に活用する
data = data.sort_values("length", ascending=False)
# sub_1とsub_2のトークン数がほぼ同じになるように分ける
sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータを選択
sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータを選択

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝率の配列を取得

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:  # TTAが有効な場合
    data = aug_data.sort_values("length", ascending=False)  # 入力の長さでソートしてスピードを向上させる
    sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータ
    sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータ

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

    tta_result_df = pd.concat(list(results), axis=0)  # TTAの結果を結合
    # TTAの順序が反転するので調整
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # 元の結果とTTA結果を平均
    proba = (proba + tta_proba) / 2

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝率を更新
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を更新
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの勝率を更新
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用のデータフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルに保存
display(submission_df)  # 提出データを表示
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
---

# コメント 

> ## Cody_Null
> 
> 推論時間を速めるアイデアはありますか？パフォーマンスを失わずに。
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > LoRA_A x LoRA_Bを最初に計算したときにキャッシュするのは簡単な方法ですが、それほど速度向上は見込めないかもしれません。
> > 
> > TensorRTやvLLMのような最適化ライブラリを使えるのかも気になります。
> > 
> > 
> > > ## Cody_Null
> > > 
> > > vLLMを試したことがありますか？私は試しましたが、うまく動かす方法がわかりませんでした。 
> > > 
> > > 
> > > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > まだ試していません。max_lengthを増やすと対数損失が減少することは認識していますが、2048を越えると改善は非常に小さいです。私のケースでは2048から4096にすると対数損失が0.002減少しました。残りの時間で最適化できる他の方法を検討しています。
> > > 
> > > 

---

> ## carvingfate
> 
> 私は以前30位でしたが、このコードのおかげで私の努力が無駄に思えてしまいます。しかし、共有する精神を尊重しており、これがインターネットの精神であると思います。
> 
> 
> > ## jointcc2
> > 
> > 業界の状況もそうだと思います、一つのモデルが全ての過去の努力を上回りますね。
> > 
> > 

---

> ## Van chrn
> 
> なぜvLLMではなく？それはもっと速いかもしれません！
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私はそれに取り組んでいます！
> > 
> > 
> > > ## Cody_Null
> > > 
> > > この使用方法を実現しましたか？私はvLLMを使ったことがなく、動作しているのを見てみたいです！
> > > 
> > > 

---

> ## Dai LinLing
> 
> 共有してくれてありがとう。これは私にとって非常に助けになり、理解も深まりました。
> 
> 

---

> ## Turbo
> 
> [@emiz6413](https://www.kaggle.com/emiz6413)   ノートブックを共有してくれてありがとう。
> 
> 

---

> ## Vitalii Bozheniuk
> 
> なぜシルバーティアの解決策を公開するのかわかりません。この0.88のノートブックを公開すれば、全員が1位になれるのですか？人々がアイデアやノートブックを共有するのは理解できますが、30位のノートブックを共有するのは意味がありません。競争とチャレンジの雰囲気が消えてしまいます。
> 
> 
> > ## G John Rao
> > 
> > まだ1ヶ月残っていますが、初心者にとってはブーストになります。経験豊富な専門家にとっては、1ヶ月は新しいアイデアを構築したり実装するには十分な時間です。 
> > 
> > 

---

> ## Korey Ma
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！私はいくつかの追加パラメータを微調整し、cv&lb(0.912&0.924)を達成しました。さらに良くするために他のトリックを試したいです😆
> 
> 
> > ## Yichuan Gao
> > 
> > もう少し詳細を共有していただけますか？ハイパーパラメータを調整しているのか、LoRAレイヤーやランクを増やしているのか？
> > 
> > > ## Korey Ma
> > > 
> > > "o_proj"と"gate_proj"を追加しただけです。 
> > > 

---

> ## samson
> 
> かなり良いノートブックを作成されていて、コメントに対する見解も妥当です。
> 
> ですが、なぜ重みを共有したのでしょう？真剣に学びたいと思っている人は、あなたの方法を使ったりそれを自分に適応したりするでしょう。しかし、あなたのせいで100以上のチームが盲目的にコピー+提出することになりました。これにより、中間にいる人々が適切なチームメートを見つけるのは不可能です。
> 
> 

---

> ## superferg
> 
> すごいですね。
> 
> 

---

> ## yuanzhe zhou
> 
> よくやりました！つまり、LLMを使うことが鍵ですか？BERTタイプのモデルは収束するには小さすぎるようです。
> 
> > ## Valentin Werner
> > 
> > まさにそうです。私もLLMがシーケンス生成で十分に微調整されていると思います。AI生成テキストをより認識し、テキストがどうあるべきかに最適化されているため、このタスクに適しています。これにより、基本的に火に火をもって戦うことになります。
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 実際、deberta-v3-smallを完全に微調整したところ、約1.1になりました。
> > > > BERTスタイルのエンコーダアーキテクチャは、理論的にはこれらの分類タスクにより適していると思います。
> > > しかし、あなたが指摘したように、実際にはLLMははるかに大きい（deberta v2 xxlargeは1.5B）ため、過学習を避けることができ、より多くのメモリ容量を持つことができます。
> > > もう一つの理由は、指示微調整という、非常に競技に似たデータを使用しているからかもしれません。
> > > 私はバニラgemma-2-9bでテストしたことはありませんが、どのように動作するかを見るのは興味深いです。
> > > 

---

> ## ano
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！微調整したモデルの検証データとcvスコアについて教えていただけますか？あなたのトレーニングノートブックに基づいて、私は行数を5で割った数に基づいて、約20％のデータを検証用に使用しました。その後、対数損失を計算しましたが、cvスコアは0.9未満でした。明らかに、検証データに間違いがあったため、cvスコアはあなたのトレーニングノートブックで書かれた0.9371よりも低くなりました。微調整モデルの検証データをどのように作成すればよいか教えていただけますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私のトレーニングノートブックで検証データを作成するには、次の行が実行されないことを確認してください。
> > 
> > ```
> > # ds = ds.select(torch.arange(100))
> > 
> > ```
> > 
> > 次に、最後のセルにあるこの行で検証セットを選択する必要があります。
> > 
> > ```
> > ds.select(eval_idx)
> > 
> > ```
> > 
> > > ## ano
> > > 
> > > 返信ありがとうございます。もちろん、データを減らすための行を削除しました。
> > > 
> > > では、検証データはあなたのトレーニングノートブックでのn_splits = 5およびfold_idx = 0で選択されるのですね。うーん、そうすると、私のコードにCVスコアを計算する上での間違いがあったかもしれません。 
> > > > [UPDATE] バグを見つけました。ありがとうございます！
> > > 

---

> ## Guillermo Perez G
> 
> 素晴らしい！しかし、スコアをさらに下げることはできないと思います。それとも、ノートブックの質によるのでしょうか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 現在のトップスコアは0.9未満です。スコアを改善するためのアイデアが残っていると思います。
> > 
> > > ## floriandev
> > > 
> > > Eisukeさん、素晴らしいノートブックをありがとう！あなたのノートブックを使うことで0.9未満になる可能性はありますか？
> > > 

---

> ## Sparsh Tewatia
> 
> 終了までにどれくらいの時間がかかりますか？時間が許せば、LLAMA3とGemma 2の2つのLLMのアンサンブルを行うことができます。
> 
> > ## Lorry Zou
> > 
> > ノートブックでは約4時間と記載されていますので、llama3とgemma2のアンサンブルは実行可能のようです。 
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 私はllama3とgemma2のアンサンブルを9時間以内で実行できました。max_length=2048およびper_device_batch_size=4を使用しました。
> > > 

---

> ## Lorry Zou
> 
> 私が行った全ての作業が無駄になりました…悲しい😅 でも素晴らしい作品です。
> 
> 

---

> ## Sam
> 
> 私はこのノートブックで提供されているのと同じ推論パラメータ（batch_size、max_length）を使ってGemmaモデルを試すことに決めました。（llamaではなくBert-likeモデルを使用している以外はすべて同じです）。このノートブックはT4x2で9時間以上かかっても終わらず、途中で止まってしまいます。
> 
> 25,000の例をGemmaモデルで処理すると、約17時間かかることがわかりました。
> 
> 何が問題かアドバイスをいただけますか？Gemmaモデルの推論を速くするにはどうすればいいですか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > [@andreysemenovmax](https://www.kaggle.com/andreysemenovmax) 
> > 
> > 8ビット重量とfp16アクティベーションでのGemma2 9bの動的量子化を行うと、提出のために4.5時間以内に実行されます。
> > 
> > 

---

> ## capyun007
> 
> Kaggleの初心者で質問があります。次のコードを実行してDataFrameをCSVファイルとして保存した後：
> 
> submission_df.to_csv('submission.csv', index=False)
> 
> submission.csvファイルはどこにありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > ノートブックをコミット&保存すると、出力タブに表示されるはずです。インタラクティブセッションで実行した場合は、作業ディレクトリ（./submission.csv）で確認できます。
> > 
> > > ## capyun007
> > > 
> > > 自分のノートブックをコミット&保存しましたが、出力タブに表示されません。
> > > 

---

> ## kanishka sriramoju
> 
> こんにちは、私はここで初心者です。あなたがKaggle入力ディレクトリから事前にトレーニングされたモデルを読み込んだことを見ました。この競技では、ノートブックが独立した環境で実行され、これらの作成されたディレクトリにアクセスできないということはありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > それらのデータセットは公開にしているので、提出時にはアクセスできるはずです。
> > 
> >
```

** @@@ Jupyter Notebook numver 1, the number of votes :165 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックでは、Gemma-2 9bモデルのトレーニング方法とその結果について説明されています。このノートブックの主な目的は、大規模言語モデル（LLM）のファインチューニングにおける最新の手法であるQLoRA (Quantized Low-Rank Adaptation)を使用して、競技会でのパフォーマンスを向上させることです。

### 問題点
ノートブックは、ユーザーが好みのチャットボット応答を予測するために、Gemma-2のトレーニングによる評価パフォーマンスを改善しようとしています。具体的には、評価セットのロスを最小化することで、実際の競技会や評価基準のスコア（LB: 0.941）を向上させることを目指しています。

### 使用手法とライブラリ
以下の手法およびライブラリが使用されています。

1. **QLoRA**: 低ランクの適応法を用いてモデルのトレーニングを効率化し、大規模モデルのメモリ使用を減らします。モデルの重みを量子化しつつトレーニングを行うため、計算コストを削減します。
2. **LoRA**: 通常のファインチューニングの代わりに、特定の層に小さな行列を介してパラメータを調整することで、トレーニング中に元の重みの更新を最小限に抑えます。
3. **Hugging Face Transformers**: 一般的なトランスフォーマーモデルのフレームワークとして使用され、`Gemma2ForSequenceClassification`などの特定のモデルやトークナイザーが利用されています。
4. **PyTorch**: モデルのトレーニングと評価に使用される深層学習ライブラリです。
5. **Sklearn**: 精度や対数損失の計算を行うために用いられています。

### ノートブックのフロー
- **データ準備**: Kaggleのデータセットを読み込み、カスタムトークナイザーを設定してデータを前処理します。
- **モデル設定**: Gemma-2の設定やトレーニングメソッドを指定し、LoRAおよびQLoRAのパラメータを設定します。
- **トレーニング**: 計算されたいくつかの評価指標（例えば、ロスと精度）を用いて、モデルのトレーニングを実施します。
- **結果の評価**: 最後に、評価セットに対するモデルのロスと精度を報告します。

このノートブックは、全体のプロセスを通じて、LLMのファインチューニングを改善し、実用的なパフォーマンスを提供するための具体的なステップと結果を示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## このノートブックについて
このノートブックでは、Gemma-2 9bをどのようにトレーニングしてLB: 0.941を取得したかを示します。推論コードは[こちら](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)にあります。
私は、unslothチームがアップロードした4ビット量子化された[Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit)をベースモデルとして使用し、LoRAアダプターを追加して1エポックのトレーニングを行いました。

## 結果

評価セットとして`id % 5 == 0`を使用し、残りをすべてトレーニングに使用しました。

| サブセット | ロス |
| - | - |
| eval | 0.9371 |
| LB | 0.941 |

## QLoRAファインチューニングとは？

従来のファインチューニングでは、重み（$\mathbf{W}$）は次のように更新されます：

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

ここで、$L$はこのステップでの損失、$\eta$は学習率です。

[LoRA](https://arxiv.org/abs/2106.09685)は、$\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$を2つの（はるかに）小さな行列、$\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$と$\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$に因子分解して近似しようとします。ここで$r \ll \text{min}(\text{d}, \text{k})$です。

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

トレーニング中、元の重みを凍結し、$\mathbf{A}$と$\mathbf{B}$のみが更新されるため、トレーニング中に更新する必要がある元の重みの割合はごくわずか（例：<1%）です。この方式により、トレーニング中のGPUメモリ使用量を大幅に削減しながら、通常の（フル）ファインチューニングと同じ性能を達成できます。

[QLoRA](https://arxiv.org/abs/2305.14314)は、LLMを量子化することでさらに効率を高めています。例えば、8Bパラメータモデルは32ビットで32GBのVRAMを占有しますが、量子化された8ビット/4ビットの8Bモデルはそれぞれ8GB/4GBしか必要ありません。
QLoRAは、LLMの重みを低精度（例：8ビット）で量子化する一方で、フォワード/バックワードの計算は高精度（例：16ビット）で行い、LoRAアダプターの重みも高精度で維持されることに注意してください。

4ビットでのA6000使用の1エポックは約15時間かかり、8ビットでは約24時間かかりましたが、ロスの違いは目立ちませんでした。

## 注意
Kaggleカーネルでの完全なトレーニングの実行には非常に長い時間がかかります。完全なトレーニングを実行するためには外部の計算リソースを使用することをお勧めします。
このノートブックではデモ目的でわずか100サンプルを使用していますが、その他はすべて私の設定と同じです。
```

---The following area is a Code cell (cell numver is 2)---
```python
# gemma-2はtransformers>=4.42.3から使用可能です
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    output_dir: str = "output"  # 出力フォルダ
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4ビット量子化されたgemma-2-9b-instruct
    max_length: int = 1024  # 最大長
    n_splits: int = 5  # データ分割数
    fold_idx: int = 0  # 現在のフォールドインデックス
    optim_type: str = "adamw_8bit"  # 使用するオプティマイザの種類
    per_device_train_batch_size: int = 2  # デバイスごとのトレーニングバッチサイズ
    gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8
    per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ
    n_epochs: int = 1  # エポック数
    freeze_layers: int = 16  # 総レイヤ数は42、最初の16レイヤにはアダプターを追加しない
    lr: float = 2e-4  # 学習率
    warmup_steps: int = 20  # ウォームアップステップ数
    lora_r: int = 16  # LoRAのランク
    lora_alpha: float = lora_r * 2  # LoRAのアルファ
    lora_dropout: float = 0.05  # LoRAのドロップアウト率
    lora_bias: str = "none"  # LoRAのバイアス

config = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
#### トレーニング引数
```

---The following area is a Code cell (cell numver is 7)---
```python
training_args = TrainingArguments(
    output_dir="output",  # 出力ディレクトリ
    overwrite_output_dir=True,  # 出力ディレクトリを上書きする
    report_to="none",  # ログの報告先
    num_train_epochs=config.n_epochs,  # トレーニングエポック数
    per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ
    gradient_accumulation_steps=config.gradient_accumulation_steps,  # グラデーション蓄積ステップ数
    per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ
    logging_steps=10,  # ログの出力間隔
    eval_strategy="epoch",  # 評価戦略
    save_strategy="steps",  # 保存戦略
    save_steps=200,  # 保存ステップ数
    optim=config.optim_type,  # オプティマイザの種類
    fp16=True,  # FP16を使用するか
    learning_rate=config.lr,  # 学習率
    warmup_steps=config.warmup_steps,  # ウォームアップステップ数
)
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
#### LoRA設定
```

---The following area is a Code cell (cell numver is 9)---
```python
lora_config = LoraConfig(
    r=config.lora_r,  # LoRAのランク
    lora_alpha=config.lora_alpha,  # LoRAのアルファ
    # 自己注意のみにターゲットを絞る
    target_modules=["q_proj", "k_proj", "v_proj"],  # ターゲットモジュール
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],  # 変換対象レイヤー
    lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率
    bias=config.lora_bias,  # LoRAのバイアス
    task_type=TaskType.SEQ_CLS,  # タスクの種類
)
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
### トークナイザーとモデルのインスタンス化
```

---The following area is a Code cell (cell numver is 11)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)  # トークナイザーの読み込み
tokenizer.add_eos_token = True  # <eos>を末尾に追加
tokenizer.padding_side = "right"  # パディングを右側に設定
```

---The following area is a Code cell (cell numver is 12)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,  # モデルの読み込み
    num_labels=3,  # 分類ラベルの数
    torch_dtype=torch.float16,  # データ型の設定
    device_map="auto",  # デバイスマッピング
)
model.config.use_cache = False  # キャッシュを使用しない
model = prepare_model_for_kbit_training(model)  # kビットトレーニングのためのモデル準備
model = get_peft_model(model, lora_config)  # LoRAモデルの取得
model  # モデルを表示
```

---The following area is a Code cell (cell numver is 13)---
```python
model.print_trainable_parameters()  # トレーニング可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### データセットのインスタンス化
```

---The following area is a Code cell (cell numver is 15)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")  # CSVからデータセットを作成
ds = ds.select(torch.arange(100))  # デモ目的で最初の100データのみを使用
```

---The following area is a Code cell (cell numver is 16)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer  # トークナイザーの初期化
        self.max_length = max_length  # 最大長の設定
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]  # プロンプトの整形
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]  # 応答Aの整形
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]  # 応答Bの整形
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # プロンプトと応答を結合
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークナイズ
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:  # モデルAが勝った場合
                label = 0  # ラベルは0
            elif b_win:  # モデルBが勝った場合
                label = 1  # ラベルは1
            else:  # 同点の場合
                label = 2  # ラベルは2
            labels.append(label)  # ラベルをリストに追加
        return {**tokenized, "labels": labels}  # トークン化されたデータとラベルを返す
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))  # テキストを処理して返す
```

---The following area is a Code cell (cell numver is 17)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化
ds = ds.map(encode, batched=True)  # データセットにマッピング
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
### 指標の計算

LBで使用するロスと補助的な指標として精度を計算します。
```

---The following area is a Code cell (cell numver is 19)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions  # 予測値の取得
    labels = eval_preds.label_ids  # ラベルの取得
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 予測値をソフトマックスで確率に変換
    loss = log_loss(y_true=labels, y_pred=probs)  # ロスの計算
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度の計算
    return {"acc": acc, "log_loss": loss}  # 精度とロスを返す
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
### データ分割

ここでは、トレーニングと評価を`id % 5`に基づいて分割します。
```

---The following area is a Code cell (cell numver is 21)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニングインデックス
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 評価インデックス
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 22)---
```python
train_idx, eval_idx = folds[config.fold_idx]  # トレーニングと評価のインデックス取得

trainer = Trainer(
    args=training_args,  # トレーナー引数
    model=model,  # モデル
    tokenizer=tokenizer,  # トークナイザー
    train_dataset=ds.select(train_idx),  # トレーニングデータセット
    eval_dataset=ds.select(eval_idx),  # 評価データセット
    compute_metrics=compute_metrics,  # 指標計算関数
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # データのコラトレータ
)
trainer.train()  # トレーニングを開始
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
---

# コメント

> ## Yichuan Gao
> 
> このノートブックから結果を再現するのが難しいです。具体的には、このノートブックをダウンロードし、入力パスを変更しただけで、サンプル(100)を削除しました。
> 
> 4090でトレーニングした後、train_lossは約1.0までしか下がらず、eval_lossも約0.98で、主張されている0.937のCVロスよりもかなり劣っています。異なるマシンの異なるGPU構成や異なるバッチサイズでトレーニングを試みましたが、すべて似たような悪い結果でした。
> 
> 何が原因か非常に疑問に思っています。アドバイスがあれば歓迎します。
> 
> > ## skurita
> > 
> > 私も同様の問題を経験しています。
> > >
> > > ノートブックをダウンロードし、入力パスを変更し、サンプル(100)を削除しましたが、トレーニング/評価ロスは図のようにしか減少せず、主張されている0.937のCVロスよりも悪いです。
> > >
> > > "unsloth/gemma-2-9b-it-bnb-4bit"を4エポックでトレーニングし、各エポックの後にバリデーションを計算しました。
> > >
> > > このトレーニングノートブックから結果を再現する方法は誰か知っていますか？
> > >
> > > どんな洞察や提案も感謝されます。
> > >
> > > 著者[@emiz6413](https://www.kaggle.com/emiz6413)にも言及します。
> > >
> > > ありがとう。
> > >
> > > 

---

> ## S J Moudry
> > [@emiz6413](https://www.kaggle.com/emiz6413)  ノートブックの共有、ありがとうございます。非常に多くを学び、あなたの仕事に全て投票しました。いくつか質問があります：
> >
> > トレーニング後のモデルの評価精度はどのくらいでしたか？
> > トレーニングにはどのプラットフォームを使用していますか？ おすすめのものはありますか？
> > ハイパーパラメータの調整に関するヒントはありますか？ また、どのようにハイパーパラメータを決めましたか？ フルデータセットで何度もトレーニングしましたか、それとも効果を測るためにいくつかのテストランを行いましたか？
> >
> > > ## Eisuke MizutaniTopic Author
> > > 
> > > コメントありがとうございます。
> > > 
> > > - 1エポック後の評価セットのロスは0.9371でした。
> > > 
> > > - 私はpaperspaceを使用しています。合理的な固定価格で多くの実験を行いたい場合は、そのプラットフォームをお勧めします。
> > > 
> > > - ハイパーパラメータの調整にはあまり時間をかけておらず、gemma2のトレーニングには非常に長い時間がかかります。いくつかの実行を行い、学習曲線を見て、不可能そうなものは手動で中止しました。したがって、ハイパーパラメータを調整する余地はたくさんあると思います。
> > > 
> > > 

---

> ## Lorry Zou
> 
> なぜ以前公表したLlama3のトレーニングノートブックを再利用しなかったのですか？ モデル名/パスをllama3からgemma2に変更するだけでTPUで約6時間トレーニングできます。
> 
> > ## Eisuke MizutaniTopic Author
> > 
> > あなたが言っているのはkishanvavdaraのノートブックだと思います（私はトレーニングノートブックを発表していません）。
> > >
> > > 私はGPUでモデルをトレーニングしており、TPUで再現するかどうかは不明でした。 特に量子化はTPUではサポートされていません。 OOMが発生しない場合は量子化は必要ありませんが。
> > >
> > > あなたはTPUでgemma2をトレーニングすることができますか？
> > >
> > > > ## Lorry Zou
> > > > はい、私はKishanvavdaraのノートブックについて話していました。彼は以前に2つのGemma 2ノートブックを投稿しました[https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1](url)
> > > > アップロードされたデータセットとして"gemma-2-9b-hf"が含まれています。 TPUでトレーニングでき、早かったです。
> > > > 
> > > > > ## Eisuke MizutaniTopic Author
> > > > > > あなたはgemmaの最終隠れ状態の上にcatboost（または何かしらの）分類器をトレーニングしただけですか？
> > > > > > gemma2-9bの読み込みを試みるとOOMが発生し、量子化はTPUではサポートされていません。したがって、gemmaの重みを微調整するためにTPUの使用をあきらめざるを得ませんでした。
> > > > > > 

---

> ## Muhammad Haroon ul Hasnain
> 
> 素晴らしいノートブックと説明をありがとうございます。

---

> ## Mohamadreza Momeni
> 
> 素晴らしい仕事です。
> >
> > 親愛なる[@emiz6413](https://www.kaggle.com/emiz6413)への偉業です。

---

> ## floriandev
> 
> まず第一に[@emiz6413](https://www.kaggle.com/emiz6413)  このノートブックには大きな感謝を!!
> 
> 不幸にも、ファインチューニングの後、モデルをハギングフェイスにプッシュして、hfからのモデルを使用して推論ノートブックを呼び出す際に次のエラーが発生しました：
> 
> …このエラーが発生します…
> 
> > ## Lorry Zou
> > 
> > num_classes=3を指定しないと、デフォルト値は2になります。
> > 
> > > ## floriandev
> > > > こんにちはLorry、迅速な返信ありがとうございます!!
> > > > 
> > > > トレーニング/推論ノートブックは同じ著者のものです。変更を加えずにトレーニングに使用したため、精度やロスは問題なかったのですが、保存後に推論ノートブックに読み込むとエラーが出ます。
> > > > 
> > > > トレーニングされたモデルは、アーキテクチャ的に推論モデルと一致するはずです...うーん
> > > >
> > > > 推論ノートブックでクラス数をどのように導入するか、ということですが、新たにトレーニングされたモデルには1つの追加のパラメータがあるようです。
> > > > 
> > > > 教えてください、サポートの感謝します。
> > > > 
> > > > フロリアン。
> > > > 
> > > > > ## floriandev
> > > > > ああ、ローカルで生成されたチェックポイントを使用して（現在試しています）見つかりました...すぐに報告します！
> > > > 
> > > > > ## floriandev
> > > > > トレーニング出力がgemma_dirに使用されたため、問題が解決できます。
> > > > >
> > > > > 解決策：トレーニング出力をlora_dirとして使用しました。
> > > > >
> > > > 

---

> ## Nikhil Tumbde
> 
> ノートブックに感謝します。
> 
> fp16のトレーニング引数について質問がありますが、ファインチューニングの際にbf16を使用したらどうなりますか？ llama 3 8b（ベースモデル）で4ビットの量子化を使用したとき、数千ステップ後に勾配がNaNになりました。何か考えがありますか？

> > ## Eisuke MizutaniTopic Author
> > 
> > 私のデバイスで利用可能な場合は、bf16が一般的に安全なオプションだと思います。
> > 
> > 

---

> ## Vavilkin Alexander
> 
> こんにちは[@emiz6413](https://www.kaggle.com/emiz6413)! あなたのノートブックに感謝します、これはLLMのファインチューニングの実践に非常に役立ちます。モデル全体をどのように保存したか教えていただけますか？ "Trainer"で微調整する際、アダプターの重みしか保存されないと理解していますが、分類ヘッドの重みは保存されません。同時に推論ノートブックのモデルは通常の量子化モデルではなく、すでに分類用のヘッドが付いています。このことについてあなたからのコメントを非常に楽しみにしています。

> > ## Eisuke MizutaniTopic Author
> > 
> > 分類ヘッドはModulesToSaveWrapperとしてラップされているため、Trainerによって自動的に保存されます。
> > >
> > > チェックポイントからロードすると、トレーニングされた重みが分類ヘッドに読み込まれます。
> > >
> > > 

---

> ## Mattia Vanzetto
> 
> 質問してもいいですか？私はこれに対して初心者ですが、ノートブックが完了し出力が得られたら、出力を保存し、推論ノートブックで再読み込みする手順は何ですか？ ありがとうございます！

> > ## raconion
> > 
> > あなたのノートブックと同じレベルに出力フォルダがあるはずです。推論には最も大きな番号（ステップ）のサブフォルダを使用してください。この場合、5748であるべきです。
> > 
> > 

---

> ## raconion
> 
> こんにちは！ この素晴らしいノートブックをありがとう。私はH100*2でこのノートブックを実行した結果、あなたの結果では5748の代わりに11495ステップが出ました。また、cvスコアはやや悪化し0.964697です。
> 
> いくつかのコメントを読んだ後、バッチサイズに問題があると感じました。
> 
> 私の設定は次の通りです。
> 
> ```
> class Config:
>     output_dir: str = "output"  
>     checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4ビット量子化されたgemma-2-9b-instruct
>     max_length: int = 1024
>     n_splits: int = 5
>     fold_idx: int = 0
>     optim_type: str = "adamw_8bit"
>     per_device_train_batch_size: int = 2 
>     gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8
>     per_device_eval_batch_size: int = 8
>     n_epochs: int = 1
>     freeze_layers: int = 16  # 総レイヤ数は42、最初の16層にはアダプターを追加しない
>     lr: float = 2e-4
>     warmup_steps: int = 20
>     lora_r: int = 16
>     lora_alpha: float = lora_r * 2 
>     lora_dropout: float = 0.05
>     lora_bias: str = "none" 
> ```
> 
> そして、
> 
> ```
> training_args = TrainingArguments(
>     output_dir="output",
>     overwrite_output_dir=True,
>     report_to="none",
>     num_train_epochs=config.n_epochs,
>     per_device_train_batch_size=config.per_device_train_batch_size,
>     gradient_accumulation_steps=config.gradient_accumulation_steps,
>     per_device_eval_batch_size=config.per_device_eval_batch_size,
>     logging_steps=10,
>     eval_strategy="epoch",
>     save_strategy="steps",
>     save_steps=200,
>     optim=config.optim_type,
>     fp16=True,
>     learning_rate=config.lr,
>     warmup_steps=config.warmup_steps,
> )
> ```
> 
> モデルを実際にトレーニングする際に異なる設定を使用していますか？ ありがとう！

> > ## Eisuke MizutaniTopic Author
> > 
> > あなたのコードで両方のGPUが表示されていますか？
> > > 
> > 

> > > > ## raconion
> > > > ノートブックのコードを直接再利用しています。デバイスマップによるとモデルの並列化が使用されているようです。nvidia-smiも両方のGPUが使用されていることを示しています。
> > > > 
> > > > デバイスマップ：
> > > > 
> > > > ```
> > > > {'model.embed_tokens': 0, 'model.layers.0': 0, 'model.layers.1': 0, 'model.layers.2': 0, 'model.layers.3': 0, 'model.layers.4': 0, 'model.layers.5': 0, 'model.layers.6': 0, 'model.layers.7': 0, 'model.layers.8': 0, 'model.layers.9': 0, 'model.layers.10': 0, 'model.layers.11': 0, 'model.layers.12': 0, 'model.layers.13': 0, 'model.layers.14': 0, 'model.layers.15': 0, 'model.layers.16': 1, 'model.layers.17': 1, 'model.layers.18': 1, 'model.layers.19': 1, 'model.layers.20': 1, 'model.layers.21': 1, 'model.layers.22': 1, 'model.layers.23': 1, 'model.layers.24': 1, 'model.layers.25': 1, 'model.layers.26': 1, 'model.layers.27': 1, 'model.layers.28': 1, 'model.layers.29': 1, 'model.layers.30': 1, 'model.layers.31': 1, 'model.layers.32': 1, 'model.layers.33': 1, 'model.layers.34': 1, 'model.layers.35': 1, 'model.layers.36': 1, 'model.layers.37': 1, 'model.layers.38': 1, 'model.layers.39': 1, 'model.layers.40': 1, 'model.layers.41': 1, 'model.norm': 1, 'score': 1}
> > > > ```
> > > > 
> > > > もしこの場合、以下のように
> > > > 
> > > > ```
> > > > per_device_train_batch_size: int = 2
> > > > gradient_accumulation_steps: int = 2
> > > > ```
> > > > グローバルバッチサイズは4になります。トレーニング時にグローバルバッチサイズが8になるようにper_device_train_batch_sizeとgradient_accumulation_stepsを異なる値に設定しますか？
> > > > 
> > > > > ## Eisuke MizutaniTopic Author
> > > > > > [@raconion](https://www.kaggle.com/raconion) 
> > > > > > 応答が遅れてすみません。
> > > > > > 
> > > > > > グローバルバッチサイズが8になるようにper_device_train_batch_sizeとgradient_accumulation_stepsを設定しています。グローバルバッチサイズが4だとスコアが悪化しました。
> > > > > > 
> > > 

---

> ## HZM
> 
> こんにちはEisuke、Llama3などの他のllmを試しましたか？ それは私にはうまくいきませんでした。
> 
> > ## Eisuke MizutaniTopic Author
> > 
> > Llama3を試しましたが、約0.98になりました。
> > 
> > 

---

> ## Kim Kumuha
> 
> トレーニングにはどれくらいの時間がかかりましたか？

---

> ## yuanzhe zhou
> 
> こんにちは、ノートブックの1エポック実行後の結果はどうなりますか？

> > ## Eisuke MizutaniTopic Author
> > 
> > 評価セット(id % 5 == 0)でのロスは0.9371です。
> > >
> > > チェックポイントを使用して結果を再現できます[こちら](https://www.kaggle.com/datasets/emiz6413/73zap2gx/data)。
> > >
> > > 
> > 
---
```

** @@@ Jupyter Notebook numver 2, the number of votes :116 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、Kaggleのコンペティション「LMSYS - Chatbot Arena」において、ユーザーの好みに基づくチャットボット応答の優劣を予測するための推論プロセスの最適化を目指したものです。具体的には、LLM（大規模言語モデル）であるLlamaを使用し、応答確率を計算する際の推論速度を38%向上させています。

### 取り組んでいる問題
ノートブックは、主に次の問題に焦点を当てています:
1. **推論時間の短縮**: 元のスクリプトの65分から、40分に推論時間を短縮することに成功。
2. **動的パディングの実装**: 入力データの長さに基づいてバッチ内でパディングを動的に適用し、冗長なパディングを減少させています。
3. **入力シーケンスの最大長の設定**: モデルが長い入力に対処できるように、`max_length`を1024から2048に拡張。

### 使用している手法・ライブラリ
- **フレームワーク**: PyTorchが使用され、特にGPUリソースを活用したモデルの推論が行われています。
- **Transformersライブラリ**: Hugging FaceのTransformersライブラリを利用してLLMを読み込み、トークナイズ、モデルの推論を実施。
- **テストタイム拡張（TTA）**: モデルの出力を改善するために、異なる応答の順序を入れ替える手法も試されていますが、効果は限定的でした。
- **メモリ効率の良い注意機構**: メモリ使用量を削減するために、Memory-Efficient Attentionを有効化しています。

### 実装の流れ
1. **データの準備**: テストデータをトークナイズし、整形します。
2. **モデルの設定と読み込み**: 複数のGPUにモデルを読み込み、LoRA（Low-Rank Adaptation）による微調整を設定します。
3. **推論プロセス**: データフレームをバッチ処理し、各モデルによる出力を計算し、結果を結合して最終的な確率を算出します。
4. **結果の保存**: モデルによる勝者の確率を提出用CSVファイルに保存します。

このノートブックは、チャットボットの応答を評価するための効率的な推論手法の実践的なケーススタディを示しており、その中で最善のプラクティスが紹介されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 🦙🦙🦙 このノートブックの内容
このノートブックは、@kishanvavdaraによる[Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)を基に作成されています。リンク先のノートブックをまだ確認していない場合は、ぜひチェックして評価をお願いします。
私は、@kishanvavdaraの作業をもとにいくつかの改善を加えました：

### 38%速い推論
このスクリプトを使って、トレーニングセットの最初の10,000サンプルに対する推論時間は40分で、一方で元のスクリプトは65分かかるため、精度に影響を与えずに38%速くなっています。主に次の2つを追加しました：

#### 1. 動的パディング
すべての入力を事前に固定長にパディングする代わりに、各ミニバッチ内の最長シーケンスまで動的にパディングが適用されます。

#### 2. テストデータを入力長でソート
動的パディングの利点を最大限に活かすため、テストデータは入力長でソートされます。これにより、各ミニバッチ内の入力がほぼ同じ長さになり、冗長なパディングを減らすことができます。

### より長い入力シーケンス
訓練データの99%は1024以内に収まっていますが、残りの1%は収まっていません。さらに、テストセットにはより長いシーケンスが含まれる可能性があるため、`max_length`は可能な限り長く設定する方が安全だと思います。
`max_length`を1024から1280に変更することでLBが0.989から0.983に改善されました。

## 試してみたが効果がなかったこと

### テストタイム拡張（TTA）
私は、response_aとresponse_bの順序を入れ替えるシンプルなTTAを試しました。これにより、サンプルごとにモデルが2回呼び出されるため推論時間が2倍になります。
二つのソフトマックス確率を平均化するか、二つのロジットを平均化してからソフトマックス確率を計算できます。両方のアプローチではLBが改善されませんでしたが、ソフトマックスの平均化がより良い結果を示しました。
TTAはサンプルごとにモデルを2回呼び出すため、推論時間が2倍になります。`max_length=1280`とTTAを有効にして提出が完了しましたが、効率的な推論により9時間以内に収まりました。

### 各入力を切り詰める
元の実装では、プロンプト + response_a + response_bという連結シーケンスが切り詰められます。直截的な切り詰めを適用すると、一部の（稀ではありますが）プロンプトが1280トークンを超えるため、モデルは勝者をランダムに推測するしかなくなります。
私は、最初に各入力を固定長に切り詰めてから、三つを連結しようとしましたが、LBは改善されませんでした。

## 🆕 バージョン4の更新
効果的な推論のおかげで入力シーケンスの長さを増やす時間が十分にあるため、`max_length`を2048に変更しました。また、ミニバッチサイズは8から4に減少させました。
さらに、メモリ使用量を減らすために[Memory-Efficient Attention](https://github.com/facebookresearch/xformers)を有効にしました。
これによりLBが0.983から0.979に改善され、提出はTTAなしで4時間未満で済みました。
ミニバッチサイズを1に減らすことでさらに長くできますが、まだテストしていません。

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
PyTorchの[ドキュメント](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention)によると、`scaled_dot_product_attention`は、自動的に最も最適な実装を選択します：
1. Flash Attention
2. メモリ効率の良い注意機構
3. PyTorchの（ナイーブな）実装

デフォルトでは、これらすべてが有効ですが、特定のバックエンドを手動で有効化/無効化することもできます。
```

---The following area is a Code cell (cell numver is 5)---
```python
assert torch.cuda.device_count() == 2, "申し訳ありませんが、マルチGPUが必要です！"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Flash AttentionはT4/P100をサポートしていないため、効果はありません。
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルの名前
    weights_path = '/kaggle/input/lmsys-model/model'  # 重みのパス
    max_length = 2048  # 最大長
    batch_size = 4  # バッチサイズ
    device = torch.device("cuda")  # 使用するデバイス
    tta = False  # テストタイム拡張。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # max_length//3を各入力に適用するか、連結入力にmax_lengthを適用するか

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# リスト内の文字列を連結する関数
def process(input_str):
    stripped_str = input_str.strip('[]')  # 文字列の先頭と末尾の角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文字列を分割してトリミング
    return  ' '.join(sentences)  # 文を結合して返す

test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'列を処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'列を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'列を処理

display(test.head(5))  # 上位5行を表示
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]  # ユーザーのプロンプトを整形
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]  # Model Aの応答を整形
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]  # Model Bの応答を整形
    if spread_max_length:
        # max_lengthを各入力に均等に分配する場合
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 入力IDの生成
        attention_mask = [[1]* len(i) for i in input_ids]  # 注意マスクの生成
    else:
        # max_lengthを全体の入力に適用する場合
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイザでトークナイズ
        input_ids = tokenized.input_ids  # 入力IDを取得
        attention_mask = tokenized.attention_mask  # 注意マスクを取得
    return input_ids, attention_mask  # 入力IDと注意マスクを返す
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーを読み込み

data = pd.DataFrame()
data["id"] = test["id"]  # ID列を作成
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークナイズを適用
data["length"] = data["input_ids"].apply(len)  # 入力IDの長さを計算

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]  # ID列を作成
# response_aとresponse_bを入れ替える
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)  # 入力IDの長さを計算
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # 最初の入力IDをデコードして表示
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  # 最初の入れ替えた入力IDをデコードして表示
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# モデルの読み込み 
各GPUに1モデルを読み込みます。
```

---The following area is a Code cell (cell numver is 15)---
```python
# BitsAndBytesの設定
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,  # モデルを8ビットで読み込む設定
    bnb_8bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定
    bnb_8bit_use_double_quant=False,  # 二重量子化を無効にする設定
)

# GPU 0にベースモデルを読み込む
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:0')  # GPU 0に割り当て
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定

# GPU 1にベースモデルを読み込む
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:1')  # GPU 1に割り当て
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 重みの読み込み
```

---The following area is a Code cell (cell numver is 17)---
```python
# LoRAの設定
peft_config = LoraConfig(
    r=16,  # 縮小次元
    lora_alpha=32,  # LoRAのスケーリング係数
    lora_dropout=0.10,  # ドロップアウト率
    bias='none',  # バイアスの設定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクの種類
    target_modules=['o_proj', 'v_proj']  # 対象モジュール
)
```

---The following area is a Code cell (cell numver is 18)---
```python
# PEFTの取得
model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTモデルを取得してGPU 0に配置
# 重みを読み込む
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()  # 評価モードに設定

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTモデルを取得してGPU 1に配置
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを読み込む
model_1.eval()  # 評価モードに設定
```

---The following area is a Code cell (cell numver is 19)---
```python
# 学習可能なパラメータ
model_0.print_trainable_parameters()  # モデル0の学習可能なパラメータを表示
model_1.print_trainable_parameters()  # モデル1の学習可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 21)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝ちと引き分けのカウントを初期化
    
    # バッチサイズに基づいてデータを分割して推論を行う
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを決定
        tmp = df.iloc[start_idx:end_idx]  # データフレームのサブセットを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # 注意マスクをリストに変換
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最長の長さでパディング
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchテンソルとして返す
        )
        outputs = model(**inputs.to(device))  # モデルに入力を渡して出力を計算
        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックス関数で処理
        
        a_win.extend(proba[:, 0].tolist())  # Model Aの勝ちの確率を追加
        b_win.extend(proba[:, 1].tolist())  # Model Bの勝ちの確率を追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加
    
    df["winner_model_a"] = a_win  # データフレームにModel Aの勝ちの確率を追加
    df["winner_model_b"] = b_win  # データフレームにModel Bの勝ちの確率を追加
    df["winner_tie"] = tie  # データフレームに引き分けの確率を追加
    
    return df  # 結果のデータフレームを返す
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

# 动的パディングを最大限に活用するために入力長でソート
data = data.sort_values("length", ascending=False)
# sub_1とsub_2のトークン数がほぼ同じである必要があります
sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー
sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー

with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝者の確率を取得

print(f"経過時間: {time.time() - st}")  # 経過時間を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # 入力長でソートして速度を向上させる
    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー
    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー

    with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行

    tta_result_df = pd.concat(list(results), axis=0)  # 結果を結合
    # TTAの結果は順序が逆であることに注意
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values  # TTAの確率を取得
    # 元の結果とTTAの結果を平均化する
    proba = (proba + tta_proba) / 2  # 確率を平均化

print(f"経過時間: {time.time() - st}")  # 経過時間を表示
```

---The following area is a Code cell (cell numver is 24)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # データフレームにModel Aの勝ちの確率を追加
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # データフレームにModel Bの勝ちの確率を追加
result_df.loc[:, "winner_tie"] = proba[:, 2]  # データフレームに引き分けの確率を追加
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルに書き出し
display(submission_df)  # 提出データを表示
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
---

# コメント 

> ## Crystal Veil
> 
> 優れたLLMファインチューニングコード
> 
> 

---

> ## Zac Wing
> 
> 印象的な作業です！
> 
> 

---

> ## toolman
> 
> ありがとう、素晴らしい作品です
> 
> 

---

> ## Matt McDonagh
> 
> 動的パディングがあることすら知らなかった。
> 
> 新しいトリックを学ぶことが好きです。大きなものもあれば、小さな工夫もあります。それらはすべて私たちをより良くします。
> 
> この作業を共有してくれてありがとう。
> 
> 

---

> ## Kishan Vavdara
> 
> 素晴らしい仕事[@emiz6413](https://www.kaggle.com/emiz6413)！ありがとうございます、これが必要でした！
> 
> 

---

> ## jiangli59
> 
> メモリ効率の良い注意機構を有効にする方法について教えてもらえますか？
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私が行ったのは、以下のコードを呼び出してメモリ効率の良い注意機構を有効にするためのグローバルフラグを設定することでした。
> > 
> > ```
> > torch.backends.cuda.enable_mem_efficient_sdp(True)
> > 
> > ```
> > 
> > 注意してください、mem_efficient_sdpはデフォルトで有効になっているので、これは冗長です。
> > 
> > 

---

> ## Lorry Zou
> 
> 素晴らしい仕事[@emiz6413](https://www.kaggle.com/emiz6413)！モデルのトレーニングについて少し共有してもらえますか？Kaggleでトレーニングしましたか、それとも別のプラットフォームで？どれくらいの時間がかかり、いくつかのハイパーパラメータの工夫についても教えていただけますか？ありがとうございます！
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > [@kishanvavdara](https://www.kaggle.com/kishanvavdara)の別の[ノートブック](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train)でモデルをトレーニングしました。50%のトレーニングデータを使用し、Kaggle TPUを使って約2時間かかります。
> > 
> > 
> > 
> > > ## Lorry Zou
> > > 
> > > お返事ありがとうございます！つまり、ファインチューニングせずに[@kishanvavdara](https://www.kaggle.com/kishanvavdara)のモデルの重みを使用して推論したのですか？
> > > 
> > > 
> > > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > はい、その通りです。
> > > 
> > > 
> > > 

---

> ## bao
> 
> 共有してくれてありがとう。質問がありますが、なぜトークナイザを呼び出すときにpadding=Trueを使用せず、pad_without_fast_tokenizer_warningを使用してパディングするのですか？
> 
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > それが動的パディングを行っている部分です。
> > 
> >
```

** @@@ Jupyter Notebook numver 3, the number of votes :97 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」における、LLM（大規模言語モデル）34Bを用いた問題解決に取り組んでいます。具体的には、モデルの推論を5時間で完了させる方法を示しています。以下に、ノートブックの主な内容と使用手法について要約します。

### 取り組んでいる問題
- **迅速なモデル提出**: 大規模言語モデルを短時間で提出する方法を求めています。具体的には、LLM 34Bモデルを用いて、提出の際に求められるレスポンスを迅速に生成することを目的としています。

### 使用手法・ライブラリ
1. **vLLM**: 高速なLLM推論ライブラリであり、推論速度を向上させるために使用されています。環境によってはエラーが出るため、再インストールが推奨されています。

2. **AWQ（Adaptive Weight Quantization）**: 4ビット量子化を用いることで、GPUのVRAMを効率的に使用する手法です。これにより、大きなモデルをメモリ制約のある環境で実行可能にしています。

3. **トークン制限**: 
   - **入力サイズ**: 最大1024トークンに制限し、速度を向上させます。
   - **出力サイズ**: 生成される応答のトークンを1トークンに制限することで、処理速度を加速させています。

4. **トークナイザー**: モデルに最適化されたトークナイザーを使用し、入力を整形します。

5. **プロンプト工夫**: モデルが出力するトークンを制御し、特定のトークン(A、B、tie)に対して予測を強化するためのロジットプロセッサーを定義しています。これにより、ユーザーが選択するべき応答を促進します。

6. **CSVファイルの生成**: 提出用のCSVファイルを作成するプロセスが含まれており、推論結果を適切な形式で出力します。

7. **CVスコア計算**: 提出を通じて、交差検証スコア（CVスコア）を計算し、結果の性能を評価します。

このノートブックは、モデルのトレーニングや微調整に加え、推論速度や精度を管理するための細やかな配慮がなされており、特にリソースの限られた環境での実用的なアプローチを提示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LLM 34Bモデルを5時間で提出する方法！
このノートブックでは、わずか5時間でLLM 34Bモデルを提出する方法を示します！すごいですね！主なポイントは以下の通りです：
* 速度のためにvLLMを使用する
* GPUのVRAM不足を避けるためにAWQ 4ビット量子化を使用する
* 入力サイズを1024トークンに制限する（速度向上のため）
* 出力サイズを1トークンに制限する（速度向上のため）

# vLLMのインストール
vLLMパッケージは非常に高速なLLM推論ライブラリです！KaggleノートブックにインストールされているvLLMではエラーが発生するため、再インストールが必要です。以下のコードはノートブック[こちら][1]から取得したものです。

[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution
```

---The following area is a Code cell (cell numver is 2)---
```python
import os, math, numpy as np  # osモジュール、mathモジュール、numpyライブラリをインポートします。

# 環境変数CUDA_VISIBLE_DEVICESを設定します。
# これにより、どのGPUデバイスを使用するかを指定できます。
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"  # 使用するGPUデバイスとして0番と1番を指定します。
```

---The following area is a Code cell (cell numver is 3)---
```python
%%time  # このセルの実行にかかる時間を計測します。

# torchパッケージをアンインストールします。
# -yオプションを使用して、確認なしでアンインストールを行います。
!pip uninstall -y torch  

# vllmパッケージをアップグレードしてインストールします。
# --no-indexオプションを使用してPyPIからのインストールを無効にし、
# --find-linksオプションで指定したディレクトリからパッケージを探します。
!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm  

# grpcioの特定バージョンをアップグレードしてインストールします。
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  

# rayの特定バージョンをアップグレードしてインストールします。
!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# vLLMを使って34B量子化モデルをロードする！
LLM 34B Bagelモデルを[こちら][1]からロードして使用します。これは非常に強力なモデルです。

[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2
```

---The following area is a Code cell (cell numver is 5)---
```python
import vllm  # vLLMライブラリをインポートします。

# LLMオブジェクトを作成します。
llm = vllm.LLM(
    "/kaggle/input/bagel-v3-343",  # モデルのパスを指定します。
    quantization="awq",  # AWQ量子化を使用します。
    tensor_parallel_size=2,  # テンソル並列処理のサイズを2に設定します。
    gpu_memory_utilization=0.95,  # GPUメモリの使用率を95%に設定します。
    trust_remote_code=True,  # リモートコードを信頼する設定です。
    dtype="half",  # データ型を半精度に設定します（メモリ効率を向上します）。
    enforce_eager=True,  # イージー実行を強制します。
    max_model_len=1024,  # モデルの最大長を1024トークンに設定します。
    #distributed_executor_backend="ray",  #（コメントアウト）分散実行のバックエンドをRayに設定します。
)

# トークナイザーを取得します。
tokenizer = llm.get_tokenizer()  # モデルに関連付けられたトークナイザーを取得します。
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# テストデータをロードする
**コミット**の際にはCVスコアを計算するために128行のトレーニングデータをロードします。**提出**の際にはテストデータをロードします。
```

---The following area is a Code cell (cell numver is 7)---
```python
import pandas as pd  # pandasライブラリをインポートします。

VALIDATE = 128  # バリデーション用の行数を128に設定します。

# テストデータをCSVファイルから読み込みます。
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv") 

# 読み込んだテストデータの行数が3の場合、
# トレーニングデータを読み込み、最初の128行を取得します。
if len(test) == 3:
    test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
    test = test.iloc[:VALIDATE]  # 最初の128行を取得します。

# テストデータの形状を出力します。
print(test.shape)  # テストデータの行数と列数を表示します。

# テストデータの最初の1行を表示します。
test.head(1)  # テストデータの最初の1行を表示します。
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# プロンプトの工夫
ゼロショットLLMを提出したい場合、CVスコアを改善するためにさまざまなシステムプロンプトを試す必要があります。モデルをファインチューニングする場合、システムプロンプトはそれほど重要ではなくなります。なぜなら、モデルはターゲットから何をすべきかを学ぶため、どのシステムプロンプトを使用しても影響を受けないからです。

ロジットプロセッサを使用して、モデルが私たちが興味を持っている3つのトークンを出力するよう強制します。
```

---The following area is a Code cell (cell numver is 9)---
```python
from typing import Any, Dict, List  # 必要な型をインポートします。
from transformers import LogitsProcessor  # LogitsProcessorクラスをインポートします。
import torch  # PyTorchライブラリをインポートします。

choices = ["A", "B", "tie"]  # 選択肢を定義します。

KEEP = []  # 出力トークンとして保持するトークンIDのリストを初期化します。
for x in choices:
    c = tokenizer.encode(x, add_special_tokens=False)[0]  # トークンをエンコードし、リストから最初の要素を取得します。
    KEEP.append(c)  # 取得したトークンIDをKEEPリストに追加します。
print(f"Force predictions to be tokens {KEEP} which are {choices}.")  # どのトークンを強制するか出力します。

class DigitLogitsProcessor(LogitsProcessor):
    def __init__(self, tokenizer):
        self.allowed_ids = KEEP  # 許可されたトークンIDを初期化します。
        
    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:
        scores[self.allowed_ids] += 100  # 許可されたトークンIDのスコアを100増加させます。
        return scores  # 修正されたスコアを返します。
```

---The following area is a Code cell (cell numver is 10)---
```python
sys_prompt = """以下のプロンプトと2つの応答を読み、それぞれの応答がどちらが優れているかを判断してください。
もし応答が比較的新しい場合は、「tie」と返信してください。それ以外の場合は、「A」または「B」と応答し、どちらが優れているかを示してください。"""
```

---The following area is a Code cell (cell numver is 11)---
```python
SS = "#"*25 + "\n"  # "#"を25個連結し、改行を追加した文字列をSSに設定します。
```

---The following area is a Code cell (cell numver is 12)---
```python
all_prompts = []  # プロンプトを保存するリストを初期化します。
for index, row in test.iterrows():  # テストデータの各行をイテレートします。
    
    # プロンプト、応答A、応答Bを取得し、nullを空文字に置き換えます。
    a = " ".join(eval(row.prompt, {"null": ""}))  # プロンプトを評価し、nullを空文字に置き換えた後、スペースで結合します。
    b = " ".join(eval(row.response_a, {"null": ""}))  # 応答Aを評価・結合します。
    c = " ".join(eval(row.response_b, {"null": ""}))  # 応答Bを評価・結合します。
    
    # フォーマットされたプロンプトを作成します。
    prompt = f"{SS}PROMPT: " + a + f"\n\n{SS}RESPONSE A: " + b + f"\n\n{SS}RESPONSE B: " + c + "\n\n"
    
    # システムプロンプトとフォーマットされたプロンプトを結合します。
    formatted_sample = sys_prompt + "\n\n" + prompt
    
    # フォーマットされたサンプルをリストに追加します。
    all_prompts.append(formatted_sample)  # フォーマットされたサンプルをall_promptsリストに追加します。
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# テストを推論する
高速なvLLMを使用してテストを推論します。vLLMに対し、最初のトークンで予測される上位5つのトークンの確率を出力するように依頼します。また、推論速度を向上させるために、予測を1トークンに制限します。

128のトレーニングサンプルを推論するのにかかる時間に基づいて、25,000のテストサンプルを推論するのにかかる時間を推測できます。
```

---The following area is a Code cell (cell numver is 14)---
```python
%%time  # このセルの実行にかかる時間を計測します。

from time import time  # timeモジュールからtime関数をインポートします。
start = time()  # 実行開始時刻を記録します。

# ロジットプロセッサを定義します。
logits_processors = [DigitLogitsProcessor(tokenizer)]
# vLLMを使用して応答を生成します。
responses = llm.generate(
    all_prompts,  # すべてのプロンプトを指定します。
    vllm.SamplingParams(
        n=1,  # 各プロンプトに対して返す出力シーケンスの数。
        top_p=0.9,  # 上位トークンを考慮する際の累積確率を制御する浮動小数点数。
        temperature=0,  # サンプリングのランダム性。
        seed=777,  # 再現性のためのシード。
        skip_special_tokens=True,  # 出力で特殊トークンをスキップするかどうか。
        max_tokens=1,  # 各出力シーケンスに生成する最大トークン数。
        logits_processors=logits_processors,  # 使用するロジットプロセッサ。
        logprobs=5  # 上位5つのトークン確率を出力。
    ),
    use_tqdm=True  # プログレスバーを表示するためのオプション。
)

end = time()  # 実行終了時刻を記録します。
elapsed = (end - start) / 60.  # 経過時間を分単位で計算します。
print(f"{VALIDATE}サンプルの推論に{elapsed}分かかりました！")  # 推論にかかった時間を出力します。
```

---The following area is a Code cell (cell numver is 15)---
```python
submit = 25_000 / 128 * elapsed / 60  # 25,000サンプルの推論にかかる時間を計算します。
print(f"提出には{submit}時間かかります")  # 推論にかかる時間を出力します。
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 推論確率の抽出
これからvLLMの予測から「A」、「B」、「tie」の確率を抽出します。
```

---The following area is a Code cell (cell numver is 17)---
```python
results = []  # 推論結果を格納するリストを初期化します。
errors = 0  # エラーのカウンタを初期化します。

for i, response in enumerate(responses):  # レスポンスの各要素をイテレートします。
    try:
        x = response.outputs[0].logprobs[0]  # 最初の出力のロジットを取得します。
        logprobs = []  # ログ確率を格納するリストを初期化します。
        for k in KEEP:  # KEEPに含まれる各トークンIDについて
            if k in x:  # トークンIDがログ確率に存在する場合
                logprobs.append(math.exp(x[k].logprob))  # ログ確率の指数を計算して追加します。
            else:
                logprobs.append(0)  # トークンIDが存在しない場合は0を追加します。
                print(f"bad logits {i}")  # 不正なロジットが見つかったことを記録します。
        logprobs = np.array(logprobs)  # ログ確率リストをNumPy配列に変換します。
        logprobs /= logprobs.sum()  # 確率を正規化します。
        results.append(logprobs)  # 結果リストに追加します。
    except:  # エラーが発生した場合
        # print(f"error {i}")  # エラーを記録（コメントアウトされています）。
        results.append(np.array([1/3., 1/3., 1/3.]))  # 一様分布の確率を追加します。
        errors += 1  # エラーカウンタを増やします。
        
print(f"{i+1}回の推論のうちエラーは{errors}回発生しました。")  # エラーの総数を出力します。
results = np.vstack(results)  # 結果を垂直スタックして2次元配列を作成します。
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# 提出用CSVを作成する
```

---The following area is a Code cell (cell numver is 19)---
```python
sub = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")  # サンプル提出CSVファイルを読み込みます。

# テストデータの長さがVALIDATEでない場合、結果をサブミッション用データフレームに代入します。
if len(test) != VALIDATE:
    sub[["winner_model_a", "winner_model_b", "winner_tie"]] = results  # 結果を対応する列に割り当てます。
    
sub.to_csv("submission.csv", index=False)  # 提出用CSVを作成します。インデックスは出力しません。
sub.head()  # サブミッションデータフレームの最初の数行を表示します。
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# CVスコアを計算する
```

---The following area is a Code cell (cell numver is 21)---
```python
if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合
    true = test[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # 真のラベルを取得します。
    print(true.shape)  # 真のラベルの形状を出力します。
```

---The following area is a Code cell (cell numver is 22)---
```python
if len(test) == VALIDATE:  # テストデータの長さがVALIDATEと等しい場合
    from sklearn.metrics import log_loss  # log_loss関数をインポートします。
    print(f"CV loglossは {log_loss(true, results)} です")  # CVのログ損失を計算して出力します。
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# コメント

> ## Cody_Null
> 
> すごい仕事ですね、クリス。vLLMのパフォーマンス低下について何か知っていますか？私は何も知らないので、他のノートブックでこれを使う際の期待値を知りたいと思っています。
> 
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > vLLMによるパフォーマンス低下はないと思います。vLLMパッケージは単に私たちのモデルを実行するだけです。選択するオプション（vllm.SamplingParams、量子化の有無、量子化の種類、max_model_length、トークナイザの切り捨てなど）がパフォーマンスに影響を与えます。
> > 
> > 
> > 
> > > ## Cody_Null
> > > 
> > > すばらしい！ONNXに似ているのか、正確に私が必要としているものなのかわからなかったです（笑）。
> > > 
> > > 
> > > 
> > > ## Chris Deotte（トピック作成者）
> > > 
> > > モデルの変換（ONNXのような）は必要ありません。Hugging Faceから非量子化モデルと量子化モデルの両方を直接取得し、推論を実行できます。（つまり、モデルは全く変更されません）。
> > > 
> > > 

---

> ## SeshuRaju 🧘‍♂️
> 
> [@cdeotte](https://www.kaggle.com/cdeotte) ありがとうございます。
> 
> このモデルはSFTまたはDPOでファインチューニングされていますか？
> 
> ファインチューニングにはどれほどのGPUが必要ですか？
> 
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > このモデルはSFTでファインチューニングされています。選択したLoRAランクパラメータr、max_model_len、バッチサイズ、およびトレーニングデータの量に応じて、任意のGPU数でファインチューニングできます。
> > 
> > ファインチューニング中に、34Bを4ビットに量子化し、サイズを20GBに削減します。したがって、要件は、GPUの総VRAMがトレーニングに十分な20GBを超えることです。Kaggleの2xT4（合計VRAM 32GB）上で、上記のパラメータを減らせばファインチューニングできると思います。
> > 
> > 
> > 
> > > ## SeshuRaju 🧘‍♂️
> > > 
> > > 限られたKaggle GPU時間のため、16GB VRAMで訓練しようとしています。バッチ=1、r=3、max_model_len=1024で訓練する他の方法を探しています。
> > > 
> > > 
> > > > ## Chris Deotte（トピック作成者）
> > > > 
> > > > 16GB VRAMで34B LLMをファインチューニングする方法はわかりません。最小要件は32GB VRAMに近いと思います。効率的なファインチューニングに関する詳細を説明した素敵なYouTubeビデオがあります [こちら](https://www.youtube.com/watch?v=XpoKB3usmKc) をご覧ください。ファインチューニング中には、次のメモリが必要です：
> > > > 
> > > > - モデルの重み
> > > > 
> > > > - 勾配
> > > > 
> > > > - オプティマイザ
> > > > 
> > > > メモリを最も削減するには、4ビット量子化されたモデルをロード（QLoRAを使用してモデル重みのメモリ使用量を削減）し、llm_int8_enable_fp32_cpu_offloadを使用してGPUとCPUメモリ間でモデルの重みをスワップできるようにし、PEFTを使用して少ないrパラメータ（QLoRA）でファインチューニングし、勾配チェックポイントを使用して（勾配メモリ使用量を削減）、paged_adam_8bitオプティマイザを使用します（小さなバッチと小さな最大トークン長で）。このオプティマイザは、必要に応じてGPUからCPUに保存された変数をスワップし、オプティマイザ変数のサイズを削減するために8ビットを使用します。
> > > 
> > > 

---

> ## ano
> 
> この素晴らしいモデルを共有していただきありがとうございます！トレーニングに使用したデータセットについて教えていただけますか？トレーニングデータセット全体を使用しましたか、それともその一部ですか？または外部データセットを使用しましたか？
> 
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > このモデルはコンペティションデータの80%でファインチューニングしました。5行ごとにすべてのデータを除外しました。したがって、正しい検証は以下のデータを使用することです pd.read_csv("train.csv").iloc[0::5]。現在、このノートブックでは pd.read_csv("train.csv").iloc[:128] を使用していますが、より正確な迅速な検証は 
> > 
> > ```
> > VALIDATE = 128
> > test = test.iloc[0:VALIDATE*5:5]
> > 
> > ```
> > 
> > その後、最初のインデックス % 5 == 0 のサンプル128を使用します。
> > 
> > 

---

> ## JM
> 
> 提出段階で実際に5時間かかっていますか？私のはずっとオーバーしています。
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > いいえ、私は5時間かかっていません。私のは8から9時間かかっています。このノートブックの別のバージョンをリリースするときに、コードセル#10の時間推定コードを修正します（「推論エラー」を削除することが含まれます）。また、導入文を「5時間」ではなく「9時間未満」と更新します。
> > 
> > 
> > 
> > > ## ano
> > > 
> > > 推論時間の推定（5時間）が実際のもの（8〜9時間）と異なる理由に心当たりはありますか？私は1000サンプルの推論を試してみましたが、25000サンプルのための推定時間（5-6時間）は似たようなものでした。
> > > 
> > > 

---

> ## Luan Ngo Dinh
> 
> こんにちは、GPTQの量子化結果がAWQとどう比較されるか、またKaggleでの可行性についてお尋ねしますか？
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > GPTQを使用するためには、vLLMパラメータで量子化="gptq"を設定し、事前にモデルをGPTQとして保存します。私はKaggleで試したことはありませんが、過去にT4 GPUでAWQとGPTQをどちらも成功裏に利用してきました。それぞれの精度は基本的に似ています。また、推論時間も似ています。
> > 
> > 
> > 
> > > ## Akhila datta dola
> > > 
> > > 共有していただきありがとうございます！
> > > 
> > > vLLMはbitsandbytesのようなオンザフライ4ビット量子化をサポートしていますか？一般的にgptqとawqと比較してどうですか？
> > > 
> > > 

---

> ## Luan Ngo Dinh
> 
> すばらしい共有をありがとうございます!!!
> 
> 推論プロセスで「128回中33回の推論エラー」が発生した理由を教えていただけますか？
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > 128の入力テキストのうち33回は、モデルが生成テキストを予測できませんでした。したがって、これらの33回には確率を抽出するためのトークンがありません。私たちのtry/exceptコードブロックがこれをキャッチし、これらのケースでは[1/3, 1/3, 1/3]を予測します。
> > 
> > 
> > 
> > > ## Luan Ngo Dinh
> > > 
> > > モデルはファインチューニングされていても、トークン「a」、「b」、「tie」を生成しなかったという意味ですか？
> > > 
> > > 
> > > 
> > > ## Chris Deotte（トピック作成者）
> > > 
> > > はい。問題は、モデルをmax_model_len=1024に切り捨てたため（速度のため）、入力テキストが1024を超えるとvLLMが1024に切り捨てられ、生成されたテキストを出力するための余地がなくなるからです。よりスマートなプロンプトエンジニアリングまたは切り捨て戦略を使用することで、これらの推論エラーを回避できます。
> > > 
> > > 
> > > > ## floriandev
> > > > 
> > > > プロンプトエンジニアリングまたは切り捨て戦略に取り組んでいます。正しい方向に進んでいるかもしれないと聞いてうれしいです;-)
> > > > 

---

> ## Xinian Guo
> 
> こんにちは、このモデルはコンペティションデータでファインチューニングされていますか？
> 
> > ## Chris Deotte（トピック作成者）
> > 
> > はい。このモデル（ノートブックバージョン8）はコンペデータでファインチューニングされています。ノートブックバージョン6 [こちら](https://www.kaggle.com/code/cdeotte/infer-34b-with-vllm?scriptVersionId=188642633) はファインチューニングなしのゼロショットです。
> > 
> > 
> > 
> > > ## yechenzhi1
> > > 
> > > こんにちは、ファインチューニングされたバージョンのLBスコアは何ですか？
> > > 
> > > 
> > > > ## floriandev
> > > > 
> > > > 私はそのノートブックで0.972を得ました。頑張ってください;-)
> > > > 

---

> ## Qihang Wang
> 
> こんにちは、クリス、あなたのプロセスを確認したいです：
> 
> CMIIW
> 
> qloraファインチューニング4ビットモデル？
> 
> qloraをモデルにマージ？
> 
> 4ビットを変換？
> 
> AWQ量子化
> 
> vLLMを使用して推論
> 
> 私は最初の3つのステップにあまり慣れていないので、これがあなたのやり方かどうかは確信がありません。
> 
> もう少し詳しく説明していただけますか？
```

** @@@ Jupyter Notebook numver 4, the number of votes :60 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、人間の好みを予測するためのモデルを訓練し、その推論を行うプロセスを示しています。

### 取り組んでいる問題
Notebookは、異なる大規模言語モデル (LLM) によって生成された応答がユーザーによってどのように評価されるかを予測することに取り組んでいます。具体的には、2つのチャットボットの応答のいずれがユーザーに好まれるか（モデルA、モデルB、または引き分け）の確率を予測するためのフレームワークを構築しています。

### 使用されている手法
1. **トークン化**: ユーザーのプロンプトとチャットボットの応答をトークン化して、モデルが理解できる形式に変換しています。`AutoTokenizer`を使用して、応答を適切にトークン化し、長さ制限を管理します。

2. **モデルの設定**: `LlamaForCausalLM`と、特定のタスク用にカスタマイズされた`Llama3ForSFT`というメソッドを利用して、大規模言語モデルのインスタンスを作成しています。

3. **並列処理**: GPUを2つ使用して、トレーニングと推論を効率的に行うために、`ThreadPoolExecutor`を使用して推論の並列処理を行っています。

4. **ソフトマックス関数の使用**: モデル出力のロジットをソフトマックス関数で正規化し、各モデルの勝確率を得ています。

### 使用されているライブラリ
- **Transformers**: モデルの構築やトークナイザー管理のために使用されています。
- **Datasets**: データフレームを統合し、モデルへの入力データセットを構築するために使用されています。
- **Torch**: 深層学習モデルの訓練と推論のために使用されています。特に、`torch.cuda.amp`で自動混合精度を有効化し、GPUメモリを効率的に活用しています。
- **Pandas**: データの操作や処理に使用されています。

最後に、Notebookは、予測結果を含むCSVファイル（`submission.csv`）として保存し、提出用に整形しています。このノートブックは、事前トレーニングされたモデルを用いた構築・推論手法を示しており、Kaggleのコンペティションルールに従ったデータ提出を実現しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# 注意
- [トレーニングスクリプト](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U transformers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U tokenizers --no-index --find-links /kaggle/input/llm-pip-2024727
!pip install -q -U peft --no-index --find-links /kaggle/input/llm-pip-2024727
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
import torch
import sklearn
import numpy as np
import pandas as pd
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed

# メモリ効率の良いSDP（スパースデータパラメータ）を有効化
torch.backends.cuda.enable_mem_efficient_sdp(True)
# フラッシュSDPを有効化
torch.backends.cuda.enable_flash_sdp(True)
# 使用するGPUの数が2であることを確認
assert torch.cuda.device_count() == 2, "申し訳ありませんが、マルチGPUが必要です！"
```

---The following area is a Code cell (cell numver is 4)---
```python
MODEL_NAME = '/kaggle/input/llama-3-1-8b-instruct-bnb-4bit'  # 使用するモデルの名前
WEIGHTS_PATH = '/kaggle/input/sft-llama3-1-lora-9174'          # 重みのパス
MAX_LENGTH = 2400   # 最大長さ
BATCH_SIZE = 2      # バッチサイズ
DEVICE = torch.device("cuda")  # デバイスをGPUに設定
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVから読み込む

def tokenize(example, tokenizer):
    # プロンプトと応答をトークン化
    prompts = tokenizer(eval(example['prompt'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_a = tokenizer(eval(example['response_a'], {"null": ""}), add_special_tokens=False)["input_ids"]
    responses_b = tokenizer(eval(example['response_b'], {"null": ""}), add_special_tokens=False)["input_ids"]
    
    # プロンプトと応答の長さが一致することを確認
    assert len(prompts) == len(responses_a) == len(responses_b), "プロンプト、応答A、応答Bの長さが一致しません"

    # プロンプトと応答を逆順にする
    prompts, responses_a, responses_b = prompts[::-1], responses_a[::-1], responses_b[::-1]

    prompt, response_a, response_b = [], [], []
    p_len, a_len, b_len = 0, 0, 0

    # プロンプトと応答を長さ制限に基づいて追加
    for p, a, b in zip(prompts, responses_a, responses_b):
        prompt.append(p)
        response_a.append(a)
        response_b.append(b)
        p_len += len(p)
        a_len += len(a)
        b_len += len(b)
        
        if p_len + a_len + b_len > MAX_LENGTH:  # 制限を超えた場合
            break

    # 順序を戻す
    prompt = [item for sublist in reversed(prompt) for item in sublist]
    response_a = [item for sublist in reversed(response_a) for item in sublist]
    response_b = [item for sublist in reversed(response_b) for item in sublist]

    # トークン数を計算
    p_a_b_len = len(prompt) + len(response_a) + len(response_b)
    cut_len = p_a_b_len - MAX_LENGTH
    
    # 制限を超えた場合、トークンを削除
    if cut_len > 0:
        prompt = prompt[:-int(len(prompt)/p_a_b_len*cut_len)]
        response_a = response_a[:-int(len(response_a)/p_a_b_len*cut_len)]
        response_b = response_b[:-int(len(response_b)/p_a_b_len*cut_len)]

    # 特殊トークンを付加
    prompt = tokenizer('<prompt>: ', add_special_tokens=False)["input_ids"] + prompt
    response_a = tokenizer('\n\n<response_a>: ', add_special_tokens=False)["input_ids"] + response_a
    response_b = tokenizer('\n\n<response_b>: ', add_special_tokens=False)["input_ids"] + response_b
    extra_prompt = tokenizer('\n\n---------\nどちらの応答がプロンプトに対して良いか？ a、b、またはtie？\n\n回答: ', add_special_tokens=False)["input_ids"]
    
    label_token_id = [128250]  # ラベルのトークンID
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + label_token_id + [tokenizer.eos_token_id]
    attention_mask = len(input_ids) * [1]
    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + label_token_id + [tokenizer.eos_token_id]

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# トークン化
```

---The following area is a Code cell (cell numver is 8)---
```python
%%time
tokenizer = AutoTokenizer.from_pretrained(WEIGHTS_PATH)  # トークナイザーをロード
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]  # ラベルIDを取得

def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)  # データフレームからデータセットを作成
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        # remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer},
    )
    return tokenized_datasets

test_ds = load_data(test, tokenizer)  # トークン化されたデータセットをロード
test_ds
```

---The following area is a Code cell (cell numver is 9)---
```python
data = test_ds.to_pandas()  # トークン化されたデータセットをPandasデータフレームに変換
data["max_len"] = data["input_ids"].apply(len)  # 各インプットIDの長さを計算
data[:3]  # 最初の3行を表示
```

---The following area is a Code cell (cell numver is 10)---
```python
data['input_ids'][0]  # 最初のインプットIDを表示
```

---The following area is a Code cell (cell numver is 11)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # 最初のインプットIDをデコードして表示
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# モデルをロード
各GPUにモデルを1つずつロードします。
```

---The following area is a Code cell (cell numver is 13)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]  # 重みのキー
    def __init__(self, config):
        super().__init__(config)  # 親クラスの初期化
        self.model = LlamaModel(config)  # Llamaモデルのインスタンス化
        self.vocab_size = config.vocab_size  # 語彙サイズの設定
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 線形層の設定
        self.post_init()  # 初期化後の処理

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        cache_position=None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]  # 隠れ層の状態を取得
        if self.config.pretraining_tp > 1:  # モデルパラレルが必要な場合
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)  # 隠れ層の状態からロジットを計算

        logits = logits.float()  # ロジットをfloat型に変換

        loss = None
        if labels is not None:  # ラベルが提供されている場合
            # トークンをシフトして、n未満のトークンがnを予測するようにする
            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフト
            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフト
            # トークンをフラットにする
            loss_fct = nn.CrossEntropyLoss()  # 損失関数の設定
            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラットにする
            shift_labels = shift_labels.view(-1)  # ラベルをフラットにする
            # モデルパラレルを有効化
            shift_labels = shift_labels.to(shift_logits.device)

            fake_label_tokens_ids = torch.tensor([128250], device=shift_labels.device)
            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)
            # index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            # true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            # true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, fake_label_tokens_ids)][:, label_tokens_ids]
            # loss = loss_fct(true_logits, true_labels)

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 14)---
```python
# GPU 0に基本モデルをロード
device0 = torch.device('cuda:0')  # デバイスの設定
base_model_0 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:0',
)  # モデルを読み込む
# GPU 1に基本モデルをロード
device1 = torch.device('cuda:1')  # デバイスの設定
base_model_1 = Llama3ForSFT.from_pretrained(
    MODEL_NAME,
    use_cache=False,
    device_map='cuda:1',
)  # モデルを読み込む
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# 重みをロード
```

---The following area is a Code cell (cell numver is 16)---
```python
# PEFTを取得
model_0 = PeftModel.from_pretrained(base_model_0, model_id=WEIGHTS_PATH).to(device0)  # モデルをPEFTから読み込む
model_0.eval()  # モデルを評価モードにする

model_1 = PeftModel.from_pretrained(base_model_1, model_id=WEIGHTS_PATH).to(device1)  # モデルをPEFTから読み込む
model_1.eval()  # モデルを評価モードにする
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 18)---
```python
@torch.no_grad()  # 勾配計算を無効化
@torch.cuda.amp.autocast()  # 自動混合精度を使用
def inference(df, model, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []  # 各モデルの勝利数を初期化

    model.eval()  # モデルを評価モードにする
    for start_idx in range(0, len(df), batch_size):  # データフレームをバッチサイズで処理
        end_idx = min(start_idx + batch_size, len(df))  
        tmp = df.iloc[start_idx:end_idx]  # 一時的なデータフレームを作成
        input_ids = tmp["input_ids"].to_list()  # インプットIDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換
        labels = tmp["labels"].to_list()  # ラベルをリストに変換
        
        # 入力をパディング
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        input_ids = inputs["input_ids"].to(device)  # デバイスに移動
        attention_mask = inputs["attention_mask"].to(device)  # デバイスに移動
        
        pad_labels = []
        for label in labels:
            # ラベルをパディング
            label = list(label) + [tokenizer.pad_token_id] * (input_ids[0].shape[0] - label.shape[0])
            pad_labels.append(label)

        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルを順伝播
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ロジットをソフトマックスで正規化
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率を保存
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率を保存
        tie.extend(proba[:, 2].tolist())  # タイの勝率を保存
        
    df['winner_model_a'] = a_win  # データフレームに勝率を追加
    df['winner_model_b'] = b_win  # データフレームに勝率を追加
    df['winner_tie'] = tie  # データフレームに勝率を追加
    return df  # 結果を返す
```

---The following area is a Code cell (cell numver is 19)---
```python
st = time.time()  # 開始時間を記録

data = data.sort_values("max_len", ascending=False)  # 最大長さでソート
sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのサブセット
sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのサブセット

# 並列で推論を実行
with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device0, device1))

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝率を抽出

print(f"経過時間: {time.time() - st}")  # 経過時間を表示
```

---The following area is a Code cell (cell numver is 20)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝率を結果データフレームに追加
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を結果データフレームに追加
result_df.loc[:, "winner_tie"] = proba[:, 2]  # タイの勝率を結果データフレームに追加
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルとして保存
display(submission_df)  # 提出データフレームを表示
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
---

# コメント

> ## Songling
> 
> このファイルが見つかりません。誰か助けてくれますか？
> 
> WARNING: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスであるか、特定のスキームが欠けています。
> 
> ERROR: bitsandbytesに対する条件を満たすバージョンが見つかりません (バージョン: なし)
> 
> ERROR: peftに対する条件を満たすバージョンが見つかりません (バージョン: なし)
> 
> WARNING: Location '../input/llm-pip-2024-7-4/' は無視されます: 存在しないパスであるか、特定のスキームが欠けています。
> 
> 

---

> ## Krens
> > "llm-pip-2024-7-4"はプライベートデータセットです。 pypiに行って対応するwhlファイルをダウンロードし、新しいデータセットを作成する必要があります。 または、[こちら](https://www.kaggle.com/datasets/jickymen/whl-llama3-1)を使うことができます。

---

> ## Shimei
> 
> こんにちは、この素晴らしいノートブックを共有してくれてありがとう！
> 
> しかし、あなたのトレーニングノートブックを微調整した後、推論時に以下のエラーが出ます。
> 
> OSError: /kaggle/input/llama3-sft/checkpoint-2800にconfig.jsonというファイルが見つかりません。利用可能なファイルについては、'https://huggingface.co//kaggle/input/llama3-sft/checkpoint-2800/tree/None'をチェックしてください。

---

> ## YEI0907
> 
> 老哥、このアーキテクチャの推論時間はどのくらいかかりましたか？自回帰モデルを使って推論しましたが、推論時間が直接分類するよりもずっと長くなりました。

---

> ## Huang Jing Stark
> 
> MAX_LENGTH = 2400を設定する特定の理由はありますか？

---

> ## Rabbit
> 
> HuggingfaceからKaggleにモデルをロードする方法を教えていただけますか？

---

> ## Dlond Mike
> 
> でも、あなたの友人、あなたは本当に英雄です。

---

> ## PaulRRR
> 
> llama3.1を使用していますが、エラーが出ます: ValueError: rope_scalingは2つのフィールド、typeとfactorを持つ辞書である必要があります。{'factor': 8.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}を取得しました。

---

> ## Aaryan Gupta
> 
> こんにちは、このノートブックのデータセットはプライベートですか？パスが見つからないため、コードがエラーを投げています。

---

> ## 박민욱peterminpark
> 
> 待ってください
> 
> モデル /kaggle/input/sft-llama3-lora-9231は公開されていますか？

---

> ## Lorry Zou
> 
> Huggingfaceからリポジトリ全体をダウンロードし、推論用のデータセットを作成しましたが、ロード時に以下のエラーが出ました:
> 
> /kaggle/input/llama-3-8b-instruct-bnb-4bit-1にpytorch_model.bin、model.safetensors、tf_model.h5、model.ckpt.index、またはflax_model.msgpackという名前のファイルが見つかりませんでした。同じ問題に遭遇した人はいますか、そしてどのように対処すればよいですか？

---

> ## OHIRA
> 
> こんにちは、すごい作品をありがとう!!!
> 
> 8bitのLoRAを使用する場合、時間制限内で推論できますか？
> 
> どのくらいの時間がかかりますか？

---

> ## Qihang Wang
> 
> こんにちは、なぜ以下のように設定しましたか？
> 
> label_token_id = [128250]

---
```

** @@@ Jupyter Notebook numver 5, the number of votes :58 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションで使用される、チャットボットの応答の好みを予測するための機械学習モデルの訓練に関するものです。具体的には、Llama3という言語モデルを用いて、ユーザーが好む応答を予測し、その性能を評価しています。

### 問題
このノートブックでは、Chatbot Arenaデータセットに基づいて、どちらのチャットボット応答が好まれるかを予測するモデルを構築しています。ユーザーからのフィードバックに基づいて、異なるモデルの応答の優劣を判断する問題に取り組みます。

### 手法とライブラリ
- **モデル:** Llama3（8ビット精度）をベースとして使用し、LoRA (Low-Rank Adaptation)を導入したカスタムモデル `Llama3ForSFT`を作成しています。これにより、モデルの重量を軽量化し、効率的にトレーニングを行えるようにしています。
  
- **トークナイザー:** `transformers` ライブラリの `AutoTokenizer`を使用して、テキストデータをトークン化しています。

- **データ処理:** `pandas`を使用してデータを読み込み、一部のデータを利用してモデルを訓練しています。また、`datasets` ライブラリを用いてデータセットを管理しています。

- **評価指標:** モデルの性能評価には、対数損失 (`log_loss`) と精度 (`accuracy_score`) が使用されています。`scikit-learn`の機能を活用して計算しています。

### 結果
ノートブック内では、評価用データセットのログ損失が 0.9231 で、リーダーボードのスコアが0.936と示されており、モデルの性能が確認されています。また、訓練時のバッチサイズやエポック数、評価手法についても詳細に説明がなされ、特にGPU環境での実行を考慮した条件が記載されています。

全体として、このノートブックは、ユーザーの応答の好みを予測するための機械学習モデルの実装と、そのトレーニングプロセスを包括的に示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 結果
- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| サブセット | ログ損失 |
| - | - |
| 評価 | 0.9231 |
| LB | 0.936 |

## 注意
コードを再現したい場合は、以下の点に注意してください：
- すべてのデータを使用すること
- per_device_train_batch_size=4に設定すること
- A10を使用して1エポックは約15時間かかること
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft  # transformers, bitsandbytes, accelerate, peftのパッケージを最新バージョンでインストールします
```

---The following area is a Code cell (cell numver is 3)---
```python
import os  # OS関連の機能をインポート
import copy  # オブジェクトをコピーするためのモジュールをインポート
from dataclasses import dataclass  # データクラスを作成するためのモジュールをインポート

import torch  # PyTorchのライブラリをインポート
import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポート
import torch.nn.functional as F  # PyTorchの関数型ニューラルネットワークモジュールをインポート
import pandas as pd  # データ処理のためのパンダスライブラリをインポート
import numpy as np  # 数値計算のためのNumPyライブラリをインポート
from datasets import Dataset  # データセット管理のためのライブラリをインポート
from scipy.special import softmax  # softmax関数をインポート
from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングのためのモジュールをインポート
from transformers import (  # Transformersライブラリから以下のクラスをインポート
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast  # モデルの出力のデータ型をインポート
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTのための設定やモデルをインポート
from sklearn.metrics import log_loss, accuracy_score  # 精度とログ損失の評価指標をインポート
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"  # 訓練データのCSVファイルのパスを指定
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"  # モデルのパスを指定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能な場合はGPUを使用、そうでなければCPUを使用
MAX_LENGTH = 1024  # トークンの最大長を設定
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなるカラムをリスト化
columns_to_vectorize = ["prompt", "response_a", "response_b"]  # ベクトル化するカラムをリスト化

train = pd.read_csv(TRAIN_CSV)  # 訓練データをCSVファイルから読み込み
train = train.head(100)  # 最初の100行を取得
train['label'] = train[target_columns].idxmax(axis=1)  # 各行で最も大きいインデックスをラベルとして設定
label_encoder = LabelEncoder()  # ラベルエンコーダのインスタンスを作成
train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコード
train = train[columns_to_vectorize + ['label']]  # 必要なカラムのみを保持
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
### トークナイザーとデータセットの準備、評価指標
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)  # 事前学習済みモデルのトークナイザーを取得
tokenizer.add_eos_token = True  # EOSトークンを追加
tokenizer.padding_side = 'right'  # パディングは右側に設定

# ラベルのトークンIDを取得
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):  # トークン化のための関数を定義
    # プロンプト、応答A、応答Bをそれぞれトークン化
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    # 最大長を超える場合は部分的にトークン化
    if len(prompt + response_a + response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nこのプロンプトに対してどちらの応答が良いか？ a または b または tie か？\n\n答え: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]  # 正解ラベルのトークンIDを取得
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids) * [1]  # アテンションマスクを作成
    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]  # ラベルを準備
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Code cell (cell numver is 8)---
```python
def load_data(df, tokenizer):  # データをロードする関数を定義
    raw_datasets = Dataset.from_pandas(df)  # pandas DataFrameからデータセットを作成
    tokenized_datasets = raw_datasets.map(
        tokenize,  # トークン化関数をマップ
        remove_columns=raw_datasets.column_names,  # 使用しないカラムは削除
        fn_kwargs={'tokenizer': tokenizer}  # トークナイザーを引数として渡す
    )
    return tokenized_datasets  # トークナイズされたデータセットを返す

def compute_metrics(pred):  # 精度や損失を計算する関数を定義
    logits, labels = pred  # 予測値とラベルを取得
    preds = logits.argmax(axis=-1)  # 最大のロジットからクラスを予測
    label_tokens_ids = np.array(LABEL_IDS)  # ラベルトークンのIDをNumPy配列に変換
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成
    labels = labels[np.isin(labels, label_tokens_ids)]  # ラベルをフィルタリング
    labels = np.array([index_mapping[label.item()] for label in labels])  # ラベルをインデックスに変換
    acc = accuracy_score(labels, preds)  # 精度を計算
    probs = softmax(logits, axis=-1)  # ソフトマックスで確率を計算
    log_loss_ = log_loss(labels, probs)  # ログ損失を計算
    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログ損失を辞書で返す

n_splits = 5  # データの分割数を指定
fold_idx = 0  # 現在のフォールドインデックスを初期化
ds = load_data(train, tokenizer)  # データをロード
# n分割交差検証のためのインデックスを作成
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを取得
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### モデル
```

---The following area is a Code cell (cell numver is 10)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):  # LlamaPreTrainedModelを継承したクラスを定義
    _tied_weights_keys = ["lm_head.weight"]  # 結びつけられた重みのキーを指定
    def __init__(self, config):
        super().__init__(config)  # 親クラスの初期化
        self.model = LlamaModel(config)  # Llamaモデルのインスタンスを作成
        self.vocab_size = config.vocab_size  # 語彙サイズを設定
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 出力層を定義
        self.post_init()  # 初期化処理

    def forward(  # フォワード関数を定義
        self,
        input_ids=None,  # 入力ID
        attention_mask=None,  # アテンションマスク
        position_ids=None,  # 位置ID
        past_key_values=None,  # 過去のキーと値
        inputs_embeds=None,  # 入力の埋め込みベクトル
        labels=None,  # ラベル
        use_cache=None,  # キャッシュを使用するか
        output_attentions=None,  # アテンションを出力するか
        output_hidden_states=None,  # 隠れ状態を出力するか
        return_dict=None,  # 辞書形式で返すか
        cache_position=None,  # キャッシュの位置
    ):
        outputs = self.model(  # モデルを通して出力を取得
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]  # 隠れ状態を取得
        if self.config.pretraining_tp > 1:  # モデル並列化が必要な場合
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)  # 重みを分割
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]  # 各スライスに対するロジットを計算
            logits = torch.cat(logits, dim=-1)  # ロジットを結合
        else:
            logits = self.lm_head(hidden_states)  # 隠れ状態を基にロジットを計算
        logits = logits.float()  # ロジットをfloat型に変換

        loss = None  # 損失を初期化
        if labels is not None:  # ラベルが与えられている場合
            # トークンをシフトして予測を行う
            shift_logits = logits[..., :-1, :].contiguous()  # 最後のトークンを除外
            shift_labels = labels[..., 1:].contiguous()  # 最初のトークンを除外
            # トークンをフラット化
            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失関数を定義
            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットをフラットに変換
            shift_labels = shift_labels.view(-1)  # ラベルをフラットに変換
            # モデル並列化を有効化
            shift_labels = shift_labels.to(shift_logits.device)  # デバイスを統一

            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルトークンIDをテンソルに変換
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 正しいラベルをフィルタリング
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)  # インデックスに変更
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 正しいロジットを取得
            loss = loss_fct(true_logits, true_labels)  # 損失を計算

        return CausalLMOutputWithPast(  # 出力を返す
            loss=loss,  # 損失
            logits=true_logits,  # 正しいロジット
        )
```

---The following area is a Code cell (cell numver is 11)---
```python
peft_config = LoraConfig(  # PEFT構成を定義
    r=16,  # ランク
    lora_alpha=32,  # LoRAアルファ
    lora_dropout=0.05,  # LoRAドロップアウト率
    bias='none',  # バイアス
    inference_mode=False,  # 推論モード
    task_type=TaskType.CAUSAL_LM,  # タスクタイプ
    target_modules=['q_proj', 'k_proj', 'v_proj'],  # 対象モジュールを指定
)

model = Llama3ForSFT.from_pretrained(  # 事前学習済みのモデルをロード
    model_path, 
    torch_dtype=torch.float16,  # 半精度でロード
)
model.config.use_cache = False  # キャッシュを使用しない設定
model = prepare_model_for_kbit_training(model)  # kビットトレーニング用にモデルを準備
model = get_peft_model(model, peft_config)  # PEFTモデルを取得
print(model)  # モデルの概要を表示
model.print_trainable_parameters()  # 訓練可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
#### 訓練引数
```

---The following area is a Code cell (cell numver is 13)---
```python
args = TrainingArguments(  # 訓練引数を定義
    output_dir='output',  # 出力ディレクトリ
    overwrite_output_dir=True,  # 出力ディレクトリを上書きするか
    evaluation_strategy="epoch",  # 評価の戦略
    save_strategy="steps",  # 保存の戦略
    save_steps=200,  # 200ステップごとに保存
    save_total_limit=1,  # 最大保存数
    logging_strategy="steps",  # ロギングの戦略
    logging_steps=10,  # 10ステップごとにログ
    warmup_steps=20,  # ウォームアップステップ数
    optim="adamw_8bit",  # 最適化アルゴリズム
    learning_rate=2e-4,  # 学習率
    per_device_train_batch_size=2,  # デバイスごとのトレーニングバッチサイズ
    per_device_eval_batch_size=4,  # デバイスごとの評価バッチサイズ
    gradient_accumulation_steps=2,  # 勾配蓄積ステップ数
    num_train_epochs=1,  # 訓練エポック数
    fp16=True,  # フロート16を使用
    metric_for_best_model="log_loss",  # 最良モデルの評価指標
    greater_is_better=False,  # 指標が大きいほど良いか
    report_to="none",  # どこに報告するか
)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
### 訓練開始！
```

---The following area is a Code cell (cell numver is 15)---
```python
trainer = Trainer(  # トレーナーインスタンスを作成
    args=args,  # 訓練引数
    model=model,  # 使用するモデル
    train_dataset=ds.select(train_idx),  # トレーニングデータセット
    eval_dataset=ds.select(eval_idx),  # 評価データセット
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレーター
    compute_metrics=compute_metrics,  # 評価指標計算用関数
)
trainer.train()  # 訓練を開始
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
---

# コメント 

> ## KeShuang Liu
> 
> こんにちは、トレーニングにmax_lengthが1024、推論に2400と設定されている理由を知りたいです。トレーニングでmax_length=2400を使用したことはありますか？
> 

---

> ## OHIRA
> 
>素晴らしい作品をありがとうございます！
> 
> コードについて質問があります。
> 
> load_best_model_at_end = Trueを設定した場合、
> 
> args = TrainingArguments(
>     output_dir='/kaggle/output',
>     overwrite_output_dir=True,
>     evaluation_strategy="steps",
>     save_strategy="steps",
>     save_steps=20,
>     save_total_limit=5,
>     logging_strategy="steps",
>     logging_steps=20,
>     warmup_steps=20,
>     optim="adamw_8bit",
>     learning_rate=2e-4,
>     per_device_train_batch_size=2,
>     per_device_eval_batch_size=4,
>     gradient_accumulation_steps=2,
>     num_train_epochs=1,
>     fp16=True,
>     metric_for_best_model="log_loss",
>     greater_is_better=False,
>     report_to="none",
>     load_best_model_at_end=True
> )
> 
> 評価セットで5つの最良モデルのパラメータを取得できますか？
> 
> それとも最後の5つのモデルのパラメータを取得するのですか？
> 
> 

---

> ## daichisaito-cs
> 
> 素晴らしい作品を共有していただきありがとうございます！
> 
> 質問があります：
> 
> Evalのスコア0.9231、LBのスコア0.936を再現するには何エポック必要ですか？
> 
> デフォルトの訓練エポック数は1に設定されていますが、これがこれらのスコアを得るために使用された値と同じですか？
> 
> ありがとう
> 
> 
> > ## ShelterWTopic 著者
> > 
> はい。       
> > 
> 

---

> ## Lorry Zou
> 
> Gemma2 9Bをこの方法（次の単語予測）で訓練しようとしましたか？Llama3の場合、この方法はLlamaForSequenceClassificationを直接使用するよりもはるかに良いパフォーマンスを持つようです。
> 

---

> ## Stringersolo
> 
> こんにちは [@shelterw](https://www.kaggle.com/shelterw)、共有していただきありがとうございます。同じ結果を再現する際に問題があります。具体的には、トレーニング中は指標がほぼ同じですが、LBでスコアを計算すると約1.2で、非常に奇妙で平均的な/ランダムな予測に近いです。
> 
> モデルの重みを直接読み取ろうとしましたが、役に立ちませんでした：
> 
> model_0.load_state_dict(torch.load(RAW_WEIGHTS), strict=False)
> 
> 
> > ## ShelterWTopic 著者
> > 
> モデルをロードする際にlm headの重みがランダムに再ロードされることが原因かもしれません。
> > 
> transformersとpeftのバージョンを更新するか、LlamaPretrainedModelの代わりにLlamaCausalModelクラスを継承するようにしてください。
> > 
> Gemma2を使用したときにも同じ現象が起きましたが、奇妙です。
> > 
> [こちら](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518408#2912471)を参照してください。
> > 
> 
> > > ## Stringersolo
> > > 
> > > ありがとう[@shelterw](https://www.kaggle.com/shelterw)、試してみます。このリンクでスコアレイヤーを保存するように提案されました：
> > > 
> > > torch.save(classifier.score.state_dict(), f'{output_directory_path}/score_state_dict.pth')
> > > 
> > > あなたの場合は次のようになります：
> > > 
> > > torch.save(trainer.model.lm_head.state_dict(), f'output/lm_head_dict.pth')
> > > 
> > > ですよね？
> > > 
> > > 
> > > 

---

> ## Yi-Fu Chen
> 
> なぜ.autoModelForCausalLMを直接使用するのではなく、Llama3ForSFTを実装する必要があるのか教えていただけますか？特別な理由はありますか？
> 
> 
> > ## ShelterWTopic 著者
> > 
> > ラベルトークンのロジットと損失のためです。
> > 
> 

---

> ## Eido Mike
> 
> 優れた作品です！共有していただきありがとうございます。
> 
> 

---

> ## AbaoJiang
> 
> こんにちは [@shelterw](https://www.kaggle.com/shelterw)、
> 
> 共有していただきありがとうございます。トレーニングにCAUSAL_LMタスクを使用したことに気付きました。LlamaForSequenceClassificationを使用してトレーニングした場合のパフォーマンスと比較しましたか？
> 
> 
> > ## ShelterWTopic 著者
> > 
> > llama3-8bとSEQ_CLSを比較したことはありませんが、以前の実験ではllama3-8bの方が悪かったですが、gemma2-9bのSEQ_CLSよりは良かったです。
> > 
> 

---

> ## __ChrisQ__
> 
> こんにちは、このノートブックをありがとうございます。
> 
> 一つ質問があります：すべてのデータを使用する場合、評価スコアはどのように計算しますか？
> 
> 
> > ## ShelterWTopic 著者
> > 
> > スクリプトの'compute_metrics'関数は、1エポック後に自動的に計算されます。
> > 
> > 
> > 
> > > ## raconion
> > > 
> > > 'compute_metrics'関数はトレーニングデータの20%をクロスバリデーションに使用します。評価データを使用してモデルをさらにトレーニングしたのでしょうか？「すべてのデータを使用する」とはどういう意味ですか？5分割CVのためにすべてのデータを使用するという意味でしょうか、それともモデルをすべてのデータでトレーニングしますか？
> > > 
> > > 
> > > > ## ShelterWTopic 著者
> > > > 
> > > > トレーニングの80%を使用し、評価の20%を使用します。
> > > 
> > > 
> > > ## raconion
> > > 
> > > ご確認いただきありがとうございます :)
> > > 
> > > 
> > >
```

** @@@ Jupyter Notebook numver 6, the number of votes :48 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Chatbot Arenaコンペティションにおける応答の好みを予測するためのモデルを訓練し、その推論を行うことを目的としています。具体的には、GemmaCausalLMという大規模言語モデルを用いて、Loraという手法を利用してモデルの重みを訓練するプロセスが示されています。

### 主な取り組みと手法
1. **データの準備**:
   - Kaggleのデータセットから応答とそれに対するユーザーの選好を取得し、訓練用データセットを構築しています。
   - 特に、テキストデータを前処理し、サロゲート（代用文字）を排除する関数が実装されています。

2. **モデルの設定**:
   - TensorFlowとKerasの環境下で、TPUを利用するための設定が行われています。
   - Kerasの分散処理機能を使い、モデルの重みを複数のTPUに分散するためのデバイスメッシュとレイアウトマップが創出されています。

3. **GemmaCausalLMの利用**:
   - GemmaCausalLMモデルをロードし、Loraを活用して特定の層を訓練不可能に設定し、学習を最適化しています。このアプローチは、モデルの過剰適合を防ぎつつ、効果的な学習を促進します。

4. **モデルの訓練**:
   - モデルアーキテクチャを構築し、訓練データを使ってモデルを訓練します。AdamWオプティマイザーとCategoricalCrossentropy損失を使用して、訓練が行われています。

5. **成果物の保存**:
   - 訓練後、トレーニングの過程で得られたモデルの重みをファイルに保存する処理が含まれています。また、Loraの重みも別に保存され、将来的な推論時に再利用できるようになっています。

### 使用技術とライブラリ
- 使用されているライブラリ: Keras、TensorFlow、JAX、Keras NLP
- 主な関数と手法: TPU環境設定、デバイスメッシュ、Lora訓練、データセットシャッフルなど

このノートブックは、実際のデータセットを用いた実践的な機械学習フォローアップの例であり、ユーザーの好みをモデル化することで、より効果的なチャットボット応答システムの構築を目指しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
1エポックは約3時間かかります。

Loraの重みを保存し、次のノートブックで2xT4sの推論に使用してください：

注意：bfloat16での学習はモデルの過剰適合を引き起こすため、学習はfloat32で行い、推論はfloat16で行うべきです。

https://www.kaggle.com/code/pranshubahadur/inference-tf-gemma-2-9b-lmsys

インスピレーション：

https://www.kaggle.com/code/matthewdwatson/gemma-2-fine-tuning-and-inference

https://colab.research.google.com/github/google/generative-ai-docs/blob/main/site/en/gemma/docs/lora_tuning.ipynb#scrollTo=_Peq7TnLtHse
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U keras-nlp tensorflow-text
# tensorflow-cpuをインストールし、tensorflowがTPUにアクセスしないようにします。
!pip install -q -U tensorflow-cpu
```

---The following area is a Code cell (cell numver is 3)---
```python
import jax

jax.devices()
```

---The following area is a Code cell (cell numver is 4)---
```python
import os

# 現在のところ、Keras 3の配布APIはJAXバックエンドのみに実装されています。
os.environ["KERAS_BACKEND"] = "jax"
# メモリの断片化と割り当てオーバーヘッドを最小限に抑えるために、すべてのTPUメモリを事前に割り当てます。
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
```

---The following area is a Code cell (cell numver is 5)---
```python
import keras
import keras_nlp
```

---The following area is a Code cell (cell numver is 6)---
```python
# 重みがすべての8つのTPUに分散されるように(1, 8)の形状を持つデバイスメッシュを作成します。
device_mesh = keras.distribution.DeviceMesh(
    (1, 8),
    ["batch", "model"],
    devices=keras.distribution.list_devices(),
)
```

---The following area is a Code cell (cell numver is 7)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# 'token_embedding/embeddings'に一致する重みは8つのTPUに分散されます。
layout_map["token_embedding/embeddings"] = (model_dim, None)
# 注意レイヤーのクエリ、キー、バリュー行列にマッチする正規表現
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
```

---The following area is a Code cell (cell numver is 8)---
```python
def remove_surrogates(text):
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))
```

---The following area is a Code cell (cell numver is 9)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0])

raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)

# データセットを構成します。
train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1).apply(lambda x: '\n\nPROMPT\n\n' + x).apply(lambda x: remove_surrogates(x)),
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
})
```

---The following area is a Code cell (cell numver is 10)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",
)

# モデルの並列処理の設定
keras.distribution.set_distribution(model_parallel)
```

---The following area is a Code cell (cell numver is 11)---
```python
#keras.config.set_floatx("bfloat16")
```

---The following area is a Code cell (cell numver is 12)---
```python
# GemmaCausalLMモデルを指定されたプリセットから読み込みます。
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1")
# モデルの概要を表示します。
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 13)---
```python
# Loraを有効にします。
gemma_lm.backbone.enable_lora(rank=16)
```

---The following area is a Code cell (cell numver is 14)---
```python
# 最初の16層を訓練不可能に設定します。
for layer in gemma_lm._backbone.layers[:16]:
    layer.trainable = False
```

---The following area is a Code cell (cell numver is 15)---
```python
# 再度モデルの概要を表示します。
gemma_lm.summary()
```

---The following area is a Code cell (cell numver is 16)---
```python
def preprocess_fn(text, label=None):
    preprocessed = gemma_lm._preprocessor(text, sequence_length=1024)[0]
    # 前処理関数が必要な入力のみを返すことを確認します。
    return {'token_ids' : preprocessed['token_ids'], 'padding_mask' : preprocessed['padding_mask']}, label if label is not None else text
```

---The following area is a Code cell (cell numver is 17)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

# モデルの入力を定義します。
inputs = {
    "token_ids": keras.Input(shape=(1024,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(1024,), dtype=tf.int32, name="padding_mask"),
}
x = gemma_lm.backbone(inputs)
print(x.shape)  # 出力の形状を表示
x = GlobalAveragePooling1D()(x)  # グローバル平均プーリングを適用
print(x.shape)  # 出力の形状を表示

# 出力層を定義します。
outputs = Dense(3, 'softmax')(x)
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 18)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,
                weight_decay=0.01,)
# バイアスとスケールを重み減衰から除外します。
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])
```

---The following area is a Code cell (cell numver is 19)---
```python
# モデルをコンパイルします。
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
```

---The following area is a Code cell (cell numver is 20)---
```python
import tensorflow as tf
# データセットを作成します。
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values)).batch(4).map(preprocess_fn)
ds = ds.shuffle(ds.cardinality())  # データセットのシャッフル
```

---The following area is a Code cell (cell numver is 21)---
```python
# データセットを訓練用と検証用に分割します。
train_split = ds.take(int(len(ds)*0.9))  # 90%を訓練データとして
val_split = ds.skip(int(len(ds)*0.9)).take(int(len(ds)*0.1))  # 残りの10%を検証データとして
# モデルの訓練を実行します。
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=4)
```

---The following area is a Code cell (cell numver is 22)---
```python
import numpy as np
layer = model.get_layer(name='dense')
weights = layer.get_weights()  # 重みを取得
kernel, bias = weights

# カーネルとバイアスを別々に保存します。
np.save('dense_1_kernel.npy', kernel)
np.save('dense_1_bias.npy', bias)
# Loraの重みを保存します。
model.layers[2].save_lora_weights("model.lora.h5")
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
---

# コメント

> ## Lorry Zou
> 
> 誰かGemma2のハイパーパラメータを調整してLBスコア<0.92を得た人はいますか？私が得た最高スコアは0.926です。
> 
> 
> 
> > ## Pranshu BahadurTopic Author
> > 
> > 待って、あなたは推論できたんですか？
> > 
> > 
> > > ## Lorry Zou
> > > 
> > > 私はPytorchを使用しましたが、TFは使っていません。
> > > 
> > > 
> > > > ## cm391
> > > > 
> > > > データを事前処理すれば簡単に0.89の検証ができましたが、増強を行うと0.87になります…Kerasは素晴らしいトレーニングコードを作っていますが、分散推論コードがないのはなぜですか？
> > > > 
> > > > > ## Pranshu BahadurTopic Author
> > > > > 
> > > > > それがわかればいいのですが、量子化によって解決できる方法がありますが、少し忙しかったです。
> > > > > 
> > > > 

---

> ## ano
> 
> 素晴らしい仕事ありがとうございます！更なる実装を促す意図はありませんが、トレーニングを再開するためにオプティマイザーも保存できますか？
> 
> 
> > ## Pranshu BahadurTopic Author
> > 
> > スケジューラを使用していないので、オプティマイザー状態は必要ないと思います。同じオプティマイザーの設定はさらなるファインチューニングにも有効です。
> > 
> > 
> > > ## ano
> > > 
> > > 申し訳ありません。あなたが正しいです。このオプティマイザーのために保存する必要はありません。
> > > > 

---

> ## lightsource<3
> 
> もしLoraの重みだけを保存する必要があるなら、次のコードを使用できます：
> 
> ```
> model.get_layer("gemma_backbone").save_lora_weights("model.lora.h5")
> 
> ```
> 
> 
> > ## Pranshu BahadurTopic Author
> > 
> > ありがとう - これを試してみます -
> > 
> > 
> > > ## Pranshu BahadurTopic Author
> > > 
> > > ねえ、このドキュメントのリンクを教えてもらえますか？
> > > 
> > > > ## lightsource<3
> > > > 
> > > > [https://keras.io/api/keras_nlp/base_classes/backbone/](https://keras.io/api/keras_nlp/base_classes/backbone/)
> > > > 
> > > > 正確にトレーニングされたモデルを保存して量子化してGPUで推論する方法を見つけるのは簡単ではありませんでした：(
> > > > 

---

> ## Dai LinLing
> 
> シェアしてくれてありがとう。良い注意です！
> 
> 

---

> ## kaggk
> 
> 良い仕事です！
> 
> 

---

> ## RobsonDSP
> 
> モデルのパフォーマンスについての情報を誰か共有していただけませんか？1エポック後のlog_lossはどのくらいですか？
> 
> 
> > ## Pranshu BahadurTopic Author
> > 
> > トレーニング損失: ~1.08; 検証損失: 0.9286
> > 
> > 

---

> ## Turbo
> 
> いい仕事ですね！どうやって推論しますか？
> 
> > ## Pranshu BahadurTopic Author
> > 
> > [https://www.kaggle.com/code/pranshubahadur/inference-tf-gemma-2-9b-lmsys?scriptVersionId=189454678](https://www.kaggle.com/code/pranshubahadur/inference-tf-gemma-2-9b-lmsys?scriptVersionId=189454678)
> > 
> > トレーニングでloraの重みを保存して、ここで読み込んでください！
> > 
> > 

---

> ## Sparsh Tewatia
> 
> どのくらいのトレーニング、検証損失がありますか、全エポックの後に？
> 
> > ## Pranshu BahadurTopic Author
> > 
> > バージョン13で確認できます。
> > 
> >
```

** @@@ Jupyter Notebook numver 7, the number of votes :33 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおけるゼロショット予測に関するもので、特にLLama3モデルを使用して、ユーザーのプロンプトに対して応答モデルの適切さを予測することに焦点を当てています。Notebookは大きく分けて、ライブラリのインポート、データの読み込み、モデルの準備、予測の実行、結果の保存と評価の5つのセクションで構成されています。

### 問題の特定:
このNotebookは、ユーザーが選好する応答を予測するタスクに取り組んでいます。具体的には、Chatbot Arenaから得られたデータを用いて、与えられたプロンプトに対して選択肢として与えられる応答A、B、そのいずれかが適切か、または両方が同等か（引き分け）を予測します。

### 手法とライブラリ:
- **手法**: プロンプト工夫によるゼロショット学習を用いて、モデルに対して強化されたプロンプトを提供し、最も可能性の高い応答を予測します。モデルの出力から得た結果を元に、正規化された確率を計算することによって対数損失を評価します。

- **使用ライブラリ**:
  - `transformers`: Hugging FaceのTransformersライブラリを使用し、LLama3モデルとそのトークナイザーを導入しています。
  - `torch`: PyTorchを用いて、モデルのトレーニングおよび推論を行います。
  - `pandas`と`numpy`: データフレームの作成や数値演算のために使用されます。
  - `sklearn.metrics`: モデルの精度を評価するために対数損失を算出するために利用されています。

Notebookは、全体を通してプロセスの各ステップを明確に示し、最終的には予測結果をCSVファイルとして保存し、評価指標を表示しています。また、次のステップとして、さらなるモデルの改良やプロンプトの強化、教師ありファインチューニング（SFT）、アンサンブル手法の検討を提案しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS ゼロショット予測

このコードはLLama3を使用してゼロショット予測を行います。

分類ヘッダーを学習する代わりに、巧妙に設計されたプロンプトを使用して、`###Answer:`の後に最も可能性が高いトークンである**A**、**B**、または**tie**のいずれかを予測します。
```

---The following area is a Code cell (cell numver is 2)---
```python
import json  # JSON形式のデータを処理するためのライブラリをインポート
import pandas as pd  # データ解析のためのライブラリをインポート
import numpy as np  # 数値計算のためのライブラリをインポート
from tqdm.auto import tqdm  # プログレスバーを表示するためのライブラリをインポート
import pickle  # オブジェクトの保存と読み込みを行うためのライブラリをインポート
import random  # ランダムな数値生成のためのライブラリをインポート
import os  # オペレーティングシステムとのインタラクションのためのライブラリをインポート
import sys  # Pythonインタプリタとのインタラクションのためのライブラリをインポート

import transformers  # トランスフォーマモデルのライブラリをインポート
from transformers import AdamW  # AdamWオプティマイザをインポート
from transformers import AutoTokenizer, AutoModel, AutoConfig  # 自動トークナイザー、モデル、設定をインポート
from transformers import get_cosine_schedule_with_warmup  # コサインスケジュールをインポート

import torch  # PyTorchライブラリをインポート
import torch.nn as nn  # ニューラルネットワーク機能のモジュールをインポート
from torch.utils.data import DataLoader, Dataset  # データローダーとデータセットの入出力を行うためのモジュールをインポート

import sklearn.metrics  # sklearnのメトリクスライブラリをインポート
from sklearn.metrics import accuracy_score  # 精度スコアを計算するための関数をインポート

# トークナイザの並列処理を有効にするための環境変数を設定
os.environ["TOKENIZERS_PARALLELISM"] = "true"
# 使用可能な場合はGPUを、そうでなければCPUをデバイスとして設定
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 使用するデバイスを出力する
print(device)  # 現在のデバイス（CPUまたはGPU）を表示
```

---The following area is a Code cell (cell numver is 3)---
```python
class CFG:
    # 入力データが保存されているディレクトリのパス
    INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
    # 出力データを保存するためのディレクトリのパス
    OUTPUT = "/kaggle/working"
    # 使用するモデルのIDが保存されているディレクトリのパス
    MODEL_ID = "/kaggle/input/llama-3/transformers/8b-hf/1"
    # 乱数の初期化に使用するシード値
    SEED = 42
    # ターンを使用するかどうかを示すフラグ。0は使用しないことを意味する
    USE_TURN = 0
```

---The following area is a Code cell (cell numver is 4)---
```python
# 訓練データをCSVファイルから読み込む
train_df = pd.read_csv(f"{CFG.INPUT_DIR}/train.csv")
# 読み込んだ訓練データを出力する
train_df  # 訓練データフレームの内容を表示
```

---The following area is a Code cell (cell numver is 5)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# 事前に訓練された因果言語モデルをロードする
model = AutoModelForCausalLM.from_pretrained(
    CFG.MODEL_ID,  # モデルのIDを指定
    torch_dtype=torch.float16,  # モデルの重みを16ビット浮動小数点数で読み込む
    device_map='auto',  # 自動的にデバイスをマッピングする
)
```

---The following area is a Code cell (cell numver is 6)---
```python
# 事前に訓練されたトークナイザーをロードする
tokenizer = AutoTokenizer.from_pretrained(
    CFG.MODEL_ID,  # モデルのIDを指定
    use_fast=False,  # 高速トークナイザーを使用しない
    trust_remote_code=True,  # リモートのコードを信頼する
    padding_side="left",  # 左側にパディングを追加する設定
    pad_token="
```

---The following area is a Code cell (cell numver is 7)---
```python
# 訓練データの5分の1のサイズを計算する
n_valid = len(train_df) // 5
# 訓練データから無作為にサンプルを抽出する
sample_df = train_df.sample(n_valid, random_state=CFG.SEED)
# 抽出したサンプルデータの最初の5行を表示する
sample_df.head()  # サンプルデータフレームの先頭を表示
```

---The following area is a Code cell (cell numver is 8)---
```python
results = []  # 予測結果を格納するリストを初期化
# サンプルデータフレームの各行を無作為にイテレーションする
for _, row in tqdm(sample_df.iterrows(), total=len(sample_df)):
    # プロンプトと応答をJSON形式から読み込む
    prompt = json.loads(row["prompt"])  
    response_a = json.loads(row["response_a"])  
    response_b = json.loads(row["response_b"])  
    
    # プロンプトと応答から現在のターンを取得
    p = prompt[CFG.USE_TURN]
    a = response_a[CFG.USE_TURN]
    b = response_b[CFG.USE_TURN]
    
    # 応答がNoneの場合やプロンプトの単語数が3未満の場合はスキップする
    if a is None or b is None or len(p.split()) < 3:
        continue
        
    # 各リストの中のNoneを"none"で置き換え、1つの文字列に結合する
    p = " ... ".join(["none" if i is None else i for i in prompt])
    a = " ... ".join(["none" if i is None else i for i in response_a])
    b = " ... ".join(["none" if i is None else i for i in response_b])
    # プロンプトの先頭128文字と末尾128文字を保持し、残りを省略
    p = p[:128] + " ... " + p[-128:]
    a = a[:256] + " ... " + a[-256:]
    b = b[:256] + " ... " + b[-256:]
    
    # テキストをフォーマットする
    text = f"""### Instruction
Which model's answer is appropriate for the prompt?　If both are appropriate, answer `tie`.

### Prompt
{p}

### A
{a}

### B
{b}

### Answer
"""

    # トークナイザーを使ってテキストをトークン化する
    toks = tokenizer(text)

    # トークンをGPUのテンソルに変換する
    for k in toks.keys():
        toks[k] = torch.tensor(toks[k]).cuda()  

    # 勾配計算を無効にしてモデルを実行
    with torch.no_grad():
        out = model(toks["input_ids"].unsqueeze(0))

    # 予測トークンIDを生成する
    pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
    # モデルの出力からソフトマックスを計算して予測を取得
    pred = out.logits[0, -1, pred_token_id].cpu().softmax(0).numpy()
    
    # 元の行を辞書形式に変換し、予測を追加
    d = row.to_dict()
    d["predict"] = pred
    results.append(d)  # 予測結果をリストに追加
```

---The following area is a Code cell (cell numver is 9)---
```python
# 予測結果をデータフレーム形式に変換する
results_df = pd.DataFrame(results)
# 生成した結果データフレームを表示する
results_df  # 予測結果のデータフレームの内容を表示
```

---The following area is a Code cell (cell numver is 10)---
```python
# 予測結果をCSVファイルとして保存する
results_df.to_csv("result.csv", index=None)  # インデックスなしでCSVファイルに書き込む
```

---The following area is a Code cell (cell numver is 11)---
```python
# 現在の作業ディレクトリ内のファイルとフォルダの一覧を表示する
!ls
```

---The following area is a Code cell (cell numver is 12)---
```python
# 結果データフレームからラベルとなるターゲットを取得し、NumPy配列に変換する
targets = results_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

# 予測結果をリスト形式からNumPy配列に変換する
predicts = np.array(results_df["predict"].tolist())  # 予測結果を配列に変換
```

---The following area is a Code cell (cell numver is 13)---
```python
# ターゲットと予測に基づいて対数損失を計算する
logloss = sklearn.metrics.log_loss(targets, predicts)
# 計算した対数損失を出力する
print(logloss)  # 対数損失の値を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
# 現在の作業ディレクトリ内のファイルとフォルダの一覧を表示する
!ls
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
### 次のステップ:

- より大きなモデル（より良いベンチマーク精度を持つ）を試す
- プロンプトを改善する
- コンペティションデータに対してSFT（教師ありファインチューニング）を行う
- アンサンブルを使用する
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
---

# コメント 

> ## PaulRRR
> 
> こんにちは！提出用のコードはありますか？
> 
> 

---

> ## Mathama
> 
> 素晴らしい仕事です！
> 
> 

---

> ## shun takinami
> 
> 素晴らしい仕事です！
> 
> とてもシンプルで、LLMの使用を始めるのが簡単です。
> 
> 

---

> ## BRH103
> 
> 素晴らしい仕事です！
> 
> 

---

> ## biyibird0317
> 
> これは素晴らしいです！
> 
> 

---
```

** @@@ Jupyter Notebook numver 8, the number of votes :29 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションの人間による好み予測タスクに取り組んでいます。本コンペティションの目的は、大規模言語モデル（LLM）によって生成されたテキストの中から、ユーザーがどの応答を好むかを予測することです。

### 取り組んでいる問題
研究者は、ユーザーからのテキストプロンプトに対する2つのモデル（AとB）の応答を比較し、人間の好みを予測するためのモデルを開発しています。本Notebookでは、与えられたデータを処理し、LLM（具体的にはLlamaモデル）とLightGBMを組み合わせて、各モデルの勝者（どちらのモデルの応答が好まれるか）を分類するタスクを行っています。

### 使用する手法とライブラリ
1. **ライブラリのインストール**:
   - `bitsandbytes`, `transformers`, `tokenizers`, `peft`などのライブラリを使用し、LLMの効率的な推論及びパラメータの微調整に活用しています。

2. **データ加工**:
   - テストデータを読み込み、テキストを前処理してモデル入力形式に整形します。

3. **トークン化**:
   - `transformers`ライブラリを用いて、テキストをトークン化し、モデルに適した形式に変換しています。

4. **モデルのロードと設定**:
   - LlamaモデルをGPUにロードし、量子化（BitsAndBytes）を使用してメモリ効率を向上させています。
   - PEFT（Parameter-Efficient Fine-Tuning）を使用して、LoRa（Low-Rank Adaptation）に基づく設定を行い、モデルの重みをロードしています。

5. **推論の実施**:
   - 自動混合精度（autocast）を利用して、効率的に推論を行います。推論結果をデータフレームに格納します。

6. **LightGBMによるモデルの統合**:
   - テキスト特徴量を抽出し、LightGBMモデルも用いて予測を行い、最終的にLlamaモデルによる予測結果とブレンドします。

7. **結果の保存**:
   - ブレンドした予測結果をCSVファイルに保存し、コンペティションへの提出用データ形式を整えています。

このNotebookは、複数の機械学習アルゴリズムと効率的なデータ処理手法を駆使して、LLMによる応答の好み予測を体系的に解決するアプローチを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# bitsandbytesをインストールします。-qオプションは静かにインストールを行います。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
# transformersをインストールします。-qオプションは静かにインストールを行います。
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
# tokenizersをインストールします。-qオプションは静かにインストールを行います。
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
# peftをインストールします。-qオプションは静かにインストールを行います。
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
このノートブックの作業は、以下のノートブックからインスパイアを受けています：
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## もしこれが役に立ちましたら、評価をいただけると嬉しいです

## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 3)---
```python
from threading import Thread
import gc
import os
import io
import json
import random
import pickle
import zipfile
import datetime
import time

# PyTorchをインポート
import torch
# NumPyをインポート
import numpy as np
# Pandasをインポート
import pandas as pd
# Transformersライブラリから必要なクラスをインポート
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
# PEFT（Parameter-Efficient Fine-Tuning）関連のクラスをインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# 自動混合精度を使用するためのクラスをインポート
from torch.cuda.amp import autocast
# ディスプレイ用の機能をインポート
from IPython.display import display
# PyTorchの関数をインポート
import torch.nn.functional as F
# tokenizersをインポート
import tokenizers
```

---The following area is a Code cell (cell numver is 4)---
```python
# CUDAのメモリ効率的なSDPを有効化
torch.backends.cuda.enable_mem_efficient_sdp(True)
# CUDAのフラッシュSDPを有効化
torch.backends.cuda.enable_flash_sdp(True)

# モデル名の設定
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
# 重みのパスの設定
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
# 最大入力長の設定
MAX_LENGTH = 2048
# バッチサイズの設定
BATCH_SIZE = 4
# デバイスの設定（GPUを使用）
DEVICE = torch.device("cuda")
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## データの準備
```

---The following area is a Code cell (cell numver is 6)---
```python
# テストデータとサンプル提出データを読み込みます
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# リスト内の文字列を連結する関数
def process(input_str):
    # 余分な括弧を取り除く
    stripped_str = input_str.strip('[]')
    # 各文を取り出してリストに格納
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # 文を空白で連結して返す
    return  ' '.join(sentences)

# 各列に対してprocess関数を適用
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# サンプル提出データとテストデータの最初の5行を表示
display(sample_sub)
display(test.head(5))

# モデル用のテキストを準備します
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']
# 最初のテキストを表示
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## トークン化
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイザーをロードします
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# テキストデータをトークン化
tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

# トークンIDとアテンションマスクをデバイスに移動
INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# テンソルをCPUに移動し、リストに変換
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

# 入力IDとアテンションマスクからデータフレームを作成
data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## モデルのロード 
> 各GPUに1つのモデルをロードします。
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytesの設定
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットの重みをロード
    bnb_8bit_compute_dtype=torch.float16,  # 計算に使用するデータ型
    bnb_8bit_use_double_quant=False)  # ダブル量子化を使用しない

# GPU 0にベースモデルをロード
device0 = torch.device('cuda:0')

# LlamaForSequenceClassificationモデルをロード
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # ラベル数を設定
    torch_dtype=torch.float16,  # 使用するデータ型を設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:0')  # GPU 0にマッピング
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDの設定
```

---The following area is a Code cell (cell numver is 12)---
```python
# GPU 1にベースモデルをロード
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # ラベル数を設定
    torch_dtype=torch.float16,  # 使用するデータ型を設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:1')  # GPU 1にマッピング
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDの設定
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## 重みのロード
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRaの設定
peft_config = LoraConfig(
    r=16,  # ローラ次元
    lora_alpha=32,  # ローラアルファ
    lora_dropout=0.10,  # ドロップアウト率
    bias='none',  # バイアス設定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクのタイプ
    target_modules=['o_proj', 'v_proj'])  # 対象モジュールの設定
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTモデルを取得
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
# 重みをロード
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_0.eval()  # 評価モードに設定

# モデル1を設定
model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
model_1.eval()  # 評価モードに設定

# 学習可能なパラメータを表示
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 16)---
```python
gc.collect()  # ガーベジコレクションを実行
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## 推論
```

---The following area is a Code cell (cell numver is 18)---
```python
# 推論を行う関数
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    generated_class_a = []  # 出力クラスA
    generated_class_b = []  # 出力クラスB
    generated_class_c = []  # 出力クラスC

    model.eval()  # 評価モードに設定
    
    # データフレームをバッチサイズごとに繰り返す
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに移動
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに移動
        
        with torch.no_grad():  # 勾配計算を無効にする
            with autocast():  # 自動混合精度を使用
                outputs = model(
                    input_ids=batch_input_ids,  # 入力IDをモデルに渡す
                    attention_mask=batch_attention_mask  # アテンションマスクをモデルに渡す
                )
        
        # 出力の確率を計算
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        # 各クラスの確率を格納
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    # データフレームに結果を追加
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  # GPUメモリをクリア

    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
st = time.time()  # 処理開始時間を記録

N_SAMPLES = len(data)  # サンプル数を取得

# データを2つのサブセットに分割
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()  # 前半のデータ
sub2 = data.iloc[half:N_SAMPLES].copy()  # 後半のデータ

# スレッド内で推論を実行する関数
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)  # 推論結果を格納

# スレッドからの結果を格納するための辞書
results = {}
```

---The following area is a Code cell (cell numver is 20)---
```python
# スレッドの開始
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッド0
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッド1

t0.start()  # スレッド0を開始
t1.start()  # スレッド1を開始

# すべてのスレッドが終了するのを待つ
t0.join()
t1.join()

# 元のデータフレームに結果を結合
data = pd.concat([results[0], results[1]], axis=0)

print(f"処理が完了しました。総時間: {time.time() - st}")  # 処理時間を表示

# サンプル提出に結果を追加
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 21)---
```python
llama_preds = data[TARGETS].values  # 予測結果を配列に格納
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 23)---
```python
TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ
RUNPOD = os.path.exists('/workspace/')  # 実行環境の確認
KAGGLE = not RUNPOD  # Kaggle環境であるかの確認
if KAGGLE: 
    print('kaggle')  # Kaggle環境である場合、メッセージを表示
```

---The following area is a Code cell (cell numver is 24)---
```python
try:
    import pandas as pd  # Pandasをインポート
except:
    # Kaggle環境でPandasがない場合、インストール
    !pip install -q kaggle
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm 
    !pip install -q protobuf 
    !pip install -q numba
```

---The following area is a Code cell (cell numver is 25)---
```python
# データのパスを設定
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

# 実行環境がRUNPODの場合
if RUNPOD:
    # Kaggle APIの設定ファイルが存在しない場合
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle  # ディレクトリを作成
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # 設定ファイルをコピー
        !chmod 600 /root/.kaggle/kaggle.json  # アクセス権を設定

    # データファイルが存在しない場合、Kaggleからダウンロード
    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/ 
        
    # ダウンロード後、データを展開
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')
```

---The following area is a Code cell (cell numver is 26)---
```python
# パスを設定
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)  # モデルパスを表示

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスの設定
SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスの設定
```

---The following area is a Code cell (cell numver is 27)---
```python
# トークナイザーの並列化を無効にする
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 28)---
```python
# トレーニングデータ、テストデータ、サンプル提出データを読み込む
train = pd.read_csv(open(DATA + 'train.csv', 'r'))
test = pd.read_csv(open(DATA + 'test.csv', 'r'))
sample = pd.read_csv(DATA + 'sample_submission.csv')
# データの長さを表示
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 29)---
```python
params = {}
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1  # フォールドを設定


params['n_epochs'] = 1  # エポック数の設定
params['n_lgb'] = 1  # LightGBMの数の設定
params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルを設定
```

---The following area is a Code cell (cell numver is 30)---
```python
# params = {}
FULL = params.get('fold', 0) < 0  # フルデータかどうかを設定
N_FOLDS = int(params.get('n_folds', 3));  # フォールド数の設定
FOLD = int(params.get('fold', 0))  # 現在のフォールドの設定
SEED = int(params.get('seed', 3))  # シード値の設定
SS = int(params.get('subsample', 1))  # サブサンプル数の設定

print(N_FOLDS, FOLD, SEED, SS)  # 設定値を表示
```

---The following area is a Code cell (cell numver is 31)---
```python
from sklearn.model_selection import StratifiedKFold

# StratifiedKFoldの設定を行う関数
def get_folds(train): 
    # StratifiedKFoldを使ってフォールドを作成
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))

# トレーニングとテスト用のIDを取得
train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
# サブサンプル数に基づいてIDを選択
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # IDの重複がないことを確認
```

---The following area is a Code cell (cell numver is 32)---
```python
# 現在のマイクロ秒に基づいてシードを設定します
torch.manual_seed(datetime.datetime.now().microsecond)
random.seed(datetime.datetime.now().microsecond)
np.random.seed(datetime.datetime.now().microsecond)
```

---The following area is a Code cell (cell numver is 33)---
```python
# トレーニング、推論、および保存のフラグを設定
TRAIN = False
INFER = True 
SAVE = False
```

---The following area is a Code cell (cell numver is 34)---
```python
# LightGBMと特徴量抽出のためのライブラリをインポート
import lightgbm as lgb
from sklearn.feature_extraction.text import CountVectorizer
```

---The following area is a Code cell (cell numver is 35)---
```python
# LightGBM関連のフラグ
LGB = True
# トレーニングするかどうかのフラグ
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0
# 推論を行うかどうかのフラグ
INFER_LGB = not TRAIN and LGB
```

---The following area is a Code cell (cell numver is 36)---
```python
# 事前に保存したCountVectorizerモデルを読み込む
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # 主要なCountVectorizer
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # カスタムCountVectorizer
```

---The following area is a Code cell (cell numver is 37)---
```python
# シンメトリック対数変換を行う関数
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)

# Dense行列を処理する関数
def dense(x):
    x = np.asarray(x.astype(np.float32).todense())  # dense形式に変換
    x = symlog(x)  # シンメトリック対数変換を適用
    return x

# 特徴量を取得する関数
def get_features(df):
    # promptから特徴量を抽出
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    # response_aから特徴量を抽出
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    # response_bから特徴量を抽出
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    # 特徴量を計算
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat), 
        ])
    try: 
        # 投票モデルの数で特徴量を割る
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    extras = []  # 追加の特徴量を格納するリスト
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']  # 特徴量として使用する文字列
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)  # 特定の文字のカウントを追加
            
    # 文字列の長さと単語数を追加
    extras.append(df[c].str.len())
    extras.append(df[c].str.split().apply(lambda x: len(x)))
    
    extras = np.stack(extras, axis = 1)  # スタックして配列に変換
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 特徴量を拡張
    return np.hstack([v, extras])  # 全特徴量を結合して返す
```

---The following area is a Code cell (cell numver is 38)---
```python
# 事前に保存したLightGBMモデルを読み込む
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))
```

---The following area is a Code cell (cell numver is 39)---
```python
# 推論を行う場合の処理
if INFER and params.get('n_lgb', 1) > 0:
    df = test  # テストデータセットを使用
    yps = []  # 予測値を格納するリスト
    b = 1000  # バッチサイズ
    # テストデータをバッチごとに処理
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得
        ypms = []  # 各モデルの予測を格納するリスト
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))  # 各モデルで予測
        
        yps.append(np.stack(ypms).mean(0))  # 予測値の平均を格納
        print('.', end = '')  # 進行状況を表示
        
        # メモリ管理
        if len(yps) % 2 == 0:
            gc.collect()  # ガーベジコレクションを実行
    print()

    yp = np.concatenate(yps)  # すべての予測を結合
```

---The following area is a Code cell (cell numver is 40)---
```python
lgb_preds = yp  # LightGBMの予測値を格納
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
## 予測のブレンド

$\operatorname{preds} = 0.05 \cdot \operatorname{lgbm \ boosting \ preds} + 0.8 \cdot \operatorname{llama \ preds}$
```

---The following area is a Code cell (cell numver is 42)---
```python
# 予測のブレンドを行う
lgb_wt = 0.05  # LightGBMの重み
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測の加重平均
```

---The following area is a Code cell (cell numver is 43)---
```python
# 結果をデータフレームに保存
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])  # 予測結果のデータフレームを作成
display(out.head())  # 最初の数行を表示
```

---The following area is a Code cell (cell numver is 44)---
```python
# 結果をCSVファイルに保存
out.to_csv('submission.csv')  # 提出用ファイルを保存
```

** @@@ Jupyter Notebook numver 9, the number of votes :25 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Kaggleコンペティション「LMSYS - Chatbot Arena」における人間による好みの予測を行うためのモデルを構築することを目的としています。具体的には、ユーザーが生成したプロンプトに対する2つの異なる応答から、どちらがより好まれるかを予測します。

ノートブックでは、以下の手法とライブラリが活用されています。

1. **ライブラリのインストールおよびインポート**：
   - `bitsandbytes`、`transformers`、`tokenizers`、`peft`など、モデルのトレーニングや推論を効率化するためのライブラリを使用しています。
   - PyTorch（`torch`）、scikit-learn（`sklearn`）、pandas（`pd`）、numpy（`np`）などの一般的なデータ処理および機械学習ライブラリもインポートされています。

2. **データ処理**：
   - テストデータを読み込み、プロンプトやレスポンスの前処理を行います。具体的には、null値の処理やトークン化を行い、モデルの入力として使用するフォーマットに整えています。

3. **トークナイゼーションとモデルの設定**：
   - Hugging FaceのTransformersを利用して、`Gemma2ForSequenceClassification`モデルを使用し、トークナイザーを設定しています。これにより、テキストデータをモデルが理解できる形式に変換します。

4. **推論の実施**：
   - 2つの異なるGPUを用いて並行処理を行い、それぞれのモデルからの出力や確率を計算します。これにより、各モデルがどの程度の確率でユーザーの好みを予測するかを評価します。

5. **特徴量エンジニアリングとLightGBMの統合**：
   - `CountVectorizer`を利用して特徴量を抽出し、LightGBMを使った予測も組み合わせています。これにより、異なるアルゴリズムを統合した予測を可能にしています。

6. **結果のブレンド**：
   - LightGBMによる予測とLLMの予測を重み付けしてブレンドし、最終的な出力を生成します。この出力は、ユーザーの好みと応答の優劣を示す確率として出されます。

最終的に、生成された予測結果はCSVファイルに保存され、Kaggleに提出するための形式に整えられます。このノートブックは、複数の技術やモデルを駆使しながら、ユーザーの好みをより正確に推測するための包括的なアプローチを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# bitsandbytesをインストールします。これは、メモリ効率の良いモデルやオペレーションを提供するライブラリです。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
# transformersライブラリをインストールします。これは、さまざまな事前学習済みモデルを使用するためのライブラリです。
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
# tokenizersライブラリをインストールします。これは、トークン化処理を効率的に行うためのライブラリです。
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
# peftライブラリをインストールします。これは、学習のためのフレームワークであり、通常はメモリの使用効率を改善するために使用されます。
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 2)---
```python
# transformers、peft、accelerate、bitsandbytesの各ライブラリをインストールします。
# これらは、モデルを簡単に使用するための重要なライブラリです。
# インストールは最新バージョンにアップグレードし、特定のローカルリンクから行います。
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
このノートブックの作業は、以下のノートブックに触発されています。
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 4)---
```python
# 必要なライブラリをインポートします。
# timeライブラリは、時間の計測を行うために使用します。
import time
# dataclassを使用することで、簡単にデータ構造を定義できます。
from dataclasses import dataclass
# ThreadPoolExecutorを使って、スレッドプールを管理し並行処理を実行します。
from concurrent.futures import ThreadPoolExecutor

# torchライブラリは、PyTorchフレームワークの主要な部分で、テンソル計算を行います。
import torch
# sklearnライブラリは、機械学習のためのツールセットを提供します。
import sklearn
# numpyは、高速な数値計算のためのライブラリです。
import numpy as np
# pandasは、データ操作と解析のためのライブラリです。
import pandas as pd
# Gemma2ForSequenceClassificationは、テキストの分類を実行するためのモデルです。
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
# データコラトを使用して、データを適切な形に整形します。
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
# PeftModelは、モデルの微調整用のライブラリです。
from peft import PeftModel
# osライブラリは、オペレーティングシステムとのインターフェースを提供します。
import os
```

---The following area is a Code cell (cell numver is 5)---
```python
# さらに必要なライブラリをインポートします。
# Threadを使って、独立したスレッドを作成して並列処理を行います。
from threading import Thread
# gcライブラリでは、ガベージコレクションを手動で管理できます。
import gc
# osライブラリは、オペレーティングシステムとのファイルシステムにアクセスするための関数を提供します。
import os
# ioライブラリは、入出力操作をサポートします。
import io
# jsonライブラリは、JSONデータを扱うための機能を提供します。
import json
# randomライブラリは、ランダム数の生成を行います。
import random
# pickleライブラリは、Pythonオブジェクトのシリアライズ（保存）とデシリアライズ（読み込み）を行います。
import pickle
# zipfileライブラリは、ZIPファイルの読み書きをサポートします。
import zipfile
# datetimeライブラリは、日付と時刻を扱うための機能を提供します。
import datetime
# timeライブラリは、時間に関する操作を行います。
import time

# torchライブラリは、PyTorchフレームワークの主要な部分で、テンソル計算を行います。
import torch
# numpyは、高速な数値計算のためのライブラリです。
import numpy as np
# pandasは、データ操作と解析のためのライブラリです。
import pandas as pd
# AutoTokenizerは、様々なモデルのトークナイザーを自動的に読み込むことができます。
# LlamaModelおよびLlamaForSequenceClassificationは、Llamaモデルとその分類用のフレームワークです。
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
# PEFT関連のモデルと設定を管理するためのライブラリです。
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# CUDAの自動キャストを使用して、計算の精度を調整します。
from torch.cuda.amp import autocast
# IPython.displayを使用して、Jupyter Notebook上での表示操作を行います。
from IPython.display import display
# Fは、PyTorchの関数を含むモジュールで、特に損失関数や活性化関数を利用するために使います。
import torch.nn.functional as F
# tokenizersライブラリは、トークン化を効率的に行うためのライブラリです。
import tokenizers
```

---The following area is a Code cell (cell numver is 6)---
```python
# 使用可能なCUDAデバイスの数が2であることを確認します。
# これは、GPUが2つある場合に正しく動作することを保証します。
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 8)---
```python
# 設定を格納するためのデータクラスを定義します。
@dataclass
class Config:
    # gemma_dirは、Gemmaモデルのディレクトリのパスです。
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    # lora_dirは、LoRAモデルのチェックポイントのパスです。
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5600'
    # max_lengthは、入力シーケンスの最大長を定義します。
    max_length = 2048
    # batch_sizeは、一度に処理するデータのバッチサイズです。
    batch_size = 4
    # deviceは、モデルが使用するデバイス（GPU）を指定します。
    device = torch.device("cuda")    
    # ttaは、テスト時のデータ拡張を有効にするオプションです。
    tta = False  # test time augmentation. <prompt>-<model-b's response>-<model-a's response>
    # spread_max_lengthは、各入力にmax_length//3を適用するか、それとも連結された入力にmax_lengthを適用するかを決定します。
    spread_max_length = False  # whether to apply max_length//3 on each input or max_length on the concatenated input

# Configインスタンスを作成します。
cfg = Config()
```

---The following area is a Code cell (cell numver is 9)---
```python
# test.csvファイルを読み込み、テストデータをDataFrameとして格納します。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 10)---
```python
# テキストを処理する関数を定義します。
# この関数は、文字列を評価し、nullを空文字列に置き換えます。
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

# テストデータフレームの'prompt'列の各テキストをprocess_text関数を使用して処理します。
test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
# 'response_a'列の各テキストも同様に処理します。
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
# 'response_b'列の各テキストも同様に処理します。
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

# 処理したテストデータの最初の5行を表示します。
display(test.head(5))
```

---The following area is a Code cell (cell numver is 11)---
```python
# トークナイザーを使用してテキストをトークン化する関数を定義します。
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # 各プロンプトに"<prompt>:"を追加します。
    prompt = ["<prompt>: " + p for p in prompt]
    # 各レスポンスに"\n\n<response_a>:"または"\n\n<response_b>:"を追加します。
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    
    # spread_max_lengthがTrueの場合、各入力に対して最大長を3で割った長さを適用します。
    if spread_max_length:
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        # 各プロンプト、レスポンスA、レスポンスBの入力IDを連結します。
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        # 入力IDに対応するアテンションマスクを生成します。
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        # プロンプト、レスポンスA、レスポンスBを連結します。
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        # トークナイザーを使用してテキストをトークン化します。
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
        
    return input_ids, attention_mask  # トークン化された入力IDとアテンションマスクを返します。
```

---The following area is a Code cell (cell numver is 12)---
```python
# このセルの実行時間を計測します。
%%time

# GemmaTokenizerFastを指定したディレクトリからロードします。
tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
# トークナイザーにEOSトークンを追加する設定を行います。
tokenizer.add_eos_token = True
# パディングの方向を右に設定します。
tokenizer.padding_side = "right"

# 新しいデータフレームを作成します。
data = pd.DataFrame()
# 元のテストデータからIDをコピーします。
data["id"] = test["id"]
# トークナイザーを使用して、入力IDとアテンションマスクを生成します。
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
# 各入力IDの長さを計算して新しい列に追加します。
data["length"] = data["input_ids"].apply(len)

# 拡張データのための新しいデータフレームを作成します。
aug_data = pd.DataFrame()
# 元のテストデータからIDをコピーします。
aug_data["id"] = test["id"]
# response_aとresponse_bを入れ替えたトークナイゼーションを実行します。
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
# 各入力IDの長さを計算して新しい列に追加します。
aug_data["length"] = aug_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 13)---
```python
# GPU 0にベースモデルをロードします。
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,  # モデルをGPU 0にマッピングします。
    use_cache=False,  # キャッシュを使用しない設定です。
)

# GPU 1にベースモデルをロードします。
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,  # モデルをGPU 1にマッピングします。
    use_cache=False,  # キャッシュを使用しない設定です。
)
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRAモデルをベースモデルに適用します。
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # GPU 0のモデルにLoRA設定を適用します。
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # GPU 1のモデルにLoRA設定を適用します。
```

---The following area is a Code cell (cell numver is 15)---
```python
# 推論を行う関数を定義します。この関数は、指定されたデータフレームに基づいてモデルに対して推論を行います。
@torch.no_grad()  # 勾配計算を無効にします。推論時には不要です。
@torch.cuda.amp.autocast()  # 自動混合精度を使用して、計算を効率化します。
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝率を格納するリストを初期化します。
    
    # データフレームをバッチ処理で処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを決定します。
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得します。
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換します。
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換します。

        # 入力をパディングしてテンソル形式に変換します。
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        
        # モデルに対して推論を実行します。
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()  # 出力のロジットをソフトマックス関数で確率に変換します。
        
        # 各モデルの勝率をリストに追加します。
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    # データフレームに勝率の結果を追加します。
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df  # 更新されたデータフレームを返します。
```

---The following area is a Code cell (cell numver is 16)---
```python
# 処理の開始時間を記録します。
st = time.time()

# 入力の長さでソートし、動的パディングを最大限に活用します。
data = data.sort_values("length", ascending=False)

# データを2つのサブセットに分割します。これにより、トークン数がほぼ同じになることを期待します。
sub_1 = data.iloc[0::2].copy()  # 偶数のインデックスを持つ行をサブセット1にコピーします。
sub_2 = data.iloc[1::2].copy()  # 奇数のインデックスを持つ行をサブセット2にコピーします。

# スレッドプールを使用して並行処理を実行します。
with ThreadPoolExecutor(max_workers=2) as executor:
    # inference関数をサブセット1とサブセット2のそれぞれにモデルを適用して実行します。
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

# 結果を結合して新しいデータフレームを作成します。
result_df = pd.concat(list(results), axis=0)

# 勝率の確率を取得します。
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values

# 処理が完了するまでの経過時間を表示します。
print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 17)---
```python
# 処理の開始時間を記録します。
st = time.time()

# テスト時データ拡張（TTA）が有効な場合の処理を行います。
if cfg.tta:
    # 入力の長さでソートし、処理速度を向上させます。
    data = aug_data.sort_values("length", ascending=False)
    # データを2つのサブセットに分割します。
    sub_1 = data.iloc[0::2].copy()
    sub_2 = data.iloc[1::2].copy()

    # スレッドプールを使用して並行処理を実行します。
    with ThreadPoolExecutor(max_workers=2) as executor:
        # inference関数をサブセット1とサブセット2のそれぞれにモデルを適用して実行します。
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    # 結果を結合して新しいデータフレームを作成します。
    tta_result_df = pd.concat(list(results), axis=0)
    # TTAの結果の順序が反転しているため、winner_model_bを先にし、winner_model_aを後にします。
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # 元の結果とTTAの結果を平均します。
    proba = (proba + tta_proba) / 2

# 処理が完了するまでの経過時間を表示します。
print(f"elapsed time: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 18)---
```python
# 結果データフレームに勝率の確率を追加します。
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝率を設定します。
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を設定します。
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの勝率を設定します。

# 提出用のデータフレームを作成します。
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
# データフレームをCSVファイルにエクスポートします。
submission_df.to_csv('submission.csv', index=False)

# 提出用データフレームの内容を表示します。
display(submission_df)
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 20)---
```python
# テストデータとサンプル提出ファイルをCSV形式で読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込みます。
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルを読み込みます。
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## トークナイズ

## モデルの読み込み 
> 各GPUに1つのモデルを読み込みます。  

## 推論
```

---The following area is a Code cell (cell numver is 22)---
```python
# サンプル提出データフレームに結果データフレームから勝率の確率を追加します。
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 対象列を定義します。

# 結果データフレームの対象列をサンプル提出データフレームにコピーします。
sample_sub[TARGETS] = result_df[TARGETS]
```

---The following area is a Code cell (cell numver is 23)---
```python
# 結果データフレームから対象列の予測をNumPy配列として取得します。
llama_preds = result_df[TARGETS].values  # モデルの予測結果を配列に格納します。
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## LGBM + TF-IDF
```

---The following area is a Code cell (cell numver is 25)---
```python
# 実行環境を確認します。
TAG = 'lmsys-chatbot-arena'  # タグを定義します。
RUNPOD = os.path.exists('/workspace/')  # 実行環境がRunPodかどうかを確認します。
KAGGLE = not RUNPOD  # Kaggleで実行しているかどうかを判断します。

# Kaggleで実行している場合、メッセージを表示します。
if KAGGLE: 
    print('kaggle')  # Kaggle環境であることを表示します。
```

---The following area is a Code cell (cell numver is 26)---
```python
# pandasライブラリをインポートします。失敗した場合は必要なライブラリをインストールします。
try:
    import pandas as pd
except:
    # Kaggle環境に必要なパッケージをインストールします。
    !pip install -q kaggle  # Kaggle APIをインストールします。
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # データ処理と可視化のためのライブラリをインストールします。
    !pip install -q protobuf  # Protobufライブラリをインストールします。
    !pip install -q numba  # Numbaライブラリをインストールします。
```

---The following area is a Code cell (cell numver is 27)---
```python
# データのパスを設定します。実行環境に応じて異なるパスを指定します。
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)  # Kaggle環境であれば、競技用のデータパスを指定します。

# RunPod環境で実行している場合の設定を行います。
if RUNPOD:
    # Kaggle APIの設定ファイルが存在しない場合、作成しコピーします。
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle  # Kaggle用のディレクトリを作成します。
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # Kaggle APIの設定ファイルをコピーします。
        !chmod 600 /root/.kaggle/kaggle.json  # アクセス権を設定します。

    # データがまだダウンロードされていない場合、Kaggleからコンペティションデータをダウンロードします。
    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/  # Kaggleのコンペティションデータをダウンロードします。
        
    # データディレクトリが存在しない場合、ZIPファイルを解凍します。
    if not os.path.exists('/data/'):
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍します。
```

---The following area is a Code cell (cell numver is 28)---
```python
# 入力データとモデルのパスを設定します。
INPUT_PATH = '/kaggle/input/'  # Kaggleの入力データパス
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'  # モデルとロジットの保存先

# Kaggle環境の場合、モデルのパスを設定します。lsys-modelsを含むディレクトリを探索します。
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'
print(MODEL_PATH)  # モデルのパスを表示します。

# コードの保存先を設定します。
CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
# 保存先を設定します。
SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # Kaggle環境では保存先を空に設定します。
```

---The following area is a Code cell (cell numver is 29)---
```python
# トークナイザーの並列処理を無効にするための環境変数を設定します。
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 30)---
```python
# 訓練データ、テストデータ、サンプル提出データをCSV形式で読み込みます。
train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # 訓練データを読み込みます。
test = pd.read_csv(open(DATA + 'test.csv', 'r'))    # テストデータを読み込みます。
sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込みます。

# 訓練データとテストデータの行数を表示します。
print(len(train), len(test))  # 訓練データとテストデータの長さを表示します。
```

---The following area is a Code cell (cell numver is 31)---
```python
# モデルのパラメータを設定します。
params = {}
# 条件に応じてパラメータを設定します。このブロックは実行されないため、何も設定しません。
if False: 
    pass;
    params['subsample'] = 30
else:
    params['fold'] = -1  # foldの値を-1に設定します。

# モデルの訓練に関するその他のパラメータを設定します。
params['n_epochs'] = 1  # 訓練のエポック数を1に設定します。
params['n_lgb'] = 1  # LightGBMのモデル数を1に設定します。
params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルをDeBERTaに設定します。
```

---The following area is a Code cell (cell numver is 32)---
```python
# パラメータに基づいて各種設定を行います。
# foldが0未満の場合、FULLをTrueに設定します。
FULL = params.get('fold', 0) < 0
# n_foldsの数を取得します。デフォルトは3です。
N_FOLDS = int(params.get('n_folds', 3))  
# foldの値を取得します。デフォルトは0です。
FOLD = int(params.get('fold', 0))
# seed値を取得します。デフォルトは3です。
SEED = int(params.get('seed', 3))
# subsample値を取得します。デフォルトは1です。
SS = int(params.get('subsample', 1))

# 設定した値を表示します。
print(N_FOLDS, FOLD, SEED, SS)  # N_FOLDS、FOLD、SEED、SSの値を表示します。
```

---The following area is a Code cell (cell numver is 33)---
```python
# StratifiedKFoldをインポートします。これは層化K-Foldクロスバリデーションを実施するためのクラスです。
from sklearn.model_selection import StratifiedKFold

# K-Foldのインデックスを取得する関数を定義します。
def get_folds(train): 
    # StratifiedKFoldを使用して、訓練データの分割を行い、インデックスを返します。
    return list(StratifiedKFold(N_FOLDS, random_state=SEED, shuffle=True)\
                    .split(X=np.zeros(len(train)), y=train.iloc[:, -3:].idxmax(1)))  # 最後の3列のインデックスが最大のものをラベルとして使用します。

# 指定されたFOLDまたはFULLを考慮して、訓練データとテストデータのIDを取得します。
train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
# subsampleが1より大きい場合、IDを間引いたリストにします。
if SS > 1:
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

# 訓練データとテストデータのIDの長さを表示します。
print(len(train_ids), len(test_ids))
# 訓練データとテストデータのIDで重複がないことを確認します。
assert set(train_ids) & set(test_ids) == set()  # 重複がある場合、エラーを出力します。
```

---The following area is a Code cell (cell numver is 34)---
```python
# 乱数のシードを設定します。これにより、毎回の実行が再現可能になります。
torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchのシードを設定します。
random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールのシードを設定します。
np.random.seed(datetime.datetime.now().microsecond)  # NumPyのシードを設定します。
```

---The following area is a Code cell (cell numver is 35)---
```python
# モードの設定を行います。
TRAIN = False  # 訓練モードを無効にします。
INFER = True   # 推論モードを有効にします。
SAVE = False   # 保存モードを無効にします。
```

---The following area is a Code cell (cell numver is 36)---
```python
# LightGBMライブラリと、テキストデータの前処理を行うためのCountVectorizerをインポートします。
import lightgbm as lgb  # LightGBMライブラリをインポートします。これは高速な決定木アルゴリズムです。
from sklearn.feature_extraction.text import CountVectorizer  # テキストデータをベクトル化するためのクラスをインポートします。
```

---The following area is a Code cell (cell numver is 37)---
```python
# LightGBMの利用設定を行います。
LGB = True  # LightGBMを使用する設定をTrueにします。
# 訓練時のLightGBMの実行設定を決定します。
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # 訓練中にLightGBMを実行すべきかどうかを決定します。
# 推論時のLightGBMの実行設定を決定します。
INFER_LGB = not TRAIN and LGB  # 訓練中でない場合にLightGBMを使用する設定を決定します。
```

---The following area is a Code cell (cell numver is 38)---
```python
# 学習済みのCountVectorizerオブジェクトをファイルから読み込みます。
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # CountVectorizerを読み込みます。
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # 別のCountVectorizerを読み込みます。
```

---The following area is a Code cell (cell numver is 39)---
```python
# シンメトリックログ変換を行う関数を定義します。
def symlog(x):
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # xのシンメトリックログを計算します。

# 疎行列をDense行列に変換し、シンメトリックログ変換を適用する関数を定義します。
def dense(x):
    x = np.asarray(x.astype(np.float32).todense())  # 疎行列をDense行列に変換します。
    x = symlog(x)  # シンメトリックログ変換を適用します。
    return x

# 特徴量を取得するための関数を定義します。
def get_features(df):
    # プロンプトから生成される特徴量
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])  # CountVectorizerを使用して特徴量を抽出します。

    # レスポンスAから生成される特徴量
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    
    # レスポンスBから生成される特徴量
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    # AとBのレスポンスの比較特徴量を作成します。
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat),  # レスポンスAとレスポンスBの差分を含む特徴量
        ])
    
    # モデル数によってvを調整します。
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)
    except:
        pass

    # 追加の特徴量を生成します。
    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']  # 特徴量を生成するための追加項目
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)  # 特定の文字の出現回数をカウントします。
            
    extras.append(df[c].str.len())  # 各列の文字数を追加します。
    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 各列の単語数を追加します。
    
    extras = np.stack(extras, axis=1)  # 追加特徴量を1つの配列にまとめます。
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 追加特徴量を変換します。
    
    return np.hstack([v, extras])  # 最後に全ての特徴量を結合して返します。
```

---The following area is a Code cell (cell numver is 40)---
```python
# 学習済みのLightGBMモデルをファイルから読み込みます。
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込みます。
```

---The following area is a Code cell (cell numver is 41)---
```python
# 推論が有効で、LightGBMモデルの数が0より大きい場合の処理を行います。
if INFER and params.get('n_lgb', 1) > 0:
    df = test  # テストデータフレームをdfに設定します。
    yps = []  # 複数の予測結果を格納するリストを初期化します。
    b = 1000  # 一度に処理するデータのバッチサイズを設定します。
    
    # データをバッチ処理で推論します。
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得します。
        ypms = []  # 各モデルの予測結果を格納するリストを初期化します。
        
        # 全てのLightGBMモデルに対して予測を行います。
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))  # モデルからの予測確率を取得します。
        
        # 各モデルの予測確率を平均化してypsに追加します。
        yps.append(np.stack(ypms).mean(0))
        print('.', end = '')  # 進行状況を表示します。
        
        # 2バッチごとにガベージコレクションを実行します。
        if len(yps) % 2 == 0:
            gc.collect()
    print()  # 改行します。

    # すべての予測結果を連結します。
    yp = np.concatenate(yps)  # 最終的な予測結果を取得します。
```

---The following area is a Code cell (cell numver is 42)---
```python
# LightGBMモデルからの最終的な予測結果をlgb_predsに格納します。
lgb_preds = yp  # 推論結果をlgb_predsに設定します。
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
## 予測のブレンド

$\operatorname{preds} = 0.12 \cdot \operatorname{LGBMブースティング予測} + 0.8 \cdot \operatorname{Llama予測}$
```

---The following area is a Code cell (cell numver is 44)---
```python
# LGBMの重みを設定し、予測をブレンドします。
lgb_wt = 0.6  # LGBMモデルの重みを設定します。
# LGBM予測とLlama予測を重み付けして加算し、最終的な予測を得ます。
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測のブレンドを実行します。
```

---The following area is a Code cell (cell numver is 45)---
```python
# 最終的な予測結果をDataFrameとして作成します。
out = pd.DataFrame(preds, index=df.id, columns=train.columns[-3:])  # 予測値をDataFrameに格納します。
# 最初の数行を表示します。
display(out.head())  # 生成したDataFrameの先頭を表示します。
```

---The following area is a Code cell (cell numver is 46)---
```python
# 最終的な予測結果をCSVファイルとして保存します。
out.to_csv('submission.csv')  # 生成したDataFrameを'submission.csv'として出力します。
```

---The following area is a Markdown cell (cell numver is 47)---
```markdown
---

# コメント 

> ## Qihang Wang
> 
> 素晴らしい仕事です。LightGBMの訓練損失と検証損失を共有していただけますか？
> 
> 
> 

---
```

** @@@ Jupyter Notebook numver 10, the number of votes :22 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のための機械学習モデルの構築に取り組んでいます。主な目標は、ユーザーのプロンプトに対する二つの異なるLLM（大規模言語モデル）が生成した応答のどちらが好まれるかを予測することです。

### 問題の概要
ノートブックでは、公開されているデータセットからトレーニングデータとテストデータを読み込み、それに基づいて予測を行うモデルを開発しています。

### 使用している手法とライブラリ
1. **データ処理**:
   - `pandas`や`datasets`ライブラリを用いて、データの読み込みや前処理を行っています。
   - テキストデータに対するクリーニング処理（小文字変換、URLやストップワードの削除など）を行う関数を定義しています。

2. **トークナイゼーション**:
   - `transformers`ライブラリのBERTモデルを使用して、テキストデータをトークン化しています。このプロセスでは、トークナイザーを初期化し、データセットへのトークン化を施しています。

3. **モデル構築**:
   - TensorFlowとKerasを利用し、カスタムBERTモデルを定義しています。`TFBertModel`を入れ子にしたクラスとして実装しており、データをBERTに通じて処理する層を作成しています。

4. **学習と評価**:
   - モデルは、`Adam`オプティマイザーを使用してコンパイルされ、エポックごとにトレーニングが行われます。訓練中の進捗は`TqdmCallback`を用いて表示されています。

5. **予測の生成**:
   - テストデータに対してモデルを用いて予測を実施し、その結果を`DataFrame`として整形してCSVファイルとして保存します。

ノートブック全体を通じて、高度な深層学習フレームワークや自然言語処理のテクニックが使用されており、特にBERTモデルの利用により、テキストデータに対して強力な表現学習を行なっています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# osモジュールをインポートします。OS関連の機能を扱うために使用します。
import os
# tensorflowライブラリをインポートします。深層学習のフレームワークです。
import tensorflow as tf
# datasetsモジュールからload_datasetとDatasetDictをインポートします。データセットの読み込みと管理に使用します。
from datasets import load_dataset, DatasetDict
# transformersモジュールからモデルとトークナイザをインポートします。自然言語処理に用います。
from transformers import BertTokenizer, BertTokenizerFast, TFBertModel, DataCollatorWithPadding, TFAutoModel
# kerasのレイヤーをインポートします。ニューラルネットワークの構築に必要なレイヤーです。
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout, GlobalAveragePooling2D
# kerasモデルをインポートします。モデルの構築と訓練に使います。
from tensorflow.keras.models import Model
# shutilモジュールをインポートします。ファイルやフォルダの操作に使用します。
import shutil
# pandasライブラリをインポートします。データの操作と解析用のライブラリです。
import pandas as pd
# tqdmのkerasコールバックをインポートします。進捗バーを表示するために使用します。
from tqdm.keras import TqdmCallback
# 正規表現を使うためのreモジュールをインポートします。
import re
# 数学関数を利用するためにmathモジュールをインポートします。
import math
# matplotlibのpyplotをインポートします。データの可視化に使用するグラフ描画ライブラリです。
import matplotlib.pyplot as plt
# multiprocessingモジュールをインポートします。マルチプロセッシングを扱うためのライブラリです。
import multiprocessing
# nltkライブラリをインポートします。自然言語処理に関連する機能を提供します。
import nltk
# nltkからストップワードをインポートします。テキスト処理で無視される単語のリストです。
from nltk.corpus import stopwords
# nltkから単語トークナイザをインポートします。文章を単語に分割するために使用します。
from nltk.tokenize import word_tokenize
# 学習率スケジューラのコールバックをインポートします。学習率を動的に変更するために使用します。
from tensorflow.keras.callbacks import LearningRateScheduler
# numpyライブラリをインポートします。数値計算のためのライブラリです。
import numpy as np
# kerasをインポートします。深層学習のためのライブラリです。
from tensorflow import keras
# Adam最適化アルゴリズムをインポートします。モデルの訓練で広く使用される最適化手法です。
from tensorflow.keras.optimizers import Adam
```

---The following area is a Code cell (cell numver is 2)---
```python
# GPUの利用可能性をチェックします。
gpus = tf.config.experimental.list_physical_devices('GPU')
# GPUが存在する場合、以下の処理を行います。
if gpus:
    try:
        # すべての物理GPUに対してメモリ成長を設定します。
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        # 論理GPUのリストを取得します。
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        # 物理GPUの数と論理GPUの数を表示します。
        print(len(gpus), "物理GPU,", len(logical_gpus), "論理GPU")
    # ランタイムエラーが発生した場合、そのエラーメッセージを表示します。
    except RuntimeError as e:
        print(e)
```

---The following area is a Code cell (cell numver is 3)---
```python
# ハードウェアを検出し、適切な分散戦略を返します。
try:
    # TPUの検出を行います。TPU_NAME環境変数が設定されている場合はパラメータは必要ありません。
    # これはKaggle上では常に当てはまります。
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    # TPUのマスターアドレスを表示します。
    print('TPUで実行中: ', tpu.master())
# 値エラーが発生した場合、TPUをNoneに設定します。
except ValueError:
    tpu = None

# TPUが存在する場合
if tpu:
    # TPUクラスタに接続します。
    tf.config.experimental_connect_to_cluster(tpu)
    # TPUシステムを初期化します。
    tf.tpu.experimental.initialize_tpu_system(tpu)
    # TPU用の分散戦略を設定します。
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # TensorFlowのデフォルトの分散戦略を使用します。これはCPUおよび単一GPUで機能します。
    strategy = tf.distribute.MirroredStrategy()

# 現在の戦略で同期しているレプリカの数を表示します。
print("レプリカの数: ", strategy.num_replicas_in_sync)
```

---The following area is a Code cell (cell numver is 4)---
```python
# データセットのパスを設定します。
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのパス
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'    # テストデータのパス

# データセットを読み込みます。
# CSVファイルからトレーニングデータを読み込みます。
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
# CSVファイルからテストデータを読み込みます。
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# テストデータセットからIDを保存します。
test_ids = test_dataset['id']  # テストデータのIDを取得します。
```

---The following area is a Code cell (cell numver is 5)---
```python
# テストセットに不足している列を追加します。
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    # 各列がテストデータセットに存在しない場合
    if col not in test_dataset.column_names:
        # 空の文字列で列を追加します。行数はテストデータセットの長さと同じです。
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# 列のデータ型をint64に変換します。
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    # トレーニングデータセットに対して、指定した列を整数に変換します。
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    # テストデータセットに対して、指定した列を整数に変換します。
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 6)---
```python
# ローカルでbert-base-casedのファイルを使用します。
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'  # ソースディレクトリのパス

# モデルの保存先ディレクトリを指定します。
model_dir = '/kaggle/working/bert-base-cased'
# 保存先ディレクトリが存在しない場合は作成します。
os.makedirs(model_dir, exist_ok=True)

# 必要なファイルをソースディレクトリからモデルディレクトリにコピーします。
shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)  # 設定ファイルのコピー
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)  # PyTorchモデルファイルのコピー
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)  # TensorFlowモデルファイルのコピー
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)  # トークナイザー設定ファイルのコピー
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)  # ボキャブラリファイルのコピー
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)  # モデルカードのコピー
```

---The following area is a Code cell (cell numver is 7)---
```python
# ストップワードのファイルパスを指定します。
stopwords_path = '/kaggle/input/stopwords/stopwords/english'

# ストップワードをファイルから読み込むための関数を定義します。
def load_stopwords(stopwords_path):
    # 指定したパスのファイルを開きます。
    with open(stopwords_path, 'r') as file:
        # ファイルの内容を読み込み、行ごとのリストに分割します。
        stopwords = file.read().splitlines()
    # ストップワードのリストをセットとして返します。集合にすることで重複を排除します。
    return set(stopwords)
```

---The following area is a Code cell (cell numver is 8)---
```python
# トークナイザーを初期化します。
tokenizer = BertTokenizerFast.from_pretrained(model_dir)  # 事前学習済みのトークナイザーを指定したディレクトリから読み込みます。
# ストップワードをダウンロードします。
stopwords = load_stopwords(stopwords_path)  # ストップワードをファイルから読み込みます。

# テキストクリーニングのための関数を定義します。
def clean_text(text):
    # テキストを小文字に変換します。
    text = text.lower()
    # URLを削除します。
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    # @メンションや#ハッシュタグを削除します。
    text = re.sub(r'\@\w+|\#','', text)
    # 句読点を削除します（英数字およびスペース以外の文字）。
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    # ストップワードを削除します。テキストを単語に分割し、ストップワードに含まれない単語だけを結合します。
    text = ' '.join([word for word in text.split() if word not in stopwords]) 
    return text  # クリーンアップされたテキストを返します。
```

---The following area is a Code cell (cell numver is 9)---
```python
def tokenize_function(examples):
    # 各テキストフィールドをクリーンアップします。
    cleaned_prompts = [clean_text(text) for text in examples['prompt']]  # プロンプトのクリーニング
    cleaned_responses_a = [clean_text(text) for text in examples['response_a']]  # 応答Aのクリーニング
    cleaned_responses_b = [clean_text(text) for text in examples['response_b']]  # 応答Bのクリーニング
    
    # クリーンアップされたテキストをトークン化します。
    return tokenizer(cleaned_prompts,
                     cleaned_responses_a,
                     cleaned_responses_b,
                     padding="max_length",  # 最大長さまでパディングします。
                     truncation=True,  # 最大長さを超えるテキストは切り捨てます。
                     max_length=512)  # 最大のトークン長を512に設定します。
```

---The following area is a Code cell (cell numver is 10)---
```python
# 関数の使用例を示します。
examples = {
    'prompt': ["これはサンプルプロンプトです。"],  # サンプルプロンプト
    'response_a': ["これはサンプル応答Aです。"],  # サンプル応答A
    'response_b': ["これはサンプル応答Bです。"]   # サンプル応答B
}

# トークン化された出力を得ます。
tokenized_output = tokenize_function(examples)
# トークン化された出力を表示します。
print(tokenized_output)
```

---The following area is a Code cell (cell numver is 11)---
```python
# マルチプロセッシングを使用してトークン化とクリーニング関数を適用します。num_procには使用するプロセッサの数を指定します。
num_proc = multiprocessing.cpu_count()  # 利用可能なCPUのコア数を取得します。

# より良いエラーハンドリングのためにtry-exceptブロックを追加します。
try:
    # トレーニングデータセットに対してトークン化関数を適用します。
    tokenized_datasets = train_dataset.map(tokenize_function, batched=True)
    # テストデータセットに対してトークン化関数を適用します。
    test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)
# エラーが発生した場合、そのエラーメッセージを表示します。
except Exception as e:
    print(f"トークン化中にエラーが発生しました: {e}")
    
# トークナイザーを使用して、パディングを行うためのデータコレータを作成します。
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークン化後にデバッグ用の印刷を追加します。
print("サンプルトークン化されたトレーニングデータセットのエントリー:")
print(tokenized_datasets[0])  # トークン化された最初のトレーニングデータセットのエントリーを表示します。
# トークン化されたトレーニングデータセットが空の場合、エラーを発生させます。
if len(tokenized_datasets) == 0:
    raise ValueError("トークン化されたトレーニングデータセットが空です。")
# トークン化されたテストデータセットが空の場合、エラーを発生させます。
if len(test_tokenized_datasets) == 0:
    raise ValueError("トークン化されたテストデータセットが空です。")
# デバッグ用にカラム名を印刷します。
print(f"トークン化されたトレーニングデータセットのカラム名: {tokenized_datasets.column_names}")
print(f"トークン化されたテストデータセットのカラム名: {test_tokenized_datasets.column_names}")
```

---The following area is a Code cell (cell numver is 13)---
```python
# tf.data.Datasetに変換し、正しい形状にします。
def convert_to_tf_dataset(dataset, label_col=None, for_inference=False):
    input_columns = tokenizer.model_input_names  # トークナイザーのモデル入力名を取得します。
    
    # ラベル列が指定されており、推論用でない場合
    if label_col and not for_inference:
        # ラベル列を除く他の列を削除します。
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in input_columns])
    else:
        # インプット列に含まれない他の列を削除します。
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in input_columns])
    
    # ラベルがシーケンスでないことを確認します。
    if label_col:
        dataset = dataset.map(lambda x: {label_col: int(x[label_col])})  # ラベルを整数に変換します。
    
    shuffle = not for_inference  # 推論用でない場合はデータをシャッフルします。
    batch_size = 16 if for_inference else 450  # 推論用の場合はバッチサイズを16、それ以外は450に設定します。

    # データセットをtf.data.Datasetに変換します。
    tf_dataset = dataset.to_tf_dataset(
        columns=input_columns,  # 入力列
        label_cols=[label_col] if label_col and not for_inference else None,  # ラベル列
        shuffle=shuffle,  # シャッフルオプション
        batch_size=batch_size,  # バッチサイズ
        collate_fn=DataCollatorWithPadding(tokenizer=tokenizer)  # パディングを行うための関数
    )

    return tf_dataset  # 変換されたtf.data.Datasetを返します。
```

---The following area is a Code cell (cell numver is 14)---
```python
# 変換を実行します。
try:
    # トークン化されたデータセットをtf.data.Dataset形式に変換します。ラベルとして'winner_model_a'を指定します。
    train_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
    # テストデータセットも同様に変換します。ここでもラベルとして'winner_model_a'を指定しますが、通常は異なるラベルを使用したほうが良いでしょう。
    test_tf_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')
# 変換中にエラーが発生した場合、そのエラーメッセージを表示します。
except Exception as e:
    print(f"データセットの変換中にエラーが発生しました: {e}")
```

---The following area is a Code cell (cell numver is 15)---
```python
# データセットの変換後にデバッグ用の印刷を追加します。
print("変換されたトレーニングtf.data.Datasetのサンプル:")
# トレーニングデータセットから1つのバッチを取得します。
for batch in train_tf_dataset.take(1):
    inputs, labels = batch  # 入力とラベルを取得します。
    # 入力IDの形状を表示します。
    print(f'入力IDの形状: {inputs["input_ids"].shape}')
    # アテンションマスクの形状を表示します。
    print(f'アテンションマスクの形状: {inputs["attention_mask"].shape}')
    # ラベルの形状を表示します。
    print(f'ラベルの形状: {labels.shape}')
```

---The following area is a Code cell (cell numver is 16)---
```python
# カスタムモデルを構築します。
class BertLayer(Layer):
    def __init__(self, **kwargs):
        super(BertLayer, self).__init__(**kwargs)
        # 事前学習済みのBERTモデルを読み込みます。
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)
    
    def call(self, inputs):
        # 入力からinput_idsとattention_maskを取得します。
        input_ids, attention_mask = inputs
        # BERTモデルに入力を渡して出力を取得します。
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        return outputs.last_hidden_state  # BERTの最終隠れ状態を返します。

def create_keras_model():
    # 入力ID用の入力レイヤーを定義します。
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')
    # アテンションマスク用の入力レイヤーを定義します。
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')

    # BERT層を通して出力を取得します。
    bert_output = BertLayer()([input_ids, attention_mask])
    # 出力をグローバル平均プーリングします。
    pooled_output = GlobalAveragePooling1D()(bert_output)
    # 最終出力レイヤーを定義します。ここでは3つのクラス用にソフトマックス活性化関数を使用します。
    output = Dense(3, activation='softmax')(pooled_output)

    # モデルを構築します。入力と出力を指定します。
    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model  # 作成したモデルを返します。
```

---The following area is a Code cell (cell numver is 17)---
```python
# ストラテジースコープ内でモデルを構築し、コンパイルします。
with strategy.scope():
    # Kerasモデルを作成します。
    model = create_keras_model()
    # モデルをコンパイルします。最適化手法としてAdamを使用し、学習率は5e-5に設定します。
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),
                  loss='sparse_categorical_crossentropy',  # 損失関数には疎形式のカテゴリカル交差エントロピーを使用します。
                  metrics=['accuracy'])  # 評価指標には精度を使用します。

    # トレーニングデータセットでモデルを訓練します。エポック数は3です。
    model.fit(train_tf_dataset, epochs=3, callbacks=[TqdmCallback(verbose=1)])  # TqdmCallbackを使用して進捗を表示します。
```

---The following area is a Code cell (cell numver is 18)---
```python
# 予測を取得します。
predictions = model.predict(test_tf_dataset)  # テストデータセットに対して予測を行います。

# 長さをチェックします。
print(f"テストIDの長さ: {len(test_ids)}")  # テストIDの数を表示します。
print(f"予測の形状: {predictions.shape}")  # 予測の形状を表示します。
# テストIDの数と予測の数が一致しない場合、予測をテストIDの数に合わせて調整します。
if len(test_ids) != predictions.shape[0]:
    predictions = predictions[:len(test_ids)]  # 余分な予測を切り捨てます。

# データフレームを作成します。
submission = pd.DataFrame({
    'id': test_ids,  # テストIDをID列として追加します。
    'winner_model_a': predictions[:, 0],  # モデルAの予測を列として追加します。
    'winner_model_b': predictions[:, 1],  # モデルBの予測を列として追加します。
    'winner_model_tie': predictions[:, 2]  # タイの予測を列として追加します。
})

# データフレームをCSVファイルとして保存します。
submission.to_csv('submission.csv', index=False)  # インデックスなしでCSVファイルに保存します。
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# コメント

> ## ハッサン・シャヒディ
> 
> 素晴らしい仕事です！その調子でがんばってください🔥
> 
> 

---

> ## ギータ・ケオテ
> 
> よくやりました！その調子でがんばってください👍
> 
> 

---

> ## フー・フランシー
> 
> あなたの仕事が好きです！
> 
> 

---

> ## デッドQ
> 
> Kaggleコミュニティのための良いコードです。ありがとうございます！その調子でがんばってください！
> 
> 

---

> ## ヴァンシュ・シャルマ
> 
> [@fulgrimthe](https://www.kaggle.com/fulgrimthe) あなたの分析は明確で非常に役立ちます。素晴らしい仕事です！
> 
> 

---
```

** @@@ Jupyter Notebook numver 11, the number of votes :21 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleのコンペティション「LMSYS - Chatbot Arena」において、与えられた埋め込みデータを用いてシンプルな分類器を訓練し、推論を行うことを目的としています。具体的には、異なるLLM（大規模言語モデル）による応答に対して、どちらのモデルの応答がユーザーに好まれるかを予測するモデルを構築します。

### 問題に取り組む背景
このNotebookは、Chatbot Arenaからの埋め込みを使用して、ユーザーがどのモデルの応答を選好するかを予測するというタスクに挑戦しています。モデルは、与えられた埋め込みを基に応答の優劣を判断する必要があります。

### 使用する手法とライブラリ
1. **埋め込みの計算**: 'Gemma 2'という大規模言語モデルを使用して、テキストデータから埋め込みを生成します。これをPyTorchとTransformersライブラリを用いて実装します。埋め込みは数値的な表現で、これが後の分類タスクに利用されます。

2. **分類器の訓練**: `CatBoostClassifier`を使用して、訓練データに基づいてモデルを構築します。CatBoostは、勾配ブースティングフレームワークの一種で、多くのデータセットにおいて良好な性能を示すとされています。パラメータ調整や学習率、イテレーション数の設定も行います。

3. **データの前処理とトークナイジング**: テストデータに対して、ユーザープロンプトとモデルの応答を適切に整形し、トークナイザーを使用してトークン化を行います。

4. **推論**: 訓練された分類器を用いて、テストデータに対する予測確率を計算します。

5. **出力形式**: 最終的な予測結果はCSV形式で保存され、提出可能な形式に整形されます。

全体として、このNotebookは深層学習と機械学習の技術を統合し、特に自然言語処理のタスクにおけるユーザーの好みを予測するための方法論を示しています。また、効率的なメモリ管理や多くの機能が備わったライブラリの利用を通じて、スムーズな実行を実現しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Gemma 2 - 9b
ここでは、[こちら](https://www.kaggle.com/code/kishanvavdara/gemma-2-9b-part-1?scriptVersionId=186083288)から得られた計算された埋め込みを入力として使用して、シンプルな分類器を訓練し、テスト用の埋め込みを計算し、訓練した分類器を用いて推論を行います。それでは始めましょう！

もしこの内容が役に立ったと思ったら、いいねを押してください！

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
# bitsandbytesライブラリをインストールします。
# -qオプションは出力を抑制し、-Uオプションは最新バージョンにアップグレードします。
# --no-indexオプションはPyPIインデックスを使用しないことを指定します。
# --find-linksオプションは、指定したローカルのパスを使ってライブラリをインストールします。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip

# transformersライブラリをインストールします。
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします。
import os  # オペレーティングシステムとの対話を行うためのライブラリ
import gc  # ガベージコレクタを扱うためのライブラリ
import re  # 正規表現を扱うためのライブラリ
from time import time  # 時間を計測するためのライブラリ

import torch  # PyTorchライブラリ
import transformers  # Hugging FaceのTransformersライブラリ
import sklearn  # Scikit-learnライブラリ
import random  # ランダム数生成のためのライブラリ
import numpy as np  # NumPyライブラリ（数値計算のため）
import pandas as pd  # Pandasライブラリ（データ操作のため）
import matplotlib.pyplot as plt  # データの可視化を行うためのライブラリ

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig  # Gemma2モデルとトークナイザーのインポート

import time  # 再度時間計測用のライブラリをインポート
from catboost import CatBoostClassifier, Pool  # CatBoost分類器のインポート
from sklearn.model_selection import train_test_split  # データ分割のための関数をインポート
from sklearn.metrics import accuracy_score, log_loss  # 評価指標の関数をインポート

from torch.cuda.amp import autocast  # 自動混合精度訓練のためのモジュールをインポート
from threading import Thread  # スレッドを扱うためのライブラリ

# メモリ効率の良いシステムダイナミクスプログラミングを有効にします。
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# CUDAが使用できない場合はエラーメッセージを表示します。
if (not torch.cuda.is_available()): 
    print("Sorry - GPU required!")  # GPUが必要です。申し訳ありません！
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 分類器の訓練
```

---The following area is a Code cell (cell numver is 5)---
```python
# 訓練データをCSVファイルから読み込みます。
train_df = pd.read_csv('/kaggle/input/gemma-2-9b-part-1/train_embed.csv')

# 訓練用の埋め込みデータをNumPy配列として読み込みます。
train_embed = np.load('/kaggle/input/gemma-2-9b-part-1/gemma2_train_embed.npy')

# 'winner_model_a', 'winner_model_b', 'winner_tie'の列の中で最大の値のインデックスを取得し、
# それを'label'列としてデータフレームに追加します。
train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  # どのモデルが勝者かを示すラベルを作成します。
```

---The following area is a Code cell (cell numver is 6)---
```python
# データを訓練セットとテストセットに分割します。
Targets = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなる列を定義します。

y = train_df['label'].values  # ラベルの配列を取得します。
# train_test_split関数を使って、訓練データとテストデータのインデックスを分割します。
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

# 訓練データとそのラベルを設定します。
X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values
# テストデータとそのラベルを設定します。
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values

# 訓練データとテストデータの形状を表示します。
print(X_train.shape, y_train.shape)  # 訓練データの形状
print(X_test.shape, y_test.shape)  # テストデータの形状
```

---The following area is a Code cell (cell numver is 7)---
```python
# ここでは、デフォルト設定で分類器を使用します。パラメータの調整を試みることもできます。

# CatBoostClassifierを初期化します。
model_cb = CatBoostClassifier(
    iterations=1000,  # 最大イテレーション数
    learning_rate=0.03,  # 学習率
    loss_function='MultiClass',  # マルチクラス分類の損失関数
    eval_metric='MultiClass',  # 評価指標
    early_stopping_rounds=10,  # 早期停止のためのラウンド数
    task_type='GPU',  # GPUを使用します。
    devices='0:1',  # 使用するGPUデバイスの指定
    verbose=100)  # 進捗状況の表示間隔

# モデルをフィッティングします。
model_cb.fit(X_train, y_train, 
              eval_set=(X_test, y_test),  # 検証セットとしてテストデータを指定します。
              early_stopping_rounds=50)  # 早期停止のためのラウンド数
```

---The following area is a Code cell (cell numver is 8)---
```python
# テストデータに対する予測確率を取得します。
y_pred_proba = model_cb.predict_proba(X_test)
# テストデータに対する予測ラベルを取得します。
y_pred = model_cb.predict(X_test)

# モデルの評価を行います。
logloss = log_loss(y_test, y_pred_proba)  # ログ損失を計算します。
accuracy = accuracy_score(y_test, y_pred)  # 精度を計算します。
gc.collect()  # ガベージコレクションを実行してメモリを解放します。

# 結果を表示します。
print(f'Log Loss: {logloss:.3f}')  # ログ損失を表示します。
print(f'Accuracy: {accuracy:.3f}')  # 精度を表示します。
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
この分類器を推論に使用します。

# Gemma 2の読み込み
```

---The following area is a Code cell (cell numver is 10)---
```python
# モデルのパスと設定を定義します。
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'  # Gemma 2のモデルパス
MAX_LENGTH = 1024  # 最大シーケンス長
BATCH_SIZE = 2  # バッチサイズ
    
# 使用するGPUデバイスを指定します。
device0 = torch.device('cuda:0')  # デバイス0を設定
device1 = torch.device('cuda:1')  # デバイス1を設定

# トークナイザーをモデルから読み込みます。
tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

# 4ビット量子化の設定を定義します。
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビットで読み込む設定
    bnb_4bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定
    bnb_4bit_use_double_quant=False)  # ダブル量子化を使用しない設定

# モデル0を読み込みます。
model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",  # float16バージョンを指定
                                        device_map='cuda:0',  # デバイスマップをデバイス0に設定
                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用        

# モデル1を読み込みます。
model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",  # float16バージョンを指定
                                        device_map='cuda:1',  # デバイスマップをデバイス1に設定
                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用
```

---The following area is a Code cell (cell numver is 11)---
```python
# 入力文字列を処理する関数を定義します。
def process(input_str):
    # 文字列の前後の角括弧を取り除きます。
    stripped_str = input_str.strip('[]')
    # 文字列をカンマで分割し、各文から前後の引用符を取り除きます。
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # 最後の文を返します。文がなければ空文字列を返します。
    return sentences[-1] if sentences else ''
  
# テストデータをCSVファイルから読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# 各カラムに対してprocess関数を適用し、結果を新しいカラムに保存します。
test.loc[:, 'prompt'] = test['prompt'].apply(process)  # ユーザープロンプトの処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答の処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答の処理

# テキストを指定の形式で組み合わせて新しいカラムを作成します。
test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'

# 生成されたテキストの最初の要素を表示します。
print(test['text'][0])  # 最初のテキストを表示します。
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 13)---
```python
# テストデータのテキストをトークナイズします。
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',  # 最大長さに合わせてパディングを行います。
                   max_length=MAX_LENGTH,  # 最大長さを設定します。
                   truncation=True,  # 長さが超えた場合は切り捨てます。
                   return_tensors='pt')  # PyTorchのテンソルを返します。

# トークナイズされたデータをデータフレームに変換します。
data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]  # 入力IDをリストとして保存
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in tokens['attention_mask']]  # アテンションマスクをリストとして保存

# データフレームの最初の2行を表示します。
data[:2]  # 最初の2行のデータを表示します。
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 埋め込みを取得する
```

---The following area is a Code cell (cell numver is 15)---
```python
# 埋め込みを取得する関数を定義します。
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    # データフレームから入力IDとアテンションマスクを取得します。
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルとして生成
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # アテンションマスクをテンソルとして生成

    embed_list = []  # 埋め込みリストの初期化

    # バッチサイズに応じてデータを処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを計算
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送
        
        gc.collect()  # ガベージコレクションを実行してメモリを解放
        torch.cuda.empty_cache()  # CUDAメモリをクリアする

        with torch.no_grad():  # 勾配計算を無効にしてメモリを節約
            # モデルに入力して埋め込みを取得します。
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]  # 最後の隠れ層の出力を取得
            embed_mean = torch.mean(embed, dim=1).cpu()  # 平均プーリングを行い、CPUに転送
            embed_list.append(embed_mean)  # 埋め込みリストに追加
            
            torch.cuda.empty_cache()  # CUDAメモリをクリアする
        
    # 埋め込みリストを結合して一つのテンソルを作成します。
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings  # 埋め込みを返します。

# 埋め込みを計算する関数を定義します。
def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)  # 指定されたインデックスに埋め込みを格納します。
```

---The following area is a Code cell (cell numver is 16)---
```python
# 処理開始時刻を記録します。
st = time.time()

N_SAMPLES = len(data)  # データのサンプル数を取得
half = round(N_SAMPLES / 2)  # サンプル数の半分を計算
sub1 = data.iloc[0:half].copy()  # データの最初の半分をコピー
sub2 = data.iloc[half:N_SAMPLES].copy()  # データの後半をコピー

results = {}  # 埋め込み結果を格納する辞書を初期化

# 2つのスレッドを作成し、それぞれのモデルとデバイスを指定します。
t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))  # モデル0のためのスレッド
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))  # モデル1のためのスレッド

# スレッドを開始します。
t0.start()
t1.start()

# スレッドの終了を待ちます。
t0.join()
t1.join()

# 処理が完了したことを表示し、処理にかかった時間を表示します。
print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")  # 処理の合計時間を表示します。
```

---The following area is a Code cell (cell numver is 17)---
```python
# 2つのスレッドから得られた埋め込みを結合します。
test_embeddings = torch.cat([results[0], results[1]], dim=0)

# 最終的な埋め込みの形状を表示します。
test_embeddings.shape  # 埋め込みの形状を表示します。
```

---The following area is a Code cell (cell numver is 18)---
```python
# ガベージコレクションを実行してメモリを解放します。
gc.collect()  

# モデル1を削除してメモリを解放します。
del model_1  
# モデル0を削除してメモリを解放します。
del model_0  

# CUDAメモリをクリアします。
torch.cuda.empty_cache()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
# テスト埋め込みに対する予測確率を取得します。
preds = model_cb.predict_proba(test_embeddings.numpy())  # テスト埋め込みをNumPy配列に変換してモデルに渡します。
preds  # 予測確率を表示します。
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# 提出用ファイルの作成
```

---The following area is a Code cell (cell numver is 22)---
```python
# サンプル提出ファイルをCSVから読み込みます。
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# ターゲット列に予測結果を代入します。
sample_sub[Targets] = preds

# 最終的な提出用データを表示します。
display(sample_sub)  # 提出用データの表示。
```

---The following area is a Code cell (cell numver is 23)---
```python
# 提出用ファイルをCSV形式で保存します。
sample_sub.to_csv('submission.csv', index=False)  # インデックスなしで'submission.csv'として保存します。
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# まとめ 

これで終わりです！アイデアを共有したかっただけです！分類器の調整や他の分類器の使用を試してみてください。ありがとうございました！

もし何かを学んだなら、ぜひいいねを押してください:)
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
---

# コメント 

> ## superferg
> 
> Gemma2を直接分類用に訓練する予定はありますか？私はローカルのバリデーションセットで良いスコアを得たのですが、公開リーダーボードで対応するスコアを達成できていません。Llama3の推論コードが直接Gemma2に変更されてしまっているのではないかと疑っています。
> 
> [詳細](https://www.kaggle.com/competitions/lmsys-chatbot-arena/discussion/518408)
> 
> 
> > ## Valentin Werner
> > 
> > 推論ノートブックで同じ処理をしていることを確認してください。トークナイザー、モデルクラス、入力処理はすべて同一であるべきです。私の場合、提出ノートブック内でCVをテストして、同様のスコアが得られているかを見るのが役に立ちます。提出時に異なる量子化を行うと、トレーニング時とは異なる結果が出ることがあるかもしれませんが（ただし0.95から1.0のような乖離ではないにしても）。
> > 
> > 

---
```

** @@@ Jupyter Notebook numver 12, the number of votes :19 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおけるXGBoostを使用したベースラインモデルの構築に関連しています。具体的には、ユーザーが好む応答を予測するためのモデルをデータを用いてトレーニングし、その精度を評価することを目的としています。

### 取り組んでいる問題
Notebookでは、異なる大規模言語モデル（LLM）が生成した応答の中から、ユーザーがどの応答を好むかを予測するという課題に対処しています。これは、データセットに含まれる「prompt」、「response_a」、「response_b」に基づいて、どちらの応答が優れているかを識別するためのものであり、最終的にはその選好をモデル学習することに繋がります。

### 使用される手法とライブラリ
- **ライブラリ**: 
  - `numpy`, `pandas`: データ操作のため。
  - `nltk`: 自然言語処理とテキストトークン化。
  - `matplotlib`, `seaborn`: データビジュアライゼーション。
  - `xgboost`: 機械学習モデルの構築用。
  - `sklearn`: 特徴量の抽出やモデル評価に利用。

- **モデルの構築**:
  - データを読み込み、前処理を行った後、特徴量エンジニアリングを実施。コサイン類似度やJaccard類似度を計算し、n-gramの重複数などの特徴を生成します。
  - `XGBoost`を使用して多クラスの分類モデルを構築し、ストラティファイドKフォールド交差検証を用いてモデルの性能を評価します。
  - 評価基準として対数損失（log loss）を使用し、モデルの精度を測定している点が特徴的です。

### 結果の提出
最終的には、テストデータに基づいた予測結果を生成し、指定されたフォーマットで「submission.csv」として保存します。このプロセスにより、モデルがどの応答を「勝者」として予測するかを示すことができます。

このNotebookは、異なる言語モデル間でのユーザーの選好を理解し、予測できるモデルを構築するための体系的なアプローチを採用しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS | XGB ベースライン

# 1. ライブラリ
```

---The following area is a Code cell (cell numver is 2)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# 2. 設定
```

---The following area is a Code cell (cell numver is 4)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")  # 訓練データのファイルパス
    test_path = os.path.join(root, "test.csv")    # テストデータのファイルパス
    sample_submission_path = os.path.join(root, "sample_submission.csv")  # サンプル提出ファイルのパス
    seed = 42  # 乱数シード
    n_splits = 10  # クロスバリデーションの分割数
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# 3. データの読み込み
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv(config.train_path)  # 訓練データを読み込む
test = pd.read_csv(config.test_path)    # テストデータを読み込む
sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込む

if test.shape[0] < 10:  # テストデータの行数が10未満なら
    train = train.iloc[:10000]  # 訓練データの最初の10000行を使用する
    
def process(input_str):
    stripped_str = input_str.strip('[]')  # 角括弧を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文字列を分割し、前後のダブルクォートを削除
    return  ' '.join(sentences)  # スペースで結合して返す

# "prompt", "response_a", "response_b"のデータを加工
train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

print(f"train shape: {train.shape}")  # 訓練データの形状を表示
print(f"test shape: {test.shape}")    # テストデータの形状を表示
print("-" * 90)  # 区切り線を表示
print(f"train missing values: {train.isnull().sum().sum()}")  # 訓練データの欠損値の合計を表示
print(f"test missing values: {test.isnull().sum().sum()}")    # テストデータの欠損値の合計を表示
print("-" * 90)

train.head()  # 訓練データの先頭5行を表示
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 4. 特徴量エンジニアリング
```

---The following area is a Code cell (cell numver is 8)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # TF-IDFベクトライザーを定義 (1-3のn-gram)
            vectorizer.fit([text1, text2])  # テキストを基にベクトライザーをフィッティング
            output = vectorizer.transform([text1, text2]).toarray()  # テキストをベクトル化
            cos_sim = cosine_similarity(output)  # コサイン類似度を計算
            return cos_sim[0][1]  # テキスト1とテキスト2のコサイン類似度を返す
        except:
            return np.nan  # エラーが発生した場合はNaNを返す

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())  # テキスト1を単語の集合に変換
        set2 = set(text2.split())  # テキスト2を単語の集合に変換
        intersection = set1.intersection(set2)  # 共通する単語を取得
        union = set1.union(set2)  # 単語の和集合を取得
        return len(intersection) / len(union)  # Jaccard類似度を計算して返す
    
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n')  # テキスト内の改行の数をカウントして返す 

    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"  # 単一引用符のパターン
        double_quote_pattern = r'"(.*?)"'   # 二重引用符のパターン
        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用符の全てを取得
        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用符の全てを取得
        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用符の数を計算
        return len(single_quotes) + len(double_quotes)  # 合計数を返す

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())  # テキストを小文字にしてトークン化 

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)  # トークン化されたテキストを取得
        return list(ngrams(tokens, n))  # n-gramを生成して返す

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # テキスト1のn-gramを生成
            ngrams2 = self.generate_ngrams(text2, n)  # テキスト2のn-gramを生成
            counter1 = Counter(ngrams1)  # テキスト1のn-gramのカウントを作成
            counter2 = Counter(ngrams2)  # テキスト2のn-gramのカウントを作成
            overlap = counter1 & counter2  # 重複するn-gramを取得
            overlap_count = sum(overlap.values())  # 重複の合計数を計算
            return overlap_count  # 重複数を返す
        except:
            return 0  # エラーが発生した場合は0を返す
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        # "response_a"と"response_b"のn-gram重複数を計算し、新しいカラムに追加
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))  # "response_a"のトークン数をカウント
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))  # "response_b"のトークン数をカウント
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))  # "prompt"のトークン数をカウント
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))  # "response_a"の改行数をカウント
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))  # "response_b"の改行数をカウント
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))  # "prompt"の改行数をカウント
        
        # 各長さの比率や差分を計算し新しいカラムに格納
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]  # "response_a"と"prompt"の長さ比
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]  # "response_b"と"prompt"の長さ比
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]  # "response_a"と"response_b"の長さ比
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]  # "response_a"と"response_b"の長さの差
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]  # "response_a"と"prompt"の長さの差
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]  # "response_b"と"prompt"の長さの差
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]  # unigramの比率
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]  # bigramの比率
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]  # trigramの比率

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]  # unigramの比率
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]  # bigramの比率
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]  # trigramの比率
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]  # unigramの差
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]  # bigramの差
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]  # trigramの差
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"]  # unigramの比率
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"]  # bigramの比率
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"]  # trigramの比率
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))  # "response_a"の引用符の数
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))  # "response_b"の引用符の数
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))  # "prompt"の引用符の数
        
        # コサイン類似度とJaccard類似度を計算
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]  # Jaccard類似度の差
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]  # Jaccard類似度の比率
        
        return data
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time
preprocessor = Preprocessor()  # Preprocessorのインスタンスを作成
train = preprocessor.run(train)  # 訓練データに前処理を適用
test = preprocessor.run(test)  # テストデータに前処理を適用
train.head()  # 訓練データの先頭5行を表示
```

---The following area is a Code cell (cell numver is 10)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]  # 削除するカラムのリスト
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]  # ターゲットカラムのリスト
target = "target"  # ターゲット名

train[target] = np.nan  # ターゲットカラムを初期化
for idx, t in enumerate(target_cols):  # ターゲットカラムを走査
    train.loc[train[t] == 1, target] = idx  # 勝者モデルをターゲットカラムに設定
train[target] = train[target].astype("int32")  # ターゲットカラムのデータ型をint32に変換
    
train.head()  # 訓練データの先頭5行を表示
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# 5. モデリング
```

---The following area is a Code cell (cell numver is 12)---
```python
X = train.drop(columns=target_cols + drop_cols + [target] + ["model_a", "model_b"], axis=1)  # 特徴量マトリックスを定義
y = train[target]  # ターゲットを定義
X_test = test.drop(columns=drop_cols, axis=1)  # テストデータから不要なカラムを削除

X = X.replace([-np.inf, np.inf], np.nan)  # 無限大をNaNに置き換え
X_test = X_test.replace([-np.inf, np.inf], np.nan)  # 無限大をNaNに置き換え
```

---The following area is a Code cell (cell numver is 13)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # ストラティファイドKフォールド交差検証の設定
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テストデータの予測結果を格納する配列
cv_scores = list()  # クロスバリデーションのスコアを格納するリスト

features = X.columns.tolist()  # 特徴量のリストを作成
feat_imp_df = pd.DataFrame({"feature": features})  # 特徴量の重要度を格納するデータフレームを作成

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):  # 各Foldについて
    print(f"| Fold {idx+1} |".center(90, "="))  # 現在のFold番号を表示
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # 訓練データを分割
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # 検証データを分割

    print(f'train: {X_train.shape}')  # 訓練データの形状を表示
    print(f'val: {X_val.shape}')  # 検証データの形状を表示
    
    model = xgb.XGBClassifier(  # XGBoostの分類器モデルを定義
        objective='multi:softprob',  # 多クラス分類のための設定
        num_class=3,  # クラスの数
        eval_metric='mlogloss',  # 評価指標に対数損失を使用
        subsample=0.8,  # サンプリング比率
        n_estimators=650,  # 弱学習器の数
        learning_rate=0.045,  # 学習率
        max_depth=5,  # 木の深さ
        random_state=config.seed  # 乱数シード
    )
    
    model.fit(  # モデルを訓練
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],  # 訓練と検証データセット
        early_stopping_rounds=75,  # 早期停止の設定
        verbose=75  # 訓練過程を75ステップごとに表示
    )
    
    val_preds = model.predict_proba(X_val)  # 検証データに対する予測確率を計算
    val_log_loss = log_loss(y_val, val_preds, eps="auto")  # 検証データの対数損失を計算
    print(f"val log loss: {val_log_loss:.5f}")  # 検証データの対数損失を表示
    cv_scores.append(val_log_loss)  # クロスバリデーションのスコアを追加
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # テストデータの予測を加算（平均化）

    feat_imp_df = feat_imp_df.merge(  # 特徴量の重要度を追加
        pd.DataFrame(
            {
                "feature": features,
                f"fold_{idx+1}_feat_imp": model.feature_importances_,  # 各Foldの特徴量重要度を取得
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)  # 区切り線を表示
print(f"CV: {np.mean(cv_scores):.5f}")  # クロスバリデーションスコアの平均を表示

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴の平均重要度を計算
plt.figure(figsize=(12, 10))  # グラフのサイズを設定
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[:50],  # 平均重要度でソートし上位50件を選択
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("全Foldの特徴量の平均重要度", size=12)  # グラフのタイトル
plt.show()  # グラフを表示
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 6. 提出の保存
```

---The following area is a Code cell (cell numver is 15)---
```python
for idx, t in enumerate(target_cols):  # 各ターゲットカラムについて
    sample_submission[t] = test_preds[:, idx]  # テストデータの予測結果をサンプル提出データに格納
sample_submission.head()  # サンプル提出データの先頭5行を表示
```

---The following area is a Code cell (cell numver is 16)---
```python
sample_submission.to_csv("submission.csv", index=False)  # 提出ファイルをCSV形式で保存（インデックスなし）
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
---

# コメント

> ## Ilya Turaev
>
> こんにちは [@sercanyesiloz](https://www.kaggle.com/sercanyesiloz) ! 一つのXGBで素晴らしい結果です。コサイン類似度の関数がうまく動作していないようです。
>
> ```
> vectors = vectorizer.toarray()
> ```
>
> これを修正すれば、スコアが改善すると思いますか？それとも、他の特徴量だけで十分ですか？
>
> > ## Sercan Yeşilöz トピック作成者
> > 
> > こんにちは！そのコードでは、vectorizerは実際には訓練セットとテストセットでフィットした出力です。間違いはないと思います。
> >
> > > ## Ilya Turaev
> > > 
> > > 間違いなくそうすべきです、もしあなたがそれを次のように上書きする場合：
> > > 
> > > ```
> > > vectorizer = vectorizer.fit_transform([text1, text2])
> > > ```
> > > 
> > > そうでなければ、vectorizerはまだTfIdfオブジェクトであり、toarray()メソッドを適用しています。私は間違っていますか？
> > > 
> > > 
> > > ## Sercan Yeşilöz トピック作成者
> > > 
> > > 私は、その行の後でvectorizerがtfidfオブジェクトになるとは思いません、なぜならfit_transform関数を呼び出すと、変換された値が返されるからです。
> > > 
> > > 

---
```

** @@@ Jupyter Notebook numver 13, the number of votes :19 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、「LMSYS - Chatbot Arena」コンペティションに参加するためのモデルを構築しています。具体的には、大規模言語モデル（LLM）を用いて、異なるチャットボットの応答を比較し、どちらの応答が好まれるかを予測するタスクに取り組んでいます。

### 問題に取り組んでいる内容
Notebookは、人間によるチャットボットの応答の選択に基づいて、どのモデルがより好ましいかを予測するために設計されています。トレーニングデータには、ユーザーが選んだ応答が含まれています。そのため、各応答が勝つ確率を計算し、それに基づいて予測を行います。

### 使用している手法やライブラリ
1. **ライブラリ**
   - `transformers`: Hugging Faceのライブラリを使用し、DeBERTa V3モデルを含む事前学習済みモデルを利用してシーケンス分類タスクを処理。
   - `datasets`: データを簡単に処理するためのライブラリ。
   - `sklearn`: モデル評価とデータ処理に利用。
   - `torch`: PyTorchを使用してニューラルネットワークを構築し、訓練します。

2. **主要な手法**
   - **データ前処理**: トークナイゼーションを行い、モデルが理解できる形式にデータを変換します。
   - **モデル構築**: DeBERTa V3モデルを使用して、分類タスクとして設定。
   - **トレーニング**: Stratified K-Fold交差検証を用いて、モデルの訓練と検証を行い、最良モデルを選択します。
   - **評価指標**: ログ損失(log loss)と精度(accuracy)を用いてモデルのパフォーマンスを評価します。

3. **訓練および推論設定**
   - 訓練と評価のためのハイパーパラメータ（学習率やバッチサイズなど）を設定し、混合精度訓練を行います。
   - 実際のデータでトレーニングを行った後、テストデータを使ってモデルが生成した予測を取得し、結果をCSVファイルとして保存します。

このNotebookは、データの読み込みからモデルの訓練、推論、結果の保存までの全過程をカバーしており、コンペティション参加者が迅速にモデルを構築し、提出できるようになっています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import os
import logging
import numpy as np
import pandas as pd
from sklearn.model_selection import StratifiedKFold
from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig
from transformers import TrainingArguments, Trainer, DataCollatorWithPadding
from datasets import Dataset
from sklearn.metrics import log_loss
import torch
from functools import partial
import warnings
from transformers import logging as transformers_logging
from transformers import EarlyStoppingCallback
import json
from pprint import pformat
from tqdm import trange
warnings.simplefilter('ignore')

TYPE = "large"
VER= 14
DATE = "0717"
os.environ["CUDA_VISIBLE_DEVICES"]="0,1"

# ロギングの設定
transformers_logging.set_verbosity_error()
logging.basicConfig(level=logging.INFO, filename=f'logs_v{VER}.log', filemode='a',
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class PATHS:
    train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのパス
    test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'    # テストデータのパス
    sub_path = '/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'  # 提出サンプルのパス
    model_name = f"deberta-v3-{TYPE}"  # モデル名
    model_path = f"/root/autodl-tmp/ase2/huggingfacedebertav3variants/{model_name}"  # モデルの保存パス
    tokenizer_path = f"/kaggle/input/lmsys-{TYPE}{VER}-{DATE}/fold_0/tokenizer"  # トークナイザーのパス
    general_tokenizer = "/kaggle/input/lmsys-base4-0704/fold_0/tokenizer"  # 一般的なトークナイザーのパス

class CFG:
    seed = 42  # 乱数シード
    max_length = 512  # 最大入力長
    lr = 5e-5  # 学習率
    weight_decay = 0.01  # 重み減衰
    warmup_ratio = 0 # 学習率ウォームアップ比率
    max_grad_norm = 1000  # 最大勾配ノルム
    lr_scheduler_type = 'linear'  # 学習率スケジュールタイプ
    frozen_embedding = False # 埋め込み層を凍結するかどうか
    frozen_num = 6  # 凍結する層の数
    train_batch_size = 32  # 訓練バッチサイズ
    eval_batch_size = 64  # 評価バッチサイズ
    evaluation_strategy = 'steps'  # 評価戦略をステップに設定
    metric_for_best_model = "eval_log_loss"  # 最良モデル選択のためのメトリック
    save_strategy = 'steps'  # ステップごとに保存
    save_steps = 200  # 200ステップごとにモデルを保存
    save_total_limit = 1  # 保存されるチェックポイントの合計数制限
    train_epochs = 5  # 訓練エポック数
    num_labels = 6  # ラベルの数
    output_dir = f'/kaggle/input/lmsys-{TYPE}{VER}-{DATE}'  # 出力ディレクトリ
    fp16 = True  # 混合精度訓練を使用
    load_best_model_at_end = True  # 訓練終了時に最良モデルをロード
    report_to = 'none'  # 外部ツールに訓練ログを報告しない
    optim = 'adamw_torch'  # オプティマイザタイプ
    logging_first_step = True  # 最初のステップのログを記録する
    logging_steps = 200  # 200ステップごとにログを記録
    logging_dir =f'logs_v{VER}'  # ログ保存ディレクトリ
    n_splits = 5  # クロスバリデーションの分割数
    model_name = PATHS.model_name  # モデル名
    greater_is_better = False  # メトリックが大きいほど良いかどうか
    early_stop = False  # 早期停止を使用するか
    early_stopping_patience = 3  # 改善なしでモデルの訓練を停止する評価の呼び出し回数
    early_stopping_threshold = 0.001  # 改善と見なすための最小変化量

def seed_everything(seed):
    import random
    import os
    import numpy as np
    import torch
    
    random.seed(seed)  # Pythonの乱数シードを設定
    os.environ['PYTHONHASHSEED'] = str(seed)  # ハッシュシードを設定
    np.random.seed(seed)  # NumPyの乱数シードを設定
    torch.manual_seed(seed)  # PyTorchの乱数シードを設定
    torch.cuda.manual_seed(seed)  # CUDAの乱数シードを設定
    torch.backends.cudnn.deterministic = True  # 再現性を持たせる

seed_everything(seed=CFG.seed)  # 乱数シードの設定

# tokenizer = AutoTokenizer.from_pretrained(PATHS.tokenizer_path)
tokenizer = AutoTokenizer.from_pretrained(PATHS.general_tokenizer)  # トークナイザーの初期化
sep_token = tokenizer.sep_token_id  # セパレータトークンのIDを取得

def log_parameters(logger):
    """PATHSとCFGクラスからすべてのパラメータをログに記録します。"""
    logger.info("=== パラメータ設定 ===")
    
    logger.info("PATHS:")
    for key, value in PATHS.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")  # 各パラメータをログに記録
    
    logger.info("CFG:")
    for key, value in CFG.__dict__.items():
        if not key.startswith('__'):
            logger.info(f"  {key}: {value}")  # 各パラメータをログに記録
    
    logger.info("=*100")

def tokenize_function(row, tokenizer):
    max_len = CFG.max_length - 2  # セパレータトークンのために2を引く
    tokens_prompt = tokenizer(row['prompt'], truncation=True, max_length=max_len//4, add_special_tokens=False)['input_ids']  # プロンプトをトークン化
    remaining_length = max_len - len(tokens_prompt)  # 残りの長さを計算
    
    tokens_response_a = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2, add_special_tokens=False)['input_ids']  # 応答Aをトークン化
    remaining_length -= len(tokens_response_a)  # 残りの長さを更新
    tokens_response_b = tokenizer(row['response_b'], truncation=True, max_length=remaining_length, add_special_tokens=False)['input_ids']  # 応答Bをトークン化
    
    input_ids = [tokenizer.cls_token_id] + tokens_prompt + [sep_token] + tokens_response_a + [sep_token] + tokens_response_b  # 入力IDを作成
    token_type_ids = [0] * (len(tokens_prompt) + 2) + [1] * (len(tokens_response_a) + 1) + [2] * len(tokens_response_b)  # トークンタイプIDsを作成
    attention_mask = [1] * len(input_ids)  # アテンションマスクを作成
    
    padding_length = CFG.max_length - len(input_ids)  # パディングの長さを計算
    if padding_length > 0:
        input_ids = input_ids + [tokenizer.pad_token_id] * padding_length  # パディングを追加
        token_type_ids = token_type_ids + [0] * padding_length  # パディングを追加
        attention_mask = attention_mask + [0] * padding_length  # パディングを追加
    
    return {
        'input_ids': input_ids[:CFG.max_length],  # 最大長に制限する
        'token_type_ids': token_type_ids[:CFG.max_length],  # 最大長に制限する
        'attention_mask': attention_mask[:CFG.max_length],  # 最大長に制限する
    }

def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)  # ラベルの配列を作成
    labels[df['winner_model_a'] == 1] = 0  # モデルAが勝った場合のラベル
    labels[df['winner_model_b'] == 1] = 1  # モデルBが勝った場合のラベル
    labels[df['winner_tie'] == 1] = 2  # 引き分けの場合のラベル
    df['labels'] = labels  # データフレームにラベルを追加
    return df

def process_data(df, mode='train'):
    dataset = Dataset.from_pandas(df)  # データフレームからデータセットを作成
    tokenized_dataset = dataset.map(partial(tokenize_function, tokenizer=tokenizer), batched=False)  # データセットをトークン化
    remove_cols = ['id', 'prompt', 'response_a', 'response_b']  # 削除する列を指定
    if mode == 'train':
        remove_cols += ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']  # トレーニングモードの場合、さらに列を削除
    tokenized_dataset = tokenized_dataset.remove_columns(remove_cols)  # 指定した列を削除
    return tokenized_dataset

def split_train_val(dataset, train_fraction):
    np.random.seed(0)  # 乱数シードを設定
    ixs = np.arange(len(dataset))  # データセットのインデックスを生成
    cutoff = int(len(ixs) * train_fraction)  # 訓練データのカットオフポイント
    np.random.shuffle(ixs)  # インデックスをシャッフル
    ixs_train = ixs[:cutoff]  # 訓練用インデックス
    ixs_val = ixs[cutoff:]  # 検証用インデックス
    fit_train = dataset.select(ixs_train)  # 訓練データを選択
    fit_val = dataset.select(ixs_val)  # 検証データを選択
    return fit_train, fit_val


def compute_metrics(eval_pred):
    logits, labels = eval_pred  # 評価予測からロジットとラベルを取得
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)  # 確率を計算
    return {
        'eval_log_loss': log_loss(labels, probabilities),  # ログ損失を計算
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()  # 精度を計算
    }
    
def train_model():
    log_parameters(logger)  # パラメータをログに記録
    train_df = pd.read_csv(PATHS.train_path)  # トレーニングデータを読み込み
    train_df = add_label(train_df)  # ラベルを追加
    train_tokenized = process_data(train_df, mode='train')  # トークン化されたデータを取得
    
    skf = StratifiedKFold(n_splits=CFG.n_splits, shuffle=True, random_state=CFG.seed)  # ストラティファイドKフォールドを作成
    
    for fold, (train_idx, val_idx) in enumerate(skf.split(train_tokenized, train_tokenized['labels'])):  # 各フォールドでトレーニングと検証インデックスを分割
        print(f"フォールド {fold + 1} のトレーニング")  # フォールド番号を出力
        logger.info(f"フォールド {fold + 1} のトレーニング")  # フォールド番号をログに記録
        
        fit_train = train_tokenized.select(train_idx)  # 訓練データを選択
        fit_val = train_tokenized.select(val_idx)  # 検証データを選択
        
        model = AutoModelForSequenceClassification.from_pretrained(  # 事前学習済みモデルを読み込み
            PATHS.model_path,
            num_labels=3,  # ラベル数を指定
            problem_type="single_label_classification"  # 問題タイプを指定
        )
        
        training_args = TrainingArguments(  # 訓練引数を設定
            output_dir=f"{CFG.output_dir}/fold_{fold}",  # モデルとチェックポイントの出力ディレクトリ
            fp16=CFG.fp16,  # 混合精度訓練を使用
            learning_rate=CFG.lr,  # 学習率
            per_device_train_batch_size=CFG.train_batch_size,  # 各デバイスでの訓練バッチサイズ
            per_device_eval_batch_size=CFG.eval_batch_size,  # 各デバイスでの評価バッチサイズ
            num_train_epochs=CFG.train_epochs,  # トレーニングのエポック数
            weight_decay=CFG.weight_decay,  # 重み減衰
            evaluation_strategy=CFG.evaluation_strategy,  # 評価戦略
            metric_for_best_model=CFG.metric_for_best_model,  # 最良モデルを選択するためのメトリック
            save_strategy=CFG.save_strategy,  # 保存戦略
            save_total_limit=CFG.save_total_limit,  # チェックポイントの総保存数制限
            load_best_model_at_end=CFG.load_best_model_at_end,  # 最良モデルを訓練終了時にロード
            report_to=CFG.report_to,  # ログを外部ツールに報告しない
            warmup_ratio=CFG.warmup_ratio,  # 学習率のウォームアップ比率
            lr_scheduler_type=CFG.lr_scheduler_type,  # 学習率スケジューラーのタイプ
            optim=CFG.optim,  # 使用するオプティマイザのタイプ
            logging_first_step=CFG.logging_first_step,  # 最初のステップのログを記録
            greater_is_better=CFG.greater_is_better,  # メトリックが大きいほど良いかどうか
            
            # max_grad_norm=CFG.max_grad_norm,  # 勾配クリッピングの設定
            
            logging_steps=CFG.logging_steps,  # 200ステップごとにログを記録
            logging_dir =f'logs_v{VER}',  # ログ保存ディレクトリ
        
            save_steps=CFG.save_steps,  # 200ステップごとにモデルを保存
            eval_steps=CFG.save_steps,  # eval_stepsパラメータを追加、save_stepsと一致させる
        )

         # 訓練引数をログに記録
        logger.info("訓練引数:")
        logger.info(pformat(training_args.to_dict()))

        if CFG.frozen_embedding:  # 埋め込み層を凍結する設定の場合
            n = CFG.frozen_num
            # 埋め込み層を凍結
            for i, layer in enumerate(model.deberta.encoder.layer[:n]):
                for param in layer.parameters():
                    param.requires_grad = False  # 訓練から除外
            for param in model.deberta.embeddings.parameters():
                param.requires_grad = False  # 訓練から除外

        # トークナイザーの初期化
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # データコラトレーターの初期化

        # EarlyStoppingCallbackの作成
        if CFG.early_stop:  # 早期停止を使用する設定の場合
            early_stopping_callback = EarlyStoppingCallback(
                early_stopping_patience=CFG.early_stopping_patience,  # 訓練停止の耐久期間
                early_stopping_threshold=CFG.early_stopping_threshold,  # 改善と見なす最小変化
            )
        
            trainer = Trainer(  # トレーナーの初期化
                model=model,
                args=training_args,
                train_dataset=fit_train,  # 訓練データセット
                data_collator=data_collator,  # データコラトレーター
                eval_dataset=fit_val,  # 検証データセット
                compute_metrics=compute_metrics,  # メトリックの計算関数
                callbacks=[early_stopping_callback],  # 早期停止コールバックを追加
            )
        else:
            trainer = Trainer(  # トレーナーの初期化
                model=model,
                args=training_args,
                train_dataset=fit_train,  # 訓練データセット
                data_collator=data_collator,  # データコラトレーター
                eval_dataset=fit_val,  # 検証データセット
                compute_metrics=compute_metrics,  # メトリックの計算関数
            )
        
        trainer.train()  # モデルの訓練を開始
        
        # モデルを保存
        trainer.save_model(f"{CFG.output_dir}/fold_{fold}/best_model")  # 最良モデルを保存
        tokenizer.save_pretrained(f"{CFG.output_dir}/fold_{fold}/tokenizer")  # トークナイザーを保存
        
        # 結果をログに記録
        eval_result = trainer.evaluate()  # 評価を実施
        logger.info(f"フォールド {fold + 1} - 評価結果: {eval_result}")  # 評価結果をログに記録
        logger.info("=*100")

def predict_test():
    test_df = pd.read_csv(PATHS.test_path)  # テストデータを読み込み
    test_tokenized = process_data(test_df, mode='test')  # トークン化されたテストデータを取得
    
    predictions = []  # 予測結果を格納するリスト
    
    for fold in trange(CFG.n_splits):  # 各フォールドで予測を行う
        model = AutoModelForSequenceClassification.from_pretrained(f"{CFG.output_dir}/fold_{fold}/best_model")  # 最良モデルを読み込み
        model.eval()  # 評価モードに設定
        
        trainer = Trainer(model=model)  # トレーナーの初期化
        fold_preds = trainer.predict(test_tokenized).predictions  # テストデータに対する予測を取得
        fold_preds = np.exp(fold_preds) / np.sum(np.exp(fold_preds), axis=1, keepdims=True)  # 確率に変換
        predictions.append(fold_preds)  # フォールドの予測結果を追加
    
    # フォールド間の予測を平均化
    final_preds = np.mean(predictions, axis=0)  # 最終予測を計算
    display(predictions)  # 予測結果を表示
    logger.info(f"最終予測: {final_preds}")  # 最終予測をログに記録
    
    # 提出ファイルの作成
    submission = pd.DataFrame({
        'id': test_df['id'],  # テストデータのID
        'winner_model_a': final_preds[:, 0],  # モデルAの勝者確率
        'winner_model_b': final_preds[:, 1],  # モデルBの勝者確率
        'winner_tie': final_preds[:, 2]  # 引き分け確率
    })
    
    submission.to_csv('submission.csv', index=False)  # 提出ファイルをCSV形式で保存
    display(submission)  # 提出結果を表示
```

---The following area is a Code cell (cell numver is 2)---
```python
%time  # コードの実行時間を計測します
if __name__ == "__main__":  # このスクリプトがメインプログラムとして実行されている場合
#     train_model()  # モデルを訓練する関数を呼び出す（コメントアウト中）
    predict_test()  # テストデータに対する予測を行う関数を呼び出す
```

---The following area is a Code cell (cell numver is 3)---
```python
# トークナイザーライブラリをインポートします
# print(tokenizers.__version__)  # トークナイザーライブラリのバージョンを出力します
```

---The following area is a Code cell (cell numver is 4)---
```python

```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
---

# コメント

> ## Rise_Hand
> 
> いつも素晴らしい仕事だね、兄弟。
> 
> 

---

> ## Yuxi Xue
> 
> 素晴らしい仕事！！！！！
> 
> 

---

> ## Korey Ma
> 
> いい仕事だね！！！🥳
> 
> 

---

> ## YEI0907
> 
> 兄弟、推論にどのくらい時間がかかりましたか？
> 
> 
> > ## Roschild.Rui（トピック作成者）
> > 
> > だいたい2〜3時間くらいだと思います。
> > 
> > 
> 

---
```

** @@@ Jupyter Notebook numver 14, the number of votes :16 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」コンペティションで使用されるモデルを用いた人間の好み予測に取り組んでいます。具体的には、Gemma2とLlama3の大規模言語モデルを用いて、ユーザーからのプロンプトに対する両モデルの応答の勝率を予測するタスクに焦点を当てています。

### 主な問題
- **目的**: 2つの異なるチャットボット（GemmaとLlama）が生成した応答のどちらがユーザーに好まれるかを予測し、その勝率を計算すること。
- **評価**: モデルの出力冒険の確率を算出し、最終的に提出用データフレームを生成します。

### 使用されている手法とライブラリ
- **ライブラリ**: 
  - `transformers`: モデルとトークナイザーの操作に使用。
  - `peft`: LoRA（Low-Rank Adaptation）技術を使用してモデルの性能を向上させる。
  - `torch`: モデルの実行に必要なPyTorchライブラリ。
  - `sklearn`, `numpy`, `pandas`: データ処理と機械学習に関連するライブラリ。

- **手法**: 
  - GemmaとLlamaモデルをそれぞれ異なるGPUに配置し、並列処理を行っています。
  - テストデータはCSVファイルから読み込み、プロンプトと応答をトークン化し、モデルに供給するための処理を実施。
  - 推論では、両モデルが生成した応答に基づいて勝つ確率を算出し、結果を統合します。
  - 最後に、確率をCSVファイルとして出力し、Kaggleコンペティションに提出する準備を整えます。

このノートブックのアプローチは、ゲームの中での応答を比較し、どのモデルがユーザーの好みにより合致しているかを判定するための強力なテクニックを示しています。結果を提出するために最終的に生成されるCSVファイルは、ユーザーの選好をモデル化する精度に基づいて評価されます。
```

---The following area is a Code cell (cell numver is 1)---
```python
"""
クレジット:

https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora

リーダーボードスコア: 0.945
"""
```

---The following area is a Code cell (cell numver is 2)---
```python
# 必要なライブラリをインストールします。
# transformers, peft, accelerate, bitsandbytesの最新バージョンを指定してインストールします。
# --no-indexオプションを使用することで、PyPI（Python Package Index）を使用せずに、
# 指定したリンクからのみパッケージを取得することを意味します。

!pip install transformers peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします。
import time  # タイマー処理用
from dataclasses import dataclass  # データクラスの作成用
from concurrent.futures import ThreadPoolExecutor  # スレッドプールによる非同期処理用

import torch  # PyTorchライブラリ
import sklearn  # 機械学習ライブラリ
import numpy as np  # 数値計算ライブラリ
import pandas as pd  # データ操作ライブラリ
from transformers import (
    Gemma2ForSequenceClassification,  # Gemma2モデルのシーケンス分類用クラス
    GemmaTokenizerFast,  # Gemmaモデル用のトークナイザー
    AutoTokenizer,  # 自動トークナイザー
    LlamaForSequenceClassification,  # Llamaモデルのシーケンス分類用クラス
    BitsAndBytesConfig  # BitsAndBytesの設定
)
from transformers.data.data_collator import pad_without_fast_tokenizer_warning  # トークナイザーの警告を抑制するためのデータコレータ
from peft import PeftModel, get_peft_model, LoraConfig, TaskType  # PEFTライブラリのモデル・設定用クラス

# メモリ効率の良いSDP（スパースディストリビューションプロセス）をCUDAバックエンドで有効にします。
torch.backends.cuda.enable_mem_efficient_sdp(True)
# フラッシュSDPも有効にします。
torch.backends.cuda.enable_flash_sdp(True)
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    # Gemmaモデルのディレクトリパスを指定します。
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    # GemmaのLoRAチェックポイントのディレクトリパスを指定します。
    gemma_lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    # Llamaモデルの名前（ディレクトリパス）を指定します。
    llama_model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    # Llamaモデルの重みファイルのパスを指定します。
    llama_weights_path = '/kaggle/input/lmsys-model/model'
    # 最大シーケンス長を設定します（ここでは2048トークン）。
    max_length = 2048
    # バッチサイズを設定します（ここでは4）。
    batch_size = 4
    # テスト時のデータ拡張を使用するかどうかを指定します（ここでは使用しない）。
    tta = False
    # 最大シーケンス長の拡張を使用するかどうかを指定します（ここでは使用しない）。
    spread_max_length = False

# 新しいConfigオブジェクトを生成します。
cfg = Config()
```

---The following area is a Code cell (cell numver is 5)---
```python
# テストデータセットをCSVファイルから読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# テキストを処理する関数を定義します。
def process_text(text: str) -> str:
    # 評価関数を用いて、テキスト内の'null'を空の文字列に置き換えます。
    return " ".join(eval(text, {"null": ""}))

# 'prompt'列のテキストを処理します。
test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
# 'response_a'列のテキストを処理します。
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
# 'response_b'列のテキストを処理します。
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)
```

---The following area is a Code cell (cell numver is 6)---
```python
# トークナイザーを使用して入力データをトークン化する関数を定義します。
def tokenize(tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length):
    # トークナイザーの種類によって処理を分けます。
    if isinstance(tokenizer, GemmaTokenizerFast):
        # Gemmaトークナイザーの場合は特定のプレフィックスを追加します。
        prompt = ["<prompt>: " + p for p in prompt]
        response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]
        response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]
    else:
        # その他のトークナイザーの場合は別のプレフィックスを追加します。
        prompt = ["User prompt: " + p for p in prompt]
        response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
        response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    
    # max_lengthをスプレッドするかどうかで処理を分けます。
    if spread_max_length:
        # 各入力の長さを最大値の1/3に設定し、トークン化します。
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        # 入力IDsを結合します。
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        # アテンションマスクを作成します。
        attention_mask = [[1]* len(i) for i in input_ids]
    else:
        # 各入力をトークン化し、max_lengthに基づいて制限します。
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    # 入力IDsとアテンションマスクを返します。
    return input_ids, attention_mask
```

---The following area is a Code cell (cell numver is 7)---
```python
# Gemmaトークナイザーを初期化します。
gemma_tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
# 終了トークンを追加することを指定します。
gemma_tokenizer.add_eos_token = True
# パディングを右側に設定します。
gemma_tokenizer.padding_side = "right"

# Llamaトークナイザーを初期化します。
llama_tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# 両方のモデル用にデータを準備します。

# Gemma用のデータフレームを作成します。
gemma_data = pd.DataFrame()
# テストデータのIDを設定します。
gemma_data["id"] = test["id"]
# トークナイザーを使って入力IDsとアテンションマスクを取得します。
gemma_data["input_ids"], gemma_data["attention_mask"] = tokenize(gemma_tokenizer, test["prompt"], test["response_a"], test["response_b"])
# 各入力の長さを計算します。
gemma_data["length"] = gemma_data["input_ids"].apply(len)

# Llama用のデータフレームを作成します。
llama_data = pd.DataFrame()
# テストデータのIDを設定します。
llama_data["id"] = test["id"]
# トークナイザーを使って入力IDsとアテンションマスクを取得します。
llama_data["input_ids"], llama_data["attention_mask"] = tokenize(llama_tokenizer, test["prompt"], test["response_a"], test["response_b"])
# 各入力の長さを計算します。
llama_data["length"] = llama_data["input_ids"].apply(len)
```

---The following area is a Code cell (cell numver is 8)---
```python
# GemmaモデルをGPU 0に読み込みます。
device_0 = torch.device('cuda:0')
gemma_model = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,  # Gemmaモデルのディレクトリを指定します。
    device_map=device_0,  # モデルを配置するデバイスを指定します。
    use_cache=False  # キャッシュを使用しないように設定します。
)
# LoRAモデルをGemmaモデルに読み込みます。
gemma_model = PeftModel.from_pretrained(gemma_model, cfg.gemma_lora_dir)  # LoRAチェックポイントのディレクトリを指定します。
```

---The following area is a Code cell (cell numver is 9)---
```python
# LlamaモデルをGPU 1に読み込みます。
device_1 = torch.device('cuda:1')
# 8ビット量子化の設定を行います。
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットでモデルをロードします。
    bnb_8bit_compute_dtype=torch.float16,  # 計算時のデータ型をfloat16に設定します。
    bnb_8bit_use_double_quant=False  # ダブル量子化を使用しない設定です。
)
# Llamaの基本モデルを読み込みます。
llama_base_model = LlamaForSequenceClassification.from_pretrained(
    cfg.llama_model_name,  # Llamaモデルの名前（ディレクトリ）を指定します。
    num_labels=3,  # 分類するラベルの数を設定します（ここでは3つのラベル）。
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定します。
    quantization_config=bnb_config,  # 量子化の設定を指定します。
    device_map='cuda:1'  # モデルを配置するデバイスを指定します。
)
# LlamaトークナイザーのパディングトークンIDを設定します。
llama_base_model.config.pad_token_id = llama_tokenizer.pad_token_id

# LoRAの設定を行います。
peft_config = LoraConfig(
    r=16,  # LoRAのランク。
    lora_alpha=32,  # LoRAのスケーリングファクター。
    lora_dropout=0.10,  # LoRAのドロップアウト率。
    bias='none',  # バイアスの設定（ここではなし）。
    inference_mode=True,  # 推論モードを有効にします。
    task_type=TaskType.SEQ_CLS,  # タスクの種類をシーケンス分類に設定します。
    target_modules=['o_proj', 'v_proj']  # ターゲットモジュールの名前を指定します。
)
# LoRAモデルを取得し、GPU 1に移動します。
llama_model = get_peft_model(llama_base_model, peft_config).to(device_1)
# Llamaモデルの重みを指定されたパスから読み込みます。
llama_model.load_state_dict(torch.load(cfg.llama_weights_path), strict=False)
# モデルを評価モードに設定します。
llama_model.eval()
```

---The following area is a Code cell (cell numver is 10)---
```python
@torch.no_grad()  # 勾配の計算をオフにします（推論中は必要ないため）。
@torch.cuda.amp.autocast()  # 自動混合精度を有効にします（計算の効率を上げるため）。
def inference(df, model, tokenizer, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    # 各モデルの勝利確率を保存するリストを初期化します。
    a_win, b_win, tie = [], [], []
    
    # データフレームをバッチ単位で処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算します。
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得します。
        input_ids = tmp["input_ids"].to_list()  # 入力IDsをリストとして取得します。
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストとして取得します。
        
        # トークナイザーを使って入力を整形します。
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},  # 入力IDsとアテンションマスクを指定します。
            padding="longest",  # 最長の入力にパディングします。
            pad_to_multiple_of=None,  # 特に指定しない場合はNoneにします。
            return_tensors="pt",  # PyTorchのテンソル形式で返します。
        )
        
        # モデルを使用して推論を行います。
        outputs = model(**inputs.to(device))  # 入力を指定されたデバイスに移動させてモデルに渡します。
        proba = outputs.logits.softmax(-1).cpu()  # ロジットからソフトマックスを適用して確率を計算します。
        
        # 各モデルの勝率をリストに追加します。
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    # 勝率をデータフレームに追加します。
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df  # 処理したデータフレームを返します。
```

---The following area is a Code cell (cell numver is 11)---
```python
st = time.time()  # 処理開始時刻を記録します。

# 入力の長さでデータをソートします（長い順）。
gemma_data = gemma_data.sort_values("length", ascending=False)
llama_data = llama_data.sort_values("length", ascending=False)

# スレッドプールを使用して並列処理を実行します。
with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, 
                           (gemma_data, llama_data),  # GemmaとLlamaのデータを引数として渡します。
                           (gemma_model, llama_model),  # 各モデルを引数として渡します。
                           (gemma_tokenizer, llama_tokenizer),  # 各トークナイザーを引数として渡します。
                           (device_0, device_1))  # 各モデルが配置されているデバイスを引数として渡します。

gemma_result_df, llama_result_df = list(results)  # 結果をリストに変換して変数に格納します。

# 結果を組み合わせます（簡単な平均を計算します）。
combined_result_df = gemma_result_df.copy()  # Gemmaの結果データフレームをコピーします。
combined_result_df["winner_model_a"] = (gemma_result_df["winner_model_a"] + llama_result_df["winner_model_a"]) / 2  # モデルAの勝率を平均します。
combined_result_df["winner_model_b"] = (gemma_result_df["winner_model_b"] + llama_result_df["winner_model_b"]) / 2  # モデルBの勝率を平均します。
combined_result_df["winner_tie"] = (gemma_result_df["winner_tie"] + llama_result_df["winner_tie"]) / 2  # 引き分け率を平均します。

# 推論にかかった時間を表示します。
print(f"Inference time: {time.time() - st:.2f} seconds")
```

---The following area is a Code cell (cell numver is 12)---
```python
# 提出用のデータフレームを作成します。
# 'id'、'winner_model_a'、'winner_model_b'、'winner_tie'の列を選択します。
submission_df = combined_result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
# データフレームをCSVファイルに書き出します（インデックスは含めません）。
submission_df.to_csv('submission.csv', index=False)
# 提出データの先頭5行を表示します。
display(submission_df.head())
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
---

# コメント

> ## Nikita Glazunov
> 
> こんにちは！良いノートブックです。Llama 3モデルを読み込もうとしたときに、このエラーが出ました >OSError: Incorrect path_or_model_id: '/kaggle/input/llama-3/transformers/8b-chat-hf/1'。ローカルフォルダへのパスまたはHub上のモデルのrepo_idを提供してください。どうすれば修正できますか？コードは一切変更していません。
> 
> 

---

> ## YEI0907
> 
> 推論にどれくらいの時間がかかりましたか？
> 
> 
> > ## G John Rao（トピック作成者）
> > 
> > 約9時間かかります。
> > 
> 
>
```

** @@@ Jupyter Notebook numver 15, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおけるXGBoostを使用したベースラインモデルを構築することを目的としています。このコンペティションでは、異なる大規模言語モデル（LLM）によるチャットボットの応答に対して、どちらがユーザーに好まれるかを予測するタスクに取り組んでいます。

### 主な内容

1. **ライブラリのインポート**:
   - NumPy、Pandas、nltk、Matplotlib、Seaborn、XGBoostなどのライブラリを使用し、データの前処理や特徴抽出、モデルの構築・評価を行います。

2. **データのロード**:
   - チャットボットのデータセットをトレーニングとテストの2つに分けて読み込み、必要に応じてデータを事前処理します。特に、テキスト内の不要な形式を整える関数を適用します。

3. **特徴エンジニアリング**:
   - `Preprocessor`クラスを用いて、コサイン類似度やジャッカード類似度を計算するメソッドを定義します。また、テキストに含まれる引用、改行、箇条書きの数などの特徴量を抽出します。
   - ユニグラム、バイグラム、トライグラムのオーバーラップを特徴量として計算し、データフレームに追加します。

4. **モデリング**:
   - XGBoost分類器を用いて、層化Kフォールド交差検証を行い、モデルのパフォーマンスを評価します。各フォールドのログ損失を計算し、最終的なテスト予測を生成します。
   - 特徴量の重要度を評価し、上位の重要な特徴量を可視化します。

5. **提出ファイルの作成**:
   - 最終的なテスト予測をサンプル提出ファイルに格納し、「submission.csv」として保存します。

### 使用される手法とライブラリ

- **手法**: XGBoostによるマルチクラス分類、層化Kフォールド交差検証、特徴量抽出に基づくテキストの類似度評価。
- **ライブラリ**: NumPy、Pandas、nltk、Matplotlib、Seaborn、XGBoost、Scikit-learn。

このノートブックは、コンペティションにおける参加者が独自のモデルを開発するための出発点として活用できます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS | XGBベースライン

(元のノートブック: https://www.kaggle.com/code/sercanyesiloz/lmsys-xgb-baseline)

# 1. ライブラリ
```

---The following area is a Code cell (cell numver is 2)---
```python
import gc  # ガーベジコレクションをインポートします。メモリを解放するのに役立ちます。
import os  # OS関連の機能を提供するモジュールをインポートします。
import re  # 正規表現操作に使用するモジュールをインポートします。
import numpy as np  # 数値計算のためのライブラリであるNumPyをインポートします。
import pandas as pd  # データ操作と解析のためのPandasをインポートします。

import nltk  # 自然言語処理のためのライブラリであるnltkをインポートします。
from nltk.util import ngrams  # n-gramsの生成に使用します。
from collections import Counter  # 要素のカウントに使用するCounterをインポートします。
import matplotlib.pyplot as plt  # データの可視化のためのMatplotlibをインポートします。
import seaborn as sns  # 高度なデータビジュアライゼーションのためのSeabornをインポートします。

import xgboost as xgb  # XGBoostライブラリをインポートします。機械学習のためのツールです。
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer  # テキストの特徴抽出に必要なベクトライザをインポートします。
from sklearn.metrics.pairwise import cosine_similarity  # コサイン類似度を計算するための関数をインポートします。
from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldを用いた交差検証を行います。
from sklearn.metrics import log_loss  # ログ損失を計算するための関数をインポートします。
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# 2. 設定
```

---The following area is a Code cell (cell numver is 4)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"  # データセットのルートディレクトリを指定します。
    train_path = os.path.join(root, "train.csv")  # トレーニングデータのファイルパスを設定します。
    test_path = os.path.join(root, "test.csv")  # テストデータのファイルパスを設定します。
    sample_submission_path = os.path.join(root, "sample_submission.csv")  # 提出ファイルのサンプルのパスを設定します。
    seed = 42  # 再現性のための乱数シードを設定します。
    n_splits = 10  # クロスバリデーションの分割数を設定します。
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# 3. データのロード
```

---The following area is a Code cell (cell numver is 6)---
```python
# 指定されたパスからトレーニング、テスト、サンプル提出データセットを読み込みます
train = pd.read_csv(config.train_path)  # トレーニングデータを読み込みます
test = pd.read_csv(config.test_path)  # テストデータを読み込みます
sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込みます

# テストデータセットの行数が10未満の場合、トレーニングデータセットを最初の10,000行に制限します
if test.shape[0] < 10:
    train = train.iloc[:10000]  # 最初の10,000行をトレーニングデータとして使用します

# 文字列を処理する関数を定義します。括弧を削除し、文を分割します
# 注意: 別の方法としてJSONに変換して結合することもできますが、これはPythonにおいて効率的だと思われます
def process(input_str):
    stripped_str = input_str.strip('[]')  # 先頭と末尾の角括弧を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # ","で分割し、周囲の引用符を削除
    return ' '.join(sentences)  # 文をスペースで結合して返します

# トレーニングデータセットのプロンプトとレスポンスの列に`process`関数を適用します
train["prompt"] = train["prompt"].apply(process)  # プロンプトの列を処理します
train["response_a"] = train["response_a"].apply(process)  # レスポンスAの列を処理します
train["response_b"] = train["response_b"].apply(process)  # レスポンスBの列を処理します

# テストデータセットのプロンプトとレスポンスの列に`process`関数を適用します
test["prompt"] = test["prompt"].apply(process)  # プロンプトの列を処理します
test["response_a"] = test["response_a"].apply(process)  # レスポンスAの列を処理します
test["response_b"] = test["response_b"].apply(process)  # レスポンスBの列を処理します

# トレーニングデータとテストデータの形状を表示します
print(f"train shape: {train.shape}")  # トレーニングデータの形状を表示します
print(f"test shape: {test.shape}")  # テストデータの形状を表示します
print("-"*90)

# トレーニングデータとテストデータの欠損値の合計を表示します
print(f"train missing values: {train.isnull().sum().sum()}")  # トレーニングデータの欠損値の数を表示します
print(f"test missing values: {test.isnull().sum().sum()}")  # テストデータの欠損値の数を表示します
print("-"*90)

# トレーニングデータの最初の数行を表示します
train.head()  # トレーニングデータの先頭行を表示します
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 4. 特徴エンジニアリング
```

---The following area is a Code cell (cell numver is 8)---
```python
class Preprocessor:

    # 2つのテキスト間のコサイン類似度を計算します
    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # n-gramを1から3まで考慮したTF-IDFベクトライザを作成します
            vectorizer.fit([text1, text2])  # 2つのテキストにベクトライザをフィットさせます
            output = vectorizer.transform([text1, text2]).toarray()  # テキストをTF-IDFベクトルに変換します
            cos_sim = cosine_similarity(output)  # ベクトル間のコサイン類似度を計算します
            return cos_sim[0][1]  # text1とtext2の類似度スコアを返します
        except:
            print(f"cosine_sim exception with '{text1}' and '{text2}'")
            return np.nan  # 例外が発生した場合はNaNを返します

    # 2つのテキスト間のジャッカード類似度を計算します
    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())  # text1を単語の集合に分割します
        set2 = set(text2.split())  # text2を単語の集合に分割します
        intersection = set1.intersection(set2)  # 2つの集合の共通部分を見つけます
        union = set1.union(set2)  # 2つの集合の和を見つけます
        return len(intersection) / len(union)  # ジャッカード類似度スコアを返します
    
    # テキスト内の引用されたセグメントの数をカウントします
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"  # 単一引用符のパターン
        double_quote_pattern = r'"(.*?)"'  # 二重引用符のパターン
        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用符で囲まれた部分をすべて見つけます
        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用符で囲まれた部分をすべて見つけます
        total_quotes = len(single_quotes) + len(double_quotes)  # 両方のカウントを合計します
        return total_quotes  # 引用されたセグメントの合計数を返します
    
    # テキスト内の改行の数をカウントします
    def count_new_lines(self, text: str) -> int:
        return text.count('\\n')  # テキスト内の改行文字の数を返します
    
    # テキスト内の箇条書きリストの数をカウントします
    def count_bulleted_lists(self, text: str) -> int:
        bullet_pattern = r'(\\n|^)[\*\-\+]\s'  # 箇条書き項目のパターン
        return len(re.findall(bullet_pattern, text))  # 箇条書き項目の数を返します
    
    # テキストを小文字の単語にトークン化します
    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())  # テキストをトークン化し、小文字に変換します

    # トークン化されたテキストからn-gramを生成します
    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)  # テキストをトークン化します
        return list(ngrams(tokens, n))  # トークンからn-gramを生成して返します

    # 2つのテキスト間のオーバーラップn-gramの数をカウントします
    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # text1のn-gramを生成します
            ngrams2 = self.generate_ngrams(text2, n)  # text2のn-gramを生成します
            counter1 = Counter(ngrams1)  # text1のn-gramをカウントします
            counter2 = Counter(ngrams2)  # text2のn-gramをカウントします
            overlap = counter1 & counter2  # 2つのカウンタのオーバーラップを見つけます
            overlap_count = sum(overlap.values())  # オーバーラップしたn-gramのカウントを合計します
            return overlap_count  # オーバーラップの数を返します
        except:
            return 0  # 例外が発生した場合は0を返します
        
    # データに対する前処理を実行します
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # response_aとresponse_b間のユニグラム、バイグラム、およびトライグラムのオーバーラップを計算します
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        # response_aとプロンプト間のユニグラム、バイグラム、およびトライグラムのオーバーラップを計算します
        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        # response_bとプロンプト間のユニグラム、バイグラム、およびトライグラムのオーバーラップを計算します
        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        # トークン化されたテキストの長さを計算します
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))  # response_aのトークン数を計算します
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))  # response_bのトークン数を計算します
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))  # プロンプトのトークン数を計算します
        
        # response_a、response_b、プロンプト間の長さ比を計算します
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]  # response_aとプロンプトの長さ比
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]  # response_bとプロンプトの長さ比
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]  # response_aとresponse_bの長さ比
        
        # response_a、response_b、プロンプト間の長さの差を計算します
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]  # response_aとresponse_bの長さの差
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]  # response_aとプロンプトの長さの差
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]  # response_bとプロンプトの長さの差
        
        # response_aとプロンプト間のユニグラム、バイグラム、トライグラムのオーバーラップ比を計算します
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]  # ユニグラムのオーバーラップ比
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]  # バイグラムのオーバーラップ比
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]  # トライグラムのオーバーラップ比

        # response_bとプロンプト間のユニグラム、バイグラム、トライグラムのオーバーラップ比を計算します
        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]  # ユニグラムのオーバーラップ比
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]  # バイグラムのオーバーラップ比
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]  # トライグラムのオーバーラップ比
        
        # response_a、response_b、プロンプト内の引用の数をカウントします
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))  # response_aの引用数
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))  # response_bの引用数
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))  # プロンプトの引用数

        # response_a、response_b、プロンプト内の改行の数をカウントします
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))  # response_aの改行数
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))  # response_bの改行数
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))  # プロンプトの改行数

        # response_a、response_b、プロンプト内の箇条書きリストの数をカウントします
        data["respa_bullets"] = data["response_a"].apply(lambda x: self.count_bulleted_lists(x))  # response_aの箇条書き数
        data["respb_bullets"] = data["response_b"].apply(lambda x: self.count_bulleted_lists(x))  # response_bの箇条書き数
        data["prompt_bullets"] = data["prompt"].apply(lambda x: self.count_bulleted_lists(x))  # プロンプトの箇条書き数
        
        # response_aとresponse_b間のコサインとジャッカードの類似度を計算します
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)  # コサイン類似度
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)  # ジャッカード類似度
        
        # response_aとプロンプト間のコサインとジャッカードの類似度を計算します
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)  # コサイン類似度
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)  # ジャッカード類似度
        
        # response_bとプロンプト間のコサインとジャッカードの類似度を計算します
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)  # コサイン類似度
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)  # ジャッカード類似度
        
        return data  # 処理されたデータフレームを返します
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time  # 処理にかかる時間を計測します

preprocessor = Preprocessor()  # Preprocessorクラスのインスタンスを作成します
train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行します
test = preprocessor.run(test)  # テストデータに対して前処理を実行します
train.head()  # トレーニングデータの先頭行を表示します
```

---The following area is a Code cell (cell numver is 10)---
```python
# データセットから削除する列のリスト
drop_cols = ["id", "response_a", "response_b", "prompt"]  # 不要な列を指定します

# 勝者を示すターゲット列のリスト
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]  # 勝者に関する列を指定します

# 最終的なターゲット列の名前
target = "target"  # ターゲット列の名前を設定します

# ターゲット列をNaNで初期化します
train[target] = np.nan  # ターゲット列を初期化します

# ターゲット列をループして、対応するインデックスをターゲット列に設定します
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx  # ターゲット列の値が1のインデックスを設定します

# ターゲット列を整数型に変換します
train[target] = train[target].astype("int32")  # ターゲット列をint32型に変換します

# 更新されたデータフレームの最初の数行を表示します
train.head()  # データフレームの先頭行を表示します
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# 5. モデリング
```

---The following area is a Code cell (cell numver is 12)---
```python
# トレーニングデータセットから指定した列を削除し、結果をXに割り当てます
X = train.drop(columns=target_cols + drop_cols + [target] + ["model_a", "model_b"], axis=1)  # 不要な列を削除します

# ターゲット列をyに割り当てます
y = train[target]  # ターゲット列をyに設定します

# テストデータセットから指定した列を削除し、結果をX_testに割り当てます
X_test = test.drop(columns=drop_cols, axis=1)  # 不要な列を削除してテストデータを準備します

# トレーニング特徴セットの無限大の値（-infとinf）をNaNに置き換えます
X = X.replace([-np.inf, np.inf], np.nan)  # 無限大の値をNaNに置き換えます

# テスト特徴セットの無限大の値（-infとinf）をNaNに置き換えます
X_test = X_test.replace([-np.inf, np.inf], np.nan)  # 無限大の値をNaNに置き換えます
```

---The following area is a Code cell (cell numver is 13)---
```python
# 指定された分割数、シャッフルオプション、シードを使って層化交差検証を設定します
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # 層化Kフォールドの初期化

# テストセットの平均予測を格納するための配列を初期化します
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テスト予測用の配列を生成します

# 各フォールドの交差検証スコア（ログ損失）を格納するためのリストを初期化します
cv_scores = list()  # ログ損失スコアを格納するリスト

# 特徴量名のリストを取得します
features = X.columns.tolist()  # 特徴量のリストを作成します

# 各フォールドの特徴量の重要度を格納するためのデータフレームを準備します
feat_imp_df = pd.DataFrame({"feature": features})  # 特徴量の重要度を格納するデータフレームを初期化します

# 交差検証の各フォールドをループします
for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):
    print(f"| Fold {idx+1} |".center(90, "="))  # フォールド番号を表示します

    # 現在のフォールドのトレーニングおよびバリデーションセットにデータを分割します
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # トレーニングデータを分割します
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # バリデーションデータを分割します

    # トレーニングセットとバリデーションセットの形状を表示します
    print(f'train: {X_train.shape}')  # トレーニングデータの形状
    print(f'val: {X_val.shape}')  # バリデーションデータの形状
    
    # 指定されたハイパーパラメータでXGBoost分類器を初期化します
    model = xgb.XGBClassifier(
        objective='multi:softprob',  # マルチクラス確率を取得する目的
        num_class=3,  # クラス数
        eval_metric='mlogloss',  # 評価指標としてのマルチクラスログ損失
        subsample=0.8,  # サブサンプリングの割合
        n_estimators=650,  # 使用する木の数
        learning_rate=0.045,  # 学習率
        max_depth=5,  # 木の最大深さ
        random_state=config.seed,  # 乱数シード
        device="gpu"  # GPUを使用するための設定
    )
    
    # バリデーションセットに対して早期停止を設定してモデルを訓練します
    model.fit(
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],  # トレーニングおよびバリデーションセット
        early_stopping_rounds=75,  # 75ラウンドで早期停止
        verbose=75  # 詳細出力の設定
    )
    
    # バリデーションセットの確率を予測します
    val_preds = model.predict_proba(X_val)  # バリデーションセットに対する予測確率

    # バリデーションセットのログ損失を計算します
    val_log_loss = log_loss(y_val, val_preds, eps="auto")  # ログ損失の計算
    print(f"val log loss: {val_log_loss:.5f}")  # バリデーションログ損失を表示します

    # 交差検証スコアのリストにログ損失を追加します
    cv_scores.append(val_log_loss)  # スコアを追加します
    
    # 現在のフォールドの予測でテスト予測を更新します（すべてのフォールドの平均）
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # テスト予測を更新します
    
    # 現在のフォールドの特徴量の重要度をデータフレームに統合します
    feat_imp_df = feat_imp_df.merge(
        pd.DataFrame(
            {
                "feature": features,  # 特徴量名
                f"fold_{idx+1}_feat_imp": model.feature_importances_,  # 特徴量の重要度
            }
        ),
        on=["feature"],
        how="left",
    )

# セパレータ行を印刷し、交差検証されたログ損失の平均を表示します
print("="*90)
print(f"CV: {np.mean(cv_scores):.5f}")  # 交差検証のスコアの平均を表示します

# すべてのフォールドの平均特徴量重要度を計算します
feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴量の平均重要度を計算

# 平均重要度が最も高い上位50の特徴量をプロットします
plt.figure(figsize=(12, 10))  # プロットのサイズを設定します
sns.barplot(
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[
        :50  # 上位50の特徴量を選択します
    ],
    x="avg_importance",  # x軸に平均重要度
    y="feature",  # y軸に特徴量
    color="royalblue",  # バーの色
    width=0.75,  # バーの幅
)
plt.title("Average Feature Importances for All Folds", size=12)  # プロットタイトル
plt.show()  # プロットを表示します
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 6. 提出ファイルの保存
```

---The following area is a Code cell (cell numver is 15)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]  # テスト予測をサンプル提出ファイルに割り当てます
sample_submission.head()  # サンプル提出ファイルの先頭行を表示します
```

---The following area is a Code cell (cell numver is 16)---
```python
sample_submission.to_csv("submission.csv", index=False)  # サンプル提出ファイルをCSV形式で保存します（インデックスなし）
```

** @@@ Jupyter Notebook numver 16, the number of votes :15 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、人間による好みの予測に挑戦しています。具体的には、異なる大規模言語モデル（LLM）が生成した応答の中から、どちらがユーザーに好まれるかを予測するための機械学習モデルを構築しています。

### 問題に対するアプローチ
Notebookは以下の主要なステップで構成されています：

1. **ライブラリのインポート**: pandas、numpy、nltkなどのデータ処理および自然言語処理ライブラリに加えて、XGBoost、LightGBM、CatBoostなどの機械学習ライブラリをインポートしています。

2. **データの読み込みおよび前処理**:
   - トレーニングデータとテストデータをCSVファイルから読み込み、不要な文字を取り除くなどの前処理を行います。
   - トークン化や特徴量の生成を行う`Preprocessor`クラスが定義され、コサイン類似度やジャッカード類似度の計算、n-gramの生成などが実装されています。

3. **特徴選択**: ANOVA F値に基づいて、最も有用な特徴量を25個選択します。

4. **モデルのトレーニングと評価**:
   - RandomForest、GradientBoosting、SVM、XGBoost、CatBoost、VotingClassifierなど、複数の機械学習モデルを使用します。
   - クロスバリデーションによってモデルのパフォーマンスを評価し、平均CVログロスを計算します。

5. **結果の報告**: 各モデルのCVログロスを比較し、最良のモデルを特定するとともに、特徴の重要度を示すDataFrameを生成します。

6. **提出ファイルの作成**: 最後に、テストデータに対する予測結果を用いて、提出用のCSVファイル（`submission.csv`）を出力します。

### 使用されている手法とライブラリ
- **データ処理**: pandas、numpy、nltk
- **特徴量生成**: コサイン類似度、ジャッカード類似度、n-gram、引用数カウント、トークン化などの手法を用いた特徴量の生成。
- **モデル**: RandomForest、GradientBoosting、SVM、XGBoost、CatBoost、VotingClassifier
- **評価手法**: StratifiedKFoldによるクロスバリデーション、log_lossによる評価

このNotebookは、テキストデータを用いた機械学習タスクに対する徹底した前処理、特徴量エンジニアリング、モデル評価のプロセスを示しており、選好予測の向上を目指しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# 📚 ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
import lightgbm as lgb
import catboost as cb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from sklearn.feature_selection import SelectKBest, f_classif
from sklearn.impute import SimpleImputer

# gcモジュール: Pythonのガーベジコレクションを提供
# osモジュール: オペレーティングシステムの機能を利用
# reモジュール: 正規表現による文字列操作
# numpy: 数値計算用のライブラリ
# pandas: データ操作用のライブラリ
# nltk: 自然言語処理用のライブラリ
# matplotlib.pyplot: データ可視化用の2Dプロットライブラリ
# seaborn: より洗練されたデータ可視化用のライブラリ
# xgboost: 勾配ブースティングアルゴリズムの実装
# lightgbm: LightGBMアルゴリズムの実装
# catboost: CatBoostアルゴリズムの実装
# sklearn: 機械学習用の多数のツールを提供
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# ⚙️ 設定クラス
```

---The following area is a Code cell (cell numver is 4)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")
    test_path = os.path.join(root, "test.csv")
    sample_submission_path = os.path.join(root, "sample_submission.csv")
    seed = 42
    n_splits = 10
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# 📊 データの読み込みと処理

トレーニングデータセットとテストデータセットを読み込み、一部の前処理を適用します。これには以下が含まれます:

1. **データの読み込み**: CSVファイルをpandasのDataFrameに読み込みます。
2. **サブサンプリング**: テストデータセットの行数が10未満の場合は、トレーニングデータセットから10,000行サブサンプリングして迅速に処理します。
3. **文字列の処理**: 不要な文字を削除して、文字列列（`prompt`, `response_a`, `response_b`）をクリーンアップし処理します。
4. **データの形状と欠損値**: データセットの形状を出力し、欠損値をカウントしてデータ構造と品質を理解します。
```

---The following area is a Code cell (cell numver is 6)---
```python
train = pd.read_csv(config.train_path)  # 訓練データを読み込む
test = pd.read_csv(config.test_path)    # テストデータを読み込む
sample_submission = pd.read_csv(config.sample_submission_path)  # 提出用のサンプルデータを読み込む

# テストデータの行数が10未満であれば、トレーニングデータの最初の10,000行を使用する
if test.shape[0] < 10:
    train = train.iloc[:10000]
    
def process(input_str):
    stripped_str = input_str.strip('[]')  # 文字列の前後のブラケットを削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文字列を分割してクォーテーションを削除
    return  ' '.join(sentences)  # 結果をスペースで繋げて返す

# 各列にprocess関数を適用して文字列をクリーンアップ
train["prompt"] = train["prompt"].apply(process)
train["response_a"] = train["response_a"].apply(process)
train["response_b"] = train["response_b"].apply(process)

test["prompt"] = test["prompt"].apply(process)
test["response_a"] = test["response_a"].apply(process)
test["response_b"] = test["response_b"].apply(process)

# データセットの形状を出力
print(f"train shape: {train.shape}")
print(f"test shape: {test.shape}")
print("-"*90)
# 欠損値の計算
print(f"train missing values: {train.isnull().sum().sum()}")
print(f"test missing values: {test.isnull().sum().sum()}")
print("-"*90)

train.head()  # トレーニングデータの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 🛠️ 前処理クラス定義

`Preprocessor`クラスは、テキストデータを処理し特徴量エンジニアリングを行ういくつかのメソッドを含んでいます。以下にその機能の説明を示します：

#### コサイン類似度
- **数式**: 
  $$\text{cosine_similarity} = \frac{A \cdot B}{\|A\| \|B\|}$$

- **説明**: コサイン類似度は2つのベクトルの間の角度のコサインを測定し、テキストの類似度の指標を提供します。

#### ジャッカード類似度
- **数式**:

  $$\text{Jaccard_similarity} = \frac{|A \cap B|}{|A \cup B|}$$

- **説明**: ジャッカード類似度は、2つの集合の交差部分と合併部分を比較することで類似度を測定します。

#### 引用のカウント
- **説明**: このメソッドは、文字列内の単一および二重引用されたテキストを特定し、カウントすることで、引用の数を把握します。

#### トークン化
- **説明**: このメソッドは、テキストを個々の単語（トークン）に分割し、n-gramを生成したり、重複を計算するためのさらなる分析に使用できます。

#### N-gramの生成
- **説明**: N-gramは、与えられたテキストからの'n'項目の連続したシーケンスです。このメソッドは、異なる粒度（単語、2-gram、3-gramなど）でテキストを分析するのに役立ちます。

#### N-gramの重複カウント
- **説明**: このメソッドは、2つのテキスト間の共通のn-gramの数を計算し、類似度を測定するのに役立ちます。

#### 実行
- **説明**: このメソッドは、上記の計算に基づいて新しい特徴量を生成し、機械学習モデルのトレーニングに使用できる全データセットを処理します。

$\frac{n!}{k!(n-k)!} = \binom{n}{k}$
```

---The following area is a Code cell (cell numver is 8)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):
        try:
            vectorizer = TfidfVectorizer().fit_transform([text1, text2])  # テキストのTF-IDFベクトルを作成
            vectors = vectorizer.toarray()  # ベクトルを配列に変換
            cos_sim = cosine_similarity(vectors)  # コサイン類似度を計算
            return cos_sim[0][1]  # コサイン類似度の結果を返す
        except:
            return np.nan  # エラーが発生した場合はNaNを返す

    def jaccard_sim(self, text1: str, text2: str):
        set1 = set(text1.split())  # テキスト1を単語の集合に変換
        set2 = set(text2.split())  # テキスト2を単語の集合に変換
        intersection = set1.intersection(set2)  # 交差部分を計算
        union = set1.union(set2)  # 合併部分を計算
        return len(intersection) / len(union)  # ジャッカード類似度を返す
    
    def count_quotes(self, text: str) -> int:
        single_quote_pattern = r"'(.*?)'"  # 単一引用のパターン
        double_quote_pattern = r'"(.*?)"'  # 二重引用のパターン
        single_quotes = re.findall(single_quote_pattern, text)  # 単一引用を抽出
        double_quotes = re.findall(double_quote_pattern, text)  # 二重引用を抽出
        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用数
        return total_quotes  # 合計の引用数を返す

    def tokenize(self, text: str):
        return nltk.word_tokenize(text.lower())  # テキストを小文字にしトークン化

    def generate_ngrams(self, text: str, n: int):
        tokens = self.tokenize(text)  # トークン化されたテキストを取得
        return list(ngrams(tokens, n))  # n-gramを生成

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # テキスト1からn-gramを生成
            ngrams2 = self.generate_ngrams(text2, n)  # テキスト2からn-gramを生成
            counter1 = Counter(ngrams1)  # テキスト1のn-gramのカウント
            counter2 = Counter(ngrams2)  # テキスト2のn-gramのカウント
            overlap = counter1 & counter2  # 共通のn-gramを計算
            overlap_count = sum(overlap.values())  # 重複の合計を計算
            return overlap_count  # 重複数を返す
        except:
            return 0  # エラーが発生した場合は0を返す
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:
        
        # それぞれの応答間でのn-gramオーバーラップを計算
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)

        # 各応答とプロンプト間でのn-gramオーバーラップを計算
        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)
        
        # 各応答とプロンプトの長さを計算
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))  # 応答Aの長さ
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))  # 応答Bの長さ
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))  # プロンプトの長さ
        
        # 異なる長さの比率や差を計算
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]  # 応答Aとプロンプトの長さの比率
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]  # 応答Bとプロンプトの長さの比率
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]  # 応答Aと応答Bの長さの比率
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]  # 長さの差
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]  # 応答Aとプロンプトの長さの差
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]  # 応答Bとプロンプトの長さの差
        
        # n-gramオーバーラップの比率を計算
        data["respa_prompt_overlap_unigram_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]
        data["respa_prompt_overlap_bigram_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]
        data["respa_prompt_overlap_trigram_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]

        data["respb_prompt_overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]
        data["respb_prompt_overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]
        data["respb_prompt_overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]
        
        # 引用のカウントを計算
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))  # 応答Aの引用数
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))  # 応答Bの引用数
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))  # プロンプトの引用数
        
        # コサイン類似度を計算
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)
        
        return data  # 処理後のデータを返す
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time

preprocessor = Preprocessor()  # Preprocessorクラスのインスタンスを作成
train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行
test = preprocessor.run(test)  # テストデータに対して前処理を実行
train.head()  # トレーニングデータの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
### データ準備
```

---The following area is a Code cell (cell numver is 11)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]  # 削除する列のリスト
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]  # ターゲット列のリスト
target = "target"  # ターゲット変数

train[target] = np.nan  # ターゲット変数の初期化
for idx, t in enumerate(target_cols):
    train.loc[train[t] == 1, target] = idx  # ターゲット列に基づいてインデックスを設定
train[target] = train[target].astype("int32")  # ターゲット変数を整数型に変換
    
train.head()  # トレーニングデータの最初の5行を表示
```

---The following area is a Code cell (cell numver is 12)---
```python
X = train.drop(columns=target_cols+drop_cols+[target]+["model_a", "model_b"], axis=1)  # 特徴量マトリックスXを作成
y = train[target]  # ターゲット変数yを設定
X_test = test.drop(columns=drop_cols, axis=1)  # テストデータの特徴量マトリックスX_testを作成

# 無限大や負無限大をNaNに置き換える
X = X.replace([-np.inf, np.inf], np.nan)
X_test = X_test.replace([-np.inf, np.inf], np.nan)
```

---The following area is a Code cell (cell numver is 13)---
```python
# 欠損値の処理
imputer = SimpleImputer(strategy='mean')  # 平均で欠損値を補完するためのインプッターを作成
X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)  # トレーニングデータの欠損値を補完
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns)  # テストデータの欠損値を補完
```

---The following area is a Code cell (cell numver is 14)---
```python
# 特徴選択
selector = SelectKBest(f_classif, k=25)  # 最良のk個の特徴量を選択
X_new = selector.fit_transform(X, y)  # トレーニングデータに対して特徴選択を適用
X_test_new = selector.transform(X_test)  # テストデータに対して同様に適用
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# 🧩 モデルのトレーニングと評価

### モデルの定義

ランダムフォレスト、勾配ブースティング、SVM、XGBoost、CatBoost、および投票分類器を含むいくつかの機械学習モデルを定義します。

### 特徴選択

ANOVA F値に基づいて、SelectKBestを使用して25の最良特徴を選択します。

### クロスバリデーション

モデル評価のために層化K分割交差検証を使用します。

### トレーニングと評価

各モデルを反復処理し、トレーニングデータを使ってトレーニングし、クロスバリデーションを使用して評価し、平均CVログロスを計算します。

### 特徴の重要度

適用可能なモデル（ランダムフォレスト、勾配ブースティング、XGBoost、CatBoost）について、特徴の重要度を計算して保存します。

### 最良モデルの特定

最も低いCVログロスに基づいて、最も良いパフォーマンスのモデルを特定します。

### 結果の表示

各モデルのCVログロスを示し、適用可能な場合は特徴の重要度を表示するDataFrameを表示します。
```

---The following area is a Code cell (cell numver is 16)---
```python
# モデルとその設定を定義
models = {
    'random_forest': {
        'model': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=config.seed
        ),
        'params': {}
    },
    'gradient_boosting': {
        'model': GradientBoostingClassifier(
            n_estimators=300,
            learning_rate=0.1,
            max_depth=5,
            subsample=0.8,
            random_state=config.seed
        ),
        'params': {}
    },
    'svm': {
        'model': SVC(
            kernel='rbf',
            C=1.0,
            gamma='scale',
            probability=True,
            random_state=config.seed
        ),
        'params': {}
    },
    'xgboost': {
        'model': xgb.XGBClassifier(
            objective='multi:softprob',
            num_class=3,
            eval_metric='mlogloss',
            subsample=0.8,
            n_estimators=650,
            learning_rate=0.045,
            max_depth=5,
            random_state=config.seed,
#             tree_method='gpu_hist'  # グラフィックデバイスが利用可能ならばGPU加速を使用
        ),
        'params': {}
    },
    'catboost': {
        'model': cb.CatBoostClassifier(
            loss_function='MultiClass',
            iterations=650,
            learning_rate=0.045,
            depth=5,
            random_seed=config.seed,
#             task_type="GPU",  # GPUが利用可能ならば使用
            verbose=75
        ),
        'params': {}
    },
    'voting': {
        'model': VotingClassifier(
            estimators=[
                ('lr', LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=200)),
                ('svc', SVC(probability=True)),
                ('knn', KNeighborsClassifier(n_neighbors=5))
            ],
            voting='soft'
        ),
        'params': {}
    }
}

# SelectKBestを使用して特徴を選択
selector = SelectKBest(f_classif, k=25)
X_new = selector.fit_transform(X, y)  # 特徴量マトリックスXを新しい特徴に変換
X_test_new = selector.transform(X_test)  # テストデータに対しても新しい特徴を適用

# クロスバリデーションの設定
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)

# 結果を格納するDataFrame
results = []

# モデルを反復処理
for model_name, model_data in models.items():
    model = model_data['model']
    print(f"モデルのトレーニング: {model_name}")

    test_preds = np.zeros(shape=(X_test_new.shape[0], y.nunique()))  # テスト予測の配列を初期化
    cv_scores = []  # CVスコアを格納するリスト

    for idx, (train_idx, val_idx) in enumerate(cv.split(X_new, y)):
        X_train, y_train = X_new[train_idx], y[train_idx]  # トレーニングデータとラベル
        X_val, y_val = X_new[val_idx], y[val_idx]  # 検証データとラベル

        if model_name == 'voting':
            model.fit(X_train, y_train)  # 投票モデルをトレーニング
        elif model_name == 'catboost':
            model.fit(
                X_train,
                y_train,
                eval_set=[(X_train, y_train), (X_val, y_val)],
                early_stopping_rounds=75,  # 早期停止
                verbose=75
            )
        else:
            model.fit(X_train, y_train)  # その他のモデルをトレーニング

        # 検証データで予測を行う
        if model_name != 'voting':
            val_preds = model.predict_proba(X_val)  # 検証データの確率予測
            val_log_loss = log_loss(y_val, val_preds, eps="auto")  # ログロスを計算
            cv_scores.append(val_log_loss)  # スコアをリストに追加

            test_preds += model.predict_proba(X_test_new) / cv.get_n_splits()  # テスト予測を集積

    if model_name != 'voting':
        mean_cv_log_loss = np.mean(cv_scores)  # 平均CVログロスを計算
        results.append({'Model': model_name, 'CV_Log_Loss': mean_cv_log_loss})  # 結果を格納
        print(f"平均CVログロス: {mean_cv_log_loss:.5f}")  # 結果を表示

# 特徴重要度を格納（適用可能なモデルに対して）
if model_name in ['random_forest', 'gradient_boosting', 'xgboost', 'catboost']:
    features = X.columns[selector.get_support()].tolist()  # 選択された特徴
    feat_imp_df = pd.DataFrame({"feature": features})  # 特徴重要度のDataFrameを作成
    feat_imp_df[f"{model_name}_avg_importance"] = 0  # 初期化

    for idx, (_, val_idx) in enumerate(cv.split(X_new, y)):
        X_val, _ = X_new[val_idx], y[val_idx]  # 検証データ
        feat_imp_df[f"{model_name}_avg_importance"] += model.feature_importances_ / cv.get_n_splits()  # 重みを集計

    results_df = pd.DataFrame(results)  # 結果のDataFrameを作成
    results_df = pd.concat([results_df, feat_imp_df], axis=1)  # 特徴重要度を結合

# 結果をDataFrameに変換
results_df = pd.DataFrame(results)

# 最良モデルの特定
best_model = results_df.loc[results_df['CV_Log_Loss'].idxmin()]  # 最小のCVログロスを持つモデルを特定
print(f"\n最良モデル:\n{best_model}")

# 結果のDataFrameを表示
print("\n結果のDataFrame:")
print(results_df)
```

---The following area is a Code cell (cell numver is 17)---
```python
for idx, t in enumerate(target_cols):
    sample_submission[t] = test_preds[:, idx]  # 各ターゲット列にテスト予測を格納
sample_submission.head()  # 提出用DataFrameの最初の5行を表示
```

---The following area is a Code cell (cell numver is 18)---
```python
sample_submission.to_csv("submission.csv", index=False)  # 提出ファイルをCSVとして保存
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
---

# コメント 

> ## Barla Che
> 
> このウルトラノートブックを共有してくれてありがとう
> 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > あなたの言葉に感謝します [@icassp](https://www.kaggle.com/icassp)
> > 
> > 
> > 


---

> ## Melissa Monfared
> 
> この素晴らしいノートブックを共有してくれてありがとう。 [@nabojyotipandey](https://www.kaggle.com/nabojyotipandey) 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > あなたがそれを役立ててくれてうれしいです 🤩 [@melissamonfared](https://www.kaggle.com/melissamonfared) 
> > 
> > 
> > 


---

> ## Akshat111111
> 
> 本当に素晴らしい仕事
> 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > ありがとう、友よ 🤩 [@akshat110203](https://www.kaggle.com/akshat110203) 
> > 
> > 
> > 


---

> ## Abhishek0032
> 
> よくやった [@nabojyotipandey](https://www.kaggle.com/nabojyotipandey) 
> 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > ありがとう 😀 [@abhishek0032](https://www.kaggle.com/abhishek0032) 
> > 
> > 
> > 


---

> ## Ahmad Rafiee
> 
> 素晴らしい仕事をしました [@nabojyotipandey](https://www.kaggle.com/nabojyotipandey) ありがとう
> 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > あなたがそれを気に入ってくれてうれしいです [@ahmadrafiee](https://www.kaggle.com/ahmadrafiee) 
> > 
> > 
> > 


---

> ## Rabie El Kharoua
> 
> [@nabojyotipandey](https://www.kaggle.com/nabojyotipandey) 良い仕事、共有してくれてありがとう。
> 
> 
> 
> > ## NABOJYOTI PANDEY話題作成者
> > 
> > ありがとう！😀 [@rabieelkharoua](https://www.kaggle.com/rabieelkharoua) 
> > 
> > 
> > 


---
```

** @@@ Jupyter Notebook numver 17, the number of votes :13 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、異なる大規模言語モデル（LLM）の生成した応答の中から、どちらがより好まれるかを予測するための問題に取り組んでいます。Notebookでは、主に以下の流れで問題解決が行われています。

### 問題の背景
コンペティションでは、ユーザーが2つの匿名化されたチャットボットの応答から好みのものを選ぶタスクに基づいて、どちらの応答が選ばれるかを予測するモデルの構築が求められています。これに対して、いくつかのアプローチとモデルが提案されています。

### 使用している手法とライブラリ
1. **ライブラリのインストールとインポート**: 
   - `transformers`, `peft`, `accelerate`, `bitsandbytes` などのライブラリを利用して、深層学習と自然言語処理のタスクを行います。
   - `torch` (PyTorch)や`sklearn` (Scikit-learn)なども使用されています。

2. **トークナイザーの設定とデータ処理**:
   - 入力データの前処理として、テキストをトークナイズし、モデルが扱いやすい形式に変換します。
   - `AutoTokenizer`や`GemmaTokenizerFast`を使用して、テキストデータをトークナイズし、アテンションマスクを生成します。

3. **モデルの設定と学習**:
   - `LlamaForSequenceClassification`や`Gemma2ForSequenceClassification`などの事前訓練済みモデルをロードし、それぞれ異なるGPUで推論を行う設定をしています。
   - LoRA (Low-Rank Adaptation)技術を使用することで、モデルの重みを効率的に調整し、推論の精度を上げています。

4. **推論の実行**:
   - スレッドを利用して並列処理を行い、モデルの出力から各応答の勝率を計算します。
   - 結果はDataFrameに格納され、最終的な予測を得るために統合されます。

5. **最終予測の算出**:
   - LlamaモデルとLightGBMを組み合わせたアンサンブル学習により、より robust な予測を実現しています。
   - `submission.csv`として結果を出力するためのデータフレームを準備し、最終的な提出用ファイルが生成されています。

このノートブックは、LLMを活用した人間の好みの予測という複雑なタスクに対して、最先端の技術を利用し、モデルのパフォーマンスを最大限に引き出すための包括的なアプローチを採用しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# 必要なライブラリをインストールする
# transformers、peft、accelerate、bitsandbytesを最新バージョンでインストールします。
!pip install transformers peft accelerate bitsandbytes \-U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 2)---
```python
# bitsandbytesライブラリをインストールする（-qで出力を抑制）
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
# transformersライブラリをインストールする（-qで出力を抑制）
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
# tokenizersライブラリをインストールする（-qで出力を抑制）
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
# peftライブラリをインストールする（-qで出力を抑制）
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# パート 1 (LLama 3 8b チャット)
```

---The following area is a Code cell (cell numver is 4)---
```python
# 必要なライブラリをインポートする
import torch  # PyTorchライブラリをインポート
import sklearn  # scikit-learnライブラリをインポート
import numpy as np  # NumPyライブラリをインポート
import pandas as pd  # Pandasライブラリをインポート
import time  # 時間関連の機能を使用するためにtimeモジュールをインポート

# Transformersライブラリから必要なモデルやトークナイザーをインポート
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
# PEFTライブラリから必要なモジュールをインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# 自動混合精度を使用するためにautocastをインポート
from torch.cuda.amp import autocast
# スレッドを使用するためにThreadをインポート
from threading import Thread

# ガベージコレクションを制御するためにgcモジュールをインポート
import gc
import os  # オペレーティングシステム関連の機能を使用するためにosモジュールをインポート
import io  # 入出力操作を行うためにioモジュールをインポート
import time  # 時間関連の機能を使用するためにtimeモジュールを再度インポート
import json  # JSONデータを扱うためにjsonモジュールをインポート
import random  # ランダム数生成のためにrandomモジュールをインポート
import pickle  # Pythonオブジェクトのシリアル化のためにpickleモジュールをインポート
import zipfile  # ZIPファイルを扱うためにzipfileモジュールをインポート
import datetime  # 日付と時刻を扱うためにdatetimeモジュールをインポート
import matplotlib.pyplot as plt  # データの可視化のためにMatplotlibをインポート
from IPython.display import display  # IPython環境での出力を行うためにdisplay関数をインポート
from collections import Counter  # 要素のカウントのためのCounterクラスをインポート
from collections import defaultdict  # デフォルト値を持つ辞書を作成するためのdefaultdictクラスをインポート
import torch  # PyTorchライブラリを再度インポート
from torch import nn  # ニューラルネットワーク関連モジュールをインポート
import torch.nn.functional as F  # ニューラルネットワークの関数をインポート
import pytorch_lightning as pl  # PyTorch Lightningをインポート
from torch.utils.data import Dataset, DataLoader  # データセットとデータローダーをインポート
from sklearn.metrics import log_loss  # ログ損失の計算のためにインポート
import tokenizers  # トークナイザーを扱うためにtokenizersライブラリをインポート
```

---The following area is a Code cell (cell numver is 5)---
```python
# データクラスを使用するためにdataclassをインポート
from dataclasses import dataclass
# スレッドプールを使用するためにThreadPoolExecutorをインポート
from concurrent.futures import ThreadPoolExecutor

# TransformersライブラリからGemmaモデルとトークナイザーをインポート
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
# トークナイザーの警告を抑制しながらパディングを行うためのデータコレーターをインポート
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
# PEFTライブラリからPeftModelをインポート
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 6)---
```python
# メモリ効率の良いシステムダイナミクスプログラミング（SDP）を無効にする
torch.backends.cuda.enable_mem_efficient_sdp(False)
# フラッシュSDPを無効にする
torch.backends.cuda.enable_flash_sdp(False)

# GPUが利用可能でない場合の警告メッセージ
if (not torch.cuda.is_available()): 
    print("申し訳ありませんが、GPUが必要です！")

# モデルと重みのパスを設定
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'

# モデルの最大入力長とバッチサイズを設定
MAX_LENGTH = 1284  # 最大入力長を1284に設定
BATCH_SIZE = 8  # バッチサイズを8に設定

# 計算デバイスを指定（GPUを使用）
DEVICE = torch.device("cuda")
```

---The following area is a Code cell (cell numver is 7)---
```python
# テストデータセットをCSVファイルから読み込む
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
# 提出サンプルをCSVファイルから読み込む
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 8)---
```python
# リスト内の文字列を結合する関数を定義
def process(input_str):
    # 入力文字列の両端のブラケットを削除
    stripped_str = input_str.strip('[]')
    # カンマで区切った部分をリストに分割し、余分な引用符を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # リストの要素を空白で結合して一つの文字列にする
    return  ' '.join(sentences)

# データフレームの各列に対してprocess関数を適用
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# 提出サンプルとテストデータの最初の5行を表示
display(sample_sub)
display(test.head(5))

# モデル用にテキストを準備
test['text'] = 'ユーザープロンプト: ' + test['prompt'] +  '\n\nモデル A:\n' + test['response_a'] +'\n\n--------\n\nモデル B:\n'  + test['response_b']
# 最初のテキストを表示
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## トークナイズ
```

---The following area is a Code cell (cell numver is 10)---
```python
# トークナイザーを指定したパスからロードする
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# テキストをトークナイズし、パディング、トランケーションを行う
tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

# 入力IDとアテンションマスクを指定したデバイスに移動し、データ型を整数32ビットに指定
INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# テンソルをCPUに移動させ、リストに変換
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

# データフレームを作成し、入力IDとアテンションマスクを格納
data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
# 最初の2行を表示
data[:2]
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## モデルのロード
> 各GPUに1つのモデルをロードします。
```

---The following area is a Code cell (cell numver is 12)---
```python
# BitsAndBytesの設定を行う
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットでのロードを指定
    bnb_8bit_compute_dtype=torch.float16,  # 計算に使用する8ビットのデータ型をfloat16に指定
    bnb_8bit_use_double_quant=False  # ダブル量子化を使用しない場合はFalse
)

# GPU 0にベースモデルをロード
device0 = torch.device('cuda:0')

# LlamaForSequenceClassificationモデルを指定した設定でロード
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # ラベルの数を3に指定
    torch_dtype=torch.float16,  # モデルパラメータのデータ型をfloat16に指定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:0'  # モデルをGPU 0にマップ
)
# パディングトークンIDを設定
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 13)---
```python
# GPU 1にベースモデルをロード
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # ラベルの数を3に指定
    torch_dtype=torch.float16,  # モデルパラメータのデータ型をfloat16に指定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:1'  # モデルをGPU 1にマップ
)
# パディングトークンIDを設定
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRa（Low-Rank Adaptation）の設定を行う
peft_config = LoraConfig(
    r=16,  # ランクを16に設定
    lora_alpha=32,  # LoRaのスケーリングファクターを32に設定
    lora_dropout=0.10,  # LoRaのドロップアウト率を10%に設定
    bias='none',  # バイアスの設定をなしに指定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクタイプを列分類に設定
    target_modules=['o_proj', 'v_proj']  # 対象モジュールを指定
)
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTモデルを取得し、GPU 0に配置
model_0 = get_peft_model(base_model_0, peft_config).to(device0)
# 重みをロード
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みを指定したパスから読み込み
model_0.eval()  # 評価モードに設定

# PEFTモデルを取得し、GPU 1に配置
model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みを指定したパスから読み込み
model_1.eval()  # 評価モードに設定

# 学習可能なパラメータを表示
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 16)---
```python
# ガベージコレクションを実行して、使用されていないメモリを解放
gc.collect()
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## 推論
```

---The following area is a Code cell (cell numver is 18)---
```python
# 推論を行う関数を定義
def inference(df, model, device, batch_size=BATCH_SIZE):
    # データフレームから入力IDとアテンションマスクを取得
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    # 各クラスの生成された確率を格納するリストを初期化
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()  # モデルを評価モードに設定
    
    # データをバッチ処理で推論
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # 現在のバッチの入力IDをデバイスに移動
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # 現在のバッチのアテンションマスクをデバイスに移動
        
        with torch.no_grad():  # 勾配計算を無効にする
            with autocast():  # 自動混合精度を使用
                outputs = model(
                    input_ids=batch_input_ids,  # モデルに入力IDを渡す
                    attention_mask=batch_attention_mask  # モデルにアテンションマスクを渡す
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # 出力のロジットをソフトマックス関数で確率に変換
        
        # 各クラスの確率をリストに追加
        generated_class_a.extend(probabilities[:, 0])  # クラスAの確率を追加
        generated_class_b.extend(probabilities[:, 1])  # クラスBの確率を追加
        generated_class_c.extend(probabilities[:, 2])  # 引き分けの確率を追加
    
    # データフレームに生成された確率を追加
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  # GPUのキャッシュメモリを解放

    return df  # 推論結果が含まれたデータフレームを返す
```

---The following area is a Code cell (cell numver is 19)---
```python
# 計測開始時刻を記録
st = time.time()

N_SAMPLES = len(data)  # サンプルの総数を取得

# データを2つのサブセットに分割
half = round(N_SAMPLES / 2)  # サンプル数の半分を計算
sub1 = data.iloc[0:half].copy()  # 最初の半分をコピー
sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りの半分をコピー

# スレッドで推論を実行する関数を定義
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)  # 指定したインデックスに推論結果を格納

# スレッドからの結果を格納する辞書を初期化
results = {}
```

---The following area is a Code cell (cell numver is 20)---
```python
# スレッドを開始
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # サブセット1の推論を実行するスレッド
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # サブセット2の推論を実行するスレッド

t0.start()  # スレッド0を開始
t1.start()  # スレッド1を開始

# すべてのスレッドが終了するのを待機
t0.join()
t1.join()

# 推論結果を元のデータフレームに統合
data = pd.concat([results[0], results[1]], axis=0)

# 処理が完了したことを出力
print(f"処理が完了しました。総時間: {time.time() - st}")

# 提出ファイル用のターゲットを設定
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

# サンプル提出データフレームに結果を追加
sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 21)---
```python
# ターゲットの結果をNumPy配列に変換
llama_preds = data[TARGETS].values  # 予測結果を取得し、NumPy配列に格納
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
## LGBM + TF-IDF
```

---The following area is a Code cell (cell numver is 23)---
```python
# コンペティションのタグを設定
TAG = 'lmsys-chatbot-arena'

import os  # osモジュールをインポート
# RUNPOD 環境であるかをチェック
RUNPOD = os.path.exists('/workspace/')  
# Kaggle 環境かどうかを設定
KAGGLE = not RUNPOD  
if KAGGLE: 
    print('kaggle')  # Kaggle環境の場合、メッセージを出力
```

---The following area is a Code cell (cell numver is 24)---
```python
# pandasライブラリをインポートを試みる
try:
    import pandas as pd
except:
    # pandasがインポートできなかった場合は、必要なライブラリをインストール
    !pip install -q kaggle  # Kaggleライブラリをインストール
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # データ処理と可視化に必要なライブラリをインストール
    !pip install -q protobuf  # protobufライブラリをインストール
    !pip install -q numba  # numbaライブラリをインストール
```

---The following area is a Code cell (cell numver is 25)---
```python
# データのパスを設定
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)

import os  # osモジュールをインポート

# RUNPOD環境での処理
if RUNPOD:
    # Kaggleの認証ファイルが存在しない場合
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle  # kaggleディレクトリを作成
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # kaggle.jsonをコピー
        !chmod 600 /root/.kaggle/kaggle.json  # ファイルのアクセス権を設定

    # Kaggleのデータセットが存在しない場合
    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/  # Kaggleからデータセットをダウンロード
        
    # データがまだ存在しない場合
    if not os.path.exists('/data/'):
        import zipfile  # zipfileモジュールをインポート
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍
```

---The following area is a Code cell (cell numver is 26)---
```python
# 入力パス、モデルパス、ロジットパスを設定
INPUT_PATH = '/kaggle/input/'  
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'

# Kaggle環境の場合のモデルパスの設定
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'

# モデルパスを出力
print(MODEL_PATH)

# コードパスと保存パスの設定
CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'
SAVE_PATH = MODEL_PATH if not KAGGLE else ''
```

---The following area is a Code cell (cell numver is 27)---
```python
# トークナイザーの並列処理を無効にするための環境変数を設定
os.environ['TOKENIZERS_PARALLELISM'] = 'false'
```

---The following area is a Code cell (cell numver is 28)---
```python
# トレーニングデータ、テストデータ、サンプル提出ファイルをCSVから読み込む
train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # トレーニングデータを読み込む
test = pd.read_csv(open(DATA + 'test.csv', 'r'))    # テストデータを読み込む
sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込む

# トレーニングデータとテストデータのサンプル数を出力
print(len(train), len(test))
```

---The following area is a Code cell (cell numver is 29)---
```python
# モデルのパラメータを設定
params = {}
if False:  # テストデータの長さが10未満でない限り
    pass;
    params['subsample'] = 30  # サブサンプルサイズを30に設定
else:
    # params['subsample'] = 2  # サブサンプルサイズを2に設定（コメントアウト）
    params['fold'] = -1  # フォールド設定を-1に設定

# エポック数とLightGBMのモデル数を設定
params['n_epochs'] = 1  # エポック数を1に設定
params['n_lgb'] = 1  # 使用するLightGBMのモデル数を1に設定
params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルを指定
```

---The following area is a Code cell (cell numver is 30)---
```python
# パラメータの設定を確認
FULL = params.get('fold', 0) < 0  # フォールドの値が0未満ならFULLをTrueに設定
N_FOLDS = int(params.get('n_folds', 3))  # フォールド数を取得（デフォルトは3）
FOLD = int(params.get('fold', 0))  # 現在のフォールドを取得（デフォルトは0）
SEED = int(params.get('seed', 3))  # シード値を取得（デフォルトは3）
SS = int(params.get('subsample', 1))  # サブサンプルサイズを取得（デフォルトは1）

# 設定したパラメータを出力
print(N_FOLDS, FOLD, SEED, SS)
```

---The following area is a Code cell (cell numver is 31)---
```python
from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldをインポート

# フォールドを生成する関数を定義
def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state=SEED, shuffle=True)\
                    .split(X=np.zeros(len(train)), y=train.iloc[:, -3:].idxmax(1)))  # StratifiedKFoldで分割

# 現在のフォールドに基づいてトレーニングIDとテストIDを取得
train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
# サブサンプルサイズが1より大きい場合、インデックスをサンプリング
if SS > 1: 
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

# トレーニングIDとテストIDの長さを出力し、重複がないことを確認
print(len(train_ids), len(test_ids));  
assert set(train_ids) & set(test_ids) == set()  # トレーニングIDとテストIDに重複がないことを確認
```

---The following area is a Code cell (cell numver is 32)---
```python
# リスト内の文字列を結合する関数を定義
def join_strings(x):
    # xがリストの場合、要素を結合して1つの文字列にする
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x
    return x  # 結合された文字列またはそのままのxを返す
```

---The following area is a Code cell (cell numver is 33)---
```python
# 結合された文字列の単語数を返す関数を定義
def len_join_strings(x):
    # join_strings関数を使って文字列を結合し、単語数をカウントして返す
    return len(join_strings(x).split())  # 結合された文字列を空白で分割し、その長さを返す
```

---The following area is a Code cell (cell numver is 34)---
```python
# JSON形式の文字列を読み込み、結合された文字列の単語数を返す関数を定義
def len_join_strings_j(x):
    # JSON文字列をPythonオブジェクトに変換
    x = json.loads(x)
    # len_join_strings関数を使用して単語数を返す
    return len_join_strings(x)  # 結合された文字列の単語数を返す
```

---The following area is a Code cell (cell numver is 35)---
```python
# ランダムシードを設定
torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchのシードを設定
random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールのシードを設定
np.random.seed(datetime.datetime.now().microsecond)  # NumPyのシードを設定
```

---The following area is a Code cell (cell numver is 36)---
```python
# トレーニング、推論、保存のフラグを設定
# TRAIN = True and not KAGGLE  # 学習を行うかどうかのフラグ（Kaggleでは行わない場合はTrue）
TRAIN = False  # トレーニングを行わない設定
INFER = True  # 推論を行う設定（Kaggleでは行う場合もTrue）
SAVE = False  # 保存を行わない設定
```

---The following area is a Code cell (cell numver is 37)---
```python
# LightGBMライブラリとテキストのカウントベクタイザーをインポート
import lightgbm as lgb  # LightGBMをインポート
from sklearn.feature_extraction.text import CountVectorizer  # CountVectorizerをインポート
```

---The following area is a Code cell (cell numver is 38)---
```python
# LightGBMのトレーニングおよび推論フラグを設定
LGB = True  # LightGBMを使用する設定
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # トレーニングの条件を満たすかどうか
INFER_LGB = not TRAIN and LGB  # 推論を行う条件を設定

# 保存されたカウントベクタイザーを読み込む
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # 通常のカウントベクタイザーを読み込む
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # クリーンカウントベクタイザーを読み込む
```

---The following area is a Code cell (cell numver is 39)---
```python
# シンログ変換を行う関数を定義
def symlog(x): 
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # シンログ変換を適用

# 密な行列に変換してシンログ変換を行う関数を定義
def dense(x):
    x = np.asarray(x.astype(np.float32).todense())  # 密な行列に変換
    x = symlog(x)  # シンログ変換を適用
    return x  # 変換後の行列を返す

# 特徴量を取得するための関数を定義
def get_features(df):
    # プロンプトの特徴量を取得
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    
    # 応答Aの特徴量を取得
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    
    # 応答Bの特徴量を取得
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    # 特徴量を結合
    v = np.hstack([
          # pfeat,  # プロンプトの特徴量を含める場合のコメント
          afeat - bfeat,  # 応答Aと応答Bの差分
          np.abs(afeat - bfeat),  # 応答Aと応答Bの絶対差
          # afeat + bfeat  # 応答Aと応答Bの和を含める場合のコメント
        ])
    
    # 全投票モデルの数で正規化
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # データフレームの長さに基づいて正規化
    except: 
        pass  # エラーが発生した場合は無視

    # 追加の特徴量を取得
    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)  # 各文字列の出現回数をカウント
            
    # 文字列の長さと単語数を追加
    extras.append(df[c].str.len())  # 文字列の長さ
    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 単語数
    
    # 追加の特徴量をスタックして変換
    extras = np.stack(extras, axis=1)  # 新たに追加された特徴量を結合
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # ルートとログ変換を適用
    
    # 結合した特徴量を返す
    return np.hstack([v, extras])  # 基本特徴量と追加特徴量を結合して返す
    # return v  # 基本特徴量のみを返す場合のコメント
```

---The following area is a Code cell (cell numver is 40)---
```python
# 保存されたLightGBMモデルを読み込む
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込んで変数に格納
```

---The following area is a Code cell (cell numver is 41)---
```python
# 推論を行う条件が満たされている場合
if INFER and params.get('n_lgb', 1) > 0:
    df = test  # テストデータをdfに設定
    yps = []  # 予測結果を格納するリストを初期化
    b = 1000  # バッチサイズを設定
    for i in range(0, len(df), b):  # バッチ処理でデータを処理
        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得
        ypms = []  # 各モデルの予測値を格納するリストを初期化
        for model in lgb_models:  # 各LightGBMモデルに対して予測
            ypms.append(model.predict_proba(arr))  # 予測確率を取得
            
        yps.append(np.stack(ypms).mean(0))  # モデルの予測値を平均してリストに追加
        # break;  # ループを中断する場合のコメント
        print('.', end = '')  # 進行状況を表示
        
        # 指定した条件ごとにガベージコレクションを実行
        if len(yps) % 2 == 0:  
            gc.collect()  # メモリを解放
    print()

    # すべての予測結果を連結
    yp = np.concatenate(yps)  # 予測値を一つの配列にまとめる
```

---The following area is a Code cell (cell numver is 42)---
```python
# LightGBMモデルによる予測結果を変数に格納
lgb_preds = yp  # 予測値をlgb_predsに設定
```

---The following area is a Code cell (cell numver is 43)---
```python
# LightGBMの予測とLLamaの予測を組み合わせて最終予測を計算
lgb_wt = 0.2  # LightGBMの重みを設定
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 重み付き平均を計算して最終予測を生成
```

---The following area is a Code cell (cell numver is 44)---
```python
# 最終予測結果をデータフレームに変換
out = pd.DataFrame(preds, 
                index=df.id,  # データフレームのインデックスをテストデータのIDに設定
                columns=train.columns[-3:])  # 列名をトレーニングデータの最後の3列に設定

# 最初の5行を表示
display(out.head())  # 予測結果データフレームの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 45)---
```markdown
# パート 2 (Gemma 2 4b QLora)
```

---The following area is a Code cell (cell numver is 46)---
```python
# 使用可能なCUDAデバイスの数が2であることを確認
assert torch.cuda.device_count() == 2  # CUDAデバイスが2つであることを確認するアサーション
```

---The following area is a Markdown cell (cell numver is 47)---
```markdown
## 設定
```

---The following area is a Code cell (cell numver is 48)---
```python
# 設定を格納するためのデータクラスを定義
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'  # Gemmaモデルのディレクトリ
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'  # LoRaモデルのチェックポイントディレクトリ
    max_length = 2048  # 最大入力長を2048に設定
    batch_size = 4  # バッチサイズを4に設定
    device = torch.device("cuda")  # 使用するデバイスをCUDAに設定
    tta = False  # テストタイムデータ拡張を無効にするフラグ
    spread_max_length = False  # 最大長の分散を無効にするフラグ

cfg = Config()  # 設定のインスタンスを作成
```

---The following area is a Markdown cell (cell numver is 49)---
```markdown
## データのロードと前処理
```

---The following area is a Code cell (cell numver is 50)---
```python
# テストデータをCSVファイルから読み込む
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む
```

---The following area is a Code cell (cell numver is 51)---
```python
# テキストを処理する関数を定義
def process_text(text: str) -> str:
    # テキストを評価し、空白で結合して返す
    return " ".join(eval(text, {"null": ""}))

# 各列に対してprocess_text関数を適用
test.loc[:, 'prompt'] = test['prompt'].apply(process_text)  # プロンプトの処理
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)  # 応答Aの処理
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)  # 応答Bの処理

# 最初の5行を表示
display(test.head(5))  # 処理後のテストデータの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 52)---
```markdown
## トークナイズ
```

---The following area is a Code cell (cell numver is 53)---
```python
# テキストをトークナイズする関数を定義
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # プロンプトにプレフィックスを追加
    prompt = ["<prompt>: " + p for p in prompt]
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]  # 応答Aにプレフィックスを追加
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]  # 応答Bにプレフィックスを追加
    
    if spread_max_length:
        # 最大長を分散させてトークナイズ
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 入力IDを結合
        attention_mask = [[1]* len(i) for i in input_ids]  # アテンションマスクを作成
    else:
        # 通常のトークナイズ
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # テキストを結合
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイズ
        input_ids = tokenized.input_ids  # 入力IDを取得
        attention_mask = tokenized.attention_mask  # アテンションマスクを取得
        
    return input_ids, attention_mask  # 入力IDとアテンションマスクを返す
```

---The following area is a Code cell (cell numver is 54)---
```python
# トークナイザーの初期化とデータ処理の計測
%%time

# Gemmaトークナイザーを指定したディレクトリからロード
tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)
tokenizer.add_eos_token = True  # 終了トークンを追加
tokenizer.padding_side = "right"  # パディングを右側に設定

# データフレームを初期化
data = pd.DataFrame()
data["id"] = test["id"]  # テストデータのIDを設定
# トークナイズして入力IDとアテンションマスクを取得
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)  # 入力IDの長さを取得

# データの拡張用データフレームを初期化
aug_data = pd.DataFrame()
aug_data["id"] = test["id"]  # 拡張データのIDを設定
# 応答Aと応答Bを入れ替えてトークナイズ
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)  # 入力IDの長さを取得
```

---The following area is a Code cell (cell numver is 55)---
```python
# 最初の入力IDをデコードして人間が読める形式に変換し、表示
print(tokenizer.decode(data["input_ids"][0]))  # 最初の入力IDをデコードして出力
```

---The following area is a Code cell (cell numver is 56)---
```python
# 拡張データの最初の入力IDをデコードして人間が読める形式に変換し、表示
print(tokenizer.decode(aug_data["input_ids"][0]))  # 拡張データの最初の入力IDをデコードして出力
```

---The following area is a Markdown cell (cell numver is 57)---
```markdown
## モデルのロード
```

---The following area is a Code cell (cell numver is 58)---
```python
# GPU 0にベースモデルをロード
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,  # 指定したディレクトリからモデルをロード
    device_map=device_0,  # モデルをGPU 0に配置
    use_cache=False,  # キャッシュを無効にする
)

# GPU 1にベースモデルをロード
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,  # 指定したディレクトリからモデルをロード
    device_map=device_1,  # モデルをGPU 1に配置
    use_cache=False,  # キャッシュを無効にする
)
```

---The following area is a Markdown cell (cell numver is 59)---
```markdown
#### LoRAアダプターのロード
```

---The following area is a Code cell (cell numver is 60)---
```python
# LoRAアダプターをモデルにロード
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # GPU 0のモデルにLoRAアダプターをロード
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # GPU 1のモデルにLoRAアダプターをロード
```

---The following area is a Markdown cell (cell numver is 61)---
```markdown
## 推論
```

---The following area is a Code cell (cell numver is 62)---
```python
# 推論を行う関数を定義
@torch.no_grad()  # 勾配計算を無効にするデコレーター
@torch.cuda.amp.autocast()  # 自動混合精度を有効にするデコレーター
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝利確率を格納するリストを初期化
    
    # データをバッチ処理で推論
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # 現在のバッチの終わりのインデックスを計算
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換
        
        # パディングを適用してテンソルを準備
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最も長いシーケンスに合わせてパディング
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchテンソルとして返す
        )
        
        # モデルによる推論を実行
        outputs = model(**inputs.to(device))  # 入力を指定したデバイスに移動
        proba = outputs.logits.softmax(-1).cpu()  # ロジットにソフトマックス関数を適用して確率を計算
        
        # 各勝利確率をリストに追加
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝利確率を追加
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝利確率を追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加
    
    # データフレームに結果を追加
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df  # 結果が追加されたデータフレームを返す
```

---The following area is a Code cell (cell numver is 63)---
```python
# 計測開始時刻を記録
st = time.time()

# 入力の長さでソートして動的パディングを最大限活用
data = data.sort_values("length", ascending=False)
# サブセット1とサブセット2のトークン数がほぼ同じになるようにする
sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピー
sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピー

# スレッドプールを使用して同時に推論を実行
with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列処理を実行

# 結果をデータフレームとして結合
result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 各モデルの勝利確率を取得

# 経過時間を表示
print(f"経過時間: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 64)---
```python
# 計測開始時刻を記録
st = time.time()

# テストタイムデータ拡張（TTA）が有効な場合
if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # 入力の長さでソートして速度を向上
    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピー
    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピー

    # スレッドプールを使用して同時に推論を実行
    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列処理を実行

    tta_result_df = pd.concat(list(results), axis=0)  # TTAの結果をデータフレームとして結合
    # TTAの順序を反転する
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values  
    # 元の結果とTTA結果を平均化
    proba = (proba + tta_proba) / 2  # 平均を取る

# 経過時間を表示
print(f"経過時間: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 65)---
```python
# データフレームに勝利確率を設定
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝利確率を追加
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝利確率を追加
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの確率を追加
```

---The following area is a Code cell (cell numver is 66)---
```python
# 最終的な結果データフレームを表示
result_df  # 予測結果を含むデータフレームを出力
```

---The following area is a Code cell (cell numver is 67)---
```python
# 予測結果データフレームを表示
out  # 最初の出力結果を含むデータフレームを出力
```

---The following area is a Code cell (cell numver is 68)---
```python
# 予測結果データフレームを結合
merged_df = pd.merge(out, result_df, on='id')  # ID列を基にoutデータフレームとresult_dfを結合
```

---The following area is a Code cell (cell numver is 69)---
```python
# 結合したデータフレームを表示
merged_df  # 結合結果を含むデータフレームを出力
```

---The following area is a Code cell (cell numver is 70)---
```python
# モデルA、モデルB、および引き分けの勝利確率を加重平均して計算
merged_df['winner_model_a'] = (0.7 * merged_df['winner_model_a_y']) + (0.3 * merged_df['winner_model_a_x'])  # モデルAの加重平均
merged_df['winner_model_b'] = (0.7 * merged_df['winner_model_b_y']) + (0.3 * merged_df['winner_model_b_x'])  # モデルBの加重平均
merged_df['winner_tie'] = (0.7 * merged_df['winner_tie_y']) + (0.3 * merged_df['winner_tie_x'])  # 引き分けの加重平均
```

---The following area is a Code cell (cell numver is 71)---
```python
# 提出用データフレームを作成
submission_df = merged_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 必要な列を選択

# CSVファイルとして保存
submission_df.to_csv('submission.csv', index=False)  # インデックスなしでCSVに保存

# 提出データフレームを表示
display(submission_df)  # 提出用データを表示
```

---The following area is a Markdown cell (cell numver is 72)---
```markdown
クレジット: [https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)
```

---The following area is a Markdown cell (cell numver is 73)---
```markdown
---

# コメント

> ## Kareem Abdelhamed
> 
> いいモデリングですね
> 
> 

---

> ## SiddhVRTopic 作者
> 
> このノートブックの提出がなぜ失敗し続けるのか診断を手伝ってくれる人はいますか？
> 
> ありがとうございます。
> 
> 
> > ## XXX
> > 
> > こんにちは [@siddhvr](https://www.kaggle.com/siddhvr)、競技はあなたのコードを再実行し、隠れたデータセットはより大きい場合があります。提出がメモリ不足により完了できなかったのかもしれません。（もちろん、これは私の推測に過ぎません😉）
> > 
> > 
> > 
> > > ## SiddhVRTopic 作者
> > > > その場合、通常のプロンプトは「ノートブックがメモリを使い果たしました」となりますが、私の場合は「ノートブックが例外を投げました」と表示されています。
> > > > 

---

> ## Luciango
> 
> 自分のファインチューニングしたGemma2チェックポイントを変更した際にOOM（Out of Memory）結果が出たのですが、アドバイスはありますか？
> 
> 

---

> ## Vitalii Bozheniuk
> 
> ひとつのGPUでLLAMAを立ち上げ、別のGPUでGemmaを立ち上げたほうが簡単ではありませんか？
> 
> 
> > ## Valentin Werner
> > 
> > おっしゃる通りです。このアプローチでは、各モデルを一度だけロードするため、時間の短縮が期待できます。それ以外はあまり違いはありません。このコードはおそらく、元の推論ノートブックが1種類のモデルのためのものであったためこのようになっています。調整するよりも、単純にコピーするほうが簡単です。
> > 
> > 
> > 
---
```

** @@@ Jupyter Notebook numver 18, the number of votes :12 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、LMSYS - Chatbot Arenaコンペティションにおいて、ユーザーの好みを予測するための機械学習モデルのトレーニングおよび推論に取り組んでいます。特に、KerasとTensorFlowを使用して、Gemma Causal Language Modelを基にしたモデルを構築しています。

### 主な問題
ノートブックの目的は、ユーザーからの応答データに基づいて、どのチャットボットの応答が好まれるかを予測するモデルを訓練することです。このモデルは、モデルのトークン化やデータ前処理を行い、各種ハイパーパラメータによる最適化を実施します。

### 使用している手法とライブラリ
1. **KerasとTensorFlow**:
   - Kerasの分散APIを利用して、TPUにモデルをデプロイし、メモリ断片化を防ぐために設定を行います。
   - JAXをバックエンドに使用し、TPU上でのトレーニングと推論を高速化します。

2. **モデル構築**:
   - Gemma Causal Language Modelをロードし、そのバックボーンを利用して、ユーザーの入力とレスポンスを処理するモデルを構築します。
   - 出力層にはソフトマックス関数を適用し、多クラス分類を行います。

3. **データ処理**:
   - CSVファイルからデータを読み込み、必要な列を抽出してサロゲートペアを除去する関数を定義します。
   - TensorFlowのデータセットAPIを使用して、バッチ化と前処理を効率的に行います。

4. **ハイパーパラメータの設定**:
   - AdamWオプティマイザーを使用し、学習率や重み減衰を調整することでモデルのトレーニング性能を向上させます。

5. **モデルのトレーニングと推論**:
   - 訓練データに対してモデルをコンパイルし、予測を行います。最終的な予測結果はCSV形式で保存され、提出用にフォーマットされます。

### コメントと次のステップ
ノートブックでは、タイムアウト問題や推論速度の向上についても言及されており、今後の改善が期待されています。また、他の参加者からのフィードバックや提案も含まれており、相互学習や情報共有の場が形成されています。

全体として、このノートブックは、自然言語処理タスクにおける現代的なアプローチを採用しており、効率的なモデルの構築とデータ処理の手法を示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
float16での推論には???時間かかるはずです。

トレーニングノートブック:

https://www.kaggle.com/code/pranshubahadur/tf-gemma-2-9b-lmsys-training-tpu
```

---The following area is a Code cell (cell numver is 2)---
```python
import os

# Keras 3の分散APIは、現時点ではJAXバックエンドのみに実装されています
os.environ["KERAS_BACKEND"] = "jax"
# TPUのメモリ断片化と割り当てのオーバーヘッドを最小限に抑えるために、すべてのTPUメモリを事前に割り当てます。
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
import keras
import keras_nlp

# (1, 2)の形状を持つデバイスメッシュを作成し、すべての8つのTPUに重みを分散させます。
device_mesh = keras.distribution.DeviceMesh(
    (1, 2),  # バッチとモデルのための次元
    ["batch", "model"],
    devices=['gpu:0', 'gpu:1'],  # 使用するGPUデバイスのリスト
)

model_dim = "model"  # モデルの次元を指定

layout_map = keras.distribution.LayoutMap(device_mesh)

# 'token_embedding/embeddings'に一致する重みが8つのTPUに分散されます
layout_map["token_embedding/embeddings"] = (model_dim, None)  # トークン埋め込みのマッピング
layout_map["position_embedding/embeddings"] = (model_dim, None)  # 位置埋め込みのマッピング

# 注意層のクエリ、キー、値行列に対してマッチする正規表現
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)

layout_map["decoder_block.*layer_norm/scale"] = (model_dim,)  # スケールのマッピング
layout_map["decoder_block.*layer_norm/bias"] = (model_dim,)  # バイアスのマッピング

# モデルの分散設定を行います
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,
    batch_dim_name="batch",  # バッチの次元名を指定
)

# モデルの分散をセットします
keras.distribution.set_distribution(model_parallel)
```

---The following area is a Code cell (cell numver is 3)---
```python
import jax

# JAXのデフォルトデバイスをCPUに設定します
jax.default_device = jax.devices('cpu')[0]

# 現在のデバイス情報を表示します
jax.devices()  # これにより、使用可能なデバイスのリストが返されます。
```

---The following area is a Code cell (cell numver is 4)---
```python
# Kerasのデータ型ポリシーをfloat16に設定します
keras.config.set_dtype_policy("float16")  # これにより、モデルのトレーニングや推論で使用されるデータの型がfloat16になります。
```

---The following area is a Code cell (cell numver is 5)---
```python
# このセルにはコードが含まれていません。必要な処理やデータをここに追加してください。
```

---The following area is a Code cell (cell numver is 6)---
```python
# Gemma Causal Language Model を事前設定からロードします
# trainable=False でモデルを読み込むと、重みが更新されない設定になります。
# dtype='int8' でデータ型を int8 に設定します。
gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma2/keras/gemma2_instruct_9b_en/1", trainable=False, dtype='int8')

# モデルの概要を表示します
gemma_lm.summary()  # これにより、モデルの構造やパラメータの数が表示されます。
```

---The following area is a Code cell (cell numver is 7)---
```python
# サロゲートペア（代理対）を削除する関数を定義します
def remove_surrogates(text):
    # サロゲートペアの範囲 (0xD800 から 0xDFFF) に含まれない文字をフィルタリングして結合します
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))  # テキストからサロゲートペアを取り除いた結果を返します。
```

---The following area is a Code cell (cell numver is 8)---
```python
from pandas import read_csv, DataFrame

# 使用する入力列の名前を指定
input_columns = ['prompt', 'response_a', 'response_b']
# ラベル列の名前を指定
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

# CSVファイルから生のテストデータセットを読み込みます
raw_test_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# 以下はコメントアウトされているコードです
# raw_test_dataset[input_columns] = raw_test_dataset[input_columns].map(lambda x: eval(x)[0])  # 各列の中身を評価して最初の要素を取得する
# raw_test_dataset = raw_test_dataset.dropna().reset_index(drop=True)  # NaNを含む行を削除し、インデックスをリセットします

# トレーニングデータセットを作成します
train_dataset = DataFrame({
    # 入力列を結合し、サロゲートペアを削除して新しいカラム 'text' を作成します
    'text' : raw_test_dataset[input_columns].agg('\n\nRESPONSE:\n\n'.join, axis=1)  # 各行の入力を2つのレスポンスで結合
                .apply(lambda x: '\n\nPROMPT\n\n' + x)  # 各行の先頭に極末文を追加
                .apply(lambda x: remove_surrogates(x)),  # サロゲートペアを削除
})
```

---The following area is a Code cell (cell numver is 9)---
```python
# Gemmaモデルからトークナイザーとバックボーンを取得します
tokenizer = gemma_lm._preprocessor  # モデルの事前処理（トークナイザー）を取得
backbone = gemma_lm.backbone  # モデルのバックボーン（基盤部分）を取得
```

---The following area is a Code cell (cell numver is 10)---
```python
# テキストとオプションのラベルを前処理する関数を定義します
def preprocess_fn(text, label=None):
    # テキストをトークナイザーを使って処理し、トークンIDとパディングマスクを取得します
    preprocessed = tokenizer(text, sequence_length=512)[0]  # シーケンスの長さは512に制限します
    print(preprocessed)  # 前処理された結果を表示します

    # 前処理関数が必要な入力のみを返すことを確認します
    return {'token_ids': preprocessed['token_ids'], 'padding_mask': preprocessed['padding_mask']}  # トークンIDとパディングマスクを含む辞書を返します
```

---The following area is a Code cell (cell numver is 11)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

# モデルの入力を定義します
inputs = {
        "token_ids": keras.Input(shape=(512,), dtype=tf.int32, name="token_ids"),  # トークンIDの入力
        "padding_mask": keras.Input(shape=(512,), dtype=tf.int32, name="padding_mask"),  # パディングマスクの入力
    }

# バックボーンモデルを通じて入力を処理します
x = backbone(inputs)  # バックボーンに入力を渡します
print(x.shape)  # 出力の形状を表示します

# グローバル平均プーリングを適用します
x = GlobalAveragePooling1D()(x)  # 1次元のデータに対してグローバル平均プーリングを行います
print(x.shape)  # プーリング後の出力の形状を表示します

# 出力層を追加します
outputs = Dense(3, 'softmax')(x)  # 3つのクラスに対してソフトマックス関数を使用する出力層を追加します

# モデルを定義します
model = Model(inputs, outputs)  # 入力と出力からモデルを構築します
```

---The following area is a Code cell (cell numver is 12)---
```python
# AdamWオプティマイザーを定義します
optimizer = keras.optimizers.AdamW(
                    learning_rate=5e-5,  # 学習率を5e-5に設定します
                    weight_decay=0.01,  # 重み減衰を0.01に設定します
                )

# バイアスとスケールの変数を重み減衰の対象から除外します
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])  # これにより、指定された変数は重み減衰が適用されません
```

---The following area is a Code cell (cell numver is 13)---
```python
# モデルをコンパイルします
model.compile(optimizer,  # 先ほど定義したオプティマイザーを使用
              loss=tf.keras.losses.CategoricalCrossentropy(),  # 多クラスのクロスエントロピー損失を使用
             )  # これにより、指定したオプティマイザーと損失関数でモデルが準備されます
```

---The following area is a Code cell (cell numver is 14)---
```python
# モデルの3番目のレイヤーにLoRAの重みをロードします
model.layers[2].load_lora_weights("/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/model.lora.h5")  # 指定されたファイルからLoRAの重みを読み込みます
```

---The following area is a Code cell (cell numver is 15)---
```python
import numpy as np

# 保存された重みとバイアスを読み込みます
dense_1_weights = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_kernel.npy')  # 重みを読み込み
dense_1_biases = np.load('/kaggle/input/tf-gemma-2-9b-lmsys-training-tpu/dense_1_bias.npy')  # バイアスを読み込み

# 読み込んだ重みとバイアスを組み合わせてリストを作成します
dense_1_combined = [dense_1_weights, dense_1_biases]

# モデルの最終レイヤーに重みを設定します
model.layers[-1].set_weights(dense_1_combined)  # 最後のレイヤーに対して重みを設定します
```

---The following area is a Code cell (cell numver is 16)---
```python
# モデル内のすべてのレイヤーをトレーニング不可に設定します
for layer in model.layers:
    layer.trainable = False  # 各レイヤーのトレーニング可能フラグをFalseに設定し、重みが更新されないようにします
```

---The following area is a Code cell (cell numver is 17)---
```python
# モデルの概要を表示します
model.summary()  # モデルのアーキテクチャ、レイヤーの数、パラメータ数などの情報を表示します
```

---The following area is a Code cell (cell numver is 18)---
```python
# テンソルスライスからデータセットを作成し、前処理関数を適用してバッチ化します
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values))  # トレーニングデータのテキストをスライスしてデータセットを作成
ds = ds.map(preprocess_fn)  # 前処理関数を適用します
ds = ds.batch(16)  # バッチサイズを16に設定してデータをバッチ化します
```

---The following area is a Code cell (cell numver is 19)---
```python
from tqdm import tqdm

# 予測結果を保存するリストを初期化します
preds = []

# データセットに対して予測を行います
for inputs in tqdm(ds):  # 進行状況を表示しながらデータセットをイテレート
    keras.backend.clear_session(free_memory=True)  # セッションをクリアしてメモリを解放します
    preds.append(model(inputs))  # モデルを使用して入力データから予測を行い、結果をリストに追加します
    keras.backend.clear_session()  # 再びセッションをクリアしてメモリを解放します
```

---The following area is a Code cell (cell numver is 20)---
```python
import numpy as np

# 予測結果を連結します
results = np.concatenate(preds)  # リストに格納された予測結果を一つの配列にまとめます
```

---The following area is a Code cell (cell numver is 21)---
```python
import pandas

# 予測結果をDataFrameに変換します
submission = pandas.DataFrame(data=results, index=raw_test_dataset.id, columns=label_columns)  # 結果をDataFrameに格納し、インデックスをテストデータセットのIDに設定します
```

---The following area is a Code cell (cell numver is 22)---
```python
# 提出用ファイルをCSV形式で保存します
submission.to_csv('submission.csv', index=False)  # インデックスを含めずに'submission.csv'として保存します
```

---The following area is a Code cell (cell numver is 23)---
```python
# 提出用DataFrameの最初の5行を表示します
submission.head()  # DataFrameの冒頭の内容を確認して、予測結果が正しく保存されているか確認します
```

---The following area is a Code cell (cell numver is 24)---
```python
# このセルにはコードが含まれていません。必要な処理やデータをここに追加してください。
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
---

# コメント

> ## ano
> 
> ノートブックのバージョン18を2回提出しました。どちらもタイムアウトによるエラー提出となりました。コードを改善してスピードを上げる必要があります。とにかく、素晴らしいスタートですね！
> 
> 
> 
> > ## Pranshu Bahadur（トピック著者）
> > 
> > そうですね…バージョン19がタイムアウトしないことを願っています。スピードアップのために何か提案はありますか？
> > 
> > 
> > 
> > > ## ano
> > > 
> > > 一番最初に思い浮かんだのは、[このノートブック](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora?scriptVersionId=187740026)のような並列計算です。cuda 0のモデルとcuda 1のモデルを使い、スレッドプールを使って同時に推論します。しかし、100%確実とは言えません。コードを書くことはできますが、正直言って私はLLMやkeras_nlpなどに新しいです…
> > > 
> > > 
> > > 
> > > ## Pranshu Bahadur（トピック著者）
> > > 
> > > スレッドプールに関して調べてみます。何らかの形で機能するかもしれません。正直、私もLLMについてはとても新しいですが、実装を通じて学んでいます！
> > > 
> > > 

---

> ## carvingfate
> 
> 素晴らしい仕事です！Gemma-2は最近のバージョンのトランスフォーマーが必要で、これが原因で古いクラウドGPUプラットフォームでのトレーニング中に多くの問題が発生しました。コードを共有してくれてありがとう。
> 
> 

---

> ## Lorry Zou
> 
> 今日は土曜日です。提出の成功はありましたか？
> 
> 
> 

---
```

** @@@ Jupyter Notebook numver 19, the number of votes :9 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、LMSYS - Chatbot Arenaコンペティションにおける人間の好み予測の問題に取り組んでいます。具体的には、異なるバージョンのQwen2モデル（特に1.5bバージョン）を使用して、与えられたプロンプトに対する応答の好みを予測するためのトレーニングと推論を実行しています。ノートブックは、チャットボットの応答の選好を予測するモデルの性能向上を目的としています。

主な手法として、以下の手法とライブラリを使用しています：

1. **モデルとトークナイザー**: `transformers`ライブラリの`Qwen2ForSequenceClassification`と`AutoTokenizer`を利用し、Qwen2モデルを使用している。事前トレーニング済みのモデルを用いて、シーケンス分類タスクを実行します。

2. **トレーニング戦略**: `PeftModel`を使用して、パラメータ効率の良いファインチューニング（PEFT）を行う。特にLoRA（Low-Rank Adaptation）に基づくアプローチを使用し、モデルの特定の部分に対しての適応を行います。

3. **データの前処理**: ''sprompt''、''response_a''、''response_b''の各列に対してテキスト処理を行い、トークン化を実施しています。トークン化の際には、プロンプトや応答に対して特定の形式の前処理（接頭辞の追加）を行います。

4. **推論手法**: 並列処理を使用して、2つのモデルによって生成された応答に対する勝率を予測します。`ThreadPoolExecutor`を用いて、同時に複数のモデルでの推論を効率化しています。

5. **評価指標**: モデルの評価には対数損失および精度を使用し、これらの指標を用いてトレーニングと評価の過程でモデルの性能を評価します。

このノートブックは、最終的にテストデータに対する予測結果を保存したCSVファイル（"submission.csv"）を生成します。全体として、複数のモデルを活用して、与えられたプロンプトに対する応答の優劣を予測する精度を高めるためのフレームワークが整備されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# はじめに

このノートブックは[こちら](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)からフォークされたものです。

Qwen2は、異なるモデルサイズのデコーダー言語モデルを含む言語モデルシリーズです。各サイズごとに、基本の言語モデルと整合されたチャットモデルをリリースしています。これは、SwiGLU活性化、注意のQKVバイアス、グループクエリアテンション、スライディングウィンドウアテンションとフルアテンションの組み合わせなどに基づくトランスフォーマーアーキテクチャに基づいています。さらに、複数の自然言語やコードに適応した改善されたトークナイザーも搭載しています。

このノートブックでは、バッチサイズ4、1エポックでQwen2 1.5bバージョンのトレーニングを行っています。トレーニング時間はA100で約1時間です。Qwen2 7bはこのタスクに対してより良いパフォーマンスを発揮することが期待されます。トレーニング用のコードは、ノートブックの最後に添付されています。
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
# transformers, peft, accelerate, bitsandbytesのライブラリをインストールします。
# -Uオプションはアップグレードを意味し,
# --no-indexはPyPIからではなくローカルのリンクを使用することを指定します。
# --find-linksは指定したディレクトリ（この場合は/kaggle/input/lmsys-wheel-files）からパッケージを探します。
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn

import numpy as np
import pandas as pd
from transformers import Qwen2ForSequenceClassification, AutoTokenizer, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel

# 必要なライブラリをインポートします。
# - time: 時間関連の関数を提供します。
# - dataclass: データクラスを作成するためのデコレーターです。
# - ThreadPoolExecutor: スレッドプールを使って非同期処理を実行するためのクラスです。
# - torch: PyTorchライブラリで、深層学習を行うための基本的なツールです。
# - sklearn: 機械学習用のライブラリです。
# - numpy: 数値計算ライブラリで、配列操作に使います。
# - pandas: データ分析のためのライブラリで、データフレームを扱います。
# - transformers: Hugging Faceのトランスフォーマーモデルに関連するライブラリです。
# - PeftModel: PEFT (Parameter-Efficient Fine-Tuning) に関連するモデルです。
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2
# GPUの数が2であることを確認するためのアサーションです。
# torch.cuda.device_count()は使用可能なGPUの数を返します。
# この条件が満たされない場合、エラーが発生します。
# つまり、2つのGPUが必要な環境でこのコードを実行することを示しています。
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    model_dir = '/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1'  # モデルの保存先ディレクトリ
    lora_dir = '/kaggle/input/lmsys-qwen2-1-5b-checkpoint/checkpoint-5748'  # LoRAチェックポイントの保存先ディレクトリ
    max_length = 2048  # 入力の最大長
    batch_size = 4  # バッチサイズ
    device = torch.device("cuda")  # 使用するデバイスをCUDA（GPU）に設定
    tta = False  # テスト時の拡張を有効にするかどうか。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結された入力にmax_lengthを適用するか

cfg = Config()  # 設定を初期化するためのConfigクラスのインスタンスを作成
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データのロードと前処理
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  
# テストデータセットをCSVファイルから読み込みます。
# pandasのread_csv関数を使用して、指定されたパスのテストデータをDataFrameとして読み取ります。
# このDataFrameには、後のモデル推論で使用するデータが含まれています。
```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))  # 文字列を評価して、nullを空文字に置き換えた後、空白で結合します。

# データフレーム内の'sprompt'、'response_a'、'response_b'列のテキストを前処理します。
test.loc[:, 'prompt'] = test['prompt'].apply(process_text)  # 各プロンプトにprocess_text関数を適用して前処理を行う
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)  # 応答Aに対しても同様に前処理
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)  # 応答Bに対しても前処理

display(test.head(5))  # 前処理したデータの最初の5行を表示します。
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トークナイズ（トークン化）
```

---The following area is a Code cell (cell numver is 11)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # プロンプトと応答をトークン化します。
    prompt = ["<User prompt>: " + p for p in prompt]  # 各プロンプトの先頭に"<User prompt>: "を追加
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]  # 応答Aの先頭に改行と"<response_a>: "を追加
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]  # 応答Bの先頭に改行と"<response_b>: "を追加
    
    if spread_max_length:
        # 各入力（プロンプト、応答A、応答B）に対してmax_lengthの1/3を適用
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        # トークンを結合し、入力IDを作成
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        attention_mask = [[1]* len(i) for i in input_ids]  # 各トークンに対する注意マスクを作成
    else:
        # プロンプト、応答A、応答Bを結合してトークン化
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークン化
        input_ids = tokenized.input_ids  # トークンのIDを取得
        attention_mask = tokenized.attention_mask  # 注意マスクを取得
    
    return input_ids, attention_mask  # トークンのIDと注意マスクを返す
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(cfg.model_dir)  # 事前学習済みのトークナイザーをモデルディレクトリから読み込みます。
tokenizer.add_eos_token = True  # 終了トークンを追加します。
tokenizer.padding_side = "right"  # パディングを右側に設定します。

data = pd.DataFrame()  # 新しいデータフレームを作成します。
data["id"] = test["id"]  # テストデータのID列をコピーします。
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークン化を実行し、入力IDと注意マスクを取得します。
data["length"] = data["input_ids"].apply(len)  # 各入力IDの長さを計算して新しい列に保存します。

aug_data = pd.DataFrame()  # 拡張データ用の新しいデータフレームを作成します。
aug_data["id"] = test["id"]  # テストデータのID列をコピーします。
# 応答Aと応答Bを入れ替えます。
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])  # 応答を入れ替えてトークン化を実行します。
aug_data["length"] = aug_data["input_ids"].apply(len)  # 各入力IDの長さを計算して新しい列に保存します。
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(data["input_ids"][0]))  
# データフレームの最初の入力IDをデコードして、元のテキストを表示します。
# tokenizer.decode関数を使用して、トークンIDからそのテキスト表現を取得します。
```

---The following area is a Code cell (cell numver is 14)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  
# 拡張データフレームの最初の入力IDをデコードして、元のテキストを表示します。
# tokenizer.decode関数を使用して、トークンIDからそのテキスト表現を取得します。
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# モデルのロード
```

---The following area is a Code cell (cell numver is 16)---
```python
# GPU 0にベースモデルをロードします
device_0 = torch.device('cuda:0')  # GPU 0をデバイスとして指定します。
model_0 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,  # 設定されたモデルディレクトリから事前学習済みモデルを読み込みます。
    num_labels=3,  # 分類するラベルの数を指定します。
    device_map=device_0,  # モデルをGPU 0にマッピングします。
    use_cache=False,  # キャッシュを使用しない設定です。
)

# GPU 1にベースモデルをロードします
device_1 = torch.device('cuda:1')  # GPU 1をデバイスとして指定します。
model_1 = Qwen2ForSequenceClassification.from_pretrained(
    cfg.model_dir,  # 設定されたモデルディレクトリから事前学習済みモデルを読み込みます。
    num_labels=3,  # 分類するラベルの数を指定します。
    device_map=device_1,  # モデルをGPU 1にマッピングします。
    use_cache=False,  # キャッシュを使用しない設定です。
)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
#### LoRAアダプターのロード
```

---The following area is a Code cell (cell numver is 18)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0にロードします。
model_0.config.pad_token_id = model_0.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。

model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1にロードします。
model_1.config.pad_token_id = model_1.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()  # 勾配計算を無効にします。推論時に計算を節約できます。
@torch.cuda.amp.autocast()  # 自動混合精度を使用して、計算の効率を向上させます。
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # モデルAの勝ち、モデルBの勝ち、引き分けの結果を格納するリストを初期化します。
    
    for start_idx in range(0, len(df), batch_size):  # データフレームをバッチサイズごとに処理します。
        end_idx = min(start_idx + batch_size, len(df))  # 現在のバッチの終了インデックスを計算します。
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得します。
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換します。
        attention_mask = tmp["attention_mask"].to_list()  # 注意マスクをリストに変換します。
        
        # トークンのパディングを行います。注意警告を表示しないようにします。
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最も長い入力に合わせてパディングします。
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchのテンソルとして返します。
        )
        
        # モデルによる出力を取得します。
        outputs = model(**inputs.to(device))
        proba = outputs.logits.softmax(-1).cpu()  # ロジットからソフトマックスを計算し、CPUに移動します。
        
        # 各モデルの勝率をリストに追加します。
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率
    
    # データフレームに結果を追加します。
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df  # 更新されたデータフレームを返します。
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()  # 処理開始時間を記録します。

# 入力の長さに基づいてソートし、動的パディングを最大限に活用します。
data = data.sort_values("length", ascending=False)  
# total #tokens in sub_1 and sub_2はほぼ同じである必要があります。
sub_1 = data.iloc[0::2].copy()  # データを偶数行で取得してサブセット1を作成します。
sub_2 = data.iloc[1::2].copy()  # データを奇数行で取得してサブセット2を作成します。

# ThreadPoolExecutorを使用して、モデル0とモデル1の推論を並列で実行します。
with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 推論を実行

result_df = pd.concat(list(results), axis=0)  # 結果をデータフレームに結合します。
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝率のデータを抽出します。

print(f"elapsed time: {time.time() - st}")  # 経過時間を表示します。
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()  # 処理開始時間を記録します。

if cfg.tta:  # テスト時の拡張が有効な場合
    data = aug_data.sort_values("length", ascending=False)  # 入力の長さに基づいてソートして速度を向上させます。
    sub_1 = data.iloc[0::2].copy()  # データを偶数行で取得してサブセット1を作成します。
    sub_2 = data.iloc[1::2].copy()  # データを奇数行で取得してサブセット2を作成します。

    # ThreadPoolExecutorを使用して、モデル0とモデル1の推論を並列で実行します。
    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 推論を実行

    tta_result_df = pd.concat(list(results), axis=0)  # 結果をデータフレームに結合します。
    # TTAの順序は反転しています。
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values  # TTAの結果を取得します。
    # 元の結果とTTA結果の平均を取ります。
    proba = (proba + tta_proba) / 2  # 勝率の平均を計算します。

print(f"elapsed time: {time.time() - st}")  # 経過時間を表示します。
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # 勝率のデータを結果データフレームに更新します。
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を更新します。
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの勝率を更新します。
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成します。
submission_df.to_csv('submission.csv', index=False)  # 提出ファイルをCSV形式で保存します。
display(submission_df)  # 提出データフレームを表示します。
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# トレーニングコード

このトレーニングコードは、https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning からフォークされ、インスパイアを受けています。

## Kaggleデータの転送
```

---The following area is a Code cell (cell numver is 25)---
```python
# # 重要: このセルを実行してKaggleデータソースを正しい位置（/kaggle/input）にインポートします。
# # その後、このセルは削除しても問題ありません。
# # 注意: このノートブック環境はKaggleのPython環境とは異なるため、
# # お使いのノートブックで使用されるライブラリが欠落している可能性があります。

# import os
# import sys
# from tempfile import NamedTemporaryFile
# from urllib.request import urlopen
# from urllib.parse import unquote, urlparse
# from urllib.error import HTTPError
# from zipfile import ZipFile
# import tarfile
# import shutil

# CHUNK_SIZE = 40960
# DATA_SOURCE_MAPPING = 'lmsys-chatbot-arena:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F66631%2F8346466%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4ae2cd10c5a4750deca45551519904c5858980d5cf8cd8ade09b2299d926c86c895b50ba333acf0db5210d0dd29197c9a9c5a525c8afd0186b88f17d3ca756f0562ad5acfa2e856e8b159f554e61f72102865abd60add751dd59bed5126536d977d6fe54d2e85f8e5baa8d3d75337d0a222a89f0f30fa6dd7c360e4a192363dc417e9e4a9c9c23368991db65b4994c2200bee494d8d5e2684f754ab1b1a511f7db3652e01ab658b04d26cc1321e783fa5509f67d4c438808adc7932a0e79a21849375023b36e90cbe288cf68a6185b2ce950464b71c9b6133d49769c67e77a5298809fb63da23c0655165e80661623bd9bb908bc9a486dbc9e09caebf2a01392,qwen2/transformers/qwen2-1.5b-instruct/1:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-models-data%2F52038%2F62308%2Fbundle%2Farchive.tar.gz%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240716%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240716T010448Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D80f6a1f964073d960129f611693afca0666128c1a319b7ab93769a1993d5910e4995cf3c6f2f87323452999b069eafb9f7b01be97dbb80a3441cd0b4871d9d35379750a3b5614397253624b097961c886c48df889ac7b1100231715e2b60bf4f3ffccd5e7fc68b3d7d0668f350a8fefbddb90275770e75aaa7f74fae68b2f5314f610ec2f1abf0436156e9426e6173e229172ca0c4ee91eb2d768de3190c9f07e6c28b73bc8c5553e2dac6320842103524591b663021a41801bb2274b5fd91dd62f174ba8976c74995012ad3ed34ecf554a9e2cb08f91813e9cacc9b8d554c6a7a037414635f30e506ea39f63fb4db01f5cf3322dca02097f9550b1a5454ae99'

# KAGGLE_INPUT_PATH='/kaggle/input'
# KAGGLE_WORKING_PATH='/kaggle/working'
# KAGGLE_SYMLINK='kaggle'

# !umount /kaggle/input/ 2> /dev/null
# shutil.rmtree('/kaggle/input', ignore_errors=True)
# os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)
# os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)

# try:
#   os.symlink(KAGGLE_INPUT_PATH, os.path.join("..", 'input'), target_is_directory=True)
# except FileExistsError:
#   pass
# try:
#   os.symlink(KAGGLE_WORKING_PATH, os.path.join("..", 'working'), target_is_directory=True)
# except FileExistsError:
#   pass

# for data_source_mapping in DATA_SOURCE_MAPPING.split(','):
#     directory, download_url_encoded = data_source_mapping.split(':')
#     download_url = unquote(download_url_encoded)
#     filename = urlparse(download_url).path
#     destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)
#     try:
#         with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:
#             total_length = fileres.headers['content-length']
#             print(f'Downloading {directory}, {total_length} bytes compressed')
#             dl = 0
#             data = fileres.read(CHUNK_SIZE)
#             while len(data) > 0:
#                 dl += len(data)
#                 tfile.write(data)
#                 done = int(50 * dl / int(total_length))
#                 sys.stdout.write(f"\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded")
#                 sys.stdout.flush()
#                 data = fileres.read(CHUNK_SIZE)
#             if filename.endswith('.zip'):
#               with ZipFile(tfile) as zfile:
#                 zfile.extractall(destination_path)
#             else:
#               with tarfile.open(tfile.name) as tarfile:
#                 tarfile.extractall(destination_path)
#             print(f'\nDownloaded and uncompressed: {directory}')
#     except HTTPError as e:
#         print(f'Failed to load (likely expired) {download_url} to path {destination_path}')
#         continue
#     except OSError as e:
#         print(f'Failed to load {download_url} to path {destination_path}')
#         continue

# print('データソースのインポートが完了しました。')
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## ライブラリのインストールとロード
```

---The following area is a Code cell (cell numver is 27)---
```python
## gemma-2はtransformers>=4.42.3から利用可能です。
# !pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft datasets
# transformersライブラリのバージョンを4.42.3以上にアップグレードし、その他の必要なライブラリ（bitsandbytes、accelerate、peft、datasets）をインストールします。
```

---The following area is a Code cell (cell numver is 28)---
```python
# import os  # オペレーティングシステムの機能を提供するライブラリをインポート
# import copy  # オブジェクトのコピーを作成するためのライブラリをインポート
# from dataclasses import dataclass  # データクラスの作成に使用するデコレーターをインポート

# import numpy as np  # 数値計算ライブラリをインポート
# import torch  # 深層学習ライブラリPyTorchをインポート
# from datasets import Dataset  # データセットの扱いに使用するクラスをインポート
# from transformers import (  # Transformersライブラリから必要なクラスをインポート
#     BitsAndBytesConfig,
#     AutoTokenizer,
#     Qwen2ForSequenceClassification,
#     PreTrainedTokenizerBase,
#     EvalPrediction,
#     Trainer,
#     TrainingArguments,
#     DataCollatorWithPadding,
# )
# from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTに関連するクラスをインポート
# from sklearn.metrics import log_loss, accuracy_score  # 機械学習の評価指標をインポート
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
## トレーニング設定
```

---The following area is a Code cell (cell numver is 30)---
```python
# @dataclass
# class Config:
#     output_dir: str = "output"  # 出力先ディレクトリの指定
#     model_name: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1"  # 使用するモデルのパス
#     checkpoint: str = "/kaggle/input/qwen2/transformers/qwen2-1.5b-instruct/1"  # Qwenモデルのローカルディレクトリ
#     max_length: int = 2048  # 入力の最大長
#     n_splits: int = 5  # クロスバリデーションの分割数
#     fold_idx: int = 0  # 現在のフォールドインデックス
#     optim_type: str = "adamw_8bit"  # オプティマイザーの種類
#     per_device_train_batch_size: int = 4  # デバイスごとのトレーニングバッチサイズ
#     gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8
#     per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ
#     n_epochs: int = 1  # エポック数
#     freeze_layers: int = 16  # 最初の16層にはアダプターを追加しない
#     lr: float = 2e-4  # 学習率
#     warmup_steps: int = 20  # ウォームアップステップ数
#     lora_r: int = 16  # LoRAのR値
#     lora_alpha: float = lora_r * 2  # LoRAのアルファ値
#     lora_dropout: float = 0.05  # LoRAのドロップアウト率
#     lora_bias: str = "none"  # LoRAのバイアス設定

# config = Config()  # 設定のインスタンスを作成
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
## トレーニング引数
```

---The following area is a Code cell (cell numver is 32)---
```python
# training_args = TrainingArguments(
#     output_dir="output",  # 出力先ディレクトリの指定
#     overwrite_output_dir=True,  # 出力ディレクトリを上書きするかどうか
#     report_to="none",  # ログの報告先（ここでは無効化）
#     num_train_epochs=config.n_epochs,  # トレーニングエポック数
#     per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ
#     gradient_accumulation_steps=config.gradient_accumulation_steps,  # 勾配累積のステップ数
#     per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ
#     logging_steps=10,  # ロギングの頻度
#     eval_strategy="epoch",  # 評価戦略（エポックごとに評価）
#     save_strategy="steps",  # モデル保存戦略（ステップごとに保存）
#     save_steps=200,  # モデルを保存するステップ数
#     optim=config.optim_type,  # 使用するオプティマイザー
#     fp16=True,  # 半精度学習を有効にするかどうか
#     learning_rate=config.lr,  # 学習率
#     warmup_steps=config.warmup_steps,  # ウォームアップステップ数
# )
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
## LoRA設定
```

---The following area is a Code cell (cell numver is 34)---
```python
# lora_config = LoraConfig(
#     r=config.lora_r,  # LoRAのR値
#     lora_alpha=config.lora_alpha,  # LoRAのアルファ値
#     # 自己注意のみを対象とする
#     target_modules=["q_proj", "k_proj", "v_proj"],  # ターゲットモジュール（自己注意のプロジェクション）
#     layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],  # 変換するレイヤーのリスト（フリーズされていないレイヤー）
#     lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率
#     bias=config.lora_bias,  # LoRAのバイアス設定
#     task_type=TaskType.SEQ_CLS,  # タスクのタイプ（シーケンス分類）
# )
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
## モデルの初期化とトークン化
```

---The following area is a Code cell (cell numver is 36)---
```python
# tokenizer = AutoTokenizer.from_pretrained(config.model_name)  # 設定されたモデル名からトークナイザーを初期化します。
# tokenizer.add_eos_token = True  # 終了トークンを追加します。
# tokenizer.padding_side = "right"  # パディングを右側に設定します。
# model = Qwen2ForSequenceClassification.from_pretrained(
#     config.model_name,  # 設定されたモデル名からモデルを初期化します。
#     num_labels=3,  # 分類するラベルの数
#     torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
#     device_map="auto",  # 自動的にデバイスをマッピングします。
# )
# model.config.use_cache = False  # キャッシュの使用を無効にします。
# model = prepare_model_for_kbit_training(model)  # モデルを8bitトレーニング用に準備します。
# model = get_peft_model(model, lora_config)  # LoRAモデルを取得します。
# model.config.pad_token_id = model.config.eos_token_id  # パディングトークンIDを終了トークンIDに設定します。
# model  # 初期化されたモデルを表示します。
```

---The following area is a Code cell (cell numver is 37)---
```python
# model.print_trainable_parameters()  # トレーニング可能なパラメータを表示します。
# この関数は、モデル内で訓練されるパラメータの数や詳細を示します。
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# トレーニングデータのロード
```

---The following area is a Code cell (cell numver is 39)---
```python
# ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")  # トレーニングデータをCSVファイルから読み込みます。
# class CustomTokenizer:
#     def __init__(
#         self,
#         tokenizer: PreTrainedTokenizerBase,  # トークナイザーの初期化
#         max_length: int  # 最大長の設定
#     ) -> None:
#         self.tokenizer = tokenizer  # トークナイザーを保存
#         self.max_length = max_length  # 最大長を保存

#     def __call__(self, batch: dict) -> dict:  # バッチデータを処理するメソッド
#         prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]  # プロンプトに接頭辞を追加
#         response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]  # 応答Aに接頭辞を追加
#         response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]  # 応答Bに接頭辞を追加
#         texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # テキストを結合
#         tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークン化
#         labels = []  # ラベルを初期化
#         for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):  # 勝率に基づくラベルを設定
#             if a_win:
#                 label = 0  # モデルAが勝つ場合
#             elif b_win:
#                 label = 1  # モデルBが勝つ場合
#             else:
#                 label = 2  # 引き分けの場合
#             labels.append(label)  # ラベルを追加
#         return {**tokenized, "labels": labels}  # トークン化されたデータとラベルを返す

#     @staticmethod
#     def process_text(text: str) -> str:  # テキストを処理する静的メソッド
#         return " ".join(eval(text, {"null": ""}))  # nullを空文字に置き換えた後、空白で結合します。
# encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化
# ds = ds.map(encode, batched=True)  # データセットにトークナイザーを適用します。
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
## トレーニングの実行
```

---The following area is a Code cell (cell numver is 41)---
```python
# def compute_metrics(eval_preds: EvalPrediction) -> dict:  # 評価指標を計算する関数
#     preds = eval_preds.predictions  # 予測結果を取得
#     labels = eval_preds.label_ids  # ラベルを取得
#     probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # ソフトマックスを適用して確率を計算
#     loss = log_loss(y_true=labels, y_pred=probs)  # ログ損失を計算
#     acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算
#     return {"acc": acc, "log_loss": loss}  # 精度とログ損失を辞書として返す

# folds = [  # クロスバリデーションのためのフォールドを作成
#     (
#         [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニングインデックス
#         [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 検証インデックス
#     )
#     for fold_idx in range(config.n_splits)  # 注目するフォールドのインデックス
# ]
```

---The following area is a Code cell (cell numver is 42)---
```python
# train_idx, eval_idx = folds[config.fold_idx]  # 指定されたフォールドインデックスに基づいてトレーニングと評価のインデックスを取得

# trainer = Trainer(  # トレーニング用のTrainerインスタンスを作成
#     args=training_args,  # トレーニング引数
#     model=model,  # トレーニングするモデル
#     tokenizer=tokenizer,  # 使用するトークナイザー
#     train_dataset=ds.select(train_idx),  # トレーニングデータセット
#     eval_dataset=ds.select(eval_idx),  # 評価データセット
#     compute_metrics=compute_metrics,  # 評価指標の計算を行う関数
#     data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # パディングを行うデータコレータ
# )
# trainer.train()  # トレーニングを開始
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
---

# コメント

> ## Jiadi Wang トピック著者
> 
> これはトレーニングの損失データです。
> 
> もしよろしければ、UPVOTEしていただけると非常に助かります！ありがとうございます！
> 
> 

---
```

** @@@ Jupyter Notebook numver 20, the number of votes :8 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、「LMSYS - Chatbot Arena」コンペティションにおける人間の好み予測に関する問題を解決するためのものです。具体的には、大規模言語モデル（LLM）を用いて、ユーザーからのプロンプトに対して選択された応答の好ましさを予測します。

### 主な内容とアプローチ
1. **ライブラリのインストール**: 
   - `bitsandbytes`, `transformers`, `tokenizers`, `peft` などのライブラリを使用して、LLMやトークナイザー、パラメータの効率的なトレーニングをサポートします。
   - また、分析のために`numpy`や`pandas`も利用されます。

2. **データの前処理**:
   - データセットはCSVファイルから読み込まれ、NaN値や重複データの削除が行われます。
   - 各応答の長さを計算し、これを元にデータをソートします。

3. **モデルの設定とトレーニング**:
   - LLaMAモデルに基づいて系列分類用のモデルを設定し、LoRA（Low-Rank Adaptation）を用いてパラメータを効率的にトレーニングします。
   - 設定構成（`Config`クラス）に基づいてトレーニング戦略が定義され、複数の分割に対する検証（交差検証）が行われます。

4. **トークナイザーのカスタマイズ**:
   - プロンプトと応答を組み合わせたテキストをトークナイズするためのカスタムトークナイザーを使用しています。

5. **メトリクスの計算**:
   - 予測精度と対数損失（log loss）が評価され、これらの結果を用いてモデルの性能を測定します。

6. **トレーニングと評価の反復**:
   - 訓練バッチごとにトレーニングを実行し、各エポック後に評価を行う構成になっています。

このノートブックでは、特にLLaMAモデルとLoRAを組み合わせて効率的な学習を実現し、Kaggleの競技における大規模言語モデルのトレーニングと評価に関する具体的なアプローチが示されています。
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U transformers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U tokenizers --no-index --find-links ../input/llama3-1-dependencies/dependencies/
!pip install -q -U peft --no-index --find-links ../input/llama3-1-dependencies/dependencies/
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -U trl
```

---The following area is a Code cell (cell numver is 3)---
```python
# このPython 3環境には多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、ここにいくつかの便利なパッケージをロードする例があります

import numpy as np # 線形代数ライブラリ
import pandas as pd # データ処理、CSVファイルのI/O（例: pd.read_csv）

# 入力データファイルは読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると（クリックするかShift+Enterを押す）、入力ディレクトリ内のすべてのファイルがリストされます

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ(/kaggle/working/)に最大20GBまで書き込むことができ、"Save & Run All"を使ってバージョンを作成すると出力として保存されます
# 一時ファイルは/kaggle/temp/に書き込むこともできますが、そのファイルは現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 4)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    DataCollatorWithPadding,
    LlamaForSequenceClassification,
    LlamaTokenizerFast,
    PreTrainedTokenizerBase,
    EvalPrediction,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType, PeftModel
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    output_dir: str = "output"  # 出力ディレクトリの設定
    checkpoint: str = "/kaggle/input/unsloth-meta-llama-3.1-8b-bnb-4bit/transformers/default/1/Meta-Llama-3.1-8B-bnb-4bit"  # チェックポイントのパス
    max_length: int = 2048  # 最大長さの設定
    n_splits: int = 5  # データの分割数
    fold_idx: int = 0  # 現在の折り目インデックス
    optim_type: str = "adamw_8bit"  # 最適化手法
    per_device_train_batch_size: int = 4  # デバイスごとのトレーニングバッチサイズ
    gradient_accumulation_steps: int = 4  # 勾配蓄積ステップ
    per_device_eval_batch_size: int = 8  # デバイスごとの評価バッチサイズ
    n_epochs: int = 1  # エポック数
    freeze_layers: int = 16  # レイヤーを凍結する数（全体で32レイヤーがあるため最初の16レイヤーにはアダプターを追加しない）
    lr: float = 2e-4  # 学習率
    warmup_steps: int = 20  # ウォームアップステップ
    lora_r: int = 4  # LoRAの設定
    lora_alpha: float = lora_r * 2  # LoRAのアルファ
    lora_dropout: float = 0.05  # LoRAのドロップアウト率
    lora_bias: str = "none"  # LoRAのバイアス設定

config = Config()  # 設定を生成
```

---The following area is a Code cell (cell numver is 6)---
```python
lora_config = LoraConfig(
    r=config.lora_r,  # LoRAのr値を設定
    lora_alpha=config.lora_alpha,  # LoRAのアルファを設定
    target_modules=["q_proj", "k_proj", "v_proj"],  # 変換対象のモジュール
    layers_to_transform=[i for i in range(32) if i >= config.freeze_layers],  # 変換するレイヤーを選択
    lora_dropout=config.lora_dropout,  # LoRAのドロップアウト率
    bias=config.lora_bias,  # LoRAのバイアス設定
    task_type=TaskType.SEQ_CLS,  # タスクタイプを定義（系列分類）
)
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = LlamaTokenizerFast.from_pretrained(config.checkpoint)  # トークナイザーをチェックポイントから初期化
tokenizer.pad_token_id = tokenizer.eos_token_id  # パディントークンIDを終端トークンIDに設定
tokenizer.pad_token = tokenizer.eos_token  # パディントークンを終端トークンに設定
tokenizer.add_eos_token = True  # 文の終わりに<eos>トークンを追加
tokenizer.padding_side = "right"  # パディングを右側に設定
```

---The following area is a Code cell (cell numver is 8)---
```python
model = LlamaForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,  # クラスの数を指定
    torch_dtype=torch.float16,  # 使用するテンソルのデータ型を指定
    device_map="auto"  # デバイスマップの自動設定
)
```

---The following area is a Code cell (cell numver is 9)---
```python
model.config.use_cache = False  # キャッシュの使用を無効に設定
model = prepare_model_for_kbit_training(model)  # モデルをkbitトレーニング用に準備
model = get_peft_model(model, lora_config)  # PEFTモデルを取得
model
```

---The following area is a Code cell (cell numver is 10)---
```python
model.print_trainable_parameters()  # 訓練可能なパラメータを出力
```

---The following area is a Code cell (cell numver is 11)---
```python
model.config.pad_token_id = tokenizer.pad_token_id  # パディントークンIDを設定
model.config.use_cache = False  # キャッシュの使用を無効に設定
model.config.pretraining_tp = 1  # 前処理のためのテンソル並列化設定
```

---The following area is a Code cell (cell numver is 12)---
```python
import pandas as pd
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # データフレームにCSVファイルを読み込む
df = df.dropna()  # NaN値を削除
df = df.drop_duplicates(subset=['response_a', 'response_b'], keep=False)  # 重複する応答を削除
df["len"] = df["prompt"].apply(len) + df["response_a"].apply(len) + df["response_b"].apply(len)  # 各行の長さを計算
df = df.sort_values(by=['len'])  # 長さでソート
df
```

---The following area is a Code cell (cell numver is 13)---
```python
ds = Dataset.from_pandas(df)  # pandasデータフレームからデータセットを作成
ds = ds.select(torch.arange(1000))  # デモ目的のために最初の1000行を選択
```

---The following area is a Code cell (cell numver is 14)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer  # トークナイザーを初期化
        self.max_length = max_length  # 最大長を設定
        
    def __call__(self, batch: dict) -> dict:
        # 各プロンプトのベターな応答を評価するためのテキストを作成
        prompt = ["Which is the better response for the prompt? response_a or response_b or tie? \n'n give score for each lable \n\n <prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]  # 応答Aを整形
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]  # 応答Bを整形
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # プロンプト、応答A、応答Bを結合
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # テキストをトークン化
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):  # 勝者を判別
            if a_win:
                label = 0  # 応答Aが勝った場合
            elif b_win:
                label = 1  # 応答Bが勝った場合
            else:
                label = 2  # 引き分けの場合
            labels.append(label)  # ラベルを追加
        return {**tokenized, "labels": labels}  # トークン化されたデータとラベルを返す
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))  # テキストを処理して返す
```

---The following area is a Code cell (cell numver is 15)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンスを作成
ds = ds.map(encode, batched=True)  # データセット全体に対してトークナイザーを適用
```

---The following area is a Code cell (cell numver is 16)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions  # 予測値を取得
    labels = eval_preds.label_ids  # ラベルを取得
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 予測値から確率に変換
    # 予測値やラベルにNaNが含まれているかチェックする
    if np.isnan(probs).any() or np.isnan(labels).any():
        raise ValueError("NaN values found in predictions or labels")  # エラーを発生させる

    loss = log_loss(y_true=labels, y_pred=probs)  # ログ損失を計算
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算
    return {"acc": acc, "log_loss": loss}  # 精度と損失を辞書型で返す
```

---The following area is a Code cell (cell numver is 17)---
```python
folds = [
        (
            [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニングインデックス
            [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 評価インデックス
        ) 
        for fold_idx in range(config.n_splits)  # 各分割について
    ]
```

---The following area is a Code cell (cell numver is 18)---
```python
from trl import SFTTrainer, SFTConfig
sft_config = SFTConfig(
    output_dir="output",  # 出力ディレクトリ
    overwrite_output_dir=True,  # 出力ディレクトリを上書きする設定
    report_to="none",  # レポートの設定
    num_train_epochs=config.n_epochs,  # トレーニングエポック数
    per_device_train_batch_size=config.per_device_train_batch_size,  # デバイスごとのトレーニングバッチサイズ
    gradient_accumulation_steps=config.gradient_accumulation_steps,  # 勾配累積ステップ数
    per_device_eval_batch_size=config.per_device_eval_batch_size,  # デバイスごとの評価バッチサイズ
    logging_steps=1000,  # ロギングのステップ数
    save_strategy="epoch",  # 保存戦略
    save_steps=100,  # 保存ステップ
    optim=config.optim_type,  # 最適化手法
    fp16=True,  # 半精度の使用
    learning_rate=config.lr,  # 学習率
    warmup_steps=config.warmup_steps,  # ウォームアップステップ
    packing=True,  # パッキングの設定
    dataset_text_field="text",  # データセット内のテキストフィールド名
    max_seq_length=config.max_length,  # 最大シーケンス長
)
```

---The following area is a Code cell (cell numver is 19)---
```python
trainer = SFTTrainer(
        model,
        train_dataset=ds,  # トレーニングデータセットを設定
        args=sft_config  # 設定を渡す
    )
```

---The following area is a Code cell (cell numver is 20)---
```python
for fold_idx in range(config.n_splits):  # 各分割に対してループ

    train_idx, eval_idx = folds[fold_idx]  # トレーニングインデックスと評価インデックスを取得

    train_data = ds.select(train_idx).sort("len")  # トレーニングデータを選択して長さでソート
    val_data = ds.select(eval_idx).sort("len")  # 評価データを選択して長さでソート
    
    # 同じ長さの範囲でトレーニングデータをバッチに分割
    batch_size = 200  # バッチサイズ
    num_batches = len(train_data) // batch_size + (1 if len(train_data) % batch_size != 0 else 0)  # バッチ数の計算
    
    for batch_idx in range(num_batches):  # 各バッチに対してループ
        start_idx = batch_idx * batch_size  # バッチの開始インデックス
        end_idx = min(start_idx + batch_size, len(train_data))  # バッチの終了インデックス
        ds_temp = train_data.select(range(start_idx, end_idx))  # 一時データセットを選択
        
        trainer.train_dataset = ds_temp  # トレーナーにトレーニングデータを設定
        
        print(f"Training batch {batch_idx + 1}/{num_batches} on fold {fold_idx + 1}/{config.n_splits}...")  # トレーニング進行状況を表示
        
        trainer.train()  # モデルのトレーニングを実行
        
        trainer.save_model(f"model_fold_{fold_idx}_batch{batch_idx}")  # モデルを保存

    
    # すべてのバッチでトレーニングした後は検証を実施
    trainer.eval_dataset = val_data  # 評価データを設定
    
    print(f"Validating on fold {fold_idx + 1}/{config.n_splits}...")  # 検証進行状況を表示
    eval_results = trainer.evaluate()  # 検証を実施

    # 必要に応じてメトリクスを保存
    print(f"Evaluation results for fold {fold_idx + 1}: {eval_results}")  # 評価結果を表示
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
---

# コメント 

> ## PaulRRR
> 
> こんにちは、推論コードはありますか？
> 
> 
> 


---
```

** @@@ Jupyter Notebook numver 21, the number of votes :6 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するための機械学習モデルを構築することを目的としています。具体的には、大規模言語モデル（LLM）が生成した応答の中から、どちらがユーザーに好まれるかを予測するモデルを開発しています。

### 問題の取り組み
ノートブックでは、トレーニングデータとテストデータを読み込み、ユーザーの選好に基づく勝者モデルの確率を予測します。データセットはCSVフォーマットであり、どちらのモデルが選ばれるか、または引き分けになるかの情報を含んでいます。最終的な目標は、各応答がどれだけ好まれるかを予測する確率を出力することです。

### 使用技術とライブラリ
#### 使用ライブラリ:
- **TensorFlow**: 機械学習のためのフレームワークで、モデルの構築やトレーニングに使用されています。
- **Transformers**: 自然言語処理のためのモデルとトークナイザを扱うライブラリ。特にBERTモデルが使用されています。
- **Datasets**: データセットの読み込みと管理を行うためのライブラリ。
- **Pandas**: データの操作と分析を行うためのツール。
- **Matplotlib**: データの可視化を行うライブラリ。

#### 手法:
1. **データの読み込み**: Kaggleから提供されたトレーニングとテストのデータセットを読み込みます。
2. **データの前処理**: 不足している列の追加やデータ型の変換を行います。
3. **トークナイゼーション**: BERTトークナイザを使用して、モデルの入力に合わせたデータのトークナイゼーションを行います。
4. **TensorFlow Datasetの生成**: tf.data.Datasetに変換し、シャッフルやバッチ化を行います。
5. **モデル構築**: Kerasを用いてカスタムBERTモデルを構築します。BERTの出力を使用し、最終的に出力層で選好モデルのクラスを予測します。
6. **モデルのトレーニング**: 指定したエポック数でモデルをトレーニングし、その進捗を表示します。
7. **予測と結果の保存**: テストデータセットに基づいて予測を行い、その結果をCSVファイルとして保存します。

このプロジェクトは、ユーザーの選好を理解し、それに基づいてより良い応答を生成するための基盤を提供します。最終的には、出力された確率を基に、どのモデルの応答が好まれるかを評価することが可能になります。
```

---The following area is a Code cell (cell numver is 1)---
```python
# osモジュールをインポート
# オペレーティングシステムに関する機能を利用します
import os

# tensorflowライブラリをインポート
# TensorFlowは機械学習のためのフレームワークです
import tensorflow as tf

# datasetsモジュールからload_datasetとDatasetDictをインポート
# データセットを簡単にロードしたり、管理したりするためのものです
from datasets import load_dataset, DatasetDict

# transformersライブラリから必要なクラスをインポート
# 自然言語処理(NLP)のためのモデルやトークナイザを使うためです
from transformers import BertTokenizer, TFBertModel, DataCollatorWithPadding

# Kerasのレイヤーをインポート
# 様々なレイヤーを神経ネットワークのモデルに追加するために使用します
from tensorflow.keras.layers import Dense, GlobalAveragePooling1D, Lambda, Layer, Input, Dropout

# Kerasのモデルをインポート
# モデル構築やトレーニングのために使用します
from tensorflow.keras.models import Model

# shutilモジュールをインポート
# ファイルの操作を簡単にするために使用されます
import shutil

# pandasライブラリをインポート
# データ操作や分析のための強力なツールです
import pandas as pd

# tqdm.kerasからTqdmCallbackをインポート
# 学習プロセスの進捗を表示するために使用されるコールバックです
from tqdm.keras import TqdmCallback

# matplotlibのpyplotをインポート
# データの可視化を行うためのライブラリです
import matplotlib.pyplot as plt
```

---The following area is a Code cell (cell numver is 2)---
```python
# ハードウェアを検出し、適切な分散戦略を返します
try:
    # TPUを検出します。TPU_NAME環境変数が設定されている場合、パラメータは必要ありません。
    # これはKaggleで常に当てはまります。
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()
    print('TPUで実行中: ', tpu.master())
except ValueError:
    # TPUが検出できなかった場合は、tpuをNoneに設定します
    tpu = None

# TPUが存在する場合
if tpu:
    # TPUクラスタに接続します
    tf.config.experimental_connect_to_cluster(tpu)
    # TPUシステムを初期化します
    tf.tpu.experimental.initialize_tpu_system(tpu)
    # TPUStrategyを使用して分散戦略を設定します
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
else:
    # デフォルトの分散戦略。CPUや単一のGPUで動作します。
    strategy = tf.distribute.MirroredStrategy()

# 同期中のレプリカ数を出力します
print("REPLICAS: ", strategy.num_replicas_in_sync)  # 現在の分散処理でのレプリカの数を表示します
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# トレーニングとテストデータセットのインポート/ロード
```

---The following area is a Code cell (cell numver is 4)---
```python
# データセットのパスを設定します
train_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのパス
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'    # テストデータのパス

# データセットをロードします
# 'csv'形式のデータを読み込み、指定したファイルからトレーニングデータを取得します
train_dataset = load_dataset('csv', data_files={'train': train_path})['train']
# テストデータも同様に読み込みます
test_dataset = load_dataset('csv', data_files={'test': test_path})['test']

# テストデータセットからIDを保存します
# 推論や後続の処理で使用するために、テストデータのIDを取得します
test_ids = test_dataset['id']
```

---The following area is a Code cell (cell numver is 5)---
```python
# テストデータセットに不足している列を追加します
# モデルの情報や勝者の情報を格納するための列を追加します
for col in ['model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie']:
    # 各列がテストデータセットに存在しない場合
    if col not in test_dataset.column_names:
        # 空の値で新しい列を追加します
        test_dataset = test_dataset.add_column(col, [""] * len(test_dataset))

# 列の値をint64型に変換します
for col in ['winner_model_a', 'winner_model_b', 'winner_tie']:
    # トレーニングデータセットに対してマップを適用し、Noneの場合は0に置き換えます
    train_dataset = train_dataset.map(lambda x: {col: int(x[col]) if x[col] is not None else 0})
    # テストデータセットに対しても同様にマップを適用し、空文字列の場合は0に置き換えます
    test_dataset = test_dataset.map(lambda x: {col: int(x[col]) if x[col] != "" else 0})
```

---The following area is a Code cell (cell numver is 6)---
```python
# ローカルでbert-base-casedのファイルを使用します
source_dir = '/kaggle/input/huggingface-bert/bert-base-cased'  # BERTモデルのソースディレクトリ

model_dir = '/kaggle/working/bert-base-cased'  # モデルを保存するディレクトリ
# ディレクトリが存在しない場合は作成します
os.makedirs(model_dir, exist_ok=True)

# 必要なファイルをソースディレクトリからモデルディレクトリにコピーします
shutil.copy(os.path.join(source_dir, 'config.json'), model_dir)        # モデルの設定ファイル
shutil.copy(os.path.join(source_dir, 'pytorch_model.bin'), model_dir)  # PyTorch用のモデルファイル
shutil.copy(os.path.join(source_dir, 'tf_model.h5'), model_dir)        # TensorFlow用のモデルファイル
shutil.copy(os.path.join(source_dir, 'tokenizer.json'), model_dir)     # トークナイザの設定ファイル
shutil.copy(os.path.join(source_dir, 'vocab.txt'), model_dir)          # 語彙情報を含むファイル
shutil.copy(os.path.join(source_dir, 'modelcard.json'), model_dir)     # モデルカードのファイル
```

---The following area is a Code cell (cell numver is 7)---
```python
# データのトークナイゼーションを行います
# 事前にトレーニングされたBERTトークナイザをモデルディレクトリから読み込みます
tokenizer = BertTokenizer.from_pretrained(model_dir)

# トークナイゼーションを行う関数を定義します
def tokenize_function(examples):
    # 'model_a'と'model_b'の入力に対してトークナイゼーションを行います
    # パディングや切り捨てを行い、最大長を512に設定します
    return tokenizer(examples['model_a'], examples['model_b'], padding="max_length", truncation=True, max_length=512)

# トレーニングデータセットに対してトークナイゼーションを適用します
tokenized_datasets = train_dataset.map(tokenize_function, batched=True)

# テストデータセットに対しても同様にトークナイゼーションを適用します
test_tokenized_datasets = test_dataset.map(tokenize_function, batched=True)

# データをバッチ化するためのコラレーターを作成します
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # トークナイザを使用してパディングを行います
```

---The following area is a Code cell (cell numver is 8)---
```python
# 正しい形状のtf.data.Datasetに変換します
def convert_to_tf_dataset(dataset, label_col=None):
    # ラベル列が指定されている場合、ラベル列以外の不要な列を削除します
    if label_col:
        dataset = dataset.remove_columns([col for col in dataset.column_names if col != label_col and col not in tokenizer.model_input_names])
    else:
        # ラベル列が指定されていない場合、モデル入力に必要な列以外を削除します
        dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])

    # tf.data.Datasetに変換します
    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,  # モデルに必要な入力列
        label_cols=[label_col] if label_col else None,  # ラベル列
        shuffle=True,  # データをシャッフル
        batch_size=64,  # バッチサイズを64に設定
        collate_fn=data_collator  # データをバッチ化するためのコラレーター
    )

# 推論用のtf.data.Datasetを作成します
def convert_to_tf_dataset_for_inference(dataset):
    # モデル入力に必要な列のみを残します
    dataset = dataset.remove_columns([col for col in dataset.column_names if col not in tokenizer.model_input_names])
    # 推論用のデータセットをtf.data.Datasetに変換します
    return dataset.to_tf_dataset(
        columns=tokenizer.model_input_names,  # モデルに必要な入力列
        shuffle=False,  # シャッフルは行わない
        batch_size=16,  # バッチサイズを16に設定
        collate_fn=data_collator  # データをバッチ化するためのコラレーター
    )
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイゼーションされたデータセットから、トレーニングデータセットを作成します
train_dataset = convert_to_tf_dataset(tokenized_datasets, 'winner_model_a')  # 'winner_model_a'をラベル列として指定
test_labels = 'winner_model_a'  # テストデータのラベルを指定
# テストデータセットを推論用に変換します
test_dataset = convert_to_tf_dataset_for_inference(test_tokenized_datasets)

# トレーニング前にデータ型と形状を確認します
for batch in train_dataset.take(1):  # トレーニングデータセットから最初のバッチを取得します
    inputs, labels = batch  # 入力とラベルを取得します
    print("データ型:")
    print(f"input_ids: {inputs['input_ids'].dtype}, attention_mask: {inputs['attention_mask'].dtype}, labels: {labels.dtype}")
    print("データの形状:")
    print(f"input_ids: {inputs['input_ids'].shape}, attention_mask: {inputs['attention_mask'].shape}, labels: {labels.shape}")  # 各データの形状を表示します
```

---The following area is a Code cell (cell numver is 10)---
```python
# カスタムモデルを構築します
class BertLayer(Layer):
    def __init__(self, **kwargs):
        # 親クラスの初期化を行います
        super(BertLayer, self).__init__(**kwargs)
        # ローカルディレクトリから事前トレーニング済みのBERTモデルをロードします
        self.bert = TFBertModel.from_pretrained(model_dir, from_pt=True)

    def call(self, inputs):
        # 入力を受け取ります
        input_ids, attention_mask = inputs
        # BERTモデルに入力を渡し、出力を取得します
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        # 最後の隠れ状態出力を返します
        return outputs.last_hidden_state


def create_keras_model():
    # 入力層を定義します
    input_ids = Input(shape=(512,), dtype=tf.int32, name='input_ids')  # input_ids用の入力
    attention_mask = Input(shape=(512,), dtype=tf.int32, name='attention_mask')  # attention_mask用の入力

    # カスタムBertLayerを介してBERT出力を取得します
    bert_output = BertLayer()([input_ids, attention_mask])
    # 最初のトークンの出力をプールします（文の表現として）
    pooled_output = Lambda(lambda x: x[:, 0], output_shape=(768,))(bert_output)
    # 出力層を追加します（クラス数は4、ソフトマックス活性化関数を使用）
    output = Dense(4, activation='softmax')(pooled_output)

    # モデルを定義します
    model = Model(inputs=[input_ids, attention_mask], outputs=output)
    return model  # 作成したモデルを返します
```

---The following area is a Code cell (cell numver is 11)---
```python
# モデルの学習を行います
with strategy.scope():  # ストラテジーのスコープ内でモデルを作成します
    model = create_keras_model()  # カスタムKerasモデルを作成します
    # モデルをコンパイルします
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=4e-5),  # Adamオプティマイザを使用し、学習率を設定
                  loss='sparse_categorical_crossentropy',  # 損失関数としてスパースカテゴリカルクロスエントロピーを使用
                  metrics=['accuracy'])  # 精度を評価指標として使用

    # モデルのトレーニングを開始します
    history = model.fit(train_dataset, epochs=10, callbacks=[TqdmCallback(verbose=2)])  # 10エポックで学習し、進捗を表示します
```

---The following area is a Code cell (cell numver is 12)---
```python
# 予測を行います
predictions = model.predict(test_dataset)  # テストデータセットに対する予測を取得します

# 提出用のDataFrameを作成します
submission = pd.DataFrame({
    'id': test_ids,  # テストIDを含めます
    'winner_model_a': predictions[:, 0],  # モデルAの勝者確率
    'winner_model_b': predictions[:, 1],  # モデルBの勝者確率
    'winner_model_tie': predictions[:, 2]  # 引き分けの確率
})

# データフレームをCSVファイルとして保存します
submission.to_csv('submission.csv', index=False)  # インデックスなしでCSVファイルを作成します
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# コメント

> ## ATHviii
> 
> いいですね。私たち二人ともClaudeから助けを受けているようです。
> 
>
```

** @@@ Jupyter Notebook numver 22, the number of votes :6 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のための深層学習モデルを構築・評価するためのコードを提供しています。以下にその要約を示します。

### 問題設定
Notebookは、人間の好みを予測するためのモデルを構築するという課題に取り組んでいます。具体的には、二つの異なるチャットボット（モデル）からの応答のうち、どちらがユーザーに好まれるかを予測することが目標です。このプロセスでは、使用するモデルのトークン化やスコアリングを行い、最終的に予測結果を生成します。

### 使用する手法とライブラリ
1. **ライブラリのインストール**:
   - `transformers`: 大規模言語モデル（LLM）を扱うライブラリ。
   - `peft`, `bitsandbytes`, `accelerate`: モデルの最適化や量子化に役立つライブラリ。
   - `einops`: テンソルの操作を簡素化するライブラリ。
   - `torch`: PyTorchライブラリを使用し、深層学習フレームワークとしての基盤を提供。

2. **モデルの準備**:
   - 複数の事前トレーニング済みモデル（1.8B、2.7B、20B）のロードと量子化を行い、GPUに配置します。
   - `BitsAndBytesConfig`を使用して、モデルのメモリ効率を向上させる4ビット量子化を実施。

3. **データの前処理**:
   - CSV形式のテストデータを読み込み、プロンプトと応答の長さを最大7200トークンに制限するカットオフ処理を行います。
   - 各データ例に対して、プロンプトと応答の整形を実施。

4. **推論の実行**:
   - モデルを使って推論し、各応答に対するスコアを計算します。スコアの差が小さい場合は引き分けとし、それ以外の場合は勝者モデルのスコアを確率に変換します。
   - 推論関数を2つのスレッドで並列実行し、効率的に処理を行います。

5. **予測結果の保存**:
   - 最終的な予測結果をDataFrameにまとめ、`submission.csv`として保存します。また、エラーカウントや予測性能評価（対数損失など）も行う準備があります。

### 結論
このNotebookは、効果的に人間の好みを予測するための一連の処理を構築し、特定のチャットボットの応答の好まれ方を理解するための基盤を提供します。使用されている技術的な手法やライブラリは、最新の深層学習モデルを精細に調整し、評価するためのものです。利用者は、最終的に生成された`submission.csv`ファイルを用いてコンペティションに参加することが可能です。
```

---The following area is a Code cell (cell numver is 1)---
```python
# transformers、peft、bitsandbytes、accelerateのライブラリを最新バージョンにアップデートしてインストールします。
# --no-indexオプションを使用して、PyPIから直接ではなく、指定したリンクからインストールします。
!pip install -U transformers peft bitsandbytes accelerate --no-index --find-links /kaggle/input/lmsys-wheel-files

# einopsライブラリを最新バージョンにアップデートしてインストールします。
# 同様に、指定されたリンクからインストールします。
!pip install -q -U einops --no-index --find-links /kaggle/input/einops-v0-8-0
```

---The following area is a Code cell (cell numver is 2)---
```python
# PyTorchライブラリをインポートします。これは深層学習フレームワークです。
import torch

# AutoModelおよびAutoTokenizerクラスをtransformersライブラリからインポートします。
# AutoModelは、指定されたモデル名に基づいて適切なモデルを自動的にロードします。
# AutoTokenizerは、入力テキストをトークンに変換するためのツールです。
from transformers import AutoModel, AutoTokenizer

# BitsAndBytesConfigクラスをtransformersライブラリからインポートします。
# これは、モデルの量子化やメモリ効率の向上に関連する設定を扱います。
from transformers import BitsAndBytesConfig
```

---The following area is a Code cell (cell numver is 3)---
```python
# 使用するモデルのパスを指定します。
# 最初は1.8Bモデル、次に2.7Bモデル、最後に20Bモデルのパスが示されています。
model_path = "/kaggle/input/internlm2-1.8b-reward/transformers/default/1/internlm_internlm2-1_8b-reward"
model_path = "/kaggle/input/internlm-2-7b/transformers/default/1/internlm_internlm2-7b-reward"
model_path = "/kaggle/input/iternlm2-20b-reward/transformers/default/1/internlm_internlm2-20b-reward"

# 指定したモデルパスからトークナイザーをロードします。
# trust_remote_code=Trueは、モデル構成のリモートコードを信頼する設定です。
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)

# BitsAndBytesConfigの設定を作成します。
# load_in_4bit=Trueは、4ビットモデルのロードを有効にします。
# bnb_4bit_quant_typeで量子化のタイプを指定します。
# bnb_4bit_use_double_quant=Trueは、ダブル量子化を使用するかどうかを示します。
# bnb_4bit_compute_dtypeは、計算時に使用するデータ型を指定します。
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.float16,
)

# model_0として指定したパスから事前トレーニング済みのモデルをロードします。
# device_map="cuda:0"は、最初のGPUデバイスを指定しています。
# torch_dtype=torch.float16は、モデルが使用するデータ型を指定しています。
model_0 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:0", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

# model_1として同様に指定したパスから事前トレーニング済みのモデルをロードします。
# device_map="cuda:1"は、2番目のGPUデバイスを指定しています。
model_1 = AutoModel.from_pretrained(
    model_path, 
    device_map="cuda:1", 
    torch_dtype=torch.float16, 
    trust_remote_code=True,
    quantization_config=bnb_config,
)

# 次に、同じモデルをGPUデバイス2000に向けてタブ使用でロードするコードのコメントアウトされた部分があります。
# コメントアウトされているため、実行されませんが、同様の意味を持っています。
# model_0 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:0", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
# model_1 = AutoModel.from_pretrained(
#     model_path, 
#     device_map="cuda:1", 
#     torch_dtype=torch.float16, 
#     trust_remote_code=True,
# )
```

---The following area is a Code cell (cell numver is 4)---
```python
# 4ビット量子化されたモデルの保存先パスを指定します。
# ここでは"internlm2-20b-rm-bnb-4bit"という名前です。
# bnb_4bit_path = "internlm2-20b-rm-bnb-4bit"

# model_0（最初のモデル）を指定したパスに保存します。
# save_pretrainedメソッドを使用して、モデルの状態をディスクに保存します。
# model_0.save_pretrained(bnb_4bit_path)

# tokenizer（トークナイザー）を指定したパスに保存します。
# これにより、このトークナイザーの設定や辞書などが保存されます。
# tokenizer.save_pretrained(bnb_4bit_path)
```

---The following area is a Code cell (cell numver is 5)---
```python
# IPython.displayライブラリからFileLinkとdisplay関数をインポートします。
# FileLinkは、ファイルへのリンクを作成するためのツールです。
# display関数は、与えられたオブジェクトを表示します。
# from IPython.display import FileLink, display

# 指定されたファイルへのリンクを作成し、そのリンクを表示します。
# ファイルのパスは"/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors"です。
# display(FileLink("/kaggle/working/internlm2-20b-rm-bnb-4bit/model-00001-of-00003.safetensors"))
```

---The following area is a Code cell (cell numver is 6)---
```python
# model_0の内容を表示します。
# これにより、モデルの情報や設定、構造などの詳細が表示されます。
model_0
```

---The following area is a Code cell (cell numver is 7)---
```python
# pandasライブラリをインポートします。
# pandasはデータ解析用の強力なツールです。
import pandas as pd

# DEBUGフラグを初期化します。このフラグは、デバッグモードを示します。
# DEBUG = False

# test.csvファイルを読み込み、DataFrameとしてdfに格納します。
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# DataFrameの行数が3の場合、デバッグモードを有効にしてトレーニング用のデータセットを読み込むようにします。
# デバッグモードであれば、train.csvファイルを読み込み、最初の1000行のみを取得します。
# if len(df) == 3:
#     DEBUG = True
#     df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
#     df = df.head(1000)

# 読み込んだDataFrame dfを表示します。
df
```

---The following area is a Code cell (cell numver is 8)---
```python
# 与えられたデータ例のプロンプトと応答を処理する関数を定義します。
# この関数は、テキストの長さが指定された最大値を超えないようにカットします。
def cut_off(example, max_length=7200):
    
    # 指定されたインデックスまでのテキストの長さをカウントする内部関数を定義します。
    def _count(example, idx):
        _len = 0    
        # プロンプトと両方の応答の長さを合計します。
        for s in example['prompt'][:idx] + example['response_a'][:idx] + example['response_b'][:idx]:
            _len += len(s)
        return _len

    # 再帰的にカットを実行する内部関数を定義します。
    def _recusive_cut(idx):
        if idx == 0:
            # インデックスが0の場合、最大の長さに切り捨てます。
            example['prompt'] = [example['prompt'][0][:1000]]
            example['response_a'] = [example['response_a'][0][:3000]]
            example['response_b'] = [example['response_b'][0][:3000]]
            return example
            
        # 指定したインデックスまでの長さが最大値を超えているか確認します。
        if _count(example, idx) > max_length:
            # 超えている場合、インデックスを1つ減らして再帰的に実行します。
            return _recusive_cut(idx-1)
        else:
            # 最大値を超えていない場合、プロンプトと応答を適切なインデックスまで切り捨てます。
            example['prompt'] = example['prompt'][:idx]
            example['response_a'] = example['response_a'][:idx]
            example['response_b'] = example['response_b'][:idx]
            return example
    # プロンプトの長さに基づいて再帰的にカットを実行します。
    return _recusive_cut(len(example['prompt']))


# 例を処理するための関数を定義します。
def process_fn(example):
    # evalを使用して、文字列をPythonのオブジェクトに変換します。
    example['prompt'] = eval(example['prompt'], {"null": ""})
    example['response_a'] = eval(example['response_a'], {"null": ""})
    example['response_b'] = eval(example['response_b'], {"null": ""})
    # カットオフを実行します。
    return cut_off(example)

# DataFrame dfの各行に対してprocess_fnを適用し、新しいDataFrame new_dfを作成します。
new_df = df.apply(lambda x: process_fn(x), axis=1)

# 新しく生成されたDataFrame new_dfを表示します。
new_df
```

---The following area is a Code cell (cell numver is 9)---
```python
# コードの実行時間を計測するためのマジックコマンドです。
%%time

# mathライブラリとnumpyライブラリをインポートします。
import math
import numpy as np

# データフレームとモデルを引数に取り、推論を行う関数を定義します。
def inference(df, model):

    error_cnt = 0  # エラーのカウントを初期化します。
    y_pred = []  # 予測結果を保存するリストを初期化します。
    
    # DataFrameの各行を繰り返します。
    for idx, row in df.iterrows():
        chat_a = []  # モデルA用のチャットデータを初期化します。
        chat_b = []  # モデルB用のチャットデータを初期化します。
        
        # 各プロンプトに対して応答を追加します。
        for i in range(len(row['prompt'])):
            chat_a.append({"role": "user", "content": row['prompt'][i]})  # ユーザーのプロンプトを追加します。
            chat_a.append({"role": "assistant", "content": row['response_a'][i]})  # モデルAの応答を追加します。

            chat_b.append({"role": "user", "content": row['prompt'][i]})  # ユーザーのプロンプトを追加します。
            chat_b.append({"role": "assistant", "content": row['response_b'][i]})  # モデルBの応答を追加します。
            
        # モデルを使ってスコアを取得します。
        try:
            score1, score2 = model.get_scores(tokenizer, [chat_a, chat_b])  # モデルからスコアを取得します。
            
            # スコアの差が0.08未満の場合は、両モデルの勝ちをほぼ同じとみなします。
            if abs(score1 - score2) < 0.08:
                y_pred.append([0.00005, 0.00005, 0.9999])  # 引き分けの場合の予測を追加します。
            else:
                # スコアを指数関数的に変換し、確率を計算します。
                score1, score2 = math.exp(score1), math.exp(score2)
                sum_ = score1 + score2  # スコアの合計を計算します。
                y_pred.append([score1/sum_ - 0.0001, score2/sum_ - 0.0001, 0.0002])  # 確率をリストに追加します。
        except:
            # エラーが発生した場合は、均等な確率を割り当てます。
            y_pred.append([0.33334, 0.33333, 0.33333])
            error_cnt += 1  # エラーカウントを増やします。
        
    # 予測結果をNumPy配列に変換します。
    y_pred = np.array(y_pred)
    
    # DataFrameに各モデルの予測結果を追加します。
    df['winner_model_a_pred'] = y_pred[:, 0]  # モデルAの勝者予測を追加します。
    df['winner_model_b_pred'] = y_pred[:, 1]  # モデルBの勝者予測を追加します。
    df['winner_tie_pred'] = y_pred[:, 2]  # 引き分け予測を追加します。
    
    print(error_cnt)  # エラーの数を表示します。
    return df  # 予測結果を含むDataFrameを返します。
```

---The following area is a Code cell (cell numver is 10)---
```python
# 推論関数をテストします。
# new_dfの最初の20行を用いて、model_0を使った推論を実行します。
# inference(new_df[:20], model_0)
```

---The following area is a Code cell (cell numver is 11)---
```python
# コードの実行時間を計測するためのマジックコマンドです。
%%time

# new_dfの偶数番目の行を選択し、コピーを作成します。これがsub_0になります。
sub_0 = new_df.iloc[0::2].copy()
# new_dfの奇数番目の行を選択し、コピーを作成します。これがsub_1になります。
sub_1 = new_df.iloc[1::2].copy()

# concurrent.futuresからThreadPoolExecutorをインポートします。
from concurrent.futures import ThreadPoolExecutor 

# ThreadPoolExecutorを使用して、2つのスレッドで推論を並列実行します。
with ThreadPoolExecutor(max_workers=2) as executor:
    # inference関数をそれぞれのサブDataFrameとモデルに対して実行します。
    results = executor.map(inference, (sub_0, sub_1), (model_0, model_1))

# 結果をDataFrameに結合し、新しいDataFrame result_dfを作成します。
result_df = pd.concat(list(results), axis=0)

# 結果DataFrameの最初の5行を表示します。
result_df.head()
```

---The following area is a Code cell (cell numver is 12)---
```python
# result_dfの列名を変更します。
# 'winner_model_a_pred'を'winner_model_a'に、
# 'winner_model_b_pred'を'winner_model_b'に、
# 'winner_tie_pred'を'winner_tie'に変更します。
result_df = result_df.rename(columns={'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})

# 列名を変更した結果のDataFrame result_dfを表示します。
result_df
```

---The following area is a Code cell (cell numver is 13)---
```python
# result_dfから'id', 'winner_model_a', 'winner_model_b', 'winner_tie'の列を選択し、'submission.csv'ファイルに書き出します。
# index=Falseは、行番号をCSVに含めないようにするオプションです。
result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)

# 書き出した'submission.csv'ファイルを読み込み、最初の5行を表示します。
pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 14)---
```python
# sklearn.metricsからlog_lossとaccuracy_scoreをインポートします。
# from sklearn.metrics import log_loss, accuracy_score

# デバッグモードが有効な場合に以下の処理を実行します。
# if DEBUG:
#     # 実際の結果を選択し、y_trueとしてリストに変換します。
#     y_true = result_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.tolist()
#     # 予測結果を選択し、y_predとしてリストに変換します。
#     y_pred = result_df[['winner_model_a_pred', 'winner_model_b_pred', 'winner_tie_pred']].values.tolist()
#     # log_lossを計算し、表示します。
#     print(log_loss(y_true, y_pred))
```

---The following area is a Code cell (cell numver is 15)---
```python
# デバッグモードが無効な場合に以下の処理を実行します。
# if not DEBUG:
#     # result_dfの列名を再度変更します。
#     result_df = result_df.rename({'winner_model_a_pred' : 'winner_model_a', 'winner_model_b_pred' : 'winner_model_b', 'winner_tie_pred' : 'winner_tie'})
#     # 'id', 'winner_model_a', 'winner_model_b', 'winner_tie'の列を選択し、'submission.csv'ファイルに書き出します。
#     result_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].to_csv('submission.csv', index=False)
#     # 書き出した'submission.csv'ファイルを読み込み、最初の5行を表示します。
#     pd.read_csv('submission.csv').head(5)
```

---The following area is a Code cell (cell numver is 16)---
```python

```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
---

# コメント 

> ## Dlond Mike
> 
> 8u nb lol:)

---
```

** @@@ Jupyter Notebook numver 23, the number of votes :6 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するためのモデル推論を行うためのものです。具体的には、大規模言語モデル（LLM）の応答を比較し、どちらの応答がユーザーに好まれるかを予測するタスクに取り組んでいます。

### 取り組む問題
ノートブックは、与えられたテストデータに対して、異なるLLMの応答（モデルAとモデルB）の勝者を予測する問題に焦点を当てています。この予測は、選択肢のうちどちらのモデルの応答が優れているか、または同等であるのかを示す確率を出力することで行われます。

### 使用する手法とライブラリ
- **ライブラリ**:
  - `bitsandbytes`: メモリ効率の良いパラメータ管理を提供。
  - `peft`: モデルのパラメータを効率的にファインチューニングするためのラッパー。
  - `transformers`: 事前学習済みのトークナイザーやモデルを使用。
  - `pandas` と `numpy`: データ操作や数値計算を行うための一般的なライブラリ。
  - `torch`: モデルの構築や訓練を行うための主要な深層学習ライブラリ。
  - `datasets`: データセットの処理に特化したライブラリ。

- **手法**:
  1. **データのインポートと前処理**: テストデータを読み込み、応答やプロンプトから不要な記号を削除し、モデルの入力形式に整形します。
  2. **トークナイゼーション**: ユーザーのプロンプトとモデルの応答をトークン化し、モデルが処理できる形式に変換します。
  3. **モデルの読み込み**: 事前学習済みのLLMを読み込み、2つのGPUデバイスを使って異なるモデルを並行して評価します。
  4. **推論**: 推論関数を使用して、トークン化されたデータに対してモデルを評価し、モデルAとモデルBの応答の勝者確率を計算します。
  5. **結果の整理**: 推論結果をデータフレームに格納し、不要な列を削除して、最終的な出力ファイル（`submission.csv`）を作成します。

全体として、このノートブックは、効率的なデータ処理と推論を通じて、ユーザーの好みを予測する能力を強化することを目的としています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# 必要なライブラリをインストールするためのコマンドです。
# 'bitsandbytes'というライブラリを指定されたホイールファイルからインストールします。
!pip install -U /kaggle/input/bitsandbytes-0-42-0-py3-none-any-whl/bitsandbytes-0.42.0-py3-none-any.whl -qq

# 次に、'peft'というライブラリを同様に指定されたホイールファイルからインストールします。
!pip install -U /kaggle/input/peft-wheel/pytorch/version1/1/peft-0.10.0-py3-none-any.whl -qq
```

---The following area is a Code cell (cell numver is 2)---
```python
# 必要なライブラリをインポートするためのコードです。

# pandasはデータ処理に便利なライブラリです。
import pandas as pd

# numpyは数値計算や配列処理のためのライブラリです。
import numpy as np

# softmax 関数は、入力された値を確率分布に変換します。
from scipy.special import softmax

# transformersライブラリから、事前学習済みのトークナイザーとモデルをインポートします。
from transformers import AutoTokenizer, AutoModelForSequenceClassification, LlamaForSequenceClassification, BitsAndBytesConfig

# peftライブラリからPeftModelとPeftConfigをインポートします。
from peft import PeftModel, PeftConfig

# PyTorchをインポートします。これにより、深層学習モデルを構築・訓練できます。
import torch

# 自動混合精度を使用するためのモジュールをインポートします。
from torch.cuda.amp import autocast

# datasetsライブラリからDatasetクラスをインポートします。これを使ってデータセットを扱います。
from datasets import Dataset

# PyTorchの機能を活用するための関数をインポートします。
import torch.nn.functional as F

# スレッドを作成するためのモジュールです。
from threading import Thread

# メモリ管理のためにガーベジコレクションを行うためのモジュールです。
import gc
```

---The following area is a Code cell (cell numver is 3)---
```python
# GPUのメモリ効率を改善するための設定です。
# ストレージ記憶に適したデータ並列処理を有効にします。
torch.backends.cuda.enable_mem_efficient_sdp(True)

# フラッシュSDP（ストレージデータ並列処理）を有効にします。
torch.backends.cuda.enable_flash_sdp(True)

# モデルが保存されているディレクトリのパスを指定します。
MODEL_NAME = "/kaggle/input/llama-3/transformers/8b-hf/1"

# モデルの入力シーケンスの最大長を指定します。この長さを超えるテキストは切り捨てられます。
MAX_LENGTH = 1284

# バッチサイズを指定します。バッチサイズは、モデルに一度に渡すデータの量を決定します。
BATCH_SIZE = 4
```

---The following area is a Code cell (cell numver is 4)---
```python
# 指定されたCSVファイルを読み込みます。
# test.csvファイルは、テストデータを含んでいます。
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

# 読み込んだデータフレームの最初の5行を表示します。
# これにより、データの構造や内容を確認できます。
df.head()
```

---The following area is a Code cell (cell numver is 5)---
```python
# 行データを変換するための関数を定義します。
def transform(row):
    # 文字列の前後から角括弧（[]）を削除して返します。
    return row.strip('[]')  # 余分な括弧を取り除くことで、データをクリーンにします。
```

---The following area is a Code cell (cell numver is 6)---
```python
# データフレームの特定の列に対して、先ほど定義したtransform関数を適用します。
# 'prompt'列の各要素にtransform関数を適用して角括弧を取り除きます。
df['prompt'] = df['prompt'].apply(transform)

# 'response_a'列の各要素にtransform関数を適用して角括弧を取り除きます。
df['response_a'] = df['response_a'].apply(transform)

# 'response_b'列の各要素にtransform関数を適用して角括弧を取り除きます。
df['response_b'] = df['response_b'].apply(transform)
```

---The following area is a Code cell (cell numver is 7)---
```python
# 新しい列'text'をデータフレームに作成します。
# この列にはユーザープロンプトと2つのモデルの応答がフォーマットされた状態で格納されます。
df['text'] = 'User prompt: ' + df['prompt'] +  '\n\nModel A :\n' + df['response_a'] +'\n\n----------\n\nModel B:\n'  + df['response_b'] 

# これにより、各行がユーザーの入力とそれに対するモデルAおよびモデルBの応答を持つ形式になります。
```

---The following area is a Code cell (cell numver is 8)---
```python
# PEFTモデルのIDを指定し、PEFT設定を取得するためのコードです。
# ただしこの行はコメントアウトされています。
# peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
# peft_config = PeftConfig.from_pretrained(peft_model_id)

# 事前学習済みのトークナイザーを指定したモデル名から読み込みます。
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

# パディングトークンのIDとトークンを設定します。
# パディングトークンは、シーケンスの長さを揃えるために使用されます。
tokenizer.pad_token_id = tokenizer.eos_token_id  # EOS（文の終了）トークンをパディング用に指定
tokenizer.pad_token = tokenizer.eos_token  # EOSトークンを使用してパディングトークンを設定
```

---The following area is a Code cell (cell numver is 9)---
```python
# データフレームのテキストをトークナイズ（分割）するための関数を定義します。
def tokenize_function(df):
    # トークナイザーを使用して、テキストをトークン化します。
    # paddingは最大長に合わせて行い、長さが超えた場合は切り詰めます。
    result = tokenizer(df, padding="max_length", truncation=True, max_length=MAX_LENGTH)

    # トークナイズした結果からinput_ids（トークンID）とattention_mask（注目マスク）を返します。
    return result['input_ids'], result['attention_mask']  # input_ids: 入力トークンのID, attention_mask: パディング位置を示すマスク
```

---The following area is a Code cell (cell numver is 10)---
```python
# データフレームの'text'列に対してトークナイズ関数を適用します。
temp = df['text'].apply(tokenize_function)

# トークナイズの結果からinput_ids（トークンID）をデータフレームの新しい列'input_ids'に格納します。
df['input_ids'] = temp.apply(lambda x: x[0])

# トークナイズの結果からattention_mask（注目マスク）をデータフレームの新しい列'attention_mask'に格納します。
# （注意: ここではinput_idsが二重に使用されているため、正しくはx[1]であるべきです。）
df['attention_mask'] = temp.apply(lambda x: x[0])  # これはx[1]に変更する必要があります。
```

---The following area is a Code cell (cell numver is 11)---
```python
# データフレームの全体を表示します。
# データの内容や新しく追加した列'example', 'input_ids', 'attention_mask'などを確認できます。
df
```

---The following area is a Code cell (cell numver is 12)---
```python
# データフレームから特定の列を使ってデータセットを作成するコードです。
# ただし、現在この行はコメントアウトされています。
# data = Dataset.from_pandas(df[['text']])

# 作成したデータセットに対して、トークナイズ関数をバッチ処理で適用します。
# data = data.map(tokenize_function, batched=True)

# データセットの形式を設定します。PyTorch形式で、特定の列を指定します。
# data.set_format(type='torch', columns=['input_ids', 'attention_mask'])
```

---The following area is a Code cell (cell numver is 13)---
```python
# データセットの内容を表示します。
# 現在のデータセットの構造や含まれているデータの確認を行います。
# ただし、現在この行はコメントアウトされています。
# data
```

---The following area is a Code cell (cell numver is 14)---
```python
# 使用するデバイスを指定します。ここでは、GPUデバイスを設定しています。
device0 = torch.device('cuda:0')  # 最初のGPUデバイスを指定
device1 = torch.device('cuda:1')  # 2つ目のGPUデバイスを指定

# BitsAndBytesConfigを使用して、メモリ効率の良いモデルを構成します。
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットでのモデル読み込みを有効にします。
    bnb_8bit_compute_dtype=torch.float16,  # 計算に使用するデータ型を16ビット浮動小数点に設定します。
    bnb_8bit_use_double_quant=False  # ダブル量子化を無効にします。
)
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTモデルのIDを指定します。
# このパスは、指定されたディレクトリに格納されているPEFTモデルを参照します。
peft_model_id = "/kaggle/input/lmsys-llama-lora/pytorch/version1/1"
```

---The following area is a Code cell (cell numver is 16)---
```python
# Llamaモデルを事前学習済みの重みから読み込みます。
# num_labelsは分類するラベルの数を指定します。
model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # 分類ラベルは3つであることを示します。
    torch_dtype=torch.float16,  # モデルのデータ型を16ビット浮動小数点に設定します。
    quantization_config=bnb_config,  # 先ほど設定した量子化の設定を適用します。
    device_map='cuda:0'  # 最初のGPUデバイスに割り当てます。
)

# トークナイザーのパディングトークンIDをモデルの設定に指定します。
model_0.config.pad_token_id = tokenizer.pad_token_id

# PEFTモデルを事前学習済みのPEFT重みから読み込み、指定したデバイスに移行します。
model_0 = PeftModel.from_pretrained(model_0, peft_model_id).to(device0)

# モデルのマージとメモリからのアンロードを行います。
model_0 = model_0.merge_and_unload()

# モデルを評価モードに設定します。これにより、ドロップアウトなどのレイヤーが無効になります。
model_0.eval()
```

---The following area is a Code cell (cell numver is 17)---
```python
# 2つ目のLlamaモデルを事前学習済みの重みから読み込みます。
# num_labelsは分類するラベルの数を指定します。
model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # 分類ラベルは3つであることを示します。
    torch_dtype=torch.float16,  # モデルのデータ型を16ビット浮動小数点に設定します。
    quantization_config=bnb_config,  # 先ほど設定した量子化の設定を適用します。
    device_map='cuda:1'  # 2つ目のGPUデバイスに割り当てます。
)

# トークナイザーのパディングトークンIDをモデルの設定に指定します。
model_1.config.pad_token_id = tokenizer.pad_token_id

# PEFTモデルを事前学習済みのPEFT重みから読み込み、指定したデバイスに移行します。
model_1 = PeftModel.from_pretrained(model_1, peft_model_id).to(device1)

# モデルのマージとメモリからのアンロードを行います。
model_1 = model_1.merge_and_unload()

# モデルを評価モードに設定します。これにより、ドロップアウトなどのレイヤーが無効になります。
model_1.eval()
```

---The following area is a Code cell (cell numver is 18)---
```python
# ガーベジコレクションを実行します。
# 不要なメモリを解放し、効率的なメモリ管理を促進するために使用します。
gc.collect()
```

---The following area is a Code cell (cell numver is 19)---
```python
# モデルを用いた推論を行うための関数を定義します。
def inference(df, model, device, batch_size=BATCH_SIZE):
    
    # 出力結果を格納するリストを初期化します。
    all_probabilities = []
    
    # データフレームをバッチサイズに基づいてループ処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終端インデックスを計算
        
        # バッチごとのinput_idsとattention_maskを取得し、指定したデバイスに転送します。
        batch_input_ids = torch.tensor(df['input_ids'][start_idx:end_idx].tolist()).to(device)
        batch_attention_mask = torch.tensor(df['attention_mask'][start_idx:end_idx].tolist()).to(device)
        
        # 勾配計算を無効にし、混合精度推論を使用します。
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask)
        logits = outputs.logits  # モデルの出力からlogitsを取得
        probabilities = F.softmax(logits, dim=-1)  # logitsにソフトマックスを適用して確率を計算
        all_probabilities.extend(probabilities.cpu().numpy())  # 確率をリストに追加
    
    # バッチのテンソルを削除し、メモリを解放します。
    del batch_input_ids, batch_attention_mask, outputs
    gc.collect()  # ゴミ収集を実行
    torch.cuda.empty_cache()  # CUDAメモリを空にします。

    all_probabilities = np.array(all_probabilities)  # 確率をNumPy配列に変換
    
    # 各モデルの勝者確率をデータフレームに追加します。
    df['winner_model_a'] = all_probabilities[:, 0]  # モデルAの勝者確率
    df['winner_model_b'] = all_probabilities[:, 1]  # モデルBの勝者確率
    df['winner_tie'] = all_probabilities[:, 2]  # 引き分けの確率
    return df  # 更新されたデータフレームを返します。
```

---The following area is a Code cell (cell numver is 20)---
```python
# データフレームのサンプル数を取得します。
N_SAMPLES = len(df)

# サンプルの数を半分に分割します。丸めて整数値にします。
half = round(N_SAMPLES / 2)

# データフレームを2つの部分に分割します。
sub1 = df.iloc[0:half].copy()  # 最初の半分のデータをコピー
sub2 = df.iloc[half:N_SAMPLES].copy()  # 残りの半分のデータをコピー
```

---The following area is a Code cell (cell numver is 21)---
```python
# 警告メッセージを無視する設定を行います。
# これにより、警告メッセージが表示されなくなります。
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 22)---
```python
# 推論を実行するための関数を定義します。
def run_inference(df, model, device, results, index):
    # 指定されたインデックスに対して、推論結果を保存します。
    results[index] = inference(df, model, device)  # inference関数を呼び出して結果を取得し、resultsリストに格納
```

---The following area is a Code cell (cell numver is 23)---
```python
# 結果を格納するための辞書を初期化します。
results = {}

# サブデータセットに対して異なるモデルで推論を行うためのスレッドを作成します。
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # モデル0用のスレッド
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # モデル1用のスレッド

# スレッドを起動します。
t0.start()
t1.start()

# すべてのスレッドが終了するのを待ちます。
t0.join()
t1.join()

# 2つの結果を結合して単一のデータフレームにします。
data = pd.concat([results[0], results[1]], axis=0)  # 縦に結合します。
```

---The following area is a Code cell (cell numver is 24)---
```python
# 不要な列をデータフレームから削除します。
# 'prompt', 'response_a', 'response_b', 'text', 'input_ids', 'attention_mask'の各列を削除します。
data.drop(columns=['prompt', 'response_a', 'response_b', 'text', 'input_ids', 'attention_mask'], axis=1, inplace=True)  # 'inplace=True' でデータフレームを直接更新します。
```

---The following area is a Code cell (cell numver is 25)---
```python
# 最終的なデータフレームの内容を表示します。
# 不要な列を削除した後のデータフレームの構造や残っているデータを確認します。
data
```

---The following area is a Code cell (cell numver is 26)---
```python
# 最終的なデータフレームをCSVファイルとして保存します。
# 'submission.csv'というファイル名で、インデックスは含めずに保存します。
data.to_csv("submission.csv", index=False)
```

** @@@ Jupyter Notebook numver 24, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションに参加するための機械学習モデルの構築と推論プロセスに焦点を当てています。以下にノートブックの要約を示します。

### 問題の概要
本ノートブックの目的は、異なる大規模言語モデル（LLM）からの応答を比較して、どちらのモデルの応答がユーザーに好まれるかを予測することです。このタスクは、人間の好みを予測するためのモデルの開発を通じて、LLMとの相互作用を改善しようとしています。

### 手法
1. **ライブラリの使用**:
   - **PyTorch**: ニューラルネットワークの構築とトレーニングに使用。
   - **Transformers**: 事前訓練済みのモデルのロードとトークナイザーの利用に使用。
   - **Datasets**: データのロードや前処理に使用。
   - **Accelerate**: モデルを効率的にトレーニングするために使用し、マルチGPUトレーニングに対応しています。

2. **モデルアーキテクチャ**:
   - **Phi-3 miniモデル**: Microsoftから提供される事前訓練済みのモデルを使用し、LoRA（Low-Rank Adaptation）を利用してモデルを微調整しています。
   - カスタムデータセットクラスとデータローダーを定義し、トークン化されたプロンプトと応答を準備します。

3. **トレーニング手順**:
   - ハイパーパラメータ（最大シーケンス長やバッチサイズなど）を設定。
   - LoRAの適用や量子化を通じてモデルの効率を向上。
   - 自動混合精度を用いて、推論中に計算リソースを最適化。

4. **データの前処理**:
   - テストデータセットを読み込み、プロンプトと応答をJSONから抽出。
   - データを2つのバッチに分割し、各GPUで別々に推論を実行。

5. **推論と結果の取得**:
   - モデルを評価モードに設定し、推論を行う関数を使用して結果を収集。
   - 複数のスレッドを用いて推論を並列実行し、処理時間を短縮。

6. **結果の保存**:
   - 予測結果を含むデータフレームを生成し、最終的な提出フォーマットに変換してCSVファイルとして保存。
   - 不要なデータやファイルの削除を行い、リソースを効率的に管理。

### 結論
このノートブックは、LMSYS Chatbot Arenaコンペティションにおける人間の好み予測モデルの構築を目的としており、効率的なトレーニングと推論の手法を採用しています。PyTorchとTransformersライブラリを用いて、LoRAトレーニングや量子化を組み合わせることで、モデルの性能を向上させています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# インストール不要
microsoft/Phi-3-mini-4k-instruct + LoRA > GPU並列トレーニング

最大シーケンス長はモデルのパフォーマンスに大きな影響を与えますが、メモリ不足のため最大長は768に設定されました。

[トレーニングコード](https://www.kaggle.com/code/argozero01/parallel-train-phi-3-mini-4k-instruct)
```

---The following area is a Code cell (cell numver is 2)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional, Tuple

# ここでは、PyTorchと関連ライブラリをインポートしています。
# Torchを使ってニューラルネットワークモデルを構築し、トレーニングや評価を行う準備をします。
```

---The following area is a Code cell (cell numver is 3)---
```python
# ハイパーパラメータの設定
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"  # モデルの名前
model_path = "/kaggle/input/checkpoint-phi3/model_checkpoint.pth"  # モデルのチェックポイントのパス
seed = 42  # 乱数シード
lora_r = 2  # LoRAのランク
quantize_bit = 16  # 量子化ビット数
test_batch_size = 1  # テスト時のバッチサイズ
test_max_len = 256  # テスト時の最大シーケンス長
device = "cuda"  # 使用するデバイス（GPU）

# これらの変数はモデルの設定やトレーニング設定のために使用されます。
```

---The following area is a Code cell (cell numver is 4)---
```python
class CustomDataset(torch.utils.data.Dataset):
    # カスタムデータセットクラスの定義

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer  # トークナイザーの初期化
        self.prompt = df['prompt']  # データフレームからプロンプトを取得
        self.response_a = df['response_a']  # データフレームから応答Aを取得
        self.response_b = df['response_b']  # データフレームから応答Bを取得
        self.max_len = max_len  # 最大シーケンス長
        self.targets = df.get('labels', None)  # ラベルの取得（存在すれば）

    def __len__(self):
        return len(self.prompt)  # データセットのサンプル数を返す

    def __getitem__(self, index):
        # 指定したインデックスのデータを取得
        
        prompt = str(self.prompt[index])  # プロンプトを文字列として取得
        response_a = str(self.response_a[index])  # 応答Aを文字列として取得
        response_b = str(self.response_b[index])  # 応答Bを文字列として取得

        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])  # プロンプトのトークン数を計算
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])  # 応答Aのトークン数を計算
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])  # 応答Bのトークン数を計算

        final_prompt_len = min(self.max_len, prompt_len)  # プロンプトの最終長を決定
        final_a_len = min(self.max_len, response_a_len)  # 応答Aの最終長を決定
        final_b_len = min(self.max_len, response_b_len)  # 応答Bの最終長を決定

        # トークン化とパディングを行い、入力IDとアテンションマスクを作成
        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')

        # 入力IDとアテンションマスクを結合
        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])  # ラベルをTensorに変換
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}  # データを辞書形式で返す
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}  # ラベルがない場合はラベルなしでデータを返す
```

---The following area is a Code cell (cell numver is 5)---
```python
def custom_collate_fn(batch, tokenizer):
    # カスタムコレート関数の定義

    input_ids = [item['input_ids'] for item in batch]  # バッチ内の入力IDを取得
    attention_masks = [item['attention_mask'] for item in batch]  # バッチ内のアテンションマスクを取得
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None  # ラベルが含まれている場合は結合

    # バッチ内のシーケンスの最大長を見つける
    max_len = max([input_id.size(0) for input_id in input_ids])

    # 新しい最大長で再トークン化
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        # 各アイテムのデータを新しい最大長に合わせて切り詰め
        input_ids = item['input_ids'][:max_len]
        attention_mask = item['attention_mask'][:max_len]

        new_input_ids.append(input_ids)  # 新しい入力IDを追加
        new_attention_masks.append(attention_mask)  # 新しいアテンションマスクを追加

    # パディングを施してテンソルを作成
    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)

    output = {
        'input_ids': new_input_ids,
        'attention_mask': new_attention_masks
    }

    if labels is not None:
        output['labels'] = labels  # ラベルがある場合は出力に追加

    return output  # 結果を返す
```

---The following area is a Code cell (cell numver is 6)---
```python
def create_dataloaders(df, tokenizer, max_len, batch_size, shuffle=True):
    # データローダーを作成する関数の定義
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len),  # カスタムデータセットを使ったデータローダー
        shuffle=shuffle,  # シャッフルオプション
        batch_size=batch_size,  # バッチサイズ
        collate_fn=lambda x: custom_collate_fn(x, tokenizer)  # コレート関数を指定
    )
    return dataloader  # データローダーを返す
```

---The following area is a Code cell (cell numver is 7)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    # テンソルを量子化する関数の定義
    qmin = 0.
    qmax = 2.**num_bits - 1.

    min_val, max_val = tensor.min(), tensor.max()  # テンソルの最小値と最大値を取得
    scale = (max_val - min_val) / (qmax - qmin)  # スケールを計算
    zero_point = qmin - min_val / scale  # ゼロポイントを計算

    quantized_tensor = torch.round(tensor / scale + zero_point)  # テンソルを量子化
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)  # クランプ処理
    quantized_tensor = (quantized_tensor - zero_point) * scale  # スケールを戻して再スケーリング

    return quantized_tensor  # 量子化されたテンソルを返す

def quantize_model(model, num_bits=8):
    # モデル全体を量子化する関数の定義
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化
        elif isinstance(module, nn.Conv2d):
            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 畳み込み層の重みを量子化
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # 畳み込み層のバイアスを量子化

    return model  # 量子化されたモデルを返す

# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Code cell (cell numver is 8)---
```python
class LoRA(nn.Module):
    # LoRAクラスの定義
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout=0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha  # スケーリングファクタ
        self.rank = rank  # LoRAのランク
        self.lora_a = nn.Linear(in_features, rank, bias=False)  # 変換A
        self.lora_b = nn.Linear(rank, out_features, bias=False)  # 変換B
        self.dropout = nn.Dropout(lora_dropout)  # ドロップアウト層

    def forward(self, x):
        lora_out = self.alpha * self.lora_b(self.lora_a(x))  # LoRAの出力を計算
        lora_out = self.dropout(lora_out)  # ドロップアウトを適用
        return lora_out  # 出力を返す
```

---The following area is a Code cell (cell numver is 9)---
```python
from transformers.models.phi3.modeling_phi3 import (
    Phi3RotaryEmbedding,
    # Phi3LongRoPEScaledRotaryEmbedding,  # コメントアウトされたコード
    apply_rotary_pos_emb,
    repeat_kv
)

# Phi3RotaryEmbeddingなどのモジュールをインポートしています。
# これにより、ロタリー埋め込みや位置埋め込みの適用を行えるようになります。
```

---The following area is a Code cell (cell numver is 10)---
```python
class Phi3Attention(nn.Module):
    """マルチヘッドアテンション 'Attention Is All You Need' 論文から"""

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"layer_idxを渡さずに{self.__class__.__name__}をインスタンス化することは推奨されず、"
                "キャッシングを使用する際にフォワードコール中にエラーが発生します。必ず`layer_idx`を指定してこのクラスを作成してください。"
            )

        self.attention_dropout = config.attention_dropout  # アテンションドロップアウト率
        self.hidden_size = config.hidden_size  # 隠れ層のサイズ
        self.num_heads = config.num_attention_heads  # アテンションヘッドの数
        self.head_dim = self.hidden_size // self.num_heads  # 各ヘッドの次元数
        self.num_key_value_heads = config.num_key_value_heads  # キー・バリューのヘッド数
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads  # キー・バリューのグループ数
        self.max_position_embeddings = config.max_position_embeddings  # 最大位置埋め込み
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta  # RoPEのスケーリングパラメータ
        self.rope_scaling = config.rope_scaling  # RoPEのスケーリング方法
        self.is_causal = True  # 因果関係を持つかどうか

        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_sizeはnum_headsで割り切れる必要があります（`hidden_size`: {self.hidden_size}"
                f" と `num_heads`: {self.num_heads}を受け取りました）。"
            )

        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)  # 出力サイズ
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)  # 出力の線形変換
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)  # QKVの線形変換
        self._init_rope()  # RoPEの初期化

        ########################## LoRAアダプタ ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)  # QKVのLoRAアダプタ
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)  # 出力のLoRAアダプタ
        ########################## LoRAアダプタ ##########################

    def _init_rope(self):
        # RoPEの初期化
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"未知のRoPEスケーリングタイプ {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # logger.warning_once("フラッシュアテンション実装を実行していないため、数値の違いが予想されます。")

        bsz, q_len, _ = hidden_states.size()  # バッチサイズとクエリ長を取得
        ########################## LoRAアダプタ ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)  # QKVプロジェクションの計算
        ########################## LoRAアダプタ ##########################
        
        query_pos = self.num_heads * self.head_dim  # クエリ位置の計算
        query_states = qkv[..., :query_pos]  # クエリ状態の取得
        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]  # キー状態の取得
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]  # バリュー状態の取得

        # 状態を再構成
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]  # キー・バリューのシーケンス長
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"キャッシュ構造はバージョンv4.36以降変更されました。k/vキャッシングを使用するために{self.__class__.__name__}を使用している場合、"
                    "レイヤーインデックスでこのアテンションクラスを初期化してください。"
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)  # 過去のキー・バリューの長さを取得
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)  # RoPEの計算

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)  # RoPEの適用

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # RoPEモデルに特有のキーワード引数
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)  # キャッシュの更新

        # n_kv_heads < n_headsの場合にk/vヘッドを繰り返す
        key_states = repeat_kv(key_states, self.num_key_value_groups)  # キー状態の繰り返し
        value_states = repeat_kv(value_states, self.num_key_value_groups)  # バリュー状態の繰り返し

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)  # アテンション重みの計算

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"アテンション重みは{(bsz, self.num_heads, q_len, kv_seq_len)}のサイズであるべきですが、"
                f"{attn_weights.size()}になっています。"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]  # 因果マスクの取得
            attn_weights += causal_mask  # マスクの適用

        # アテンションをfp32にアップキャスト
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)  # ソフトマックスの適用
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)  # ドロップアウトの適用

        attn_output = torch.matmul(attn_weights, value_states)  # アテンションの出力計算

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"attn_outputは{(bsz, self.num_heads, q_len, self.head_dim)}のサイズであるべきですが、"
                f"{attn_output.size()}になっています。"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()  # 転置と連続性の保証
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)  # 隠れ層のサイズに変形
        ########################## LoRAアダプタ ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)  # 出力プロジェクションとLoRAアダプタの適用
        ########################## LoRAアダプタ ##########################
        
        if not output_attentions:
            attn_weights = None  # アテンション重みを返さない場合はNoneを設定

        return attn_output, attn_weights, past_key_value  # 結果を返す
```

---The following area is a Code cell (cell numver is 11)---
```python
def replace_attention_module(config, layer, layer_idx):
    # アテンションモジュールを置き換える関数
    if hasattr(layer, 'self_attn') and layer_idx > 12:  # レイヤーがself_attnを持ち、レイヤーインデックスが12より大きい場合

        new_attention = Phi3Attention(config, layer_idx)  # 新しいアテンションを作成

        # ウェイトのコピー
        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention  # 置き換え
```

---The following area is a Code cell (cell numver is 12)---
```python
loss_fn = nn.CrossEntropyLoss()  # クロスエントロピー損失関数の定義

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # LoRAモデルの初期化
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # モデルの設定
        self.peft_model = lora_model  # LoRAモデルの保存
        self.dropout = nn.Dropout(0.1)  # ドロップアウト層
        self.classifier = nn.Linear(self.config.hidden_size, 3)  # 分類器の定義（3クラス分類）

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)  # LoRAモデルのフォワードパス
        pooled_output = outputs.last_hidden_state.mean(dim=1)  # プーリング操作
        output_dropout = self.dropout(pooled_output)  # ドロップアウト適用
        logits = self.classifier(output_dropout)  # 分類器による出力

        loss = None
        if labels is not None:
            loss = loss_fn(logits, labels)  # ラベルが存在する場合は損失を計算
        return loss, logits  # 損失とロジットを返す
```

---The following area is a Code cell (cell numver is 13)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータの読み込み
len(test)  # テストデータのサンプル数を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
import json
# JSON形式の文字列をデータフレームの各カラムに適用する
test["prompt"] = test["prompt"].apply(lambda x: json.loads(x)[0])  # プロンプトの抽出
test["response_a"] = test["response_a"].apply(lambda x: json.loads(x)[0])  # 応答Aの抽出
test["response_b"] = test["response_b"].apply(lambda x: json.loads(x)[0])  # 応答Bの抽出
```

---The following area is a Code cell (cell numver is 15)---
```python
test_0 = test[:len(test)//2].reset_index(drop=True)  # テストデータの前半を取得
test_1 = test[len(test)//2:].reset_index(drop=True)  # テストデータの後半を取得
```

---The following area is a Code cell (cell numver is 16)---
```python
from torch.cuda.amp import autocast

def infer(model, dataloader, device):
    # モデルを評価モードに設定
    model.eval()

    target_list = []

    for batch in dataloader:
        with torch.no_grad():
            with autocast():  # 自動混合精度の適用
                input_ids = batch["input_ids"].to(device)  # 入力IDをデバイスに転送
                attention_mask = batch["attention_mask"].to(device)  # アテンションマスクをデバイスに転送
                _, logits = model(input_ids=input_ids, attention_mask=attention_mask)  # モデル推論
                softmax_logits = torch.nn.functional.softmax(logits, dim=1)  # ソフトマックスを適用
                target_list.append(softmax_logits)  # 結果をリストに追加

    return target_list  # 推論結果を返す
```

---The following area is a Code cell (cell numver is 17)---
```python
from threading import Thread

gpu0 = "cuda:0"  # 使用するGPUデバイス
gpu1 = "cuda:1"  # 使用するGPUデバイス
```

---The following area is a Code cell (cell numver is 18)---
```python
model0 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  , device_map="cpu")  # モデルのロード
model0 = quantize_model(model0)  # モデルの量子化
for idx, layer in enumerate(model0.layers):
    replace_attention_module(model0.config, layer, idx)  # アテンションモジュールの置き換え
model0 = LoraModelForClassification(model0)  # LoRAモデルの構築

model1 = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16
                                  , device_map="cpu")  # モデルのロード
model1 = quantize_model(model1)  # モデルの量子化
for idx, layer in enumerate(model1.layers):
    replace_attention_module(model1.config, layer, idx)  # アテンションモジュールの置き換え
model1 = LoraModelForClassification(model1)  # LoRAモデルの構築


model0.load_state_dict(torch.load(model_path))  # モデルの重みをロード
model1.load_state_dict(torch.load(model_path))  # モデルの重みをロード
model0.to(gpu0)  # GPUデバイスにモデルを転送
model1.to(gpu1)  # GPUデバイスにモデルを転送
```

---The following area is a Code cell (cell numver is 19)---
```python
tokenizer0 = AutoTokenizer.from_pretrained(model_name)  # トークナイザーのロード

if tokenizer0.pad_token is None:
    tokenizer0.pad_token = tokenizer0.eos_token  # パディングトークンの設定
tokenizer0.padding_side = "right"  # fp16トレーニングでの変 overflow問題を修正

tokenizer1 = AutoTokenizer.from_pretrained(model_name)  # トークナイザーのロード

if tokenizer1.pad_token is None:
    tokenizer1.pad_token = tokenizer1.eos_token  # パディングトークンの設定
tokenizer1.padding_side = "right"  # fp16トレーニングでの変 overflow問題を修正

test_dataloader0 = create_dataloaders(test_0, tokenizer0, test_max_len, test_batch_size, shuffle=False)  # データローダの作成
test_dataloader1 = create_dataloaders(test_1, tokenizer1, test_max_len, test_batch_size, shuffle=False)  # データローダの作成
```

---The following area is a Code cell (cell numver is 20)---
```python
def run_inference(model, dataloader, device, results, index):
    # 推論処理を別スレッドで実行
    results[index] = infer(model, dataloader, device)  # 推論結果を格納

results = {}

process0 = Thread(target=run_inference, args=(model0, test_dataloader0, gpu0, results, 0))  # スレッドの作成
process1 = Thread(target=run_inference, args=(model1, test_dataloader1, gpu1, results, 1))  # スレッドの作成

# プロセスを開始
process0.start()
process1.start()

# 両方のプロセスが終了するのを待つ
process0.join()
process1.join()
```

---The following area is a Code cell (cell numver is 21)---
```python
device = 'cuda:0'  # 移動するデバイスを指定
for k, v in results.items():
    for i in range(len(v)):
        results[k][i] = v[i].to(device)  # 結果を指定デバイスに転送

# 辞書の値を一つにまとめる
target_list = torch.cat([torch.cat(v, dim=0) for v in results.values()], dim=0)  # 結果を結合
```

---The following area is a Code cell (cell numver is 22)---
```python
sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルの読み込み
```

---The following area is a Code cell (cell numver is 23)---
```python
df_list = []  # データフレームを格納するリストを初期化
for tensor in target_list:
    # テンソルをデータフレームに変換
    df = pd.DataFrame(tensor.unsqueeze(0).detach().cpu().numpy(), columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
    df_list.append(df)  # データフレームをリストに追加

combined_df = pd.concat(df_list, axis=0, ignore_index=True)  # リスト内のデータフレームを結合

sub = sub.set_index(pd.Index(combined_df.index))  # インデックスを設定

final_df = pd.concat([sub[['id']], combined_df], axis=1)  # ID列と結果を結合
```

---The following area is a Code cell (cell numver is 24)---
```python
def delete_files_and_folders(path):
    # 指定したパスが存在するか確認
    if not os.path.exists(path):
        print(f"エラー: {path} は存在しません。")
        return

    # パス内のすべてのファイルとフォルダを走査
    for root, dirs, files in os.walk(path, topdown=False):
        # ファイルを削除
        for name in files:
            if name == "submission.csv":
                print(f"スキップするファイル: {os.path.join(root, name)}")
                continue
            file_path = os.path.join(root, name)  # ファイルパスを組み立て
            print(f"削除するファイル: {file_path}")  # 削除するファイルを出力
            os.remove(file_path)  # ファイルを削除

    print(f"{path}内のすべてのファイルとフォルダが削除されました。")  # 削除完了メッセージ

# 例としてのパス
path_to_delete = "/kaggle/working/"  # 削除対象のパス

# ファイルとフォルダ削除関数を呼び出し
delete_files_and_folders(path_to_delete)
```

---The following area is a Code cell (cell numver is 25)---
```python
final_df.to_csv('submission.csv', index=False)  # 結果をCSVファイルとして保存
```

---The following area is a Code cell (cell numver is 26)---
```python
final_df.head()  # データフレームの先頭を表示
```

---The following area is a Code cell (cell numver is 27)---
```python
# GPUメモリをクリアする関数
def clear_gpu_memory():
    torch.cuda.empty_cache()  # GPUメモリのキャッシュをクリア
    gc.collect()  # ガベージコレクタを呼び出し

# 学習後にGPUメモリをクリア
clear_gpu_memory()  # メモリをクリアする関数を実行
```

** @@@ Jupyter Notebook numver 25, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena Human Preference Predictions」コンペティションに参加するためのモデルを構築するプロセスを示しています。主な目的は、異なる大規模言語モデル（LLM）が生成した応答に対するユーザーの好みを予測することです。

### 問題
ノートブックは、ユーザーが2つの異なる応答のどちらを好むかを予測する、マルチクラス分類問題に取り組んでいます。具体的には、相手にどのモデルの応答が好まれるかを分析し、結果を予測するモデリングを行います。

### 手法
このプロジェクトでは、以下の手法とライブラリが使用されています。
- **PandasおよびNumpy**: データの操作と数値計算を行うために使用されています。
- **Scikit-learn**: データの分割やTF-IDFベクトル化、評価指標（対数損失）の計算に用いられています。
- **LightGBM**: 勾配ブースティングフレームワークを用いて、マルチクラス分類モデルのトレーニングに使用しています。
- **Optuna**: ハイパーパラメータの最適化に使用され、最適なモデル性能を導くための試行が実施されています。

### プロセス
1. データの読み込み: トレーニングデータとテストデータをCSVファイルから取得します。
2. データ前処理: 欠損値の処理や応答文の結合を行い、モデルに入力できる形式に整形します。
3. テキストのベクトル化: TF-IDFを使用して、テキストデータを数値的な特徴量に変換します。
4. データ分割: トレーニングデータをトレーニングセットとバリデーションセットに分けます。
5. モデルのトレーニング: LightGBMを使用してモデルを構築し、Optunaでハイパーパラメータチューニングを行います。
6. テストセットの予測: 最終モデルを用いてテストデータの予測確率を計算します。
7. 提出ファイルの作成: 予測結果を基にCSV形式の提出ファイルを生成します。

このノートブックは、予測のための機械学習モデルの構築と評価、そして最終的な提出ファイルの作成という一連の流れを包括的にカバーしています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# ライブラリのインポート
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import log_loss
import lightgbm as lgb
import optuna
```

---The following area is a Code cell (cell numver is 2)---
```python
# データの読み込み
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータを読み込む
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む

print(train_data.head())  # トレーニングデータの最初の数行を表示
print(train_data.info())  # トレーニングデータの情報を表示
print(train_data.describe())  # トレーニングデータの統計情報を表示
```

---The following area is a Code cell (cell numver is 3)---
```python
# データの前処理
# 欠損値があれば処理する
train_data.fillna('', inplace=True)  # トレーニングデータの欠損値を空文字で埋める
test_data.fillna('', inplace=True)  # テストデータの欠損値を空文字で埋める

# テキストデータを結合してベクトル化のための準備をする
train_data['combined_text'] = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']  # プロンプトと応答を結合
test_data['combined_text'] = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']  # 同様にテストデータも結合

# テキストデータをベクトル化する
vectorizer = TfidfVectorizer(max_features=10000)  # TF-IDFベクトルライザーを初期化（最大10,000特徴）
X_train = vectorizer.fit_transform(train_data['combined_text'])  # トレーニングデータをフィッティングして変換
X_test = vectorizer.transform(test_data['combined_text'])  # テストデータを変換

# 目的変数を抽出する
train_data['winner'] = np.where(train_data['winner_model_a'] == 1, 0, np.where(train_data['winner_model_b'] == 1, 1, 2))  # 勝者モデルを設定
y_train = train_data['winner']  # 勝者ラベルをターゲット変数として抽出
```

---The following area is a Code cell (cell numver is 4)---
```python
# データをトレーニングセットとバリデーションセットに分割する
X_train_split, X_val, y_train_split, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)  # 80:20で分割

# LightGBM用のデータセットに変換する
train_data_lgb = lgb.Dataset(X_train_split, label=y_train_split)  # トレーニングデータセットを作成
val_data_lgb = lgb.Dataset(X_val, label=y_val, reference=train_data_lgb)  # バリデーションデータセットを作成

# Optunaの目的関数（ハイパーパラメータチューニング用）
def objective(trial):
    params = {
        'feature_pre_filter': False,  # 特徴の事前フィルタリングを無効にする
        'objective': 'multiclass',  # マルチクラス分類を指定
        'num_class': 3,  # クラス数
        'metric': 'multi_logloss',  # 評価指標にマルチクラス対数損失を指定
        'boosting': 'gbdt',  # 勾配ブースティング
        'num_leaves': trial.suggest_int('num_leaves', 20, 150),  # 葉の数のハイパーパラメータ
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.25),  # 学習率のハイパーパラメータ
        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),  # 特徴の一部を使用する割合
        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),  # バギング時のサンプルの割合
        'bagging_freq': trial.suggest_int('bagging_freq', 1, 10),  # バギングの頻度
        'max_depth': trial.suggest_int('max_depth', 3, 12),  # 木の最大深さ
        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 30, 100),  # 葉に必要な最小データ数
    }
    
    model = lgb.train(params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])  # モデルをトレーニング
    
    y_val_pred_proba = model.predict(X_val, num_iteration=model.best_iteration)  # バリデーションセットの予測確率を取得
    loss = log_loss(y_val, y_val_pred_proba)  # 対数損失を計算
    return loss  # 損失を返す

# Optunaでハイパーパラメータチューニングを実行する
study = optuna.create_study(direction='minimize')  # スタディを作成
study.optimize(objective, n_trials=20)  # 20回の試行で最適化

# ベストハイパーパラメータを取得する
best_params = study.best_trial.params  # ベストな試行のパラメータを取得
best_params.update({'objective': 'multiclass', 'num_class': 3, 'metric': 'multi_logloss', 'boosting': 'gbdt'})  # 追加情報を更新

# 最終モデルをベストハイパーパラメータでトレーニング
final_model = lgb.train(best_params, train_data_lgb, valid_sets=[val_data_lgb], callbacks=[lgb.early_stopping(stopping_rounds=10), lgb.log_evaluation(10)])  # 最終モデルをトレーニング
```

---The following area is a Code cell (cell numver is 5)---
```python
# テストセットの予測確率を計算する
test_pred_proba = final_model.predict(X_test, num_iteration=final_model.best_iteration)  # テストセットの予測確率を取得

# 提出ファイルを作成する
submission = pd.DataFrame(test_pred_proba, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # データフレームを作成
submission['id'] = test_data['id']  # ID列を追加
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 列の順序を整える
submission.to_csv('submission.csv', index=False)  # CSVファイルとして保存
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# コメント

> ## dedq
> 
> シンプルで実行できます。ありがとうございます！コードにコメントを付けるのではなく、Kaggleコミュニティ向けに明確に説明するためにもっとマークダウンを使ってください！この調子で頑張ってください！
> 
>
```

** @@@ Jupyter Notebook numver 26, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、ユーザーに好まれる回答を予測するモデルを構築することを目指しています。具体的には、異なる言語モデルによって生成された回答の中から、どちらがより好まれるかを予測するための統計的手法に基づくアプローチを紹介しています。

### 主な取り組み
1. **データの読み込みと前処理**:
   - トレーニングデータとテストデータがそれぞれCSVファイルから読み込まれ、`pandas`ライブラリを用いてデータフレームとして処理されています。
   - 不要な列（'id', 'model_a', 'model_b'）が削除され、勝者を示す新しい列（'winner'）が生成されています。

2. **BERTを用いた特徴量抽出**:
   - `transformers`ライブラリからBERTモデルが使用されています。テキストデータの埋め込みベクトルを生成するために、`BertTokenizer`と`BertModel`が適用されています。
   - テキストデータ（プロンプトおよび応答）からBERTの埋め込みをバッチ処理で取得し、トレーニングデータに追加しています。

3. **機械学習モデルの構築と評価**:
   - 埋め込まれた特徴量を用いて、`CatBoost`ライブラリの`CatBoostClassifier`を使用した多クラス分類モデルが構築されています。
   - データはトレーニングセットとバリデーションセットに分割され、モデルのフィッティングと評価が行われています。精度はバリデーションデータを使って計算され、その結果が出力されています。

4. **予測と提出ファイルの作成**:
   - テストデータに対してモデルによる予測が行われ、予測確率が計算されます。
   - 最終的に、テストデータのIDと各モデルの勝者確率を含むデータフレームが生成され、`submission.csv`として保存されます。

### 使用しているライブラリ
- **NumPy**: 数値計算とデータ操作
- **Pandas**: データの読み込み、処理、そして操作に使用
- **PyTorch**: 深層学習モデルの構築とトレーニング
- **Transformers**: 自然言語処理用のBERTモデルとトークナイザー
- **TQDM**: 実行中の進捗バー表示
- **CatBoost**: 勾配ブースティングアルゴリズムを用いた分類器
- **Scikit-learn**: データ分割と評価指標の計算

このノートブックは、データ処理からモデルの構築・評価、予測結果の提出まで、全てのプロセスを体系的に示しており、機械学習の実践的な応用を提供しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下のいくつかの便利なパッケージを読み込むことができます

import numpy as np # 線形代数用ライブラリ
import pandas as pd # データ処理用ライブラリ。CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると（実行ボタンをクリックするか、Shift+Enterを押すことで）入力ディレクトリ内のファイルを一覧表示します

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    # os.walkを使用して、指定されたパス内のすべてのファイルを探索します
    for filename in filenames:
        # 各ファイルのフルパスを表示します
        print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）には最大20GBのデータを書き込むことができ、
# "Save & Run All"を使用してバージョンを作成する際に出力として保存されます
# 一時ファイルは/kaggle/temp/に書き込むこともできますが、
# 現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます
# pd.read_csv関数を使用して、指定されたパスにあるCSVファイルをデータフレームとして読み込み、
# 変数train_dataに格納します。これにより、データ分析やモデルのトレーニングに使用できるデータが得られます。
```

---The following area is a Code cell (cell numver is 3)---
```python
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます
# pd.read_csv関数を使用して、指定されたパスにあるCSVファイルをデータフレームとして読み込み、
# 変数test_dataに格納します。このデータはモデルの評価や予測に使用されます。
```

---The following area is a Code cell (cell numver is 4)---
```python
train_data.head()  # トレーニングデータの最初の5行を表示します
# head()メソッドを使用することで、DataFrameの先頭部分を確認でき、
# データ構造や内容を簡単に把握することができます。これにより、データが正しく読み込まれたかどうかを確認できます。
```

---The following area is a Code cell (cell numver is 5)---
```python
train_data_processed = train_data.drop('id', axis = 1)  # トレーニングデータから'id'列を削除します
test_data_processed = test_data.drop('id', axis = 1)   # テストデータから'id'列を削除します
# drop()メソッドを使用して特定の列（ここでは'id'）を削除しています。
# axis=1を指定することで、列を削除することを明示しています。
# 'id'列はデータ分析やモデルのトレーニングには不要なため、クリーンなデータセットを作成します。
```

---The following area is a Code cell (cell numver is 6)---
```python
train_data_processed = train_data_processed.drop(['model_a', 'model_b'], axis = 1)  # トレーニングデータから'model_a'と'model_b'列を削除します
# drop()メソッドを使用して、指定された列（ここでは'model_a'と'model_b'）を削除しています。
# axis=1を指定することで、削除する対象が列であることを明示的に指定しています。
# これにより、解析に不要な列を取り除き、より焦点を絞ったデータセットを作成します。
```

---The following area is a Code cell (cell numver is 7)---
```python
train_data_processed['winner'] = train_data_processed[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1).apply(lambda x: {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}[x])  # 'winner'列を追加します
# まず、'winner_model_a', 'winner_model_b', 'winner_tie'の3列の中で最大の値を持つ列名を取得し、そのインデックスを使用して勝者を決定します。
# idxmax(axis=1)は行ごとに最大値のインデックスを取得し、apply()を使用してそのインデックスを0、1、2の整数にマッピングしています。

train_data_processed.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)  # 不要な列を削除します
# drop()メソッドを使用して、'winner_model_a', 'winner_model_b', 'winner_tie'の列を削除します。
# inplace=Trueを指定することで、元のDataFrameを直接変更し、新しいDataFrameを生成しないようにしています。
```

---The following area is a Code cell (cell numver is 8)---
```python
train_data_processed  # 処理されたトレーニングデータを表示します
# これにより、前のステップで行ったデータの変更が正しく適用されたかを確認できます。
# DataFrameの内容を確認することで、必要な列が残り、不要な列が削除されたことをチェックできます。
```

---The following area is a Code cell (cell numver is 9)---
```python
!pip install transformers  # transformersライブラリをインストールします
# このライブラリは、トランスフォーマーモデルを使用した自然言語処理タスクに役立ちます。

!pip install torch  # PyTorchライブラリをインストールします
# PyTorchは深層学習フレームワークで、モデルの構築やトレーニングに役立ちます。
# これにより、トランスフォーマーやその他の機械学習アルゴリズムを実装できるようになります。
```

---The following area is a Code cell (cell numver is 10)---
```python
import torch  # PyTorchライブラリをインポートします
# PyTorchは、テンソル計算や深層学習モデルの構築に特化したライブラリです。

from tqdm import tqdm  # tqdmライブラリをインポートします
# tqdmは、ループの進行状況を表示するための便利なプログレスバーを提供します。
# これを使用することで、長い処理がどれくらいの進行状況かを視覚的に把握できます。
```

---The following area is a Code cell (cell numver is 11)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 実行環境に応じたデバイスを設定します
# この行では、GPUが利用可能であればそのデバイスを使用し、そうでなければCPUを使用します。
# 'cuda'はNVIDIAのGPUを指し、計算を加速するために使用されます。
# これにより、モデルのトレーニングや推論がより効率的に行えるようになります。
```

---The following area is a Code cell (cell numver is 12)---
```python
from transformers import BertTokenizer, BertModel  # Hugging FaceのtransformersライブラリからBERTトークナイザーとBERTモデルをインポートします
# BertTokenizerは、テキストデータをBERTモデルの入力形式に変換するためのツールです。
# BertModelは、BERTアーキテクチャに基づいた事前学習済みモデルを提供し、文脈を考慮した特徴表現を生成します。
# これらを使用することで、自然言語処理タスクにおいて効果的なベクトル表現を実現できます。
```

---The following area is a Code cell (cell numver is 13)---
```python
# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')  # 事前学習済みBERTトークナイザーを読み込みます
# model = BertModel.from_pretrained('bert-base-uncased')  # 事前学習済みBERTモデルを読み込みます

# tokenizer.save_pretrained('./bert-base-uncased')  # トークナイザーを指定したディレクトリに保存します
# model.save_pretrained('./bert-base-uncased')  # モデルを指定したディレクトリに保存します
# 上記のコメントアウトされたコードでは、'bert-base-uncased'という事前学習済みモデルを使用し、
# トークナイザーとモデルをそれぞれ読み込んでいます。
# その後、それらをローカルの'./bert-base-uncased'ディレクトリに保存することにより、
# 再利用のための準備を行っています。これにより、同じ環境下で再度ロードする際に便利になります。
```

---The following area is a Code cell (cell numver is 14)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')  # 保存されたBERTトークナイザーを指定したパスから読み込みます
model = BertModel.from_pretrained('/kaggle/input/bert-base-uncased/pytorch/uncased/1')  # 保存されたBERTモデルを指定したパスから読み込みます
model.to(device)  # モデルを前述のデバイス（GPUまたはCPU）に移動します
# ここでは、トークナイザーとモデルをローカルの保存先から読み込んでおり、これにより、事前学習済みのBERTを利用した処理が可能になります。
# model.to(device)を使用することで、計算リソースに応じてモデルを適切なデバイスに配置し、
# 演算が高速に行えるようになります。
```

---The following area is a Code cell (cell numver is 15)---
```python
def get_bert_embeddings_batch(text_list, batch_size=32):  # BERTの埋め込みをバッチ処理で取得する関数を定義します
    embeddings = []  # 埋め込みを格納するリストを初期化します
    for i in tqdm(range(0, len(text_list), batch_size)):  # 指定したバッチサイズでテキストリストをループ処理します
        batch_texts = text_list[i:i+batch_size]  # 現在のバッチのテキストを取得します
        inputs = tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)  # テキストをトークン化し、モデルの入力形式に変換します
        inputs = {key: value.to(device) for key, value in inputs.items()}  # 入力データをGPUに移動します（使用可能な場合）
        with torch.no_grad():  # 勾配計算を無効にしてメモリを節約します
            outputs = model(**inputs)  # モデルを実行し、出力を取得します
        batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()  # 最後の隠れ層の出力から埋め込みを取得し、CPUに戻してNumPy配列に変換します
        embeddings.append(batch_embeddings)  # バッチの埋め込みをリストに追加します
    return np.vstack(embeddings)  # すべてのバッチの埋め込みを結合して返します
```

---The following area is a Code cell (cell numver is 16)---
```python
train_prompt_embeddings = get_bert_embeddings_batch(train_data_processed['prompt'].tolist())  # トレーニングデータのプロンプトからBERT埋め込みを取得します
train_response_a_embeddings = get_bert_embeddings_batch(train_data_processed['response_a'].tolist())  # トレーニングデータの応答AからBERT埋め込みを取得します
train_response_b_embeddings = get_bert_embeddings_batch(train_data_processed['response_b'].tolist())  # トレーニングデータの応答BからBERT埋め込みを取得します
# それぞれの行では、get_bert_embeddings_batch関数を使用して、
# プロンプトや応答のテキストデータのリストからBERTの埋め込みベクトルを生成しています。
# これにより、自然言語処理用のベクトル表現が得られ、後の分析やモデルのトレーニングに利用できます。
```

---The following area is a Code cell (cell numver is 17)---
```python
test_prompt_embeddings = get_bert_embeddings_batch(test_data_processed['prompt'].tolist())  # テストデータのプロンプトからBERT埋め込みを取得します
test_response_a_embeddings = get_bert_embeddings_batch(test_data_processed['response_a'].tolist())  # テストデータの応答AからBERT埋め込みを取得します
test_response_b_embeddings = get_bert_embeddings_batch(test_data_processed['response_b'].tolist())  # テストデータの応答BからBERT埋め込みを取得します
# それぞれの行では、get_bert_embeddings_batch関数を使用して、
# テストデータのプロンプトや応答のテキストデータのリストからBERTの埋め込みベクトルを生成しています。
# これにより、自然言語処理用のベクトル表現が得られ、モデルの評価や予測に役立てられます。
```

---The following area is a Code cell (cell numver is 18)---
```python
train_embeddings = np.hstack([train_prompt_embeddings, train_response_a_embeddings, train_response_b_embeddings])  # トレーニングデータの埋め込みを水平方向に結合します
test_embeddings = np.hstack([test_prompt_embeddings, test_response_a_embeddings, test_response_b_embeddings])  # テストデータの埋め込みを水平方向に結合します
# np.hstack()関数を使用して、プロンプトの埋め込み、応答Aの埋め込み、応答Bの埋め込みを1つの配列に結合し、
# 各データポイントに対しての完全な埋め込みベクトルを生成します。
# これにより、モデルのトレーニングや評価で使用するための一貫した特徴ベクトルが得られます。
```

---The following area is a Code cell (cell numver is 19)---
```python
train_data_processed['prompt_embedding'] = list(train_embeddings[:, :768])  # トレーニングデータにプロンプトの埋め込みを追加します
train_data_processed['response_a_embedding'] = list(train_embeddings[:, 768:1536])  # トレーニングデータに応答Aの埋め込みを追加します
train_data_processed['response_b_embedding'] = list(train_embeddings[:, 1536:2304])  # トレーニングデータに応答Bの埋め込みを追加します

test_data_processed['prompt_embedding'] = list(test_embeddings[:, :768])  # テストデータにプロンプトの埋め込みを追加します
test_data_processed['response_a_embedding'] = list(test_embeddings[:, 768:1536])  # テストデータに応答Aの埋め込みを追加します
test_data_processed['response_b_embedding'] = list(test_embeddings[:, 1536:2304])  # テストデータに応答Bの埋め込みを追加します
# ここでは、train_embeddingsとtest_embeddingsからそれぞれの埋め込みを分割し、
# 各データフレームに新しい列として追加しています。
# 具体的には、プロンプト、応答A、応答Bのそれぞれに関連する768次元の埋め込みベクトルを格納しています。
```

---The following area is a Code cell (cell numver is 20)---
```python
train_data_processed  # 処理されたトレーニングデータを表示します
# これにより、前のステップで行ったデータの変更が正しく適用されたかを確認できます。
# 特に、プロンプト、応答Aおよび応答Bの埋め込みが新しい列として追加されているかどうかをチェックできます。
```

---The following area is a Code cell (cell numver is 21)---
```python
X = train_embeddings  # 特徴量行列Xにトレーニングデータの埋め込みを格納します
y = train_data_processed['winner']  # 目的変数yにトレーニングデータの'winner'列を格納します
# ここでは、Xにモデルが学習するための入力データ（埋め込みベクトル）を、
# yにはその入力に対する正解ラベル（勝者のインデックス）を設定しています。
# これにより、モデルをトレーニングする準備が整います。
```

---The following area is a Code cell (cell numver is 22)---
```python
X  # 特徴量行列Xを表示します
# これにより、トレーニングデータの埋め込みベクトルが正しく設定されているかを確認できます。
# 各行はトレーニングデータのインスタンスを表し、各列はBERTによって生成された特徴（埋め込み）を表します。
```

---The following area is a Code cell (cell numver is 23)---
```python
y  # 目的変数yを表示します
# これにより、トレーニングデータの'winner'列が正しく設定されているかを確認できます。
# 各要素は、対応する入力データに対してどのモデルが勝者であるかを示しています（0 = モデルA, 1 = モデルB, 2 = 引き分け）。
```

---The following area is a Code cell (cell numver is 24)---
```python
from sklearn.model_selection import train_test_split  # train_test_split関数をインポートします
# この関数は、データセットをトレーニングセットとテストセットに分割するために使用されます。
# 適切な割合でデータを分けることで、モデルの評価や汎化能力を確認することが可能になります。
```

---The following area is a Code cell (cell numver is 25)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)  # データをトレーニングセットとバリデーションセットに分割します
# train_test_split関数を使用して、特徴量Xと目的変数yをそれぞれトレーニングセット（90%）とバリデーションセット（10%）に分けています。
# test_size=0.1は全体の10%をバリデーションセットとすることを意味し、random_state=42は分割の再現性を保つためのシード値です。
```

---The following area is a Code cell (cell numver is 26)---
```python
pip install catboost  # CatBoostライブラリをインストールします
# CatBoostは、勾配ブースティングアルゴリズムをベースにした機械学習のためのライブラリで、
# 特にカテゴリカルデータを扱うのに適しています。
# これにより、モデルのトレーニングや予測の精度向上に役立ちます。
```

---The following area is a Code cell (cell numver is 27)---
```python
from catboost import CatBoostClassifier  # CatBoostClassifierをインポートします
from sklearn.metrics import accuracy_score  # 精度を計算するためのaccuracy_scoreをインポートします
# CatBoostClassifierは、CatBoostライブラリの中で勾配ブースティングに基づいた分類器です。
# accuracy_scoreは、予測の正確性を評価するために使用される指標で、
# モデルの性能を測定する際に非常に役立ちます。
```

---The following area is a Code cell (cell numver is 28)---
```python
model = CatBoostClassifier(  # CatBoostクラス分類器のインスタンスを作成します
    iterations = 1460,  # 学習に使用するブースティングの反復回数（イテレーション数）
    learning_rate = 0.01,  # 学習率、モデルが学習する際のステップサイズ
    depth = 7,  # 決定木の深さ、モデルの複雑さを制御します
    loss_function = 'MultiClass',  # 多クラス分類のための損失関数を指定します
    eval_metric = 'Accuracy',  # モデルの評価指標として精度を使用します
    random_seed = 0,  # 再現性を保つためのシード値
    task_type = 'GPU',  # GPUを使用してトレーニングを実行することを指定します
    verbose = 100  # 進捗状況を100イテレーションごとに表示します
)
```

---The following area is a Code cell (cell numver is 29)---
```python
model.fit(X_train, y_train, eval_set=(X_val, y_val))  # モデルをトレーニングデータでフィッティングします
# fit()メソッドを使用して、トレーニングデータ（X_train）とそのラベル（y_train）でモデルを訓練します。
# eval_set引数にバリデーションデータ（X_val, y_val）を指定することで、トレーニング中にモデルの性能を評価し、
# 過学習を防ぐための監視を行います。
```

---The following area is a Code cell (cell numver is 30)---
```python
val_preds = model.predict(X_val)  # バリデーションデータに対する予測を行います
val_preds_class = val_preds.argmax(axis=1)  # 予測結果からクラスラベルを取得します

accuracy = accuracy_score(y_val, val_preds_class)  # バリデーションセットの精度を計算します
print(f'Validation Accuracy: {accuracy:.4f}')  # 精度を小数点以下4桁で表示します
# ここでは、predict()メソッドを使用してバリデーションデータに対する予測を行い、
# argmax()を用いて各サンプルの予測されたクラスを取得しています。
# 最後に、accuracy_scoreを使用して、実際のラベルと予測ラベルの一致度を算出し、その結果を出力しています。
```

---The following area is a Code cell (cell numver is 31)---
```python
X_test = test_embeddings  # テストデータの埋め込みを特徴量行列X_testに格納します
# この行では、テストデータから生成した埋め込みベクトルをX_testに設定しています。
# これにより、後でこのデータを使ってモデルによる予測を行う準備が整います。
```

---The following area is a Code cell (cell numver is 32)---
```python
test_preds_prob = model.predict_proba(X_test)  # テストデータに対する予測確率を取得します
# predict_proba()メソッドを使用して、各クラスに対する予測確率を計算します。
# これにより、モデルが各サンプルについてどのクラスに属する可能性が高いかを示す確率を得ることができます。
# この情報は、最終的な予測の精度を向上させるために使用されることがあります。
```

---The following area is a Code cell (cell numver is 33)---
```python
submission = pd.DataFrame({  # 提出用のDataFrameを作成します
    'id': test_data['id'],  # テストデータのIDを含めます
    'prob_winner_model_a': test_preds_prob[:, 0],  # モデルAの勝者確率を追加します
    'prob_winner_model_b': test_preds_prob[:, 1],  # モデルBの勝者確率を追加します
    'prob_winner_tie': test_preds_prob[:, 2]  # 引き分けの勝者確率を追加します
})

submission.to_csv('submission.csv', index=False)  # DataFrameをCSVファイルとして保存します
# ここでは、各モデルの勝者確率とテストデータのIDを含む新しいDataFrameを作成し、
# 提出ファイルとして' submission.csv'に書き出しています。
# index=Falseを指定することで、インデックス列がCSVに含まれないようにしています。
```

---The following area is a Code cell (cell numver is 34)---
```python
# セルが空のため、特に実行する内容はありません。 
# 必要に応じて、ここにコードを追加することでさらなる処理や分析を行うことができます。
```

** @@@ Jupyter Notebook numver 27, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」競技において、ユーザーの好みを予測するために設計されています。具体的には、与えられた2つの応答（モデルAとモデルB）から、ユーザーがどちらの応答を好むかを予測するための機械学習モデルを構築します。この競技の目標は、応答の好みを正確に予測するための「報酬モデル」を作成することです。

### 問題の概要
Notebookでは、チャットボットからの異なる応答（レスポンスAとレスポンスB）が与えられ、それらがユーザーによってどのように評価されるかを予測するためのモデルを構築しています。データは、プロンプトとそれに対する2つの応答、およびそれに関連するラベルから構成されています。

### 使用されている手法とライブラリ
このノートブックでは、以下の手法とライブラリが使用されています：

- **ライブラリ**:
  - `Keras`とその関連ライブラリ`keras_nlp`: 深層学習モデルの構築に使用。
  - `TensorFlow`: モデルのトレーニングと推論に使用。
  - `pandas`: データの読み込みと処理に使用。
  - `scikit-learn`: データの分割に使用。
  - `tqdm`: プロセス中の進捗バー表示に使用。
  - `transformers`: 事前訓練された言語モデルを利用するために使用（このコードでは特にDeBERTa V3が利用されています）。

- **手法**:
  - **データ処理**: JSONデータを読み込んでプロンプトと応答を結合し、クラスラベルにマッピングします。
  - **データセットの構築**: TensorFlowのデータセットを使用して効率的にバッチ処理を行います。
  - **モデルのアーキテクチャ**: DeBERTa V3をバックボーンとして使用し、応答AとBの埋め込みを結合して最終的な予測を行います。
  - **モデルのトレーニング**: 渡されたデータセットを使用し、ロスが最小のモデルを選択するよう設定されています。

### 結果の出力
トレーニングが完了したら、最良のモデルの重みが読み込まれ、テストデータに対する予測が行われます。予測結果はCSVファイルとして保存され、最終的にKaggleへ提出する形式に整理されています。

このNotebookは、効果的なデータ処理とモデルの構築を通じて、ユーザーの応答選好を予測するための実践的なアプローチを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下はいくつかの便利なパッケージの読み込み例です

import numpy as np # 線形代数用のライブラリ
import pandas as pd # データ処理用のライブラリ、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは、読み取り専用の"../input/"ディレクトリで利用可能です
# 例えば、これを実行すると（実行ボタンをクリックするか、Shift+Enterを押すことで）入力ディレクトリ内のすべてのファイルをリストします

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    # 指定されたディレクトリ（dirname）内のファイルをすべて走査します
    for filename in filenames:
        # 各ファイルのフルパスを出力します
        print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）には最大20GBを書き込むことができ、
# 「すべてを保存して実行」したときに出力として保持されます
# また、/kaggle/temp/に一時ファイルを書き込むこともできますが、現在のセッション外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
# Kerasのバックエンドを設定します。"tensorflow"または"torch"のいずれかを指定できます
os.environ["KERAS_BACKEND"] = "tensorflow"  # ここではtensorflowを選択

import keras_nlp  # Kerasの自然言語処理用ライブラリをインポート
import keras  # Kerasライブラリをインポート
import tensorflow as tf  # TensorFlowライブラリをインポート

import numpy as np  # 数値計算用のライブラリをインポート
import pandas as pd  # データ処理用のライブラリをインポート
from tqdm import tqdm  # 進捗バーを表示するライブラリをインポート
import json  # JSONデータを扱うためのライブラリをインポート

import matplotlib.pyplot as plt  # グラフ描画用ライブラリをインポート
import matplotlib as mpl  # Matplotlibの設定を行うためのライブラリをインポート
import transformers  # Transformersライブラリをインポート（自然言語処理モデル用）
```

---The following area is a Code cell (cell numver is 3)---
```python
# TensorFlow、Keras、KerasNLPのバージョンを表示します
print("TensorFlow:", tf.__version__)  # TensorFlowのバージョンを出力
print("Keras:", keras.__version__)  # Kerasのバージョンを出力
print("KerasNLP:", keras_nlp.__version__)  # KerasNLPのバージョンを出力
```

---The following area is a Code cell (cell numver is 4)---
```python
# モデルの設定を保持するためのクラスを定義します
class CFG:
    seed = 50  # 乱数シードを指定します（再現性を保つため）
    sequence_length = 1024  # 入力シーケンスの長さを指定します
    epochs = 2  # 学習のエポック数を指定します
    batch_size = 2  # バッチサイズを指定します
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}  
    # ラベルをモデル名にマッピングする辞書を定義します
    name2label = {v:k for k, v in label2name.items()}  
    # モデル名をラベルにマッピングする辞書を生成します
    class_labels = list(label2name.keys())  # クラスラベルのリストを生成します
    class_names = list(label2name.values())  # クラス名のリストを生成します
```

---The following area is a Code cell (cell numver is 5)---
```python
# 乱数シードを設定して、再現性のある結果を得るための準備をします
keras.utils.set_random_seed(CFG.seed)  # Kerasの乱数シードを設定します

# 計算精度のポリシーを設定します。ここではbfloat16を使用します
keras.mixed_precision.set_global_policy("bfloat16")

# データセットのベースパスを定義します
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'  # データセットが格納されているパスを設定します
```

---The following area is a Code cell (cell numver is 6)---
```python
# JSON形式のデータを読み込み、リストからNoneではない要素を結合して返す関数を定義します
def load_data(row):
    # 行データをJSONとして読み込みます
    row_list = json.loads(row)
    # Noneでない要素をスペースで結合して返します
    return " ".join(row for row in row_list if row != None)  # Noneでない要素を結合して1つの文字列にします
```

---The following area is a Code cell (cell numver is 7)---
```python
# 学習データを読み込みます
df = pd.read_csv(f'{BASE_PATH}/train.csv')  # CSVファイルを読み込み、データフレームを作成します
# df = df.iloc[:100] # デモ用にデータのサブセットを使用する場合はこちらを有効にします

# 最初のプロンプトとそれに関連するレスポンスを取得します
df["prompt"] = df["prompt"].apply(load_data)  # プロンプト列にload_data関数を適用
df["response_a"] = df["response_a"].apply(load_data)  # レスポンスA列にload_data関数を適用
df["response_b"] = df["response_b"].apply(load_data)  # レスポンスB列にload_data関数を適用

# ラベルの変換を行います
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)  
# 各行の勝者モデルを判定し、対応するクラス名を取得
df["class_label"] = df.class_name.map(CFG.name2label)  # クラス名をクラスラベルにマッピング

# サンプルを表示します
df.head()  # データフレームの最初の数行を表示して確認します
```

---The following area is a Code cell (cell numver is 8)---
```python
# テストデータを読み込みます
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')  # テスト用のCSVファイルを読み込み、データフレームを作成します

# 最初のプロンプトとレスポンスを取得します
test_df["prompt"] = test_df["prompt"].apply(load_data)  # プロンプト列にload_data関数を適用
test_df["response_a"] = test_df["response_a"].apply(load_data)  # レスポンスA列にload_data関数を適用
test_df["response_b"] = test_df["response_b"].apply(load_data)  # レスポンスB列にload_data関数を適用

# サンプルを表示します
test_df.head()  # テストデータフレームの最初の数行を表示して確認します
```

---The following area is a Code cell (cell numver is 9)---
```python
# オプション列を作成します。レスポンスAとレスポンスBをリストに格納します
df['options'] = df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(df.head(2))  # データフレームdfの最初の2行を表示します

# テストデータにも同様にオプション列を作成します
test_df['options'] = test_df.apply(lambda row: [row.response_a, row.response_b], axis=1)
display(test_df.head(2))  # テストデータフレームtest_dfの最初の2行を表示します
```

---The following area is a Code cell (cell numver is 10)---
```python
from sklearn.model_selection import train_test_split  # パッケージをインポートします（データ分割用）

# データを訓練セットと検証セットに分割します
train_df, valid_df = train_test_split(df, test_size=0.1, stratify=df["class_label"])  
# 10%を検証セットとして分割し、クラスラベルに基づいて層化サンプリングを行います
```

---The following area is a Code cell (cell numver is 11)---
```python
# DeBERTa V3の前処理器を設定します
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    "deberta_v3_small_en",  # 使用するモデルのプリセット名を指定します
    sequence_length=CFG.sequence_length,  # 最大シーケンス長を設定します。短い場合はパディングされます
    dtype="bfloat16",  # データ型をbfloat16に設定します
)
```

---The following area is a Code cell (cell numver is 12)---
```python
# 最初の行のオプションを前処理します
outs = preprocessor(df.options.iloc[0])  # 最初の行のレスポンスオプションを処理

# 各処理された出力の形状を表示します
for k, v in outs.items():
    print(k, ":", v.shape)  # キーと対応する出力の形状を表示します
```

---The following area is a Code cell (cell numver is 13)---
```python
# テキストとラベルを前処理する関数を定義します
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # テキストを前処理します
    # ラベルが存在する場合は、処理されたテキストとラベルを返します
    return (text, label) if label is not None else text  # ラベルがなければ処理されたテキストのみを返します
```

---The following area is a Code cell (cell numver is 14)---
```python
# テキストとラベルからデータセットを構築する関数を定義します
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNEオプションを設定
    # ラベルがない場合はテキストのみのスライスを作成し、ある場合はラベルをワンホットエンコーディングします
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  
    ds = tf.data.Dataset.from_tensor_slices(slices)  # スライスからデータセットを作成します
    ds = ds.cache() if cache else ds  # キャッシュを有効にする場合はデータセットをキャッシュ
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # 前処理関数をマッピングします
    opt = tf.data.Options()  # データセットオプションを作成します
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # シャッフルが有効な場合、データセットをシャッフルします
        opt.experimental_deterministic = False  # 確定的でないオプションを設定
    ds = ds.with_options(opt)  # データセットオプションを設定します
    ds = ds.batch(batch_size, drop_remainder=False)  # バッチサイズでデータセットをバッチ化します
    ds = ds.prefetch(AUTO)  # 次のバッチを事前取得します
    return ds  # 構築したデータセットを返します
```

---The following area is a Code cell (cell numver is 15)---
```python
# JAXライブラリをインポートします（コメントアウトされています）
# import jax
# GPUデバイスを取得します
# devices = jax.devices("gpu")
# デバイスを表示します
# print("devices", devices)

# Kerasのデータ並列処理を設定するためのオブジェクトを作成します（コメントアウトされています）
# data_parallel = keras.distribution.DataParallel(devices=devices)
```

---The following area is a Code cell (cell numver is 16)---
```python
# データセットを取得する関数を定義します
def get_data():
    train_texts = train_df.options.tolist()  # 訓練用のテキストを抽出します
    train_labels = train_df.class_label.tolist()  # 訓練用のラベルを抽出します
    train_ds = build_dataset(train_texts, train_labels,
                             batch_size=CFG.batch_size,  # バッチサイズを設定
                             shuffle=True)  # 訓練データはシャッフルします

    # 検証データ
    valid_texts = valid_df.options.tolist()  # 検証用のテキストを抽出します
    valid_labels = valid_df.class_label.tolist()  # 検証用のラベルを抽出します
    valid_ds = build_dataset(valid_texts, valid_labels,
                             batch_size=CFG.batch_size,  # バッチサイズを設定
                             shuffle=False)  # 検証データはシャッフルしません

    # テストデータセットを構築します
    test_texts = test_df.options.tolist()  # テスト用のテキストを抽出します
    test_ds = build_dataset(test_texts,
                             batch_size=min(len(test_df), CFG.batch_size),  # テストデータのサイズに応じてバッチサイズを設定
                             shuffle=False)  # テストデータはシャッフルしません
    return train_ds, valid_ds, test_ds  # 訓練、検証、テストデータセットを返します
```

---The following area is a Code cell (cell numver is 17)---
```python
# Kerasのデータ並列処理を設定します（コメントアウトされています）
# keras.distribution.set_distribution(data_parallel)  # モデルを両方のGPUで複製します
```

---The following area is a Code cell (cell numver is 18)---
```python
# バックボーンモデルを取得する関数を定義します
def get_backbone():
    # DeBERTa V3のバックボーンモデルをプリセットから取得します
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        "deberta_v3_small_en",  # 使用するモデルのプリセット名を指定します
        dtype="bfloat16",  # データ型をbfloat16に設定します
    )
    backbone.enable_lora(8)  # LoRA（Low-Rank Adaptation）の有効化と設定を行います
    
    return backbone  # バックボーンモデルを返します
```

---The following area is a Code cell (cell numver is 19)---
```python
# モデルのチェックポイントを設定するコールバックを定義します
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',  # モデルの重みを保存するファイル名
                                  monitor='val_log_loss',  # 検証ロスを監視
                                  save_best_only=True,  # 最良のモデルのみを保存
                                  save_weights_only=True,  # 重みのみを保存
                                  mode='min')  # 最小化を目指す設定（ロスが最小の時に保存）
```

---The following area is a Code cell (cell numver is 20)---
```python
# カテゴリカルクロスエントロピーのメトリクスを定義します
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")  # ロスの名前を"log_loss"と設定します
```

---The following area is a Code cell (cell numver is 21)---
```python
# バックボーンを使用してモデルを構築する関数を定義します
def get_model(backbone):
    # 入力層を定義します
    inputs = {
        "token_ids": keras.Input(shape=(2, None), dtype=tf.int16, name="token_ids"),  # トークンID入力
        "padding_mask": keras.Input(shape=(2, None), dtype=tf.int16, name="padding_mask"),  # パディングマスクリスト
    }

    # 最初のレスポンス（Response A）に対する埋め込みを計算します: (P + R_A)
    response_a = {k: v[:, 0, :] for k, v in inputs.items()}  # 入力からAのレスポンスを抽出
    embed_a = backbone(response_a)  # バックボーンを通して埋め込みを計算

    # 2番目のレスポンス（Response B）に対する埋め込みを計算します: (P + R_B)
    response_b = {k: v[:, 1, :] for k, v in inputs.items()}  # 入力からBのレスポンスを抽出
    embed_b = backbone(response_b)  # 同じバックボーンを使用して埋め込みを計算

    # 最終出力を計算します
    embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])  # AとBの埋め込みを結合
    embeds = keras.layers.GlobalAveragePooling1D()(embeds)  # グローバル平均プーリングを適用
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)  # 出力層を定義（3クラス分類）

    # モデルを定義
    model = keras.Model(inputs, outputs)

    # 最適化アルゴリズム、損失、メトリクスを指定してモデルをコンパイル
    model.compile(
        optimizer=keras.optimizers.Adam(5e-6),  # Adamオプティマイザを使用
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),  # カテゴリカルクロスエントロピーを損失関数とする
        metrics=[
            log_loss,  # ロスメトリクス
            keras.metrics.CategoricalAccuracy(name="accuracy"),  # カテゴリカル精度メトリクス
        ],
    )
    
    return model  # 構築したモデルを返します
```

---The following area is a Code cell (cell numver is 22)---
```python
# 分散処理のための戦略を設定します
strategy = tf.distribute.MirroredStrategy()  # 複数のGPUでの計算を有効にします

# 戦略スコープをオープンします
with strategy.scope():
    
    train_ds, valid_ds, test_ds = get_data()  # データセットを取得します
    
    backbone = get_backbone()  # バックボーンモデルを取得します
    
    model = get_model(backbone)  # モデルを構築します
    
    # モデルを訓練します
    history = model.fit(
        train_ds,  # 訓練データ
        epochs=CFG.epochs,  # 訓練のエポック数
        validation_data=valid_ds,  # 検証データ
        callbacks=[ckpt_cb]  # モデルチェックポイントコールバック
    )
```

---The following area is a Code cell (cell numver is 23)---
```python
# 最良のモデルの重みを読み込みます
model.load_weights('/kaggle/working/best_model.weights.h5')  # 保存したモデルの重みを指定したパスから読み込みます
```

---The following area is a Code cell (cell numver is 24)---
```python
# 訓練済みモデルを使用してテストデータに対する予測を行います
test_preds = model.predict(test_ds, verbose=1)  # テストデータに対する予測を実行し、進捗を表示します
```

---The following area is a Code cell (cell numver is 25)---
```python
# 提出用のデータフレームを作成します
sub_df = test_df[["id"]].copy()  # テストデータからID列をコピーします
sub_df[CFG.class_names] = test_preds.tolist()  # 予測結果をデータフレームに追加します
sub_df.to_csv("submission.csv", index=False)  # 提出用のCSVファイルとして保存します
sub_df.head()  # 作成したデータフレームの最初の数行を表示して確認します
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# 📌 | 参考文献

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLPスターター](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
* [LMSYS: KerasNLPスターター](https://www.kaggle.com/code/awsaf49/lmsys-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 28, the number of votes :5 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、人間による好みの予測に関する問題に取り組んでいます。具体的には、二つのチャットボットが生成した応答のどちらが好まれるかを予測するための確率を算出し、提出用のCSVファイルを作成します。

ノートブックでは、以下の手法とライブラリが使用されています：

1. **ライブラリのインポート**: NumPyとPandasを使用してデータを処理し、Scikit-learnから対数損失（log loss）をインポートしています。

2. **データの読み込み**: Kaggleから提供されたテストデータセット（`test.csv`）をPandasを用いて読み込んでいます。

3. **特徴量の生成**:
   - 各応答（`response_a`と`response_b`）の文字数を計算し、それぞれ新しいカラム（`response_a_len`と`response_b_len`）に追加します。
   - その後、応答の長さの比率（`ratio_response_b_response_a`）を計算し、新たなカラムに追加します。

4. **確率の割り当て**:
   - 訓練データセットから事前に計算された確率の平均値（`avg_1`, `avg_2`, `avg_3`）を使用して、特定のしきい値に基づいて各行に勝者モデルの確率（モデルAの勝率、モデルBの勝率、引き分けの確率）を割り当てる関数を定義しています。

5. **提出用データフレームの作成**: 確率値を含む新しいデータフレームを作成し、元のデータから`id`カラムを取得して提出用の形式に整えています。

6. **CSVファイルの作成**: 最後に、作成したデータフレームを`submission.csv`というファイル名で保存します。

このNotebookは、特徴量エンジニアリングとしきい値を指定した条件分岐を通じて、チャットボットの応答に基づく好みの予測を行う手法を示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import numpy as np
import pandas as pd

from sklearn.metrics import log_loss
# データセットを読み込む
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# response_a の長さを計算して新しいカラムに追加
df['response_a_len'] = df['response_a'].apply(lambda x: len(x))
# response_b の長さを計算して新しいカラムに追加
df['response_b_len'] = df['response_b'].apply(lambda x: len(x))

# response_b と response_a の長さの比率を計算して新しいカラムに追加
df["ratio_response_b_response_a"] = df['response_b_len'] / df['response_a_len']

# これは訓練データセットから計算した値
avg_1 = {'winner_model_a': 0.2550469811145223, 'winner_model_b': 0.4547399758117034, 'winner_tie': 0.2902130430737743}
avg_2 = {'winner_model_a': 0.31478770131771594, 'winner_model_b': 0.33601756954612005, 'winner_tie': 0.349194729136164}
avg_3 = {'winner_model_a': 0.45816715010877446, 'winner_model_b': 0.25086113125453224, 'winner_tie': 0.29097171863669324}

# 以下のしきい値も訓練データセットから計算したもの
def assign_prob(row):
    # 比率がある閾値以下の場合は avg_3 の値を返す
    if row['ratio_response_b_response_a'] <= 0.838296:
        return avg_3.values()
    # 比率が別の閾値以下の場合は avg_2 の値を返す
    elif row['ratio_response_b_response_a'] <= 1.004711:
        return avg_2.values()
    # それ以外の場合は avg_1 の値を返す
    else:
        return avg_1.values()

# 各行に対して確率を割り当てる
df[['winner_model_a', 'winner_model_b', 'winner_tie']] = df.apply(
    lambda x: assign_prob(x), axis=1, result_type='expand'
)
```

---The following area is a Code cell (cell numver is 2)---
```python
# 提出用のデータフレームを作成
submission_df = pd.DataFrame(
    {
        'id': df['id'],  # 元のデータから id カラムを取得
        'winner_model_a': df['winner_model_a'],  # モデル A の勝者確率を取得
        'winner_model_b': df['winner_model_b'],  # モデル B の勝者確率を取得
        'winner_tie': df['winner_tie']  # 引き分け確率を取得
    }
)

# 提出用の CSV ファイルを作成
submission_df.to_csv("submission.csv", index=False)

# 作成したデータフレームを表示
submission_df
```

** @@@ Jupyter Notebook numver 29, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおける、人間が好む応答を予測するための機械学習モデルの構築を目的としています。具体的には、異なる言語モデル（LLM）が生成した応答に基づき、どちらのモデルの応答が優れているかを予測する問題に取り組んでいます。

### 手法
ノートブックでは、次の手法やライブラリが使用されています。

1. **データ前処理**:
   - NLTKやSpaCyを用いてテキストデータのトークン化、ストップワードの除去、類似度の計算を行っています。
   - 特徴量エンジニアリングにより、応答の長さ、文の多様性、共通単語数などを特徴量として抽出しています。

2. **モデルの構築**:
   - 勾配ブースティングアルゴリズムを使用し、XGBoost、CatBoost、GradientBoostingClassifierなどのモデルが選定されています。
   - データセットは80:20の比率で訓練データと検証データに分割され、層別交差検証を適用してモデルの検証が行われています。

3. **モデルの評価**:
   - モデルのパフォーマンスは、対数損失（log loss）を使用して評価されています。
   - 各モデルの平均ロスを比較し、最良のモデルのハイパーパラメータチューニングにはグリッドサーチが利用されています。

4. **結果の予測と提出**:
   - 最終的に得られたモデルを用いてテストデータに対する予測を行い、その結果を提出用フォーマットに整形しています。

### 使用ライブラリ
ノートブックでは、以下の主要なライブラリがインポートされています：
- **Pandas** と **NumPy**: データ操作と数値計算のため。
- **Matplotlib** と **Seaborn**: データの可視化のため。
- **Scikit-learn**: 機械学習モデル、評価指標、データ分割のため。
- **XGBoost** と **CatBoost**: 高効率な勾配ブースティングアルゴリズムのため。
- **NLTK** と **SpaCy**: 自然言語処理のためのライブラリです。

このノートブックは、応答の優劣を判別するモデルの作成を通じて、ユーザーの好みを予測することに注力しており、様々な特徴量をデータから抽出し、機械学習技術を駆使して問題解決に取り組んでいます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSys Chatbot Arena

提案、文書作成、そして定義した基準に基づいてどの回答が優れているか、または引き分けかを判断するソリューションを擁護します。  

**提案**: ブースティングは、複数の弱いモデルのトレーニング結果を組み合わせて強いモデルを作成する機械学習手法です。このプロセスでは、一連の相互作用が使用され、各トレーニングセッションで得られた誤差に重みが付けられます。

**例**: XGBoostは、勾配ブースティングに基づく機械学習アルゴリズムであり、決定木を使用します。各木は前の木の誤差を修正しようとします。

**特徴**: この問題に対して、私たちのデータセットに含まれるテキストから抽出した特徴を使用します。

この場合、特徴を確立するために探査的分析を使用し、分類に追加できる要素としては以下のようなものがあります：
- テキストのサイズ；
- 質問と回答に存在する単語；
- 回答間の違いなど。

## 1 - 前処理
```

---The following area is a Code cell (cell numver is 2)---
```python
# ライブラリのインポート
import pandas as pd  # データ操作のためのPandasライブラリをインポートします。
import numpy as np  # 数値計算のためのNumPyライブラリをインポートします。
import matplotlib.pyplot as plt  # グラフ作成のためのMatplotlibライブラリをインポートします。
plt.rcParams['figure.figsize'] = 14,5  # グラフのサイズを14x5に設定します。

import seaborn as sns  # データ可視化のためのSeabornライブラリをインポートします。
sns.set_style = 'whitegrid'  # グラフのスタイルを白いグリッドに設定します。

import plotly.express as px  # インタラクティブなグラフ作成のためのPlotly Expressライブラリをインポートします。
px.defaults.template = "plotly_dark"  # グラフのテンプレートをダークテーマに設定します。

from sklearn import metrics  # 評価指標を提供するScikit-learnのmetricsモジュールをインポートします。
from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割するための関数をインポートします。
from sklearn.model_selection import StratifiedKFold  # 層別交差検証のためのクラスをインポートします。
from sklearn.model_selection import GridSearchCV  # ハイパーパラメータのチューニングを行うためのクラスをインポートします。

# 分類モデル
from sklearn.ensemble import GradientBoostingClassifier  # 勾配ブースティング分類器をインポートします。
from xgboost import XGBClassifier  # XGBoost分類器をインポートします。
from catboost import CatBoostClassifier  # CatBoost分類器をインポートします。

import nltk  # 自然言語処理のためのNLTKライブラリをインポートします。
from nltk.tokenize import word_tokenize, sent_tokenize  # 単語と文のトークン化を行うための関数をインポートします。
from nltk.corpus import stopwords  # ストップワードを提供するNLTKのコーパスをインポートします。
from nltk.stem import PorterStemmer  # ステミングに使用するポーターステマーをインポートします。
from nltk.stem import WordNetLemmatizer  # レmmatizationに使用するWordNetレmmatizerをインポートします。

from gensim.models import Word2Vec  # Word2Vecモデルをインポートします。
import spacy  # SpaCyライブラリをインポートします。
nlp = spacy.load('en_core_web_sm')  # 英語用のSpaCyモデルをロードします。

import re  # 正規表現処理のためのreライブラリをインポートします。
import string  # 文字列処理のためのstringライブラリをインポートします。

from warnings import filterwarnings  # 警告のフィルタリングを行うための関数をインポートします。
filterwarnings('ignore')  # 警告を無視するように設定します。
```

---The following area is a Code cell (cell numver is 3)---
```python
# データセットの読み込み
df_train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # 訓練データをCSVファイルから読み込みます。
df_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。
df_submission = df_test.copy()  # テストデータのコピーを提出用データフレームとして作成します。
sample_example = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルを読み込みます。
df_train.head()  # 訓練データの最初の5行を表示します。
```

---The following area is a Code cell (cell numver is 4)---
```python
# サンプル例の可視化
sample_example.head()  # サンプル提出ファイルの最初の5行を表示します。
```

---The following area is a Code cell (cell numver is 5)---
```python
# テストデータセットの表示
df_test.head()  # テストデータの最初の5行を表示します。
```

---The following area is a Code cell (cell numver is 6)---
```python
# データセットの形状を表示
print(f'Train dataset shape: {df_train.shape}')  # 訓練データの形状（行数と列数）を表示します。
print(f'Test dataset shape: {df_test.shape}')  # テストデータの形状（行数と列数）を表示します。
```

---The following area is a Code cell (cell numver is 7)---
```python
# データセットの情報を表示
df_train.info()  # 訓練データの情報（各列のデータ型や欠損値の数など）を表示します。
```

---The following area is a Code cell (cell numver is 8)---
```python
# 訓練データの欠損値を確認
df_train.isnull().sum()  # 訓練データの各列に存在する欠損値の数を表示します。
```

---The following area is a Code cell (cell numver is 9)---
```python
# テストデータの欠損値を確認
df_test.isnull().sum()  # テストデータの各列に存在する欠損値の数を表示します。
```

---The following area is a Code cell (cell numver is 10)---
```python
# テキストの前処理
def preprocess(string):  # 前処理を行う関数を定義します。
    strip = string.strip('[]')  # 文字列の両端から'[]'を削除します。
    parts = [x.strip('"') for x in strip.split('","')]  # 文字列を'","'で分割し、各部分から'"'を削除します。
    return ''.join(parts)  # 部分を結合して一つの文字列を返します。

columns = ['prompt', 'response_a', 'response_b']  # 前処理を行う列を指定します。

# 訓練データの各指定列に対して前処理を適用
for colum in df_train[columns]:
    df_train[colum] = df_train[colum].apply(preprocess)  # 各列に前処理関数を適用します。

# テストデータの各指定列に対して前処理を適用
for colum in df_test[columns]:
    df_test[colum] = df_test[colum].apply(preprocess)  # 各列に前処理関数を適用します。

df_train.head(2)  # 訓練データの最初の2行を表示します。
```

---The following area is a Code cell (cell numver is 11)---
```python
# データ探索のための新しい列を作成
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 勝者モデルを示す列を指定します。

def class_label(df):  # クラスラベルを設定する関数を定義します。
    df['class_label'] = None  # 新しい列'class_label'をNoneで初期化します。
    
    # データフレーム内の各行に対して処理を行います。
    for index, row in df.iterrows():
        for col in columns:  # 指定した各列をチェックします。
            if row[col] == 1:  # 列の値が1の場合
                df.at[index, 'class_label'] = col  # 'class_label'列に該当する列名を設定します。
                break  # 一度設定したらループを抜けます。

# データフレームに対して関数を呼び出す
class_label(df_train)  # 訓練データに対してクラスラベルを設定します。
```

---The following area is a Code cell (cell numver is 12)---
```python
# クラス番号を定義
class_number = {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 2}  # 勝者モデルのクラス番号を定義します。

df_train['class'] = df_train['class_label'].map(class_number)  # 'class_label'から対応するクラス番号を新しい列'class'にマッピングします。
df_train.head(2)  # 訓練データの最初の2行を表示します。
```

---The following area is a Code cell (cell numver is 13)---
```python
# 選ばれたモデルを決定する関数
def chose_model(row):  # 行に基づいて選ばれたモデルを返す関数を定義します。
    if row['class'] == 0:  # クラスが0の場合
        return row['model_a']  # model_aを返します。
    elif row['class'] == 1:  # クラスが1の場合
        return row['model_b']  # model_bを返します。
    else:
        return 'tie'  # それ以外（引き分け）の場合は'tie'を返します。

# データフレームの各行に対して関数を適用し、新しい列'chose_model'を作成
df_train['chose_model'] = df_train.apply(chose_model, axis=1)  # 各行に関数を適用して選ばれたモデルを列に設定します。
df_train.head(2)  # 訓練データの最初の2行を表示します。
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## 2 - データ探索
```

---The following area is a Code cell (cell numver is 15)---
```python
df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].mean()  # 各モデルの勝率の平均を計算して表示します。
```

---The following area is a Code cell (cell numver is 16)---
```python
# クラス分布の分析
# 目的: 基本がバランスされているかどうかを確認します。
fig = px.pie(df_train[['winner_model_a', 'winner_model_b', 'winner_tie']].mean(), names=df_train['class_label'], title='クラス分布')  # 各モデルの勝率に基づいて円グラフを作成します。
fig.update_layout(width=600, height=400)  # グラフのサイズを設定します。
fig.show()  # グラフを表示します。
```

---The following area is a Code cell (cell numver is 17)---
```python
# モデルの分布
# 目的: 比較における最も勝利したモデルを確認します。
df_models = df_train.groupby('chose_model')['class'].count().reset_index().sort_values(by='class', ascending=False)  # 選ばれたモデルごとにカウントし、降順にソートします。
df_models.rename(columns={'chose_model': 'model'}, inplace=True)  # 列名を' ch ose_model'から'model'に変更します。

# 結果のグラフ
fig = px.bar(df_models[1:11].sort_values(by='class'), y='model', x='class', title='最も選ばれたモデルTOP10', text_auto='.3s')  # 上位10モデルの棒グラフを作成します。
fig.update_layout(width=600, height=400)  # グラフのサイズを設定します。
fig.show()  # グラフを表示します。
```

---The following area is a Code cell (cell numver is 18)---
```python
# 語及されたモデルの選択
# 目的: これらのモデルの中で最も選ばれたものを確認します。
df_uni1 = df_train.groupby('model_a')['id'].count().reset_index().sort_values('id', ascending=False)  # model_aごとの出現回数をカウントし、降順でソートします。
df_uni1.rename(columns={'model_a': 'model'}, inplace=True)  # 列名を'model_a'から'model'に変更します。
df_uni2 = df_train.groupby('model_b')['id'].count().reset_index().sort_values('id', ascending=False)  # model_bごとの出現回数をカウントし、降順でソートします。
df_uni2.rename(columns={'model_b': 'model'}, inplace=True)  # 列名を'model_b'から'model'に変更します。

# model_aとmodel_bのデータをマージして統一データフレームを作成
df_uni = df_uni1.merge(df_uni2, how='left', on='model')  # 左外部結合でデータを統合します。
df_uni['total_mentions'] = df_uni['id_x'] + df_uni['id_y']  # id_xとid_yの合計を新しい列'total_mentions'に格納します。
mentions = df_uni['total_mentions'].sum()  # 総出現回数を計算します。
df_uni['%mentions'] = (df_uni['total_mentions'] / mentions) * 100  # 出現割合を計算します。

# モデルの選出割合を追加
df_uni = df_uni.merge(df_models, how='left', on='model')  # df_modelsとマージして選出回数を追加します。
df_uni['chose_in_mentions'] = (df_uni['class'] / df_uni['total_mentions']) * 100  # 選出回数の割合を計算します。
df_uni = df_uni.sort_values(by='chose_in_mentions', ascending=False)[:10]  # 選出回数の割合でソートし上位10件を取得します。
df_uni.head(3)  # 上位3件を表示します。
```

---The following area is a Code cell (cell numver is 19)---
```python
# 結果 - グラフ
fig = px.bar(df_uni.sort_values('chose_in_mentions'), y='model', x='chose_in_mentions', title='% 最も選ばれたモデルTOP10',
             text_auto='.3s')  # 上位10モデルの選出割合の棒グラフを作成します。
fig.update_layout(width=600, height=400)  # グラフのサイズを設定します。
fig.show()  # グラフを表示します。
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## 3 モデリング
```

---The following area is a Code cell (cell numver is 21)---
```python
%%time

df = df_train.copy()  # 訓練データのコピーを作成します。

# 単語処理のためのストップワードリストを取得
stopwords_list = stopwords.words('english')

def word_processing(text):  # テキストを処理する関数を定義します。
    '''テキストを処理します: 特殊文字を削除し、小文字に変換し、トークン化し、ストップワードを削除します'''
    text = re.sub(r'[/<>()|\+\-\$%&#@\'\"]+', ' ', text)  # 特殊文字を削除します。
    text = text.lower()  # 小文字に変換します。
    tokens = word_tokenize(text)  # テキストをトークン化します。
    filtered_tokens = [token for token in tokens if token not in stopwords_list]  # ストップワードを削除します。
    return filtered_tokens

# 特徴量エンジニアリング
def count_token(tokens):  # トークンの数をカウントする関数
    '''トークンの数をカウントします'''
    return len(tokens)

def diff_response(tokens1, tokens2):  # 二つの応答のトークン数の違いを計算する関数
    '''応答間のトークン数の違い'''
    len_1 = len(tokens1)
    len_2 = len(tokens2)
    diff = abs(len_1 - len_2)  # 二つの応答のトークン数の差の絶対値を計算します。
    return len_1, len_2, diff

def prompt_u_response(text1, text2):  # プロンプトと応答間の共通単語を計算する関数
    '''プロンプトと応答間の共通単語'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)  # 二つの集合の共通部分を計算します。
    return len(intersec)  # 共通部分の長さを返します。

def aUb(text1, text2):  # 応答間の共通単語を計算する関数
    '''応答間の共通単語'''
    conj1 = set(text1.split())
    conj2 = set(text2.split())
    intersec = conj1.intersection(conj2)  # 二つの集合の共通部分を計算します。
    return len(intersec)  # 共通部分の長さを返します。

def lexical_diversity(text):  # 単語の多様性を計算する関数
    '''ユニークな単語の比率'''
    return len(set(text)) / len(text) if text else 0  # ユニークな単語の数を全体の単語数で割ります。

def avg_words_per_sentence(text):  # 文章あたりの単語数の平均を計算する関数
    '''文あたりの平均単語数を計算します'''
    sentences = sent_tokenize(text)  # 文をトークン化します。
    word_count = sum(len(word_tokenize(sentence)) for sentence in sentences)  # 各文の単語数をカウントします。
    return word_count / len(sentences) if sentences else 0  # 平均を返します。

def sentence_diversity(text):  # 文の多様性を計算する関数
    '''文の多様性を計算します（文の長さのバリエーション）'''
    sentences = sent_tokenize(text)  # 文をトークン化します。
    lengths = [len(word_tokenize(sentence)) for sentence in sentences]  # 各文の単語数をカウントします。
    return len(set(lengths)) / len(lengths) if lengths else 0  # 文の長さのユニークな値の比率を返します。

def Simylarity(text1, text2):  # プロンプトと応答の類似度を計算する関数
    '''プロンプトと応答間の類似度を計算します'''
    tokenA = nlp(text1)  # テキストをSpaCyでトークン化します。
    tokenB = nlp(text2)  # テキストをSpaCyでトークン化します。
    return tokenA.similarity(tokenB)  # 類似度を計算して返します。

# プロンプトと応答間の類似度を新しい列に追加
df['similaty_promptUresponse_a'] = df.apply(lambda row: Simylarity(row['prompt'], row['response_a']), axis=1)
df['similaty_promptUresponse_b'] = df.apply(lambda row: Simylarity(row['prompt'], row['response_b']), axis=1)

# 指定した列に対して単語処理を適用
for col in ['prompt', 'response_a', 'response_b']:
    df[col] = df[col].apply(word_processing)  # 各列に前処理を適用します。
    df[f'{col}_count_token'] = df[col].apply(count_token)  # トークンの数をカウントします。

# 追加の特徴を適用
for index, row in df.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)  # トークンを結合してテキストを再構築します。
        
        df.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)  # 単語の多様性を計算して設定します。
        df.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)  # 文章あたりの平均単語数を計算して設定します。
        #df.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)  # キーワード使用の計算を行う（コメントアウト中）。
        df.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)  # 文の多様性を計算して設定します。
        
        # 既存の特徴も設定
        df.at[index, f'{col}_count_token'] = count_token(tokens)  # トークンのカウントを設定します。

# 訓練データフレームにおける差異関数を適用
for index, row in df.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)  # トークンを結合してテキストを再構築します。
    response_b_text = ' '.join(response_b_tokens)  # トークンを結合してテキストを再構築します。
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)  # 応答の長さと差を計算します。
    df.at[index, 'response_len_a'] = len_a  # 応答Aのトークン数を設定します。
    df.at[index, 'response_len_b'] = len_b  # 応答Bのトークン数を設定します。
    df.at[index, 'response_diff'] = diff  # 応答間のトークン数の差を設定します。
    
    common_words_ab = aUb(response_a_text, response_b_text)  # 応答間の共通単語数を計算します。
    df.at[index, 'common_words_ab'] = common_words_ab  # 共通単語数を設定します。
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)  # プロンプトのトークンを結合してテキストを再構築します。
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)  # プロンプトと応答Aの共通単語数を計算します。
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)  # プロンプトと応答Bの共通単語数を計算します。
    df.at[index, 'common_words_prompt_a'] = common_words_prompt_a  # 共通単語数を設定します。
    df.at[index, 'common_words_prompt_b'] = common_words_prompt_b  # 共通単語数を設定します。

df.head(2)  # 訓練データの最初の2行を表示します。
```

---The following area is a Code cell (cell numver is 22)---
```python
df[['similaty_promptUresponse_a', 'similaty_promptUresponse_b']].describe().T  # プロンプトと応答間の類似度の統計情報を表示します。
```

---The following area is a Code cell (cell numver is 23)---
```python
%%time

# 特徴量とターゲットの選択
X = df.drop(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie',
            'class_label', 'chose_model', 'class'], axis=1).values  # 特徴量を選択します。
y = df['class'].values  # ターゲットのクラスを選択します。

# 訓練データと検証データの分割
X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)  # データを80:20で分割します。

# 使用するモデル
models = {
    'XGBoost': XGBClassifier(random_state=42),  # XGBoostモデルを定義します。
    'GradientBoosting': GradientBoostingClassifier(random_state=42),  # 勾配ブースティングモデルを定義します。
    'CatBoost': CatBoostClassifier(random_state=42, verbose=False)  # CatBoostモデルを定義します（出力を抑制します）。
}

# モデルのトレーニング
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # 層別交差検証を設定します。

# メトリクス
result_loss = []  # 各モデルのロスを記録するリスト
result_models = []  # 各モデルの結果を記録するリスト

# 各モデルに対するトレーニング
for model_name, model in models.items():
    i = 0
    print(f'Model {model_name}')  # 現在のモデル名を表示します。

    model_losses = []  # ロスを記録するリスト
    
    for train_index, valid_index in skf.split(X, y):  # クロスバリデーションの分割
        X_train_fold, X_test_fold = X[train_index], X[valid_index]  # 学習データと検証データを分割します。
        y_train_fold, y_test_fold = y[train_index], y[valid_index]  # ターゲットを分割します。
        
        # モデルのトレーニング
        model.fit(X_train_fold, y_train_fold)  # 学習データでモデルを訓練します。
        
        # メトリクスの評価
        y_test_pred_proba = model.predict_proba(X_test_fold)  # 検証データに対する予測確率を計算します。
        loss = metrics.log_loss(y_test_fold, y_test_pred_proba)  # ロスを計算します。
        model_losses.append(loss)  # ロスをリストに追加します。
        
        # 結果を表示
        print(f'Fold {i} | Log Loss: {loss}')  # 各フォールドのロスを表示します。
        i += 1
        
    mean_loss = np.mean(model_losses)  # 各モデルの平均ロスを計算します。
    result_loss.append(model_losses)  # モデルごとのロスを結果に追加します。
    result_models.append((model_name, model_losses))  # モデル名とロスをペアで結果に追加します。

    print(f'Mean Loss for model {model_name} is {mean_loss}')  # 各モデルの平均ロスを表示します。
    print('---' * 30)  # 区切り線を表示します。
```

---The following area is a Code cell (cell numver is 24)---
```python
# 結果のプロット
data_dict = {'CV': list(range(0, 5))}  # クロスバリデーションのフォールド番号を作成します。

for model, values in result_models:  # 各モデルとその値のペアをループします。
    data_dict[model] = values  # モデル名をキーとしてロスの値を辞書に追加します。

results_df = pd.DataFrame(data_dict)  # 辞書からデータフレームを作成します。

# モデルロスの線グラフを作成
fig = px.line(results_df, x='CV', y=['XGBoost', 'GradientBoosting', 'CatBoost'], title='モデルロスの比較')
fig.update_layout(width=600, height=400)  # グラフのサイズを設定します。
fig.show()  # グラフを表示します。
```

---The following area is a Code cell (cell numver is 25)---
```python
%%time
# 最良のモデルはGradientBoostingです
# ハイパーパラメータのチューニングのためにグリッドサーチを使用します
model_grad = GradientBoostingClassifier()  # GradientBoostingのインスタンスを作成します。
params = {  # ハイパーパラメータの候補を設定します。
    'n_estimators': [100, 150, 170],  # 決定木の数の候補リスト。
    'max_depth': [1, 3, 5],  # 各決定木の最大深さの候補リスト。
    'learning_rate': [0.1, 0.01]  # 学習率の候補リスト。
}

# グリッドサーチの設定
grid = GridSearchCV(model_grad, params, n_jobs=-1, cv=5, scoring='neg_log_loss', verbose=False)  # グリッドサーチを設定します。
grid.fit(X_train, y_train)  # 訓練データでグリッドサーチを実行します。
```

---The following area is a Code cell (cell numver is 26)---
```python
print(f'最良のハイパーパラメータは {grid.best_params_} です。')  # 最良のハイパーパラメータを表示します。
```

---The following area is a Code cell (cell numver is 27)---
```python
# 最良のモデルを選択
best_model = grid.best_estimator_  # グリッドサーチで得られた最良のモデルを選択します。
```

---The following area is a Code cell (cell numver is 28)---
```python
# 最良モデルの評価
y_predict_valid_prob = best_model.predict_proba(X_valid)  # 検証データに対する予測確率を計算します。

logloss = metrics.log_loss(y_valid, y_predict_valid_prob)  # 検証データに対するロス（LogLoss）を計算します。

print(f'グリッドサーチによる最良モデルのLogLoss: {logloss}')  # 最良モデルのロスを表示します。
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
グリッドサーチによる改善は見られませんでした。
```

---The following area is a Code cell (cell numver is 30)---
```python
# 特徴量の重要度
features_importance = best_model.feature_importances_  # 最良モデルから特徴量の重要度を取得します。
features_names = ['prompt_count_token', 'response_a_count_token',
       'response_b_count_token', 'response_a_lexical_diversity',
       'response_a_avg_words_per_sentence', 'response_a_sentence_diversity',
       'response_b_lexical_diversity', 'response_b_avg_words_per_sentence',
       'response_b_sentence_diversity', 'response_len_a', 'response_len_b',
       'response_diff', 'common_words_ab', 'common_words_prompt_a',
       'common_words_prompt_b', 'similaty_promptUresponse_a', 'similaty_promptUresponse_b']  # 特徴量名のリストを定義します。

fig = px.bar(y=features_names, x=features_importance, color=features_importance, title='特徴量の重要度')  # 特徴量の重要度を棒グラフで可視化します。
scale = px.colors.sequential.BuGn  # カラーグラデーションを設定します。
fig.update_traces(marker=dict(colorscale=scale))  # グラフの色を設定します。
fig.update_layout(width=800, height=500)  # グラフのサイズを設定します。
fig.show()  # グラフを表示します。
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
上記のグラフでは、モデルにおける最も重要な特徴量を確認できます。
```

---The following area is a Code cell (cell numver is 32)---
```python
# 予測のためのテストデータの準備
df_test['similaty_promptUresponse_a'] = df_test.apply(lambda row: Simylarity(row['prompt'], row['response_a']), axis=1)  # プロンプトと応答A間の類似度を計算します。
df_test['similaty_promptUresponse_b'] = df_test.apply(lambda row: Simylarity(row['prompt'], row['response_b']), axis=1)  # プロンプトと応答B間の類似度を計算します。

# 指定した列に対して単語処理を適用
for col in ['prompt', 'response_a', 'response_b']:
    df_test[col] = df_test[col].apply(word_processing)  # 各列に前処理を適用します。
    df_test[f'{col}_count_token'] = df_test[col].apply(count_token)  # トークンの数をカウントします。

# 追加の特徴を適用
for index, row in df_test.iterrows():
    for col in ['response_a', 'response_b']:
        tokens = row[col]
        text = ' '.join(tokens)  # トークンを結合してテキストを再構築します。
        
        df_test.at[index, f'{col}_lexical_diversity'] = lexical_diversity(tokens)  # 単語の多様性を計算して設定します。
        df_test.at[index, f'{col}_avg_words_per_sentence'] = avg_words_per_sentence(text)  # 文章あたりの平均単語数を計算して設定します。
        # df_test.at[index, f'{col}_keyword_usage'] = keyword_usage(text, keywords)  # キーワード使用の計算を行う（コメントアウト中）。
        df_test.at[index, f'{col}_sentence_diversity'] = sentence_diversity(text)  # 文の多様性を計算して設定します。
        
        # 既存の特徴も設定
        df_test.at[index, f'{col}_count_token'] = count_token(tokens)  # トークンのカウントを設定します。

# テストデータフレームにおける差異関数を適用
for index, row in df_test.iterrows():
    response_a_tokens = row['response_a']
    response_b_tokens = row['response_b']
    response_a_text = ' '.join(response_a_tokens)  # トークンを結合してテキストを再構築します。
    response_b_text = ' '.join(response_b_tokens)  # トークンを結合してテキストを再構築します。
    
    len_a, len_b, diff = diff_response(response_a_tokens, response_b_tokens)  # 応答の長さと差を計算します。
    df_test.at[index, 'response_len_a'] = len_a  # 応答Aのトークン数を設定します。
    df_test.at[index, 'response_len_b'] = len_b  # 応答Bのトークン数を設定します。
    df_test.at[index, 'response_diff'] = diff  # 応答間のトークン数の差を設定します。
    
    common_words_ab = aUb(response_a_text, response_b_text)  # 応答間の共通単語数を計算します。
    df_test.at[index, 'common_words_ab'] = common_words_ab  # 共通単語数を設定します。
    
    prompt_tokens = row['prompt']
    prompt_text = ' '.join(prompt_tokens)  # プロンプトのトークンを結合してテキストを再構築します。
    common_words_prompt_a = prompt_u_response(prompt_text, response_a_text)  # プロンプトと応答Aの共通単語数を計算します。
    common_words_prompt_b = prompt_u_response(prompt_text, response_b_text)  # プロンプトと応答Bの共通単語数を計算します。
    df_test.at[index, 'common_words_prompt_a'] = common_words_prompt_a  # 共通単語数を設定します。
    df_test.at[index, 'common_words_prompt_b'] = common_words_prompt_b  # 共通単語数を設定します。

df_test.drop(columns=['id', 'prompt', 'response_a', 'response_b'], axis=1, inplace=True)  # 不要な列を削除します。
df_test.head()  # テストデータの最初の5行を表示します。
```

---The following area is a Code cell (cell numver is 33)---
```python
# テストデータに対する予測確率を計算
y_sub_proba = best_model.predict_proba(df_test)  # 最良モデルを使用してテストデータに対する予測確率を計算します。
y_sub_proba  # 予測確率を表示します。
```

---The following area is a Code cell (cell numver is 34)---
```python
# 提出用データフレームの作成
submission = pd.DataFrame({
    'id': df_submission['id'],  # 提出用のID列を設定します。
    'winner_model_a': y_sub_proba[:, 0],  # モデルAの勝者確率を設定します。
    'winner_model_b': y_sub_proba[:, 1],  # モデルBの勝者確率を設定します。
    'winner_tie': y_sub_proba[:, 2]  # 引き分けの確率を設定します。
})

submission.head()  # 提出用データフレームの最初の5行を表示します。
```

---The following area is a Code cell (cell numver is 35)---
```python
# 提出ファイルの保存
submission.to_csv('/kaggle/working/submission.csv', index=False)  # 提出用データフレームをCSVファイルとして保存します。
```

** @@@ Jupyter Notebook numver 30, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するためのモデルを構築するプロセスを示しています。Notebookは、様々なデータ処理と機械学習手法を用いて、与えられたプロンプトに対する異なるチャットボットの応答の好みを予測する問題に取り組んでいます。

### 主な内容と手法

1. **データの読み込みと前処理**:
   - データは`pandas`を用いてCSVファイルから読み込まれ、必要に応じてトレーニングデータの一部をサブセットとして利用します。
   - 特定の文字列処理を行い、プロンプトや応答から余分な文字を取り除く`process`関数が定義されています。

2. **特徴量エンジニアリング**:
   - `Preprocessor`クラスでは、テキスト間のコサイン類似度やジャッカード類似度の計算、n-グラムの生成、改行や引用符のカウント、トークン化など、さまざまなテキスト特徴量を計算します。
   - 各応答とプロンプトの類似性を数値化することに焦点をあて、生成した特徴量を基に予測モデルを訓練します。

3. **モデルの構築と評価**:
   - モデルには`XGBoost`ライブラリを使用し、多クラス分類器を訓練します。
   - `StratifiedKFold`を用いてクロスバリデーションを行い、各フォールドの対数損失（log loss）を評価指標として使用します。

4. **結果の可視化と提出ファイルの作成**:
   - 各特徴量の重要度を計算し、`matplotlib`および`seaborn`を使って重要度のバープロットを作成します。
   - 最後に、テストデータに基づいた予測結果を`submission.csv`としてエクスポートし、コンペティションへの提出を可能にします。

### 使用ライブラリ
- `numpy`, `pandas`: データ操作
- `nltk`: テキスト処理
- `matplotlib`, `seaborn`: データ可視化
- `xgboost`: 機械学習モデル
- `sklearn`: モデル評価とデータ前処理

このNotebookは、機械学習における特徴量エンジニアリングの重要性を示すとともに、フレームワークを利用して有効な予測モデルを構築する一連の流れを具体化したものとなっています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import gc
import os
import re
import numpy as np
import pandas as pd

import nltk
from nltk.util import ngrams
from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns

import xgboost as xgb
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
```

---The following area is a Code cell (cell numver is 2)---
```python
class config:
    root = "/kaggle/input/lmsys-chatbot-arena/"
    train_path = os.path.join(root, "train.csv")  # トレーニングデータのパス
    test_path = os.path.join(root, "test.csv")    # テストデータのパス
    sample_submission_path = os.path.join(root, "sample_submission.csv")  # サンプル提出データのパス
    seed = 42  # 乱数シードを42に設定
    n_splits = 10  # クロスバリデーションの分割数を10に設定
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv(config.train_path)  # トレーニングデータを読み込む
test = pd.read_csv(config.test_path)    # テストデータを読み込む
sample_submission = pd.read_csv(config.sample_submission_path)  # サンプル提出データを読み込む

if test.shape[0] < 10:  # テストデータのサンプル数が10未満の場合
    train = train.iloc[:10000]  # 最初の10000行のみをトレーニングデータに使用
    
def process(input_str):  # 入力文字列を処理する関数
    stripped_str = input_str.strip('[]')  # 角括弧を除去
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 行を分割し、不要なダブルクオートを除去
    return ' '.join(sentences)  # 処理した文を空白区切りで結合して返す

train["prompt"] = train["prompt"].apply(process)  # "prompt"列を処理
train["response_a"] = train["response_a"].apply(process)  # "response_a"列を処理
train["response_b"] = train["response_b"].apply(process)  # "response_b"列を処理

test["prompt"] = test["prompt"].apply(process)  # テストデータの"prompt"列を処理
test["response_a"] = test["response_a"].apply(process)  # テストデータの"response_a"列を処理
test["response_b"] = test["response_b"].apply(process)  # テストデータの"response_b"列を処理

print(f"train shape: {train.shape}")  # トレーニングデータの形状を表示
print(f"test shape: {test.shape}")  # テストデータの形状を表示
print("-"*90)
print(f"train missing values: {train.isnull().sum().sum()}")  # トレーニングデータの欠損値を表示
print(f"test missing values: {test.isnull().sum().sum()}")  # テストデータの欠損値を表示
print("-"*90)

train.head()  # トレーニングデータの先頭を表示
```

---The following area is a Code cell (cell numver is 4)---
```python
class Preprocessor:

    def cosine_sim(self, text1: str, text2: str):  # コサイン類似度を計算するメソッド
        try:
            vectorizer = TfidfVectorizer(ngram_range=(1, 3))  # TF-IDFベクトル化器を作成（1～3グラム）
            vectorizer.fit([text1, text2])  # テキストをフィッティング
            output = vectorizer.transform([text1, text2]).toarray()  # TF-IDF行列を生成
            cos_sim = cosine_similarity(output)  # コサイン類似度を計算
            return cos_sim[0][1]  # 二つのテキスト間のコサイン類似度を返す
        except:
            return np.nan  # エラーが発生した場合はNaNを返す

    def jaccard_sim(self, text1: str, text2: str):  # ジャッカード類似度を計算するメソッド
        set1 = set(text1.split())  # text1を単語に分割しセットに変換
        set2 = set(text2.split())  # text2を単語に分割しセットに変換
        intersection = set1.intersection(set2)  # 二つのセットの積集合を計算
        union = set1.union(set2)  # 二つのセットの和集合を計算
        return len(intersection) / len(union)  # ジャッカード類似度を計算して返す
    
    def count_new_lines(self, text: str) -> int:  # テキスト内の改行数をカウントするメソッド
        return text.count('\\n')  # 改行の数を返す
    
    def count_quotes(self, text: str) -> int:  # テキスト内の引用符の数をカウントするメソッド
        single_quote_pattern = r"'(.*?)'"  # シングルクォートのパターン
        double_quote_pattern = r'"(.*?)"'  # ダブルクォートのパターン
        single_quotes = re.findall(single_quote_pattern, text)  # シングルクォートを探す
        double_quotes = re.findall(double_quote_pattern, text)  # ダブルクォートを探す
        total_quotes = len(single_quotes) + len(double_quotes)  # 合計の引用符の数をカウント
        return len(single_quotes) + len(double_quotes)  # 合計の引用符の数を返す

    def tokenize(self, text: str):  # テキストをトークン化するメソッド
        return nltk.word_tokenize(text.lower())  # 小文字に変換してトークン化して返す

    def generate_ngrams(self, text: str, n: int):  # n-グラムを生成するメソッド
        tokens = self.tokenize(text)  # テキストをトークン化
        return list(ngrams(tokens, n))  # n-グラムのリストを生成して返す

    def count_ngram_overlaps(self, text1: str, text2: str, n: int) -> int:  # n-グラムの重複数をカウントするメソッド
        try:
            ngrams1 = self.generate_ngrams(text1, n)  # text1からn-グラムを生成
            ngrams2 = self.generate_ngrams(text2, n)  # text2からn-グラムを生成
            counter1 = Counter(ngrams1)  # text1のn-グラムをカウント
            counter2 = Counter(ngrams2)  # text2のn-グラムをカウント
            overlap = counter1 & counter2  # カウンターの共通部分を計算
            overlap_count = sum(overlap.values())  # 重複の合計数を計算
            return overlap_count  # 重複の数を返す
        except:
            return 0  # エラーが発生した場合は0を返す
        
    def run(self, data: pd.DataFrame) -> pd.DataFrame:  # データを処理するメソッド
        data["respa_respb_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 1), axis=1)  # response_aとresponse_bのユニグラム重複を計算
        data["respa_respb_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 2), axis=1)  # response_aとresponse_bのバイグラム重複を計算
        data["respa_respb_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["response_b"], 3), axis=1)  # response_aとresponse_bのトライグラム重複を計算

        data["respa_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 1), axis=1)  # response_aとpromptのユニグラム重複を計算
        data["respa_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 2), axis=1)  # response_aとpromptのバイグラム重複を計算
        data["respa_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_a"], x["prompt"], 3), axis=1)  # response_aとpromptのトライグラム重複を計算

        data["respb_prompt_overlap_unigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 1), axis=1)  # response_bとpromptのユニグラム重複を計算
        data["respb_prompt_overlap_bigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 2), axis=1)  # response_bとpromptのバイグラム重複を計算
        data["respb_prompt_overlap_trigram"] = data.apply(lambda x: self.count_ngram_overlaps(x["response_b"], x["prompt"], 3), axis=1)  # response_bとpromptのトライグラム重複を計算
        
        data["respa_len"] = data["response_a"].apply(lambda x: len(self.tokenize(x)))  # response_aのトークン数を計算
        data["respb_len"] = data["response_b"].apply(lambda x: len(self.tokenize(x)))  # response_bのトークン数を計算
        data["prompt_len"] = data["prompt"].apply(lambda x: len(self.tokenize(x)))  # promptのトークン数を計算
        
        data["respa_new_lines"] = data["response_a"].apply(lambda x: self.count_new_lines(x))  # response_aの改行数を数える
        data["respb_new_lines"] = data["response_b"].apply(lambda x: self.count_new_lines(x))  # response_bの改行数を数える
        data["prompt_new_lines"] = data["prompt"].apply(lambda x: self.count_new_lines(x))  # promptの改行数を数える
        
        data["respa_prompt_len_ratio"] = data["respa_len"] / data["prompt_len"]  # response_aの長さとpromptの比率を計算
        data["respb_prompt_len_ratio"] = data["respb_len"] / data["prompt_len"]  # response_bの長さとpromptの比率を計算
        data["respa_respb_len_ratio"] = data["respa_len"] / data["respb_len"]  # response_aとresponse_bの長さの比率を計算
        
        data["respa_respb_len_diff"] = data["respa_len"] - data["respb_len"]  # response_aとresponse_bの長さの差を計算
        data["respa_prompt_len_diff"] = data["respa_len"] - data["prompt_len"]  # response_aとpromptの長さの差を計算
        data["respb_prompt_len_diff"] = data["respb_len"] - data["prompt_len"]  # response_bとpromptの長さの差を計算
        
        data["respa_prompt_overlap_unigram_len_ratio"] = data["respa_prompt_overlap_unigram"] / data["prompt_len"]  # ユニグラム重複とpromptの長さの比率を計算
        data["respa_prompt_overlap_bigram_len_ratio"] = data["respa_prompt_overlap_bigram"] / data["prompt_len"]  # バイグラム重複とpromptの長さの比率を計算
        data["respa_prompt_overlap_trigram_len_ratio"] = data["respa_prompt_overlap_trigram"] / data["prompt_len"]  # トライグラム重複とpromptの長さの比率を計算

        data["respb_prompt_overlap_unigram_len_ratio"] = data["respb_prompt_overlap_unigram"] / data["prompt_len"]  # ユニグラム重複とpromptの長さの比率を計算
        data["respb_prompt_overlap_bigram_len_ratio"] = data["respb_prompt_overlap_bigram"] / data["prompt_len"]  # バイグラム重複とpromptの長さの比率を計算
        data["respb_prompt_overlap_trigram_len_ratio"] = data["respb_prompt_overlap_trigram"] / data["prompt_len"]  # トライグラム重複とpromptの長さの比率を計算
        
        data["overlap_unigram_diff"] = data["respa_prompt_overlap_unigram"] - data["respb_prompt_overlap_unigram"]  # ユニグラム重複の差を計算
        data["overlap_bigram_diff"] = data["respa_prompt_overlap_bigram"] - data["respb_prompt_overlap_bigram"]  # バイグラム重複の差を計算
        data["overlap_trigram_diff"] = data["respa_prompt_overlap_trigram"] - data["respb_prompt_overlap_trigram"]  # トライグラム重複の差を計算
        
        data["overlap_unigram_ratio"] = data["respb_prompt_overlap_unigram"] / data["respa_prompt_overlap_unigram"]  # ユニグラム重複の比率を計算
        data["overlap_bigram_ratio"] = data["respb_prompt_overlap_bigram"] / data["respa_prompt_overlap_bigram"]  # バイグラム重複の比率を計算
        data["overlap_trigram_ratio"] = data["respb_prompt_overlap_trigram"] / data["respa_prompt_overlap_trigram"]  # トライグラム重複の比率を計算 
        
        data["respa_quotes"] = data["response_a"].apply(lambda x: self.count_quotes(x))  # response_aの引用符の数をカウント
        data["respb_quotes"] = data["response_b"].apply(lambda x: self.count_quotes(x))  # response_bの引用符の数をカウント
        data["prompt_quotes"] = data["prompt"].apply(lambda x: self.count_quotes(x))  # promptの引用符の数をカウント
        
        data["respa_respb_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["response_b"]), axis=1)  # response_aとresponse_bのコサイン類似度を計算
        data["respa_respb_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["response_b"]), axis=1)  # response_aとresponse_bのジャッカード類似度を計算
        
        data["respa_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_a"], x["prompt"]), axis=1)  # response_aとpromptのコサイン類似度を計算
        data["respa_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_a"], x["prompt"]), axis=1)  # response_aとpromptのジャッカード類似度を計算
        
        data["respb_prompt_cosine_sim"] = data.apply(lambda x: self.cosine_sim(x["response_b"], x["prompt"]), axis=1)  # response_bとpromptのコサイン類似度を計算
        data["respb_prompt_jaccard_sim"] = data.apply(lambda x: self.jaccard_sim(x["response_b"], x["prompt"]), axis=1)  # response_bとpromptのジャッカード類似度を計算
        
        data["jaccard_sim_diff"] = data["respa_prompt_jaccard_sim"] - data["respb_prompt_jaccard_sim"]  # ジャッカード類似度の差を計算
        data["jaccard_sim_ratio"] = data["respb_prompt_jaccard_sim"] / data["respa_prompt_jaccard_sim"]  # ジャッカード類似度の比率を計算
        
        return data  # 処理後のデータを返す
```

---The following area is a Code cell (cell numver is 5)---
```python
%%time
preprocessor = Preprocessor()  # Preprocessorのインスタンスを生成
train = preprocessor.run(train)  # トレーニングデータに対して前処理を実行
test = preprocessor.run(test)  # テストデータに対して前処理を実行
train.head()  # トレーニングデータの先頭を表示
```

---The following area is a Code cell (cell numver is 6)---
```python
drop_cols = ["id", "response_a", "response_b", "prompt"]  # 除外する列のリスト
target_cols = ["winner_model_a", "winner_model_b", "winner_tie"]  # ターゲットとなる列のリスト
target = "target"  # ターゲットの名前

train[target] = np.nan  # ターゲット列をNaNで初期化
for idx, t in enumerate(target_cols):  # ターゲット列をループ
    train.loc[train[t] == 1, target] = idx  # 勝者モデルをターゲットとして設定
train[target] = train[target].astype("int32")  # ターゲット列の型を整数型に変換
    
train.head()  # トレーニングデータの先頭を表示
```

---The following area is a Code cell (cell numver is 7)---
```python
X = train.drop(columns=target_cols + drop_cols + [target] + ["model_a", "model_b"], axis=1)  # 特徴量Xを定義（不要な列を除外）
y = train[target]  # ターゲットyを定義
X_test = test.drop(columns=drop_cols, axis=1)  # テストデータから特徴量X_testを定義

X = X.replace([-np.inf, np.inf], np.nan)  # 特徴量Xの無限値をNaNに置き換え
X_test = X_test.replace([-np.inf, np.inf], np.nan)  # テストデータの無限値をNaNに置き換え
```

---The following area is a Code cell (cell numver is 8)---
```python
cv = StratifiedKFold(n_splits=config.n_splits, shuffle=True, random_state=config.seed)  # ストラティファイドKフォールドの設定
test_preds = np.zeros(shape=(X_test.shape[0], y.nunique()))  # テスト予測の初期化
cv_scores = list()  # クロスバリデーションスコアのリスト

features = X.columns.tolist()  # 特徴量のリストを取得
feat_imp_df = pd.DataFrame({"feature": features})  # 特徴量の重要度を格納するデータフレームを作成

for idx, (train_idx, val_idx) in enumerate(cv.split(X, y)):  # Kフォールド分割に基づいて訓練インデックスと検証インデックスを取得
    print(f"| Fold {idx+1} |".center(90, "="))  # 現在のフォールドの表示
    X_train, y_train = X.loc[train_idx], y.loc[train_idx]  # 訓練データとラベルの取得
    X_val, y_val = X.loc[val_idx], y.loc[val_idx]  # 検証データとラベルの取得

    print(f'train: {X_train.shape}')  # 訓練データの形状を表示
    print(f'val: {X_val.shape}')  # 検証データの形状を表示
    
    model = xgb.XGBClassifier(  # XGBoostの分類器を定義
        objective='multi:softprob',  # 目的関数を多クラスの確率に設定
        num_class=3,  # クラス数を3に設定
        eval_metric='mlogloss',  # 評価指標を対数損失に設定
        subsample=0.8,  # サンプリング率を0.8に設定
        n_estimators=650,  # 弱学習器の数を650に設定
        learning_rate=0.045,  # 学習率を0.045に設定
        max_depth=5,  # 木の最大深さを5に設定
        random_state=config.seed  # 乱数シードを設定
    )
    
    model.fit(  # モデルの訓練
        X_train,
        y_train,
        eval_set=[(X_train, y_train), (X_val, y_val)],  # 評価セットを訓練データと検証データに設定
        early_stopping_rounds=75,  # 75ラウンドで早期停止
        verbose=75  # 75回ごとに出力
    )
    
    val_preds = model.predict_proba(X_val)  # 検証データに対する確率予測
    val_log_loss = log_loss(y_val, val_preds, eps="auto")  # 検証データに対する対数損失を計算
    print(f"val log loss: {val_log_loss:.5f}")  # 検証データの対数損失を表示
    cv_scores.append(val_log_loss)  # クロスバリデーションスコアを追加
    
    test_preds += model.predict_proba(X_test) / cv.get_n_splits()  # 各フォールドのテスト予測を累積
    
    feat_imp_df = feat_imp_df.merge(  # 特徴量の重要度をデータフレームに追加
        pd.DataFrame(
            {
                "feature": features,  # 特徴量名
                f"fold_{idx+1}_feat_imp": model.feature_importances_,  # 各フォールドの特徴量重要度
            }
        ),
        on=["feature"],
        how="left",
    )

print("="*90)  # 区切り線を表示
print(f"CV: {np.mean(cv_scores):.5f}")  # クロスバリデーションの平均スコアを表示

feat_imp_df["avg_importance"] = feat_imp_df.iloc[:, 1:].mean(axis=1)  # 各特徴量の平均重要度を計算
plt.figure(figsize=(12, 10))  # プロットサイズを設定
sns.barplot(  # バープロットを作成
    data=feat_imp_df.sort_values(by="avg_importance", ascending=False).iloc[:50],  # 重要度が高い50の特徴量を表示
    x="avg_importance",
    y="feature",
    color="royalblue",
    width=0.75,
)
plt.title("Average Feature Importances of All Folds", size=12)  # タイトルを設定
plt.show()  # プロットを表示
```

---The following area is a Code cell (cell numver is 9)---
```python
for idx, t in enumerate(target_cols):  # ターゲット列をループ
    sample_submission[t] = test_preds[:, idx]  # ターゲット列にテスト予測を格納
sample_submission.head()  # サンプル提出データの先頭を表示
```

---The following area is a Code cell (cell numver is 10)---
```python
sample_submission.to_csv("submission.csv", index=False)  # 提出ファイルとしてCSVに保存
```

** @@@ Jupyter Notebook numver 31, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、大規模言語モデル（LLM）を利用して、ユーザーからのプロンプトに対する2つの異なる応答モデルのどちらが好まれるかを予測する問題に取り組んでいます。競技はKaggleの「LMSYS - Chatbot Arena」コンペティションの一環であり、具体的にはユーザーの好みに基づく会話システムを改善するための機械学習モデルの開発が目指されています。

### 使用する手法とライブラリ
ノートブックでは、次のような手法とライブラリが使用されています：

1. **ライブラリのインポート**:
   - 主に`numpy`, `pandas`, `torch`, `transformers`, `sklearn`, `keras`などのライブラリを使用して、データの処理、モデルの構築、トレーニングを行っています。

2. **トークナイザー**:
   - `transformers` ライブラリの `AutoTokenizer` を用い、テキストデータをトークン化します。特に、Qwen2のトークナイザーを使用し、入力シーケンスの最大長やパディングトークンの設定を行います。

3. **データ処理**:
   - トレーニングデータの前処理として、プロンプトと応答のクリーンアップを行い、さらにテキストを結合してモデルの入力用データを準備します。

4. **モデルの構築**:
   - CNN（畳み込みニューラルネットワーク）とLSTM（長短期記憶ネットワーク）のハイブリッドモデルを作成します。このモデルは、テキストの埋め込みを用いて、アテンションマスクを考慮した上での分類を行います。

5. **トレーニングと評価**:
   - モデルを訓練する際に、早期停止やモデルチェックポイントを使用して最適なパフォーマンスを保証します。訓練データを用いてモデルをトレーニングし、バリデーションデータで評価します。

6. **予測**:
   - テストデータに対してモデルを適用し、それぞれの応答モデルの勝者を予測します。最終的には、結果をCSVファイルとして出力します。

ノートブックの結論部分では、さらなるデータの追加や異なるハイパーパラメータの調整によってモデルの最適化の余地があることが示唆されています。全体として、ユーザーの好みを反映した会話生成モデルの構築に向けた包括的なアプローチが取られています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel
import torch.nn.functional as F
np.random.seed(1337)  # 乱数のシードを1337に設定します。
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# トークナイザー
```

---The following area is a Code cell (cell numver is 4)---
```python
tokenizer = AutoTokenizer.from_pretrained("qwen/Qwen2-7B-Instruct")
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンをEOSトークンに設定
tokenizer.padding_side = 'right'  # パディングを右側に設定
tokenizer.add_eos_token = True  # EOSトークンを追加

# 推論時にオフラインでトークナイザーを読み込むために保存
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 5)---
```python
# トークンの長さを取得するユーティリティ関数
def get_token_lengths(texts):
    # テキストごとにトークン化し、input_idsを取得
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # 各テキストのinput_idsの長さを返す
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# トレーニングデータの準備
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込む

def process(input_str):
    stripped_str = input_str.strip('[]')  # 文字列からブラケットを削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文を分割してクオートを削除
    return  ' '.join(sentences)  # 文をスペースで結合して返す

train.loc[:, 'prompt'] = train['prompt'].apply(process)  # プロンプトを処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # モデルAの応答を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # モデルBの応答を処理
```

---The following area is a Code cell (cell numver is 8)---
```python
train.head(5)  # データフレームの先頭5行を表示
```

---The following area is a Code cell (cell numver is 9)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])  # 4番目のテキストを表示
```

---The following area is a Code cell (cell numver is 10)---
```python
# トレーニングデータセットの50％のみを使用
train = train[:int(len(train) * 1)]

train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # 各テキストのトークン数を取得

# モデル用のラベルを準備
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# データを表示
display(train.head())
```

---The following area is a Code cell (cell numver is 11)---
```python
train.label.value_counts()  # 各ラベルのカウントを表示
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークンのカウントの要約表示
display(train['token_count'].describe().to_frame().astype(int))
```

---The following area is a Code cell (cell numver is 13)---
```python
# データの90%をカバーするトークンの長さを取得。ここでは1024の長さを使用します！
np.percentile(train['token_count'], 90)
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# トークン化
```

---The following area is a Code cell (cell numver is 15)---
```python
# データをトークン化
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')  # numpy配列として返す

# INPUT_IDSはトークンIDです
INPUT_IDS = tokens['input_ids']
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS = tokens['attention_mask']
# テキストのラベル
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDSの形状: {INPUT_IDS.shape}, ATTENTION_MASKSの形状: {ATTENTION_MASKS.shape}')  # 形状を表示
print(f'LABELSの形状: {LABELS.shape}')  # ラベルの形状を表示
```

---The following area is a Code cell (cell numver is 16)---
```python
max_features = 14300  # 最大特徴量数
maxlen = 1024  # 最大トークン数
batch_size = 16  # バッチサイズ
embedding_dims = 100  # 埋め込み次元数
nb_filter = 150  # フィルター数
filter_length = 3  # フィルターの長さ
hidden_dims = 100  # 隠れ層の次元数
nb_epoch = 100  # エポック数
```

---The following area is a Code cell (cell numver is 17)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta,Adamax
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda,Input,TimeDistributed,Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
num_samples = INPUT_IDS.shape[0]  # サンプル数を取得

# X_validのサンプル数（X_trainの20%）
num_valid_samples = int(num_samples * 0.2)

# X_trainのインデックスをシャッフル
indices = np.random.permutation(num_samples)

# 最初の20%のインデックスをX_validのインデックスとして選択
valid_indices = indices[:num_valid_samples]

# 残りのインデックスをX_trainのインデックスとして選択
train_indices = indices[num_valid_samples:]

# 選ばれたインデックスから新しいX_validとX_trainを作成
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=maxlen)  # パディングを適用
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=maxlen)  # アテンションマスクにもパディングを適用
y_train = LABELS[train_indices]  # トレーニングラベル

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=maxlen)  # バリデーションデータのパディング
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=maxlen)  # バリデーションアテンションマスクのパディング
y_valid = LABELS[valid_indices]  # バリデーションラベル
```

---The following area is a Code cell (cell numver is 18)---
```python
X_train = np.array(X_train)  # X_trainをnumpy配列に変換
y_train = np.array(y_train)  # y_trainをnumpy配列に変換
X_valid = np.array(X_valid)  # X_validをnumpy配列に変換
y_valid = np.array(y_valid)  # y_validをnumpy配列に変換
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# モデルの定義
```

---The following area is a Code cell (cell numver is 20)---
```python
'''この例は、テキスト分類におけるConvolution1Dの使用を示しています。
2エポック後にテスト精度は0.88に達します。
Intel i5 2.4GHz CPUで90秒/エポック。
Tesla K40 GPUで10秒/エポック。
'''
from tensorflow.keras.layers import Layer
from keras.layers import Concatenate
from keras.layers import  GlobalMaxPooling1D
import tensorflow as tf

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})

# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        embeddings, attention_mask = inputs  # 入力から埋め込みとアテンションマスクを取得
        return embeddings * tf.expand_dims(attention_mask, -1)  # アテンションマスクを使用して埋め込みを適用

model = Sequential()

input_layer = Input(shape=(maxlen,),dtype='int32', name='main_input')  # 入力層を定義
attention_masks = Input(shape=(maxlen,), dtype='float32', name="attention_masks")  # アテンションマスクの入力層

emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)  # 埋め込み層

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])  # アテンションマスクを適用

def max_1d(X):
    return K.max(X, axis=1)  # 1Dの最大値を取得する関数

# 3のサイズのフィルターを持つConvolution1Dを追加
con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size =3,
                    strides=1)(masked_embeddings)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)  # 最大プーリング

# 4のサイズのフィルターを持つConvolution1Dを追加
con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)  # 最大プーリング

# 5のサイズのフィルターを持つConvolution1Dを追加
con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(masked_embeddings)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)  # 最大プーリング

cnn_layer = Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])  # CNNの出力を結合

# LSTM層を追加
x = masked_embeddings
lstm_layer = LSTM(128)(x)

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])  # LSTMとCNNの出力を結合

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)  # 隠れ層を追加
output_layer= Dropout(0.2)(dense_layer)  # ドロップアウトを追加
output_layer = Dense(3, trainable=True, activation='softmax')(output_layer)  # 出力層を定義

model = Model(inputs=[input_layer, attention_masks], outputs=[output_layer])  # モデルを定義
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)  # 最適化アルゴリズムを定義
adamax = Adamax(learning_rate=0.001)  # 別の最適化アルゴリズムを定義
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])  # モデルをコンパイル
model.summary()  # モデルの概要を表示
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# モデルのトレーニング
```

---The following area is a Code cell (cell numver is 22)---
```python
from keras.callbacks import EarlyStopping
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')  # 最良のモデルを保存するためのチェックポイント
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)  # アーリーストッピングの設定

model.fit([X_train,X_train_attention], y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint, early_stopping],
          validation_data=([X_valid,X_valid_attention], y_valid))  # モデルのトレーニング
```

---The following area is a Code cell (cell numver is 23)---
```python
model.compile(loss='categorical_crossentropy',
              optimizer='adadelta',
              metrics=['accuracy'])  # モデルを再コンパイル
model.save('model_LSTM_mix_CNN.keras')  # モデルの保存
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# モデルのテスト
```

---The following area is a Code cell (cell numver is 25)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込む

test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプトを処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答を処理

# トレーニング用に'Null'を削除
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)  # 'Null'行を削除
test.reset_index(inplace=True, drop=True)  # インデックスをリセット

print(f"合計 {len(indexes)} 行のNull応答が削除されました")
print('合計トレーニングサンプル数: ', len(test))  # トレーニングサンプル数を表示
```

---The following area is a Code cell (cell numver is 26)---
```python
test.head()  # テストデータの先頭5行を表示
```

---The following area is a Code cell (cell numver is 27)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])  # テキストを表示
```

---The following area is a Code cell (cell numver is 28)---
```python
# データをトークン化
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')  # numpy配列として返す

# INPUT_IDSはトークンIDです
INPUT_test = tokens_test['input_ids']
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS2 = tokens_test['attention_mask']

print(f'INPUT_IDSの形状: {INPUT_test.shape}, ATTENTION_MASKSの形状: {ATTENTION_MASKS2.shape}')  # 形状を表示
```

---The following area is a Code cell (cell numver is 29)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)  # テストデータにパディングを適用
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=maxlen)  # テストアテンションマスクにもパディングを適用
```

---The following area is a Code cell (cell numver is 30)---
```python
test  # テストデータを表示
```

---The following area is a Code cell (cell numver is 31)---
```python
y_predict = model.predict([X_test,X_test_attention])  # テストデータで予測を実行
y_predict  # 予測結果を表示
```

---The following area is a Code cell (cell numver is 32)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # 予測結果をデータフレームに変換
result_df = pd.concat([test['id'], winner_df], axis=1)  # 結果データフレームを作成
```

---The following area is a Code cell (cell numver is 33)---
```python
result_df.to_csv('submission.csv', index=False)  # 結果をCSVファイルに保存
```

---The following area is a Code cell (cell numver is 34)---
```python
result_df  # 予測結果データフレームを表示
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# 結論

トレーニングの速度を上げて最適化する余地はまだまだあります！ もっと多くのデータ、異なるバッチサイズ、学習率を試してみてください。ご健闘を祈ります！
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
---

# コメント

> ## carvingfate
> 
> Qwen2はGemma2よりも優れているのでしょうか？
> 
> > ## Nguyễn Anh Tú（トピック作成者）
> > 
> > Gemmaにアクセスできなかったため、Qwen 2を使用しています :(( ですので、レビューをお届けすることはできません。異なるトークナイザーを試して、モデルのハイパーパラメータを同じに保つことでジャッジしてください。私のモデルの構造を構築する際に参照した論文です：[https://www.researchgate.net/publication/321259272_Multi-channel_LSTM-CNN_model_for_Vietnamese_sentiment_analysis](https://www.researchgate.net/publication/321259272_Multi-channel_LSTM-CNN_model_for_Vietnamese_sentiment_analysis)
> > 
> > 

---
```

** @@@ Jupyter Notebook numver 32, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、ファインチューニングされたLlama-3 8bモデルを利用して、大規模なデータセット（25,000サンプル）に対する推論を行うことを目的としています。使用する環境は、T4 GPUを2台使用しており、トレーニング済みのモデルへのアクセスが必要です。

### 取り組む問題
ノートブックは、特に大規模なテストセットに対して各応答モデルの好みを予測し、どのモデルが選ばれるかを判断することに焦点を当てています。具体的には、ユーザーのプロンプトに対してモデルAとモデルBの応答を比較し、その選好を予測します。

### 手法
1. **ライブラリのインポート**:
   - `torch`, `sklearn`, `numpy`, `pandas`などのライブラリを使用。
   - Hugging Faceの`transformers`および`peft`ライブラリを使用して、事前学習済みのLlamaモデルのトークナイゼーションと推論を行います。

2. **データの準備**:
   - テストデータセットから必要なカラムを読み込み、文字列を処理して適切な形式に変換します。

3. **トークナイズ**:
   - `AutoTokenizer`を用いて入力テキストをトークン化し、PyTorchテンソル形式に変換します。

4. **モデルのロード**:
   - 2台のGPUそれぞれに異なるモデルを配置し、`AutoModelForSequenceClassification`を用いてモデルを読み込みます。

5. **LoRa設定**:
   - 減少されたメモリフットプリントでファインチューニングを行うために、LoRa（Low-Rank Adaptation）を設定します。

6. **推論**:
   - データフレームをバッチ処理しながら、モデルに対する推論を実行します。そして、モデルAやB、引き分けの確率を計算します。

7. **マルチスレッド推論**:
   - 結果を並行して計算するために、Pythonのスレッドを使用し、2つのサブセットに分割したデータを各モデルで処理します。

8. **結果の統合**:
   - 最終的にモデルの出力を統合し、提出用のCSVファイルに保存します。

推論は約4.5時間で完了しますが、さらなる改善の余地があるため、異なる後処理方法の検討が提案されています。全体として、このノートブックは大規模言語モデルを用いたユーザー選好予測の実装例を示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# 推論 - llama-3 8b 超高速 🚀
このノートブックでは、T4 GPUを2台使ってファインチューニング済みのllama-3 8bモデルを用いた推論を行います。このノートブックを作成した理由は、テストサイズが非常に大きい（25,000サンプル）からです。

前提条件: Llama-3へのアクセスが必要です。もし同意書に記入していない場合は、[こちら](https://www.kaggle.com/models/metaresearch/llama-3)に行き、同意書に記入してLlama-3にアクセスしてください。

トレーニングノートブックは[こちら](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train/notebook)にあります。

役立つと感じたら、ぜひアップボートしてください！

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

if (not torch.cuda.is_available()): print("申し訳ありませんが、GPUが必要です！")
```

---The following area is a Code cell (cell numver is 4)---
```python
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
WEIGHTS_PATH = '/kaggle/input/lmsys-llama-3-8b-fine-tuned/checkpoint-700/LMSYS/output_v1/checkpoint-700'
MAX_LENGTH = 2048
BATCH_SIZE = 4
DEVICE = torch.device("cuda")
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 6)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# リスト内の文字列を連結する関数
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

# データフレームの各列に対してprocess関数を適用
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

display(sample_sub)
display(test.head(5))
```

---The following area is a Code cell (cell numver is 7)---
```python
# モデル用のテキストを準備
test['text'] = 'ユーザーのプロンプト: ' + test['prompt'] +  '\n\nモデル A :\n' + test['response_a'] +'\n\n--------\n\nモデル B:\n'  + test['response_b']
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 9)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, add_prefix_space=True)
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.padding_side = 'right'
tokenizer.add_eos_token = True

# tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# トークンを生成、パディング、最大長さにトランクションし、PyTorchテンソルとして返す
tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# テンソルをCPUに移動し、リストに変換
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# モデルの読み込み 
一台のGPUに1つのモデルを読み込みます。
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytesの設定
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True, 
    bnb_4bit_quant_type = 'nf4',
    bnb_4bit_use_double_quant = True, 
    bnb_4bit_compute_dtype = torch.bfloat16 
)

# GPU 0にベースモデルをロード
device0 = torch.device('cuda:0')

base_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:0',
    use_cache=False,
)

# パディングトークンIDを設定
base_model_0.config.pad_token_id = tokenizer.pad_token_id

# GPU 1にベースモデルをロード
device1 = torch.device('cuda:1')
base_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
#     quantization_config=quantization_config,
    num_labels=3,
    device_map='cuda:1',
    use_cache=False,
)
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
これで、各GPUに1つのモデルを正常にロードできました！

# 重みのロード
```

---The following area is a Code cell (cell numver is 13)---
```python
# LoRaの設定

lora_config = LoraConfig(
    r = 16, 
    lora_alpha = 8,
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],
    lora_dropout = 0.05, 
    bias = 'none',
    inference_mode=True,
    task_type = 'SEQ_CLS'
)
```

---The following area is a Code cell (cell numver is 14)---
```python
# PEFTモデルを取得
# model_0 = get_peft_model(base_model_0, lora_config).to(device0) 
model_0 = PeftModel.from_pretrained(base_model_0, WEIGHTS_PATH)
# model_0 = model_0.merge_and_unload()
# model_0.config.pad_token_id = tokenizer.pad_token_id
# model_0.config.pretraining_tp = 1
model_0.eval()

# model_1 = get_peft_model(base_model_1, lora_config).to(device1) 
model_1 = PeftModel.from_pretrained(base_model_1, WEIGHTS_PATH)
# model_1 = model_1.merge_and_unload()
# model_1.config.pad_token_id = tokenizer.pad_token_id
# model_1.config.pretraining_tp = 1
model_1.eval()
```

---The following area is a Code cell (cell numver is 15)---
```python
# 学習可能なパラメータ
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 17)---
```python
import gc
gc.collect()
```

---The following area is a Code cell (cell numver is 18)---
```python
def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()
    
    # データフレームをバッチに分割して推論を実行
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():
            with autocast():
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        generated_class_a.extend(probabilities[:, 0])  # モデル A の確率
        generated_class_b.extend(probabilities[:, 1])  # モデル B の確率
        generated_class_c.extend(probabilities[:, 2])  # 引き分けの確率
    
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    torch.cuda.empty_cache()  

    return df
```

---The following area is a Code cell (cell numver is 19)---
```python
st = time.time()

N_SAMPLES = len(data)

# データを2つのサブセットに分割
half = round(N_SAMPLES / 2)
sub1 = data.iloc[0:half].copy()
sub2 = data.iloc[half:N_SAMPLES].copy()

# スレッドで推論を実行するための関数
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)

# スレッドからの結果を格納する辞書
results = {}

# スレッドを開始
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))

t0.start()
t1.start()

# すべてのスレッドが終了するのを待つ
t0.join()
t1.join()

# 結果を元のデータフレームに統合
data = pd.concat([results[0], results[1]], axis=0)

print(f"処理完了。総時間: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 20)---
```python
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

sample_sub[TARGETS] = data[TARGETS]
display(sample_sub)
```

---The following area is a Code cell (cell numver is 21)---
```python
sample_sub.to_csv('submission.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
推論は約4.5時間で完了しますが、まだ改善すべき点があります。異なる後処理を試してみて、ぜひ共有してください。Kaggleのやり方です :)
```

** @@@ Jupyter Notebook numver 33, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、ユーザーの好みを予測するためのモデルを構築する過程を示しています。特に、異なるチャットボットモデルの応答の評価を基に、どちらがより好まれるかを予測するタスクに取り組んでいます。

### 問題とデータセット
- コンペティションは、ユーザーが異なる応答を選択する際の選好を予測することを目指しています。このノートブックでは、与えられたデータセットを用いて、どちらのモデルが勝者かを予測するための特徴量を生成し、分類モデルを訓練します。

### 手法とライブラリ
1. **データ前処理**:
   - `textstat`, `nltk`, `pandas`を使用して、テキストデータを読み込み、処理を行っています。具体的には、テキストから得られる単語数、文字数、文数、平均単語長、平均文長などの特徴量を計算しています。

2. **特徴量生成**:
   - 各応答の読みやすさスコアや頻度情報も計算され、最終的に訓練データセットに特徴量として追加されます。

3. **モデル構築**:
   - モデルとして、**Gradient Boosting**および**XGBoost**が使用されており、Scikit-learnの`RandomizedSearchCV`を用いてハイパーパラメータのチューニングが行われています。
   - 層化K分割交差検証を用いてモデルのパフォーマンスを評価し、ログ損失（log loss）をスコアとして用いて最良のモデルを選択します。

4. **予測と提出準備**:
   - 最終的に選ばれたモデルを用いてテストデータに対し予測を行い、結果を指定された提出形式（`submission.csv`）で出力します。

このノートブック全体を通じて、データの準備から特徴量の生成、モデルの訓練、予測および提出ファイルの生成まで一連の流れが分かりやすく実装されており、効果的な機械学習パイプラインが示されています。
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install ../input/textstat/Pyphen-0.10.0-py3-none-any.whl  # Pyphenライブラリをインストール
!pip install ../input/textstat/textstat-0.7.0-py3-none-any.whl  # textstatライブラリをインストール
import textstat  # textstatをインポートして、テキスト統計に関する機能を使用できるようにする
```

---The following area is a Code cell (cell numver is 2)---
```python
import sklearn  # sklearnをインポート
import numpy as np  # numpyをインポート（数値計算のためのライブラリ）
import pandas as pd  # pandasをインポート（データ操作のためのライブラリ）
import matplotlib.pyplot as plt  # matplotlibをインポート（グラフ描画のためのライブラリ）
import time  # timeをインポート（時間計測のために使用）
from xgboost import XGBClassifier  # XGBoostの分類器をインポート
from sklearn.ensemble import GradientBoostingClassifier  # 勾配ブースティングの分類器をインポート
from sklearn.metrics import log_loss  # ログ損失を評価指標としてインポート
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV  # データ分割と交差検証を行うための関数をインポート
import nltk  # nltkをインポート（自然言語処理ライブラリ）
import textstat  # textstatを再インポート（重複していますが、他の機能を使う可能性があるため）
from textblob import TextBlob  # テキスト処理のためのTextBlobをインポート
from collections import Counter  # 要素のカウントを行うためのCounterをインポート

import warnings  # 警告を管理するためのライブラリをインポート
warnings.filterwarnings("ignore")  # 警告を無視する設定
warnings.filterwarnings('ignore')  # 再度同じ設定（冗長）
pd.options.display.float_format = '{:.2f}'.format  # pandasの浮動小数点数の表示形式を設定
pd.set_option('display.max_rows', None)  # 行数を制限せずに表示設定
pd.set_option('display.max_columns', None)  # 列数を制限せずに表示設定
```

---The following area is a Code cell (cell numver is 3)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータを読み込む
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出ファイルを読み込む
```

---The following area is a Code cell (cell numver is 4)---
```python
train.head()  # トレーニングデータの最初の5行を表示する
```

---The following area is a Code cell (cell numver is 5)---
```python
test.head()  # テストデータの最初の5行を表示する
```

---The following area is a Code cell (cell numver is 6)---
```python
print(f"The size of the train data: {train.shape} is and the test data is: {test.shape}")  # トレーニングデータとテストデータのサイズを表示する
```

---The following area is a Code cell (cell numver is 7)---
```python
print(train['winner_model_a'].value_counts())  # winner_model_aの値のカウントを表示
print(train['winner_model_b'].value_counts())  # winner_model_bの値のカウントを表示
print(train['winner_tie'].value_counts())  # winner_tieの値のカウントを表示
```

---The following area is a Code cell (cell numver is 8)---
```python
# 図と軸を作成する
fig, axes = plt.subplots(3, 1, figsize=(7, 6))

# プロットする列を指定
columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

# 0と1の色を定義
colors = {0: 'steelblue', 1: 'salmon'}

# 各列をそれぞれのサブプロットにプロット
for i, column in enumerate(columns):
    ax = axes[i]
    value_counts = train[column].value_counts().sort_index()
    
    # 指定した色とラベルでバーをプロット
    bars = ax.bar(value_counts.index.astype(str), value_counts, color=[colors[idx] for idx in value_counts.index],
                  label=value_counts.index.map({0: 'Lose (0)', 1: 'Win (1)'}))
    
    # バーにカウントを注釈する
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),  # 3ポイントの垂直オフセット
                    textcoords="offset points",
                    ha='center', va='bottom')
    
    ax.set_xlabel('Winner')  # x軸のラベル
    ax.set_ylabel('Count')  # y軸のラベル
    ax.set_title(f'Model {column.split("_")[-1].capitalize()} Counts')  # タイトル設定
    ax.legend(title='Outcome', loc='upper right')  # 凡例を設定

# 全体のタイトルを追加し、レイアウトを調整
fig.suptitle('Distribution of Winners Across Models', fontsize=16)  # 図全体のタイトル
plt.tight_layout(rect=[0, 0.03, 1, 0.95])  # レイアウトを調整

# プロットを表示
plt.show()
```

---The following area is a Code cell (cell numver is 9)---
```python
def process(input_str):  # 入力された文字列を処理する関数
    stripped_str = input_str.strip('[]')  # 先頭と末尾の[]を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文を分割して不要な""を削除
    return  ' '.join(sentences)  # 文を結合して返す

test.loc[:, 'prompt'] = test['prompt'].apply(process)  # テストデータのprompt列を処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # テストデータのresponse_a列を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # テストデータのresponse_b列を処理

train.loc[:, 'prompt'] = train['prompt'].apply(process)  # トレーニングデータのprompt列を処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # トレーニングデータのresponse_a列を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # トレーニングデータのresponse_b列を処理
```

---The following area is a Code cell (cell numver is 10)---
```python
train.head(3)  # トレーニングデータの最初の3行を表示
```

---The following area is a Code cell (cell numver is 11)---
```python
test.head(3)  # テストデータの最初の3行を表示
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time  # 次の処理にかかる時間を計測するマジックコマンド

# 単語数を計算する関数
def word_count(text):
    return len(nltk.word_tokenize(text))  # テキストをトークンに分割し、単語数を返す

# 文字数を計算する関数
def char_count(text):
    return len(text)  # テキストの文字数を返す

# 文の数を計算する関数
def sentence_count(text):
    return len(nltk.sent_tokenize(text))  # テキストを文に分割し、文の数を返す

# 平均単語長を計算する関数
def avg_word_length(text):
    words = nltk.word_tokenize(text)  # テキストをトークンに分割
    if len(words) == 0:  # 単語がない場合
        return 0  # 0を返す
    return sum(len(word) for word in words) / len(words)  # 単語の長さの合計を単語数で割る

# 平均文長を計算する関数
def avg_sentence_length(text):
    words = nltk.word_tokenize(text)  # テキストをトークンに分割
    sentences = nltk.sent_tokenize(text)  # テキストを文に分割
    if len(sentences) == 0:  # 文がない場合
        return 0  # 0を返す
    return len(words) / len(sentences)  # 単語数を文の数で割る

# 型トークン比を計算する関数
def ttr(text):
    words = nltk.word_tokenize(text)  # テキストをトークンに分割
    if len(words) == 0:  # 単語がない場合
        return 0  # 0を返す
    unique_words = set(words)  # ユニークな単語を取得
    return len(unique_words) / len(words)  # ユニーク単語数を全単語数で割る

# 単語頻度を計算する関数
def word_freq(text):
    words = nltk.word_tokenize(text)  # テキストをトークンに分割
    return Counter(words)  # 単語のカウントを返す

# バイグラム頻度を計算する関数
def bigram_freq(text):
    words = nltk.word_tokenize(text)  # テキストをトークンに分割
    bigrams = list(nltk.bigrams(words))  # バイグラムを取得
    return Counter(bigrams)  # バイグラムのカウントを返す

# 読みやすさスコアを計算する関数
def readability_scores(text):
    scores = {  # 各種読みやすさスコアを計算
        "flesch_kincaid_score": textstat.flesch_kincaid_grade(text),
        "gunning_fog_index": textstat.gunning_fog(text),
        "smog_index": textstat.smog_index(text),
        "ari": textstat.automated_readability_index(text)
    }
    return scores  # スコア辞書を返す

# 追加のメトリックを計算し、データフレームに追加
for column in ["prompt", "response_a", "response_b"]:
    train[f"{column}_word_count"] = train[column].apply(word_count)  # 各列に単語数を追加
    train[f"{column}_char_count"] = train[column].apply(char_count)  # 各列に文字数を追加
    train[f"{column}_sentence_count"] = train[column].apply(sentence_count)  # 各列に文の数を追加
    train[f"{column}_avg_word_length"] = train[column].apply(avg_word_length)  # 各列に平均単語長を追加
    train[f"{column}_avg_sentence_length"] = train[column].apply(avg_sentence_length)  # 各列に平均文長を追加
#     train[f"{column}_ttr"] = train[column].apply(ttr)  # 型トークン比を計算して追加（コメントアウト）
#     readability = train[column].apply(readability_scores)  # 読みやすさスコアを計算
#     train[f"{column}_flesch_kincaid_score"] = readability.apply(lambda x: x["flesch_kincaid_score"])  # Flesch-Kincaidスコアを追加
#     train[f"{column}_gunning_fog_index"] = readability.apply(lambda x: x["gunning_fog_index"])  # Gunning-Fogインデックスを追加
#     train[f"{column}_smog_index"] = readability.apply(lambda x: x["smog_index"])  # SMOGインデックスを追加
#     train[f"{column}_ari"] = readability.apply(lambda x: x["ari"])  # ARIスコアを追加

train.head()  # トレーニングデータの最初の5行を表示
```

---The following area is a Code cell (cell numver is 13)---
```python
%%time  # 次の処理にかかる時間を計測するマジックコマンド

import time  # timeを再インポート（重複していますが、他の機能を使う可能性があるため）
from sklearn.ensemble import GradientBoostingClassifier  # 勾配ブースティングの分類器を再インポート（重複していますが、他の機能を使う可能性があるため）
from xgboost import XGBClassifier  # XGBoostの分類器を再インポート（重複していますが、他の機能を使う可能性があるため）
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV  # データ分割と交差検証を行うための関数を再インポート（重複していますが、他の機能を使う可能性があるため）
from sklearn.metrics import log_loss  # ログ損失を評価指標として再インポート（重複していますが、他の機能を使う可能性があるため）
from scipy.stats import uniform, randint  # scipyから乱数生成のためのライブラリをインポート

# ターゲットをカテゴリカルラベルの単一列に変換
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# 特徴量とターゲットを定義
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]  # 特徴量のリストを作成

X = train[features]  # 特徴量データをXに設定
y = train['winner'] - 1  # ターゲットデータをyに設定

# データをトレーニングセットとバリデーションセットに分割
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# モデルを定義
models = {
    'GradientBoostingClassifier': GradientBoostingClassifier(),  # 勾配ブースティング分類器
    'XGBClassifier': XGBClassifier()  # XGBoost分類器
}

# ランダムサーチのためのパラメータの分布を定義
param_distributions = {
    'GradientBoostingClassifier': {
        'n_estimators': [100,200,350,300],  # 使用する決定木の数
        'max_depth': [2,3,4,5,7,9]  # 決定木の最大深さ
    },
    'XGBClassifier': {
        'n_estimators': [100,200,350,300],  # 使用する決定木の数
        'max_depth': [2,3,4,5,7,9]  # 決定木の最大深さ
    }
}

skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=1234)  # 層化K分割交差検証を設定

best_models = {}  # 最良モデルを保存する辞書

# 各モデルに対して繰り返し
for model_name, model in models.items():
    print(f"Model training for {model_name}")  # 現在のモデル名を表示
    
    # ランダムサーチの実行
    random_search = RandomizedSearchCV(model, param_distributions[model_name], n_iter=10, scoring='neg_log_loss', 
                                       n_jobs=-1, cv=skf, random_state=42)
    random_search.fit(X_train, y_train)  # トレーニングデータでモデルをフィッティング
    
    best_model = random_search.best_estimator_  # ベストモデルを取得
    best_models[model_name] = best_model  # 現在のモデルタイプのベストモデルを保存
    
    logloss_scores = []  # ログ損失スコアを保存するリスト
    start_time = time.time()  # 計測開始時間
    
    count = 0  # カウント初期化
    for train_index, test_index in skf.split(X, y):  # K分割交差検証のための分割
        X_train_fold, X_test_fold = X.iloc[train_index], X.iloc[test_index]  # トレーニングフォールドとテストフォールド
        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index]  # ターゲットのフォールド

        best_model.fit(X_train_fold, y_train_fold)  # 最良モデルでトレーニングフォールドをフィッティング
        y_test_pred_proba = best_model.predict_proba(X_test_fold)  # テストフォールドに対する確率を予測

        logloss = log_loss(y_test_fold, y_test_pred_proba)  # ログ損失を計算
        logloss_scores.append(logloss)  # スコアをリストに追加
        print(f"The log loss score for fold {count}: {logloss}")  # 現在のフォールドのログ損失を表示
        count += 1  # カウントをインクリメント

    average_logloss = sum(logloss_scores) / len(logloss_scores)  # 平均ログ損失を計算
    print(f"The average log loss score for {model_name} across all folds: {average_logloss}")  # すべてのフォールドの平均ログ損失を表示
    
    elapsed_time = time.time() - start_time  # 経過時間を計測
    print(f"Time taken for {model_name}: {elapsed_time:.2f} seconds")  # モデルのトレーニングにかかった時間を表示
    
    # バリデーションセットに対する確率を予測
    y_val_prob = best_model.predict_proba(X_val)
    # バリデーションセットでのログ損失を計算
    val_loss = log_loss(y_val, y_val_prob)
    print(f'Log Loss using {model_name} on validation set: {val_loss}')  # バリデーションセットでのログ損失を表示

# バリデーションセットのパフォーマンスに基づいてベストモデルを特定
best_model_name = min(best_models, key=lambda k: log_loss(y_val, best_models[k].predict_proba(X_val)))  # 最良モデル名を取得
best_average_logloss = log_loss(y_val, best_models[best_model_name].predict_proba(X_val))  # 最良モデルの平均ログ損失を計算

print(f"The best model is {best_model_name} with an average log loss score of {best_average_logloss}")  # 最良モデルとその平均ログ損失を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
model_to_use = best_models[best_model_name]  # 使用するモデルを最良モデルに設定
model_to_use  # モデル情報を表示
```

---The following area is a Code cell (cell numver is 15)---
```python
# 追加のメトリックを計算し、データフレームに追加
for column in ["prompt", "response_a", "response_b"]:
    test[f"{column}_word_count"] = test[column].apply(word_count)  # 各列に単語数を追加
    test[f"{column}_char_count"] = test[column].apply(char_count)  # 各列に文字数を追加
    test[f"{column}_sentence_count"] = test[column].apply(sentence_count)  # 各列に文の数を追加
    test[f"{column}_avg_word_length"] = test[column].apply(avg_word_length)  # 各列に平均単語長を追加
    test[f"{column}_avg_sentence_length"] = test[column].apply(avg_sentence_length)  # 各列に平均文長を追加
    
test.head()  # テストデータの最初の5行を表示
```

---The following area is a Code cell (cell numver is 16)---
```python
test_features = test[features]  # テストデータの特徴量を取得
test_predictions = model_to_use.predict_proba(test_features)  # テストデータに対する予測確率を計算
```

---The following area is a Code cell (cell numver is 17)---
```python
test_predictions  # テストデータに対する予測確率を表示
```

---The following area is a Code cell (cell numver is 18)---
```python
# 提出ファイルを準備する
submission = pd.DataFrame({  # データフレームを生成
    'id': test['id'],  # テストデータのIDを含める
    'winner_model_a': test_predictions[:, 0],  # モデルAの勝者確率
    'winner_model_b': test_predictions[:, 1],  # モデルBの勝者確率
    'winner_tie': test_predictions[:, 2]  # 引き分けの確率
})
```

---The following area is a Code cell (cell numver is 19)---
```python
submission.head()  # 提出ファイルの最初の5行を表示
```

---The following area is a Code cell (cell numver is 20)---
```python
submission.to_csv('/kaggle/working/submission.csv', index=False)  # 提出ファイルをCSV形式で保存
```

** @@@ Jupyter Notebook numver 34, the number of votes :4 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおいて、チャットボットの応答に対するユーザーの好みを予測するモデルを構築することを目的としています。

**取り組む問題**
コンペティションは、与えられたプロンプトに対して生成された2つの異なる応答から、どちらが好まれるかを予測することです。具体的には、ユーザーが選好を示すデータセットを基に、大規模言語モデル（LLM）の応答を評価する選好モデルを構築します。

**手法とライブラリ**
- **データの準備**:
  - `pandas`ライブラリを使用して、トレーニングデータとテストデータを読み込み、トレーニングデータを50%にランダムサンプリングしています。
  
- **特徴抽出**:
  - プロンプトと二つの応答（`response_a` と `response_b`）を結合し、それぞれのテキストを準備します。次に、`tensorflow.keras`ライブラリを用いて、テキストをトークン化し、シーケンスに変換し、パディング処理を行います。

- **モデルの定義**:
  - LSTM（Long Short-Term Memory）モデルが`tensorflow.keras`を使用して構築されています。モデルには埋め込み層、LSTM層、ドロップアウト層、出力層が含まれます。これにより、時系列データであるテキストの特徴を適切に捉えます。

- **モデルの学習**:
  - 各応答に対して別々のモデルが訓練され、トレーニングデータを使用して好みの予測を学習します。5エポックでの訓練が行われ、データの70%をバリデーションに使用しています。

- **予測と提出ファイルの作成**:
  - テストデータでの各モデルによる予測が行われ、最終的な提出ファイルが準備されます。引き分けの確率は単純に均一分布（1/3）で計算されています。

このノートブックは、ユーザーの好みを予測するために、機械学習の手法であるLSTMを用いたモデルを訓練し、データの前処理からモデルの予測、最終的な提出データの準備までを一貫して行っています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# データを50%に削減する

import pandas as pd

# train.csvファイルを読み込む
train_data_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # 正しいパスで更新
train_data = pd.read_csv(train_data_path)

test_data_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'
test_data = pd.read_csv(test_data_path)

# データの50%をランダムにサンプリングする
sampled_train_data = train_data.sample(frac=0.5, random_state=42)

# 必要に応じてサンプリングしたデータを保存
sampled_train_data_path = '/kaggle/working/sample_train.csv'
sampled_train_data.to_csv(sampled_train_data_path, index=False)
```

---The following area is a Code cell (cell numver is 2)---
```python
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# 特徴抽出のためにプロンプトと応答を結合
sampled_train_data['text_a'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_a']
sampled_train_data['text_b'] = sampled_train_data['prompt'] + " " + sampled_train_data['response_b']
test_data['text_a'] = test_data['prompt'] + " " + test_data['response_a']
test_data['text_b'] = test_data['prompt'] + " " + test_data['response_b']

# トークナイザーの初期化
tokenizer = Tokenizer()
# サンプルデータとテストデータを結合し、トークナイザーでフィッティング
tokenizer.fit_on_texts(pd.concat([sampled_train_data['text_a'], sampled_train_data['text_b'], test_data['text_a'], test_data['text_b']]))

# テキストをシーケンスに変換
X_train_a = tokenizer.texts_to_sequences(sampled_train_data['text_a'])
X_train_b = tokenizer.texts_to_sequences(sampled_train_data['text_b'])
X_test_a = tokenizer.texts_to_sequences(test_data['text_a'])
X_test_b = tokenizer.texts_to_sequences(test_data['text_b'])

# シーケンスの長さを揃えるためにパディングを行う
max_length = max(max(len(seq) for seq in X_train_a), max(len(seq) for seq in X_train_b))
X_train_a = pad_sequences(X_train_a, maxlen=max_length, padding='post')  # 末尾にパディング
X_train_b = pad_sequences(X_train_b, maxlen=max_length, padding='post')  # 末尾にパディング
X_test_a = pad_sequences(X_test_a, maxlen=max_length, padding='post')  # 末尾にパディング
X_test_b = pad_sequences(X_test_b, maxlen=max_length, padding='post')  # 末尾にパディング

# ターゲットを抽出
y_train_a = sampled_train_data['winner_model_a']
y_train_b = sampled_train_data['winner_model_b']
```

---The following area is a Code cell (cell numver is 3)---
```python
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# LSTMモデルを定義
def create_lstm_model(input_length):
    model = Sequential()
    model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=128, input_length=input_length))  # 埋め込み層
    model.add(LSTM(128, return_sequences=True))  # LSTM層（シーケンスを返す）
    model.add(Dropout(0.5))  # 過学習を防ぐためのドロップアウト層
    model.add(LSTM(128))  # もう一つのLSTM層
    model.add(Dense(1, activation='sigmoid'))  # 出力層（シグモイド活性化関数）
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # モデルのコンパイル
    return model

input_length = X_train_a.shape[1]

# text_aのモデルを訓練
model_a = create_lstm_model(input_length)
model_a.fit(X_train_a, y_train_a, epochs=5, batch_size=64, validation_split=0.7)  # 学習の実行
```

---The following area is a Code cell (cell numver is 4)---
```python
# text_bのモデルを訓練
model_b = create_lstm_model(input_length)
model_b.fit(X_train_b, y_train_b, epochs=5, batch_size=64, validation_split=0.7)  # 学習の実行
```

---The following area is a Code cell (cell numver is 5)---
```python
import numpy as np

# テストセットで予測を行う
test_pred_a = model_a.predict(X_test_a).flatten()  # model_aによる予測を平坦化
test_pred_b = model_b.predict(X_test_b).flatten()  # model_bによる予測を平坦化

# タイの確率を計算（単純化のため均一分布を仮定）
test_pred_tie = np.full(test_pred_a.shape, 1/3)  # 全ての要素を1/3で初期化
```

---The following area is a Code cell (cell numver is 6)---
```python
# 提出ファイルを準備
submission = pd.DataFrame({
    'id': test_data['id'],
    'winner_model_a': test_pred_a,
    'winner_model_b': test_pred_b,
    'winner_tie': test_pred_tie
})

# 提出ファイルを保存
submission_path = '/kaggle/working/submission.csv'
submission.to_csv(submission_path, index=False)  # CSV形式で保存

print(f"提出ファイルが {submission_path} に保存されました")
```

** @@@ Jupyter Notebook numver 35, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションに関連し、特にPrompt/Responseに対してキーワード抽出を行うことに焦点を当てています。具体的には、KeyBERTライブラリを使用して、提供されたテキストデータからキーフレーズやキーワードを抽出し、それをもとにトレーニングとテストデータセットを作成することを目的としています。

### 適用されている手法とライブラリ
1. **KeyBERT**: Hugging FaceのTransformersライブラリに基づいているキーワード抽出ライブラリです。このノートブックでは、事前学習済みのモデル「distilbert-base-nli-mean-tokens」を利用して、与えられたテキストから有用なキーワードを抽出します。

2. **データ処理ライブラリ**:
   - **NumPy**: 数値計算のためのライブラリ。
   - **Pandas**: データ操作や分析のためのライブラリ。
   - **Matplotlib**: データの可視化を行うためのライブラリ。

### 処理の流れ
- 最初に、KeyBERTライブラリをインストールし、必要なライブラリをインポートします。
- トレーニングとテスト用のデータセットをCSVファイルから読み込み、各データセットに新しいカラムを追加して初期化します。
- トレーニングデータとテストデータそれぞれの'prompt'、'response_a'、'response_b'各列からキーワードを抽出し、対応するカラムに保存します。
- 抽出したキーワードに基づいて、新たに作成されたカラムを組み合わせてコンパイルし、最終的なデータセットを形成します。
- 最後に、処理されたトレーニングデータとテストデータをそれぞれCSVファイルとして保存します。

このノートブックは、チャットボットの応答の品質を向上させるための前処理ステップとして、キーワード抽出機能を効果的に利用することを目指しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS プロンプト/レスポンス ワーズ KeyBERT

KeyBERTは、キーワード抽出およびキーフレーズ抽出のためのPythonライブラリです。これはHugging Face Transformersライブラリの上に構築されており、BERTのような事前学習済みのトランスフォーマーモデルを利用して、与えられたテキストからキーフレーズやキーワードを抽出します。KeyBERTは、文書要約、コンテンツ分析、情報検索などのタスクに特に有用です。
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install keybert  # KeyBERTライブラリをインストールするコマンドです。このコマンドを実行することで、キーワード抽出機能を利用できるようになります。
```

---The following area is a Code cell (cell numver is 3)---
```python
import numpy as np  # 数値計算のためのライブラリnumpyをインポートします。
import pandas as pd  # データ操作および分析のためのライブラリpandasをインポートします。
import random  # ランダム数生成のためのrandomモジュールをインポートします。
import os  # オペレーティングシステムとの対話のためのosモジュールをインポートします。
from keybert import KeyBERT  # KeyBERTライブラリからキーワード抽出のためのKeyBERTクラスをインポートします。
import matplotlib.pyplot as plt  # グラフ作成のためのmatplotlibライブラリをインポートします。このモジュールを用いることでデータの可視化が可能になります。
```

---The following area is a Code cell (cell numver is 4)---
```python
model = KeyBERT('distilbert-base-nli-mean-tokens')  # 'distilbert-base-nli-mean-tokens'という事前学習済みモデルを用いてKeyBERTのインスタンスを作成します。このモデルは、自然言語処理タスクでよく使用されるDistilBERTを基にしており、キーワードやキーフレーズを抽出するのに役立ちます。
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます。
print(len(train))  # トレーニングデータの行数を出力します。
train['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。
train['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。
train['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。
test['prompt_kw'] = '-'  # 'prompt_kw'カラムを追加し、初期値を'-'で設定します。
test['res_a_kw'] = '-'  # 'res_a_kw'カラムを追加し、初期値を'-'で設定します。
test['res_b_kw'] = '-'  # 'res_b_kw'カラムを追加し、初期値を'-'で設定します。
```

---The following area is a Code cell (cell numver is 6)---
```python
kw0 = model.extract_keywords(train['prompt'], top_n=5)  # トレーニングデータの'prompt'列から上位5つのキーワードを抽出します。
kw1 = model.extract_keywords(train['response_a'], top_n=20)  # トレーニングデータの'response_a'列から上位20のキーワードを抽出します。
kw2 = model.extract_keywords(train['response_b'], top_n=20)  # トレーニングデータの'response_b'列から上位20のキーワードを抽出します。

tkw0 = model.extract_keywords(test['prompt'], top_n=5)  # テストデータの'prompt'列から上位5つのキーワードを抽出します。
tkw1 = model.extract_keywords(test['response_a'], top_n=20)  # テストデータの'response_a'列から上位20のキーワードを抽出します。
tkw2 = model.extract_keywords(test['response_b'], top_n=20)  # テストデータの'response_b'列から上位20のキーワードを抽出します。
```

---The following area is a Code cell (cell numver is 7)---
```python
for i, w in enumerate(kw0):  # kw0の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiに対して、キーワードのリストをループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、つまり無視するべきでなければ、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    train.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'prompt_kw'列に格納します。

for i, w in enumerate(kw1):  # kw1の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiについてループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    train.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_a_kw'列に格納します。

for i, w in enumerate(kw2):  # kw2の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiについてループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    train.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、トレーニングデータの'res_b_kw'列に格納します。

train['res_a_kw'] = train['prompt_kw'] + ' / ' + train['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。
train['res_b_kw'] = train['prompt_kw'] + ' / ' + train['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。
train = train.iloc[:, 6:]  # トレーニングデータフレームの最初の6列を除外します。
display(train)  # トレーニングデータフレームを表示します。

train.to_csv('train_key.csv', index=False)  # トレーニングデータをCSVファイルとして保存します。インデックスは保存しません。
```

---The following area is a Code cell (cell numver is 8)---
```python
for i, w in enumerate(tkw0):  # tkw0の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiに対してループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    test.loc[i, 'prompt_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'prompt_kw'列に格納します。

for i, w in enumerate(tkw1):  # tkw1の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiについてループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    test.loc[i, 'res_a_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_a_kw'列に格納します。

for i, w in enumerate(tkw2):  # tkw2の各キーワードに対して、インデックスiとキーワードwを取得します。
    ws = []  # 有効なキーワードを格納するリストを初期化します。
    for wi in w:  # 各キーワードwiについてループします。
        if '_' not in wi[0]:  # キーワードwiが'_'を含まない場合、
            ws += [wi[0]]  # 有効なキーワードリストに追加します。
    test.loc[i, 'res_b_kw'] = ' '.join(ws)  # 有効なキーワードを空白で結合し、テストデータの'res_b_kw'列に格納します。

test['res_a_kw'] = test['prompt_kw'] + ' / ' + test['res_a_kw']  # 'res_a_kw'列に'prompt_kw'と'res_a_kw'を結合して新しい形式の文字列を格納します。
test['res_b_kw'] = test['prompt_kw'] + ' / ' + test['res_b_kw']  # 'res_b_kw'列に'prompt_kw'と'res_b_kw'を結合して新しい形式の文字列を格納します。
test = test.iloc[:, 4:]  # テストデータフレームの最初の4列を除外します。
display(test)  # テストデータフレームを表示します。

test.to_csv('test_key.csv', index=False)  # テストデータをCSVファイルとして保存します。インデックスは保存しません。
```

** @@@ Jupyter Notebook numver 36, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaのコンペティションにおけるデータ分析とモデル構築に関連したタスクに取り組んでいます。以下に要約を示します。

### 問題の概要
コンペティションは、大規模言語モデル（LLM）による生成応答の好みを予測するものであり、参加者はモデルAとモデルBの応答に基づいてどちらが好まれるかを予測することが求められています。このNotebookでは、提供されるデータを分析し、モデルのパフォーマンスを評価し、最終的に新たなモデルを構築して予測を行うプロセスが示されています。

### 主な手法と使用ライブラリ
1. **データ分析**:
   - Pandasを使用してデータセットを読み込み、モデルの使用状況やパフォーマンスを可視化しています。特に、Plotlyを用いてモデルの使用頻度やトップモデルのパフォーマンスをバーグラフで表示しました。
   
2. **モデル性能分析**:
   - 各モデルの勝率を計算するために、データフレームをグループ化し集計しています。加えて、勝ち負けの応答を抽出し、それらの特徴的な言語パターンを分析しています。

3. **データ準備**:
   - データセットをトレーニングセットと検証セットに分割し（scikit-learnの`train_test_split`を使用）、前処理によりテキストを整形しています。

4. **モデル構築**:
   - KerasとKeras-NLPを用いて、DeBERTaV3モデルを実装し、カスタムモデルを構築しています。このモデルは、応答エンコーディング、クラシファイア、損失関数を設定するために十分に設計されています。

5. **トレーニングおよび評価**:
   - 学習率スケジューリングとモデルチェックポイントを設定し、トレーニングを実施。トレーニング後、最良のモデル重みを読み込みます。

6. **予測と提出ファイル作成**:
   - テストデータに対してモデルを用いて予測を行い、最終的に提出用のCSVファイルを作成しています。

このNotebookは、データの前処理、モデルの訓練、および評価過程を一連の流れで示しており、特に自然言語処理におけるLLMの応用を具体的に扱っています。使用するライブラリ（NumPy, Pandas, Plotly, Keras, Keras-NLPなど）は、データ処理、可視化、およびディープラーニングモデルの構築に不可欠な要素とされています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import numpy as np
import pandas as pd

import plotly.graph_objects as go
from plotly import express as px
from plotly.offline import init_notebook_mode, iplot

import matplotlib.pyplot as plt
from wordcloud import WordCloud, STOPWORDS
```

---The following area is a Code cell (cell numver is 2)---
```python
init_notebook_mode(connected=True)

# 最大列幅を500に設定
pd.set_option("display.max_colwidth", 500)
```

---The following area is a Code cell (cell numver is 3)---
```python
DATA_PATH = "/kaggle/input/lmsys-chatbot-arena/train.csv"
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# データ分析
```

---The following area is a Code cell (cell numver is 5)---
```python
df = pd.read_csv(DATA_PATH)
df.head()
```

---The following area is a Code cell (cell numver is 6)---
```python
df.info()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# モデルの性能分析
```

---The following area is a Code cell (cell numver is 8)---
```python
model_usage = pd.concat([df["model_a"], df["model_b"]]).value_counts()

# ユニークなモデルの数は？
len(model_usage.keys())
```

---The following area is a Code cell (cell numver is 9)---
```python
# モデルの使用状況をプロットする
fig = px.bar(
    x=model_usage.index,
    y=model_usage.values,
    labels={"x": "モデル", "y": "使用回数"},
    title="モデル使用状況の分布",
)

fig.update_layout(width=1200, height=700)

iplot(fig)
```

---The following area is a Code cell (cell numver is 10)---
```python
# モデルのペアが比較された回数をカウント（順序に関係なく）
compared_models_count = (
    pd.DataFrame(
        np.sort(df[["model_a", "model_b"]].values, axis=1),
        columns=["model_a", "model_b"],
    )
    .value_counts()
    .reset_index(name="counts")
)

len(compared_models_count)
```

---The following area is a Code cell (cell numver is 11)---
```python
top_compared_models = compared_models_count.head(20)

fig = px.bar(
    x=top_compared_models["model_a"] + " vs " + top_compared_models["model_b"],
    y=top_compared_models["counts"],
    labels={"x": "モデル比較", "y": "比較回数"},
    title="トップ20のモデル比較分布",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 12)---
```python
# 比較されたモデルの組み合わせは1275個あります。各比較数がどのくらいの頻度で発生するか？
compared_models_count["counts"].value_counts()
```

---The following area is a Code cell (cell numver is 13)---
```python
# 比較数の分布を10個のビンに分類する
comparision_count_distribution_bins = pd.cut(
    compared_models_count["counts"],
    bins=[
        0,
        5,
        10,
        20,
        30,
        40,
        50,
        60,
        70,
        80,
        90,
        100,
        compared_models_count["counts"].max(),
    ],
    precision=0,
    retbins=False,
)

comparision_count_distribution_bins.value_counts(normalize=True).sort_index()
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
**どのモデルが最も良いパフォーマンスを示し、どのモデルが最も悪いパフォーマンスを示すか？**
```

---The following area is a Code cell (cell numver is 15)---
```python
model_scores_part1 = df.groupby("model_a")["winner_model_a"].sum()
model_scores_part2 = df.groupby("model_b")["winner_model_b"].sum()

model_scores = model_scores_part1.add(model_scores_part2, fill_value=0)
model_scores = model_scores / (
    df["model_a"].value_counts() + df["model_b"].value_counts()
)
model_scores = model_scores.sort_values(ascending=False)
```

---The following area is a Code cell (cell numver is 16)---
```python
# パフォーマンスが最も良いトップ10モデル
fig = px.bar(
    x=model_scores.head(10).index,
    y=model_scores.head(10).values,
    labels={"x": "モデル", "y": "スコア"},
    title="トップ10の最高評価モデル",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 17)---
```python
# パフォーマンスが最も悪いトップ10モデル
fig = px.bar(
    x=model_scores.tail(10).index,
    y=model_scores.tail(10).values,
    labels={"x": "モデル", "y": "スコア"},
    title="トップ10の最低評価モデル",
)

iplot(fig)
```

---The following area is a Code cell (cell numver is 18)---
```python
tie_df = df.query("winner_tie == 1")
(tie_df["winner_model_a"] + tie_df["winner_model_b"]).value_counts()
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
**引き分けはどのくらいの頻度であるか？**
```

---The following area is a Code cell (cell numver is 20)---
```python
f"{(tie_df.shape[0] / df.shape[0]) * 100:.1f}%"
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# ポジティブとネガティブな応答の分析
```

---The following area is a Code cell (cell numver is 22)---
```python
# 引き分けの例を除外する
df_no_ties = df.query("winner_tie == 0")

loosing_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 0 else x["response_b"]),
    axis=1,
)

loosing_responses
```

---The following area is a Code cell (cell numver is 23)---
```python
winning_responses = df_no_ties.apply(
    lambda x: (x["response_a"] if x["winner_model_a"] == 1 else x["response_b"]),
    axis=1,
)
```

---The following area is a Code cell (cell numver is 24)---
```python
# 負けた応答はクリーンアップする必要があります
loosing_responses = loosing_responses.str.strip("[]")
loosing_responses = loosing_responses.str.strip('"')

winning_responses = winning_responses.str.strip("[]")
winning_responses = winning_responses.str.strip('"')
```

---The following area is a Code cell (cell numver is 25)---
```python
df_no_ties["winning_response"] = winning_responses
df_no_ties["loosing_response"] = loosing_responses
```

---The following area is a Code cell (cell numver is 26)---
```python
texts = [
    "私は快適ではありません",
    "申し訳ありませんが",
    "ごめんなさい、しかし",
    "申し訳ありませんが",
    "残念ながら私は",
    "十分ではありません",
    "私はAIに過ぎません",
    "私はAIであり、",
    "私は恐れている",
    "私はできません",
]

unprecise_responses = []
for _, row in df_no_ties.iterrows():
    if row["loosing_response"].startswith(tuple(texts)):
        if row["winning_response"].startswith(tuple(texts)):
            unprecise_responses.append("両方")
        else:
            unprecise_responses.append("負け")
    elif row["winning_response"].startswith(tuple(texts)):
        unprecise_responses.append("勝ち")
    else:
        continue

pd.Series(unprecise_responses).value_counts(normalize=True)
```

---The following area is a Code cell (cell numver is 27)---
```python
import os
os.environ["KERAS_BACKEND"] = "jax"  # "tensorflow" または "torch" を指定

import keras_nlp
import keras
import tensorflow as tf

import numpy as np 
import pandas as pd
from tqdm import tqdm
import json

import matplotlib.pyplot as plt
import matplotlib as mpl
import plotly.express as px
```

---The following area is a Code cell (cell numver is 28)---
```python
class CFG:
    seed = 42  # ランダムシード
    preset = "deberta_v3_extra_small_en" # 事前学習済みモデルの名前
    sequence_length = 512  # 入力シーケンスの長さ
    epochs = 3 # トレーニングエポック数
    batch_size = 16  # バッチサイズ
    scheduler = 'cosine'  # 学習率スケジューラ
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    name2label = {v:k for k, v in label2name.items()}
    class_labels = list(label2name.keys())
    class_names = list(label2name.values())
```

---The following area is a Code cell (cell numver is 29)---
```python
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Code cell (cell numver is 30)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Code cell (cell numver is 31)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Code cell (cell numver is 32)---
```python
BASE_PATH
```

---The following area is a Code cell (cell numver is 33)---
```python
# トレーニングデータを読み込む
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# データのサンプリング
# df = df.sample(frac=0.10)

# 最初のプロンプトとその関連する応答を取得
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# ラベル変換
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# サンプルを表示
df.head()
```

---The following area is a Code cell (cell numver is 34)---
```python
# テストデータを読み込む
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# 最初のプロンプトと応答を取得
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# サンプルを表示
test_df.head()
```

---The following area is a Code cell (cell numver is 35)---
```python
# プロンプトと選択肢に基づいてオプションを作成する関数を定義
def make_pairs(row):
    row["encode_fail"] = False
    try:
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        prompt = ""
        row["encode_fail"] = True

    try:
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        response_a = ""
        row["encode_fail"] = True

    try:
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        response_b = ""
        row["encode_fail"] = True
        
    row['options'] = [f"プロンプト: {prompt}\n\n応答: {response_a}",  # モデルAからの応答
                      f"プロンプト: {prompt}\n\n応答: {response_b}"  # モデルBからの応答
                     ]
    return row
```

---The following area is a Code cell (cell numver is 36)---
```python
df = df.apply(make_pairs, axis=1)  # make_pairs関数をdfの各行に適用
display(df.head(2))  # dfの最初の2行を表示

test_df = test_df.apply(make_pairs, axis=1)  # make_pairs関数をtest_dfの各行に適用
display(test_df.head(2))  # test_dfの最初の2行を表示
```

---The following area is a Code cell (cell numver is 37)---
```python
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Code cell (cell numver is 38)---
```python
model_df = pd.concat([df.model_a, df.model_b])
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Plotlyを使用してカスタムスタイリングのバープロットを作成
fig = px.bar(counts, x='LLM', y='Count',
             title='LLMの分布',
             color='Count', color_continuous_scale='viridis')

fig.update_layout(xaxis_tickangle=-45)  # x軸のラベルを回転して読みやすくする

fig.show()
```

---The following area is a Code cell (cell numver is 39)---
```python
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

fig = px.bar(counts, x='Winner', y='Win Count',
             title='訓練データの勝者の分布',
             labels={'Winner': '勝者', 'Win Count': '勝利回数'},
             color='Winner', color_continuous_scale='viridis')

fig.update_layout(xaxis_title="勝者", yaxis_title="勝利回数")

fig.show()
```

---The following area is a Code cell (cell numver is 40)---
```python
from sklearn.model_selection import train_test_split  # パッケージをインポート

train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Code cell (cell numver is 41)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # モデルの名前
    sequence_length=CFG.sequence_length, # 最大シーケンス長、短い場合はパディングされる
)
```

---The following area is a Code cell (cell numver is 42)---
```python
outs = preprocessor(df.options.iloc[0])  # 最初の行のオプションを処理する

# 各処理された出力の形状を表示
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Code cell (cell numver is 43)---
```python
def preprocess_fn(text, label=None):
    text = preprocessor(text)  # テキストを前処理
    return (text, label) if label is not None else text  # ラベルがあれば処理済みテキストとラベルを返す
```

---The following area is a Code cell (cell numver is 44)---
```python
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNEオプション
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # スライスを作成
    ds = tf.data.Dataset.from_tensor_slices(slices)  # スライスからデータセットを作成
    ds = ds.cache() if cache else ds  # キャッシュを有効にする場合はデータセットをキャッシュ
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # 前処理関数をマッピング
    opt = tf.data.Options()  # データセットオプションを作成
    if shuffle: 
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # シャッフルが有効な場合はデータセットをシャッフル
        opt.experimental_deterministic = False
    ds = ds.with_options(opt)  # データセットオプションを設定
    ds = ds.batch(batch_size, drop_remainder=False)  # データセットをバッチ処理
    ds = ds.prefetch(AUTO)  # 次のバッチを事前取得
    return ds  # 構築したデータセットを返す
```

---The following area is a Code cell (cell numver is 45)---
```python
# トレーニング
train_texts = train_df.options.tolist()  # トレーニングテキストを抽出
train_labels = train_df.class_label.tolist()  # トレーニングラベルを抽出
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# 検証
valid_texts = valid_df.options.tolist()  # 検証テキストを抽出
valid_labels = valid_df.class_label.tolist()  # 検証ラベルを抽出
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 46)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8

    def lrfn(epoch):  # 学習率更新関数
        if epoch < lr_ramp_ep: lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start
        elif epoch < lr_ramp_ep + lr_sus_ep: lr = lr_max
        elif mode == 'exp': lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min
        elif mode == 'step': lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # plotがTrueの場合はlr曲線をプロット
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('エポック'); plt.ylabel('学習率')
        plt.title('学習率スケジューラ')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # lrコールバックを作成
```

---The following area is a Code cell (cell numver is 47)---
```python
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Code cell (cell numver is 48)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',
                                          save_best_only=True,
                                          save_weights_only=True,
                                          mode='min')
```

---The following area is a Code cell (cell numver is 49)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Code cell (cell numver is 50)---
```python
# 入力レイヤーの定義
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# DebertaV3Classifierバッックボーンの作成
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# 最初の応答の埋め込みを計算: (P + R_A) バックボーンを使用
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# 2番目の応答の埋め込みを計算: (P + R_B) 同じバックボーンを使用
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# 最終出力を計算
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# オプティマイザ、損失、およびメトリクスを指定してモデルをコンパイル
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Code cell (cell numver is 51)---
```python
model.summary()
```

---The following area is a Code cell (cell numver is 52)---
```python
# モデルのトレーニングを開始
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Code cell (cell numver is 53)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Code cell (cell numver is 54)---
```python
# テストデータセットを構築
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 55)---
```python
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Code cell (cell numver is 56)---
```python
sub_df = test_df[["id"]].copy()
sub_df[CFG.class_names] = test_preds.tolist()
sub_df.to_csv("submission.csv", index=False)
sub_df.head()
```

** @@@ Jupyter Notebook numver 37, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおけるデータ分析および前処理に焦点を当てています。具体的には、大規模言語モデル（LLM）が生成した応答を基に、どちらのモデルの応答がユーザーに好まれるかを予測するためのトレーニングデータとテストデータを扱っています。

### 問題に取り組む内容
- コンペティションの目的として、2つのモデルの応答の中からユーザーがどちらを好むかを予測するための機械学習モデルを構築することが掲げられています。このために、大規模なチャットボット応答のデータセットが使用されます。

### 使用している手法・ライブラリ
- **データの読み込みと探索**: `pandas`を使用して、トレーニングデータ、テストデータ、サンプル提出ファイルをCSV形式で読み込み、初めの数行やデータの概要（メタ情報、欠損値）を表示しています。
- **データ前処理**: `re`ライブラリを用いて、テキストデータのクリーニングを行います。具体的には、HTMLタグの削除、英数字以外の文字の除去、小文字への変換、余分な空白の削除を行う`clean_text`関数が定義されています。このクリーニング処理は、トレーニングデータとテストデータの各テキスト列に適用されています。

全体を通して、このNotebookは、データの読み込みから基本的な分析、欠損値の確認、さらにテキストデータの前処理までのプロセスを示しており、機械学習モデルの訓練に向けた準備段階を説明しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下は読み込むのに便利なパッケージのいくつかです

import numpy as np # 線形代数を扱うためのライブラリ
import pandas as pd # データ処理、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは読み取り専用の"../input/"ディレクトリで利用可能です
# 例えば、これを実行すると（クリックするかShift+Enterを押すことで）入力ディレクトリ内のすべてのファイルがリストされます

import os
# '/kaggle/input'ディレクトリ内のファイルを探索するためのos.walkを使用
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        # フルパスを出力する
        print(os.path.join(dirname, filename))

# 最大20GBのデータを現在のディレクトリ（/kaggle/working/）に書き込むことができ、
# これは"Save & Run All"を使用してバージョンを作成したときに出力として保存されます。
# 一時ファイルを/kaggle/temp/に書き込むこともできますが、これは現在のセッションの外で保存されることはありません。
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
# ステップ1: データ探索

**データの読み込み**
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd

# データセットを読み込む
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv') # トレーニングデータを読み込む
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')   # テストデータを読み込む
sample_submission = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv') # サンプル提出ファイルを読み込む

# 各データセットの最初の数行を表示する
print("Train Data:") # トレーニングデータのタイトルを表示
print(train_data.head()) # トレーニングデータの最初の5行を表示

print("\nTest Data:") # テストデータのタイトルを表示
print(test_data.head()) # テストデータの最初の5行を表示

print("\nSample Submission:") # サンプル提出データのタイトルを表示
print(sample_submission.head()) # サンプル提出データの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
**データの検査**
```

---The following area is a Code cell (cell numver is 5)---
```python
# 各データセットの概要を表示する
print("Train Data Info:") # トレーニングデータに関する情報を表示
print(train_data.info()) # トレーニングデータの要約情報を表示

print("\nTest Data Info:") # テストデータに関する情報を表示
print(test_data.info()) # テストデータの要約情報を表示

print("\nSample Submission Info:") # サンプル提出データに関する情報を表示
print(sample_submission.info()) # サンプル提出データの要約情報を表示

# 欠損値をチェックする
print("Missing Values in Train Data:") # トレーニングデータの欠損値を表示する旨を表示
print(train_data.isnull().sum()) # トレーニングデータの各列の欠損値の数を表示

print("\nMissing Values in Test Data:") # テストデータの欠損値を表示する旨を表示
print(test_data.isnull().sum()) # テストデータの各列の欠損値の数を表示
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
**データの分析**
```

---The following area is a Code cell (cell numver is 7)---
```python
# 数値特徴量の基本統計量を取得する
print("Train Data Description:") # トレーニングデータの説明を表示
print(train_data.describe()) # トレーニングデータの数値的な要約統計を表示

# 目的変数の分布を調べる
print("Distribution of Winner Model (Train Data):") # トレーニングデータにおける勝者モデルの分布を表示
# 勝者モデルの合計を表示
print(train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].sum()) # 各勝者モデルの合計を計算し表示
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# ステップ2: データ前処理
```

---The following area is a Code cell (cell numver is 9)---
```python
import re

def clean_text(text):
    # HTMLタグを削除する（もしあれば）
    text = re.sub(r'<.*?>', '', text)
    
    # 英数字以外の文字を削除する
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)
    
    # 小文字に変換する
    text = text.lower()
    
    # 余分な空白を削除する
    text = text.strip()
    
    return text # 処理したテキストを返す
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
**クリーニング関数の適用:**
```

---The following area is a Code cell (cell numver is 11)---
```python
# トレーニングデータのテキストデータをクリーンアップする
train_data['prompt'] = train_data['prompt'].apply(clean_text) # 'prompt'列の各行にclean_text関数を適用
train_data['response_a'] = train_data['response_a'].apply(clean_text) # 'response_a'列の各行にclean_text関数を適用
train_data['response_b'] = train_data['response_b'].apply(clean_text) # 'response_b'列の各行にclean_text関数を適用

# テストデータのテキストデータをクリーンアップする
test_data['prompt'] = test_data['prompt'].apply(clean_text) # 'prompt'列の各行にclean_text関数を適用
test_data['response_a'] = test_data['response_a'].apply(clean_text) # 'response_a'列の各行にclean_text関数を適用
test_data['response_b'] = test_data['response_b'].apply(clean_text) # 'response_b'列の各行にclean_text関数を適用
```

---The following area is a Code cell (cell numver is 12)---
```python
# トレーニングデータとテストデータの列名をチェックする
print("Train Data Columns:", train_data.columns) # トレーニングデータの列名を表示
print("Test Data Columns:", test_data.columns) # テストデータの列名を表示
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
---

# コメント 

> ## アハメド・アルハム
> 
> 素晴らしい作業ですね。私のノートブックもレビューしてください。
> 
> 
> 

---
```

** @@@ Jupyter Notebook numver 38, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、チャットボットの応答の好みを予測する問題に取り組んでいます。具体的には、2つの異なる言語モデルが生成した応答のうち、どちらがユーザーに好まれるかを判定するモデルを構築することを目指しています。

### 主要な手法とライブラリ
- **ライブラリ**: 主に`pandas`（データ処理）、`torch`（深層学習処理）、および`transformers`（言語モデルの利用）を使用しています。
  
- **データ処理**: `process_data`関数を用いて、CSVファイルから読込んだプロンプトおよび応答を整形しています。このプロセスでは、前後の角括弧やダブルクオーテーションを取り除き、文章を結合しています。

- **モデルのロード**: `AutoTokenizer`と`AutoModelForCausalLM`を利用して、事前トレーニングされたGemma-2-9Bモデルをロードしています。モデルはbfloat16の形式でCUDAデバイスにロードされ、推論に対応しています。

- **推論処理**: `predict`関数を定義し、入力クエリと2つの応答を元にどちらの応答が優れているかを判断します。評価基準には関連性、正確性、完全性、一貫性が考慮されており、応答の判定結果に基づいて出力が行われます。

- **結果の蓄積**: ループ処理を通じて、全テストデータに対して予測を行い、その結果をリストにまとめ、最終的に`submission.csv`という形式で出力します。各エントリは、クエリID、モデルAの勝者、モデルBの勝者、および引き分けのフラグを含みます。

全体的には、このノートブックは機械学習を介して言語モデルの応答を評価し、人間の好みに基づいて最適な応答を予測するための手法を示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import pandas as pd
import ast
import json
import re
import torch
from tqdm import tqdm
```

---The following area is a Code cell (cell numver is 3)---
```python
torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを無効にする
torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にする
```

---The following area is a Code cell (cell numver is 4)---
```python
def process_data(input_str):
    stripped_str = input_str.strip('[]')  # 文字列の前後の角括弧を削除する
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文章を分割し、各部分の前後のダブルクオーテーションを削除
    sentences = ' '.join(sentences)  # 分割した文章をスペースで結合
    return sentences

def get_data(path, system_prompt=None):
    df = pd.read_csv(path)  # 指定したパスからCSVファイルを読み込む
    df['prompt'] = df['prompt'].apply(process_data)  # プロンプト列に対してprocess_dataを適用
    df['response_a'] = df['response_a'].apply(process_data)  # 応答A列に対してprocess_dataを適用
    df['response_b'] = df['response_b'].apply(process_data)  # 応答B列に対してprocess_dataを適用
    return df  # 処理済みのデータフレームを返す
```

---The following area is a Code cell (cell numver is 5)---
```python
test_path = '/kaggle/input/lmsys-chatbot-arena/test.csv'  # テストデータのパスを指定
test_df = get_data(test_path)  # テストデータを取得
test_df.head()  # テストデータの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## Gemma-2-9Bモデルのロード
> google/gemma-2-9b-it 

私はすでにbfloat16の重みをダウンロードして保存しました。
```

---The following area is a Code cell (cell numver is 7)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer  # Transformersライブラリからモデルとトークナイザーをインポート
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-tokenizer'  # トークナイザーのパスを指定
model_path = '/kaggle/input/gemma-2-9b-it/gemma-2-9b-it-palash-model'  # モデルのパスを指定
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)  # トークナイザーを指定したパスからロード
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time
model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype=torch.bfloat16)  # モデルを指定したパスからロードし、デバイスマップを自動で取得、データ型をbfloat16に設定
```

---The following area is a Code cell (cell numver is 11)---
```python
model  # モデルオブジェクトを表示
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## シンプルな推論を行ってみましょう
```

---The following area is a Code cell (cell numver is 13)---
```python
%%time
prompt = 'Write a conversation between gemma and llama llm models'  # プロンプトを定義
input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')  # プロンプトをトークン化し、テンソル形式に変換してCUDAデバイスに移動

outputs = model.generate(**input_ids, max_new_tokens=200)  # モデルを使って新しいトークンを生成
response = tokenizer.decode(outputs[0])  # 生成したトークンをデコードして応答を取得
print(response)  # 応答を表示
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## テストセットに対して、プロンプトエンジニアリングを行った推論を行いましょう
```

---The following area is a Code cell (cell numver is 15)---
```python
test_df  # テストデータフレームを表示
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_prompt(query, response_a, response_b):
    prompt = f"""
あなたは、異なるモデルによって生成された2つの応答を評価し、どちらの方が優れているかを判断する任務を負っています。クエリと2つの応答（モデルAからの応答AとモデルBからの応答B）を与えられた場合、各応答の質を関連性、正確性、完全性、全体的な一貫性に基づいて評価します。両方の応答が同等に良いか悪い場合は、引き分けと宣言することができます。

指示：

クエリ: {query}
応答A（モデルA）: {response_a}
応答B（モデルB）: {response_b}

評価基準：

関連性: 応答はクエリにどれだけ対応しているか？
正確性: 提供された情報は正確で信頼性があるか？
完全性: 応答は包括的な回答を提供しているか？
一貫性: 応答は論理的に構成されており、理解しやすいか？
出力：

応答Aが優れている場合、出力: 応答A
応答Bが優れている場合、出力: 応答B
両方の応答が同等に良いまたは悪い場合、出力: 引き分け

単一行を出力する必要があります – 応答Aまたは応答Bまたは引き分けのいずれかの単語\n
出力: 

    """
    return prompt
```

---The following area is a Code cell (cell numver is 17)---
```python
def predict(query, response_a, response_b, max_new_tokens=50, do_sample=False, temperature=1.0):
    prompt = get_prompt(query=query, response_a=response_a, response_b=response_b)

    input_ids = tokenizer(prompt, return_tensors='pt').to('cuda')  # プロンプトをトークン化し、テンソル形式に変換してCUDAデバイスに移動

    outputs = model.generate(**input_ids, max_new_tokens=50, do_sample=do_sample, temperature=temperature)  # モデルが新しいトークンを生成
    response = tokenizer.decode(outputs[0])  # 生成したトークンをデコードして応答を取得

    pattern = r"OUTPUT:\s*(RESPONSE_A|RESPONSE_B|TIE)"  # 出力のパターンを定義
    match = re.search(pattern, response)  # 応答の中から出力パターンを検索
    if match:
        pred = match.group(1)  # マッチした場合、出力を取得
    else:
        pred = None  # マッチしなかった場合はNoneを設定
    return pred  # 予測結果を返す
```

---The following area is a Code cell (cell numver is 18)---
```python
%%time
id_list = []
winner_model_a_list = []
winner_model_b_list = []
winner_tie_list = []
for idx in tqdm(range(0, len(test_df))):  # テストデータフレームの各行についてループ
    query_id = test_df.iloc[idx]['id']  # クエリIDを取得
    query = test_df.iloc[idx]['prompt']  # クエリを取得
    response_a = test_df.iloc[idx]['response_a']  # 応答Aを取得
    response_b = test_df.iloc[idx]['response_b']  # 応答Bを取得
    pred = predict(query, response_a, response_b, max_new_tokens=20, do_sample=True, temperature=0.7)  # 予測を実行
    id_list.append(query_id)  # IDをリストに追加
    if pred is not None:  # 予測結果がNoneでない場合
        if 'A' in pred or 'a' in pred:  # 応答Aが優れている場合
            winner_model_a_list.append(1)  # 応答Aの勝者リストに1を追加
            winner_model_b_list.append(0)  # 応答Bの勝者リストに0を追加                    
            winner_tie_list.append(0)  # 引き分けリストに0を追加
        if 'B' in pred or 'b' in pred:  # 応答Bが優れている場合
            winner_model_a_list.append(0)  # 応答Aの勝者リストに0を追加
            winner_model_b_list.append(1)  # 応答Bの勝者リストに1を追加        
            winner_tie_list.append(0)  # 引き分けリストに0を追加            
    else:
        winner_model_a_list.append(0)  # 応答Aの勝者リストに0を追加
        winner_model_b_list.append(0)  # 応答Bの勝者リストに0を追加        
        winner_tie_list.append(1)  # 引き分けリストに1を追加
    torch.cuda.empty_cache()  # CUDAメモリのキャッシュをクリア
```

---The following area is a Code cell (cell numver is 19)---
```python
submission_df = pd.DataFrame({'id': id_list, 'winner_model_a': winner_model_a_list, 'winner_model_b': winner_model_b_list, 'winner_tie': winner_tie_list})  # 提出用のデータフレームを作成
```

---The following area is a Code cell (cell numver is 20)---
```python
submission_df  # 提出用データフレームを表示
```

---The following area is a Code cell (cell numver is 21)---
```python
submission_df.to_csv('submission.csv', index=False)  # 提出データフレームをCSVファイルとして保存
```

** @@@ Jupyter Notebook numver 39, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「Chatbot Arena Competition」に参加するために設計されており、AIチャットボットの評価を行い、ユーザーの好みを予測することを目的としています。

### 問題の概要
本コンペティションでは、ユーザーのプロンプトに対して2つの異なるAIモデルが生成した応答のどちらが好まれるかを予測することが主な課題です。Notebookは、与えられたデータセットを基にし、どちらの応答がユーザーの好みに合致するかを予測するためのモデルを構築します。

### 手法とライブラリ
1. **環境の設定**: TensorFlow、Pandas、NumPy、Scikit-learnなどのライブラリを使用し、必要なパッケージをインストールします。
2. **データのロード**: 訓練データとテストデータをCSVファイルから読み込みます。
3. **テキストデータの前処理**: プロンプトとモデルの応答を連結してトークン化し、シーケンス化します。この際、パディングを行い、モデルの入力形式に整えます。
4. **モデルの構築**: Kerasを使用して、LSTMネットワークを含むニューラルネットワークを構築します。プロンプトと応答を入力として受け取り、各応答の勝者を予測するための3つの出力を持つモデルを設計します。
5. **モデルのトレーニング**: モデルを訓練データでトレーニングし、バリデーションデータを用いてその性能を確認します。
6. **モデルの評価**: トレーニング後、検証データを用いてモデルの精度や損失を評価します。
7. **予測を行う**: テストデータに対して予測を行い、その結果を取得します。
8. **提出ファイルの準備**: 予測結果を整理し、Kaggleに提出するためのCSVファイルを作成します。

### まとめ
このNotebookは、ユーザーの好みを予測するための機械学習モデルを効果的に構築し、評価するフレームワークを提供しています。使用されている手法やライブラリは、データ前処理からモデルの設計、トレーニング、評価まで、機械学習プロセス全体を網羅しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 30px; 
    border-radius: 15px; 
    text-align: center; 
    box-shadow: 0 6px 12px rgba(0, 0, 0, 0.4);
    margin-bottom: 30px;
">
    <h1 style="font-size: 2.5em;">🧠 LMSYS - AIチャット評価者: 人間とAIの相互強化</h1>
</div>


<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-bottom: 20px;
">
    <h1>私のノートブックへようこそ！🙏</h1>
    <p>このノートブックは<strong>Chatbot Arena Competition</strong>に捧げられています。ここでは、会話AIの領域で探求し、革新を行います。</p>
    <p><strong>コンペティションの概要:</strong>目標は、自然で関連性のある会話を通じてユーザーを理解し、関与させるチャットボットを開発することです。ここでは、最高のチャットボットを作成するためのデータ、テクニック、洞察が見つかります。</p>
    <p>この旅に参加していただきありがとうございます。ぜひ深く掘り下げて探検し、チャットボットで魔法を作りましょう！</p>
</div>



# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
 1. 環境の設定
</h1>
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install tensorflow pandas numpy scikit-learn
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
2. ライブラリのインポート
</h1>
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd  # データ操作のためのライブラリ
import numpy as np  # 数値計算のためのライブラリ
from sklearn.model_selection import train_test_split  # データ分割のための関数
from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングのためのクラス
from sklearn.metrics import classification_report  # 分類結果のレポートを生成するための関数
import tensorflow as tf  # TensorFlowライブラリ
from tensorflow.keras.models import Model  # Kerasのモデルを構築するためのクラス
from tensorflow.keras.layers import Input, Dense, Embedding, LSTM, Concatenate  # ニューラルネットワークの層
from tensorflow.keras.preprocessing.text import Tokenizer  # テキストを前処理するためのクラス
from tensorflow.keras.preprocessing.sequence import pad_sequences  # シーケンスのパディングを行う関数
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
3. データのロード
</h1>
```

---The following area is a Code cell (cell numver is 6)---
```python
# データセットの読み込み
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # 訓練データをCSV形式で読み込む
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSV形式で読み込む
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
4. テキストデータの前処理
</h1>
```

---The following area is a Code cell (cell numver is 8)---
```python
# 応答とプロンプトを連結してトークン化する
texts = train_df['prompt'] + ' ' + train_df['response_a'] + ' ' + train_df['response_b']  # プロンプトと応答を結合
tokenizer = Tokenizer()  # トークナイザーのインスタンスを作成
tokenizer.fit_on_texts(texts)  # テキストに基づいてトークナイザーをフィットさせる

# テキストをシーケンスに変換する関数
def text_to_sequences(texts):
    sequences = tokenizer.texts_to_sequences(texts)  # テキストをシーケンスに変換
    return pad_sequences(sequences, maxlen=500)  # maxlenに基づいてパディング処理を行う

# 訓練データの処理
X_prompt_a = text_to_sequences(train_df['prompt'])  # プロンプトをシーケンスに変換
X_response_a = text_to_sequences(train_df['response_a'])  # 応答Aをシーケンスに変換
X_response_b = text_to_sequences(train_df['response_b'])  # 応答Bをシーケンスに変換

# ラベルの取得
y_a = train_df['winner_model_a']  # モデルAの勝者ラベル
y_b = train_df['winner_model_b']  # モデルBの勝者ラベル
y_tie = train_df['winner_tie']  # 引き分けラベル

# 訓練データの分割
X_train_a, X_val_a, y_train_a, y_val_a = train_test_split(X_prompt_a, y_a, test_size=0.2, random_state=42)  # 訓練データと評価データに分割
X_train_b, X_val_b, y_train_b, y_val_b = train_test_split(X_prompt_a, y_b, test_size=0.2, random_state=42)  # 訓練データと評価データに分割
X_train_tie, X_val_tie, y_train_tie, y_val_tie = train_test_split(X_prompt_a, y_tie, test_size=0.2, random_state=42)  # 訓練データと評価データに分割
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
5. モデルの構築
</h1>
```

---The following area is a Code cell (cell numver is 10)---
```python
# モデルのパラメータを定義する
vocab_size = len(tokenizer.word_index) + 1  # ボキャブラリーのサイズを取得
embedding_dim = 100  # 埋め込み次元
max_len = 500  # 最大シーケンス長

# 入力の定義
input_prompt = Input(shape=(max_len,))  # プロンプト入力
input_response_a = Input(shape=(max_len,))  # 応答A入力
input_response_b = Input(shape=(max_len,))  # 応答B入力

# 埋め込み層の定義
embedding_layer = Embedding(vocab_size, embedding_dim, input_length=max_len)

# 入力を埋め込みベクトルに変換
embedded_prompt = embedding_layer(input_prompt)  # プロンプトの埋め込み
embedded_response_a = embedding_layer(input_response_a)  # 応答Aの埋め込み
embedded_response_b = embedding_layer(input_response_b)  # 応答Bの埋め込み

# LSTM層の定義
lstm_layer = LSTM(64)  # LSTMユニット数は64

# 入力をLSTM層に通す
encoded_prompt = lstm_layer(embedded_prompt)  # プロンプトのエンコード
encoded_response_a = lstm_layer(embedded_response_a)  # 応答Aのエンコード
encoded_response_b = lstm_layer(embedded_response_b)  # 応答Bのエンコード

# プロンプトとエンコードされた応答を連結する
concat_a = Concatenate()([encoded_prompt, encoded_response_a])  # 応答Aとの連結
concat_b = Concatenate()([encoded_prompt, encoded_response_b])  # 応答Bとの連結

# 分類用の全結合層
dense_layer = Dense(64, activation='relu')  # 全結合層（ReLU活性化関数）

# 出力層の定義
output_a = Dense(1, activation='sigmoid')(dense_layer(concat_a))  # 応答Aの勝者を予測
output_b = Dense(1, activation='sigmoid')(dense_layer(concat_b))  # 応答Bの勝者を予測
output_tie = Dense(1, activation='sigmoid')(dense_layer(concat_a))  # 引き分けの勝者を予測

# モデルの定義
model = Model(inputs=[input_prompt, input_response_a, input_response_b], 
              outputs=[output_a, output_b, output_tie])  # モデルの入力と出力を設定

model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # モデルをコンパイル

# モデルの要約（構造）を表示
model.summary()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
6. モデルのトレーニング
</h1>
```

---The following area is a Code cell (cell numver is 12)---
```python
# モデルのコンパイル
model.compile(
    optimizer='adam',  # 最適化手法にAdamを使用
    loss=['categorical_crossentropy', 'categorical_crossentropy', 'categorical_crossentropy'],  # 損失関数にカテゴリカルクロスエントロピーを使用
    metrics=[['accuracy'], ['accuracy'], ['accuracy']]  # 各出力の精度を評価指標で指定
)

# モデルのトレーニング
history = model.fit(
    [X_train_a, X_train_b, X_train_b],  # 訓練データ
    [y_train_a, y_train_b, y_train_tie],  # 訓練ラベル
    validation_data=([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie]),  # 検証データ
    epochs=5,  # エポック数は必要に応じて調整
    batch_size=32  # バッチサイズ
)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
7. モデルの評価
</h1>
```

---The following area is a Code cell (cell numver is 14)---
```python
# モデルを評価する
eval_results = model.evaluate([X_val_a, X_val_b, X_val_b], [y_val_a, y_val_b, y_val_tie])  # 検証データに対する評価を行う

# 結果を展開する
val_loss = eval_results[0]  # 検証損失
loss_a = eval_results[1]  # モデルAの損失
loss_b = eval_results[2]  # モデルBの損失
loss_tie = eval_results[3]  # 引き分けの損失
acc_a = eval_results[4]  # モデルAの精度
acc_b = eval_results[5]  # モデルBの精度
acc_tie = eval_results[6]  # 引き分けの精度

print(f'検証損失: {val_loss}')  # 検証損失を出力
print(f'モデルAの予測損失: {loss_a}')  # モデルAの損失を出力
print(f'モデルBの予測損失: {loss_b}')  # モデルBの損失を出力
print(f'引き分け予測の損失: {loss_tie}')  # 引き分けの損失を出力
print(f'モデルAの予測精度: {acc_a}')  # モデルAの精度を出力
print(f'モデルBの予測精度: {acc_b}')  # モデルBの精度を出力
print(f'引き分け予測の精度: {acc_tie}')  # 引き分けの精度を出力
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
8. 予測を行う
</h1>
```

---The following area is a Code cell (cell numver is 16)---
```python
# テストデータの前処理
X_test_prompt = text_to_sequences(test_df['prompt'])  # テストデータのプロンプトをシーケンスに変換
X_test_response_a = text_to_sequences(test_df['response_a'])  # テストデータの応答Aをシーケンスに変換
X_test_response_b = text_to_sequences(test_df['response_b'])  # テストデータの応答Bをシーケンスに変換

# 予測
predictions = model.predict([X_test_prompt, X_test_response_a, X_test_response_b])  # テストデータに対する予測を実施

# 予測結果を展開する
predicted_a = predictions[0].flatten()  # モデルAの予測結果を平坦化
predicted_b = predictions[1].flatten()  # モデルBの予測結果を平坦化
predicted_tie = predictions[2].flatten()  # 引き分けの予測結果を平坦化
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
# 
<h1 style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Comic Sans MS', cursive, sans-serif; 
    padding: 15px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
    border: 2px solid #3776ab;
">
9. 提出ファイルの準備
</h1>
```

---The following area is a Code cell (cell numver is 18)---
```python
# 提出用に予測をフォーマットする
submission = pd.DataFrame({
    'id': test_df['id'],  # テストデータのIDを取得
    'winner_model_a': predicted_a,  # モデルAの予測結果
    'winner_model_b': predicted_b,  # モデルBの予測結果
    'winner_tie': predicted_tie  # 引き分けの予測結果
})

# CSV形式で保存する
submission.to_csv('/kaggle/working/submission.csv', index=False)  # 提出ファイルをCSV形式で保存
```

---The following area is a Code cell (cell numver is 19)---
```python
submission.head()  # 提出ファイルの先頭5行を表示
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
<div style="
    background: #f0f8ff; 
    color: #3777ac; 
    font-family: 'Arial', sans-serif; 
    padding: 20px; 
    border-radius: 10px; 
    text-align: center; 
    box-shadow: 0 4px 8px rgba(0, 0, 0, 0.3);
    margin-top: 30px;
">
    <h2><strong>私のノートブックを探究していただきありがとうございます！ 🙏<strong></h2>
    <p>この<strong>LMSYS - AIチャット評価者</strong>プロジェクトを通じて、内容や洞察が価値あるものであったことを願っています。</p>
    <p>あなたのフィードバックとサポートは私にとって非常に大切です。このノートブックが楽しく、役立ったと感じた場合は、<span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"> <strong>アップボートを検討してください ⬆️</strong></span>。あなたの励ましは、私が今後も改善し、有用なコンテンツを共有し続けるための助けになります。</p>
    <p><span style="background-color: #dde9f4; color: #3777ac; padding: 5px 10px; border-radius: 5px;"><strong> 改めて、あなたの時間とサポートに感謝します！</strong></span></p>
</div>
```

** @@@ Jupyter Notebook numver 40, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleコンペティション「LMSYS - Chatbot Arena」において、大規模言語モデル（LLM）によって生成されたチャット応答の優劣を予測するための機械学習モデルを構築することを目的としています。具体的には、モデルが異なる応答の中でどちらがユーザーに好まれるかを予測し、データセットの特徴量作成やモデルのトレーニング・評価を行っています。

### 問題の概要
コンペティションでは、2つの異なるチャットボット（モデルAおよびモデルB）が生成した応答が与えられ、その中でどちらが好まれるかを予測することが求められています。データセットには、これらの応答や、それに対するユーザーの好み（勝者）が含まれています。

### 使用される手法とライブラリ
1. **データの読み込みと前処理**
   - `pandas`を用いてデータセットを読み込み、基本的な情報を確認する。
   - 各応答の特徴量を生成するために、句読点の数、言葉の多様性（ユニークな単語数）、ストップワードの比率、応答の長さ、先頭文字の大文字チェックなどの計算を行う。

2. **特徴量エンコード**
   - `OneHotEncoder`や`LabelEncoder`を使った勝者やモデル名のエンコード。
   - 機械学習モデルに適した形にデータを加工。

3. **機械学習モデルの構築**
   - `train_test_split`でデータを訓練セットとテストセットに分割し、`StandardScaler`を使用してデータの標準化を行う。
   - `RandomForestClassifier`を利用してモデルをトレーニングし、テストセットに対して予測を行い、混同行列や分類レポートで評価。

4. **感情分析**
   - `TextBlob`ライブラリを使用して、応答の感情分析を行い、感情スコアやエラーカウントを新しい特徴量として追加。

5. **結果の保存**
   - 最終的な応答や勝者モデルを含むデータをCSVファイルとしてエクスポートし、勝者モデルに基づく集計を行う。

このNotebookでは、様々な特徴量エンジニアリングや機械学習手法をインタラクティブに駆使することで、モデルの性能を向上させ、チャットボットの応答に対するユーザーの好みを予測するタスクに取り組んでいます。
```

---The following area is a Code cell (cell numver is 1)---
```python
# この Python 3 環境には、多くの便利な分析ライブラリがインストールされています
# これは kaggle/python Docker イメージによって定義されています: https://github.com/kaggle/docker-python
# たとえば、以下のいくつかの便利なパッケージをロードします

import numpy as np # 線形代数
import pandas as pd # データ処理、CSVファイルの入出力 (例: pd.read_csv)

# 入力データファイルは読み取り専用の "../input/" ディレクトリにあります
# 例えば、これを実行すると（実行ボタンをクリックするか、Shift + Enterを押すことで）、入力ディレクトリ内のすべてのファイルがリストされます

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ (/kaggle/working/) に最大20GBのデータを書き込むことができ、
# 「すべてを保存して実行」オプションでバージョンを作成すると、その結果が保存されます
# 一時ファイルを /kaggle/temp/ に書き込むこともできますが、
# 現在のセッション外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd

# データセットを読み込む
df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# データセットの最初の数行を確認する
df.head()
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
from textblob import TextBlob
import string
```

---The following area is a Code cell (cell numver is 4)---
```python
from sklearn.preprocessing import OneHotEncoder

# 各モデルの応答に含まれる句読点の数を特徴量として追加する
df['response_a_punctuation_count'] = df['response_a'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))
df['response_b_punctuation_count'] = df['response_b'].apply(lambda x: sum([1 for char in x if char in string.punctuation]))

# 言葉の多様性を示す特徴量を追加する
df['response_a_unique_words'] = df['response_a'].apply(lambda x: len(set(x.split())))
df['response_b_unique_words'] = df['response_b'].apply(lambda x: len(set(x.split())))

# ストップワードの比率を示す特徴量を追加する（例としてストップワードリストを使用）
stop_words = set(['the', 'and', 'is', 'in', 'at', 'of', 'it', 'to'])
df['response_a_stop_words_ratio'] = df['response_a'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))
df['response_b_stop_words_ratio'] = df['response_b'].apply(lambda x: len([word for word in x.lower().split() if word in stop_words]) / len(x.split()))

# 文の最初の文字が大文字であるかを示す特徴量を追加する
df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: 1 if x[0].isupper() else 0)
df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: 1 if x[0].isupper() else 0)

# 応答の長さとその差を示す特徴量を追加する
df['response_a_length'] = df['response_a'].apply(len)
df['response_b_length'] = df['response_b'].apply(len)
df['response_length_difference'] = df['response_a_length'] - df['response_b_length']

# 勝者モデルの列を追加する
df['winner'] = df.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1)

# モデル名の列を追加する
df['model_name'] = df.apply(lambda row: row['model_a'] if row['winner'] == 'model_a' else row['model_b'] if row['winner'] == 'model_b' else '', axis=1)

# One-hotエンコーディングを実施する
encoder = OneHotEncoder()
winner_encoded = encoder.fit_transform(df[['winner']])

# エンコードされた列をデータフレームに追加する
df_encoded = pd.concat([df, pd.DataFrame(winner_encoded.toarray(), columns=encoder.categories_[0])], axis=1)

# 結果を表示する
```

---The following area is a Code cell (cell numver is 5)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# データセットを準備するための関数
def prepare_data_for_ml(df):
    # 句読点のカウントを行う関数
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;:?')

    # 新しい列を追加する
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation)
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation)
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper()))
    df['response_a_length'] = df['response_a'].str.len() # 応答の長さを計算
    df['response_b_length'] = df['response_b'].str.len() # 応答の長さを計算
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length']) # 応答の長さの絶対差を計算

    # 勝者のエンコードされた列を作成
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner']) # 勝者情報を数値でエンコードする

    # モデル名の列を作成
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b']) # 勝ったモデルを表示

    # カテゴリ変数をエンコードする
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col]) # カテゴリ変数を数値にエンコードする

    # ブール値を0と1に変換
    df['model_a_win'] = df['winner'] == 'model_a'
    df['model_b_win'] = df['winner'] == 'model_b'
    df['winner_tie'] = df['winner'] == 'tie'
    df['tie'] = df['winner'] == 'tie'

    boolean_columns = ['response_b_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int) # ブール値を整数に変換

    return df

# データセットを準備する
prepared_df = prepare_data_for_ml(df)

# 必要な列を選択し、欠損列を確認する
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]
```

---The following area is a Code cell (cell numver is 6)---
```python
prepared_df.head()
```

---The following area is a Code cell (cell numver is 7)---
```python
# データを準備する
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncodedを使用してエンコードされたターゲット変数

# データをトレーニングセットとテストセットに分割する
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# データを標準化する
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train) # トレーニングデータを標準化
X_test_scaled = scaler.transform(X_test) # テストデータを標準化

# モデルをトレーニングする
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train) # モデルをトレーニングデータでフィット

# モデルを使用して予測を行う
y_pred = model.predict(X_test_scaled) # テストデータで予測を行う

# 結果を評価する
print("混同行列:\n", confusion_matrix(y_test, y_pred)) # 混合の行列を显示
print("\n分類レポート:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie'])) # 各モデルのパフォーマンスを表示
```

---The following area is a Code cell (cell numver is 8)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

# データセットを準備するための関数
def prepare_data_for_ml(df):
    # 句読点のカウントを行う関数
    def count_punctuation(text):
        return sum(1 for char in str(text) if char in '.,;!?') # 句読点の種類を数えます

    # 新しい列を追加する
    df['response_a_punctuation_count'] = df['response_a'].apply(count_punctuation) # モデルAの応答の句読点数
    df['response_b_punctuation_count'] = df['response_b'].apply(count_punctuation) # モデルBの応答の句読点数
    df['response_b_startswith_upper'] = df['response_b'].apply(lambda x: int(str(x)[0].isupper())) # モデルBの応答が大文字で始まるか
    df['response_a_startswith_upper'] = df['response_a'].apply(lambda x: int(str(x)[0].isupper())) # モデルAの応答が大文字で始まるか
    df['response_a_length'] = df['response_a'].str.len() # モデルAの応答の長さ
    df['response_b_length'] = df['response_b'].str.len() # モデルBの応答の長さ
    df['response_length_difference'] = abs(df['response_a_length'] - df['response_b_length']) # 長さの絶対値の差を計算

    # 勝者のエンコードされた列を作成
    le = LabelEncoder()
    df['winner_encoded'] = le.fit_transform(df['winner']) # 勝者をエンコードする

    # モデル名の列を作成
    df['model_name'] = np.where(df['winner_model_a'] == 1, df['model_a'], df['model_b']) # 勝者モデルの名前をつける

    # カテゴリ変数をエンコードする
    categorical_columns = ['model_a', 'model_b', 'model_name']
    for col in categorical_columns:
        le = LabelEncoder()
        df[f'{col}_encoded'] = le.fit_transform(df[col]) # カテゴリ変数を数値にエンコード

    # ブール値を0と1に変換
    df['model_a_win'] = df['winner'] == 'model_a' # モデルAが勝つ場合
    df['model_b_win'] = df['winner'] == 'model_b' # モデルBが勝つ場合
    df['winner_tie'] = df['winner'] == 'tie' # 引き分けかどうか
    df['tie'] = df['winner'] == 'tie' # 引き分けかどうかを示す

    boolean_columns = ['response_b_startswith_upper', 'response_a_startswith_upper', 'model_a_win', 'model_b_win', 'winner_tie', 'tie']
    for col in boolean_columns:
        df[col] = df[col].astype(int) # ブール値を整数型に変換

    return df

# データセットを準備する
prepared_df = prepare_data_for_ml(df)

# 必要な列を選択し、欠損列を確認する
columns_to_keep = [
    'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
    'winner', 'model_a_win', 'model_b_win', 'winner_tie',
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_b_startswith_upper', 'response_a_startswith_upper',  # response_a_startswith_upper を追加
    'response_a_length', 'response_b_length',
    'response_length_difference', 'winner_encoded',
    'model_name', 'tie', 'model_a_encoded', 'model_b_encoded', 'model_name_encoded'
]

prepared_df = prepared_df[columns_to_keep]

# データを準備する
X = prepared_df[['response_a_punctuation_count', 'response_b_punctuation_count',
        'response_a_startswith_upper', 'response_b_startswith_upper',
        'response_a_length', 'response_b_length', 'response_length_difference']]
y = prepared_df['winner_encoded']  # LabelEncodedを使用してエンコードされたターゲット変数

# データをトレーニングセットとテストセットに分割する
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# データを標準化する
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train) # トレーニングデータを標準化
X_test_scaled = scaler.transform(X_test) # テストデータを標準化

# モデルをトレーニングする
model = RandomForestClassifier(random_state=42)
model.fit(X_train_scaled, y_train) # モデルをトレーニングデータで適合

# モデルを使用して予測を行う
y_pred = model.predict(X_test_scaled) # テストデータで予測を行う

# 結果を評価する
print("混同行列:\n", confusion_matrix(y_test, y_pred)) # 混合行列を表示
print("\n分類レポート:\n", classification_report(y_test, y_pred, target_names=['model_a', 'model_b', 'tie'])) # モデルのパフォーマンスを表示
```

---The following area is a Code cell (cell numver is 9)---
```python
df['winner']  # 勝者の列を表示する
```

---The following area is a Code cell (cell numver is 10)---
```python
import pandas as pd
import numpy as np
import string
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.impute import SimpleImputer
```

---The following area is a Code cell (cell numver is 11)---
```python
# 勝者モデルをエンコードする
label_encoder = LabelEncoder()
df['winner_encoded'] = label_encoder.fit_transform(df['winner']) # 勝者のラベルを数値にエンコードする

# 数値特徴量とテキスト特徴量の列を分ける
numeric_features = [
    'response_a_punctuation_count', 'response_b_punctuation_count',
    'response_a_unique_words', 'response_b_unique_words',
    'response_a_stop_words_ratio', 'response_b_stop_words_ratio',
    'response_a_length', 'response_b_length', 'response_length_difference'
]

text_features = ['prompt', 'response_a', 'response_b']

# ColumnTransformerを作成する
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numeric_features), # 数値特徴量を標準化する
        ('text', 'passthrough', text_features)  # テキスト特徴量はそのまま通過させる
    ])

# データをX（特徴量）とy（ターゲット変数）に分ける
X = df[numeric_features + text_features]
y = df['winner_encoded']

# トレーニングセットとテストセットを作成する
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# パイプラインを作成する
pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(random_state=42)) # ランダムフォレストモデルを使用する
])

# モデルを訓練する
pipeline.fit(X_train, y_train)

# テストセットでモデルを評価する
accuracy = pipeline.score(X_test, y_test)
print(f"モデルの精度: {accuracy}") # モデルの精度を表示する
```

---The following area is a Code cell (cell numver is 12)---
```python
import pandas as pd

# CSVファイルを読み込む
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# データを結合する
df = pd.concat([train_df, test_df], ignore_index=True)

# 結果を確認する
df.head() # 結合したデータの最初の数行を表示する
```

---The following area is a Code cell (cell numver is 13)---
```python
df.shape # データフレームの形状を表示する
```

---The following area is a Code cell (cell numver is 14)---
```python
len(df.id) # データフレームのIDの数を表示する
```

---The following area is a Code cell (cell numver is 15)---
```python
df.info() # データフレームの情報を表示する
```

---The following area is a Code cell (cell numver is 16)---
```python
winner_counts = {
    'model_a': df['winner_model_a'].value_counts(), # model_aの勝者数をカウントする
    'model_b': df['winner_model_b'].value_counts(), # model_bの勝者数をカウントする
    'tie': df['winner_tie'].sum() # 引き分けの合計をカウントする
}

print(winner_counts) # 勝者のカウントを表示する
```

---The following area is a Code cell (cell numver is 17)---
```python
# winner_model_aとwinner_model_bがともに0である行をフィルタリングする
zero_winner_df = df[(df['winner_model_a'] == 1.0) & (df['winner_model_b'] == 1.0)].astype("float64")
```

---The following area is a Code cell (cell numver is 18)---
```python
# winner_model_aが1である行をフィルタリングする
model_a_winner_df = df[df['winner_model_a'] == 1.0]

# winner_model_bが1である行をフィルタリングする
model_b_winner_df = df[df['winner_model_b'] == 1.0]

# 1である行におけるmodel_aとmodel_bの最も多く出現する値を取得する
model_a_counts = model_a_winner_df['model_a'].value_counts() # model_aの出現回数をカウント
model_b_counts = model_b_winner_df['model_b'].value_counts() # model_bの出現回数をカウント

print("model_aの最も多く出現する値 (combined):")
print(model_a_counts.head()) # model_aで最も多く出現する値の上位を表示

print("\nmodel_bの最も多く出現する値 (combined):")
print(model_b_counts.head()) # model_bで最も多く出現する値の上位を表示
```

---The following area is a Code cell (cell numver is 19)---
```python
# model_a列での最も多く出現する応答を取得する
response_a_counts = model_a_winner_df['response_a'].value_counts() # model_aの応答の出現回数をカウント
response_b_counts = model_b_winner_df['response_b'].value_counts() # model_bの応答の出現回数をカウント
```

---The following area is a Code cell (cell numver is 20)---
```python
# response_a列での最も多く出現する応答を取得し、勝者モデルに基づいてグループ分けする
response_a_counts_df = model_a_winner_df.groupby(['model_a', 'prompt']).size().reset_index(name='count') # model_aの応答をグループ化
# response_b列での最も多く出現する応答を取得し、勝者モデルに基づいてグループ分けする
response_b_counts_df = model_b_winner_df.groupby(['model_b', 'prompt']).size().reset_index(name='count') # model_bの応答をグループ化

print("response_aの最も多く出現する値 (combined):")
print(response_a_counts_df.head()) # model_aの最も多く出現する応答の上位を表示

print("\nresponse_bの最も多く出現する値 (combined):")
print(response_b_counts_df.head()) # model_bの最も多く出現する応答の上位を表示
```

---The following area is a Code cell (cell numver is 21)---
```python
response_a_counts_df # model_aの応答の出現回数データフレームを表示する
```

---The following area is a Code cell (cell numver is 22)---
```python
top_models = ['gpt-4-1106-preview', 'gpt-4-0613', 'gpt-3.5-turbo-0613', 'gpt-4-0314'] # 最も高性能なモデルのリスト
```

---The following area is a Code cell (cell numver is 23)---
```python
# 各モデルの最も多く出現する10の応答を取得する
for model in top_models:
    print(f"\nモデル: {model} - 最も多く出現する10の応答 (response_a):")
    top_responses_a = response_a_counts_df[response_a_counts_df['model_a'] == model].sort_values(by='count', ascending=False).head(10) # 出現回数でソートして上位10を取得
    print(top_responses_a)

    print(f"\nモデル: {model} - 最も多く出現する10の応答 (response_b):")
    top_responses_b = response_b_counts_df[response_b_counts_df['model_b'] == model].sort_values(by='count', ascending=False).head(10) # 出現回数でソートして上位10を取得
    print(top_responses_b)
```

---The following area is a Code cell (cell numver is 24)---
```python
# 勝者モデルの名前を決定し、新しい列を作成する関数
def determine_winner(row):
    if row['winner_model_a'] == 1.0:
        return row['model_a'] # model_aが勝者の場合
    elif row['winner_model_b'] == 1.0:
        return row['model_b'] # model_bが勝者の場合
    else:
        return 'tie' # 引き分けの場合

# 勝者列を作成する
df['winner'] = df.apply(determine_winner, axis=1)

# promptとwinner列のみを取得し、グループ化する
grouped_df = df[['prompt', 'winner']].groupby('prompt')['winner'].apply(list).reset_index() # 各promptに対する勝者をリスト化する

# CSVとして出力する
grouped_df.to_csv('prompt_winners.csv', index=False) # 結果をCSVファイルとして保存する

print("CSVファイルが正常に作成されました: 'prompt_winners.csv'") # メッセージ表示
```

---The following area is a Code cell (cell numver is 25)---
```python
prompt = pd.read_csv("/kaggle/working/prompt_winners.csv") # 作成したCSVファイルを読み込む
prompt.groupby("winner").count().reset_index().head(20) # 勝者ごとのカウントを表示する
```

---The following area is a Code cell (cell numver is 26)---
```python
# 勝者モデルに基づいてpromptをグループ化し、最も多く出現するものを見つける
winner_counts = prompt.explode('winner').groupby('winner').size().reset_index(name='count') # 各勝者の出現回数を集計する
winner_counts = winner_counts.sort_values(by='count', ascending=False) # 出現回数でソートする

# CSVとして出力する
winner_counts.to_csv('prompt_top_winners.csv', index=False) # 出現回数をCSVファイルとして保存する

print("CSVファイルが正常に作成されました: 'prompt_top_winners.csv'") # メッセージ表示
```

---The following area is a Code cell (cell numver is 27)---
```python
winner_counts.head(20) # 出現回数の上位20を表示する
```

---The following area is a Code cell (cell numver is 28)---
```python
# promptと勝者モデルの情報で出力を作成する
prompt_winners = df.groupby(['winner', 'prompt']).sum().reset_index() # 勝者とpromptでグループ化

prompt_winners # 結果を表示する
```

---The following area is a Code cell (cell numver is 29)---
```python
top_prompt = pd.read_csv("/kaggle/working/prompt_top_winners.csv") # 先程作成したCSVファイルを読み込む
top_prompt.head(20) # 最も多く出現する勝者の情報を表示する
```

---The following area is a Code cell (cell numver is 30)---
```python
# 'tie'の場合のresponse_aとresponse_bの値をグループ化して出力する
tie_responses = df[df['winner'] == 'tie'].groupby(['response_a', 'response_b']).sum().reset_index()[['response_a', 'response_b',"prompt"]] # 引き分けの応答をグループ化

tie_responses # 引き分けの応答を表示する
```

---The following area is a Code cell (cell numver is 31)---
```python
def determine_winner_and_loser(row):
    if row['winner_model_a'] == 1.0:
        winner = row['model_a'] # 勝者はmodel_a
        loser = row['model_b'] # 敗者はmodel_b
        winner_response = row['response_a'] # 勝者の応答はresponse_a
        loser_response = row['response_b'] # 敗者の応答はresponse_b
    elif row['winner_model_b'] == 1.0:
        winner = row['model_b'] # 勝者はmodel_b
        loser = row['model_a'] # 敗者はmodel_a
        winner_response = row['response_b'] # 勝者の応答はresponse_b
        loser_response = row['response_a'] # 敗者の応答はresponse_a
    else:
        winner = 'tie' # 引き分けの場合
        loser = 'tie' # 敗者も引き分け
        winner_response = '' # 勝者の応答は空
        loser_response = '' # 敗者の応答も空
    
    return pd.Series([winner, loser, len(winner_response), len(loser_response)], index=['winner', 'loser', 'winner_response_length', 'loser_response_length'])

# 勝者と敗者の列を作成する
winner_loser_lengths = df.apply(determine_winner_and_loser, axis=1) # 勝者と敗者の情報を取得

# 以前のデータフレームに追加して新しいデータフレームを作成する
prompt_df = pd.concat([df[['prompt']], winner_loser_lengths], axis=1) # promptと勝者・敗者情報を結合する

prompt_df.head(20) # 結果を表示する
```

---The following area is a Code cell (cell numver is 32)---
```python
# CSVファイルを読み込む
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# データを結合する
df = pd.concat([train_df, test_df], ignore_index=True) # トレーニングデータとテストデータを結合する
```

---The following area is a Code cell (cell numver is 33)---
```python
train_df # トレーニングデータフレームを表示する
```

---The following area is a Code cell (cell numver is 34)---
```python
df # 結合されたデータフレームを表示する
```

---The following area is a Code cell (cell numver is 35)---
```python
# Polars DataFrameを作成する
import polars as pl
from textblob import TextBlob
import numpy as np
```

---The following area is a Code cell (cell numver is 36)---
```python
import polars as pl
import numpy as np
from textblob import TextBlob

# Polars DataFrameを作成する
df = pl.DataFrame(df)

# 感情分析とエラー数カウントの関数を定義
def batch_analyze_sentiment(texts):
    return np.array([TextBlob(text).sentiment.polarity for text in texts]) # 各テキストの感情極性を取得

def batch_count_errors(texts):
    return np.array([text.lower().count('error') for text in texts]) # 各テキストでの"error"の数をカウント
    
# 新しい列を作成する
df = df.with_columns([
    pl.col('response_a').str.lengths().alias('response_a_length'), # response_aの長さを計算
    pl.col('response_b').str.lengths().alias('response_b_length'), # response_bの長さを計算
    (pl.col('response_a').str.lengths() - pl.col('response_b').str.lengths()).abs().alias('response_length_difference'), # 長さの差を正の絶対値にする
])

# 感情分析とエラー数計数のために最初の100行を処理
if len(df) > 0:
    n_rows = min(100, len(df)) # 処理する行数を決定
    
    sentiment_a = batch_analyze_sentiment(df['response_a'].head(n_rows).to_numpy()) # response_aの感情分析
    sentiment_b = batch_analyze_sentiment(df['response_b'].head(n_rows).to_numpy()) # response_bの感情分析
    errors_a = batch_count_errors(df['response_a'].head(n_rows).to_numpy()) # response_aのエラー数をカウント
    errors_b = batch_count_errors(df['response_b'].head(n_rows).to_numpy()) # response_bのエラー数をカウント

    # 新しい列を追加
    df = df.with_columns([
        pl.Series('sentiment_a', sentiment_a).extend(pl.Series([None] * (len(df) - n_rows))), # sentiment_aの列を追加
        pl.Series('sentiment_b', sentiment_b).extend(pl.Series([None] * (len(df) - n_rows))), # sentiment_bの列を追加
        pl.Series('errors_a', errors_a).extend(pl.Series([None] * (len(df) - n_rows))), # errors_aの列を追加
        pl.Series('errors_b', errors_b).extend(pl.Series([None] * (len(df) - n_rows))), # errors_bの列を追加
    ])

# 勝者と敗者の列を作成する（古いバージョンとの互換性）
df = df.with_columns([
    pl.when(pl.col('sentiment_a') > pl.col('sentiment_b')) # sentiment_aが大きければ
      .then(pl.col('response_a')) # winner_sentimentはresponse_a
      .when(pl.col('sentiment_a') <= pl.col('sentiment_b')) # sentiment_aが小さければ
      .then(pl.col('response_b')) # winner_sentimentはresponse_b
      .alias('winner_sentiment'), # 列名を設定
    pl.when(pl.col('sentiment_a') < pl.col('sentiment_b')) # sentiment_aが小さければ
      .then(pl.col('response_a')) # loser_sentimentはresponse_a
      .when(pl.col('sentiment_a') >= pl.col('sentiment_b')) # sentiment_aが大きければ
      .then(pl.col('response_b')) # loser_sentimentはresponse_b
      .alias('loser_sentiment') # 列名を設定
])

df # 結果を表示する
```

** @@@ Jupyter Notebook numver 41, the number of votes :3 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、Metaの**Llama3**（8Bモデル）をファインチューニングする方法を示しています。具体的には、**QLoRA**や**trl**ライブラリの**SFTTrainer**を使用して、大規模言語モデル（LLM）のパラメータ効率の高い調整（PEFT）を行うプロセスに取り組んでいます。

### 問題の対象
ノートブックは主に、Chatbot Arenaコンペティションに関連するデータセットを用いて、どのモデルの応答がユーザーに好まれるかを予測する問題に取り組んでいます。具体的には、2つの異なるモデル（応答Aと応答B）からの出力に基づき、どちらが好まれるかを分類するタスクです。

### 使用している手法
- **QLoRA**: Low-Rank Adaptation（LoRA）に基づき、ファインチューニング中のメモリ使用量を削減しつつ、モデルのパフォーマンスを維持または向上させる手法です。これにより、大規模モデルを少ないGPUで効率的に訓練できます。
- **SFTTrainer**: Hugging Faceの**trl**ライブラリによって提供されるシンプルなAPIを用いており、ファインチューニングプロセスを容易にします。
- **PEFT**: 事前訓練済みモデルのパラメータを再利用することで、計算資源と時間を節約しながら、タスク特有の性能を引き上げる技術です。

### 利用ライブラリ
- **Transformers**: 自然言語処理のための高度なモデルを提供し、モデルのロードやファインチューニングが可能です。
- **Torch**: ディープラーニングフレームワークとして使用され、モデルの構築や訓練に利用されています。
- **PEFT**: パラメータ効率の良いファインチューニングを行うためのライブラリです。

このノートブックは、事前データ処理、モデルの設定、学習のトレーニングを行い、最後にはモデルのパフォーマンスを評価することで完了します。最終的に、予測結果をサンプルとして表示することも含まれており、コンペティションの要件に従ったデータサイエンスプロジェクトの構築に役立つ内容となっています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# はじめに

このノートブックでは、**QLoRA** と **tlr** の **SFTTrainer** を使用して、Meta の **Llama3** (8B) モデルをファインチューニングする方法を示します。

## Llama3 とは？

Llama3 は、Meta が提供するオープンソースの最新の大規模言語モデル（LLM）で、8B および 70B パラメータを持つ事前訓練済みおよび指示に基づいてファインチューニングされた言語モデルを特徴としています。

## LoRA とは？

LoRA は Low-Rank Adaptation の略です。これは、大規模言語モデル（LLMs）をファインチューニングするための方法で、LLM の重みを固定し、トレーニング可能なランク分解行列を注入します。ファインチューニング中のトレーニング可能なパラメータの数は、かなり減少します。LoRA の論文によると、この数は 10,000 倍減少し、計算資源のサイズは 3 倍減少するとされています。

## QLoRA とは？

QLoRA は LoRA に基づいて、モデルの性能を維持または向上させながらメモリ使用量をさらに削減するために量子化手法を組み合わせたものです。QLoRA を使用すると、36 台の GPU が必要な 70B パラメータモデルをわずか 2 台の GPU でファインチューニングすることが可能になります！

## PEFT とは？

パラメータ効率の良いファインチューニング（PEFT）は、特定の下流タスクにおいて事前訓練した言語モデルの性能を向上させるための技術です。これは、事前訓練済みモデルのパラメータを再利用し、より小さなデータセットでファインチューニングすることで、全モデルをゼロからトレーニングするのに比べて、計算資源と時間を節約します。PEFT は、事前訓練済みモデルのいくつかの層を固定し、下流タスクに特有の最後の数層だけをファインチューニングすることによって、この効率が実現されます。

## SFTTrainer とは？

SFTTrainer の SFT はフォローアップのファインチューニングを指します。HuggingFace の **trl** (Transformer Reinforcement Learning) ライブラリは、SFTTrainer を使用してモデルをファインチューニングするためのシンプルな API を提供します。

## UltraChat200k とは？

UltraChat-200k は、自然言語理解、生成、および対話システム研究にとって非常に貴重なリソースです。多数のトピックにわたる 1.4 百万の対話を含み、この parquet 形式のデータセットは、研究者が研究を支援するために 4 つの異なるフォーマットを提供します: test_sft, train_sft, train_gen, test_gen。詳細は [こちら](https://www.kaggle.com/datasets/thedevastator/ultrachat-200k-nlp-dataset)をご覧ください。

## インスピレーション

このノートブックでは、いくつかの情報源からインスピレーションを得ました：
* [PyTorch FSDP と Q-Lora を使用して Llama 3 を効率的にファインチューニングする](https://www.philschmid.de/fsdp-qlora-llama3)  
* [LoRA を使用した LLM のファインチューニング](https://medium.com/@rajatsharma_33357/fine-tuning-llama-using-lora-fb3f48a557d5)  
* [低コストリソースを使用した Llama-3–8B–Instruct QLORA のファインチューニング](https://medium.com/@avishekpaul31/fine-tuning-llama-3-8b-instruct-qlora-using-low-cost-resources-89075e0dfa04)  
* [Gaudi 2 プロセッサ上での Low-Rank Adaptations (LoRA) を用いた Llama2 のファインチューニング](https://eduand-alvarez.medium.com/llama2-fine-tuning-with-low-rank-adaptations-lora-on-gaudi-2-processors-52cf1ee6ce11)  

# ライブラリのインストールとインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes  # bitsandbytesライブラリをインストールします。これは、低ランクモデルのファインチューニングに役立つツールです。
!pip install -q -U transformers  # transformersライブラリをインストールします。このライブラリは、自然言語処理のための高度な機械学習モデルを提供します。
!pip install -q -U peft  # パラメータ効率の良いファインチューニングを行うためのpeftライブラリをインストールします。
!pip install -q -U accelerate  # accelerateライブラリをインストールします。これは、訓練の高速化や効率的な実行を支援します。
!pip install -q -U datasets  # データセットライブラリをインストールします。さまざまなデータセットにアクセスして利用するためのライブラリです。
!pip install -q -U trl  # トランスフォーマーの強化学習ライブラリであるtrlをインストールします。モデルの強化学習に役立ちます。
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd  # データ操作を容易にするためのpandasライブラリをインポートします。これはデータフレームの作成や操作に役立ちます。
from torch.utils.data import Dataset  # PyTorchのデータセットクラスをインポートします。カスタムデータセットを作成するために使用します。
from torch.utils.data import DataLoader  # PyTorchのデータローダークラスをインポートします。バッチ処理やデータのシャッフルをサポートします。
```

---The following area is a Code cell (cell numver is 4)---
```python
# from kaggle_secrets import UserSecretsClient  # Kaggleの秘密情報にアクセスするためのUserSecretsClientをインポートします。
# user_secrets = UserSecretsClient()  # UserSecretsClientのインスタンスを作成します。
# wandb_key = user_secrets.get_secret("wandb_api")  # WandB（Weights & Biases）APIキーを取得します。このキーはWandBの機能を利用するために必要です。
# import wandb  # WandBライブラリをインポートします。これは、実験の追跡や可視化に役立ちます。
# ! wandb login $wandb_key  # 取得したWandB APIキーを使ってWandBにログインします。これにより、実験データをWandBにアップロードできるようになります。
```

---The following area is a Code cell (cell numver is 5)---
```python
import os  # オペレーティングシステムの機能を提供するosライブラリをインポートします。ファイルパスなどの操作に使用します。
import torch  # PyTorchライブラリをインポートします。ディープラーニングモデルの構築とトレーニングに使用します。
from time import time  # 現在の時刻を取得するためのtime関数をインポートします。トレーニング時間の計測に使用します。
from datasets import load_dataset  # datasetsライブラリからload_dataset関数をインポートします。データセットを簡単に読み込むために使用します。
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training  # PEFT（パラメータ効率の良いファインチューニング）に関連するクラスや関数をインポートします。
from transformers import (  # transformersライブラリから様々なクラスや関数をインポートします。
    AutoConfig,  # モデルの自動設定を行うためのクラスをインポートします。
    AutoModelForCausalLM,  # 自然言語生成用の因果モデルを自動で取得するためのクラスをインポートします。
    AutoTokenizer,  # モデルに適したトークナイザーを自動で取得するためのクラスをインポートします。
    BitsAndBytesConfig,  # メモリ効率化のためのビットおよびバイト設定を管理するクラスをインポートします。
    AutoTokenizer,  # トークナイザーを再度インポート（重複）していますが、必要に応じて実際のインポートを行います。
    TrainingArguments,  # トレーニングの際の引数を設定するためのクラスをインポートします。
    AutoModelForSequenceClassification,  # シーケンス分類用のモデルを自動で取得するためのクラスをインポートします。
    Trainer  # モデルのトレーニングを管理するための教師クラスをインポートします。
)
from trl import SFTTrainer, setup_chat_format  # SFTTrainerクラスとチャット形式を設定する関数をインポートします。ファインチューニングに使用します。
import numpy as np  # 数値計算を効率的に行うためのnumpyライブラリをインポートします。配列や行列の操作に便利です。
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:  # コンフィグレーションクラスを定義します。モデルのトレーニングに関連する設定を保持します。
    NUM_EPOCHS = 1  # エポック数を設定します。モデルの学習を何回繰り返すかを決定します。
    BATCH_SIZE = 16  # バッチサイズを設定します。一度に処理するサンプルの数です。
    DROPOUT = 0.05  # ドロップアウト率を設定します。過学習を防ぐために、一部のニューロンの出力を確率的に無効にします。
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # 使用するモデルのパスを定義します。

    SEED = 2024  # ランダムシードを設定します。再現性のある結果を得るために使用します。
    MAX_LENGTH = 1024  # 最大入力長を設定します。モデルへの入力として受け入れる最大トークン数です。
    NUM_WARMUP_STEPS = 128  # ウォームアップステップの数を設定します。学習率を徐々に増加させるステップ数です。
    LR_MAX = 5e-5  # 学習率の最大値を設定します。これにより、モデルが効果的に学習できるようになります。
    NUM_LABELS = 3  # 分類タスクにおけるラベルの数を設定します。
    LORA_RANK = 4  # LoRAのランクを設定します。モデルのランク近似の次元を決定します。
    LORA_ALPHA = 8  # LoRAで使用するスケーリング係数を設定します。
    LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモデルのモジュール名をリストとして定義します。ここでは出力プロジェクションとバリュー仮想プロジェクションを指定しています。
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データセットの準備
```

---The following area is a Code cell (cell numver is 8)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータセットをCSVファイルから読み込みます。

def process(input_str):  # 入力文字列を処理する関数を定義します。
    stripped_str = input_str.strip('[]')  # 文字列の前後の角括弧を取り除きます。
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文字列を分割し、余分な引用符を取り除きます。
    return  ' '.join(sentences)  # 分割した文を空白で結合して返します。

# 'prompt'、'response_a'、'response_b' 列に対して process 関数を適用します。
train.loc[:, 'prompt'] = train['prompt'].apply(process)  # プロンプト列の処理。
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # モデルAの応答を処理。
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # モデルBの応答を処理。

# トレーニング用に 'Null' をドロップします。
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 応答Aと応答Bが両方とも'null'である行のインデックスを取得します。
train.drop(indexes, inplace=True)  # その行をデータフレームから削除します。
train.reset_index(inplace=True, drop=True)  # インデックスをリセットします。
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # 各行に対して勝者モデルのラベルを計算して追加します。

print(f"Total {len(indexes)} Null response rows dropped")  # ドロップした行数を表示します。
print('Total train samples: ', len(train))  # 残りのトレーニングサンプル数を表示します。

# テキスト形式にデータを整形します。
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])  # フォーマットされたテキストの5番目のサンプルを表示します。
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# モデルの初期化

使用されるモデルは以下の通りです：

* **モデル**: Llama3  
* **フレームワーク**: Transformers  
* **サイズ**: 8B  
* **タイプ**: 8b-chat-hf (hfはHuggingFaceを指します)  
* **バージョン**: V1
```

---The following area is a Code cell (cell numver is 10)---
```python
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification  # ビットおよびバイトの設定とシーケンス分類用のモデルをインポートします。

# 量子化設定を定義します。
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビットでモデルをロードするように設定します。
    bnb_4bit_quant_type='nf4',  # 4ビット量子化のタイプを設定します。ここでは「nf4」を指定します。
    bnb_4bit_use_double_quant=True,  # 二重量子化を使用する設定です。これにより、さらにメモリを節約します。
    bnb_4bit_compute_dtype=torch.bfloat16  # 計算時に使用するデータ型をtorchのbfloat16に設定します。これにより、計算効率が向上します。
)

model_name = "/kaggle/input/llama-3/transformers/8b-chat-hf/1"  # 使用するモデルの名前とパスを指定します。

# 特徴を持つモデルを事前訓練済みから読み込みます。
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,  # 読み込むモデルの名前。
    quantization_config=quantization_config,  # 量子化設定を適用します。
    num_labels=4,  # モデルで扱うラベルの数を設定します。
    device_map='auto'  # デバイスマップを自動で設定し、利用可能なGPUを使用します。
)
```

---The following area is a Code cell (cell numver is 11)---
```python
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model  # LoRA関連のクラスと関数をインポートします。

# LoRAの設定を定義します。
lora_config = LoraConfig(
    r=16,  # LoRAのランクを設定します。ここでは16を指定します。
    lora_alpha=8,  # LoRAで使用するスケーリング係数を設定します。
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # LoRAを適用するターゲットモジュールを指定します。ここでは、クエリ、キー、バリュー、および出力プロジェクションを指定します。
    lora_dropout=0.05,  # LoRAのドロップアウト率を設定します。これにより、過学習を防ぎます。
    bias='none',  # バイアスの設定をしていません。バイアスを使用しないことを示します。
    task_type='SEQ_CLS'  # タスクのタイプをシーケンス分類に設定します。
)

# モデルをkbitトレーニング用に準備します。
model = prepare_model_for_kbit_training(model)  
# LoRA設定を使用してモデルを取得（適用）します。
model = get_peft_model(model, lora_config)
```

---The following area is a Code cell (cell numver is 12)---
```python
from transformers import AutoTokenizer  # トークナイザーを自動で取得するためのクラスをインポートします。

# 事前訓練済みのトークナイザーをモデル名から読み込みます。このとき、トークンの前にスペースを追加する設定をします。
tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)

# パディングトークンIDを終了トークンIDに設定します。これにより、パディング時に同じトークンを使用します。
tokenizer.pad_token_id = tokenizer.eos_token_id
# パディングトークンを終了トークンに設定します。これにより、パディング時に使用するトークンを明示的に指定します。
tokenizer.pad_token = tokenizer.eos_token
```

---The following area is a Code cell (cell numver is 13)---
```python
model.config.pad_token_id = tokenizer.pad_token_id  # モデルの設定にパディングトークンIDを追加します。
model.config.use_cache = False  # キャッシュの使用を無効にします。これにより、メモリ使用量が削減されます。
model.config.pretraining_tp = 1  # 事前訓練時のテンソル並列処理の設定を1にします。これはモデルの並列処理に影響します。
```

---The following area is a Code cell (cell numver is 14)---
```python
from datasets import DatasetDict, Dataset  # データセットに関連するクラスをインポートします。

def data_preprocesing(row):  # 各行に対してデータ前処理を行う関数を定義します。
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')  # テキストをトークン化し、パディング、切り捨て、トーチテンソルで返します。

# pandasデータフレームからデータセットを作成します。
dataset = Dataset.from_pandas(train)  
# データセットをトークン化し、'text'列を削除します。
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])  
# トークン化されたデータのフォーマットをtorchに設定します。
tokenized_data.set_format("torch")
```

---The following area is a Code cell (cell numver is 15)---
```python
import gc  # ガーベジコレクションを管理するためのgcライブラリをインポートします。

del train  # メモリ使用量を削減するために、trainデータフレームを削除します。
gc.collect()  # ガーベジコレクションを手動で実行し、不要なメモリを解放します。
```

---The following area is a Code cell (cell numver is 16)---
```python
from transformers import DataCollatorWithPadding  # パディングを扱うためのデータコレレーターをインポートします。

# トークナイザーを使ってパディングを行うデータコレーション関数を作成します。
collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)  # データコレレーターを初期化します。これにより、バッチ内のすべてのサンプルが同じ長さになるようにパディングされます。
```

---The following area is a Code cell (cell numver is 17)---
```python
def compute_metrics(evaluations):  # 評価指標を計算する関数を定義します。
    predictions, labels = evaluations  # 評価から予測値とラベルを取得します。
    predictions = np.argmax(predictions, axis=1)  # 予測結果から最大値のインデックスを取得し、クラスラベルに変換します。
    # バランス精度と通常の精度を計算し、辞書形式で返します。
    return {
        'balanced_accuracy': balanced_accuracy_score(predictions, labels),  # バランス精度を計算します。
        'accuracy': accuracy_score(predictions, labels)  # 通常の精度を計算します。
    }
```

---The following area is a Code cell (cell numver is 18)---
```python
import torch.nn.functional as F  # PyTorchの関数型APIから機能をインポートします。

class CustomTrainer(Trainer):  # Trainerクラスを拡張したカスタムトレーナークラスを定義します。
    def __init__(self, *args, class_weights=None, **kwargs):  # コンストラクタを定義し、クラス重みを受け取ります。
        super().__init__(*args, **kwargs)  # 親クラスのコンストラクタを呼び出します。
        if class_weights is not None:  # クラス重みが指定されている場合
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)  # テンソルとしてクラス重みを作成し、デバイスに移動します。
        else:
            self.class_weights = None  # クラス重みが指定されていない場合はNoneに設定します。

    def compute_loss(self, model, inputs, return_outputs=False):  # 損失を計算するメソッドを定義します。
#         print(inputs)  # デバッグ用に入力値を表示（コメントアウトされています）。
        labels = inputs.pop("labels").long()  # 入力からラベルを取り出し、long型に変換します。

        outputs = model(**inputs)  # モデルを入力して予測を得ます。

        logits = outputs.get('logits')  # モデルの出力からロジットを取得します。

        if self.class_weights is not None:  # クラス重みが指定されている場合
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)  # 重み付きの交差エントロピー損失を計算します。
        else:
            loss = F.cross_entropy(logits, labels)  # 通常の交差エントロピー損失を計算します。

        return (loss, outputs) if return_outputs else loss  # 出力を返すか、損失のみを返します。
```

---The following area is a Code cell (cell numver is 19)---
```python
training_args = TrainingArguments(  # トレーニングの引数を設定するためのTrainingArgumentsオブジェクトを作成します。
    output_dir='sentiment_classification',  # モデルの出力先ディレクトリを指定します。
    learning_rate=1e-4,  # 学習率を設定します。
    per_device_train_batch_size=8,  # 各デバイスでのトレーニングバッチサイズを設定します。
    per_device_eval_batch_size=8,  # 各デバイスでの評価バッチサイズを設定します。
    num_train_epochs=1,  # トレーニングのエポック数を設定します。
    logging_steps=1,  # ロギングを行うステップ間隔を設定します。
    weight_decay=0.01,  # 重み減衰率を設定します。これにより過学習を防ぎます。
    evaluation_strategy='epoch',  # 評価の戦略としてエポック毎に評価を設定します。
    save_strategy='epoch',  # モデルの保存戦略としてエポック毎に保存を設定します。
    load_best_model_at_end=True,  # トレーニングの終了時に最良のモデルを読み込む設定です。
    report_to="none"  # ロギングを行わない設定です。
)
```

---The following area is a Code cell (cell numver is 20)---
```python
trainer = CustomTrainer(  # カスタムトレーナーのインスタンスを作成します。
    model=model,  # 使用するモデルを指定します。
    args=training_args,  # トレーニング引数を指定します。
    train_dataset=tokenized_data,  # トレーニングデータセットを指定します。
    tokenizer=tokenizer,  # 使用するトークナイザーを指定します。
    data_collator=collate_fn,  # データコレーターを指定します。バッチのパディングを行います。
    compute_metrics=compute_metrics,  # 評価指標を計算する関数を指定します。
#     class_weights=class_weights,  # ここでクラス重みを使用する場合は、コメントを外します。
)

train_result = trainer.train()  # トレーニングを開始し、結果を取得します。
```

---The following area is a Code cell (cell numver is 21)---
```python

```

** @@@ Jupyter Notebook numver 42, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、LMSYS - Chatbot Arenaのコンペティションにおける「人間による好み予測」のための機械学習モデルを構築しています。具体的には、ユーザーが好むチャットボットの応答を予測することを目的としています。ノートブックはPythonを使用し、さまざまなデータ処理や深層学習技術を活用しています。

### 主な内容と手法

1. **ライブラリのインポート**:
   - データ処理には`pandas`と`numpy`を使用。
   - トークナイゼーションやモデルには、`transformers`ライブラリの各種トークナイザーとモデル（特にDeBERTa V2）を利用。
   - モデルの構築には`TensorFlow`とそのKeras APIを使用。

2. **データの読み込み**:
   - トレーニングデータ（`train.csv`）とテストデータ（`test.csv`）を読み込み、それぞれのデータフレームを作成。

3. **データの前処理**:
   - テキストデータの整形や結合を行う`combine_text`関数を定義し、プロンプトおよびレスポンスを処理。
   - ラベルを生成するための`create_label`関数も定義し、互換性のある形式でラベルを作成。

4. **トークナイゼーション**:
   - `DebertaV2Tokenizer`を用いて、プロンプトとレスポンスに対するトークンを生成し、必要なトークン型、注意マスクを作成。

5. **モデルの構築**:
   - CNN、LSTM、またはそのハイブリッドモデルとしてCNN-LSTMモデルを構築。モデルは、埋め込み層、畳み込み層、LSTM層を含み、最終的な出力層は3クラスのラベルに対応。

6. **モデルのトレーニング**:
   - モデルを訓練（フィッティング）するための設定を行い、早期終了の仕組みを導入。トレーニングは、トレーニングデータからバリデーションを使用して行われます。

7. **テストと予測**:
   - テストデータに対して同様の前処理を行い、モデルを用いて予測を実施。
   - 予測結果はデータフレームに変換され、最終的に`submission.csv`として保存されます。

全体を通じて、このノートブックは特に大規模言語モデルを活用したコンペティションに適したデータ処理とモデル設計に焦点を当てています。モデルの選択やトレーニング方法は、ユーザーの応答の好みを正確に予測するために設計されています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これは、kaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下のいくつかの便利なパッケージをロードします

import numpy as np # 線形代数ライブラリ
import pandas as pd # データ処理ライブラリ、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは、読み取り専用の"../input/"ディレクトリで利用できます
# 例えば、これを実行すると（実行ボタンをクリックするかShift + Enterを押すことで）、入力ディレクトリ内のすべてのファイルがリストされます

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        # 各ファイルのパスを表示します
        print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）に最大20GBまで書き込むことができます
# これは、「Save & Run All」を使用してバージョンを作成する際に出力として保存されます
# また、一時ファイルを/kaggle/temp/に書き込むこともできますが、これらは現在のセッションの外部に保存されません
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd  # データを処理するためのpandasライブラリをインポートします
import numpy as np  # 数値計算のためのnumpyライブラリをインポートします
from datasets import Dataset  # datasetsライブラリからDatasetクラスをインポートします（データセットの扱いに便利です）
from functools import partial  # 関数を部分的に適用するためのpartial関数をインポートします
from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割するためのtrain_test_split関数をインポートします
from transformers import AutoTokenizer, TFAutoModel  # Hugging Faceのトランスフォーマーモデルとトークナイザーをインポートします
from transformers import DebertaV2Tokenizer  # DeBERTa V2用のトークナイザーをインポートします
import tensorflow as tf  # 深層学習ライブラリのTensorFlowをインポートします
from tensorflow.keras.models import Sequential  # Kerasの順次モデルをインポートします
from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Input  # Kerasのレイヤーをインポートします
from keras.preprocessing import sequence as sq  # シーケンス処理のためのKerasのpreprocessingモジュールをインポートします
```

---The following area is a Code cell (cell numver is 4)---
```python
# train.csvファイルを読み込み、データをtrainデータフレームに格納します
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# データフレームの最初の5行を表示します
train.head(5)  # データの最初の5行を表示して、内容を確認します
```

---The following area is a Code cell (cell numver is 5)---
```python
# test.csvファイルを読み込み、データをtestデータフレームに格納します
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# データフレームの最初の5行を表示します
test.head(5)  # データの最初の5行を表示して、内容を確認します
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## 前処理
```

---The following area is a Code cell (cell numver is 7)---
```python
def combine_text(df):
    def process(input_str):
        # 文字列の前後のブラケットを取り除きます
        stripped_str = input_str.strip('[]')
        # 文字列を分割し、各文の前後の引用符を取り除きます
        sentences = [s.strip('"') for s in stripped_str.split('","')]
        # 文をスペースで結合して返します
        return  ' '.join(sentences)

    # 配列を単一の文字列に変換します
    df['prompt'] = df['prompt'].apply(process)  # 'prompt'列の各要素にprocess関数を適用します
    df['response_a'] = df['response_a'].apply(process)  # 'response_a'列の各要素にprocess関数を適用します
    df['response_b'] = df['response_b'].apply(process)  # 'response_b'列の各要素にprocess関数を適用します
    
    # テキストデータを結合します
    # df['combined_text'] = '[PROMPT] ' + df['prompt'] + ' [RESPONSE_A] ' + df['response_a'] + ' [RESPONSE_B] ' + df['response_b']  # 結合済みテキストを新しい列に格納するためのコメントがあります
```

---The following area is a Code cell (cell numver is 8)---
```python
combine_text(train)  # trainデータフレームに対してcombine_text関数を呼び出して、テキストを結合します

# print(train['combined_text'][69])  # 結合されたテキストの69番目の要素を表示するためのコメントがあります（現在はコメントアウトされています）
```

---The following area is a Code cell (cell numver is 9)---
```python
# ラベルを作成します
def create_label(df):
    def process(row):
        # model_aが勝者の場合、ラベル0を返します
        if row['winner_model_a'] == 1:
            return 0
        # model_bが勝者の場合、ラベル1を返します
        elif row['winner_model_b'] == 1:
            return 1
        # 引き分けの場合、ラベル2を返します
        elif row['winner_tie'] == 1:
            return 2
        
    # 各行に対してprocess関数を適用し、新しい'label'列を作成します
    df['label'] = df.apply(process, axis=1)  # axis=1は行ごとに処理することを示しています
```

---The following area is a Code cell (cell numver is 10)---
```python
create_label(train)  # trainデータフレームに対してcreate_label関数を呼び出して、ラベルを作成します
print(train['label'][69])  # 作成されたラベルの69番目の要素を表示します
```

---The following area is a Code cell (cell numver is 11)---
```python
# trainデータフレームの形状を表示します
print("train.shape", train.shape)  # 行数と列数を出力します

# trainデータフレームの最初の5行を表示します
train.head()  # データの最初の5行を確認して、内容を表示します
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
## トークナイザー
```

---The following area is a Code cell (cell numver is 13)---
```python
# トークンの最大長を設定します
max_length = 1024  # モデルに入力できるトークンの最大数を1024に設定します
```

---The following area is a Code cell (cell numver is 14)---
```python
# DebertaV2Tokenizerを使用したトークナイゼーション
model_name = "/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1"  # 使用するモデルのパスを指定します
# model_name = "/kaggle/input/deberta-v3/pytorch/large/1"  # 別のモデル名をコメントアウトしています
tokenizer = AutoTokenizer.from_pretrained(model_name)  # 指定したモデルからトークナイザーをロードします
# 語彙のサイズを制限します
# tokenizer.model_max_length = max_length  # 最大トークン長を設定するコメントがあります
tokenizer.add_tokens(['[CLS]', '[SEP]', '[PAD]'], special_tokens=True)  # 特殊トークンを追加します
```

---The following area is a Code cell (cell numver is 15)---
```python
def tokenize_df(df, tokenizer):
    # 特殊トークンが存在しない場合、チェックして設定します
    if tokenizer.cls_token_id is None:
        tokenizer.cls_token_id = tokenizer.convert_tokens_to_ids('[CLS]')  # '[CLS]'トークンのIDを設定します
    if tokenizer.sep_token_id is None:
        tokenizer.sep_token_id = tokenizer.convert_tokens_to_ids('[SEP]')  # '[SEP]'トークンのIDを設定します
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids('[PAD]')  # '[PAD]'トークンのIDを設定します
        
    def process(row):
        max_len = max_length - 2  # セパレータトークン2つ分を考慮した最大長を計算します
        # プロンプトをトークナイズします
        prompt_tokens = tokenizer(row['prompt'], truncation=True, max_length=max_len//4)['input_ids']
        remaining_length = max_len - len(prompt_tokens)  # 残りの長さを計算します

        # レスポンスAをトークナイズします
        response_a_tokens = tokenizer(row['response_a'], truncation=True, max_length=remaining_length//2)['input_ids']
        remaining_length -= len(response_a_tokens)  # 残りの長さを更新します

        # レスポンスBをトークナイズします
        response_b_tokens = tokenizer(row['response_b'], truncation=True, max_length=remaining_length//2)['input_ids']

        # トークンを追加します
        input_ids = [tokenizer.cls_token_id] + prompt_tokens + [tokenizer.sep_token_id] + response_a_tokens + [tokenizer.sep_token_id] + response_b_tokens
        token_type_ids = [0] * (len(prompt_tokens) + 2) + [1] * (len(response_a_tokens) + 1) + [2] * len(response_b_tokens)
        attention_mask = [1] * len(input_ids)

        # パディングを追加します
        padding_length = max_length - len(input_ids)
        if padding_length > 0:
            input_ids = input_ids + [tokenizer.pad_token_id] * padding_length  # パディングトークンで埋めます
            token_type_ids = token_type_ids + [0] * padding_length
            attention_mask = attention_mask + [0] * padding_length

        input_ids = input_ids[:max_length]  # 最大長を超える場合は切り詰めます
        token_type_ids = token_type_ids[:max_length]
        attention_mask = attention_mask[:max_length]
        
        return input_ids, token_type_ids, attention_mask
    
    # 各行に対してprocess関数を適用し、新しいトークン関連の列を作成します
    df[['input_ids', 'token_type_ids', 'attention_mask']] = df.apply(lambda row: pd.Series(process(row)), axis=1)
#     tokenized = df.apply(lambda row: pd.Series(process(row)), axis=1)  # コメントアウトされたコード
#     df.loc[:, ['input_ids', 'token_type_ids', 'attention_mask']] = tokenized  # コメントアウトされたコード
#     return df  # コメントアウトされたコード
```

---The following area is a Code cell (cell numver is 16)---
```python
# ラベルをカテゴリカル形式に変換します
labels = tf.keras.utils.to_categorical(train['label'], num_classes=3)  # 3つのクラスでカテゴリカルラベルを生成します
```

---The following area is a Code cell (cell numver is 17)---
```python
# trainデータフレームに対して、トークナイゼーションを実施します
tokenize_df(train, tokenizer)  # tokenize_df関数を呼び出して、トークン関連の列を生成します
```

---The following area is a Code cell (cell numver is 18)---
```python
# トレーニングのためのデータを準備します
input_ids = train['input_ids']  # 'input_ids'列を取得します
attention_mask = train['attention_mask']  # 'attention_mask'列を取得します

# input_idsとattention_maskを最大長に合わせてパディングします
X_train = sq.pad_sequences(input_ids, maxlen=max_length)  # 入力IDにパディングを適用します
X_train_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)  # 注意マスクにパディングを適用します

y_train = labels  # ラベルをy_trainに格納します
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## モデル
```

---The following area is a Code cell (cell numver is 20)---
```python
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D, Masking  # 必要なKerasレイヤーをインポートします

# CNNモデルを定義します
def create_cnn_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します
        Conv1D(filters=256, kernel_size=5, activation='relu'),  # 畳み込み層を追加します
        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します
        Dense(128, activation='relu'),  # 全結合層を追加します
        Dropout(0.5),  # ドロップアウト層を追加して過学習を防ぎます
        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします
    return model

# LSTMモデルを定義します
def create_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します
        LSTM(256, return_sequences=True),  # LSTM層を追加します
        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します
        Dense(128, activation='relu'),  # 全結合層を追加します
        Dropout(0.5),  # ドロップアウト層を追加します
        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします
    return model

# CNN LSTMモデルを定義します
def create_cnn_lstm_model(vocab_size, embedding_dim, max_length):
    model = Sequential([
        Input(shape=(max_length,), dtype=tf.int32, name='input_ids'),  # 入力の形状を指定します
        Embedding(input_dim=vocab_size, output_dim=embedding_dim),  # 埋め込み層を追加します
        Conv1D(filters=128, kernel_size=5, activation='relu'),  # 畳み込み層を追加します
        LSTM(128, return_sequences=True),  # LSTM層を追加します
        GlobalMaxPooling1D(),  # グローバルマックスプーリング層を追加します
        Dense(64, activation='relu'),  # 全結合層を追加します
        Dropout(0.5),  # ドロップアウト層を追加して過学習を防ぎます
        Dense(3, activation='softmax')  # 最終出力層、3クラスのソフトマックス出力を追加します
    ])
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # モデルをコンパイルします
    return model
```

---The following area is a Code cell (cell numver is 21)---
```python
# パラメータを設定します
vocab_size = tokenizer.vocab_size  # トークナイザーから語彙サイズを取得します
# vocab_size = max_length  # コメントアウトされたコード
embedding_dim = 100  # 埋め込み次元を設定します
max_length = max_length  # 最大長を設定します
max_features = tokenizer.vocab_size  # 最大特徴量を語彙サイズで設定します
# max_features = max_length * 2  # コメントアウトされたコード
max_len = max_length  # 最大長をmax_lenに再設定します
maxlen = max_len  # maxlenにその値を設定します
batch_size = 16  # バッチサイズを設定します
embedding_dims = 100  # 埋め込み次元を設定します
nb_filter = 150  # フィルター数を設定します
filter_length = 3  # フィルターの長さを設定します
hidden_dims = 100  # 隠れ層の次元を設定します
nb_epoch = 100  # エポック数を設定します

# モデルを作成します
# model = create_lstm_model(vocab_size, embedding_dim, max_length)  # LSTMモデルを作成するためのコメントアウトされたコード
model = create_cnn_lstm_model(vocab_size, embedding_dim, max_length)  # CNN LSTMモデルを作成します
model.summary()  # モデルの要約を表示します
```

---The following area is a Code cell (cell numver is 22)---
```python
from __future__ import print_function  # 将来のバージョンとの互換性のためのインポート
import numpy as np  # 数値計算のためのnumpyライブラリをインポートします

from keras.preprocessing import sequence  # シーケンス処理のためのKerasのpreprocessingモジュールをインポートします
from keras.models import Sequential  # Kerasの順次モデルをインポートします
from keras.layers import Dense, Dropout, Activation, Lambda  # 様々なKerasレイヤーをインポートします
from keras.layers import Embedding  # 埋め込み層をインポートします
from keras.layers import Convolution1D, LSTM  # 1次元畳み込み層とLSTMをインポートします
from keras.datasets import imdb  # IMDbデータセットをインポートします
from keras import backend as K  # Kerasバックエンドをインポートします
from keras.optimizers import Adadelta, Adamax  # 最適化アルゴリズムをインポートします
from keras.preprocessing import sequence as sq  # シーケンス処理のためのKerasのpreprocessingモジュールを再インポートします

from keras.layers import Dense, Dropout, Activation, Lambda, Input, TimeDistributed, Flatten  # 複数のKerasレイヤーをインポートします
from keras.models import Model  # Kerasのモデルクラスをインポートします
from keras.callbacks import ModelCheckpoint  # モデルチェックポイント用のコールバックをインポートします
```

---The following area is a Code cell (cell numver is 23)---
```python
# 使用されていないインポートのコメントアウトされたコード
# from tensorflow.keras.layers import Layer  # Kerasレイヤーをインポートします
# from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D  # 複数のKerasレイヤーをインポートします
# from keras.layers import GlobalMaxPooling1D  # グローバルマックスプーリング層をインポートします
# import tensorflow as tf  # TensorFlowライブラリをインポートします

# ApplyAttentionMaskクラスを定義します
# class ApplyAttentionMask(Layer):
#     def call(self, inputs):
#         embeddings, attention_mask = inputs  # 入力の分離
#         return embeddings * tf.expand_dims(attention_mask, -1)  # アテンションマスクを適用します

# 入力層を定義します
# input_layer = Input(shape=(max_length,), dtype='int32', name='main_input')  # 主入力層を定義します
# attention_masks = Input(shape=(max_length,), dtype='float32', name="attention_masks")  # アテンションマスク用の入力層を定義します

# 埋め込み層を定義します
# emb_layer = Embedding(max_features,
#                       embedding_dims,
#                       input_length=max_len
#                       )(input_layer)  # 埋め込み層を定義します

# アテンションマスクを適用した埋め込みを定義します
# masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])

# LSTMブランチを定義します
# lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)  # 最初のLSTM層を追加します
# lstm_out = LSTM(64, return_sequences=True)(lstm_out)  # 2つ目のLSTM層を追加します
# lstm_out = LSTM(32)(lstm_out)  # 3つ目のLSTM層を追加します
# lstm_out = BatchNormalization()(lstm_out)  # バッチ正規化を適用します
# lstm_out = Dropout(0.5)(lstm_out)  # ドロップアウトを適用します
# lstm_out = GlobalMaxPooling1D()(lstm_out)  # グローバルマックスプーリングを適用します

# CNN層ブランチを定義します
# cnn_out = Conv1D(128, 5, activation='relu')(masked_embeddings)  # 畳み込み層を追加します
# cnn_out = Conv1D(64, 5, activation='relu')(cnn_out)  # 2つ目の畳み込み層を追加します
# cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)  # 3つ目の畳み込み層を追加します
# cnn_out = BatchNormalization()(cnn_out)  # バッチ正規化を適用します
# cnn_out = Dropout(0.5)(cnn_out)  # ドロップアウトを適用します
# cnn_out = GlobalMaxPooling1D()(cnn_out)  # グローバルマックスプーリングを適用します

# LSTMとCNNの出力を連結します
# merged = concatenate([lstm_out, cnn_out])  # 出力を連結します
# merged = Dense(32, activation='sigmoid')(merged)  # 全結合層を追加します
# merged = BatchNormalization()(merged)  # バッチ正規化を適用します
# merged = Dropout(0.5)(merged)  # ドロップアウトを適用します
# pred = Dense(3, activation='softmax')(merged)  # 最終出力層を追加します

# モデルを構築します
# model = Model(inputs=[input_layer, attention_masks], outputs=[pred])  # モデルを構築します
# adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)  # Adadeltaオプティマイザの設定です
# adamax = Adamax(learning_rate=0.001)  # Adamaxオプティマイザの設定です
# model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])  # モデルをコンパイルします
# model.summary()  # モデルの要約を表示します
```

---The following area is a Code cell (cell numver is 24)---
```python
# import tensorflow as tf  # TensorFlowライブラリをインポートします
# from tensorflow.keras.layers import Input, Conv1D, LSTM, GRU, Dense, Masking  # Kerasレイヤーをインポートします
# from tensorflow.keras.models import Model  # Kerasのモデルクラスをインポートします
# from transformers import DebertaTokenizer, AutoModel  # Hugging Faceのトランスフォーマーモデルをインポートします

# ハイブリッドCNN-LSTMモデルを作成する関数を定義します
# def create_cnn_lstm_hybrid_model(base_model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes):
#     # 事前学習済みBERTモデルをロードします
#     model = AutoModel.from_pretrained(base_model_name)  # 指定されたモデル名からモデルをロードします
    
#     # 入力を定義します
#     input_ids = Input(shape=(max_length,), dtype=tf.int32, name='input_ids')  # input_ids用の入力層
#     attention_mask = Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')  # attention_mask用の入力層

#     # BERTの出力を取得します
#     outputs = model(input_ids, attention_mask=attention_mask)  # モデルによって出力を取得します
#     seq_output = outputs.last_hidden_state  # seq_outputの形状: (バッチサイズ, シーケンス長, 隠れサイズ)

#     # CNNを適用します
#     cnn_output = Conv1D(filters=cnn_output_channels, kernel_size=cnn_kernel_size, padding='same', activation='relu')(seq_output)  # 畳み込み層を適用します

#     # パディングされたシーケンスを処理するためにマスキングを適用します
#     masked_cnn_output = Masking()(cnn_output)  # マスキングを適用します
    
#     # LSTMを適用します
#     rnn_output = LSTM(hidden_dim)(masked_cnn_output)  # LSTM層を適用します

#     # クラス分類用の層を定義します
#     logits = Dense(num_classes, activation='softmax')(rnn_output)  # クラス数に応じた出力層を設定します

#     # モデルを作成します
#     model = Model(inputs=[input_ids, attention_mask], outputs=logits)  # モデルを構築します

#     return model  # モデルを返します
```

---The following area is a Code cell (cell numver is 25)---
```python
# ハイパーパラメータを定義します
# bert_model_name = '/kaggle/input/deberta_v3/keras/deberta_v3_large_en/2'  # 使用するBERTモデルのパス
# bert_model_name = '/kaggle/input/deberta-v3/pytorch/large/1'  # 別のBERTモデルのパス
# bert_model_name = 'deberta_v3_large_en'  # 使用するBERTモデル名
# model_name = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'  # 使用するQwenモデルのパス
# cnn_output_channels = 128  # CNNの出力チャンネル数
# cnn_kernel_size = 5  # CNNのカーネルサイズ
# hidden_dim = 256  # 隠れ層の次元
# num_classes = 3  # クラス数

# # モデルを初期化します
# model = create_cnn_lstm_hybrid_model(model_name, cnn_output_channels, cnn_kernel_size, hidden_dim, num_classes)  # ハイブリッドモデルを作成します
# model.summary()  # モデルの要約を表示します
```

---The following area is a Code cell (cell numver is 26)---
```python
from keras.callbacks import EarlyStopping  # 早期終了用のコールバックをインポートします

# モデルをトレーニングします
early_stopping = EarlyStopping(monitor='val_loss', patience=8, verbose=1)  # バリデーション損失を監視し、8エポックで改善がない場合にトレーニングを停止します

# モデルのフィッティングを行います
history = model.fit([X_train, X_train_attention_mask], y_train, epochs=20, batch_size=32, validation_split=0.2,  # バリデーションデータを分割して使用します
                    callbacks=[early_stopping])  # 早期終了のコールバックを適用します
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
## テスト
```

---The following area is a Code cell (cell numver is 28)---
```python
# テストデータをエンコードします
combine_text(test)  # テストデータに対してテキストを結合します
tokenize_df(test, tokenizer)  # テストデータに対してトークナイゼーションを実施します

input_ids = test['input_ids']  # 'input_ids'列を取得します
attention_mask = test['attention_mask']  # 'attention_mask'列を取得します

# input_idsとattention_maskを最大長に合わせてパディングします
X_test = sq.pad_sequences(input_ids, maxlen=max_length)  # テストデータの入力IDにパディングを適用します
X_test_attention_mask = sq.pad_sequences(attention_mask, maxlen=max_length)  # テストデータの注意マスクにパディングを適用します
```

---The following area is a Code cell (cell numver is 29)---
```python
# テストデータに対して予測を行います
predictions = model.predict([X_test, X_test_attention_mask])  # モデルを使用して予測を行います
predictions  # 予測結果を表示します
```

---The following area is a Code cell (cell numver is 30)---
```python
# 予測結果をデータフレームに変換します
winner = pd.DataFrame(predictions, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # 予測結果をデータフレームに格納します
result = pd.concat([test['id'], winner], axis=1)  # テストデータのIDと予測結果を結合します

# 結果をCSVファイルに保存します
result.to_csv('submission.csv', index=False)  # 提出用のCSVファイルを作成します
result  # 結果を表示します
```

** @@@ Jupyter Notebook numver 43, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、「LMSYS - Chatbot Arena」コンペティションのためにユーザーの好みに基づいた予測を行うモデルの構築に取り組んでいます。具体的には、2つの異なる応答からどちらがユーザーに好まれるかを予測し、その結果を確率形式で出力することを目指しています。

### 問題の概要
Notebookは、与えられたチャットボットの応答に対して、どちらが優れているかを判断するための機械学習モデルを開発することに焦点を当てています。ユーザーが与えたプロンプトに対して生成された2つの応答を基に、いずれかの応答が優れているか、または同等であるかを予測します。

### 使用されている手法・ライブラリ
- **Transformersライブラリ**: Hugging FaceのTransformersを利用して大規模言語モデル（特にLlamaモデル）を扱います。
- **Datasetライブラリ**: PyTorchのデータセット、特に`datasets`ライブラリを使用してデータの読み込みと前処理を行います。
- **PEFT (Parameter-Efficient Fine-Tuning)**: モデルのファインチューニングには、データ効率を高めるためにLoRA（Low-Rank Adaptation）を利用しています。
- **Scikit-learn**: おもに評価メトリクス（精度と対数損失の計算）を行うために使用されています。
- **PandasとNumPy**: データ操作に利用され、特にデータフレームを使用してデータの読み込みや変換を行っています。

### 主要なプロセス
1. **データのセットアップ**: Kaggleのデータセットを読み込んで20行分のサンプルデータを取得し、ラベルをエンコードします。
2. **トークナイゼーションと前処理**: 入力データのトークン化を行い、モデルに適した形式に整形します。
3. **モデルアーキテクチャの定義**: カスタムLlamaモデルを定義し、適切なロジックを実装します。損失計算やトークンシフトなどの処理も含まれています。
4. **トレーニングの設定**: トレーニング引数を設定し、評価戦略や最適化手法を明確にします。
5. **モデルのトレーニング**: Trainerを使用してモデルをトレーニングし、トレーニング済みモデルを保存します。
6. **推論プロセスの実装**: 新しいデータに対してモデルを評価し、応答の優劣に基づく確率を算出します。

### 結果の出力
最終的に、Notebookは選択された応答に対するモデルAとモデルBの勝利確率、及び引き分け確率を返す結果データフレームを生成します。この確率は、提出用のCSVファイルのフォーマットに適合しています。

このノートブックは、ユーザーの好みを効果的に予測するためのモデル構築とその実行フローを示しており、実践的な機械学習のスキルを活用しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# セットアップ
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## モデルの設定
```

---The following area is a Code cell (cell numver is 5)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## サンプルデータの読み込み
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv(TRAIN_CSV)
train = train.head(20)  # データセットの最初の20行を取得
train['label'] = train[target_columns].idxmax(axis=1)  # 各行の最大値のインデックスを 'label' 列に追加
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])  # 'label' 列をエンコード
train = train[columns_to_vectorize + ['label']]  # 必要な列のみを残す
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## トークナイザーとデータセットの前処理
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)  # モデルからトークナイザーを読み込む
tokenizer.add_eos_token = True  # 終端トークンを追加
tokenizer.padding_side = 'right'  # パディングを右側に設定

# 各ラベルに対するトークンIDを取得
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

# 各サンプルをトークナイズする関数
def tokenize(example, tokenizer):
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    
    # 最大長を超える場合、プロンプトと応答を切り詰める
    if len(prompt + response_a + response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    label_token_id = LABEL_IDS[int(example['label'])]  # ラベルのトークンIDを取得
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids) * [1]  # アテンションマスクの作成
    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Code cell (cell numver is 10)---
```python
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)  # Pandasデータフレームをデータセットに変換
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,  # 元の列を削除
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

n_splits = 5  # 交差検証の分割数
fold_idx = 0  # 使用するフォールドのインデックス
ds = load_data(train, tokenizer)  # トークン化されたデータをロード
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],  # トレーニングインデックス
        [i for i in range(len(ds)) if i % n_splits == fold_idx]  # 評価インデックス
    ) 
    for fold_idx in range(n_splits)
]
train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを取得
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## メトリクス
```

---The following area is a Code cell (cell numver is 12)---
```python
def compute_metrics(pred):
    logits, labels = pred  # 予測結果とラベルを取得
    preds = logits.argmax(axis=-1)  # 最も高いロジット値を持つインデックスを取得
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスのマッピングを作成
    labels = labels[np.isin(labels, label_tokens_ids)]  # 有効なラベルのみをフィルタリング
    labels = np.array([index_mapping[label.item()] for label in labels])  # マッピングされたラベルを取得
    acc = accuracy_score(labels, preds)  # 精度を計算
    probs = softmax(logits, axis=-1)  # ソフトマックスを計算して確率を取得
    log_loss_ = log_loss(labels, probs)  # ログロスを計算
    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログロスを辞書で返す
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## モデル
```

---The following area is a Code cell (cell numver is 14)---
```python
class CustomLlama3(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]  # 重みの関連付けを作成
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)  # Llamaモデルの構成
        self.vocab_size = config.vocab_size  # 単語のボキャブラリサイズ
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 言語モデルのヘッド
        self.post_init()  # 初期化後の処理

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        cache_position=None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]  # 隠れ層の状態を取得
        if self.config.pretraining_tp > 1:  # モデル並列化を使用する場合
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)  # ロジットを結合
        else:
            logits = self.lm_head(hidden_states)  # ロジットを算出

        logits = logits.float()  # ロジットを浮動小数点数に変換

        loss = None
        if labels is not None:
            # 次のトークンを予測するためにシフトする
            shift_logits = logits[..., :-1, :].contiguous()  # シフトしたロジット
            shift_labels = labels[..., 1:].contiguous()  # シフトしたラベル
            # トークンをフラット化する
            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失を定義
            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # フラット化
            shift_labels = shift_labels.view(-1)  # フラット化
            # モデル並列化を有効にする
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルのトークンIDをテンソルに変換
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 有効なラベルのみを取得
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 有効なロジットを取得
            loss = loss_fct(true_logits, true_labels)  # 損失を計算

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 15)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 16)---
```python
model = CustomLlama3.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False  # キャッシュを使用しない
model = prepare_model_for_kbit_training(model)  # k-bitトレーニングの準備
model = get_peft_model(model, peft_config)  # PEFTモデルの取得
print(model)  # モデルの情報を表示
model.print_trainable_parameters()  # 学習可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
### トレーニング引数
```

---The following area is a Code cell (cell numver is 18)---
```python
args = TrainingArguments(
    output_dir='output',  # 出力ディレクトリ
    overwrite_output_dir=True,  # 上書きを許可
    evaluation_strategy="epoch",  # エポックごとに評価
    save_strategy="steps",  # ステップごとに保存
    save_steps=5,  # 5ステップごとに保存
    save_total_limit=1,  # 保存される最大数
    logging_strategy="steps",  # ステップごとのロギング
    logging_steps=10,  # 10ステップごとにログを記録
    warmup_steps=20,  # ウォームアップステップ数
    optim="adamw_8bit",  # 最適化手法
    learning_rate=2e-4,  # 学習率
    per_device_train_batch_size=2,  # トレーニングのバッチサイズ（デバイスあたり）
    per_device_eval_batch_size=2,  # 評価のバッチサイズ（デバイスあたり）
    gradient_accumulation_steps=2,  # 勾配の累積ステップ数
    num_train_epochs=1,  # トレーニングエポック数
    fp16=True,  # 混合精度トレーニングを有効にする
    metric_for_best_model="log_loss",  # 最良モデルのメトリクス
    greater_is_better=False,  # メトリクスの良好さ
    report_to="none",
)
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# トレーニング（トレーニングまたは推論のいずれかを実行）
```

---The following area is a Code cell (cell numver is 20)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),  # トレーニングデータセットを選択
    eval_dataset=ds.select(eval_idx),  # 評価データセットを選択
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータの設定
    compute_metrics=compute_metrics,  # メトリクスの計算
)
trainer.train()  # トレーニングを実行
```

---The following area is a Code cell (cell numver is 21)---
```python
model.save_pretrained('pretrained_model')  # モデルを保存
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# 推論（トレーニングまたは推論のいずれかを実行）
```

---The following area is a Code cell (cell numver is 23)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 24)---
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import LoraConfig, get_peft_model, TaskType

# 事前にトレーニングされたモデルとLoraアダプタのファイルパス
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
lora_adapter_path = "/kaggle/working/pretrained_model"

# 原モデルを読み込む
model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# 対応するトークナイザーを読み込む
tokenizer = AutoTokenizer.from_pretrained(model_path)

# Loraの設定
lora_config = peft_config

# 必要に応じてk-bitトレーニングの準備
model_1 = prepare_model_for_kbit_training(model_1)

# モデルにLoraアダプタを適用
model_1 = get_peft_model(model_1, lora_config)

# 保存されたLoraアダプタのパラメータを読み込む
model_1.load_adapter(lora_adapter_path, adapter_name="test")

# 完全なモデルが使用できる状態になりました
model_1.eval()  # 評価モードに設定
```

---The following area is a Code cell (cell numver is 25)---
```python
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
import pandas as pd
import numpy as np

def softmax(row):
    e_row = np.exp(row - np.max(row))  # ソフトマックス計算のための安定化
    return e_row / e_row.sum()  # ソフトマックスを計算して正規化
```

---The following area is a Code cell (cell numver is 26)---
```python
data = ds.to_pandas()[0:10]  # データセットをPandasデータフレームに変換し最初の10行を取得
data["max_len"] = data["input_ids"].apply(len)  # 各入力の長さを計算
display(data[:3])  # 最初の3行を表示
print()

print(tokenizer.decode(data["input_ids"][0]))  # 0番目の入力のデコード結果を表示
```

---The following area is a Code cell (cell numver is 27)---
```python
@torch.no_grad()  # 勾配計算を無効にする
@torch.cuda.amp.autocast()  # 自動混合精度を有効にする
def inference(df, model, device, batch_size=2, max_length=1024):
    a_win, b_win, tie = [], [], []  # 各モデルの勝ち確率を格納するリスト

    model.eval()  # モデルを評価モードに設定
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを決定
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチデータを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換
        labels = tmp["labels"].to_list()  # ラベルをリストに変換
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最長の長さに合わせてパディング
            pad_to_multiple_of=None,
            return_tensors="pt",  # Pytorchテンソルとして返す
        )
        input_ids = inputs["input_ids"].to(device)  # デバイスに移動
        attention_mask = inputs["attention_mask"].to(device)  # デバイスに移動
        pad_labels = []  # パディングされたラベルを格納するリスト
        for label in labels:
            label = list(label) + [tokenizer.pad_token_id] * (input_ids[0].shape[0] - label.shape[0])  # ラベルをパディング
            pad_labels.append(label)
        labels = torch.tensor(pad_labels).to(device)  # デバイスに移動
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)  # モデルの出力を取得
        proba = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # ロジットにソフトマックスを適用して確率に変換
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝ち確率を追加
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝ち確率を追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加
    df['winner_model_a'] = a_win  # データフレームにモデルAの勝ち確率を追加
    df['winner_model_b'] = b_win  # データフレームにモデルBの勝ち確率を追加
    df['winner_tie'] = tie  # データフレームに引き分けの確率を追加
    return df  # 結果のデータフレームを返す
```

---The following area is a Code cell (cell numver is 28)---
```python
result_df = inference(data[0:4], model_1, device, batch_size=2, max_length=1024)  # 推論を実行

proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 確率の値を取得される

result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝ち確率をデータフレームに設定
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝ち確率をデータフレームに設定
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの確率をデータフレームに設定

# 確率のリストをフラットにする
result_df['winner_model_a'] = result_df['winner_model_a'].apply(lambda x: x[0])  
result_df['winner_model_b'] = result_df['winner_model_b'].apply(lambda x: x[0])  
result_df['winner_tie'] = result_df['winner_tie'].apply(lambda x: x[0])  

result_df  # 最終結果のデータフレームを表示
```

** @@@ Jupyter Notebook numver 44, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleのコンペティション「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、チャットボットの応答の好ましさを予測する問題に取り組んでいます。この課題は、大規模な言語モデル（LLM）からの応答の中で、どちらが人間に好まれるかを選ぶためのモデルを構築することです。

Notebookでは、以下のような手法やライブラリが使用されています：

1. **ライブラリのインポート**:
   - `pandas`: データ操作のために使用。
   - `torch`: PyTorchライブラリは深層学習の実行に必要。
   - `transformers`: 自然言語処理モデルのトークナイザーとモデルを利用するために使用。
   - `scikit-learn`: 機械学習アルゴリズムや評価指標のために利用。

2. **データの読み込みと前処理**:
   - 訓練データセット（`train.csv`）を読み込み、その構造を確認。
   - 自然言語処理モデルの一つであるBERT（`bert-base-uncased`）を使用してテキストをトークン化し、さらに埋め込みを生成。この埋め込みは、後の機械学習モデルの特徴量として使用されます。

3. **特徴量とターゲットの形成**:
   - トークン化されたテキストから埋め込みを取得し、それらを結合して特徴量データフレームを形成。
   - 勝者モデルを識別するためのターゲット変数 `y` を生成。

4. **モデルの訓練と評価**:
   - ランダムフォレスト分類器を使用してモデルを訓練し、検証データに対する精度を評価。

5. **テストデータの処理と予測**:
   - テストデータセット（`test.csv`）を読み込み、同様のトークン化と埋め込み処理を実施。
   - 訓練したモデルを用いてテストデータに対する予測を行い、予測確率を取得。

6. **提出ファイルの生成**:
   - 予測結果をCSVファイル形式で保存し、コンペティションに提出する準備を整えています。

全体として、NotebookはBERTを用いた自然言語処理とランダムフォレストを組み合わせ、チャットボットの応答を基に人間の好みを予測する問題に取り組んでいます。
```

---The following area is a Code cell (cell numver is 1)---
```python
# pandasをpdとしてインポートします
import pandas as pd
# PyTorchライブラリをインポートします
import torch
# TransformersライブラリからAutoTokenizerとAutoModelをインポートします
from transformers import AutoTokenizer, AutoModel
# scikit-learnからtrain_test_splitをインポートします
from sklearn.model_selection import train_test_split
# scikit-learnからRandomForestClassifierをインポートします
from sklearn.ensemble import RandomForestClassifier
# scikit-learnからaccuracy_scoreをインポートします
from sklearn.metrics import accuracy_score

# これは、データの前処理やモデルの訓練に必要なライブラリをインポートしている部分です。
# pandasはデータフレームの操作に、torchは深層学習に必要なライブラリです。
# transformersはNLP（自然言語処理）モデルの使用に使います。
# scikit-learnは機械学習の様々なアルゴリズムや評価指標を提供します。
```

---The following area is a Code cell (cell numver is 2)---
```python
# 入力ディレクトリのパスを定義します
INPUT_DIR = "/kaggle/input/lmsys-chatbot-arena/"
# train.csvファイルを読み込み、train_dfというデータフレームに格納します
train_df = pd.read_csv(f"{INPUT_DIR}/train.csv")

# ここでは、訓練データセット（train.csv）を指定したディレクトリから読み込み、
# pandasのデータフレーム形式で変数train_dfに保存しています。
# このデータフレームには、モデルの訓練に使用するデータが含まれています。
```

---The following area is a Code cell (cell numver is 3)---
```python
# train_dfデータフレームの内容を表示します
train_df

# このセルでは、先ほど読み込んだ訓練データセット（train_df）の中身を確認しています。
# データフレームがどのようなデータを含んでいるか、行数や列数、各列のデータ型などを
# 確認することができます。これはデータ解析や前処理を行う前に必要なステップです。
```

---The following area is a Code cell (cell numver is 4)---
```python
# train_dfデータフレームの構造と情報を表示します
train_df.info()

# このセルでは、train_dfのデータフレームの詳細情報を表示しています。
# info()メソッドは、データフレームの各列のデータ型、欠損値の数、データ数などを
# 確認するために使用されます。これにより、データの準備や前処理に必要な問題を特定する助けとなります。
```

---The following area is a Code cell (cell numver is 5)---
```python
# 使用するモデルのIDを定義します（ここではBERTを使用）
MODEL_ID = "bert-base-uncased"
# 指定したモデルIDからトークナイザーを読み込んでtokenizer変数に保存します
tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
# 指定したモデルIDから事前学習済みのモデルを読み込んでmodel変数に保存します
model = AutoModel.from_pretrained(MODEL_ID)

# このセルでは、自然言語処理のためのBERTモデルを設定しています。
# MODEL_IDで指定された事前学習済みのBERTモデルを使用し、トークナイザー（tokenizer）と
# モデル（model）をそれぞれ初期化しています。これにより、テキストをトークン化し、モデルを通じて
# 処理する準備が整います。
```

---The following area is a Code cell (cell numver is 6)---
```python
# 使用可能なデバイス（GPUまたはCPU）を判別します
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# 判別したデバイスの情報を表示します
print(device)

# このセルでは、PyTorchを使用して、計算に使用するデバイスを設定しています。
# GPUが利用可能な場合は'cuda'（GPU）を、そうでない場合は'cpu'（CPU）を使用します。
# 最後に、選択されたデバイスを表示することで、後続の計算がどのデバイスで行われるかを確認します。
```

---The following area is a Code cell (cell numver is 7)---
```python
# モデルを選択したデバイス（GPUまたはCPU）に移動します
model.to(device)

# このセルでは、事前に設定したデバイス（device）にモデルを移動させています。
# これにより、計算リソースを有効活用し、訓練や推論のプロセスを高速化することができます。
# GPUを使用することで、特に大規模なモデルやデータセットに対してパフォーマンスが向上します。
```

---The following area is a Code cell (cell numver is 8)---
```python
# トークナイザーにパディングトークンが設定されていない場合
if tokenizer.pad_token is None:
    # 特殊トークンとしてパディングトークンを追加します
    tokenizer.add_special_tokens({'pad_token': '[PAD]'})
    # パディングトークンを設定します
    tokenizer.pad_token = '[PAD]'

# このセルでは、トークナイザーにパディングトークンが存在しない場合に、新たにパディングトークンを追加しています。
# パディングトークンは、バッチ内のシーケンスの長さを揃えるために使用されます。
# これにより、異なる長さの入力データを同じ次元に揃えることができ、モデルにとって処理しやすくなります。
```

---The following area is a Code cell (cell numver is 9)---
```python
# テキストをトークン化する関数を定義します
def tokenize_text(text):
    # トークン化を行い、パディングと切り捨てを適用し、テンソル形式で返します
    return tokenizer(text, padding=True, truncation=True, return_tensors="pt").to(device)

# このセルでは、与えられたテキストをトークン化するための関数tokenize_textを定義しています。
# この関数は、テキストをトークナイザーに渡し、パディング（長さを調整）と切り捨て（長さを制限）を行い、
# PyTorchのテンソル形式に変換して選択されたデバイスに移動させます。
# これにより、後続のモデルへの入力が適切な形式で提供されることになります。
```

---The following area is a Code cell (cell numver is 10)---
```python
# 'prompt'列をトークン化して新しい列'prompt_tokens'を作成します
train_df['prompt_tokens'] = train_df['prompt'].apply(tokenize_text)
# 'response_a'列をトークン化して新しい列'response_a_tokens'を作成します
train_df['response_a_tokens'] = train_df['response_a'].apply(tokenize_text)
# 'response_b'列をトークン化して新しい列'response_b_tokens'を作成します
train_df['response_b_tokens'] = train_df['response_b'].apply(tokenize_text)

# このセルでは、訓練データフレーム（train_df）の各列に対し、
# それぞれのテキストデータをトークン化し、新しい列を作成しています。
# 'prompt'、'response_a'、'response_b'の各列からトークン化された結果を
# 'prompt_tokens'、'response_a_tokens'、'response_b_tokens'という新しい列に格納します。
# これにより、モデルへの入力形式が整えられます。
```

---The following area is a Code cell (cell numver is 11)---
```python
# train_dfデータフレームの内容を表示します
train_df

# このセルでは、トークン化された結果を含む訓練データフレーム（train_df）の中身を確認しています。
# 新たに追加された'tokens'列が含まれており、各テキストがどのようにトークン化されたかを
# 確認することができます。データフレームの変化を把握することで、次の処理を行う準備をします。
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークン化されたテキストから埋め込みを取得する関数を定義します
def get_embeddings(text_tokens):
    # 勾配計算を無効にしてモデルの出力を取得します
    with torch.no_grad():
        outputs = model(**text_tokens)
    # 最後の隠れ状態の平均を計算し、1次元削減してNumPy配列に変換して返します
    return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

# このセルでは、トークン化されたテキストからモデルを使用して埋め込み（特徴ベクトル）を取得するための
# 関数get_embeddingsを定義しています。
# モデルの出力は勾配計算を行わずに取得し、最終的な隠れ状態の平均を計算することで
# 各入力テキストに対する埋め込みを生成します。得られた埋め込みはNumPy配列として返され、
# 機械学習モデルへの入力として使用されることが想定されています。
```

---The following area is a Code cell (cell numver is 13)---
```python
# 'prompt_tokens'列のトークン化結果から埋め込みを取得し、新しい列'prompt_embeddings'を作成します
train_df['prompt_embeddings'] = train_df['prompt_tokens'].apply(lambda x: get_embeddings(x))
# 'response_a_tokens'列のトークン化結果から埋め込みを取得し、新しい列'response_a_embeddings'を作成します
train_df['response_a_embeddings'] = train_df['response_a_tokens'].apply(lambda x: get_embeddings(x))
# 'response_b_tokens'列のトークン化結果から埋め込みを取得し、新しい列'response_b_embeddings'を作成します
train_df['response_b_embeddings'] = train_df['response_b_tokens'].apply(lambda x: get_embeddings(x))

# このセルでは、トークン化されたテキストから埋め込みを取得し、それぞれの埋め込みを新しい列に格納しています。
# 'prompt'、'response_a'、'response_b'のトークン化結果に対してget_embeddings関数を適用し、
# それぞれの埋め込み結果（'prompt_embeddings'、'response_a_embeddings'、'response_b_embeddings'）をデータフレームに追加します。
# これにより、モデルの出力がデータフレーム内で利用可能となり、今後の機械学習処理や分析に活用される予定です。
```

---The following area is a Code cell (cell numver is 14)---
```python
# 各埋め込みをデータフレーム形式に変換し、Xという特徴量データフレームを作成します
X = pd.concat([pd.DataFrame(train_df['prompt_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_a_embeddings'].tolist()), 
               pd.DataFrame(train_df['response_b_embeddings'].tolist())], axis=1)
# ターゲット変数yを定義し、各行で最も大きな値を持つ列のインデックスを取得します
y = train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values.argmax(axis=1)

# このセルでは、'prompt_embeddings'、'response_a_embeddings'、'response_b_embeddings'の埋め込みを
# 一つの特徴量データフレームXに結合しています。
# さらに、3つの勝者モデルの情報から最も大きな値を持つインデックスを取得し、yというターゲット変数を作成します。
# yは、各行の勝者モデルを示すため、後のモデル訓練で使用されます。
```

---The following area is a Code cell (cell numver is 15)---
```python
# 特徴量データフレームXの内容を表示します
X

# このセルでは、訓練データの特徴量データフレーム（X）の中身を確認しています。
# Xには、prompt、response_a、response_bの埋め込みが結合されており、
# 今後の機械学習モデルで使用するための入力データが整えられています。
# モデルの訓練や予測を行う前に、データの変化や形状を確認するためのステップです。
```

---The following area is a Code cell (cell numver is 16)---
```python
# ターゲット変数yの内容を表示します
y

# このセルでは、ターゲット変数yの中身を確認しています。
# yは、特徴量データに対するモデルの勝者を示すインデックスを持ちます。
# 各エントリは、対応する特徴量データのどのモデルが選ばれたかを表しており、
# モデルの訓練や評価に使用される重要な情報です。
```

---The following area is a Code cell (cell numver is 17)---
```python
# データを訓練セットと検証セットに分割します
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# このセルでは、特徴量データXとターゲット変数yを訓練データと検証データに分割しています。
# train_test_split関数を使用しており、全データの20%を検証データとして確保します。
# random_stateを指定することで、再現性を持たせた分割が行われます。
# これによりモデルの訓練と評価を行う準備が整います。
```

---The following area is a Code cell (cell numver is 18)---
```python
# ランダムフォレスト分類器を初期化します（決定木の数として100を指定）
clf = RandomForestClassifier(n_estimators=100, random_state=42)
# 訓練データを使用してモデルを訓練します
clf.fit(X_train, y_train)

# このセルでは、ランダムフォレスト分類器を作成し、訓練データ（X_train）とターゲット変数（y_train）を使ってモデルを訓練しています。
# n_estimatorsで指定した数の決定木を持つランダムフォレストモデルが構築され、
# random_stateを利用することで訓練の再現性を確保しています。これにより、将来の予測や評価を行うためのモデルが準備されます。
```

---The following area is a Code cell (cell numver is 19)---
```python
# 検証データを使用して予測を行います
y_pred = clf.predict(X_val)
# 検証データに対する精度を計算します
accuracy = accuracy_score(y_val, y_pred)
# 精度を表示します
print(f'Validation Accuracy: {accuracy:.4f}')

# このセルでは、訓練したモデルを用いて検証データ（X_val）に対する予測を行い、
# その結果を基に精度を計算しています。
# accuracy_score関数を使用して、実際の値（y_val）と予測値（y_pred）を比較し
# 精度を求め、結果を小数点以下4桁で表示します。
# モデルの性能を評価する重要なステップです。
```

---The following area is a Code cell (cell numver is 20)---
```python
# テストデータセットを読み込み、test_dfというデータフレームに格納します
test_df = pd.read_csv(f"{INPUT_DIR}/test.csv")

# このセルでは、指定したディレクトリからテストデータ（test.csv）を読み込み、
# pandasのデータフレーム形式で変数test_dfに保存しています。
# このデータフレームは、モデルの性能を評価するためのテストに利用されます。
```

---The following area is a Code cell (cell numver is 21)---
```python
# テストデータを処理します
test_df['prompt_tokens'] = test_df['prompt'].apply(tokenize_text)
test_df['response_a_tokens'] = test_df['response_a'].apply(tokenize_text)
test_df['response_b_tokens'] = test_df['response_b'].apply(tokenize_text)

# 埋め込みを取得します
test_df['prompt_embeddings'] = test_df['prompt_tokens'].apply(get_embeddings)
test_df['response_a_embeddings'] = test_df['response_a_tokens'].apply(get_embeddings)
test_df['response_b_embeddings'] = test_df['response_b_tokens'].apply(get_embeddings)

# テスト特徴量を準備します
X_test = pd.concat([pd.DataFrame(test_df['prompt_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_a_embeddings'].tolist()), 
                    pd.DataFrame(test_df['response_b_embeddings'].tolist())], axis=1)

# このセルでは、テストデータフレーム（test_df）の各列をトークン化し、
# それからトークン化されたデータを基に埋め込みを取得する過程を経ています。
# 'prompt'、'response_a'、'response_b'それぞれの埋め込みを作成し、
# 最後にこれらを結合してテスト特徴量データフレーム（X_test）を作成しています。
# モデルによる予測を行う準備が整います。
```

---The following area is a Code cell (cell numver is 22)---
```python
# テスト特徴量データフレームX_testの内容を表示します
X_test

# このセルでは、処理されたテスト特徴量データフレーム（X_test）の中身を確認しています。
# X_testには、'prompt'、'response_a'、'response_b'の埋め込みが結合されており、
# モデルによる予測に利用するための入力データが整えられています。
# データの変化や形状を確認することで、次のステップに進む準備をします。
```

---The following area is a Code cell (cell numver is 23)---
```python
# テスト特徴量に基づいて各クラスの予測確率を取得します
y_test_pred_prob = clf.predict_proba(X_test)

# 予測確率を持つデータフレームを作成し、カラム名を指定します
submission_df = pd.DataFrame(y_test_pred_prob, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
# テストデータフレームの'id'列を最初のカラムとして挿入します
submission_df.insert(0, 'id', test_df['id'])

# このセルでは、訓練したモデルを使ってテストデータ（X_test）に対する各クラスの予測確率を取得しています。
# 予測結果を新しいデータフレーム（submission_df）に格納し、カラム名を指定しています。
# さらに、元のテストデータに含まれる'id'を最初のカラムに追加して、提出用のフォーマットを整えています。
```

---The following area is a Code cell (cell numver is 24)---
```python
# 提出用データフレームsubmission_dfの内容を表示します
submission_df

# このセルでは、作成した提出用データフレーム（submission_df）の中身を確認しています。
# submission_dfには、テストデータの各エントリに対するモデルの予測確率（winner_model_a、winner_model_b、winner_tie）が含まれており、
# 提出ファイルとして適切な形式になっています。これを後で提出することで、コンペティションの結果を評価してもらうことができます。
```

---The following area is a Code cell (cell numver is 25)---
```python
# 作成した提出用データフレームをCSVファイルとして保存します
submission_df.to_csv('/kaggle/working/submission.csv', index=False)

# 提出ファイルが作成されたことを表示します
print("Submission file created.")

# このセルでは、生成した提出用データフレーム（submission_df）をCSV形式で保存し、
# 提出ファイル（submission.csv）を作成しています。index=Falseを設定することで、行番号をファイルに含めません。
# 最後のメッセージにより、提出ファイルが正常に作成されたことを確認できます。
```

** @@@ Jupyter Notebook numver 45, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける提出ファイルの作成を目的としています。具体的には、テストデータセットに基づいて、どのチャットボット（モデルA、モデルB、または引き分け）が選ばれるかを予測する問題に取り組んでいます。

以下は、Notebook内で使用されている主な手法とライブラリです：

1. **使用ライブラリ**:
   - **NumPy**: 線形代数を行うためのライブラリで、勝者をランダムに選ぶ際に使用されています。
   - **Pandas**: CSVファイルの入出力やデータ処理に利用されており、テストデータの読み込みやDataFrameの操作に使われています。

2. **主要な処理**:
   - テスト用CSVファイルを読み込み、予測用のDataFrameを構築します。
   - 各行について、モデルA、モデルB、または引き分けのいずれかをランダムに選択して、勝者の列を生成します。
   - 勝者列の合計が各行で1になることを確認し、データの整合性を保つためのアサーションも行っています。
   - 最後に、生成したDataFrameを'submission.csv'として保存し、形式どおりの提出ファイルを作成しています。

このNotebookは、生成されたランダムな予測を使って、Kaggleのコンペティションに参加するための初期的なステップを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import numpy as np  # 線形代数を使用するためのNumPyライブラリをインポートします
import pandas as pd  # データ処理のためのPandasライブラリをインポートします。CSVファイルの入出力も行います（例: pd.read_csv）
```

---The following area is a Code cell (cell numver is 2)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テスト用のCSVファイルを読み込む
```

---The following area is a Code cell (cell numver is 3)---
```python
# 'test'と同じ'id'列を持つ新しいDataFrameを作成します
sample_submission = pd.DataFrame({'id': test['id']})

# ランダムな勝者列を生成します
n_rows = len(test)  # テストデータの行数を取得します
winners = np.random.choice(['a', 'b', 'tie'], size=n_rows)  # 'a', 'b', 'tie'からランダムに勝者を選択します

# 勝者列を作成します
sample_submission['winner_model_a'] = (winners == 'a').astype(int)  # 勝者がモデルAの場合は1、そうでない場合は0
sample_submission['winner_model_b'] = (winners == 'b').astype(int)  # 勝者がモデルBの場合は1、そうでない場合は0
sample_submission['winner_tie'] = (winners == 'tie').astype(int)  # 勝者が引き分けの場合は1、そうでない場合は0

# 勝者列の合計が各行で1になっていることを確認します
assert (sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1) == 1).all()  # 各行の勝者列の合計が1であることを確認します

# 新しいDataFrameの最初の数行を表示します
print(sample_submission.head())  # DataFrameの最初の数行を出力します
```

---The following area is a Code cell (cell numver is 4)---
```python
sample_submission.to_csv('submission.csv', index=False)  # DataFrameを'submission.csv'という名前でCSVファイルに保存します。インデックスは含めません
```

** @@@ Jupyter Notebook numver 46, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、Hugging FaceのTransformersライブラリを使用して、テキスト分類モデルのロード、前処理、およびトレーニングに関する包括的なガイドを提供しています。具体的には、人間による好み予測に関連するデータセットを扱い、プロンプトに対する応答モデルのパフォーマンスを評価します。以下にノートブックの各セクションの概要を示します。

1. **データのロードとサンプリング**:
   - CSVファイルからデータを読み込み、全体のデータセットの10%をランダムにサンプリングします。これにより、計算リソースを効率的に使いながら処理を迅速化します。

2. **データの分割**:
   - サンプリングしたデータをトレーニングセットとバリデーションセットに分割し、モデル性能の評価を可能にします。

3. **トークナイザーの準備**:
   - 小型のRobertaモデルのトークナイザーを使用して、テキストデータをトークン化します。この処理には、パディング、切り捨て、およびPyTorchテンソル形式での戻り値が含まれています。

4. **モデル設定とトレーニング**:
   - モデルの設定を行い、トレーニング引数を定義した後、トレーニングを開始します。デバイスは、利用可能な場合はGPUを使用します。

5. **推論関数の定義**:
   - トレーニングされたモデルを使用して、トークン化されたデータに対して推論を行う関数を定義します。これにより、ノートブックは推論時のメモリ管理を最適化します。

6. **評価と結果の保存**:
   - テストデータをトークン化し、推論を行った結果をCSVファイルに保存します。最終的に、モデルのロジット結果とデータフレームを結合し、提出用のファイルを作成します。

このノートブックは、データサンプリング、トークン化、モデル設定、推論、および結果保存という体系的なアプローチを通じて、テキスト分類モデルのトレーニングの流れを示しています。特に、計算リソースの効率的な使用を保ちながらモデルのパフォーマンスを維持することに重点が置かれています。使用されている主要なライブラリは、Pandas、Scikit-learn、およびHugging Face Transformersです。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# はじめに
このノートブックは、Hugging Face Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する包括的なガイドを提供します。計算リソースを効率的に管理するために、ロバートトークナイザーと小型のロバートモデルを使用します。プロセスには、データのロード、サンプリング、トークナイゼーション、およびモデルの準備が含まれます。

# ステップ 1: データのロードとサンプリング
最初に、CSVファイルからデータセットをロードし、処理を迅速化するためにデータセットのサイズを減少させるためにランダムサンプルを取得します。
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from sklearn.model_selection import train_test_split

# データをロードする
df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")

# データの10%をサンプリングする
df_sample = df.sample(frac=0.1, random_state=42)

# サンプルデータを確認する
print(df_sample.head())  # サンプルデータの最初の5行を表示します。
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# ステップ 2: データをトレーニングセットとバリデーションセットに分割する
データをトレーニングセットとバリデーションセットに分割し、モデルのパフォーマンスを評価します。
```

---The following area is a Code cell (cell numver is 4)---
```python
# データをトレーニングセットとバリデーションセットに分割する
train_texts, val_texts, train_labels, val_labels = train_test_split(
    df_sample['prompt'].tolist(),  # プロンプト（入力テキスト）のリストを取得します
    df_sample['winner_model_a'],    # モデルAの勝者ラベルを取得します
    test_size=0.1,                  # データの10%をテストセットとします
    random_state=42                 # 再現性のために乱数シードを設定します
)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# ステップ 3: トークナイザーをロードして準備する
効率のために小型のロバートモデルのトークナイザーを使用し、テキストデータをトークン化します。
```

---The following area is a Code cell (cell numver is 6)---
```python
from transformers import RobertaTokenizer

# トークナイザーをロードする
tokenizer = RobertaTokenizer.from_pretrained("roberta-base")  # ロバートベースモデルのトークナイザーを取得します

# トークン化関数
def tokenize_function(texts):
    return tokenizer(
        texts,
        padding="max_length",  # トークンのパディングを最大長に設定します
        truncation=True,        # テキストが最大長を超えた場合に切り捨てます
        max_length=128,        # 最大トークン数を128に設定します
        return_tensors='pt'    # PyTorchテンソルとして戻します
    )

# テキストをトークン化する
train_encodings = tokenize_function(train_texts)  # トレーニングデータをトークン化します
val_encodings = tokenize_function(val_texts)      # バリデーションデータをトークン化します

# トークン化されたデータを確認する
print(train_encodings)  # トークン化されたトレーニングデータを表示します
print(val_encodings)    # トークン化されたバリデーションデータを表示します
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# ステップ 4: モデルの設定を準備する
モデルの設定を行い、適切なデバイス（利用可能な場合はGPU）を使用するようにします。
```

---The following area is a Code cell (cell numver is 8)---
```python
from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification
from torch.utils.data import Dataset
import torch
import numpy as np

# 設定クラス
class Config:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")  # GPUが使用可能であればGPUを、そうでなければCPUを使用します
        self.batch_size = 16  # バッチサイズを16に設定します

cfg = Config()

# モデルをロードする
model = AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=2).to(cfg.device)  # ロバートベースモデルを2つのラベルで分類するためにロードし、指定したデバイスに移動します

# カスタムデータセットクラス
class CustomDataset(Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings  # エンコーディングを格納します
        self.labels = np.array(labels, dtype=int)  # ラベルを整数のNumPy配列として格納します
    
    def __getitem__(self, idx):
        # 指定したインデックスのエンコーディングとラベルを取得します
        item = {key: torch.tensor(val[idx], dtype=torch.long) for key, val in self.encodings.items()}  # エンコーディングをテンソル化します
        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)  # ラベルをテンソル化します
        return item
    
    def __len__(self):
        return len(self.labels)  # データセットの長さを返します

# データセットを作成する
train_dataset = CustomDataset(train_encodings, train_labels)  # トレーニングデータセットを作成します
val_dataset = CustomDataset(val_encodings, val_labels)        # バリデーションデータセットを作成します

# トレーニング引数
training_args = TrainingArguments(
    output_dir='./results',                # 結果を保存するディレクトリ
    num_train_epochs=3,                    # トレーニングを行うエポック数
    per_device_train_batch_size=cfg.batch_size,  # デバイスごとのトレーニングバッチサイズ
    per_device_eval_batch_size=cfg.batch_size,   # デバイスごとの評価バッチサイズ
    warmup_steps=500,                       # ウォームアップステップ数
    weight_decay=0.01,                     # 重み減衰率
    logging_dir='./logs',                   # ログを保存するディレクトリ
    logging_steps=10,                       # ロギングのステップ頻度
)

# トレーナーを初期化する
trainer = Trainer(
    model=model,                            # モデル
    args=training_args,                     # トレーニング引数
    train_dataset=train_dataset,            # トレーニングデータセット
    eval_dataset=val_dataset                 # バリデーションデータセット
)

# モデルをトレーニングする
trainer.train()  # モデルのトレーニングを開始します
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# ステップ 5: 推論関数
トレーニングされたモデルを使用してトークン化されたデータに対して推論を行う関数を定義します。
```

---The following area is a Code cell (cell numver is 10)---
```python
def infer(model, input_ids, attention_mask, batch_size=cfg.batch_size):
    model.eval()  # モデルを評価モードに設定します
    results = []  # 結果を格納するリストを初期化します
    with torch.no_grad():  # 勾配計算を無効にしてメモリを節約します
        for i in range(0, len(input_ids), batch_size):  # バッチサイズごとにループします
            batch_input_ids = input_ids[i:i + batch_size].to(cfg.device)  # バッチの入力IDをデバイスに移動します
            batch_attention_mask = attention_mask[i:i + batch_size].to(cfg.device)  # バッチのアテンションマスクをデバイスに移動します
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask)  # モデルの出力を取得します
            results.extend(outputs.logits.cpu().numpy())  # 出力のロジットをCPUに移動し、NumPy配列に変換して結果に追加します
    return np.array(results)  # 結果をNumPy配列として返します
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# ステップ 6: 評価と結果の保存
テストデータをトークン化し、推論を行い、結果をCSVファイルに保存します。
```

---The following area is a Code cell (cell numver is 12)---
```python
# テストデータをトークン化する
test_texts = df_sample['prompt'].tolist()  # プロンプトのリストを取得します
test_encodings = tokenize_function(test_texts)  # テストデータをトークン化します

# 推論を行う
results = infer(model, test_encodings['input_ids'], test_encodings['attention_mask'])  # トークン化されたデータに対して推論を実行します

# 結果をDataFrameに変換し、CSVとして保存する
results_df = pd.DataFrame(results, columns=['logit_a', 'logit_b'])  # ロジットの結果をDataFrameに変換します
submission = pd.concat([df_sample[['id']], results_df], axis=1)  # IDと結果を結合します
submission.to_csv('submission.csv', index=False)  # CSVファイルとして保存します
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 結論
このノートブックでは、Transformersライブラリを使用してテキスト分類モデルのロード、前処理、トレーニングに関する体系的なアプローチを提供しました。ステップにはデータのサンプリング、トークン化、モデルの設定、推論が含まれています。最終的な結果はさらなる分析のために保存されました。この方法論は、モデルのパフォーマンスを維持しながら計算リソースの効率的な使用を確保します。
```

** @@@ Jupyter Notebook numver 47, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」における人間の好み予測タスクに取り組んでいます。主な目的は、ユーザーが異なるチャットボットからの応答の中でどちらを選ぶかを予測するためのモデルを開発することです。ノートブックではトレーニングデータのサイズが大きいため、そのわずか0.5%のみを使用して、データのクリーニングと特徴量エンジニアリングを行います。

### 使用される手法やライブラリ
1. **データ処理と分析**:
   - `pandas`, `numpy`: データフレームの操作や数値計算。
   - `textstat`: テキストの読みやすさや統計分析。
   - `SweetViz`: データ分析と視覚化。

2. **自然言語処理**:
   - `nltk`: ストップワードの取得やテキストの分析。
   - `transformers`: BERTモデルを用いたトークン化と埋め込み生成。

3. **機械学習**:
   - `scikit-learn`: データの分割、モデル評価、さまざまな分類器（ランダムフォレスト、勾配ブースティング、サポートベクターマシンなど）の実装。
   - `tensorflow`: ニューラルネットワークモデルの構築とトレーニング。

### 主な工程
- **データの読み込みと前処理**: トレーニングデータとテストデータをCSVから読み込み、クリーニングを実施（特殊文字の削除、正規化など）。
- **特徴量エンジニアリング**: テキストの単語数、文字数、感情分析、およびBERTを用いた埋め込み生成を含む多様な特徴量を抽出。
- **モデルの構築と評価**: 複数の機械学習モデルをトレーニングし、その性能を精度やログ損失を用いて評価。最良のモデルを選択します。
- **テストデータに対する予測**: 最良モデルを用いてテストデータに対する予測を実施し、提出用フォーマットでCSVファイルを生成。

このノートブックは、機械学習や自然言語処理を用いて実際の問題にアプローチし、チャットボットの応答のユーザー選好を予測するための包括的なフレームワークを提供しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS - Chatbot Arenaでの人間の好み予測



**トレーニングデータのサイズが大きいため、私はトレーニングデータのわずか0.5%のみを使用しています！**

作業中：TPUを使用して別のノートブックで埋め込みを計算し、フルトレーニングデータを使用した後、ここに埋め込みをロードします！

## ライブラリのインストールと読み込み
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install textstat SweetViz  # textstatとSweetVizをインストールします。

# textstatはテキストの統計情報を分析するためのライブラリで、例えば読みやすさのスコアを計算することができます。
# SweetVizはデータの可視化を提供するライブラリで、データセットの探索に役立ちます。
```

---The following area is a Code cell (cell numver is 3)---
```python
import warnings  # 警告を表示しないように設定します。
warnings.filterwarnings('ignore')  # すべての警告を無視します。

import pandas as pd  # データ操作のためのライブラリ
import numpy as np  # 数値計算のためのライブラリ

import seaborn as sns  # データの可視化に役立つライブラリ
import matplotlib.pyplot as plt  # グラフを描画するためのライブラリ
import plotly.express as px  # インタラクティブなグラフを描画するためのライブラリ
import sweetviz as sv  # データセットの探索に役立つ可視化ライブラリ

import re  # 正規表現を扱うためのライブラリ
import string  # 文字列操作のためのライブラリ
import nltk  # 自然言語処理のためのライブラリ
from nltk.corpus import stopwords  # ストップワード（意味を持たない単語）のリストを取得するためのモジュール

import tensorflow as tf  # 機械学習と深層学習のフレームワーク
from tensorflow.keras.preprocessing.text import Tokenizer  # テキストのトークン化を行うためのクラス
from tensorflow.keras.preprocessing.sequence import pad_sequences  # シーケンスを一定の長さにパディングするための関数

from nltk.sentiment.vader import SentimentIntensityAnalyzer  # 感情分析を行うためのモジュール

import textstat  # テキストの統計情報を計算するためのライブラリ
from sklearn.feature_extraction.text import CountVectorizer  # テキストデータを数値に変換するためのクラス

from sklearn.model_selection import train_test_split  # データセットをトレーニングとテストに分割するための関数

from sklearn.metrics.pairwise import cosine_similarity  # コサイン類似度を計算するための関数
from transformers import BertTokenizer, TFBertModel  # BERTトークン化とBERTモデルを使用するためのライブラリ

from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # ランダムフォレストと勾配ブースティングのクラス
from sklearn.svm import SVC  # サポートベクターマシンのクラス

from tensorflow.keras.models import Sequential  # Kerasのシーケンシャルモデル
from tensorflow.keras.layers import Dense, Dropout  # 全結合層とドロップアウト層
from tensorflow.keras.optimizers import Adam  # Adamオプティマイザ

from sklearn.metrics import accuracy_score, \  # 精度を評価するための関数
                            log_loss, \  # ログ損失を評価するための関数
                            confusion_matrix, \  # 混同行列を計算するための関数
                            classification_report  # 分類結果のレポートを生成するための関数
```

---The following area is a Code cell (cell numver is 4)---
```python
nltk.download('stopwords')  # NLTKライブラリからストップワードのリストをダウンロードします。
stop_words = set(stopwords.words('english'))  # 英語のストップワードを取得し、セットとして格納します。

# ストップワードとは、テキストデータの中で特に意味を持たない単語のことであり、
# これを除外することで、テキスト分析の精度を向上させることができます。
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## データの読み込み
```

---The following area is a Code cell (cell numver is 6)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます。
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。

# pd.read_csv関数は、指定されたファイルパスからCSV形式のデータを読み込むための関数です。
# 読み込まれたデータはpandasのDataFrame形式で格納され、データの操作や分析に利用できます。
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## 探索的データ分析（EDA）
```

---The following area is a Code cell (cell numver is 8)---
```python
train_analysis = sv.analyze(train_data)  # トレーニングデータに対してSweetVizを用いて分析を行います。

# sv.analyze関数は、指定されたデータセットに関する詳細なレポートを生成し、
# データの特性や分布を視覚的に示すための情報を提供します。 
# このレポートは、データの概要を把握するのに役立ちます。
```

---The following area is a Code cell (cell numver is 9)---
```python
train_analysis.show_html('train_analysis.html')  # SweetVizによるトレーニングデータの分析結果をHTMLファイルとして保存します。

# show_html関数は、分析結果をHTML形式で出力し、指定されたファイル名で保存します。
# このHTMLファイルは、データの可視化や分析結果をブラウザで確認するのに便利です。
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data.head()  # トレーニングデータの最初の5行を表示します。

# head()関数は、DataFrameの最初の数行を表示するための関数で、
# データの構造や内容をすばやく確認するのに役立ちます。
# 引数を指定することで、表示する行数を変更することも可能です。デフォルトでは5行表示されます。
```

---The following area is a Code cell (cell numver is 11)---
```python
print("Training Data -", train_data.shape)  # トレーニングデータの形状（行数と列数）を表示します。
print("Test Data -", test_data.shape)  # テストデータの形状（行数と列数）を表示します。

# shape属性は、データフレームの行数と列数をタプル形式で返します。
# これにより、データセットのサイズを簡単に把握することができます。
```

---The following area is a Code cell (cell numver is 12)---
```python
train_data.describe(include=['O'])  # トレーニングデータのオブジェクト型（文字列）の列に関する要約統計量を表示します。

# describe()関数は、データフレームの基本的な統計情報を提供します。
# include=['O']を指定することで、オブジェクト型の列（通常は文字列データ）のみを対象にした要約が取得できます。
# 出力には、ユニークな値の数や最頻値などが含まれ、データの特徴を把握するのに役立ちます。
```

---The following area is a Code cell (cell numver is 13)---
```python
print(train_data.info())  # トレーニングデータの情報を表示します。

# info()関数は、データフレームの構造に関する詳細情報を提供します。
# 出力には、各列のデータ型、非NULL値のカウント、メモリの使用量などが含まれます。
# これにより、データセットの各列の状態を簡単に確認することができます。
```

---The following area is a Code cell (cell numver is 14)---
```python
train_data.drop("id", axis=1).duplicated().sum()  # "id"列を除いたトレーニングデータの重複行の数をカウントします。

# drop("id", axis=1)は、"id"という列をデータフレームから削除します。
# duplicated()メソッドは、重複している行を検出し、重複行に対してTrueを返します。
# sum()は、Trueの合計を計算し、重複行の総数を返します。 
# これにより、データセット内の重複するデータの有無を確認できます。
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
14行の重複が存在し、7つのグループを形成しています。私は各グループごとに1行のみを保持します。
```

---The following area is a Code cell (cell numver is 16)---
```python
train_data = train_data.drop_duplicates(keep="first", ignore_index=True)  # 重複行を削除し、先頭の行を保持します。

# drop_duplicates()メソッドは、データフレーム内の重複行を削除します。
# 引数keep="first"は、最初の出現を保持し、他の重複行を削除することを指定します。
# ignore_index=Trueを指定することで、インデックスをリセットし、新たな連続したインデックスを付与します。
# これにより、重複行が削除された後でも、インデックスが整然とした状態になります。
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
`id`列に関してトレーニングデータの品質を確認します。
```

---The following area is a Code cell (cell numver is 18)---
```python
train_data.nunique()  # トレーニングデータの各列におけるユニークな値の数を表示します。

# nunique()メソッドは、データフレーム内の各列においてユニークな値の個数を計算します。
# この情報は、各列のデータにどれだけのバリエーションがあるかを把握するのに役立ちます。
```

---The following area is a Code cell (cell numver is 19)---
```python
assert train_data["id"].nunique() == len(train_data)  # `id`列のユニークな値の数がトレーニングデータの行数と等しいことを確認します。

# assert文は、指定された条件がTrueであることをチェックします。
# もし条件が満たされない場合、エラーが発生し、プログラムの実行が停止します。
# この条件により、`id`列に重複がないこと（すなわち、全ての`id`がユニークであること）を確認しています。
```

---The following area is a Code cell (cell numver is 20)---
```python
train_data.isna().sum()  # トレーニングデータの各列における欠損値（NaN）の合計を表示します。

# isna()メソッドは、データフレームの各セルが欠損値であるかどうかを確認し、ブーリアン値（TrueまたはFalse）を返します。
# sum()関数を使用することで、各列に含まれる欠損値の合計を計算し、結果を表示します。
# この情報は、データのクリーニングや前処理において欠損値の処理を行う前に確認するのに役立ちます。
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
### 分布
```

---The following area is a Code cell (cell numver is 22)---
```python
model_df = pd.concat([train_data.model_a, train_data.model_b])  # model_aとmodel_bの列を結合します。
counts = model_df.value_counts().reset_index()  # 各モデルの出現回数をカウントし、データフレームにリセットします。
counts.columns = ['LLM', 'Count']  # カラム名を設定します。

# Plotlyを使用してカスタムスタイルの棒グラフを作成します。
fig = px.bar(counts, x='LLM', y='Count',
             title='モデルの分布',
             color='Count')  # モデルに応じた色を設定します。

fig.update_layout(xaxis_tickangle=-45)  # x軸のラベルを45度回転させて見やすくします。

fig.show()  # グラフを表示します。
```

---The following area is a Code cell (cell numver is 23)---
```python
counts_a = train_data['winner_model_a'].value_counts().reset_index()  # モデルAの勝利数をカウントします。
counts_b = train_data['winner_model_b'].value_counts().reset_index()  # モデルBの勝利数をカウントします。
counts_tie = train_data['winner_tie'].value_counts().reset_index()  # 引き分けの数をカウントします。

# 分かりやすくするためにカラム名を変更します。
counts_a.columns = ['Winner', 'Count']
counts_b.columns = ['Winner', 'Count']
counts_tie.columns = ['Winner', 'Count']

# モデルを識別するためのカラムを追加します。
counts_a['Model'] = 'Model A'
counts_b['Model'] = 'Model B'
counts_tie['Model'] = 'Tie'

# すべてのカウントを結合します。
counts = pd.concat([counts_a, counts_b, counts_tie])

# カスタムスタイルの棒グラフを作成します。
fig = px.bar(counts, x='Model', y='Count', 
             color='Model',
             title='トレーニングデータの勝者分布',
             labels={'Model': 'モデル', 'Count': '勝利数', 'Winner': '勝者'})

fig.update_layout(xaxis_title="モデル", yaxis_title="勝利数")  # x軸とy軸のタイトルを設定します。

fig.show()  # グラフを表示します。
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
結論:

* トレーニングデータは57,477行、テストデータは3行です。
    * 注: スコアリングフェーズ中にテストデータは完全なテストセット（約25,000行、70%はプライベートLB用）に置き換えられます。
* `id`列には重複値がありません。
* モデルの識別子はテストセットでは明らかにされていません。
* `prompt`、`response_a`、`response_b`の各列の文字列はリストにラップされています。
    * これは、各チャットが複数のプロンプト/レスポンスのペアを含む可能性があるためです。
* `id`列を削除した後、14行の重複が存在し、7つのグループを形成しています。各グループごとに1行だけを保持した結果、トレーニングデータフレームの形状は(57,470, 8)になります。

## データ準備と特徴量エンジニアリング

* データのクリーニング: 特殊文字を削除し、小文字に正規化し、ストップワードを削除し、トークン化するなどのテキストをクリーンアップします。
* 入力のトークン化: トレーニングデータに基づいてTensorFlow/Kerasのトークナイザーを使用し、トレーニングデータとテストデータの両方にフィットさせます。
* シーケンスを`max_len`にパディングします。
* BERT埋め込みを作成します。
* 各モデルのプロンプトとレスポンス間でBERTを使用した類似度特徴量を計算します。
* 各レスポンスについて、単語数、文字数、語彙の多様性を計算します。
* BERTモデル用にテキスト入力をトークン化します。
```

---The following area is a Code cell (cell numver is 25)---
```python
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv').sample(frac=0.001)  # トレーニングデータをCSVファイルから読み込み、全体の0.1%をサンプリングします。

# read_csv()関数は、指定されたファイルパスからCSV形式のデータを読み込みます。
# sample(frac=0.001)は、データフレームから全体の0.1%（0.001）のランダムなサンプルを抽出します。
# これにより、大規模なデータセットの一部を使用して迅速に実験や分析を行うことができます。
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
### データのクリーニング

特殊文字を削除し、小文字に正規化し、ストップワードを削除するなどのクリーンなテキストを作成します。
```

---The following area is a Code cell (cell numver is 27)---
```python
def clean_text(text):  # テキストをクリーンアップするための関数を定義します。
    text = text.lower()  # テキストを小文字に変換します。
    text = re.sub(r'\[.*?\]', '', text)  # 括弧内の内容を削除します。
    text = re.sub(r'https?://\S+|www\.\S+', '', text)  # URLを削除します。
    text = re.sub(r'<.*?>+', '', text)  # HTMLタグを削除します。
    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # 句読点を削除します。
    text = re.sub(r'\n', '', text)  # 改行を削除します。
    text = re.sub(r'\w*\d\w*', '', text)  # 数字を含む単語を削除します。
    text = ' '.join(word for word in text.split() if word not in stop_words)  # ストップワードを削除し、単語を再結合します。
    return text  # クリーンアップされたテキストを返します。
```

---The following area is a Code cell (cell numver is 28)---
```python
# テキストのクリーニングを行います。
train_data['prompt_clean'] = train_data['prompt'].apply(clean_text)  # 'prompt'列にclean_text関数を適用し、クリーニングした結果を'prompt_clean'列に格納します。
train_data['response_a_clean'] = train_data['response_a'].apply(clean_text)  # 'response_a'列にclean_text関数を適用し、クリーニングした結果を'response_a_clean'列に格納します。
train_data['response_b_clean'] = train_data['response_b'].apply(clean_text)  # 'response_b'列にclean_text関数を適用し、クリーニングした結果を'response_b_clean'列に格納します。

# apply()メソッドは、指定した関数をデータフレームの各要素に適用し、新しいシリーズを返します。
# これにより、元のテキストデータがクリーニングされ、分析やモデルの入力に適した形式へと変換されます。
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
### 入力のトークン化

TensorFlow/Kerasのトークナイザーを使用して、トレーニングデータとテストデータの両方に対してトークン化を行います。シーケンスを`max_len`にパディングします。
```

---The following area is a Code cell (cell numver is 30)---
```python
max_len = 512  # シーケンスの最大長を512に設定します。

# max_lenは、BERTなどのトランスフォーマーモデルで処理できる入力シーケンスの最大の長さを指定します。
# これにより、すべての入力が均一な長さになるようにパディングやカットが適用されます。
```

---The following area is a Code cell (cell numver is 31)---
```python
tokenizer = Tokenizer(num_words=20000)  # 語彙サイズを20,000語に制限したトークナイザーを初期化します。

# Tokenizerクラスは、テキストを数値のトークンに変換するためのもので、num_words引数は使用する語彙の最大数を指定します。
# これにより、頻出の単語を保持し、あまり使われない単語は除外されるため、モデルの効率が向上します。
```

---The following area is a Code cell (cell numver is 32)---
```python
tokenizer.fit_on_texts(pd.concat([train_data['prompt_clean'], train_data['response_a_clean'], train_data['response_b_clean']]))  # トークナイザーをトレーニングデータのクリーニングされたプロンプトとレスポンスでフィットさせます。

# fit_on_texts()メソッドは、指定されたテキストデータから単語の頻度に基づいて語彙を構築します。
# pd.concat()を使用して、プロンプトとレスポンスを結合し、一緒にトークン化することで、すべてのテキストに基づいた語彙を作成します。
```

---The following area is a Code cell (cell numver is 33)---
```python
train_sequences = tokenizer.texts_to_sequences(train_data['prompt_clean'])  # クリーニングされたプロンプトを数値のシーケンスに変換します。
response_a_sequences = tokenizer.texts_to_sequences(train_data['response_a_clean'])  # クリーニングされたレスポンスAを数値のシーケンスに変換します。
response_b_sequences = tokenizer.texts_to_sequences(train_data['response_b_clean'])  # クリーニングされたレスポンスBを数値のシーケンスに変換します。

# texts_to_sequences()メソッドは、テキストを数値のシーケンスに変換し、各単語を対応するインデックスに置き換えます。
# これにより、モデルに入力できる形式のデータが得られ、後の処理や学習のために準備されます。
```

---The following area is a Code cell (cell numver is 34)---
```python
# パディングを行います。
train_sequences = pad_sequences(train_sequences, maxlen=max_len, padding='post')  # プロンプトのシーケンスを最大長にパディングします。シーケンスの末尾にパディングを追加します。
response_a_sequences = pad_sequences(response_a_sequences, maxlen=max_len, padding='post')  # レスポンスAのシーケンスを最大長にパディングします。
response_b_sequences = pad_sequences(response_b_sequences, maxlen=max_len, padding='post')  # レスポンスBのシーケンスを最大長にパディングします。

# pad_sequences()は、すべてのシーケンスを指定した最大長に整形し、短いシーケンスにはゼロや指定したパディング値を追加します。
# padding='post'を指定することで、シーケンスの後ろにパディングを追加し、長さを統一します。これにより、モデルの入力データが整った形式になります。
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
### 感情分析

`vaderSentiment`を使用した感情分析。VADER（Valence Aware Dictionary and sEntiment Reasoner）は、特にソーシャルメディアで表現される感情に調整された、辞書およびルールベースの感情分析ツールです。
```

---The following area is a Code cell (cell numver is 36)---
```python
analyzer = SentimentIntensityAnalyzer()  # 感情分析のためのSentimentIntensityAnalyzerを初期化します。

# SentimentIntensityAnalyzerは、テキストの感情を評価するための便利なツールで、
# 各テキストに対してポジティブ、ネガティブ、ニュートラルなスコアを生成し、
# 全体的な感情の強度を示すコンパウンドスコアも計算します。
```

---The following area is a Code cell (cell numver is 37)---
```python
def sentiment_analysis(text):  # 感情分析を行う関数を定義します。
    return analyzer.polarity_scores(text)['compound']  # テキストの感情スコアを計算し、コンパウンドスコアを返します。

# polarity_scores()メソッドは、入力テキストに対して感情のポジティブ、ネガティブ、ニュートラル、およびコンパウンドスコアを計算します。
# コンパウンドスコアは、全体的な感情の強度を示し、-1（非常にネガティブ）から1（非常にポジティブ）の範囲で評価されます。
```

---The following area is a Code cell (cell numver is 38)---
```python
train_data['sentiment_prompt'] = train_data['prompt_clean'].apply(sentiment_analysis)  # クリーニングされたプロンプトに対して感情分析を行い、結果を'sentiment_prompt'列に格納します。
train_data['sentiment_response_a'] = train_data['response_a_clean'].apply(sentiment_analysis)  # クリーニングされたレスポンスAに対して感情分析を行い、結果を'sentiment_response_a'列に格納します。
train_data['sentiment_response_b'] = train_data['response_b_clean'].apply(sentiment_analysis)  # クリーニングされたレスポンスBに対して感情分析を行い、結果を'sentiment_response_b'列に格納します。

# apply()メソッドを使用して、各クリーニングされたテキストにsentiment_analysis関数を適用することで、
# 各テキストの感情スコアを計算し、新しい列としてデータフレームに追加します。
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
### テキスト特徴量

単語数、文字数、語彙の多様性、音節数、文の数、各レスポンスの読みやすさを定量的に測定するFlesch Reading Easeスコアなどのテキスト特徴量を計算します。

テキスト統計を分析するために`textstat`ライブラリを使用しています。
```

---The following area is a Code cell (cell numver is 40)---
```python
def word_count(text):  # 単語数を計算する関数を定義します。
    return len(text.split())  # テキストをスペースで分割し、単語数を返します。

def char_count(text):  # 文字数を計算する関数を定義します。
    return len(text)  # テキストの全体の長さ（文字数）を返します。

def lexical_diversity(text):  # 語彙の多様性を計算する関数を定義します。
    words = text.split()  # テキストを単語に分割します。
    return len(set(words)) / len(words) if words else 0  # ユニークな単語の数を全単語数で割り、語彙の多様性を計算します。

def syllable_count(text):  # 音節数を計算する関数を定義します。
    return textstat.syllable_count(text)  # textstatライブラリを使用して音節数を返します。

def sentence_count(text):  # 文の数を計算する関数を定義します。
    return textstat.sentence_count(text)  # textstatライブラリを使用して文の数を返します。

def flesch_reading_ease(text):  # Flesch Reading Easeスコアを計算する関数を定義します。
    return textstat.flesch_reading_ease(text)  # textstatライブラリを使用してFlesch Reading Easeスコアを返します。
```

---The following area is a Code cell (cell numver is 41)---
```python
train_data['word_count_prompt'] = train_data['prompt_clean'].apply(word_count)  # クリーニングされたプロンプトに対して単語数を計算し、'word_count_prompt'列に格納します。
train_data['word_count_response_a'] = train_data['response_a_clean'].apply(word_count)  # クリーニングされたレスポンスAに対して単語数を計算し、'word_count_response_a'列に格納します。
train_data['word_count_response_b'] = train_data['response_b_clean'].apply(word_count)  # クリーニングされたレスポンスBに対して単語数を計算し、'word_count_response_b'列に格納します。
train_data['char_count_prompt'] = train_data['prompt_clean'].apply(char_count)  # クリーニングされたプロンプトに対して文字数を計算し、'char_count_prompt'列に格納します。
train_data['char_count_response_a'] = train_data['response_a_clean'].apply(char_count)  # クリーニングされたレスポンスAに対して文字数を計算し、'char_count_response_a'列に格納します。
train_data['char_count_response_b'] = train_data['response_b_clean'].apply(char_count)  # クリーニングされたレスポンスBに対して文字数を計算し、'char_count_response_b'列に格納します。
train_data['lexical_diversity_prompt'] = train_data['prompt_clean'].apply(lexical_diversity)  # クリーニングされたプロンプトの語彙の多様性を計算し、'lexical_diversity_prompt'列に格納します。
train_data['lexical_diversity_response_a'] = train_data['response_a_clean'].apply(lexical_diversity)  # クリーニングされたレスポンスAの語彙の多様性を計算し、'lexical_diversity_response_a'列に格納します。
train_data['lexical_diversity_response_b'] = train_data['response_b_clean'].apply(lexical_diversity)  # クリーニングされたレスポンスBの語彙の多様性を計算し、'lexical_diversity_response_b'列に格納します。
train_data['syllable_count_prompt'] = train_data['prompt_clean'].apply(syllable_count)  # クリーニングされたプロンプトの音節数を計算し、'syllable_count_prompt'列に格納します。
train_data['syllable_count_response_a'] = train_data['response_a_clean'].apply(syllable_count)  # クリーニングされたレスポンスAの音節数を計算し、'syllable_count_response_a'列に格納します。
train_data['syllable_count_response_b'] = train_data['response_b_clean'].apply(syllable_count)  # クリーニングされたレスポンスBの音節数を計算し、'syllable_count_response_b'列に格納します。
train_data['sentence_count_prompt'] = train_data['prompt_clean'].apply(sentence_count)  # クリーニングされたプロンプトの文の数を計算し、'sentence_count_prompt'列に格納します。
train_data['sentence_count_response_a'] = train_data['response_a_clean'].apply(sentence_count)  # クリーニングされたレスポンスAの文の数を計算し、'sentence_count_response_a'列に格納します。
train_data['sentence_count_response_b'] = train_data['response_b_clean'].apply(sentence_count)  # クリーニングされたレスポンスBの文の数を計算し、'sentence_count_response_b'列に格納します。
train_data['flesch_reading_ease_prompt'] = train_data['prompt_clean'].apply(flesch_reading_ease)  # クリーニングされたプロンプトのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_prompt'列に格納します。
train_data['flesch_reading_ease_response_a'] = train_data['response_a_clean'].apply(flesch_reading_ease)  # クリーニングされたレスポンスAのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_response_a'列に格納します。
train_data['flesch_reading_ease_response_b'] = train_data['response_b_clean'].apply(flesch_reading_ease)  # クリーニングされたレスポンスBのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_response_b'列に格納します。
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
### BERT埋め込みの作成

トレーニングデータおよびテストデータのプロンプトとレスポンスに対してBERT埋め込みを計算します。また、各モデルのプロンプトとレスポンス間でBERTを使用したコサイン類似度特徴量も計算します。

効率的なパイプラインを作成するために`tf.data.Dataset`を使用し、GPUを利用してバッチで特徴量を処理します。また、`joblib`ライブラリを使用して中間埋め込みを保存する予定です。
```

---The following area is a Code cell (cell numver is 43)---
```python
# BERTモデルをロードします。
bert_model_name = 'bert-base-uncased'  # 使用するBERTモデルの名前を指定します。
bert_tokenizer = BertTokenizer.from_pretrained(bert_model_name)  # BERTトークナイザーを事前学習済みモデルから初期化します。
bert_model = TFBertModel.from_pretrained(bert_model_name)  # BERTモデルを事前学習済みモデルから初期化します。

# BertTokenizerはテキストをBERTモデル用にトークン化するためのもので、
# TFBertModelはTensorFlowで動作するBERTモデル本体を提供します。
# これにより、自然言語処理タスクにおいてBERTを利用できるようになります。
```

---The following area is a Code cell (cell numver is 44)---
```python
@tf.function  # TensorFlowの関数として最適化されることを示します。
def get_bert_embeddings(texts):  # テキストに対してBERT埋め込みを取得する関数を定義します。
    inputs = bert_tokenizer(texts,  # テキストをトークナイズします。
                       return_tensors='tf',  # TensorFlowテンソルとして返します。
                       padding=True,  # パディングを有効にします。
                       truncation=True,  # テキストがmax_lengthを超える場合は切り捨てます。
                       max_length=512)  # 最大入力長を512に設定します。
    outputs = bert_model(inputs)  # BERTモデルを使用して埋め込みを生成します。
    return outputs.last_hidden_state[:, 0, :]  # 最後の隠れ層の出力の最初のトークン（[CLS]トークン）の埋め込みを返します。

# この関数は、与えられたテキストからBERT埋め込みを効率的に計算し、
# 特に分類タスクなどで重要な役割を果たす[CLS]トークンの埋め込みを取得します。
```

---The following area is a Code cell (cell numver is 45)---
```python
def process_column(column_data):  # 指定された列のデータを処理する関数を定義します。
    column_data = column_data.dropna().tolist()  # 欠損値を削除し、リストに変換します。
    column_data = [str(text) for text in column_data]  # 各テキストを文字列に変換します。  
    dataset = tf.data.Dataset.from_tensor_slices(column_data)  # リストからTensorFlowデータセットを作成します。
    dataset = dataset.batch(8)  # バッチサイズを8に設定します。

    embeddings = []  # 埋め込みを格納するリストを初期化します。
    for batch in dataset:  # データセットの各バッチに対してループします。
        batch_list = [str(text) for text in batch.numpy().tolist()]  # バッチ内のテキストをリストに変換します。
        batch_embeddings = get_bert_embeddings(batch_list)  # BERT埋め込みを取得します。
        embeddings.append(batch_embeddings)  # 埋め込みをリストに追加します。
    
    return np.concatenate(embeddings, axis=0)  # 埋め込みを結合し、1つのNumPy配列として返します。

# この関数は、与えられた列のテキストデータを処理し、BERTを使用して埋め込みを生成します。
# バッチ処理を使用することで、メモリ使用量を最適化し、GPUを効率的に活用できます。
```

---The following area is a Code cell (cell numver is 46)---
```python
def add_embeddings_to_dataframe(df, column_names):  # データフレームに埋め込みを追加する関数を定義します。
    for column in column_names:  # 各指定された列に対してループします。
        print(f"Processing column: {column}")  # 現在処理中の列を表示します。
        embeddings = process_column(df[column])  # 指定された列に対して埋め込みを生成します。
        df[f'{column}_embedding'] = list(embeddings)  # 生成された埋め込みを新しい列としてデータフレームに追加します。
    return df  # 更新されたデータフレームを返します。

# この関数は、指定された列の埋め込みを計算し、それらを元のデータフレームに追加します。
# これにより、元のデータとその埋め込み表現を一緒に保持することができます。
```

---The following area is a Code cell (cell numver is 47)---
```python
columns_to_embed = ['prompt_clean', 'response_a_clean', 'response_b_clean']  # 埋め込みを計算する列のリストを定義します。

# このリストには、クリーンなプロンプトおよびレスポンスのテキストデータが含まれており、
# 後でこれらの列に対してBERT埋め込みを生成するために使用されます。
```

---The following area is a Code cell (cell numver is 48)---
```python
train_data = add_embeddings_to_dataframe(train_data, columns_to_embed)  # 指定された列に対して埋め込みを追加し、更新されたデータフレームを保存します。

# add_embeddings_to_dataframe関数を呼び出すことで、トレーニングデータに埋め込みを計算し、
# 新たに生成された埋め込みがデータフレームに追加されます。これにより、元のテキストデータとその埋め込みが一緒に使用できる状態になります。
```

---The following area is a Code cell (cell numver is 49)---
```python
train_data['similarity_prompt_response_a'] = train_data.apply(  # プロンプトとレスポンスAのコサイン類似度を計算して新しい列を作成します。
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),  # プロンプト埋め込みを1次元の配列に変形します。
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)  # レスポンスA埋め込みを1次元の配列に変形し、コサイン類似度を計算します。

train_data['similarity_prompt_response_b'] = train_data.apply(  # プロンプトとレスポンスBのコサイン類似度を計算して新しい列を作成します。
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),  # プロンプト埋め込みを1次元の配列に変形します。
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)  # レスポンスB埋め込みを1次元の配列に変形し、コサイン類似度を計算します。

# apply()メソッドを使用して、各行に対してプロンプト埋め込みとレスポンス埋め込みのコサイン類似度を計算し、
# 結果を新しい列に格納します。コサイン類似度は、二つのベクトルの角度を基にした類似度の指標であり、
# 値が1に近いほど二つのベクトルが類似していることを意味します。
```

---The following area is a Markdown cell (cell numver is 50)---
```markdown
### データの準備
```

---The following area is a Code cell (cell numver is 51)---
```python
X = train_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',  # プロンプトおよびレスポンスAとBの単語数
                'char_count_prompt', 'char_count_response_a', 'char_count_response_b',  # プロンプトおよびレスポンスの文字数
                'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',  # 語彙の多様性
                'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',  # 音節数
                'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',  # 文の数
                'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',  # Flesch Reading Easeスコア
                'similarity_prompt_response_a', 'similarity_prompt_response_b',  # コサイン類似度
                'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]  # 感情スコア

# このコードは、モデルの入力に使用する特徴量（X）のデータフレームを作成しています。
# 各列はテキスト特徴量、感情スコア、コサイン類似度を表しており、
# 学習に用いるデータとして後で利用されます。
```

---The following area is a Code cell (cell numver is 52)---
```python
# 目標列を定義します。
train_data['winner'] = train_data.apply(lambda x: 0 if x['winner_model_a'] == 1 else (1 if x['winner_model_b'] == 1 else 2), axis=1)  # モデルAの勝利を0、モデルBの勝利を1、引き分けを2として設定します。

# apply()メソッドを使用して、各行に対して条件に基づいて勝者のラベルを決定します。
# これにより、モデルの出力として使用される目標変数（winner）が作成されます。
```

---The following area is a Code cell (cell numver is 53)---
```python
y = train_data['winner']  # 目標変数（勝者）をyとして定義します。

# このコードにより、トレーニングデータにおける各サンプルの勝者ラベルを含むシリーズが作成され、学習用モデルのターゲットとして使用されます。
```

---The following area is a Markdown cell (cell numver is 54)---
```markdown
### トレーニングデータとバリデーションデータの分割
```

---The following area is a Code cell (cell numver is 55)---
```python
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2,  # データをトレーニングセットとバリデーションセットに分割します。
                                                  random_state=42)  # テストサイズを20%に設定し、乱数シードを42にします。

# train_test_split()関数は、指定されたデータ（特徴量Xとターゲットy）をトレーニングデータとバリデーションデータに分割します。
# random_stateを設定することで、分割結果を再現可能にします。これにより、毎回同じデータが分割されることが保証されます。
```

---The following area is a Markdown cell (cell numver is 56)---
```markdown
## モデルの定義

* ランダムフォレスト
* ロジスティック回帰
* サポートベクターマシン
* 勾配ブースティング
* ニューラルネットワーク
```

---The following area is a Code cell (cell numver is 57)---
```python
models = {  # モデルの辞書を定義します。
    'Random Forest': RandomForestClassifier(),  # ランダムフォレスト分類器を追加します。
    'SVM': SVC(probability=True),  # サポートベクターマシンを追加し、確率予測を有効にします。
    'Gradient Boosting': GradientBoostingClassifier()  # 勾配ブースティング分類器を追加します。
}

# この辞書は、後で使用するための異なるモデルを格納し、
# モデルのトレーニングや評価を簡単に行えるようにします。
```

---The following area is a Code cell (cell numver is 58)---
```python
# ニューラルネットワークを作成します。
def create_nn_model(input_shape):  # 入力形状を受け取る関数を定義します。
    model = Sequential()  # シーケンシャルモデルを初期化します。
    model.add(Dense(128, input_shape=(input_shape,), activation='relu'))  # 入力層と128のユニットを持つ全結合層を追加します。
    model.add(Dropout(0.2))  # ドロップアウト層を追加し、過学習を防ぎます。
    model.add(Dense(64, activation='relu'))  # 64のユニットを持つ全結合層を追加します。
    model.add(Dropout(0.2))  # 再度ドロップアウト層を追加します。
    model.add(Dense(3, activation='softmax'))  # 出力層を追加し、3つのクラスの確率を出力します。
    model.compile(optimizer=Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])  # モデルをコンパイルします。
    return model  # 定義したモデルを返します。

# この関数は、指定された入力形状を持つニューラルネットワークモデルを構築し、
# トレーニングに使用するための準備を行います。
```

---The following area is a Code cell (cell numver is 59)---
```python
nn_model = create_nn_model(X_train.shape[1])  # トレーニングデータの特徴量の数に基づいてニューラルネットワークモデルを作成します。
models['Neural Network'] = nn_model  # 作成したニューラルネットワークモデルをモデルの辞書に追加します。

# これにより、ニューラルネットワークモデルが定義されたモデルのリストに追加され、
# 他のモデルと同様にトレーニングや評価が可能になります。
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
モデルのトレーニングと評価:
```

---The following area is a Code cell (cell numver is 61)---
```python
results = {}  # モデルの評価結果を格納する辞書を初期化します。

for name, model in models.items():  # 定義されたすべてのモデルに対してループします。
    print(f"トレーニングと評価を行っています: {name}...")
    
    if name == 'Neural Network':  # ニューラルネットワークの場合
        model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), verbose=2)  # モデルをフィットし、検証データを使用します。
        y_pred = np.argmax(model.predict(X_val), axis=1)  # バリデーションデータに対する予測結果を取得します。
        y_pred_proba = model.predict(X_val)  # 予測確率を取得します。
    else:  # その他のモデルの場合
        model.fit(X_train, y_train)  # モデルをフィットします。
        y_pred = model.predict(X_val)  # バリデーションデータに対する予測結果を取得します。
        y_pred_proba = model.predict_proba(X_val)  # 予測確率を取得します。

    accuracy = accuracy_score(y_val, y_pred)  # 精度を計算します。
    logloss = log_loss(y_val, y_pred_proba)  # ログ損失を計算します。

    results[name] = {  # 結果を辞書に格納します。
        '精度': accuracy,
        'ログ損失': logloss,
        '分類レポート': classification_report(y_val, y_pred),
        '混同行列': confusion_matrix(y_val, y_pred)
    }

    print(f"{name}の精度: {accuracy}")  # モデルの精度を表示します。
    print(f"{name}のログ損失: {logloss}")  # モデルのログ損失を表示します。
    print(f"{name}の分類レポート:\n{classification_report(y_val, y_pred)}")  # 分類レポートを表示します。
    print(f"{name}の混同行列:\n{confusion_matrix(y_val, y_pred)}\n")  # 混同行列を表示します。
```

---The following area is a Markdown cell (cell numver is 62)---
```markdown
ログ損失に基づいて最良のモデルを選択しています。
```

---The following area is a Code cell (cell numver is 63)---
```python
best_model_name = max(results, key=lambda name: results[name]['Log Loss'])  # 最小のログ損失を持つモデル名を取得します。
best_model = models[best_model_name]  # 最良のモデルを辞書から取得します。

# これにより、ログ損失が最も小さいモデルを選択し、
# そのモデルを後で使用するための変数best_modelに格納します。
```

---The following area is a Markdown cell (cell numver is 64)---
```markdown
## 予測と提出

テストデータの準備、テストデータに対する予測、および提出用データの作成を行います。
```

---The following area is a Code cell (cell numver is 65)---
```python
# テストデータのテキストをクリーンアップします。
test_data['prompt_clean'] = test_data['prompt'].apply(clean_text)  # 'prompt'列にclean_text関数を適用し、クリーンアップした結果を'prompt_clean'列に格納します。
test_data['response_a_clean'] = test_data['response_a'].apply(clean_text)  # 'response_a'列にclean_text関数を適用し、クリーンアップした結果を'response_a_clean'列に格納します。
test_data['response_b_clean'] = test_data['response_b'].apply(clean_text)  # 'response_b'列にclean_text関数を適用し、クリーンアップした結果を'response_b_clean'列に格納します。

# このコードにより、テストデータのテキストが前処理され、モデルへの入力に適した形式になります。
```

---The following area is a Code cell (cell numver is 66)---
```python
# テストデータのテキストをトークン化します。
test_sequences = tokenizer.texts_to_sequences(test_data['prompt_clean'])  # クリーンなプロンプトを数値のシーケンスに変換します。
response_a_test_sequences = tokenizer.texts_to_sequences(test_data['response_a_clean'])  # クリーンなレスポンスAを数値のシーケンスに変換します。
response_b_test_sequences = tokenizer.texts_to_sequences(test_data['response_b_clean'])  # クリーンなレスポンスBを数値のシーケンスに変換します。

# texts_to_sequences()メソッドを使用することで、テキストをトークンに変換し、
# モデルの入力として使用できる形式に準備します。
```

---The following area is a Code cell (cell numver is 67)---
```python
# テストデータのシーケンスにパディングを行います。
test_sequences = pad_sequences(test_sequences, maxlen=max_len, padding='post')  # プロンプトのシーケンスを最大長にパディングします。
response_a_test_sequences = pad_sequences(response_a_test_sequences, maxlen=max_len, padding='post')  # レスポンスAのシーケンスを最大長にパディングします。
response_b_test_sequences = pad_sequences(response_b_test_sequences, maxlen=max_len, padding='post')  # レスポンスBのシーケンスを最大長にパディングします。

# pad_sequences()を使用することで、すべてのシーケンスを指定した最大長に整形し、
# 短いシーケンスにはゼロパディングを追加して、モデルに入力できる形式にします。
```

---The following area is a Code cell (cell numver is 68)---
```python
# テストデータに対して感情分析を行います。
test_data['sentiment_prompt'] = test_data['prompt_clean'].apply(sentiment_analysis)  # クリーニングされたプロンプトに感情分析を適用し、結果を'sentiment_prompt'列に格納します。
test_data['sentiment_response_a'] = test_data['response_a_clean'].apply(sentiment_analysis)  # クリーニングされたレスポンスAに感情分析を適用し、結果を'sentiment_response_a'列に格納します。
test_data['sentiment_response_b'] = test_data['response_b_clean'].apply(sentiment_analysis)  # クリーニングされたレスポンスBに感情分析を適用し、結果を'sentiment_response_b'列に格納します。

# このコードにより、テストデータの各テキストの感情スコアが計算され、
# 後のモデルに対する入力として使用できるようになります。
```

---The following area is a Code cell (cell numver is 69)---
```python
# テストデータからテキスト構造の特徴量を作成します。
test_data['word_count_prompt'] = test_data['prompt_clean'].apply(word_count)  # プロンプトの単語数を計算し、'word_count_prompt'列に格納します。
test_data['word_count_response_a'] = test_data['response_a_clean'].apply(word_count)  # レスポンスAの単語数を計算し、'word_count_response_a'列に格納します。
test_data['word_count_response_b'] = test_data['response_b_clean'].apply(word_count)  # レスポンスBの単語数を計算し、'word_count_response_b'列に格納します。
test_data['char_count_prompt'] = test_data['prompt_clean'].apply(char_count)  # プロンプトの文字数を計算し、'char_count_prompt'列に格納します。
test_data['char_count_response_a'] = test_data['response_a_clean'].apply(char_count)  # レスポンスAの文字数を計算し、'char_count_response_a'列に格納します。
test_data['char_count_response_b'] = test_data['response_b_clean'].apply(char_count)  # レスポンスBの文字数を計算し、'char_count_response_b'列に格納します。
test_data['lexical_diversity_prompt'] = test_data['prompt_clean'].apply(lexical_diversity)  # プロンプトの語彙の多様性を計算し、'lexical_diversity_prompt'列に格納します。
test_data['lexical_diversity_response_a'] = test_data['response_a_clean'].apply(lexical_diversity)  # レスポンスAの語彙の多様性を計算し、'lexical_diversity_response_a'列に格納します。
test_data['lexical_diversity_response_b'] = test_data['response_b_clean'].apply(lexical_diversity)  # レスポンスBの語彙の多様性を計算し、'lexical_diversity_response_b'列に格納します。
test_data['syllable_count_prompt'] = test_data['prompt_clean'].apply(syllable_count)  # プロンプトの音節数を計算し、'syllable_count_prompt'列に格納します。
test_data['syllable_count_response_a'] = test_data['response_a_clean'].apply(syllable_count)  # レスポンスAの音節数を計算し、'syllable_count_response_a'列に格納します。
test_data['syllable_count_response_b'] = test_data['response_b_clean'].apply(syllable_count)  # レスポンスBの音節数を計算し、'syllable_count_response_b'列に格納します。
test_data['sentence_count_prompt'] = test_data['prompt_clean'].apply(sentence_count)  # プロンプトの文の数を計算し、'sentence_count_prompt'列に格納します。
test_data['sentence_count_response_a'] = test_data['response_a_clean'].apply(sentence_count)  # レスポンスAの文の数を計算し、'sentence_count_response_a'列に格納します。
test_data['sentence_count_response_b'] = test_data['response_b_clean'].apply(sentence_count)  # レスポンスBの文の数を計算し、'sentence_count_response_b'列に格納します。
test_data['flesch_reading_ease_prompt'] = test_data['prompt_clean'].apply(flesch_reading_ease)  # プロンプトのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_prompt'列に格納します。
test_data['flesch_reading_ease_response_a'] = test_data['response_a_clean'].apply(flesch_reading_ease)  # レスポンスAのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_response_a'列に格納します。
test_data['flesch_reading_ease_response_b'] = test_data['response_b_clean'].apply(flesch_reading_ease)  # レスポンスBのFlesch Reading Easeスコアを計算し、'flesch_reading_ease_response_b'列に格納します。
```

---The following area is a Code cell (cell numver is 70)---
```python
# テストデータに埋め込みを追加します。
test_data = add_embeddings_to_dataframe(test_data, columns_to_embed)  # 定義した列に対して埋め込みを生成し、テストデータフレームに追加します。

# このコードにより、テストデータの各テキストに対してBERT埋め込みが計算され、
# 他の特徴量と一緒に保持されるようになります。これによって、モデルに対する入力が整います。
```

---The following area is a Code cell (cell numver is 71)---
```python
# テストデータのコサイン類似度を計算します。
test_data['similarity_prompt_response_a'] = test_data.apply(  # プロンプト埋め込みとレスポンスA埋め込みのコサイン類似度を計算して新しい列を作成します。
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),  # プロンプト埋め込みを1次元の配列に変形します。
                                np.array(x['response_a_clean_embedding']).reshape(1, -1))[0][0], axis=1)  # レスポンスA埋め込みを1次元の配列に変形し、コサイン類似度を計算します。

test_data['similarity_prompt_response_b'] = test_data.apply(  # プロンプト埋め込みとレスポンスB埋め込みのコサイン類似度を計算して新しい列を作成します。
    lambda x: cosine_similarity(np.array(x['prompt_clean_embedding']).reshape(1, -1),  # プロンプト埋め込みを1次元の配列に変形します。
                                np.array(x['response_b_clean_embedding']).reshape(1, -1))[0][0], axis=1)  # レスポンスB埋め込みを1次元の配列に変形し、コサイン類似度を計算します。

# apply()メソッドを使用して、各行に対してプロンプト埋め込みとレスポンス埋め込みのコサイン類似度を計算し、
# 結果を新しい列に格納します。コサイン類似度は、二つのベクトルの角度を基にした類似度の指標であり、
# 値が1に近いほど二つのベクトルが類似していることを意味します。
```

---The following area is a Code cell (cell numver is 72)---
```python
X_test = test_data[['word_count_prompt', 'word_count_response_a', 'word_count_response_b',  # プロンプトおよびレスポンスAとBの単語数
                    'char_count_prompt', 'char_count_response_a', 'char_count_response_b',  # プロンプトおよびレスポンスの文字数
                    'lexical_diversity_prompt', 'lexical_diversity_response_a', 'lexical_diversity_response_b',  # 語彙の多様性
                    'syllable_count_prompt', 'syllable_count_response_a', 'syllable_count_response_b',  # 音節数
                    'sentence_count_prompt', 'sentence_count_response_a', 'sentence_count_response_b',  # 文の数
                    'flesch_reading_ease_prompt', 'flesch_reading_ease_response_a', 'flesch_reading_ease_response_b',  # Flesch Reading Easeスコア
                    'similarity_prompt_response_a', 'similarity_prompt_response_b',  # コサイン類似度
                    'sentiment_prompt', 'sentiment_response_a', 'sentiment_response_b']]  # 感情スコア

# このコードは、テストデータの特徴量（X_test）のデータフレームを作成しています。
# 各列はテキスト特徴量、感情スコア、コサイン類似度を表しており、
# モデルによる予測のために使用されます。
```

---The following area is a Code cell (cell numver is 73)---
```python
test_pred_proba = best_model.predict(X_test)  # 最良のモデルを使用してテストデータに対する予測確率を計算します。

# このコードにより、テストデータに対して最良のモデルが予測を行い、
# 各クラスに対する確率が'test_pred_proba'に格納されます。これにより、後でクラスラベルを決定するために使用できます。
```

---The following area is a Code cell (cell numver is 74)---
```python
submission = pd.DataFrame(test_data['id'])  # テストデータの'id'列を含む新しいデータフレームを作成します。
submission['winner_model_a'] = test_pred_proba[:, 0]  # モデルAの勝率を追加します。
submission['winner_model_b'] = test_pred_proba[:, 1]  # モデルBの勝率を追加します。
submission['winner_tie'] = test_pred_proba[:, 2]  # 引き分けの勝率を追加します。

submission.to_csv('submission.csv', index=False)  # 提出用データフレームをCSVファイルに保存します。

# このコードにより、予測結果を含む提出ファイルが作成され、コンペティションへの提出に利用できます。
```

---The following area is a Code cell (cell numver is 75)---
```python
submission  # 作成した提出用データフレームを表示します。

# これにより、提出ファイルに含まれる内容を確認できます。各行にはテストデータのIDと、その勝者モデルに対する予測確率が含まれています。
```

** @@@ Jupyter Notebook numver 48, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間の好み予測に関連する問題に取り組んでいます。具体的には、ユーザーが2つのチャットボットからの応答のどちらを好むかを予測するための機械学習モデルを構築しています。このプロジェクトでは、以下の手法やライブラリが使用されています。

1. **主要なライブラリ**:
   - **Transformers**: 事前学習済みのGemmaモデルを使用するために必要です。また、トークナイザーやトレーニングの設定を行うための機能も提供します。
   - **Datasets**: データセットの操作を行います。
   - **PEFT**: パラメータ効率的なファインチューニングを行うためにLoRA（低ランクアダプター）を設定します。
   - **Scikit-learn**: モデルの評価に使用されるメトリクスを計算するために利用されます。

2. **問題解決のアプローチ**:
   - ノートブックはまず、Gemmaモデルのインストールと必要なライブラリのインポートから始まります。そして、トレーニングや評価に必要な設定を一元管理するためのデータクラス`Config`を定義します。
   - LoRAの設定が行われ、特定のモデルモジュールへの適用が定義されています。これにより、トレーニングの効率を高めることが可能になります。
   - カスタムトークナイザーを用いて、データセットからの入力を適切に処理し、ユーザーの好みに基づいたラベルを生成します。
   - モデルのトレーニングは、データセットを5つの分割にして、交差検証を通じて行われます。この過程で、評価メトリクスとしてロス（log-loss）や精度（accuracy）を計算し、モデルの性能を評価します。

全体として、このノートブックは、深層学習を用いてチャットボットの応答に対するユーザーの好みを予測するためのフレームワークを構築することを目的としており、GemmaモデルのLoRAによるファインチューニング技術を活用しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# gemma-2は transformers>=4.42.3 から利用可能です
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import copy
from dataclasses import dataclass

import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score

# 必要なライブラリをインポートします。
# os, copy: ファイル操作やオブジェクトのコピーに使用されます。
# dataclass: データクラスを作成するために使います。
# numpy: 数値演算用のライブラリです。
# torch: PyTorchライブラリで、深層学習モデルの構築に使用します。
# datasets: データセットの操作に使用します。
# transformers: 事前学習済みの変換器モデルを扱うための機能群です。
# peft: パラメータ効率的ファインチューニングに関連する機能を提供します。
# sklearn.metrics: メトリクスの計算に利用します。
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 4)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "/kaggle/input/checkpoint-5200/checkpoint-5200"  # 4ビット量子化されたgemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 2
    gradient_accumulation_steps: int = 2  # グローバルバッチサイズは8です
    per_device_eval_batch_size: int = 8
    n_epochs: int = 1
    freeze_layers: int = 16  # 合計42層のうち、最初の16層にはアダプターを追加しません
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()

# 設定用のデータクラスConfigを定義します。
# 各属性にデフォルト値を設定し、モデルのトレーニングや評価に使用します。
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
#### トレーニング引数
```

---The following area is a Code cell (cell numver is 6)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)

# トレーニング時の引数を設定するTrainingArgumentsを作成します。
# 各パラメータはモデルのトレーニングにどのように影響を与えるかを指定します。
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
#### LoRA設定
```

---The following area is a Code cell (cell numver is 8)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # 自己注意のみにターゲットを絞る
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)

# LoRA(低ランクアダプター)の設定を定義します。
# モデルの特定のモジュールに適用し、トレーニングの効率を高めます。
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### トークナイザーとモデルのインスタンス化
```

---The following area is a Code cell (cell numver is 10)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # 終端記号<eos>を追加します
tokenizer.padding_side = "right"

# Gemmaトークナイザーを事前学習済みのチェックポイントからロードし、
# 必要な設定を行います。
```

---The following area is a Code cell (cell numver is 11)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)
model

# モデルを事前学習済みのチェックポイントからロードし、
# シーケンス分類用に構成し、LoRAによるファインチューニングの準備をします。
```

---The following area is a Code cell (cell numver is 12)---
```python
model.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
### データセットのインスタンス化
```

---The following area is a Code cell (cell numver is 14)---
```python
ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
#ds = ds.select(torch.arange(100))  # デモ目的で最初の100件のみ使用します
```

---The following area is a Code cell (cell numver is 15)---
```python
ds
```

---The following area is a Code cell (cell numver is 16)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0
            elif b_win:
                label = 1
            else:
                label = 2
            labels.append(label)
        return {**tokenized, "labels": labels}
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))

# カスタムトークナイザーを定義します。
# トークナイザーを使って入力データを処理し、ラベルを生成します。
```

---The following area is a Code cell (cell numver is 17)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)
ds = ds.map(encode, batched=True)
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
### メトリクスの計算

LBで使用されるロス（log-loss）と補助的メトリクスとしての精度（accuracy）を計算します。
```

---The following area is a Code cell (cell numver is 19)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions
    labels = eval_preds.label_ids
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()
    loss = log_loss(y_true=labels, y_pred=probs)
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))
    return {"acc": acc, "log_loss": loss}

# 評価予測からメトリクス（精度とロス）を計算します。
# softmaxを使って確率を計算し、評価指標を返します。
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
### 分割

ここでは、trainとevalを`id % 5`に従って分割します。
```

---The following area is a Code cell (cell numver is 21)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]
    ) 
    for fold_idx in range(config.n_splits)
]

# データセットを5分割し、各分割にインデックスを割り当てます。
# トレーニング用インデックスと評価用インデックスを生成します。
```

---The following area is a Code cell (cell numver is 22)---
```python
train_idx, eval_idx = folds[config.fold_idx]

trainer = Trainer(
    args=training_args, 
    model=model,
    tokenizer=tokenizer,
    train_dataset=ds.select(train_idx),
    eval_dataset=ds.select(eval_idx),
    compute_metrics=compute_metrics,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
)
trainer.train(resume_from_checkpoint="/kaggle/input/checkpoint-5200/checkpoint-5200")

# Trainerをインスタンス化し、トレーニングを行います。
# 前回のチェックポイントから再開することも可能です。
```

** @@@ Jupyter Notebook numver 49, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に取り組んでいます。具体的には、大規模言語モデル（LLM）によって生成されたチャット応答の中から、ユーザーが好みそうな応答を予測するための機械学習モデルを構築しています。

主な手法としては、Natural Language Processing (NLP)を用いたテキスト分析が行われており、以下のようなライブラリや技術が使用されています：

- **NumPy**や**Pandas**：基本的なデータ処理を行うために用いられます。
- **NLTK**：テキストをトークン化したり、品詞タグ付け、頻度分布の計算などに使用されます。
- **TextStat**：可読性スコアの計算に利用されており、Flesch-KincaidグレードやGunning-Fogインデックスなど多様な指標を提供します。
- **TextBlob**：テキストの感情分析を実行します。
- **SpaCy**：テキスト処理（トークン化など）に使われ、受動態文のカウント等も行います。
- **scikit-learn**：Gradient BoostingやXGBoost、LightGBM、CatBoostなどの強化学習アルゴリズムを使用してモデルを訓練し、性能評価には対数損失（log loss）が採用されています。
- **Optuna**：ハイパーパラメータの最適化に利用されており、異なるモデルタイプ（XGBClassifier, LGBMClassifier, CatBoostClassifier）に対して最適なパラメータを選定します。
- **Concurrent Futures**：テキスト統計を並列処理するために使用され、計算効率が向上しています。

具体的な流れとしては、まずトレーニングデータとテストデータを読み込み、各テキストに対して可読性スコアや感情スコア、文の統計を計算し、特徴量を生成します。次に、これらの特徴量を用いてモデルを訓練し、最終的にテストデータに対する予測を行い、結果をCSVファイルとして保存します。

全体的には、ユーザーの好みを正確に予測するための多様なテキスト分析手法と機械学習アルゴリズムを組み合わせたアプローチが強調されています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これは、kaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下のいくつかの便利なパッケージを読み込むことができます

import numpy as np # 線形代数
import pandas as pd # データ処理、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは読み取り専用の "../input/" ディレクトリにあります
# 例えば、これを実行すると（実行ボタンをクリックするかShift+Enterを押す）、入力ディレクトリ内のファイルがすべてリストされます

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ(/kaggle/working/)には最大20GBまで書き込むことができ、バージョンを作成する際に出力として保存されます
# 一時ファイルは/kaggle/temp/に書き込むことができますが、現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
import sys
sys.path.append('/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/Pyphen-0.9.3-py2.py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
sys.path.append('/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl')
!pip install '/kaggle/input/textstat-pypi/textstat-0.7.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 5)---
```python
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import textstat
from textblob import TextBlob
import spacy
import concurrent.futures
import optuna
from sklearn.metrics import log_loss
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
# nltk.download('punkt')  # NLTKのpunktモデルをダウンロードします
```

---The following area is a Code cell (cell numver is 6)---
```python
def calculate_readability_scores(text):
    # この関数は、さまざまな可読性スコアを計算します
    return {
        "flesch_kincaid_grade": textstat.flesch_kincaid_grade(text),  # Flesch-Kincaid グレードを計算
        "gunning_fog": textstat.gunning_fog(text),  # Gunning-Fog インデックスを計算
        "smog_index": textstat.smog_index(text),  # SMOGインデックスを計算
        "ari": textstat.automated_readability_index(text),  # 自動可読性インデックスを計算
        "coleman_liau_index": textstat.coleman_liau_index(text)  # Coleman-Liau インデックスを計算
    }

def count_noun_phrases(text):
    blob = TextBlob(text)  # TextBlobオブジェクトを作成
    return len(blob.noun_phrases)  # 名詞句の数を返す

def analyze_sentiment(text):
    blob = TextBlob(text)  # TextBlobオブジェクトを作成
    return blob.sentiment.polarity  # テキストの感情極性を返す

def count_passive_voice(text):
    doc = nlp(text)  # SpaCyを使用してテキストを処理
    return sum(1 for token in doc if token.dep_ == 'auxpass')  # 受動態の文の数をカウント

def pos_tag_frequencies(text):
    words = word_tokenize(text)  # テキストを単語にトークン化
    tags = nltk.pos_tag(words)  # 各単語に品詞タグを付与
    freq_dist = nltk.FreqDist(tag for (word, tag) in tags)  # 品詞の頻度分布を計算
    # 一貫した辞書形式で頻度を格納することを保証
    return {tag: freq for tag, freq in freq_dist.items()}

def text_statistics(text):
    stats = calculate_readability_scores(text)  # 可読性スコアの計算
    stats.update({
        "word_count": len(word_tokenize(text)),  # 単語数を計算
        "char_count": len(text),  # 文字数を計算
        "sentence_count": len(sent_tokenize(text)),  # 文数を計算
        "avg_word_length": sum(len(word) for word in word_tokenize(text)) / len(word_tokenize(text)),  # 平均単語長を計算
        "avg_sentence_length": sum(len(sent) for sent in sent_tokenize(text)) / len(sent_tokenize(text)),  # 平均文長を計算
        "lexical_diversity": len(set(word_tokenize(text))) / len(word_tokenize(text)),  # 語彙の多様性を計算
        "noun_phrases_count": count_noun_phrases(text),  # 名詞句の数を計算
        "sentiment": analyze_sentiment(text),  # 感情分析を実行
        "passive_voice_count": count_passive_voice(text),  # 受動態のカウントを実行
    })
    # POSタグの頻度を主な統計辞書にマージ
    pos_tags = pos_tag_frequencies(text)
    for tag, count in pos_tags.items():
        stats[f'pos_tag_{tag}'] = count  # 各品詞の頻度を辞書に追加
    return stats

def parallel_apply(df, column):
    # NaN値をドロップしてエラーを回避
    texts = df[column].dropna()  # NaNを除外したテキストのリストを作成

    # ProcessPoolExecutorを使用して並列に関数を適用
    with concurrent.futures.ProcessPoolExecutor() as executor:
        results = list(executor.map(text_statistics, texts))  # 並列処理でテキスト統計を計算

    # 辞書のリストをDataFrameに変換
    results_df = pd.DataFrame(results)

    # 欠損POSタグを0で埋め、データ型を適切に変換する処理を自動的に行う
    results_df.fillna(0, inplace=True)  # NaNを0で埋める
    for col in results_df.columns:
        if results_df[col].dtype == float:
            results_df[col] = results_df[col].astype(int)  # float型の列をint型に変換

    return results_df
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータを読み込む
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # 提出サンプルを読み込む
print('データがインポートされました')  # データインポート完了のメッセージ
```

---The following area is a Code cell (cell numver is 8)---
```python
train.shape  # トレーニングデータの形状を表示
```

---The following area is a Code cell (cell numver is 9)---
```python
# def parallel_apply(df, column):
#     with concurrent.futures.ProcessPoolExecutor() as executor:
#         results = list(executor.map(text_statistics, df[column].dropna()))  # NaNを処理するためにdropnaを使用
#     return pd.DataFrame(results)  # DataFrameを返す関数（コメントアウト）
```

---The following area is a Code cell (cell numver is 10)---
```python
# 'prompt'および'response'列に対してparallel_applyを適用
nlp = spacy.load('en_core_web_sm')  # SpaCyの英語モデルをロード
prompt_stats_df = parallel_apply(train, 'prompt')  # プロンプトの統計を計算
response_a_stats_df = parallel_apply(train, 'response_a')  # レスポンスAの統計を計算
response_b_stats_df = parallel_apply(train, 'response_b')  # レスポンスBの統計を計算
```

---The following area is a Code cell (cell numver is 11)---
```python
train = train.join(prompt_stats_df.add_suffix('_prompt'))  # プロンプトの統計をトレーニングデータに結合
train = train.join(response_a_stats_df.add_suffix('_response_a'))  # レスポンスAの統計をトレーニングデータに結合
train = train.join(response_b_stats_df.add_suffix('_response_b'))  # レスポンスBの統計をトレーニングデータに結合
```

---The following area is a Code cell (cell numver is 12)---
```python
train.shape  # 結合後のトレーニングデータの形状を表示
```

---The following area is a Code cell (cell numver is 13)---
```python
train.head()  # トレーニングデータの最初の数行を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
%%time

import time  # 時間計測のためのモジュールをインポート
from sklearn.ensemble import GradientBoostingClassifier
from xgboost import XGBClassifier
from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.metrics import log_loss
from scipy.stats import uniform, randint

# ターゲットを単一の列に変換し、カテゴリーベースにします
train['winner'] = (train['winner_model_a'] * 1 + train['winner_model_b'] * 2 + train['winner_tie'] * 3).astype(int)

# 特徴量とターゲットの定義
columns_to_remove = {'id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 
                     'winner_model_a', 'winner_model_b', 'winner_tie', 'winner'}

features = [col for col in train.columns if col not in columns_to_remove]  # 残す特徴量をリストアップ

X = train[features]  # 特徴量データ
y = train['winner'] - 1  # ターゲットデータ（0から始まるインデックスに変換）
```

---The following area is a Code cell (cell numver is 15)---
```python
# 最適化関数の定義
def objective(trial):
    # トライアル内でのデータ分割
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    # 最適化するモデルとハイパーパラメータの定義
    model_type = trial.suggest_categorical('model_type', ['XGBClassifier', 'LGBMClassifier', 'CatBoostClassifier'])
    n_estimators = trial.suggest_int('n_estimators', 100, 500)  # 決定木の数を指定
    max_depth = trial.suggest_int('max_depth', 2, 10)  # 木の深さを指定
    learning_rate = trial.suggest_loguniform('learning_rate', 1e-4, 0.1)  # 学習率を指定

    if model_type == 'XGBClassifier':
        model = XGBClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, use_label_encoder=False, eval_metric='logloss', random_state=42)
    elif model_type == 'LGBMClassifier':
        model = LGBMClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, random_state=42)
    elif model_type == 'CatBoostClassifier':
        model = CatBoostClassifier(n_estimators=n_estimators, max_depth=max_depth, learning_rate=learning_rate, verbose=0, random_state=42)

    # モデルの訓練と評価
    model.fit(X_train, y_train)  # モデルを訓練
    y_val_pred = model.predict_proba(X_val)  # バリデーションデータで予測
    return log_loss(y_val, y_val_pred)  # ログ損失を返す

# Optuna最適化の実行
study = optuna.create_study(direction='minimize')  # 最小化方向でスタディを作成
study.optimize(objective, n_trials=5)  # トライアルを実行

print('最良のトライアル:', study.best_trial.params)  # 最良のパラメータを表示

# フルデータで最良モデルを訓練
best_params = study.best_trial.params  # 最良のパラメータを取得
model_type = best_params.pop('model_type')  # モデルタイプを取得し、辞書から削除

if model_type == 'XGBClassifier':
    final_model = XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss', random_state=42)
elif model_type == 'LGBMClassifier':
    final_model = LGBMClassifier(**best_params, random_state=42)
elif model_type == 'CatBoostClassifier':
    final_model = CatBoostClassifier(**best_params, verbose=0, random_state=42)

final_model.fit(X, y)  # フルデータセットで訓練
```

---The following area is a Code cell (cell numver is 16)---
```python
final_model  # 最終モデルを表示
```

---The following area is a Code cell (cell numver is 17)---
```python
# 'prompt'および'response'列に対してparallel_applyを再度適用
prompt_stats_df_test = parallel_apply(test, 'prompt')  # テストデータのプロンプト統計を計算
response_a_stats_df_test = parallel_apply(test, 'response_a')  # テストデータのレスポンスA統計を計算
response_b_stats_df_test = parallel_apply(test, 'response_b')  # テストデータのレスポンスB統計を計算
```

---The following area is a Code cell (cell numver is 18)---
```python
test = test.join(prompt_stats_df_test.add_suffix('_prompt'))  # プロンプトの統計をテストデータに結合
test = test.join(response_a_stats_df_test.add_suffix('_response_a'))  # レスポンスAの統計をテストデータに結合
test = test.join(response_b_stats_df_test.add_suffix('_response_b'))  # レスポンスBの統計をテストデータに結合
```

---The following area is a Code cell (cell numver is 19)---
```python
test = test[features]  # テストデータから特徴量のみ抽出
test  # テストデータを表示
```

---The following area is a Code cell (cell numver is 20)---
```python
train.head()  # トレーニングデータの最初の数行を表示
```

---The following area is a Code cell (cell numver is 21)---
```python
test_predictions = final_model.predict_proba(test)  # テストデータに対する予測確率を計算
```

---The following area is a Code cell (cell numver is 22)---
```python
test_predictions  # テストデータの予測確率を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
test_raw = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv', usecols=['id'])  # テストデータのIDを読み込む
```

---The following area is a Code cell (cell numver is 24)---
```python
# 提出ファイルの準備
submission = pd.DataFrame({
    'id': test_raw['id'],  # ID列
    'winner_model_a': test_predictions[:, 0],  # モデルAの勝者予測
    'winner_model_b': test_predictions[:, 1],  # モデルBの勝者予測
    'winner_tie': test_predictions[:, 2]  # 引き分けの勝者予測
})

submission.head()  # 提出ファイルの最初の数行を表示
```

---The following area is a Code cell (cell numver is 25)---
```python
submission.to_csv('/kaggle/working/submission.csv', index=False)  # 提出ファイルをCSV形式で保存
```

** @@@ Jupyter Notebook numver 50, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、ユーザーに対する大規模言語モデル（LLM）の応答の好みを予測するための機械学習モデルを構築することを目的としています。主に、LightGBMとTF-IDF（Term Frequency-Inverse Document Frequency）を用いるアプローチが採用されています。

### 問題に取り組む内容
ノートブックでは、ユーザーからのプロンプトに対する二つのLLMの応答の中で、どちらが好まれるかを予測するためのタスクが中心です。具体的には、次のステップが含まれています。

1. **データの読み込みと探索的データ分析（EDA）**: データを読み込んで、基本的な統計情報や分布を確認します。
2. **TF-IDFベクトル化**: テキストデータを数値フォーマットに変換するためにTF-IDFを用います。この手法は、重要な単語を特定し、文書の類似性を測定するために使われます。
3. **データの前処理**: 各応答の情報を含む特徴量を追加し、類似性や距離を計算します。
4. **モデルの訓練**: LightGBMを用いてデータを訓練し、交差検証を行いながら性能を評価します。
5. **モデルの推論**: テストデータに対して予測を行い、結果を元に混同行列を作成し評価します。
6. **提出用ファイルの生成**: 予測結果を適切なフォーマットでCSVファイルとして保存します。

### 使用している主な手法とライブラリ
- **LightGBM**: 勾配ブースティングフレームワークを使用してモデルを訓練します。これは大容量のデータセットに対して効率的かつ効果的に学習を行えるライブラリです。
- **TF-IDF**: テキストデータを数値ベクトルに変換するために使用されています。具体的には、単語の重要性を測定するための手法です。
- **Label Encoding**: 複数のラベルを単一のラベルに統合するためのカスタム関数が定義されています。
- **混同行列**: モデルのパフォーマンスを評価するために使用されています。

全体として、このノートブックはチャットボットの応答の好みを予測するために高度な機械学習を実装しており、ユーザーのニーズに対してより優れた応答を生成するための強力なフレームワークを提供しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# イントロダクション 📜

✔️ このノートブックの目的は何ですか？

目標は、LightGBMとTF-IDFベクトル化を使用して、ユーザーのLLM応答への好みを予測するための堅牢で効率的なソリューションを作成することです。

---

✔️ このノートブックでは何が扱われていますか？

- `データの読み込みと探索的データ分析（EDA）`

- `TF-IDFの理論`

- `データの前処理`

- `モデルの訓練`

- `モデルの推論`
     
# インポート 📦
```

---The following area is a Code cell (cell numver is 2)---
```python
# 警告メッセージを処理する
import warnings
# 警告を無視する設定を行います
warnings.filterwarnings('ignore')  # これにより、実行時に表示される警告が表示されなくなります
```

---The following area is a Code cell (cell numver is 3)---
```python
# データの前処理
import numpy as np  # NumPyライブラリをインポート。数値計算や配列操作に使います。
import pandas as pd  # pandasライブラリをインポート。データフレーム作成や操作に使用します。
from pathlib import Path  # パス操作用のPathライブラリをインポート

# データの視覚化
import plotly.graph_objects as go  # Plotlyライブラリをインポート。インタラクティブなグラフ作成に使用します。
from sklearn.metrics import confusion_matrix  # 混同行列の計算用のメトリクスをインポート

# モデルの開発
import lightgbm as lgb  # LightGBMライブラリをインポート。勾配ブースティングを用いたモデル作成に使用します。
from sklearn.model_selection import StratifiedKFold  # 層化Kフォールド交差検証用のクラスをインポート

# TF-IDFベクトル化
from sklearn.decomposition import TruncatedSVD  # 次元削減用のトランケイテッドSVDをインポート
from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFベクトル化のためのクラスをインポート

# TF-IDFベクトルの類似性/距離特徴
from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, laplacian_kernel  # コサイン類似度、ユークリッド距離、ラプラシアンカーネルの計算用メトリクスをインポート
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 設定 ⚙️
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:
    # コンペティションデータへのパス
    train_data = Path("/kaggle/input/lmsys-chatbot-arena/train.csv")  # トレーニングデータのパス
    test_data = Path("/kaggle/input/lmsys-chatbot-arena/test.csv")    # テストデータのパス
    subm_data = Path("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")  # 提出ファイルのサンプルパス
    
    # 混同行列のカラースケール
    colorscale = "peach"  # 混同行列を表示する際のカラースケール設定
    
    # TF-IDFベクトル化のパラメータ
    components = 32  # 次元削減で保持する成分の数
    ngrams = (1, 7)  # 使用するn-gramの範囲
    max_freq = 0.95  # 95%以上の文書に出現する単語は除外
    min_freq = 10    # 10文書未満に出現する単語は除外
    
    # トレーニングの引数
    num_classes = 3   # 分類するクラスの数
    early_stop = 50   # 早期終了のためのイテレーション数
    log_steps = 100   # ログを記録するステップ数
    
    # LightGBMのパラメータ
    params = {
        "objective": "multiclass",  # マルチクラス分類を指定
        "colsample_bytree": 0.8,     # 木のサンプリング比率
        "colsample_bynode": 0.8,      # ノードでのサンプリング比率
        "metric": "multiclass",      # 評価指標をマルチクラスに設定
        "learning_rate": 0.02,        # 学習率
        "extra_trees": True,          # 追加の木を使用するかどうか
        "num_rounds": 3000,           # 学習のラウンド数
        "reg_lambda": 1.3,            # L2正則化
        "num_classes": 3,             # 分類するクラス数
        "num_leaves": 64,             # 葉の数
        "reg_alpha": 0.1,             # L1正則化
        "device": "cpu",              # デバイスはCPUを指定
        "max_depth": 6,               # 木の最大深さ
        "max_bin": 128,               # 最大ビンの数
        "verbose": -1,                # 出力の詳細度設定
        "seed": 42                    # 再現性のためのシード値
    }
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# 探索的データ分析（EDA） 🗃️
```

---The following area is a Code cell (cell numver is 7)---
```python
class EDA:
    def read_data(self, path):
        # 指定したパスからデータフレームを読み込み
        df = pd.read_csv(path)
        
        # データフレームの形状と最初の3行を表示
        print(f"データフレームの形状は: {df.shape}です")
        display(df.head(3))  # データの最初の3行を表示
        
        return df  # 読み込んだデータフレームを返す
    
    def pie_chart(self, data):
        # 各勝者の列のカウントを計算
        counts = {
            'winner_model_a': data['winner_model_a'].sum(),  # モデルAの勝ち数
            'winner_model_b': data['winner_model_b'].sum(),  # モデルBの勝ち数
            'winner_tie': data['winner_tie'].sum()           # 引き分けの数
        }

        # カラーを定義
        colors = ['#a89192', '#8083a8', '#a8c28c']  # クリーム、ライトブルー、ミント
        identifiers = ['Creme', 'Light Blue', 'Mint']  # 各色の識別子
        
        # 円グラフを作成
        fig = go.Figure(data=[go.Pie(labels=identifiers, 
                                     values=list(counts.values()), 
                                     textinfo='percent', 
                                     hole=0.1,
                                     marker=dict(colors=colors, line=dict(color='#FFFFFF')))])
        
        # 背景を透明にし、円グラフを左寄せにするためにレイアウトを更新
        fig.update_layout(plot_bgcolor='rgba(0,0,0,0)', 
                          paper_bgcolor='rgba(0,0,0,0)', 
                          margin=dict(l=0, r=0, t=0, b=0))
        
        # 凡例を非表示にする
        fig.update_layout(showlegend=False)
        
        # プロットを表示
        fig.show()

        # カウントをテーブルとして表示
        counts_df = pd.DataFrame(list(counts.items()), columns=['クラス', 'カウント'])
        counts_df['識別子'] = identifiers
        display(counts_df)  # カウントテーブルを表示
        
    def response_length(self, data):
        # 元のデータを変更しないためにデータフレームのコピーを作成
        data_copy = data.copy()
        
        # 各応答の単語数を計算
        data_copy['word_count_a'] = data_copy['response_a'].apply(lambda x: len(str(x).split()))  # モデルAの単語数
        data_copy['word_count_b'] = data_copy['response_b'].apply(lambda x: len(str(x).split()))  # モデルBの単語数
        
        # 各勝者クラスの平均単語数を計算
        word_counts = {
            'winner_model_a': int(
                data_copy[data_copy['winner_model_a'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()  # モデルAの平均単語数
            ),
            
            'winner_model_b': int(
                data_copy[data_copy['winner_model_b'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()  # モデルBの平均単語数
            ),
            
            'winner_tie': int(
                data_copy[data_copy['winner_tie'] == 1][
                    ['word_count_a', 
                     'word_count_b']
                ].mean().mean()  # 引き分けの平均単語数
            )
        }
        
        # カスタムホバーテキストを作成
        hover_texts = [f"単語数: {value}<br>{key}" for key, value in word_counts.items()]
        
        # 棒グラフを作成
        fig = go.Figure(data=[go.Bar(
            x=list(word_counts.keys()),  # 勝者クラスラベルをx軸に
            y=list(word_counts.values()),  # 単語数をy軸に
            marker=dict(color=['#a89192', '#8083a8', '#a8c28c']),
            hovertext=hover_texts,  # ホバー時のテキスト
            hoverinfo='text',
            orientation='v'  # 棒を縦にする
        )])
        
        # レイアウトを更新
        fig.update_layout(
            title='勝者クラスによる平均応答単語数',
            xaxis_title='',
            yaxis_title='平均応答単語数',
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis=dict(showticklabels=False)  # x軸のラベルを非表示
        )
        
        # プロットを表示
        fig.show()
```

---The following area is a Code cell (cell numver is 8)---
```python
eda = EDA()  # EDAクラスのインスタンスを作成し、探索的データ分析を実行できるようにします。
```

---The following area is a Code cell (cell numver is 9)---
```python
train_data = eda.read_data(CFG.train_data)  # 設定されたパスからトレーニングデータを読み込み、データフレームを取得します。
```

---The following area is a Code cell (cell numver is 10)---
```python
test_data = eda.read_data(CFG.test_data)  # 設定されたパスからテストデータを読み込み、データフレームを取得します。
```

---The following area is a Code cell (cell numver is 11)---
```python
subm_data = eda.read_data(CFG.subm_data)  # 設定されたパスから提出データのサンプルを読み込み、データフレームを取得します。
```

---The following area is a Code cell (cell numver is 12)---
```python
print("クラスの分布（勝者）:")  # 勝者のクラス分布を表示するメッセージ
eda.pie_chart(train_data)  # トレーニングデータを使ってクラス分布の円グラフを作成し表示します。
```

---The following area is a Code cell (cell numver is 13)---
```python
# 勝者モデルごとの平均応答単語数をプロットする
eda.response_length(train_data)  # トレーニングデータを使用して、各勝者モデルの応答の平均単語数をプロットします。
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 理論 📒

✔️ **単語頻度 - 逆文書頻度**、または **TF-IDF**ベクトル化は、テキストマイニングや情報検索で使用され、文書内の単語の重要性をコーパスに対して評価します。この技術は、テキストデータを機械学習アルゴリズムに適した数値フォーマットに変換します。

---

✔️ **TF-IDFの構成要素**

1. 単語頻度 (TF):

   - *定義:* 文書内の特定単語の出現頻度を測定します。
   
   - *式:* $ \text{TF}(t,d) = \frac{f_{t,d}}{\sum\limits_{t' \in d} f_{t',d}} $ , ここで $ f_{t,d} $ は文書 $ d $ 内の単語 $ t $ の出現頻度です。

2. 逆文書頻度 (IDF):

   - *定義:* コーパス全体における特定単語の重要性を測定します。
   
   - *式:* $ \text{IDF}(t) = \log \left( \frac{N}{1 + n_t} \right) $ , ここで $ N $ は文書の総数、$ n_t $ は単語 $ t $ を含む文書の数です。

3. TF-IDFスコア:

   - *定義:* TFとIDFスコアの積です。
   
   - *式:* $ \text{TF-IDF}(t,d) = \text{TF}(t,d) \times \text{IDF}(t) $
   
---

✔️ ***N-grams* の説明**

*N-grams* は、テキスト文書から抽出される連続した $ n $ 項目（トークン）のシーケンスです。これにより、個々の単語に比べて言語構造と文脈のより包括的な表現が得られます。

*式:* $ N\text{-grams} = [t_1, t_2, ..., t_n] $

*例:* `ngrams = (1, 3)` の場合、テキスト文書内の長さ3のスライディングウィンドウで可能な全てのトークンの組み合わせを考慮します。各3トークンの組み合わせはトライグラムを表します。

例えば、「I love coding.」という文を考えます。

`ngrams = (1, 3)`の場合、この文から抽出されるn-gramsには以下が含まれます：

   * ユニグラム (1-grams): ["I"], ["love"], ["coding"]
    
   * バイグラム (2-grams): ["I love"], ["love coding"]
    
   * トリグラム (3-grams): ["I love coding"]

このように、$ N-grams $ は個々の単語だけでなく、フレーズや文中のコンテキスト情報も捉えます。

---
   
✔️ **TF-IDFのステップ**

1. トークン化:

   - *定義:* テキストをトークンに分けます。
   
   - *例:* "I love coding" -> ["I", "love", "coding"]

2. 文書頻度の計算:

   - *定義:* 各単語を含む文書の数をカウントします。
   
   - *例:* "love" は1つの文書に出現します。

3. TF-IDFの計算:

   - *定義:* 各文書内の各単語のTF-IDFスコアを計算します。
   
   - *例:* ngrams = (1, 3) の場合、"love"は文書1に出現し、そのTFとIDFに基づいてTF-IDFスコアが計算されます。

4. ベクトル化:

   - *定義:* 各文書をTF-IDFスコアのベクトルとして表現します。
   
   - *例:* 各文書は、高次元ベクトルになり、各次元はユニークな単語またはn-gramに対応します。

# データ前処理 🛠️
```

---The following area is a Code cell (cell numver is 15)---
```python
class DataPreprocessing:
    # 入力リスト内にNoneが存在するかチェック
    @staticmethod
    def retrieve_none(vals):
        return int(any(val is None for val in vals))  # 1つでもNoneがあれば1を返す

    # 入力リスト内の文字列の合計長を計算
    @staticmethod
    def retrieve_length(vals):
        length = 0
        for val in vals:
            if isinstance(val, str):  # valが文字列であれば
                length += len(val)  # その長さを加算
        return length
    
    # 入力リスト内のユニークな単語のカウントを計算
    @staticmethod
    def retrieve_nuniques(vals):
        if isinstance(vals, str):  # valsが文字列の場合
            return len(set(vals.split()))  # ユニークな単語の数をカウントして返す
        return 0
    
    # リスト内の'None'を'STR'に置き換え、要素をスペースで結合
    @staticmethod
    def clean_response(text):
        if isinstance(text, list):  # textがリストの場合
            cleaned_text = ' '.join([str(item) if item is not None else 'NONE' for item in text])  # Noneを'STR'に置き換え
            return cleaned_text

        return text  # それ以外は元のtextを返す

    def add_features(self, data):
        # 応答列の長さやNoneの有無に関連する特徴を追加
        data[f"response_a_len"] = data[f"response_a"].apply(self.retrieve_length)  # 応答Aの長さ
        data[f"response_b_len"] = data[f"response_b"].apply(self.retrieve_length)  # 応答Bの長さ

        # 応答のユニークな単語数を計算
        data[f"response_a_unique"] = data[f"response_a"].apply(self.retrieve_nuniques)  # 応答Aのユニーク単語数
        data[f"response_b_unique"] = data[f"response_b"].apply(self.retrieve_nuniques)  # 応答Bのユニーク単語数

        # 長さの差、平均長さ、長さ差比を計算
        data["response_len_diff"] = data["response_a_len"] - data["response_b_len"]  # 長さの差
        data["response_len_mean"] = (data["response_a_len"] + data["response_b_len"]) / 2  # 平均長さ
        data["response_diff_ratio"] = data["response_len_diff"] / data["response_len_mean"]  # 長さ差比

        # ユニーク単語数の差、平均、および比を計算
        data["response_unique_diff"] = data["response_a_unique"] - data["response_b_unique"]  # ユニーク単語数の差
        data["response_unique_mean"] = (data["response_a_unique"] + 
                                        data["response_b_unique"]) / 2  # ユニーク単語数の平均
        data["response_unique_ratio"] = (data["response_unique_diff"] / 
                                         data["response_unique_mean"])  # ユニーク単語数差比

        # 応答列内にNoneが含まれているかをチェック
        data["a_has_none"] = data["response_a"].apply(self.retrieve_none)  # 応答AにNoneがあるか
        data["b_has_none"] = data["response_b"].apply(self.retrieve_none)  # 応答BにNoneがあるか
        data["has_none_diff"] = data["a_has_none"] - data["b_has_none"]  # Noneの差

        return data  # 加工したデータを返す
    
    # プロンプトと応答間のコサイン類似度を計算
    @staticmethod
    def calculate_cosine_similarity(tfidf_matrix, 
                                    prompt_idx, 
                                    response_a_idx, 
                                    response_b_idx):
        
        # プロンプト（p）と応答A（a）のコサイン類似度
        similarity_pa = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]

        # プロンプト（p）と応答B（b）のコサイン類似度
        similarity_pb = cosine_similarity(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]

        return similarity_pa, similarity_pb  # 類似度を返す

    # プロンプトと応答間の距離（ユークリッド/ラプラシアン）を計算
    @staticmethod
    def calculate_distances(tfidf_matrix, 
                            prompt_idx, 
                            response_a_idx, 
                            response_b_idx, 
                            distance_metric):
        
        # プロンプト（p）と応答A（a）の距離
        distance_pa = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1), 
                tfidf_matrix[response_a_idx].reshape(1, -1)
        )[0][0]
        
        # プロンプト（p）と応答B（b）の距離
        distance_pb = distance_metric(
                tfidf_matrix[prompt_idx].reshape(1, -1),
                tfidf_matrix[response_b_idx].reshape(1, -1)
        )[0][0]
        
        return distance_pa, distance_pb  # 距離を返す

    def create_tfidf_features(self, train, test, ngrams, min_freq, max_freq, components):
        # TF-IDFベクトルライザを初期化
        tfidf_vectorizer = TfidfVectorizer(analyzer='char', 
                                           ngram_range=ngrams, 
                                           min_df=min_freq, 
                                           max_df=max_freq,
                                           lowercase=False,
                                           sublinear_tf=True)

        # トレーニングデータとテストデータを単一のデータフレームに結合
        full_data = pd.concat([train, test], ignore_index=True)

        # テキスト列をクリーンアップして準備
        for col in ['prompt', 'response_a', 'response_b']:
            full_data[col] = full_data[col].apply(self.clean_response)

        # TF-IDFベクトル化のためにすべてのテキスト列を結合
        full_corpus = pd.concat([full_data['prompt'], 
                                 full_data['response_a'], 
                                 full_data['response_b']], 
                                 ignore_index=True)

        # TF-IDFマトリックスを計算
        tfidf_matrix = tfidf_vectorizer.fit_transform(full_corpus)

        # 次元削減をトランケイテッドSVDで実施
        svd = TruncatedSVD(n_components=components, random_state=42)
        reduced_matrix = svd.fit_transform(tfidf_matrix)

        # コーパスの異なる部分を分割するためのインデックスを計算
        len_full = len(full_data)
        split_index_01 = len_full
        split_index_02 = len_full * 2

        # 短縮マトリックスをプロンプト、応答A、および応答B部分に分割
        full_tfidf_prompts = reduced_matrix[:split_index_01]
        full_tfidf_response_a = reduced_matrix[split_index_01:split_index_02]
        full_tfidf_response_b = reduced_matrix[split_index_02:]

        # 短縮マトリックスをトレーニングセットとテストセットに分割
        len_train = len(train)
        train_tfidf_prompts = full_tfidf_prompts[:len_train]
        train_tfidf_response_a = full_tfidf_response_a[:len_train]
        train_tfidf_response_b = full_tfidf_response_b[:len_train]
        test_tfidf_prompts = full_tfidf_prompts[len_train:]
        test_tfidf_response_a = full_tfidf_response_a[len_train:]
        test_tfidf_response_b = full_tfidf_response_b[len_train:]

        # トレーニングおよびテストセットのSVD特徴を保持するためのデータフレームを作成
        feature_names = [f'svd_feature_{i}' for i in range(components)]
        train_features = pd.DataFrame(index=train.index)
        test_features = pd.DataFrame(index=test.index)

        # SVD特徴を特徴データフレームの対応する列に割り当て
        for i in range(components):
            train_features[f'svd_prompts_{i}'] = train_tfidf_prompts[:, i]
            train_features[f'svd_response_a_{i}'] = train_tfidf_response_a[:, i]
            train_features[f'svd_response_b_{i}'] = train_tfidf_response_b[:, i]
            test_features[f'svd_prompts_{i}'] = test_tfidf_prompts[:, i]
            test_features[f'svd_response_a_{i}'] = test_tfidf_response_a[:, i]
            test_features[f'svd_response_b_{i}'] = test_tfidf_response_b[:, i]

        # 新しい特徴を元のトレーニングおよびテストデータフレームと連結
        train = pd.concat([train, train_features], axis=1)
        test = pd.concat([test, test_features], axis=1)

        # 類似度と距離の特徴を計算
        for df, len_df in zip([train, test], [len(train), len(test)]):
            prompt_indices = df.index

            # コサイン類似度の特徴を計算
            df['similarity_pa'], df['similarity_pb'] = zip(*[
                self.calculate_cosine_similarity(reduced_matrix, i, i + len_df, i + 2 * len_df)
                for i in prompt_indices
            ])

            # ユークリッド距離の特徴を計算
            df['euclidean_pa'], df['euclidean_pb'] = zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         euclidean_distances)
                for i in prompt_indices
            ])

            # ラプラシアンカーネル距離の特徴を計算
            df['laplacian_pa'], df['laplacian_pb']= zip(*[
                self.calculate_distances(reduced_matrix, i, i + len_df, i + 2 * len_df, 
                                         laplacian_kernel)
                for i in prompt_indices
            ])

        return train, test  # 加工したトレーニングデータとテストデータを返す
    
    # 複数のラベルを単一のラベルにマージ
    def merge_label(self, row):
        if row["winner_model_a"] == 1:
            return 0  # モデルAが勝者の場合
        if row["winner_model_b"] == 1:
            return 1  # モデルBが勝者の場合
        if row["winner_tie"] == 1:
            return 2  # 引き分けの場合
        raise ValueError("値が無効です。")  # 無効な値の場合エラーを発生
```

---The following area is a Code cell (cell numver is 16)---
```python
dp = DataPreprocessing()  # データ前処理クラスのインスタンスを作成し、データの前処理を実行できるようにします。
```

---The following area is a Code cell (cell numver is 17)---
```python
# 長さ、類似度、および距離の特徴を追加する
train_data = dp.add_features(train_data)  # トレーニングデータに特徴を追加
test_data = dp.add_features(test_data)    # テストデータに特徴を追加
```

---The following area is a Code cell (cell numver is 18)---
```python
# TF-IDF特徴を抽出し、次元削減を実施する
train_data, test_data = dp.create_tfidf_features(train_data,  # トレーニングデータを使用
                                                 test_data,   # テストデータを使用
                                                 CFG.ngrams,  # n-gramの設定
                                                 CFG.min_freq,  # 最小頻度の設定
                                                 CFG.max_freq,  # 最大頻度の設定
                                                 CFG.components)  # コンポーネントの数の設定
```

---The following area is a Code cell (cell numver is 19)---
```python
# 複数のラベルを単一のラベルにマージする
train_data["target"] = train_data[  # ターゲット列を作成
    ["winner_model_a", "winner_model_b", "winner_tie"]
].apply(lambda x: dp.merge_label(x), axis=1)  # 各行にmerge_label関数を適用してターゲットを設定
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# モデル開発 🧠
```

---The following area is a Code cell (cell numver is 21)---
```python
class ModelDevelopment:
    def train_lgb(self, train_data, test_data, feature_cols, params, early_stop, log_steps):
        # トレーニングデータとテストデータから特徴量とターゲットラベルを抽出
        X_train = train_data[feature_cols].values  # 特徴量の行列
        X_test = test_data[feature_cols].values    # テスト用特徴量
        Y_train = train_data["target"]              # ターゲットラベル

        # 予測を保存するリスト
        train_preds_list = []
        test_preds_list = []

        # StratifiedKFoldを初期化
        cv = StratifiedKFold(n_splits=5, random_state=42, shuffle=True)
        for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train, Y_train)):
            # 現在のフォールドのためにトレーニングデータをトレーニングセットとバリデーションセットに分割
            x_train, x_valid = X_train[train_index], X_train[valid_index]
            y_train, y_valid = Y_train[train_index], Y_train[valid_index]

            # トレーニングとバリデーションのためのLightGBMデータセットオブジェクトを作成
            train = lgb.Dataset(x_train, y_train)
            valid = lgb.Dataset(x_valid, y_valid, reference=train)

            # 現在のフォールドでモデルを訓練
            model = lgb.train(
                params,
                train,
                valid_sets=[train, valid],
                feature_name=feature_cols,
                callbacks=[lgb.early_stopping(early_stop),
                           lgb.log_evaluation(log_steps)])

            # トレーニングセットとテストセットに対して予測を行う
            train_preds = model.predict(X_train)  # トレーニングデータの予測
            test_preds = model.predict(X_test)    # テストデータの予測

            train_preds_list.append(train_preds)  # トレーニング予測をリストに追加
            test_preds_list.append(test_preds)    # テスト予測をリストに追加

        # 予測の平均を計算
        train_preds = np.mean(train_preds_list, axis=0)  # トレーニング予測の平均
        test_preds = np.mean(test_preds_list, axis=0)    # テスト予測の平均

        return train_preds, test_preds  # 予測を返す
    
    # トレーニングデータ予測用の混同行列をプロット
    def plot_cm(self, y_true, y_pred, labels, colorscale):
        cm = confusion_matrix(y_true, y_pred, labels=labels)  # 混同行列を計算

        # カスタムホバーテキストフォーマッタを作成
        def format_hover_text(value):
            if value >= 10000:
                return str(int(value))  # 整数値に変換してカンマなしで返す
            else:
                return str(value)

        # ヒートマップを作成
        fig = go.Figure(data=go.Heatmap(
            z=cm,
            x=labels,
            y=labels,
            colorscale=colorscale,
            zmin=0,
            zmax=20000,
            text=cm,
            texttemplate="%{text:.0f}",
            hovertemplate="真実: %{y}<br>予測: %{x}<br>カウント: %{z:,.0f}<extra></extra>",
            customdata=[format_hover_text(value) for value in cm.flatten()]
        ))

        # 背景を透明にし、正方形のアスペクト比に設定するためにレイアウトを更新
        fig.update_layout(
            plot_bgcolor='rgba(0,0,0,0)',
            paper_bgcolor='rgba(0,0,0,0)',
            xaxis_title="予測ラベル",
            yaxis_title="真実のラベル",
            xaxis=dict(constrain='domain'),
            yaxis=dict(constrain='domain', scaleanchor='x'),
            width=650,  
            height=650,  
            margin=dict(t=65, b=65, l=65, r=65) 
        )

        # プロットを表示
        fig.show()
```

---The following area is a Code cell (cell numver is 22)---
```python
md = ModelDevelopment()  # モデル開発クラスのインスタンスを作成し、モデルの訓練や評価を実行できるようにします。
```

---The following area is a Code cell (cell numver is 23)---
```python
# ラベル列を定義
label_cols = ["winner_model_a", "winner_model_b", "winner_tie"]

# トレーニングデータから除外する特徴量のリストを定義
excluded_features = ['id', 
                     'model_a', 
                     'model_b', 
                     'prompt', 
                     'response_a', 
                     'response_b',
                     'winner_model_a', 
                     'winner_model_b', 
                     'winner_tie', 
                     'target', 
                     'fold_id']

# 除外リストに含まれない列を特徴量としてリスト化
features = [col for col in train_data.columns if col not in excluded_features]  # 使用する特徴量のリストを作成
```

---The following area is a Code cell (cell numver is 24)---
```python
# LightGBMを訓練する
train_preds, test_preds = md.train_lgb(  # トレーニングデータとテストデータを用いてLightGBMを訓練
    train_data, 
    test_data, 
    features,  # 使用する特徴量
    CFG.params,  # モデルパラメータ
    CFG.early_stop,  # 早期終了のための基準
    CFG.log_steps  # ログステップの設定
)
```

---The following area is a Code cell (cell numver is 25)---
```python
# トレーニングデータに対する（平均）予測の混同行列
md.plot_cm(train_data['target'], np.argmax(train_preds, axis=1), [0, 1, 2], CFG.colorscale)  # ターゲットと予測ラベルを用いて混同行列をプロット
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# 予測を提出する 💡
```

---The following area is a Code cell (cell numver is 27)---
```python
# 提出用データフレームに予測したテストラベルを割り当て
subm_data[label_cols] = test_preds  # テストデータの予測結果を提出データフレームに追加

# 提出データフレームを保存し、最初の3行を表示
subm_data.to_csv("submission.csv", index=False)  # 提出ファイルをCSV形式で保存
display(subm_data.head(3))  # 提出データフレームの最初の3行を表示
```

** @@@ Jupyter Notebook numver 51, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena Human Preference Predictions」コンペティションにおいて、ユーザーが好むチャットボットの応答を予測するための機械学習モデルの構築に取り組んでいます。具体的には、与えられたプロンプトに対して生成された複数の応答の中から、どの応答が好まれるかを予測するモデルを構築しています。

### 使用ライブラリと手法
- **PandasおよびNumPy**: データ処理や数値計算に使用。
- **Scikit-learn**: 特徴量のベクトル化およびモデルの学習データの分割に用いる。特に`LabelEncoder`と`HashingVectorizer`を使用しています。
- **XGBoost**: モデルの構築にはXGBoost（eXtreme Gradient Boosting）を使用し、特にマルチクラスの確率を予測するための設定が行われています。
- **SciPy**: スパース行列の操作に使用。

### 主な手順
1. **データの読み込み**: トレーニングデータをCSVファイルから読み込む。
2. **特徴量ベクトルの作成**: プロンプトや応答をHashingVectorizerを使用してベクトル化し、スパース行列として結合。モデル識別子はLabelEncoderを使用してエンコーディングされます。
3. **データの分割**: 学習データセットとテストデータセットに分割。
4. **XGBoostモデルの訓練**: 複数クラスのソフトマックス出力を形成する目的でXGBoostモデルを訓練。
5. **予測と評価**: テストデータセットに対して予測を行い、その精度を計算して表示。

最後に、テストデータに対して得られた予測確率を含むDataFrameを作成し、指定された形式でCSVファイルとして保存しています。これにより、コンペティションの提出要件に従った出力が得られています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの役立つ分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、こちらにはいくつかの便利なパッケージをロードする方法が示されています

import numpy as np # 線形代数のためのライブラリ
import pandas as pd # データ処理、CSVファイルの入出力 (例: pd.read_csv)

# 入力データファイルは読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると (クリックするか、Shift + Enterを押して) 入力ディレクトリ内のすべてのファイルがリストされます

import os
for dirname, _, filenames in os.walk('/kaggle/input'): # ディレクトリを探索する
    for filename in filenames: # 発見されたファイルについて
        print(os.path.join(dirname, filename)) # ファイルのパスを表示する

# 現在のディレクトリ (/kaggle/working/) に最大20GBのデータを書き込むことができ、これは「すべて保存して実行」を使用してバージョンを作成するときに出力として保持されます
# 一時ファイルを/kaggle/temp/に書き込むこともできますが、それらは現在のセッションの外には保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder
from sklearn.feature_extraction.text import HashingVectorizer
from sklearn.model_selection import train_test_split
import xgboost as xgb
from scipy.sparse import hstack, csr_matrix

# データを読み込む
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# ベクトライザーとラベルエンコーダの初期化
n_features = 2**10 # ベクトルの次元数を設定
vectorizers = {
    'prompt': HashingVectorizer(n_features=n_features), # プロンプト用のベクトライザー
    'response_a': HashingVectorizer(n_features=n_features), # 応答A用のベクトライザー
    'response_b': HashingVectorizer(n_features=n_features) # 応答B用のベクトライザー
}
model_encoder = LabelEncoder() # モデル識別子用のラベルエンコーダを初期化

# モデル識別子をエンコードする
train['model_a_encoded'] = model_encoder.fit_transform(train['model_a']) # モデルAのエンコード
train['model_b_encoded'] = model_encoder.transform(train['model_b']) # モデルBのエンコード

# テキストデータをベクトルに変換して結合する関数
def process_and_concat_features(data, vectorizers):
    features_list = [] # 特徴量のリストを初期化
    for column, vectorizer in vectorizers.items():
        print(f"Vectorizing '{column}'...") # ベクトライジングの進捗を表示
        transformed_data = vectorizer.transform(data[column]) # ベクトルに変換
        features_list.append(transformed_data) # リストに追加
    final_features = hstack(features_list)  # スパース行列として保持
    return final_features

train_features = process_and_concat_features(train, vectorizers) # 特徴量を処理して結合

# モデル識別子とテキスト特徴を組み合わせる
model_features = csr_matrix(train[['model_a_encoded', 'model_b_encoded']]) # スパース行列を使用
X_combined = hstack([model_features, train_features]) # 結合する

# 目的変数をエンコードする
train['winner'] = train.apply(lambda row: 'model_a' if row['winner_model_a'] == 1 else 'model_b' if row['winner_model_b'] == 1 else 'tie', axis=1) # 勝者の決定
label_encoder = LabelEncoder() # ラベルエンコーダを初期化
y_encoded = label_encoder.fit_transform(train['winner']) # エンコードされたラベル

# データを分割する
X_train, X_test, y_train, y_test = train_test_split(X_combined, y_encoded, test_size=0.2, random_state=42) # 学習データとテストデータに分割

# データをDMatrixに変換する
dtrain = xgb.DMatrix(X_train, label=y_train) # 学習用データ
dtest = xgb.DMatrix(X_test, label=y_test) # テスト用データ

# パラメータの設定
params = {
    'objective': 'multi:softprob', # 複数クラスの確率を予測
    'num_class': len(np.unique(y_encoded)), # クラスの数
    'eval_metric': 'mlogloss', # 損失関数
    'tree_method':'hist', # 木構造の学習方法
    'device':'cuda' # GPUを使用する
}

# モデルの学習
num_boost_round = 100 # ブーストラウンドの数
bst = xgb.train(params, dtrain, num_boost_round) # モデルの学習

# 確率を予測する
pred_probs = bst.predict(dtest) # 確率を予測
predictions = np.argmax(pred_probs, axis=1) # 予測されたクラスの取得

# モデルを評価する
from sklearn.metrics import accuracy_score
test_accuracy = accuracy_score(y_test, predictions) # テストデータの精度を計算
print(f"Test Accuracy: {test_accuracy:.2f}") # 精度を表示
```

---The following area is a Code cell (cell numver is 3)---
```python
# データを読み込む
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv') # テストデータの読み込み
test_features = process_and_concat_features(test, vectorizers) # テストデータの特徴量を処理して結合

dtest = xgb.DMatrix(test_features) # DMatrix形式に変換

# 確率を予測する
pred_probs = bst.predict(dtest) # 確率を予測

# 各クラスの予測確率を持つDataFrameを作成
df_submission = pd.DataFrame(pred_probs, columns=label_encoder.classes_) # 確率をDataFrameに変換
df_submission.insert(0, 'id', test['id']) # 'id'列を最初に挿入
df_submission.columns = ['id', 'winner_model_a', 'winner_model_b', 'winner_tie'] # 列名を設定

# 予測をCSVに保存
df_submission.to_csv('submission.csv', index=False) # インデックスなしでCSVに保存
print(df_submission.head()) # 保存したデータの最初の5行を表示
```

** @@@ Jupyter Notebook numver 52, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、チャットボットの応答に対する人間の好みを予測するための機械学習モデルを構築することに取り組んでいます。具体的には、提供されたデータセット内のPythonとTensorFlow、およびKerasを使用して、自然言語処理を行い、複数の異なるモデルの出力を評価し、勝者を予測するシステムを作成します。

### 使用ライブラリと手法
1. **ライブラリのインポート**: `Keras`, `TensorFlow`, `NumPy`, `Pandas`など、モデル構築やデータ処理のために多くのライブラリをインポートします。また、`keras_nlp`を利用して自然言語処理を実行します。
  
2. **ハードウェアの設定**: 利用可能なGPUやTPUを確認し、最適な学習戦略を選択します。これにより、分散学習を利用してトレーニングの効率を向上させます。

3. **データの前処理**: トレーニングデータとテストデータを読み込み、重複を削除するなどして基本的な整形を行います。また、応答をプロンプトに基づいて文脈化してペアを作成し、各応答をテキストクリーニングします。

4. **特徴量の生成**: `DataFrameStatsProcessor`クラスを使用し、データフレームから特徴量を計算し、欠損値の統計や応答の長さに関する情報を整理します。

5. **モデル構築**: `DebertaV3Backbone`を用いたニューラルネットワークモデルを構築し、層を追加して最後はSoftmax関数を用いてクラス確率を予測します。ここでは、L2正則化やドロップアウトレイヤーを使用して過学習を防ぎます。

6. **トレーニング**: 学習率のスケジュールを設定し、モデルをトレーニングします。学習が進むにつれてモデルの性能を評価しつつ、最良のモデルを保存します。

7. **評価と予測**: トレーニングを経たモデルを使用して、テストデータセットに対して予測を実行します。また、FGM（Fast Gradient Method）を利用してモデルの堅牢性を評価します。

8. **出力ファイルの作成**: 最後に、予測結果をCSVファイルとして保存し、コンペティションに提出します。

このノートブックは、全体を通して深層学習を活用した自然言語処理モデルの構築と評価に焦点を当てており、特にモデルの再現性を高めるための工夫が随所に見られます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
os.environ["KERAS_BACKEND"] = "tensorflow"  # または "jax" または "torch"
# Kerasのバックエンドとして使用するフレームワークを環境変数に設定します

import re  # 正規表現を操作するためのライブラリをインポートします

import keras_nlp  # Kerasの自然言語処理用ライブラリをインポートします
import keras  # Kerasライブラリをインポートします
import tensorflow as tf  # TensorFlowライブラリをインポートします

import numpy as np  # NumPyライブラリをインポートします（数値計算用）
import pandas as pd  # Pandasライブラリをインポートします（データ操作用）
from tqdm import tqdm  # プログレスバー表示のためのライブラリをインポートします
import json  # JSONデータを扱うためのライブラリをインポートします
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# 利用可能なGPUの数
```

---The following area is a Code cell (cell numver is 4)---
```python
print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))
# 利用可能なGPUの数を出力します

strategy = tf.distribute.MirroredStrategy()
# モデルの分散学習のために、MirroredStrategyを使用します
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
# 同期中のデバイス（GPU）の数を出力します
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# TPU（Tensor Processing Unit）
```

---The following area is a Code cell (cell numver is 6)---
```python
# ハードウェアを検出し、適切な分散戦略を返します
try:
    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPUの検出。TPU_NAME環境変数が設定されていればパラメータは不要です。Kaggleでは常にこの設定が行われています。
    print('Running on TPU ', tpu.master())
    # TPUが見つかった場合、そのマスターの情報を出力します
except ValueError:
    tpu = None  # TPUが見つからない場合、tpuをNoneに設定します

if tpu:
    tf.config.experimental_connect_to_cluster(tpu)
    # TPUクラスタへの接続を確立します
    tf.tpu.experimental.initialize_tpu_system(tpu)
    # TPUシステムを初期化します
    strategy = tf.distribute.experimental.TPUStrategy(tpu)
    # TPU用の分散戦略を設定します
else:
    strategy = tf.distribute.get_strategy()  # TensorFlowのデフォルトの分散戦略を取得します。CPUおよび単一GPUで動作します。

print("REPLICAS: ", strategy.num_replicas_in_sync)
# 同期中の複製数を出力します
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# 設定
```

---The following area is a Code cell (cell numver is 8)---
```python
class CFG:
    seed = 42  # ランダムシードの設定
    preset = "deberta_v3_extra_small_en"  # 使用するモデルのプリセット
    sequence_length = 512  # 入力シーケンスの最大長
    epochs = 6  # 学習するエポック数
    batch_size = 16  # バッチサイズ
#     batch_size = 16 * strategy.num_replicas_in_sync  # (コメントアウトされたコード) 複製に応じたバッチサイズの計算
    scheduler = 'cosine'  # 学習率スケジューラーの設定
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}  # ラベルと名前のマッピング
    name2label = {v:k for k, v in label2name.items()}  # 名前からラベルへの逆マッピング
    class_labels = list(label2name.keys())  # 使用するクラスラベルのリスト
    class_names = list(label2name.values())  # 使用するクラス名のリスト
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# 再現性
ランダムシードの値を設定することで、毎回の実行で類似の結果を得ることができます。
```

---The following area is a Code cell (cell numver is 10)---
```python
keras.utils.set_random_seed(CFG.seed)  # Kerasのランダムシードを設定します。これにより、モデルの再現性が確保されます。
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# 混合精度

本ノートブックでは、GPUのメモリ使用量を減少させるために、float32精度の代わりに混合精度を使用してトレーニングと推論を行います。これにより、より大きなバッチサイズを使用できるようになり、トレーニングと推論の時間を短縮できます。
```

---The following area is a Code cell (cell numver is 12)---
```python
keras.mixed_precision.set_global_policy("mixed_float16")
# "mixed_float16"ポリシーを設定します。この設定により、モデルの一部は自動的にfloat16で計算され、他の部分（例えば損失関数の計算）は安定性を保つために引き続きfloat32を使用します。
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# データセットのパス
```

---The following area is a Code cell (cell numver is 14)---
```python
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'  # データセットのベースパスを設定します。このパスはKaggleの入力データを指します。
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# メタデータ
## ファイル

### `train.csv`
- `id`: 各行のユニーク識別子。
- `model_[a/b]`: モデルの識別子。train.csvには存在するが、test.csvには存在しません。
- `prompt`: 両モデルに与えられた入力プロンプト。
- `response_[a/b]`: モデル_[a/b]のプロンプトに対する応答。
- `winner_model_[a/b/tie]`: 審査員の選択を示すバイナリ列（正解ターゲット）。

### `test.csv`
- `id`: 各行のユニーク識別子。
- `prompt`: 両モデルに与えられた入力プロンプト。
- `response_[a/b]`: モデル_[a/b]のプロンプトに対する応答。
```

---The following area is a Code cell (cell numver is 16)---
```python
# トレーニングデータを読み込む
df = pd.read_csv(f'{BASE_PATH}/train.csv')  # train.csvファイルを読み込みます
ultrachat_df = pd.read_csv('/kaggle/input/ultrachat-train/ultrachat_s42_a0.5.csv')  # ultrachatデータを読み込みます
df = pd.concat([df, ultrachat_df], axis=0)  # データフレームを縦に結合します
lmsys_33k_deduplicated = pd.read_csv('/kaggle/input/lmsys-33k-deduplicated/lmsys-33k-deduplicated.csv')  # 重複のないlmsysデータを読み込みます
df = pd.concat([df, lmsys_33k_deduplicated], axis=0)  # 再びデータフレームを結合します
# ultrafeedback_lmsysformat = pd.read_parquet('/kaggle/input/ultrafeedback-lmsysformat/ultrafeedback_lmsysformat.parquet', engine='pyarrow')
# ultrafeedback_lmsysformat['prompt'] = ultrafeedback_lmsysformat['prompt'].apply(lambda x: f'["{x}"]')
# df = pd.concat([df, ultrafeedback_lmsysformat], axis=0)  # コメントアウトされたコード。別のデータフレームの読み込みと結合。

# テストデータを読み込む
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')  # test.csvファイルを読み込みます

# display(ultrafeedback_lmsysformat.head())  # コメントアウトされたコード。ultrafeedbackデータの先頭を表示。
display(df.head())  # トレーニングデータフレームの先頭5行を表示します。
```

---The following area is a Code cell (cell numver is 17)---
```python
df = df.drop("id", axis=1)  # 'id'列を削除します
df = df.drop_duplicates(keep="first", ignore_index=True)  # 重複行を削除し、インデックスを再設定します

for col in ["prompt"]:
    df[col] = df[col].apply(lambda x: eval(x))  # 'prompt'列の各要素を評価してリストに変換します
    test_df[col] = test_df[col].apply(lambda x: eval(x))  # テストデータの'prompt'列も同様に処理します
for col in ["response_a", "response_b"]:
    df[col] = df[col].apply(lambda x: eval(x.replace("null", "None")))  # 'null'を'None'に置き換え、評価します
    test_df[col] = test_df[col].apply(lambda x: eval(x.replace("null", "None")))  # テストデータも同様に処理します
    
# サンプリングデータ（コメントアウト）
# df = df.sample(frac=0.01)  # データの1%をサンプリングします

# ラベル変換
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)  # 各行で最大値のインデックスを取得し、クラス名を設定
df["class_label"] = df.class_name.map(CFG.name2label)  # クラス名をラベルに変換します

# サンプルを表示
display(df.head())  # トレーニングデータフレームの先頭5行を表示します
# サンプルを表示
display(test_df.head())  # テストデータフレームの先頭5行を表示します
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
## プロンプトで応答を文脈化する

私たちのアプローチでは、すべての応答に対して単一のプロンプトを使用するのではなく、各応答をプロンプトに基づいて文脈化します。これは、各応答ごとに、モデルに同じプロンプトのセットとその各応答（例えば、「(P + R_A)」、「(P + R_B)」など）を提供することを意味します。

> 一部のプロンプトおよび応答は `utf-8` エンコーディングが使用されていない場合があり、その結果データローダーの作成時にエラーが発生する可能性があります。この場合、これらを空の文字列に置き換えます。
```

---The following area is a Code cell (cell numver is 19)---
```python
def make_pairs(row):
    row['options'] = []  # optionsリストを初期化します
    row["encode_fail"] = False  # エンコード失敗フラグを初期化します

    try:
        # 必要なキーがすべてrow辞書に存在することを確認します
        prompts = row['prompt']  # プロンプトを取得します
        responses_a = row['response_a']  # 応答Aを取得します
        responses_b = row['response_b']  # 応答Bを取得します
        
        # リストの長さが一致することを確認します
        if not (len(prompts) == len(responses_a) == len(responses_b)):
            raise ValueError("The lists 'prompt', 'response_a', and 'response_b' must be of the same length.")
            # プロンプト、応答A、および応答Bのリストが同じ長さである必要があります
            
        response_a_str = ''
        response_b_str = ''
        
        for idx in range(len(prompts)):
            response_a_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_a[idx]}"
            response_b_str += f"Prompt: {prompts[idx]}\n\nResponse: {responses_b[idx]}"
        
        # テキストのクリーニング、たとえば、認識できないUnicode文字を除去または置き換えます
        clean_response_a_str = "".join(filter(lambda x: ord(x) < 128, response_a_str))  # ASCII文字のみを残します
        clean_response_b_str = "".join(filter(lambda x: ord(x) < 128, response_b_str))  # ASCII文字のみを残します
        
        row['options'].append(clean_response_a_str)  # クリーンされた応答Aをoptionsリストに追加
        row['options'].append(clean_response_b_str)  # クリーンされた応答Bをoptionsリストに追加
        
    except KeyError as e:
        print(f"Missing key in row: {e}")  # 指定されたキーが見つからない場合のエラーメッセージ
        row["encode_fail"] = True  # エンコード失敗フラグを立てる
    except ValueError as e:
        print(e)  # 値エラーの場合のメッセージ
        row["encode_fail"] = True  # エンコード失敗フラグを立てる
    except Exception as e:
        # その他すべての例外をキャッチ
        print(f"An unexpected error occurred: {e}")  # 予期しないエラーのメッセージ
        row["encode_fail"] = True  # エンコード失敗フラグを立てる

    return row  # 処理された行を返します
```

---The following area is a Code cell (cell numver is 20)---
```python
df = df.apply(make_pairs, axis=1)  # 各行に対してmake_pairs関数を適用します
display(df.head(2))  # トレーニングデータフレームの最初の2行を表示します

test_df = test_df.apply(make_pairs, axis=1)  # テストデータフレームの各行にも同様にmake_pairs関数を適用します
display(test_df.head(2))  # テストデータフレームの最初の2行を表示します
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## エンコーディング失敗の統計
```

---The following area is a Code cell (cell numver is 22)---
```python
df.encode_fail.value_counts(normalize=False)  # エンコーディング失敗フラグのカウントを表示します（正規化せず）。
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# EDA（探索的データ分析）
```

---The following area is a Code cell (cell numver is 24)---
```python
class DataFrameStatsProcessor:
    def __init__(self, df):
        self.df = df  # 初期化時にデータフレームを設定します

    def _is_empty(self, string: str) -> bool:
        return bool(re.match("^\s*$", string))  # 文字列が空かどうかを判断します

    def _len(self, string: str) -> int:
        if string is None:
            return 0  # Noneの場合は長さ0を返します
        return len(string)  # 文字列の長さを返します

    def _add_len_stats(self, col: str) -> pd.DataFrame:
        if col == "prompt":
            col_prefix = "p_len"  # プロンプトの列接頭辞
        elif col == "response_a":
            col_prefix = "res_a_len"  # 応答Aの列接頭辞
        elif col == "response_b":
            col_prefix = "res_b_len"  # 応答Bの列接頭辞
        
        # 各長さの統計を計算
        self.df[f"{col_prefix}_sum"] = self.df[col].apply(lambda x: sum(self._len(s) for s in x))
        self.df[f"{col_prefix}_mean"] =  self.df[col].apply(lambda x: np.mean(list(self._len(s) for s in x)))
        self.df[f"{col_prefix}_max"] = self.df[col].apply(lambda x: max(self._len(s) for s in x))
        self.df[f"{col_prefix}_sum_log"] = np.log1p(self.df[f"{col_prefix}_sum"])
        self.df[f"{col_prefix}_mean_log"] =  np.log1p(self.df[f"{col_prefix}_mean"])
        self.df[f"{col_prefix}_max_log"] = np.log1p(self.df[f"{col_prefix}_max"])
        
        return self.df  # 更新されたデータフレームを返します
    
    def z_score_normalize(self, columns):
        """
        指定した列にZスコア正規化を適用します。
        パラメータ:
            columns (list): Zスコア正規化を実施する列名のリスト。
        """
        for col in columns:
            self.df[col] = (self.df[col] - self.df[col].mean()) / self.df[col].std()  # Zスコア正規化を実施します
    
    def process_dataframe(self):
        # データフレームの処理を実施します
        self.df["n_prompts"] = self.df["prompt"].apply(lambda x: len(x))  # プロンプトの数を数えます
        self.df["n_res_a"] = self.df["response_a"].apply(lambda x: len(x))  # 応答Aの数を数えます
        self.df["n_res_b"] = self.df["response_b"].apply(lambda x: len(x))  # 応答Bの数を数えます
        assert ((self.df["n_prompts"] == self.df["n_res_a"]) & (self.df["n_prompts"] == self.df["n_res_b"])).all()  # 数が一致することを確認します

        # プロンプトと応答の欠損値や空の値をカウントします
        self.df["n_na_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_prompts"] = self.df["prompt"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_a"] = self.df["response_a"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))
        self.df["n_na_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is None else 0 for p in ps))
        self.df["n_empty_res_b"] = self.df["response_b"].apply(lambda ps: sum(1 if p is not None and self._is_empty(p) else 0 for p in ps))

        # 欠損値の合計を計算します
        self.df["n_miss_res_a"] = self.df["n_na_res_a"] + self.df["n_empty_res_a"]
        self.df["n_miss_res_b"] = self.df["n_na_res_b"] + self.df["n_empty_res_b"]

        # 有効な応答数を計算します
        self.df["n_eff_res_a"] = self.df["n_res_a"] - self.df["n_miss_res_a"]
        self.df["n_eff_res_b"] = self.df["n_res_b"] - self.df["n_miss_res_b"]

        # 長さの統計を追加します
        self._add_len_stats("prompt")
        self._add_len_stats("response_a")
        self._add_len_stats("response_b")

        self.df["res_len_mean_diff"] = self.df["res_a_len_mean"] - self.df["res_b_len_mean"]  # 応答AとBの長さの平均の差を計算
        self.df["res_len_mean_diff_clip"] = self.df["res_len_mean_diff"].clip(-6000, 6000)  # 差の範囲をクリップします

        # プロンプトの欠損値をカウントします
        self.df["n_miss_prompts"] = self.df["n_na_prompts"] + self.df["n_empty_prompts"]
        self.df["n_eff_prompts"] = self.df["n_prompts"] - self.df["n_miss_prompts"]

        # 比率を計算します
        self.df["na_prompt_ratio"] = self.df["n_na_prompts"] / self.df["n_prompts"]
        self.df["empty_prompt_ratio"] = self.df["n_empty_prompts"] / self.df["n_prompts"]
        self.df["miss_prompt_ratio"] = self.df["n_miss_prompts"] / self.df["n_prompts"]
        
        self.df["na_res_a_ratio"] = self.df["n_na_res_a"] / self.df["n_res_a"]
        self.df["empty_res_a_ratio"] = self.df["n_empty_res_a"] / self.df["n_res_a"]
        self.df["miss_res_a_ratio"] = self.df["n_miss_res_a"] / self.df["n_res_a"]
        self.df["na_res_b_ratio"] = self.df["n_na_res_b"] / self.df["n_res_b"]
        self.df["empty_res_b_ratio"] = self.df["n_empty_res_b"] / self.df["n_res_b"]
        self.df["miss_res_b_ratio"] = self.df["n_miss_res_b"] / self.df["n_res_b"]

        # 各種長さの統計を追加します
        for col, col_prefix in zip(["prompt", "response_a", "response_b"], ["p_len", "res_a_len", "res_b_len"]):
            self.df[f"{col_prefix}_med"] = self.df[col].apply(lambda x: np.median(list(self._len(s) for s in x)))  # 中央値を計算します
            self.df[f"{col_prefix}_std"] = self.df[col].apply(lambda x: np.std(list(self._len(s) for s in x)))  # 標準偏差を計算します

        # 有効な長さの平均を計算します
        self.df["p_len_eff_mean"] = self.df["p_len_sum"] / self.df["n_eff_prompts"]
        self.df["res_a_len_eff_mean"] = self.df["res_a_len_sum"] / self.df["n_eff_res_a"]
        self.df["res_b_len_eff_mean"] = self.df["res_b_len_sum"] / self.df["n_eff_res_b"]

        # 長さの差を計算します
        for stats in ["sum", "mean", "max", "med", "eff_mean"]:
            self.df[f"p_a_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_a_len_{stats}"]
            self.df[f"p_b_{stats}_diff"] = self.df[f"p_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            self.df[f"a_b_{stats}_diff"] = self.df[f"res_a_len_{stats}"] - self.df[f"res_b_len_{stats}"]
            
        # 特徴量列を定義します
        len_feature_a_col = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                     "p_a_eff_mean_diff"]
        
        len_feature_b_col = ["res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                             "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                             "p_b_eff_mean_diff"]
        
        numerical_feature_columns = ["res_a_len_sum","res_a_len_mean","res_a_len_max","res_a_len_sum_log","res_a_len_mean_log","res_a_len_max_log",
                                     "res_a_len_med","res_a_len_std","res_a_len_eff_mean","p_a_sum_diff","p_a_mean_diff","p_a_max_diff","p_a_med_diff",
                                     "p_a_eff_mean_diff", "res_b_len_sum","res_b_len_mean","res_b_len_max","res_b_len_sum_log","res_b_len_mean_log","res_b_len_max_log",
                                     "res_b_len_med","res_b_len_std","res_b_len_eff_mean","p_b_sum_diff","p_b_mean_diff","p_b_max_diff","p_b_med_diff",
                                     "p_b_eff_mean_diff"]
        
        # ゼロで割らないように正規化します
        for col in numerical_feature_columns:
            if self.df[col].std() == 0:
                print(f"Warning: Standard deviation is zero for column {col}. Skipping normalization.")
            else:
                self.z_score_normalize([col])  # Zスコア正規化を適用します
                
        self.df = self.df.fillna(0)  # NaNを0で埋めます
        
        # 特徴量をリストに変換します
        len_features_a = self.df[len_feature_a_col].values.tolist()
        len_features_b = self.df[len_feature_b_col].values.tolist()

        return len_features_a, len_features_b  # 長さ特徴量を返します
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# データ分割

以下のコードスニペットでは、class_label列の層化を使用して、既存のデータをトレーニングデータと検証データに分割します。
```

---The following area is a Code cell (cell numver is 26)---
```python
from sklearn.model_selection import train_test_split  # パッケージをインポートします

# データフレームをトレーニングデータ（80%）と検証データ（20%）に分割します
train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])  # class_label列をもとに層化分割を行います
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# 前処理
```

---The following area is a Code cell (cell numver is 28)---
```python
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset,  # 設定されたプリセットを使用して前処理器を作成します
    sequence_length=CFG.sequence_length,  # 設定されたシーケンス長を指定します
)
```

---The following area is a Code cell (cell numver is 29)---
```python
def preprocess_fn(text, label=None, features_a=None, features_b=None):
    text = preprocessor(text)  # テキストを前処理します
    if features_a is not None:
        text['features_a'] = features_a  # features_aが指定されている場合、テキストに追加します
    if features_b is not None:
         text['features_b'] = features_b  # features_bが指定されている場合、テキストに追加します
    return (text, label) if label is not None else text  # ラベルがあれば、処理されたテキストとラベルを返します
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
# FGM（Fast Gradient Method）
```

---The following area is a Code cell (cell numver is 31)---
```python
# FGMの摂動関数を追加します
def fgm_perturb(features, epsilon=1.0):
    # 摂動量を計算します。epsilonは摂動の割合です。
    perturbation = np.random.uniform(-1, 1, features.shape) * epsilon  # 特徴量の形状に基づいてランダムな摂動を生成します
    # 摂動を適用します
    return features + perturbation  # 特徴量に摂動を加えたものを返します
```

---The following area is a Code cell (cell numver is 32)---
```python
# データ前処理関数を修正してFGM摂動を含めます
def preprocess_fn(text, label=None, features_a=None, features_b=None, is_fgm=False, epsilon=1.0):
    # テキストを前処理します
    text = preprocessor(text)
    if features_a is not None:
        if is_fgm:
            # FGMの場合、摺動を適用します
            features_a = fgm_perturb(features_a, epsilon)  # 特徴量AにFGM摂動を加えます
        text['features_a'] = features_a  # 特徴量Aをテキストに追加します
    if features_b is not None:
        if is_fgm:
            # FGMの場合、摺動を適用します
            features_b = fgm_perturb(features_b, epsilon)  # 特徴量BにFGM摂動を加えます
        text['features_b'] = features_b  # 特徴量Bをテキストに追加します
    return (text, label) if label is not None else text  # ラベルがあれば、処理されたテキストとラベルを返します
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# AWP（Adversarial Weight Perturbation）
```

---The following area is a Code cell (cell numver is 34)---
```python
# AWP摺動関数を定義します
def awp_perturb(model, epsilon=1e-4):
    for layer in model.layers:
        if hasattr(layer, 'kernel'):
            # 重みを取得します
            weights = layer.kernel
            # 摺動を計算します
            perturbation = tf.random.normal(weights.shape, stddev=epsilon)  # 標準偏差epsilonの正規分布から摺動を生成します
            # 摺動を適用します
            layer.kernel.assign_add(perturbation)  # 重みに摺動を加えます

# AWPコールバック関数を作成します
class AWPCallback(keras.callbacks.Callback):
    def __init__(self, epsilon):
        super(AWPCallback, self).__init__()  # 親クラスの初期化
        self.epsilon = epsilon  # 摺動の値を保存します

    def on_batch_begin(self, batch, logs=None):
        # 各バッチの開始時にAWP摺動を適用します
        awp_perturb(self.model, self.epsilon)  # モデルにAWP摺動を加えます
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# DataLoader

以下のコードは、tf.data.Datasetを使用してデータ処理のための堅牢なデータフロー・パイプラインを設定します。
```

---The following area is a Code cell (cell numver is 36)---
```python
def build_dataset_with_features(texts, labels=None, features_a=None, features_b=None, batch_size=32, is_fgm=False,  epsilon=1.0,
                                cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # 自動調整を有効にします
    if (features_a is not None) and (features_b is not None):
        # ラベルがない場合はテキストと特徴量のみ、ある場合はラベルも追加します
        slices = (texts, None, features_a, features_b) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3), features_a, features_b)  # スライスを作成します
    else:
        slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # スライスを作成します
    ds = tf.data.Dataset.from_tensor_slices(slices)  # テンソルのスライスからデータセットを生成します
    ds = ds.cache() if cache else ds  # キャッシュを使用するかどうかを設定します
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  # 前処理関数を各要素に適用します
#     ds = ds.map(lambda x: preprocess_fn(x, features_a=features_a, features_b=features_b, is_fgm=is_fgm, epsilon=epsilon),
#                 num_parallel_calls=tf.data.AUTOTUNE)  # (コメントアウトされたコード)摺動を考慮した前処理
    opt = tf.data.Options()
    if shuffle:
        ds = ds.shuffle(shuffle, seed=CFG.seed)  # シャッフルを有効にします
        opt.experimental_deterministic = False  # 結果の非決定性を許可します
    ds = ds.with_options(opt)  # オプションを適用します
    ds = ds.batch(batch_size, drop_remainder=False)  # バッチに分けます
    ds = ds.prefetch(AUTO)  # プリフェッチを使用してデータを事前に読み込む

    return ds  # データセットを返します
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
## トレーニング/検証データローダーの構築
```

---The following area is a Code cell (cell numver is 38)---
```python
train_features_processor = DataFrameStatsProcessor(train_df.copy())  # トレーニングデータフレームのコピーを使用して特徴量処理器を作成します
train_features_a, train_features_b = train_features_processor.process_dataframe()  # トレーニングデータの特徴量を処理します
valid_features_processor = DataFrameStatsProcessor(valid_df.copy())  # 検証データフレームのコピーを使用して特徴量処理器を作成します
valid_features_a, valid_features_b = valid_features_processor.process_dataframe()  # 検証データの特徴量を処理します
```

---The following area is a Code cell (cell numver is 39)---
```python
# トレーニングデータの準備
train_texts = train_df.options.tolist()  # トレーニングデータのテキストをリストに変換します
train_labels = train_df.class_label.tolist()  # トレーニングデータのラベルをリストに変換します
train_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b, 
                         batch_size=CFG.batch_size,  # 設定されたバッチサイズを使用します
                         shuffle=True)  # シャッフルを有効にします

# 検証データの準備
valid_texts = valid_df.options.tolist()  # 検証データのテキストをリストに変換します
valid_labels = valid_df.class_label.tolist()  # 検証データのラベルをリストに変換します
valid_ds = build_dataset_with_features(valid_texts, valid_labels, valid_features_a, valid_features_b, 
                         batch_size=CFG.batch_size,  # 設定されたバッチサイズを使用します
                         shuffle=False)  # シャッフルを無効にします
print(train_ds)  # トレーニングデータセットの情報を表示します
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# 学習率スケジュール

学習率スケジューラの実装は、転移学習において重要です。

学習率は、lr_startから始まり、さまざまな手法を使用してlr_minまで徐々に減少します。これには以下が含まれます：

- step：階段状に学習率を段階的に減少させます。
- cos：コサイン曲線を利用して学習率を徐々に減少させます。
- exp：指数関数的に学習率を減少させます。

**重要性**：適切に構造化された学習率スケジュールは、モデルの効果的なトレーニングに不可欠であり、最適な収束を保証し、オーバーシュートや停滞などの問題を回避します。
```

---The following area is a Code cell (cell numver is 41)---
```python
import math

def get_lr_callback(batch_size=8, mode='cos', epochs=10):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6  # 学習率の開始値、最大値、最小値を設定します
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8  # 学習率のランプアップエポック、持続エポック、減衰率を設定します

    def lrfn(epoch):  # 学習率更新関数
        # エポックに応じて学習率を計算します
        if epoch < lr_ramp_ep: 
            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start  # ランプアップ段階
        elif epoch < lr_ramp_ep + lr_sus_ep: 
            lr = lr_max  # 最大学習率を維持します
        elif mode == 'exp:': 
            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min  # 指数的減衰
        elif mode == 'step': 
            lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)  # ステップ減衰
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs  # コサインの位相を計算します
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min  # コサイン曲線による減衰
        return lr  # 計算された学習率を返します
    
    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # 学習率コールバックを作成します
```

---The following area is a Code cell (cell numver is 42)---
```python
lr_cb = get_lr_callback(CFG.batch_size, epochs=CFG.epochs)  # 設定されたバッチサイズとエポック数で学習率コールバックを取得します
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
# モデルチェックポイント

以下のコードは、トレーニング中にモデルの最良のチェックポイントを保存するコールバックを作成します。これは、提出時の推論に使用します。
```

---The following area is a Code cell (cell numver is 44)---
```python
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',  # 検証ロスを監視します
                                          save_best_only=True,  # 最良のモデルのみを保存します
                                          save_weights_only=True,  # 重みのみを保存します
                                          mode='min')  # 最小化モードで監視します（ロスが最小のときに保存）
```

---The following area is a Markdown cell (cell numver is 45)---
```markdown
# メトリック

今回のコンペティションの評価指標は対数損失です。この尺度は数学的に次のように表されます：

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$
```

---The following area is a Code cell (cell numver is 46)---
```python
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss", label_smoothing=0.1, from_logits=False)  # 対数損失を計算するためのメトリックを定義します。ラベルスムージングを適用します。
```

---The following area is a Markdown cell (cell numver is 47)---
```markdown
# モデリング
```

---The following area is a Code cell (cell numver is 48)---
```python
from tensorflow.keras import regularizers
from tensorflow.keras.layers import Dropout

with strategy.scope():

    # すべての入力層を辞書にまとめます
    inputs = {
        "token_ids": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),  # トークンIDの入力
        "padding_mask": keras.layers.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),  # パディングマスクの入力
        "features_a": keras.layers.Input(shape=(14,), name="features_a", dtype=tf.float32),  # 特徴量Aの入力
        "features_b": keras.layers.Input(shape=(14,), name="features_b", dtype=tf.float32),  # 特徴量Bの入力
    }

    # DebertaV3Classifierバックボーンを作成します
    backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
        CFG.preset,
    )

    # response_aとresponse_bの作成方法を修正し、padding_maskを含めます
    response_a = {
        "token_ids": inputs["token_ids"][:, 0, :],  # 最初のトークンIDを取得
        "padding_mask": inputs["padding_mask"][:, 0, :]  # 最初のパディングマスクを取得
    }
    embed_a = backbone(response_a)  # バックボーンを通して埋め込みを取得します

    response_b = {
        "token_ids": inputs["token_ids"][:, 1, :],  # 2番目のトークンIDを取得
        "padding_mask": inputs["padding_mask"][:, 1, :]  # 2番目のパディングマスクを取得
    }
    embed_b = backbone(response_b)  # バックボーンを通して埋め込みを取得します
    
    # 数値特徴を埋め込みます
    len_features_a_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_a"])  # 特徴量Aの数値埋め込み
    len_features_b_embedding = keras.layers.Dense(512, activation='relu')(inputs["features_b"])  # 特徴量Bの数値埋め込み
    
    # Flatten層を使用して数値特徴埋め込みを2次元テンソルに展開します
    flattened_len_features_a = keras.layers.Flatten()(len_features_a_embedding)  # Flattenによる変換
    flattened_len_features_b = keras.layers.Flatten()(len_features_b_embedding)  # Flattenによる変換
    
    embed_a = keras.layers.GlobalAveragePooling1D()(embed_a)  # グローバル平均プーリング
    embed_b = keras.layers.GlobalAveragePooling1D()(embed_b)  # グローバル平均プーリング
    embeds_text_features_a = keras.layers.Concatenate(axis=-1)([embed_a, flattened_len_features_a])  # テキスト埋め込みと数値特徴を結合
    embeds_text_features_b = keras.layers.Concatenate(axis=-1)([embed_b, flattened_len_features_b])  # テキスト埋め込みと数値特徴を結合
    
    # テキスト埋め込みと数値特徴埋め込みを結合します
    combined_embeds = keras.layers.Concatenate(axis=-1)([embeds_text_features_a, embeds_text_features_b])  # テキストと特徴を結合
    
    # モデルにL2正則化とDropoutを追加します
    combined_embeds = keras.layers.Dense(256, activation='relu', kernel_regularizer=regularizers.l2(1e-5))(combined_embeds)  # L2正則化
    combined_embeds = Dropout(0.05)(combined_embeds)  # Dropoutレイヤー、5%のニューロンをドロップ
    
    # 温度スケール関数を定義します
    def temperature_scale(logits, T=1.0):
        return logits / T  # ロジットを温度で割ります
    
    # 温度パラメータTを定義します
    T = 0.85
    # 温度スケーリングを適用します
    scaled_logits = temperature_scale(combined_embeds, T)  # 温度スケールを適用したロジット
    outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(scaled_logits)  # 出力層を定義します
    
    model = keras.Model(inputs, outputs)  # モデルを作成します
    
    # オプティマイザ、損失、メトリックでモデルをコンパイルします
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=1e-6, clipnorm=1.0),  # Adamオプティマイザを使用
        loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.1, from_logits=False),  # カテゴリカルクロスエントロピー損失
        metrics=[
            log_loss,  # 定義した対数損失
            keras.metrics.CategoricalAccuracy(name="accuracy"),  # 精度メトリック
        ],
    )
    
    # AWPコールバックをモデルのトレーニングに追加します
    awp_cb = AWPCallback(epsilon=1e-4)  # epsilon値は必要に応じて調整できます
```

---The following area is a Markdown cell (cell numver is 49)---
```markdown
### モデルの概要
```

---The following area is a Code cell (cell numver is 50)---
```python
model.summary()  # モデルのサマリーを表示します。各層の出力形状やパラメータ数を確認できます。
```

---The following area is a Markdown cell (cell numver is 51)---
```markdown
# トレーニング
```

---The following area is a Code cell (cell numver is 52)---
```python
# tryブロック内でエラー処理を実装します
try:
    history = model.fit(
        train_ds,  # トレーニングデータセット
        epochs=CFG.epochs,  # 設定されたエポック数で学習
        validation_data=valid_ds,  # 検証データセット
        callbacks=[lr_cb, ckpt_cb, awp_cb]  # 学習率コールバック、チェックポイントコールバック、AWPコールバックを追加
    )
except tf.errors.InvalidArgumentError as e:
    print(f"無効な引数エラーが発生しました：{e}")  # エラーが発生した場合、そのメッセージを表示します
```

---The following area is a Markdown cell (cell numver is 53)---
```markdown
## 最良モデルの読み込み
```

---The following area is a Code cell (cell numver is 54)---
```python
model.load_weights('/kaggle/working/best_model.weights.h5')  # 最良モデルの重みを読み込みます
```

---The following area is a Markdown cell (cell numver is 55)---
```markdown
# 予測
```

---The following area is a Code cell (cell numver is 56)---
```python
# FGM摺動を使用したデータセットでモデルを評価します
fgm_ds = build_dataset_with_features(train_texts, train_labels, train_features_a, train_features_b,
                                     is_fgm=True, epsilon=1.0)  # FGMを使用してデータセットを構築します
evaluation_results = model.evaluate(fgm_ds)  # 評価を実行します

print(f"FGM摺動データセットの評価結果: {evaluation_results}")  # FGM摺動データセットでの評価結果を表示します
```

---The following area is a Code cell (cell numver is 57)---
```python
test_df_features_processor = DataFrameStatsProcessor(test_df)  # テストデータフレームの特徴量処理器を作成します
test_df_features_a, test_df_features_b = test_df_features_processor.process_dataframe()  # テストデータの特徴量を処理します
```

---The following area is a Code cell (cell numver is 58)---
```python
test_texts = test_df.options.tolist()  # テストデータのテキストをリストに変換します
test_ds = build_dataset_with_features(test_texts, features_a=test_df_features_a, features_b=test_df_features_b,
                         batch_size=min(len(test_df), CFG.batch_size),  # テストデータの長さと設定されたバッチサイズの最小値を使用します
                         shuffle=False)  # シャッフルを無効にします
print(test_ds)  # テストデータセットの情報を表示します
```

---The following area is a Code cell (cell numver is 59)---
```python
test_preds = model.predict(test_ds, verbose=1)  # テストデータセットに対してモデルの予測を実行します。進捗を表示します。
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
# 提出
```

---The following area is a Code cell (cell numver is 61)---
```python
sub_df = test_df[["id"]].copy()  # テストデータから'id'列をコピーします
sub_df[CFG.class_names] = test_preds.tolist()  # 予測結果をコピーして新しいデータフレームに追加します
sub_df.to_csv("submission.csv", index=False)  # 提出用ファイルをCSV形式で保存します
sub_df.head()  # 提出データフレームの先頭を表示します
```

** @@@ Jupyter Notebook numver 53, the number of votes :2 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するためのモデルの構築とファインチューニングに関する内容です。以下に詳細をまとめます。

### 問題の概要
このノートブックは、ユーザーが提示するプロンプトに対して異なる大規模言語モデル（LLM）が生成した応答のうち、どの応答が好まれるかを予測するための機械学習モデルを構築しています。具体的には、与えられた応答がどちらのモデル（モデルAまたはモデルB）がユーザーに選ばれるかを予測するテキスト分類の問題を扱っています。

### 使用されている手法およびライブラリ
- **モデル選定**:
  - LLMとしてGemmaやLLAMAモデルを使用するためのハイパーパラメータ設定が行われています。
  - モデルのファインチューニングには、`LoRA`（Low-Rank Adaptation）手法が使用されており、特に`SFTTrainer`を用いています。
  
- **データ処理**:
  - データセットには、トレーニングファイルとテストファイルが用意され、Pandasライブラリを通じてデータの読み込み、前処理、特徴量エンジニアリングが行われています。
  - プロンプトおよび応答は、特定のフォーマットで連結されたテキストとして処理されています。

- **モデルのトレーニング**:
  - `transformers`ライブラリを使用して、トレーニングデータをトークナイズし、モデルの訓練を行います。
  - トレーニング後、精度、再現率、F1スコアなどの評価指標が計算され、各メトリックの可視化にMatplotlibが使用されています。

- **推論**:
  - モデルに対する入力テキストを用いて、分類結果を得るための関数が定義されています。
  - 最後に得た結果を提出用のCSVファイルとして保存します。

### ステップの概要
1. **設定**: ハイパーパラメータや使用するモデルの選定、データセットのパス設定。
2. **ライブラリのインポートとインストール**: 必要なライブラリをインポートし、オフライン環境用のキャッシュされたライブラリをインストール。
3. **データの読み込みと前処理**: トレーニングデータの読み込み、欠損値のチェック、各列のユニークな値の表示。プロンプトと応答を一つのテキストとして形成。
4. **モデル構築とトレーニング**: モデルの選択、設定、ファインチューニングの実行。
5. **評価**: モデル性能の評価およびメトリックの計算。
6. **推論と提出ファイルの生成**: テストデータに基づいて予測を行い、CSV形式で出力。

このNotebookは、ユーザーの好みを正確に予測するモデルの開発を目指しており、複雑なテキスト分類タスクに適した多様なアプローチを備えています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# 設定
```

---The following area is a Code cell (cell numver is 2)---
```python
class CFG:
    OFFLINE = True  # False # 提出コンペティションの要件。開発中はオンラインモデルサポートを推奨
    USE_LLAMA3 = False  # GPUバージョン用
    USE_GEMMA2 = False  # GPUバージョン専用
    TASK_GEN = False  # 生成的テキスト出力タスク用（このコンペには適していないと思われる）
    TASK_CLASSIFICATION = True  # テキスト分類用（このコンペに適している）
#     model1 = "/kaggle/input/llama-3/transformers/8b-hf/1"  # llama3 8B |
    model2 = "/kaggle/input/gemma/transformers/2b-it/3"  # gemma 2B
    model3 = "/kaggle/input/gemma/transformers/7b-it/3"  # gemma 7B
#     model4 = "/kaggle/input/gemma-2/pytorch/gemma-2-9b-it/1"  # gemma 2 9B
    trainFile = "/kaggle/input/lmsys-chatbot-arena/train.csv"  # トレーニングデータファイルのパス
    testFile = "/kaggle/input/lmsys-chatbot-arena/test.csv"  # テストデータファイルのパス
    submitSample = "/kaggle/input/lmsys-chatbot-arena/sample_submission.csv"  # 提出サンプルファイルのパス
    FEW_SHOT_TEST = False  # True # Few-shot テストを使用するかどうか
    USE_RAG = False  # False # True # このプロジェクトではファインチューニングの使用を優先
    USE_WANDB = False  # True # LLMの評価とデバッグ用、ファインチューニングのパフォーマンスを追跡
    USE_TRULENS = False  # RAG用のLLM評価
    USE_DEEPEVAL = False  # LLM評価用（OpenAI APIキーが必要）
    USE_TRAIN = True  # GPUでトレーニングを行うかどうか
    USE_INFER = False  # 提出用予測のみ、テストモデルはなし
    loggingSteps = 10  # ロギングのステップ数
    maxTrainData = 1500  # 最大トレーニングデータ数
    maxEvalData = 20  # 最大評価データ数
    maxToken = 650  # 使用する最大トークン数 (テスト用は512)
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# ライブラリのインストール
```

---The following area is a Code cell (cell numver is 4)---
```python
# installDir = #"/kaggle/input/ai-math-llm-install-package/Universal-LLM-install-page/Universal-LLM-install-page"
installDir = "/kaggle/input/universal-llm-install-package2/Universal-LLM-install-page"  # インストールディレクトリの指定

# オフライン用ライブラリのインストール
if CFG.OFFLINE:
    !pip install transformers --no-index --no-deps --find-links=file://{installDir}/tranforemers  # transformersライブラリのインストール
    !pip install -U datasets --no-index --no-deps --find-links=file://{installDir}/datasets  # datasetsライブラリのインストール
    !pip install -U accelerate --no-index --no-deps --find-links=file://{installDir}/accelerate  # accelerateライブラリのインストール
    !pip install build --no-index --no-deps --find-links=file://{installDir}/build-1.2.1-py3-none-any.whl  # buildライブラリのインストール
    !pip install -U bitsandbytes --no-index --no-deps --find-links=file://{installDir}/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl  # bitsandbytesライブラリのインストール
    !pip install langchain --no-index --no-deps --find-links=file://{installDir}/langchain-0.2.6-py3-none-any.whl  # langchainライブラリのインストール
    !pip install langchain-core --no-index --no-deps --find-links=file://{installDir}/langchain_core-0.2.10-py3-none.whl  # langchain-coreライブラリのインストール
    !pip install langsmith --no-index --no-deps --find-links=file://{installDir}/langsmith-0.1.82-py3-none-any.whl  # langsmithライブラリのインストール
    !pip install langchain-community --no-index --no-deps --find-links=file://{installDir}/langchain_community-0.2.5-py3-none-any.whl  # langchain-communityライブラリのインストール
    !pip install sentence-transformers --no-index --no-deps --find-links=file://{installDir}/sentence_transformers-3.0.1-py3-none-any.whl  # sentence-transformersライブラリのインストール
    !pip install chromadb --no-index --no-deps --find-links=file://{installDir}/chromadb-0.5.3-py3-none-any.whl  # chromadbライブラリのインストール
    !pip install faiss-cpu --no-index --no-deps --find-links=file://{installDir}/faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl  # faiss-cpuライブラリのインストール
    !pip install -U huggingface_hub --no-index --no-deps --find-links=file://{installDir}/huggingface_hub  # huggingface_hubライブラリのインストール
    !pip install -qU langchain-text-splitters --no-index --no-deps --find-links=file://{installDir}/langchain_text_splitters-0.2.2-py3-none-any.whl  # langchain-text-splittersライブラリのインストール
    !pip install -U peft --no-index --no-deps --find-links=file://{installDir}/peft-0.11.1-py3-none-any.whl  # peftライブラリのインストール
    !pip install -U trl --no-index --no-deps --find-links=file://{installDir}/trl-0.9.4-py3-none-any.whl  # trlライブラリのインストール
    !pip install umap-learn --no-index --no-deps --find-links=file://{installDir}/umap_learn  # umap-learnライブラリのインストール
    !pip install evaluate --no-index --no-deps --find-links=file://{installDir}/evaluate-0.4.2-py3-none-any.whl  # evaluateライブラリのインストール
    !pip install deepeval --no-index --no-deps --find-links=file://{installDir}/deepeval-0.21.62-py3-none-any.whl  # deepevalライブラリのインストール
    !pip install weave --no-index --no-deps --find-links=file://{installDir}/weave-0.50.2-py3-none-any.whl  # weaveライブラリのインストール
    !pip install openai --no-index --no-deps --find-links=file://{installDir}/openai-1.35.7-py3-none-any.whl  # openaiライブラリのインストール
    !pip install langchain_openai --no-index --no-deps --find-links=file://{installDir}/langchain_openai-0.1.13-py3-none-any.whl  # langchain_openaiライブラリのインストール
    !pip install trulens --no-index --no-deps --find-links=file://{installDir}/trulens-0.13.4-py3-none-any.whl  # trulensライブラリのインストール
    !pip install trulens-eval --no-index --no-deps --find-links=file://{installDir}/trulens_eval-0.32.0-py3-none-any.whl  # trulens-evalライブラリのインストール
    
else:  # オンライン用ライブラリのインストール
    !pip install git+https://github.com/huggingface/transformers.git  # ソースからtransformerをインストール
    # !pip install --upgrade torch datasets accelerate peft bitsandbytes trl
    # !pip install --upgrade accelerate peft bitsandbytes trl
    !pip install --upgrade datasets accelerate bitsandbytes  # datasets、accelerate、bitsandbytesをアップグレード
    !pip install langchain langchain-community sentence-transformers chromadb faiss-cpu  # langchain関連ライブラリのインストール
    !pip install --upgrade huggingface_hub  # huggingface_hubをアップグレード
    !pip install -qU langchain-text-splitters  # langchain-text-sプリッターをアップグレード
    # LoRAファインチューニング用
    !pip install --upgrade peft trl  # peftとtrlをアップグレード
    # 高度なRAGおよびLLM評価用
    !pip install portalocker openai langchain_openai  # deepevalの依存ライブラリ（OpenAI APIキーが必要）をインストール
    !pip install --upgrade umap-learn evaluate deepeval weave trulens trulens-eval  # これらのライブラリをアップグレード
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 6)---
```python
import os, json, time  # osモジュール、jsonモジュール、timeモジュールのインポート
import gc  # ガベージコレクション用モジュールのインポート
from IPython.display import display, Markdown  # Jupyter Notebookでの表示用モジュールのインポート
import numpy as np  # 線形代数用のNumPyモジュールのインポート
import pandas as pd  # データ処理用のPandasモジュールのインポート（例：pd.read_csvでCSVファイルを読み込む）
import transformers  # Transformersライブラリのインポート
import torch  # PyTorchライブラリのインポート
from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, TrainingArguments  # モデルとトークナイザーのインポート
from transformers import AutoModelForSequenceClassification, DataCollatorWithPadding  # シーケンス分類用モデルとデータコラファイアのインポート
from langchain_community.document_loaders import TextLoader  # 新しいバージョンのテキストローダーのインポート
from langchain.prompts.prompt import PromptTemplate  # プロンプトテンプレートのインポート
from langchain_core.runnables import ConfigurableField  # 設定可能なフィールドのインポート
from langchain_community.vectorstores import FAISS, Chroma  # FAISSおよびChromaのインポート

# テキストチャンク分割器のインポート
from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter, SentenceTransformersTokenTextSplitter  # テキストスプリッター
from langchain.embeddings import HuggingFaceEmbeddings  # HuggingFaceの埋め込みモデルのインポート
from datasets import Dataset, DatasetDict, load_dataset  # データセット用モジュールのインポート

# LLM評価用
import evaluate  # 評価用ライブラリのインポート
import pytest  # テスト用モジュールのインポート
import trulens  # TruLensライブラリのインポート
from sklearn.metrics import (classification_report, ConfusionMatrixDisplay,  # 分類レポートと混同行列表示のインポート
                             f1_score, accuracy_score, precision_score, recall_score)  # 精度、再現率、F1スコア、正確度のインポート

import warnings  # 警告用モジュールのインポート
# warnings.filterwarnings("error")  # 警告をエラーとして扱う
# warnings.filterwarnings("ignore", category=DeprecationWarning)  # 非推奨警告を無視する設定
```

---The following area is a Code cell (cell numver is 7)---
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればCUDAを使用し、なければCPUを使用する
device  # 使用しているデバイスを表示する
```

---The following area is a Code cell (cell numver is 8)---
```python
def clearMemory():
    # メモリをクリアする関数
    for _ in range(5):  # 5回ループを実行する
        torch.cuda.empty_cache()  # CUDAキャッシュをクリアする
        gc.collect()  # ガベージコレクションを実行してメモリを解放する
        time.sleep(0.3)  # 0.3秒待機する
```

---The following area is a Code cell (cell numver is 9)---
```python
clearMemory()  # メモリをクリアする関数を呼び出す
```

---The following area is a Code cell (cell numver is 10)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これは、kaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、以下は読み込むのに役立ついくつかのパッケージです

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると（実行をクリックするかShift + Enterを押すことで）、入力ディレクトリ内のすべてのファイルがリスト表示されます
if False:
    for dirname, _, filenames in os.walk('/kaggle/input'):
        for filename in filenames:
            print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）に最大20GBまで書き込むことができ、"Save & Run All"を使用してバージョンを作成する際に出力として保存されます  
# 一時ファイルは/kaggle/temp/に書き込むこともできますが、現在のセッションの外では保存されません
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# W&Bの初期化（オンライン専用、LLM評価ツール）
```

---The following area is a Code cell (cell numver is 12)---
```python
if CFG.USE_WANDB:
    import wandb  # Weights & Biasesライブラリのインポート
    from kaggle_secrets import UserSecretsClient  # Kaggleの秘密情報管理クライアントのインポート
    user_secrets = UserSecretsClient()  # ユーザーの秘密情報クライアントを初期化
    my_secret = user_secrets.get_secret("wandb_api_key")  # Weights & Biases APIキーを取得
    wandb.login(key=my_secret)  # ログイン
    reportTo = "wandb"  # ファインチューニングトレーニングのロギングに使用
else:
    reportTo = "none"  # None (ロギングなし)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# データセットの読み込み
```

---The following area is a Code cell (cell numver is 14)---
```python
trainDF = pd.read_csv(CFG.trainFile)  # トレーニングデータファイルを読み込み、Pandas DataFrameに格納する
trainDF  # 読み込んだトレーニングデータを表示する
```

---The following area is a Code cell (cell numver is 15)---
```python
trainDF[trainDF["winner_tie"] == 1]  # 'winner_tie'列が1の行だけをフィルタリングして表示する
```

---The following area is a Code cell (cell numver is 16)---
```python
nullTranDF = trainDF[trainDF.response_a == 'null']  # 'response_a'列が'null'である行だけをフィルタリングして新しいデータフレームに格納する
```

---The following area is a Code cell (cell numver is 17)---
```python
len(nullTranDF)  # 'response_a'が'null'の行数を取得して表示する
```

---The following area is a Code cell (cell numver is 18)---
```python
trainDF.isnull().sum()  # データフレーム内の各列における欠損値の数をカウントして表示する
```

---The following area is a Code cell (cell numver is 19)---
```python
testDF = pd.read_csv(CFG.testFile)  # テストデータファイルを読み込み、Pandas DataFrameに格納する
testDF.head()  # 読み込んだテストデータの最初の5行を表示する
```

---The following area is a Code cell (cell numver is 20)---
```python
submitDF = pd.read_csv(CFG.submitSample)  # 提出サンプルファイルを読み込み、Pandas DataFrameに格納する
submitDF.head()  # 読み込んだ提出サンプルデータの最初の5行を表示する
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# EDA（探索的データ分析）
```

---The following area is a Code cell (cell numver is 22)---
```python
def printUniqueValue(df, showAll=True):
    # データフレームの各列のユニークな値を表示する関数
    for col in df.columns:
        if showAll == True:
            print(f"""{col} :  {df[col].unique()}""")  # 各列のユニークな値を表示
        else:
            if df[col].dtype == "object":  # オブジェクト型の列のユニークな値のみ表示
                print(f"{col} : {df[col].unique()}")
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
## ユニークな値を表示する
```

---The following area is a Code cell (cell numver is 24)---
```python
printUniqueValue(trainDF, showAll=True)  # トレーニングデータフレームのユニークな値を表示する関数を呼び出す
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## モデル分布
```

---The following area is a Code cell (cell numver is 26)---
```python
trainDF.columns  # トレーニングデータフレームの列名を表示する
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# LLMモデルの読み込み
```

---The following area is a Code cell (cell numver is 28)---
```python
if CFG.USE_TRAIN == True:
    # LoRAファインチューニング用
    from trl import SFTTrainer  # SFTTrainerをインポート
    from peft import LoraConfig, PeftModel, get_peft_model, prepare_model_for_kbit_training  # LoRA設定、モデル、取得関数などをインポート
    # prepare_model_for_int8_trainingは非推奨
```

---The following area is a Code cell (cell numver is 29)---
```python
do_sample = True  # サンプリングを使用するかどうか
top_p = 0.95  # nucleus samplingの確率
top_k = 2  # top-k samplingのk値
temperature = 0.2  # テンプレチャー（生成の多様性を調整）
num_beams = 3  # ビームサーチのビーム数
max_length = 512  # 生成するテキストの最大長

# GPU専用の量子化設定
bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,  # 4ビットで読み込む
        bnb_4bit_quant_type="nf4",  # 4ビットの量子化タイプ
        bnb_4bit_compute_dtype=torch.bfloat16,  # 計算のデータ型
        bnb_4bit_use_double_quant=True  # 4ビットベースモデルの二重量子化を有効にする
)
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
# テキスト分類のラベルを定義する
```

---The following area is a Code cell (cell numver is 31)---
```python
id2label = {0: "winner_model_a", 1: "winner_model_b", 2: "winner_tie"}  # ラベルIDからラベル名へのマッピング
label2id = {"winner_model_a": 0, "winner_model_b": 1, "winner_tie": 2}  # ラベル名からラベルIDへのマッピング
```

---The following area is a Code cell (cell numver is 32)---
```python
if device.type == "cuda":  # CUDAデバイスが使用されている場合（7B/8B/9Bモデルを使用し性能向上）
    if CFG.USE_LLAMA3:
        modelSel = CFG.model1  # 使用するモデルを選択
        llmModel = "llama3_8b"  # モデル名を設定
        
    elif CFG.USE_GEMMA2:
        modelSel = CFG.model4  # 使用するモデルを選択
        llmModel = "gemma2_9b"
    
    else:
        modelSel = CFG.model3  # 使用するモデルを選択
        llmModel = "gemma_7b"
    
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config=bnb_config)  # 生成モデルのロード
#         model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels=3,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type="multi_label_classification",  # 問題の不一致による注釈
                                                 quantization_config=bnb_config)  # 分類モデルのロード
#          model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto", num_labels=3)
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto",  
                                                 quantization_config=bnb_config)

    tokenizer = AutoTokenizer.from_pretrained(modelSel)  # トークナイザーの初期化
    tokenizer.add_eos_token = True  # 終了トークン<eos>を追加する
    tokenizer.padding_side = "right"  # パディングを右側に設定
    
else:  # CPUの場合は小さなモデルを選択
    modelSel = CFG.model2  # 使用するモデルを選択
    llmModel = 'gemma_2b'
    
    if CFG.TASK_GEN:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")  # 生成モデルのロード
        
    elif CFG.TASK_CLASSIFICATION:
        model = AutoModelForSequenceClassification.from_pretrained(modelSel, device_map="auto", 
                                                                   num_labels=3,
                                                                   id2label=id2label, 
                                                                   label2id=label2id,
#                                                                    problem_type="multi_label_classification",
                                                                  )
    else:
        model = AutoModelForCausalLM.from_pretrained(modelSel, device_map="auto")  # モデルをロード

    tokenizer = AutoTokenizer.from_pretrained(modelSel)  # トークナイザーの初期化
    tokenizer.add_eos_token = True  # 終了トークン<eos>を追加する
    tokenizer.padding_side = "right"  # パディングを右側に設定
```

---The following area is a Code cell (cell numver is 33)---
```python
model  # 読み込んだモデルを表示する
```

---The following area is a Code cell (cell numver is 34)---
```python
llmModel  # 使用しているLLMモデルの名前を表示する
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
# プロンプトエンジニアリング
```

---The following area is a Code cell (cell numver is 36)---
```python
if CFG.TASK_GEN:
    templatePrompt1 = """Question: {question}.\n最終結果をJSON形式で、キー'answer'のみで要求してください。
            """  # 質問を受け取り、JSON形式での回答を求めるプロンプトテンプレート
    templatePrompt2 = "ユーザーの質問に答えてください。\n###\n{format_instructions}\n###\nQuestion: {query}\n"  # 質問に回答するためのプロンプトテンプレート
```

---The following area is a Markdown cell (cell numver is 37)---
```markdown
### LLMからの応答生成
```

---The following area is a Code cell (cell numver is 38)---
```python
if CFG.TASK_GEN:
    def generateResponse(query, maxNewToken=256):
        """
        LLMモデルにメッセージを直接送信し、応答を取得する関数
        """
    
        inputIds = tokenizer(query, return_tensors="pt").to(device)  # クエリをトークナイズしてテンソルに変換
        response = model.generate(**inputIds,
                                  do_sample=True,  # サンプリングを使用
                                  top_p=0.95,  # nucleus samplingの確率
                                  top_k=3,  # top-k samplingのk値
                                  temperature=0.5,  # テンプレチャー
#                                   max_length=1024,  # 最大長（コメントアウトされている）
                                  max_new_tokens=maxNewToken,  # 新しく生成するトークンの最大数
                                 )
    
        # 生成された応答をデコードし、特別なトークンをスキップして返す
        return tokenizer.decode(response[0][len(inputIds["input_ids"]):], skip_special_tokens=True)
```

---The following area is a Markdown cell (cell numver is 39)---
```markdown
# データを抽出するためのシンプルなパーサー
```

---The following area is a Code cell (cell numver is 40)---
```python
import re  # 正規表現モジュールのインポート
from json.decoder import JSONDecodeError  # JSONデコードエラークラスのインポート

if CFG.TASK_GEN:

    def isInteger(text):
        # テキストが非負の整数かどうかを判断する関数
        try:
            if int(text) >= 0:
                return True  # 整数である場合はTrueを返す
            else:
                return False
        except ValueError:
            return False  # 整数に変換できない場合はFalseを返す

    def llmJSONparser(txt, key="answer:", integerOut=False):
        """
        LLMからの応答からJSON形式で答えを取得しようとする関数
        """
        try:
            subText = txt.split("{")  # {}で区切られた部分をリストに分割
            for txtSeg in subText:  # リスト内で答えを探すループ
                end = txtSeg.find("}")  # テキストセグメント内の終了位置を見つける
                sub = txtSeg[:end]  # {}のコンテキストを持つサブ文字列
                temp = sub.replace("*", "")  # *記号を削除
                temp = temp.replace("\"", "")  # \"記号を削除
                temp = temp.lower()  # 小文字に変換
                answerloc = temp.find(key)  # "answer"の位置を見つける
                if answerloc != -1:
                    print(f"答えの位置を見つけました: {answerloc}")
                    newTxt = temp[answerloc:]  # 答えの部分文字列を取得
#                   print("Temp: ", temp)
                    subTxt = newTxt.split("\n")  # 新しいテキストを行ごとに分割
                    #       print(subTxt)
                    rel = subTxt[0][len(key):].strip()  # 空白を削除して答えの値を取得
                    rel = rel.replace(',', '')  # ,記号を削除
                    print(rel)
                    if integerOut:  # 整数の出力を期待する場合
                        if isInteger(rel):
                            return rel  # 整数であれば返す
                        else:
                            continue  # 値が見つからなければ次へ
                    else:
                        return rel  # 整数でない場合はそのまま返す
                
            return None  # 答えが見つからない場合
        except:
            print(f"""エラー LLM JSONパーサーの入力テキスト {txt}""")
            return None
        return None

    def getLLMAnswerParser(txt, key="answer:"):
        """
        JSONパーサーが失敗した場合、回答がJSON形式でないと思われる場合に使用する関数
        "answer"というキーワードを使用して最終的な答えを検索
        """
        # 答えを見つける  
        temp = txt.replace("*", "")  # *記号を削除
        temp = temp.replace("\"", "")  # ""記号を削除
        temp = temp.lower()  # 小文字に変換
        # 答えのキーワードを見つける
        start = temp.find(key)
        print(f"開始位置: {start}")
        subStr = temp[start:]
        if start != -1:
            subTxt = subStr.split("\n")  # 行ごとに分割
           #print(subTxt)
            rel = subTxt[0][len(key):].strip()  # 空白を削除して答えの値を取得
            rel = rel.replace(',', '')  # ,記号を削除
            print(rel)
            return rel  # 得られた答えを返す
    
        print(subStr)
        return None  # 答えが見つからない場合
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# LLMの構造化出力からデータを抽出するためのパーサーを追加する
```

---The following area is a Code cell (cell numver is 42)---
```python
from langchain_core.output_parsers import (StrOutputParser,  # 文字列出力パーサーのインポート
                                           JsonOutputParser,  # JSON出力パーサーのインポート
                                           PydanticOutputParser,  # Pydantic出力パーサーのインポート
                                          )
# LLMの構造化出力用
from langchain_core.pydantic_v1 import BaseModel, Field, validator  # Pydanticモデルのインポート
# from pydantic import BaseModel, Field  # PydanticのBaseModelとFieldをインポート（コメントアウトされている）
```

---The following area is a Code cell (cell numver is 43)---
```python
if CFG.TASK_GEN:
    # LLM出力構造のデータ構造を定義する
    class Answer(BaseModel):
        answer: str = Field(description="LLMからの質問応答の答え")
        explanation: str = Field(description="答えがどのように生じたか、及びその理由")
```

---The following area is a Code cell (cell numver is 44)---
```python
# パーサーのテスト
if CFG.TASK_GEN:
    jsonParser = JsonOutputParser(pydantic_object=Answer)  # JSONパーサーのインスタンスを作成
    pydanticParser = PydanticOutputParser(pydantic_object=Answer)  # Pydanticベースのパーサーのインスタンスを作成
```

---The following area is a Code cell (cell numver is 45)---
```python
if CFG.TASK_GEN:
    print(jsonParser.get_format_instructions())  # JSONパーサーのフォーマット指示を表示する
```

---The following area is a Code cell (cell numver is 46)---
```python
if CFG.TASK_GEN:
    print(pydanticParser.get_format_instructions())  # Pydanticパーサーのフォーマット指示を表示する
```

---The following area is a Code cell (cell numver is 47)---
```python
%%time
if CFG.TASK_GEN:
    ret = generateResponse("What is Machine Learning?", maxNewToken=256)  # モデルをテストするための応答を生成する
```

---The following area is a Code cell (cell numver is 48)---
```python
if CFG.TASK_GEN:
    print(ret)  # LLMのデフォルト出力がMarkdown形式であるように見えるので表示する
```

---The following area is a Code cell (cell numver is 49)---
```python
if CFG.TASK_GEN:
    display(Markdown(ret))  # Markdown形式の出力を表示する
```

---The following area is a Code cell (cell numver is 50)---
```python
clearMemory()  # メモリをクリアする関数を呼び出す
```

---The following area is a Code cell (cell numver is 51)---
```python
%%time 
if CFG.TASK_GEN:
    # 構造化フォーマットでプロンプトテンプレートをテストし、パーサーをテストする
    query = "What is Machine Learning?"  # 質問を設定
    newPrompt = PromptTemplate(input_variables=["question"], template=templatePrompt1)  # プロンプトテンプレートを作成
    finalPrompt = newPrompt.format(
        question=query  # 質問をテンプレートに挿入
    )
    rel = generateResponse(finalPrompt, maxNewToken=1024)  # 応答を生成
    jsonTxt = llmJSONparser(rel, key="answer:", integerOut=False)  # JSONパーサーを使用して応答を解析
    print(f"Question : {query}\nResponse Answer: {jsonTxt}")  # 出力構造フォーマットを変換して表示
```

---The following area is a Code cell (cell numver is 52)---
```python
if CFG.TASK_GEN:
    print(rel)  # 生成された応答を表示する
```

---The following area is a Code cell (cell numver is 53)---
```python
if CFG.TASK_GEN:
    templatePrompt2  # プロンプトテンプレート2を表示する
```

---The following area is a Code cell (cell numver is 54)---
```python
%%time 
if CFG.TASK_GEN:
    # PydanticOutputParserによる構造出力制御をテストする
    query = "What is Machine Learning?"  # 質問を設定
    newPrompt = PromptTemplate(template=templatePrompt2,
                          input_variables=["query"],
                          partial_variables={"format_instructions": pydanticParser.get_format_instructions()},  # Pydanticパーサーの指示を部分変数に設定
                          )
    finalPrompt = newPrompt.format(
            query=query,  # 質問をテンプレートに挿入
        )
    print(f"最終プロンプト: {finalPrompt}")  # 最終プロンプトを表示
    print("応答:\n")  # 応答のセクションを表示
    rel = generateResponse(finalPrompt, maxNewToken=2048)  # 応答を生成
    print(rel)  # 生成された応答を表示
```

---The following area is a Code cell (cell numver is 55)---
```python
clearMemory()  # メモリをクリアする関数を呼び出す
```

---The following area is a Code cell (cell numver is 56)---
```python
# pydanticParser.parse(rel)  # 生成された応答をPydanticパーサーで解析するコード（現在コメントアウトされている）
```

---The following area is a Markdown cell (cell numver is 57)---
```markdown
# ファインチューニング用データセットの準備
```

---The following area is a Code cell (cell numver is 58)---
```python
tempTrainData = (trainDF["prompt"])  # トレーニングデータフレームからプロンプト列を抽出する
```

---The following area is a Code cell (cell numver is 59)---
```python
tempTrainData[0]  # 抽出したトレーニングデータの最初のプロンプトを表示する
```

---The following area is a Code cell (cell numver is 60)---
```python
def dataPreprocess(inputStr):
    # リスト内の文字列を連結する関数
    stripStr = inputStr.strip("[]")  # リスト記号を削除
#     print(stripStr)
    sentence = [s.strip("\"") for s in stripStr.split('","')]  # リストを分割し、先頭の"を削除
#     print(sentence)
    finalStr = " ".join(sentence)  # 文を単一の文字列に連結
    return finalStr  # 最終的な文字列を返す
```

---The following area is a Markdown cell (cell numver is 61)---
```markdown
# トレーニング／テストデータセットの前処理
```

---The following area is a Code cell (cell numver is 62)---
```python
trainDF  # トレーニングデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 63)---
```python
tempTrainData[0]  # 抽出したトレーニングデータの最初のプロンプトを表示する
```

---The following area is a Code cell (cell numver is 64)---
```python
dataPreprocess(tempTrainData[0])  # 最初のトレーニングデータプロンプトを前処理関数に渡して処理する
```

---The following area is a Code cell (cell numver is 65)---
```python
trainDF  # トレーニングデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 66)---
```python
testDF  # テストデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 67)---
```python
# トレーニングデータセットのクリーニング
trainDF.loc[:, "prompt"] = trainDF["prompt"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換
trainDF.loc[:, "response_a"] = trainDF["response_a"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換
trainDF.loc[:, "response_b"] = trainDF["response_b"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換

# テストデータセットのクリーニング 
testDF.loc[:, "prompt"] = testDF["prompt"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換
testDF.loc[:, "response_a"] = testDF["response_a"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換
testDF.loc[:, "response_b"] = testDF["response_b"].apply(dataPreprocess)  # ストリングのリストを単一の文に変換
```

---The following area is a Code cell (cell numver is 68)---
```python
trainDF  # クリーニング後のトレーニングデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 69)---
```python
testDF  # クリーニング後のテストデータフレームの内容を表示する
```

---The following area is a Markdown cell (cell numver is 70)---
```markdown
## トレーニングデータセットの作成
```

---The following area is a Code cell (cell numver is 71)---
```python
if CFG.USE_TRAIN:
    def getTokenLength(texts):
        # テキストのトークン長を取得する関数
        ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']  # テキストをトークナイズしてIDを取得
        # 各テキストのinput_idsの長さを返す
        return [len(t) for t in ids]  # 各トークンIDの長さをリストで返す
```

---The following area is a Code cell (cell numver is 72)---
```python
# ターゲットをワンホットエンコーディングからカテゴリ（数値）に変換
targetCol = ["winner_model_a", "winner_model_b", "winner_tie"]
trainDF["label"] = np.argmax(trainDF[targetCol].values, axis=1)  # 0: winner model a, 1: winner model b, 2: winner tie
```

---The following area is a Code cell (cell numver is 73)---
```python
trainDF["label"].value_counts()  # ターゲットラベルの各カテゴリの出現回数をカウントして表示する
```

---The following area is a Code cell (cell numver is 74)---
```python
trainDF["label"]  # トレーニングデータフレームのラベル列を表示する
```

---The following area is a Code cell (cell numver is 75)---
```python
trainDF["label"].value_counts().plot(kind="bar");  # ターゲットラベルの出現回数を棒グラフでプロットする
```

---The following area is a Code cell (cell numver is 76)---
```python
# trainDF["labelStr"] = trainDF["label"].apply(str)  # ラベルを文字列に変換して新しい列に追加するコード（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 77)---
```python
# trainDF["labelStr"]  # 文字列形式のラベル列を表示する（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 78)---
```python
# トレーニング特徴のためのトレーニングデータセットを作成する
# trainDF["text"] =  ("User Prompt: " + trainDF["prompt"] + 
#                      "\n\n---\n\nModel A: " + trainDF["response_a"] +
#                      "\n\n---\n\nModel B: " + trainDF["response_b"] 
#                    )  # ユーザープロンプトとモデルの応答を組み合わせて新しいテキスト列を作成するコード（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 79)---
```python
trainDF["text"] =  ("<prompt>: " + trainDF["prompt"] +  # プロンプトを含むテキスト列を作成する
                     "\n\n<response_a>: " + trainDF["response_a"] +  # モデルAの応答を追加
                     "\n\n<response_b>: " + trainDF["response_b"]  # モデルBの応答を追加
                   )
```

---The following area is a Code cell (cell numver is 80)---
```python
testDF  # テストデータフレームの内容を表示する
```

---The following area is a Markdown cell (cell numver is 81)---
```markdown
## 提出用のテストデータセットの準備
```

---The following area is a Code cell (cell numver is 82)---
```python
# testDF["text"] = ("User Prompt: " + testDF["prompt"] +  # テストデータのテキスト列を作成するコード（現在コメントアウトされている）
#                      "\n\n---\n\nModel A: " + testDF["response_a"] +
#                      "\n\n---\n\nModel B: " + testDF["response_b"]
#                  )
```

---The following area is a Code cell (cell numver is 83)---
```python
testDF["text"] =  ("<prompt>: " + testDF["prompt"] +  # テストデータのテキスト列を作成する
                     "\n\n<response_a>: " + testDF["response_a"] +  # モデルAの応答を追加
                     "\n\n<response_b>: " + testDF["response_b"]  # モデルBの応答を追加
                   )
```

---The following area is a Code cell (cell numver is 84)---
```python
print(trainDF["text"][1])  # トレーニングデータのサンプルを表示する
```

---The following area is a Code cell (cell numver is 85)---
```python
if False:  # トークンの長さをカウントするためだけの条件
    trainDF.loc[:, 'token_count'] = getTokenLength(trainDF['text'])  # 各テキストのトークン長を計算して追加する
```

---The following area is a Markdown cell (cell numver is 86)---
```markdown
## ファインチューニング
```

---The following area is a Code cell (cell numver is 87)---
```python
trainDF  # ファインチューニング用のトレーニングデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 88)---
```python
if False:  # トークンの長さをカウントするためだけの条件
    print(trainDF['token_count'].describe().to_frame().astype(int))  # トークン長の統計情報を表示する（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 89)---
```python
if False:  # トークンの長さをカウントするためだけの条件
    # データの80%をカバーするトークンの長さを取得、長さはまだ1024を使用
    print(np.percentile(trainDF["token_count"], q=65))  # 65パーセンタイルを計算して表示（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 90)---
```python
if False:
    trainDF.drop("token_count", axis=1, inplace=True)  # token_count列をトレーニングデータフレームから削除するコード（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 91)---
```python
trainDF  # トレーニングデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 92)---
```python
print(testDF["text"][0])  # テストデータの最初のテキストを表示する
```

---The following area is a Code cell (cell numver is 93)---
```python
if CFG.USE_WANDB and CFG.USE_TRAIN:
    # 新しいwandbランを開始
    wandbFineTuningProject = "lmsys-chatbot-arena-fine-tuning"  # プロジェクト名を設定
    runTask1 = wandb.init(project=wandbFineTuningProject, job_type="generation", anonymous="allow")  # W&Bランを初期化
    # W&Bテーブルを定義
    wandbCol1 = ["model", "user query", "modela_ans", "modelb_ans", "label"]  # 記録用のカラムを定義
    wandbFineTuneTable = wandb.Table(columns=wandbCol1)  # W&Bテーブルを作成
```

---The following area is a Code cell (cell numver is 94)---
```python
if CFG.USE_TRAIN == True:  # GPUサポートが必要
    # LoRAファインチューニング用
    from trl import SFTTrainer  # SFTTrainerをインポート
    from peft import LoraConfig, PeftModel, get_peft_model  # LoRA設定、モデル、取得関数をインポート
    # prepare_model_for_kbit_trainingは非推奨、prepare_model_for_int8_trainingもコメントアウトされている。
```

---The following area is a Markdown cell (cell numver is 95)---
```markdown
# LoRAファインチューニングの設定
```

---The following area is a Code cell (cell numver is 96)---
```python
if CFG.USE_TRAIN:
    # LoRA設定
    lora_config = LoraConfig(
        r=16,  # LoRAのランク
        lora_alpha=32,  # LoRAのスケーリングファクター
        task_type="SEQ_CLS",  # シーケンス分類用
        target_modules=["q_proj", "o_proj", "k_proj", "v_proj",  # ターゲットモジュールのリスト
                        "gate_proj", "up_proj", "down_proj"],
        lora_dropout=0.05,  # LoRAのドロップアウト率
    )
```

---The following area is a Code cell (cell numver is 97)---
```python
# if False: # CFG.USE_TRAIN:
#     model = prepare_model_for_kbit_training(model)  # float32からint8に変換（T4 GPUでメモリ不足に注意）
#     model = get_peft_model(model, lora_config)  # LoRAを用いてモデルを取得
#     model.print_trainable_parameters()  # 学習可能なパラメータを表示する
    
if False:  # True:
    model.config.use_cache = False  # キャッシュの使用を無効化
    model = prepare_model_for_kbit_training(model)  # モデルをkビットトレーニング用に準備
    model = get_peft_model(model, lora_config)  # LoRA設定でモデルを取得
    model.print_trainable_parameters()  # 学習可能なパラメータを表示する
```

---The following area is a Code cell (cell numver is 98)---
```python
model  # 準備したモデルの内容を表示する
```

---The following area is a Code cell (cell numver is 99)---
```python
if CFG.USE_TRAIN:
    # トレーニングデータをトークナイズし、シーケンスを切り捨てるための前処理関数を作成
    def tokenizeProcess(sample):
        return tokenizer(sample["text"], max_length=CFG.maxToken, padding=True, truncation=True)  # テキストをトークナイズし、パディングと切り捨てを適用する
```

---The following area is a Markdown cell (cell numver is 100)---
```markdown
# トレーニング/バリデーションデータセットの作成
```

---The following area is a Code cell (cell numver is 101)---
```python
trainDF.describe()  # トレーニングデータフレームの統計情報を表示する
```

---The following area is a Code cell (cell numver is 102)---
```python
if CFG.USE_TRAIN:
    #     maxTrainData = 300 # 5000 # 10000 # 最大トレーニングデータ数の設定（コメントアウトされている）
    #     maxEvalData = 20 # 100  # 最大評価データ数の設定（コメントアウトされている）
    tempTrainDF = trainDF[:CFG.maxTrainData]  # トレーニングデータのサンプリング
    tempEvalDF = trainDF[CFG.maxTrainData: CFG.maxTrainData + CFG.maxEvalData]  # 評価データのサンプリング
    # HuggingFaceデータセットに変換
    trainDataset = Dataset.from_pandas(tempTrainDF, split="train")  # トレーニングデータセットを作成
    evalDataset = Dataset.from_pandas(tempEvalDF, split="test")  # 評価データセットを作成
```

---The following area is a Code cell (cell numver is 103)---
```python
if CFG.USE_TRAIN:
    print(tempTrainDF["label"].value_counts())  # サンプリングしたトレーニングデータのラベルの出現回数を表示
    print(tempEvalDF["label"].value_counts())  # サンプリングした評価データのラベルの出現回数を表示
```

---The following area is a Markdown cell (cell numver is 104)---
```markdown
### 提出用データセットの作成
```

---The following area is a Code cell (cell numver is 105)---
```python
submitDataset = Dataset.from_pandas(testDF, split="test")  # テストデータフレームから提出用データセットを作成
```

---The following area is a Code cell (cell numver is 106)---
```python
submitDataset  # 提出用データセットの内容を表示する
```

---The following area is a Code cell (cell numver is 107)---
```python
if CFG.USE_TRAIN:
    del tempTrainDF  # サンプリングしたトレーニングデータを削除
    del tempEvalDF  # サンプリングした評価データを削除
    print(len(trainDataset), len(evalDataset))  # トレーニングデータセットと評価データセットの長さを表示
```

---The following area is a Code cell (cell numver is 108)---
```python
if CFG.USE_TRAIN:
    # データセットをDatasetDictに変換
    datasetDict = DatasetDict({
        "train": trainDataset,  # トレーニングデータセットを設定
        'test': evalDataset  # 評価データセットを設定
    })
```

---The following area is a Code cell (cell numver is 109)---
```python
submitDataDict = DatasetDict({
    "test": submitDataset  # 提出用データセットを含むDatasetDictを作成
})
```

---The following area is a Code cell (cell numver is 110)---
```python
submitDataDict  # 提出用データセットの辞書の内容を表示する
```

---The following area is a Code cell (cell numver is 111)---
```python
if CFG.USE_TRAIN:
    print(datasetDict)  # データセット辞書の内容を表示する
```

---The following area is a Code cell (cell numver is 112)---
```python
if CFG.USE_TRAIN:
    # 使用しない列を削除
#     datasetDict = datasetDict.remove_columns(['id', 'model_a', 'model_b', 'response_a', 'response_b'])  # コメントアウトされた行
    datasetDict = datasetDict.remove_columns(['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b',
                                 'winner_model_a', 'winner_model_b', 'winner_tie'])  # 不要な列を削除して新しいデータセット辞書を設定
```

---The following area is a Code cell (cell numver is 113)---
```python
# トレーニングデータセットをトークナイズに変換
if CFG.USE_TRAIN:
    datasetDict = datasetDict.map(tokenizeProcess, batched=True)  # 前処理関数をバッチ処理で適用してトークナイズする
```

---The following area is a Code cell (cell numver is 114)---
```python
# datasetDict["train"].remove_columns(['id', 'model_a', 'model_b', 'winner_model_a', 'winner_model_b', 'winner_tie'])  # トレーニングデータセットから不要な列を削除するコード（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 115)---
```python
datasetDict  # データセット辞書の内容を表示する
```

---The following area is a Markdown cell (cell numver is 116)---
```markdown
## 提出用データセット辞書の変換
```

---The following area is a Code cell (cell numver is 117)---
```python
submitDataDict = submitDataDict.map(tokenizeProcess, batched=True)  # 提出用データセットに前処理関数をバッチ処理で適用してトークナイズする
```

---The following area is a Code cell (cell numver is 118)---
```python
submitDataDict["test"][0]  # 提出用データセットの最初の項目を表示する
```

---The following area is a Code cell (cell numver is 119)---
```python
datasetDict["train"][0]  # トレーニングデータセットの最初の項目を表示する
```

---The following area is a Code cell (cell numver is 120)---
```python
if True:
    datasetDict = datasetDict.rename_column("label", "labels")  # 「label」列の名前を「labels」に変更する
```

---The following area is a Code cell (cell numver is 121)---
```python
datasetDict  # データセット辞書の内容を表示する
```

---The following area is a Code cell (cell numver is 122)---
```python
if CFG.USE_TRAIN:
    accList = []  # 精度リストの初期化
    f1List = []  # F1スコアリストの初期化
    recallList = []  # 再現率リストの初期化
    preciseList = []  # 精度リストの初期化

    def compute_metrics1(pred):
        logits, labels = pred  # 予測とラベルを取得
        predictions = np.argmax(logits, axis=1)  # 予測の最大値インデックスを取得
        return {"accuracy": (predictions == labels).mean()}  # 精度を計算して返す
    
    def compute_metrics2(pred):
        logits, labels = pred  # 予測とラベルを取得
        predictions = np.argmax(logits, axis=-1)  # 予測の最大値インデックスを取得
        accuracy = (predictions == labels).mean()  # 精度を計算
        f1score = f1_score(labels, predictions, average='weighted')  # F1スコアを計算
        recallScore = recall_score(labels, predictions, average='weighted')  # 再現率を計算
        precision = precision_score(labels, predictions, average='weighted')  # 精度を計算
        accList.append(accuracy)  # 精度をリストに追加
        f1List.append(f1score)  # F1スコアをリストに追加
        recallList.append(recallScore)  # 再現率をリストに追加
        preciseList.append(precision)  # 精度をリストに追加
        return {"accuracy": accuracy, "recall": recallScore, "precision": precision, 'f1-score': f1score}  # 各メトリックを返す
    
    def compute_metrics3(eval_preds) -> dict:
        preds = eval_preds.predictions  # 評価結果から予測を取得
        labels = eval_preds.label_ids  # 評価結果からラベルを取得
        probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 確率を計算
        loss = log_loss(y_true=labels, y_pred=probs)  # ロスを計算
        acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算
        return {"acc": acc, "log_loss": loss}  # 精度とロスを返す
    
    def formatFuc(sample):
        text = f"{sample['text']}"  # サンプルのテキストをフォーマット
        return [text]  # テキストをリストとして返す
```

---The following area is a Code cell (cell numver is 123)---
```python
datasetDict["test"]  # テストデータセットの内容を表示する
```

---The following area is a Code cell (cell numver is 124)---
```python
if CFG.USE_TRAIN:
    from transformers import DataCollatorWithPadding  # データコラレーターのインポート

    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)  # パディングを使用するデータコラレーターのインスタンスを作成
    trainArg = TrainingArguments(
        per_device_train_batch_size=1,  # デバイスごとのトレーニングバッチサイズ
        per_device_eval_batch_size=1,  # デバイスごとの評価バッチサイズ
        gradient_accumulation_steps=4,  # 勾配蓄積ステップ数
        eval_accumulation_steps=1,  # 評価の蓄積ステップ数
        warmup_steps=2,  # ウォームアップステップ数
#         max_steps=300,  # トレーニングステップの総数を設定（エポック数を上書き）
        num_train_epochs=1,  # トレーニングのエポック数
        learning_rate=2e-5,  # 学習率
#         evaluation_strategy='steps',  # 古いバージョン
        eval_strategy="steps",  # 評価の戦略
        save_strategy="steps",  # 保存の戦略
        fp16=True,  # 16ビット浮動小数点演算を使用するか
        logging_steps=CFG.loggingSteps,  # ロギングのステップ数
#         save_steps=100,  # 保存ステップ数
        output_dir='/kaggle/working/lora_model',  # 出力先ディレクトリ
        optim="paged_adamw_8bit",  # 最適化手法
        weight_decay=0.01,  # 重み減衰
        load_best_model_at_end=True,  # 最後に最良モデルをロードするか
#         overwrite_output_dir=True,  # 出力ディレクトリを上書きするか
#         label_names=["label"],  # ラベル名（コメントアウトされている）
        report_to=reportTo  # トレーニング中のW&Bトークン要求を回避
    )
    
    trainer = SFTTrainer(
        model=model,  # 使用するモデル
        train_dataset=datasetDict["train"],  # トレーニングデータセット
        eval_dataset=datasetDict["test"],  # 評価データセット
        args=trainArg,  # トレーニング引数
        max_seq_length=2048,  # 最大シーケンス長
        peft_config=lora_config,  # LoRA設定
#         formatting_func=formatFuc,  # フォーマット関数（コメントアウトされている）
        data_collator=data_collator,  # データコラレーター
        compute_metrics=compute_metrics2  # メトリック計算関数
    )
```

---The following area is a Code cell (cell numver is 125)---
```python
clearMemory()  # メモリをクリアする関数を呼び出す
```

---The following area is a Code cell (cell numver is 126)---
```python
device  # 使用しているデバイスを表示する
```

---The following area is a Code cell (cell numver is 127)---
```python
%%time
if CFG.USE_TRAIN:
    print("モデルをトレーニング中")
    trainer.train()  # モデルのトレーニングを実行
#     print("モデルを保存中!")
    # トレーニングした増分の🤗 PEFT重み（adapter_model.bin）だけを保存し、保存、転送、ロードが非常に効率的になります。
#     trainer.model.save_pretrained('fine-tuned-model')  # ファインチューニングしたモデルを保存するコード（現在コメントアウトされている）
```

---The following area is a Code cell (cell numver is 128)---
```python
# トレーニングした増分の🤗 PEFT重み（adapter_model.bin）だけを保存し、保存、転送、ロードが非常に効率的になります。
if CFG.USE_TRAIN:
    print("モデルを保存中!")
    trainer.model.save_pretrained('lora_model')  # モデルを指定したディレクトリに保存する
```

---The following area is a Code cell (cell numver is 129)---
```python
clearMemory()  # メモリをクリアする関数を呼び出す
```

---The following area is a Markdown cell (cell numver is 130)---
```markdown
# 推論テスト
```

---The following area is a Code cell (cell numver is 131)---
```python
def getClassifierOutput(text):
        """
        テキストをLLMモデルに直接送信し、分類出力を取得する関数
        """
        with torch.no_grad():
            inputIds = tokenizer(text, return_tensors="pt").to(device)  # テキストをトークナイズしてテンソル化
            logits = model(**inputIds).logits  # モデルからロジットを取得
            probabilities = nn.functional.softmax(logits, dim=-1)  # ソフトマックス関数で確率を計算
            classID = logits.argmax().item()  # 最大のロジットを持つクラスのIDを取得
            return probabilities, classID  # 確率とクラスIDを返す
```

---The following area is a Code cell (cell numver is 132)---
```python
print(submitDataDict["test"]["text"][0])  # 提出用データセットの最初のテキストを表示する
```

---The following area is a Code cell (cell numver is 133)---
```python
# accList  # 精度リスト（コメントアウトされている）
# f1List  # F1スコアリスト（コメントアウトされている）
# recallList  # 再現率リスト（コメントアウトされている）
# preciseList  # 精度リスト（コメントアウトされている）
```

---The following area is a Code cell (cell numver is 134)---
```python
list(range(1, len(accList) + 1))  # 精度リストの長さに基づいて1からリストを生成する
```

---The following area is a Code cell (cell numver is 135)---
```python
import matplotlib.pyplot as plt  # データの可視化のためにMatplotlibをインポート
```

---The following area is a Code cell (cell numver is 136)---
```python
plt.plot(list(range(1, len(accList) + 1)), accList)  # 精度リストをプロット
plt.xlabel("Step")  # x軸のラベルを設定
plt.ylabel("Accuracy")  # y軸のラベルを設定
plt.title("Accuracy")  # グラフのタイトルを設定
plt.show();  # グラフを表示
```

---The following area is a Code cell (cell numver is 137)---
```python
plt.plot(list(range(1, len(f1List) + 1)), f1List)  # F1スコアリストをプロット
plt.xlabel("Step")  # x軸のラベルを設定
plt.ylabel("F1-Score")  # y軸のラベルを設定
plt.title("F1 Score")  # グラフのタイトルを設定
plt.show();  # グラフを表示
```

---The following area is a Code cell (cell numver is 138)---
```python
plt.plot(list(range(1, len(recallList) + 1)), recallList)  # 再現率リストをプロット
plt.xlabel("Step")  # x軸のラベルを設定
plt.ylabel("Recall")  # y軸のラベルを設定
plt.title("Recall")  # グラフのタイトルを設定
plt.show();  # グラフを表示
```

---The following area is a Code cell (cell numver is 139)---
```python
plt.plot(list(range(1, len(preciseList) + 1)), preciseList)  # 精度リストをプロット
plt.xlabel("Step")  # x軸のラベルを設定
plt.ylabel("Precision")  # y軸のラベルを設定
plt.title("Precision")  # グラフのタイトルを設定
plt.show();  # グラフを表示
```

---The following area is a Code cell (cell numver is 140)---
```python
submitDF  # 提出用データフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 141)---
```python
finalDF = submitDF.copy()  # 提出用データフレームのコピーを作成
finalDF  # コピーしたデータフレームの内容を表示する
```

---The following area is a Code cell (cell numver is 142)---
```python
from torch import nn  # PyTorchのニューラルネットワークモジュールをインポート
```

---The following area is a Code cell (cell numver is 143)---
```python
wina, winb, tie = [], [], []  # 各結果を格納するリストを初期化
for i, text in enumerate(submitDataDict["test"]["text"]):
    probs, classID = getClassifierOutput(text)  # テキストから分類出力を取得
#     print("{}: ", probs.tolist()[0])
    rounded_probs = [round(val, 6) for val in probs.tolist()[0]]  # 確率を小数点以下6桁で丸める
    print(rounded_probs)  # 丸めた確率を表示
    print("Classified ID : ", classID)  # 分類されたIDを表示
    wina.append(rounded_probs[0])  # モデルAの勝率をリストに追加
    winb.append(rounded_probs[1])  # モデルBの勝率をリストに追加
    tie.append(rounded_probs[2])  # タイの勝率をリストに追加
    # 確率を計算
#     print("Probabilities: ", [round(val, 6) for val in probabilities.tolist()[0]])
    # 提出DFに書き込み
#     finalDF.iloc[i, 1:] = [round(val, 6) for val in probabilities.tolist()[0]]  # カラムを更新
finalDF["winner_model_a"] = wina  # モデルAの結果をデータフレームに追加
finalDF["winner_model_b"] = winb  # モデルBの結果をデータフレームに追加
finalDF["winner_tie"] = tie  # タイの結果をデータフレームに追加
```

---The following area is a Code cell (cell numver is 144)---
```python
finalDF.head()  # 最初の5行を表示して、最終データフレームの内容を確認する
```

---The following area is a Code cell (cell numver is 145)---
```python
finalDF.to_csv('submission.csv', index=False)  # 最終データフレームをCSVファイルとして保存する（インデックスなし）
```

---The following area is a Code cell (cell numver is 146)---
```python
# clearMemory()  # メモリをクリアする関数を呼び出すコード（現在コメントアウトされている）
```

---The following area is a Markdown cell (cell numver is 147)---
```markdown
---

# コメント

> ## Toko Fumihiro
> 
> 素晴らしい作品です。私にとって非常に有益なノートブックです。よろしくお願いします、Toko
> 
> 

---
```

** @@@ Jupyter Notebook numver 54, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックでは、4ビット量子化された「Gemma-2 9b Instruct」モデルに基づくLoRAアダプターを使用した推論の手法が示されています。主な目的は、Chatbot Arenaコンペティションにおける、2つの言語モデル間のユーザー応答の選好予測を行うことです。

### 問題の扱い
ノートブックは、LoRAアダプターを用いて、量子化による誤差の影響を軽減しつつ、推論を迅速化する方法を探求しています。モデルをマージすることで誤差が生じる可能性があるため、LoRAアダプターを維持した状態での推論を推奨しています。また、モデルの性能は評価セットでの対数損失が0.9371、公開リーダーボードでの対数損失が0.941であると報告しています。

### 使用ライブラリと手法
ノートブックでは、主に以下のライブラリが使用されています:
- **Transformers**: 「Gemma2ForSequenceClassification」と「GemmaTokenizerFast」を使用してモデルの呼び出しやトークン化を実施。
- **Peft**: LoRAアダプターの適用に用いられます。
- **Torch**: GPU利用や自動混合精度計算をサポートし、モデルの推論が行われます。
- **PandasとNumPy**: データ処理と操作のためにデータフレームを使用し、結果の格納に役立てています。

### データ処理と推論
データはCSVファイルから読み込まれ、テキストの前処理を行った後、トークナイズされます。推論は、2つのGPUを用いてバッチごとに行われ、モデルAとモデルBの各々の勝率を計算します。結果として、モデルごとの勝率や引き分け確率をデータフレームに格納し、最終的に提出用のCSVファイルとして成果物を保存します。

全体として、このノートブックは、量子化されたモデルを最適に利用するための手法と細かな設定を提供し、効率的に予測を行うフレームワークを構築しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## このノートブックについて

これは、4ビット量子化された [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) と、私がアップロードしたスクリプトを使用してトレーニングしたLoRAアダプターを利用した推論ノートブックです [ここ](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune)で確認できます。
LoRAアダプターをベースモデルにマージすることで推論を速くすることもできますが、安易にそうすると無視できない量子化誤差が発生する可能性があります。そのため、私はLoRAアダプターをマージせずに維持することにしました。

## 結果

| サブセット | 対数損失 |
| - | - |
| 評価セット | 0.9371 |
| 公開LB | 0.941 |

提出には約4時間かかります。`max_length=2048`でTTAは使用していません。
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # テスト時のデータ拡張。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するか

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データの読み込みと前処理
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 11)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]  # プロンプトに"<prompt>: "を追加
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]  # 応答Aにプレフィックスを追加
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]  # 応答Bにプレフィックスを追加
    if spread_max_length:  # spread_max_lengthがTrueの場合
        # 各要素をmax_length//3でトークナイズ
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 各リストを結合
        attention_mask = [[1]* len(i) for i in input_ids]  # 各入力の長さに応じたアテンションマスクを作成
    else:
        # 各要素を結合してトークン化
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイズ
        input_ids = tokenized.input_ids  # トークンIDを取得
        attention_mask = tokenized.attention_mask  # アテンションマスクを取得
    return input_ids, attention_mask  # トークンIDとアテンションマスクを返す
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)  # Gemmaトークナイザーを読み込む
tokenizer.add_eos_token = True  # 終了トークンを追加
tokenizer.padding_side = "right"  # パディングの位置を右に設定

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークン化した結果をデータフレームに格納
data["length"] = data["input_ids"].apply(len)  # 各入力の長さを計算して追加

aug_data = pd.DataFrame()  # 拡張データ用のデータフレームを作成
aug_data["id"] = test["id"]
# response_aとresponse_bを入れ替える
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])  # トークナイズして格納
aug_data["length"] = aug_data["input_ids"].apply(len)  # 長さを計算
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # トークナイズしたデータの最初の要素をデコードして表示
```

---The following area is a Code cell (cell numver is 14)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  # 拡張データの最初の要素をデコードして表示
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# モデルを読み込む
```

---The following area is a Code cell (cell numver is 16)---
```python
# GPU 0にベースモデルを読み込む
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# GPU 1にベースモデルを読み込む
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
#### LoRAアダプターを読み込む
```

---The following area is a Code cell (cell numver is 18)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0に適用
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1に適用
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()  # 勾配計算を無効にする
@torch.cuda.amp.autocast()  # 自動混合精度を使って演算を行う
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝率と引き分けを記録するリスト
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終端インデックスを計算
        tmp = df.iloc[start_idx:end_idx]  # データフレームからバッチを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDを取得
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクを取得
        # トークナイザーを使ってデータをパディング
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))  # モデルで出力を計算
        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックスで確率に変換
        
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率をリストに追加
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率をリストに追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率をリストに追加
    
    df["winner_model_a"] = a_win  # モデルAの勝率をデータフレームに追加
    df["winner_model_b"] = b_win  # モデルBの勝率をデータフレームに追加
    df["winner_tie"] = tie  # 引き分けの確率をデータフレームに追加
    
    return df  # 更新されたデータフレームを返す
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()  # 処理開始時間を記録

# 入力の長さでソートして動的パディングを最大限に活用する
data = data.sort_values("length", ascending=False)
# sub_1とsub_2のトークン数がほぼ同じになるように分ける
sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータを選択
sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータを選択

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝率の配列を取得

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:  # TTAが有効な場合
    data = aug_data.sort_values("length", ascending=False)  # 入力の長さでソートしてスピードを向上させる
    sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータ
    sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータ

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

    tta_result_df = pd.concat(list(results), axis=0)  # TTAの結果を結合
    # TTAの順序が反転するので調整
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # 元の結果とTTA結果を平均
    proba = (proba + tta_proba) / 2

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝率を更新
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を更新
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの勝率を更新
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用のデータフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルに保存
display(submission_df)  # 提出データを表示
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
---

# コメント 

> ## Cody_Null
> 
> 推論時間を速めるアイデアはありますか？パフォーマンスを失わずに。
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > LoRA_A x LoRA_Bを最初に計算したときにキャッシュするのは簡単な方法ですが、それほど速度向上は見込めないかもしれません。
> > 
> > TensorRTやvLLMのような最適化ライブラリを使えるのかも気になります。
> > 
> > 
> > > ## Cody_Null
> > > 
> > > vLLMを試したことがありますか？私は試しましたが、うまく動かす方法がわかりませんでした。 
> > > 
> > > 
> > > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > まだ試していません。max_lengthを増やすと対数損失が減少することは認識していますが、2048を越えると改善は非常に小さいです。私のケースでは2048から4096にすると対数損失が0.002減少しました。残りの時間で最適化できる他の方法を検討しています。
> > > 
> > > 

---

> ## carvingfate
> 
> 私は以前30位でしたが、このコードのおかげで私の努力が無駄に思えてしまいます。しかし、共有する精神を尊重しており、これがインターネットの精神であると思います。
> 
> 
> > ## jointcc2
> > 
> > 業界の状況もそうだと思います、一つのモデルが全ての過去の努力を上回りますね。
> > 
> > 

---

> ## Van chrn
> 
> なぜvLLMではなく？それはもっと速いかもしれません！
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私はそれに取り組んでいます！
> > 
> > 
> > > ## Cody_Null
> > > 
> > > この使用方法を実現しましたか？私はvLLMを使ったことがなく、動作しているのを見てみたいです！
> > > 
> > > 

---

> ## Dai LinLing
> 
> 共有してくれてありがとう。これは私にとって非常に助けになり、理解も深まりました。
> 
> 

---

> ## Turbo
> 
> [@emiz6413](https://www.kaggle.com/emiz6413)   ノートブックを共有してくれてありがとう。
> 
> 

---

> ## Vitalii Bozheniuk
> 
> なぜシルバーティアの解決策を公開するのかわかりません。この0.88のノートブックを公開すれば、全員が1位になれるのですか？人々がアイデアやノートブックを共有するのは理解できますが、30位のノートブックを共有するのは意味がありません。競争とチャレンジの雰囲気が消えてしまいます。
> 
> 
> > ## G John Rao
> > 
> > まだ1ヶ月残っていますが、初心者にとってはブーストになります。経験豊富な専門家にとっては、1ヶ月は新しいアイデアを構築したり実装するには十分な時間です。 
> > 
> > 

---

> ## Korey Ma
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！私はいくつかの追加パラメータを微調整し、cv&lb(0.912&0.924)を達成しました。さらに良くするために他のトリックを試したいです😆
> 
> 
> > ## Yichuan Gao
> > 
> > もう少し詳細を共有していただけますか？ハイパーパラメータを調整しているのか、LoRAレイヤーやランクを増やしているのか？
> > 
> > > ## Korey Ma
> > > 
> > > "o_proj"と"gate_proj"を追加しただけです。 
> > > 

---

> ## samson
> 
> かなり良いノートブックを作成されていて、コメントに対する見解も妥当です。
> 
> ですが、なぜ重みを共有したのでしょう？真剣に学びたいと思っている人は、あなたの方法を使ったりそれを自分に適応したりするでしょう。しかし、あなたのせいで100以上のチームが盲目的にコピー+提出することになりました。これにより、中間にいる人々が適切なチームメートを見つけるのは不可能です。
> 
> 

---

> ## superferg
> 
> すごいですね。
> 
> 

---

> ## yuanzhe zhou
> 
> よくやりました！つまり、LLMを使うことが鍵ですか？BERTタイプのモデルは収束するには小さすぎるようです。
> 
> > ## Valentin Werner
> > 
> > まさにそうです。私もLLMがシーケンス生成で十分に微調整されていると思います。AI生成テキストをより認識し、テキストがどうあるべきかに最適化されているため、このタスクに適しています。これにより、基本的に火に火をもって戦うことになります。
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 実際、deberta-v3-smallを完全に微調整したところ、約1.1になりました。
> > > > BERTスタイルのエンコーダアーキテクチャは、理論的にはこれらの分類タスクにより適していると思います。
> > > しかし、あなたが指摘したように、実際にはLLMははるかに大きい（deberta v2 xxlargeは1.5B）ため、過学習を避けることができ、より多くのメモリ容量を持つことができます。
> > > もう一つの理由は、指示微調整という、非常に競技に似たデータを使用しているからかもしれません。
> > > 私はバニラgemma-2-9bでテストしたことはありませんが、どのように動作するかを見るのは興味深いです。
> > > 

---

> ## ano
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！微調整したモデルの検証データとcvスコアについて教えていただけますか？あなたのトレーニングノートブックに基づいて、私は行数を5で割った数に基づいて、約20％のデータを検証用に使用しました。その後、対数損失を計算しましたが、cvスコアは0.9未満でした。明らかに、検証データに間違いがあったため、cvスコアはあなたのトレーニングノートブックで書かれた0.9371よりも低くなりました。微調整モデルの検証データをどのように作成すればよいか教えていただけますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私のトレーニングノートブックで検証データを作成するには、次の行が実行されないことを確認してください。
> > 
> > ```
> > # ds = ds.select(torch.arange(100))
> > 
> > ```
> > 
> > 次に、最後のセルにあるこの行で検証セットを選択する必要があります。
> > 
> > ```
> > ds.select(eval_idx)
> > 
> > ```
> > 
> > > ## ano
> > > 
> > > 返信ありがとうございます。もちろん、データを減らすための行を削除しました。
> > > 
> > > では、検証データはあなたのトレーニングノートブックでのn_splits = 5およびfold_idx = 0で選択されるのですね。うーん、そうすると、私のコードにCVスコアを計算する上での間違いがあったかもしれません。 
> > > > [UPDATE] バグを見つけました。ありがとうございます！
> > > 

---

> ## Guillermo Perez G
> 
> 素晴らしい！しかし、スコアをさらに下げることはできないと思います。それとも、ノートブックの質によるのでしょうか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 現在のトップスコアは0.9未満です。スコアを改善するためのアイデアが残っていると思います。
> > 
> > > ## floriandev
> > > 
> > > Eisukeさん、素晴らしいノートブックをありがとう！あなたのノートブックを使うことで0.9未満になる可能性はありますか？
> > > 

---

> ## Sparsh Tewatia
> 
> 終了までにどれくらいの時間がかかりますか？時間が許せば、LLAMA3とGemma 2の2つのLLMのアンサンブルを行うことができます。
> 
> > ## Lorry Zou
> > 
> > ノートブックでは約4時間と記載されていますので、llama3とgemma2のアンサンブルは実行可能のようです。 
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 私はllama3とgemma2のアンサンブルを9時間以内で実行できました。max_length=2048およびper_device_batch_size=4を使用しました。
> > > 

---

> ## Lorry Zou
> 
> 私が行った全ての作業が無駄になりました…悲しい😅 でも素晴らしい作品です。
> 
> 

---

> ## Sam
> 
> 私はこのノートブックで提供されているのと同じ推論パラメータ（batch_size、max_length）を使ってGemmaモデルを試すことに決めました。（llamaではなくBert-likeモデルを使用している以外はすべて同じです）。このノートブックはT4x2で9時間以上かかっても終わらず、途中で止まってしまいます。
> 
> 25,000の例をGemmaモデルで処理すると、約17時間かかることがわかりました。
> 
> 何が問題かアドバイスをいただけますか？Gemmaモデルの推論を速くするにはどうすればいいですか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > [@andreysemenovmax](https://www.kaggle.com/andreysemenovmax) 
> > 
> > 8ビット重量とfp16アクティベーションでのGemma2 9bの動的量子化を行うと、提出のために4.5時間以内に実行されます。
> > 
> > 

---

> ## capyun007
> 
> Kaggleの初心者で質問があります。次のコードを実行してDataFrameをCSVファイルとして保存した後：
> 
> submission_df.to_csv('submission.csv', index=False)
> 
> submission.csvファイルはどこにありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > ノートブックをコミット&保存すると、出力タブに表示されるはずです。インタラクティブセッションで実行した場合は、作業ディレクトリ（./submission.csv）で確認できます。
> > 
> > > ## capyun007
> > > 
> > > 自分のノートブックをコミット&保存しましたが、出力タブに表示されません。
> > > 

---

> ## kanishka sriramoju
> 
> こんにちは、私はここで初心者です。あなたがKaggle入力ディレクトリから事前にトレーニングされたモデルを読み込んだことを見ました。この競技では、ノートブックが独立した環境で実行され、これらの作成されたディレクトリにアクセスできないということはありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > それらのデータセットは公開にしているので、提出時にはアクセスできるはずです。
> > 
> >
```

** @@@ Jupyter Notebook numver 55, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYSの「Chatbot Arena」コンペティションにおける、2つのチャットボットの応答の優劣を予測する問題に取り組んでいます。具体的には、与えられたプロンプトやチャットボットの応答からキーワードを抽出し、これらの情報を基に予測モデルを構築しています。

主な手法としては、以下の要素が使用されています：

1. **KeyBERTライブラリ**: キーワード抽出のために、KeyBERTを使用しています。このモデルは、文書から重要なキーワードを抽出するためのもので、事前トレーニングされたDistilBERTモデルと組み合わせて利用されています。

2. **PyTorchおよびTransformersライブラリ**: PyTorchを基盤とした深層学習モデルを使用して、提案されたデータセットに対して応答の優劣を予測します。具体的には、RoBERTaモデルを用いてシーケンス分類を行っています。

3. **データの処理**: テストデータを整形し、プロンプトや応答からキーワードを抽出した後、これらのキーワードを新しい特徴量としてデータフレームに追加しています。抽出したキーワードは、モデルの入力として使用されます。

4. **データセットクラスの定義**: BERTに基づくデータセットクラスを定義し、トークン化やパディングなど前処理を行っています。これにより、モデルに与えるためにデータが適切に整形されています。

5. **予測関数の実装**: 除外ファイルからモデルの保存状態を読み込み、テストデータに対して予測を行います。最終的に、モデルAとモデルBの勝者の予測結果を集約し、適切な形式で出力しています。

このNotebookでは、指定されたタスクに対する実行時間の制限を考慮しつつ、ライブラリのインストールやデータ処理を行い、提出用のCSVファイルの生成までを行っています。最終的に、モデルによる予測に基づいた結果を含んだ提出ファイルを作成しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS キーワード torch RoBERTa での提出
インターネットオフ条件で

- https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert <br/>
トレーニングデータの処理

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta <br/>
処理されたトレーニングデータを使用したモデルのトレーニング

- https://www.kaggle.com/code/stpeteishii/download-keybert <br/>
KeyBERTのダウンロード

- https://www.kaggle.com/code/stpeteishii/save-distilbert-base-nli-mean-tokens <br/>
distilbert-base-nli-mean-tokensのダウンロード

- https://www.kaggle.com/code/stpeteishii/lmsys-keywords-torch-roberta-for-submission <br/>
テストデータの処理、推論（このノートブック）
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install keybert --no-index --find-links=file:///kaggle/input/download-keybert
# KeyBERTライブラリをインストールします。
# --no-indexフラグは、PyPIインデックスからの検索を無効にします。
# --find-linksオプションは、指定されたローカルのファイルパスからパッケージをインストールするために使用します。
```

---The following area is a Code cell (cell numver is 3)---
```python
from keybert import KeyBERT
# KeyBERTライブラリからKeyBERTクラスをインポートします。
# KeyBERTは、文書からキーワードを抽出するためのモデルです。
```

---The following area is a Code cell (cell numver is 4)---
```python
import numpy as np 
import pandas as pd 
import os
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import StratifiedKFold
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from tqdm import tqdm
import matplotlib.pyplot as plt 
import transformers
import random
import warnings

warnings.simplefilter('ignore')  # 警告を無視するように設定します。

scaler = torch.cuda.amp.GradScaler()  # 自動混合精度トレーニングのスケーラーを作成します。
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればGPUを使用し、そうでなければCPUを使用します。
device  # 現在のデバイス（GPUまたはCPU）を表示します。
```

---The following area is a Code cell (cell numver is 5)---
```python
def random_seed(SEED):
    # 乱数シードを設定する関数です。
    random.seed(SEED)  # Pythonのrandomモジュールのシードを設定します。
    os.environ['PYTHONHASHSEED'] = str(SEED)  # 環境変数でハッシュシードを設定します。
    np.random.seed(SEED)  # NumPyの乱数シードを設定します。
    torch.manual_seed(SEED)  # PyTorchのCPU乱数シードを設定します。
    torch.cuda.manual_seed(SEED)  # PyTorchのGPU乱数シードを設定します。
    torch.cuda.manual_seed_all(SEED)  # 全てのGPUの乱数シードを設定します。
    torch.backends.cudnn.deterministic = True  # CuDNNの決定論的動作を有効にします。
    
SEED = 508  # 使用する乱数シードを508に設定します。
random_seed(SEED)  # 上記のシードで乱数の再現性を確保します。
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# テストデータの処理
```

---The following area is a Code cell (cell numver is 7)---
```python
from sentence_transformers import SentenceTransformer

# SentenceTransformerライブラリからSentenceTransformerクラスをインポートします。
local_model = SentenceTransformer('/kaggle/input/save-distilbert-base-nli-mean-tokens')  
# 保存したDistilBERTモデルをローカルパスから読み込みます。

modelky = KeyBERT(model=local_model)  
# KeyBERTのインスタンスを作成し、先ほど読み込んだモデルを使用します。これはキーワード抽出のための準備です。
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます。
# encoding='iso-8859-1'という引数は、必要に応じてコメントアウトされています。

# 新しい列をデータフレームに追加し、初期値を'-'に設定します。
test['prompt_kw'] = '-'  
test['res_a_kw'] = '-'  
test['res_b_kw'] = '-'  

# 'prompt'列からトップ5のキーワードを抽出します。
tkw0 = modelky.extract_keywords(test['prompt'], top_n=5)  
# 'response_a'列からトップ10のキーワードを抽出します。
tkw1 = modelky.extract_keywords(test['response_a'], top_n=10)  
# 'response_b'列からトップ10のキーワードを抽出します。
tkw2 = modelky.extract_keywords(test['response_b'], top_n=10)
```

---The following area is a Code cell (cell numver is 9)---
```python
for i, w in enumerate(tkw0): 
    ws = []  # 空のリストを作成してキーワードを保存します。
    for wi in w:
        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合
            ws += [wi[0]]  # キーワードをリストに追加します。
    test.loc[i, 'prompt_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。
    
for i, w in enumerate(tkw1): 
    ws = []  # 空のリストを作成してキーワードを保存します。
    for wi in w:
        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合
            ws += [wi[0]]  # キーワードをリストに追加します。
    test.loc[i, 'res_a_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。

for i, w in enumerate(tkw2): 
    ws = []  # 空のリストを作成してキーワードを保存します。
    for wi in w:
        if '_' not in wi[0]:  # キーワードにアンダースコアが含まれていない場合
            ws += [wi[0]]  # キーワードをリストに追加します。
    test.loc[i, 'res_b_kw'] = ' '.join(ws)  # 抽出したキーワードをスペースで結合し、データフレームに保存します。

# 'res_a_kw'列のキーワードと'prompt_kw'列のキーワードを結合します。
test['res_a_kw'] = test['res_a_kw'] + ' // ' + test['prompt_kw']
# 'res_b_kw'列のキーワードと'prompt_kw'列のキーワードを結合します。
test['res_b_kw'] = test['res_b_kw'] + ' // ' + test['prompt_kw']
test = test.iloc[:, 4:]  # 最初の4列を除外して新しいデータフレームを作成します。
display(test)  # 最終的なデータフレームを表示します。

# test.to_csv('test_key.csv', index=False)  # （コメントアウトされた）データフレームをCSVファイルとして保存する行です。
```

---The following area is a Code cell (cell numver is 10)---
```python
testA = test[['res_a_kw']]  # 'res_a_kw'列のみを抽出して新しいデータフレームtestAを作成します。
testA['label'] = 0  # 新しい列'label'を追加し、すべての値を0に設定します。
testA.columns = ['text', 'label']  # 列名を'text'と'label'に変更します。

testB = test[['res_b_kw']]  # 'res_b_kw'列のみを抽出して新しいデータフレームtestBを作成します。
testB['label'] = 0  # 新しい列'label'を追加し、すべての値を0に設定します。
testB.columns = ['text', 'label']  # 列名を'text'と'label'に変更します。

TEST = pd.concat([testA, testB], axis=0)  # testAとtestBを行方向（axis=0）で結合して新しいデータフレームTESTを作成します。
```

---The following area is a Code cell (cell numver is 11)---
```python
max_sens = 8  # 最大文の数を8に設定します。これはデータの処理やモデル入力の際に使用される可能性があります。
p_test = TEST.reset_index(drop=True)  # TESTデータフレームのインデックスをリセットし、新しいインデックスでデータフレームp_testを作成します。
```

---The following area is a Code cell (cell numver is 12)---
```python
class BERTDataSet(Dataset):
    # BERTに基づくデータセットクラスを定義します。このクラスはPyTorchのDatasetを継承しています。
    
    def __init__(self, sentences, targets):        
        # コンストラクタ：文とターゲットを初期化します。
        self.sentences = sentences  # 文を保存します。
        self.targets = targets  # 対応するターゲットを保存します。
        
    def __len__(self):        
        # データセットの長さ（文の数）を返します。
        return len(self.sentences)
    
    def __getitem__(self, idx):        
        # 指定されたインデックスの文とターゲットを取得します。
        sentence = self.sentences[idx]  
        
        # 文をBERTモデルの入力形式にエンコードします。
        bert_sens = tokenizer.encode_plus(
            sentence,
            add_special_tokens = True,  # 特殊トークンを追加します。
            max_length = max_sens,  # 最大長を設定します。
            pad_to_max_length = True,  # 最大長にパディングを行います。
            return_attention_mask = True  # 注意マスクを返します。
        )

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)  # 入力IDをテンソルに変換します。
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)  # 注意マスクをテンソルに変換します。

        target = torch.tensor(self.targets[idx], dtype=torch.float)  # 対応するターゲットをテンソルに変換します。
        
        return {
            'ids': ids,  # 入力IDを戻します。
            'mask': mask,  # 注意マスクを戻します。
            'targets': target  # ターゲットを戻します。
        }
```

---The following area is a Code cell (cell numver is 13)---
```python
test_dataset = BERTDataSet(p_test["text"], p_test["label"])  
# p_testの'text'と'label'を使用してBERTDataSetのインスタンスを作成します。このデータセットはテストデータを扱います。

test_batch = 32  # テストデータのバッチサイズを32に設定します。
test_dataloader = DataLoader(test_dataset, batch_size=test_batch, shuffle=False, num_workers=8, pin_memory=True)  
# DataLoaderを使用して、テストデータをバッチ処理します。
# - num_workers=8: データの読み込みを並列で処理するために8つのワーカーを使用します。
# - pin_memory=True: GPUへのデータ転送の効率を上げるためにメモリをピン留めします。
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 予測を行う関数
保存したモデルを使用します
```

---The following area is a Code cell (cell numver is 15)---
```python
# モデルの初期化
tokenizer = transformers.RobertaTokenizer.from_pretrained("/kaggle/input/roberta-base")  
# 指定したパスからRoBERTaトークナイザーを読み込みます。

model = transformers.RobertaForSequenceClassification.from_pretrained("/kaggle/input/roberta-base", num_labels=1)  
# 指定したパスからRoBERTaモデルを読み込み、性質分類のためのモデルを作成します。
# num_labels=1: 出力ラベルの数を1に設定します。

# 指定したディレクトリ内の.pthファイルのパスをリストとして取得します。
pths = [os.path.join("/kaggle/input/lmsys-keywords-torch-roberta", s) for s in os.listdir("/kaggle/input/lmsys-keywords-torch-roberta") if ".pth" in s]
print(pths)  # 取得したパスを表示します。
```

---The following area is a Code cell (cell numver is 16)---
```python
def predicting(
    test_dataloader,
    model,
    pths 
):
    # モデルを使用して予測を行う関数です。
    allpreds = []  # すべての予測結果を保存するリストです。    
    for pth in pths:  # 指定された各モデルのパスについてループします。  
        state = torch.load(pth, map_location=torch.device('cpu'))  # モデルの状態をロードします。
        model.load_state_dict(state["state_dict"])  # モデルにロードした状態を設定します。
        model.to(device)  # モデルをデバイス（GPUまたはCPU）に移動します。
        model.eval()  # モデルを評価モードに設定します。これにより、ドロップアウトやバッチ正規化が無効になります。      
        
        preds = []  # 各モデルの予測結果を保存するリストです。
        allvalloss = 0  # オプションで全体損失を蓄積するための変数ですが、現時点では使用されていません。

        with torch.no_grad():  # 勾配計算を無効にします（メモリの節約と計算速度の向上）。
            for a in test_dataloader:  # テストデータローダーからバッチを繰り返します。
                ids = a["ids"].to(device)  # バッチの入力IDをデバイスに移動します。
                mask = a["mask"].to(device)  # バッチの注意マスクをデバイスに移動します。
                output = model(ids, mask)  # モデルに入力を渡し、出力を取得します。
                output = output["logits"].squeeze(-1)  # 出力からロジットを取得し、次元を縮小します。
                preds.append(output.cpu().numpy())  # 予測をCPUに移動し、NumPy配列として追加します。

            preds = np.concatenate(preds)  # すべてのバッチの予測を結合します。           
            allpreds.append(preds)  # モデルによる予測をリストに追加します。

    return allpreds  # すべてのモデルからの予測結果を返します。
```

---The following area is a Code cell (cell numver is 17)---
```python
tpreds = predicting(test_dataloader, model, pths)  
# テストデータローダー、モデル、およびモデルのパスを使って予測を行います。
# 予測結果はtpredsに保存されます。
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# 予測結果
```

---The following area is a Code cell (cell numver is 19)---
```python
test_pred = []  # 予測結果を保存するための空のリストを作成します。
for p in tpreds[0]:  # 最初のモデルの予測結果についてループします。
    test_pred += [p]  # それぞれの予測をリストに追加します。
```

---The following area is a Code cell (cell numver is 20)---
```python
submit = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  
# サンプル提出ファイルをCSVから読み込みます。

submit['winner_model_a'] = test_pred[0:len(test)]  # モデルAの勝者の予測を提出データフレームに追加します。
submit['winner_model_b'] = test_pred[len(test):]  # モデルBの勝者の予測を提出データフレームに追加します。

pa = submit['winner_model_a']  # モデルAの予測結果を変数に保存します。
pb = submit['winner_model_b']  # モデルBの予測結果を変数に保存します。

# モデルAとモデルBの予測の合計をクリップして、0から1の範囲に制限します（この値を勝者が引き分けた場合のための列に使用します）。
submit['winner_tie'] = np.clip((pa + pb), 0, 1)  
display(submit)  # 最終的な提出データフレームを表示します。

# 提出ファイルをCSV形式で保存します。
submit.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 56, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションにおけるチャットボットの応答の好みを予測するための機械学習モデルの開発に取り組んでいます。具体的には、RoBERTaという事前学習済みモデルを用いて、与えられたテキストに対してラベルを予測するタスクを実行しています。

## 問題の概要
Notebookは、チャットボットからの応答を基にしたユーザーの好みを予測するためのモデルを構築することを目的としています。具体的には、トレーニングデータには、各モデルの応答が含まれており、どちらのモデルが好まれるかを予測するためのラベルが与えられています。

## 使用している手法およびライブラリ
1. **ライブラリ**:
   - NumPy: 数値計算
   - pandas: データ操作
   - scikit-learn: モデルの評価（平均二乗誤差など）やデータの分割
   - PyTorch: ニューラルネットワークの構築
   - transformers: 事前学習済みのRoBERTaモデルの使用
   - matplotlib: データの可視化
   - tqdm: プログレスバーの表示

2. **手法**:
   - **データの準備**: 訓練データとテストデータの読み込みと前処理を行い、層化K分割交差検証を使用してデータを分割。
   - **トークナイゼーション**: RoBERTaトークナイザーを用いてテキストデータをトークン化し、モデルに入力できる形式に変換。
   - **モデルの構築**: PyTorchを使用してRoBERTaモデルベースの分類器を構築し、訓練データに基づいて学習。
   - **訓練と評価**: モデルを訓練し（自動混合精度を使用）、検証データで評価するルーチンを実装。
   - **予測**: テストデータに対して予測を実行し、最終的な結果を提出用の形式に整形。

このNotebookは、RoBERTaを利用した深層学習を通じて、ユーザーの好みを予測するモデルを開発するための一連のプロセスを詳しく示しています。最終的に得られた予測結果を元に提出ファイルを生成する準備を行っています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS キーワード torch RoBERTa
インターネット上の条件の下で

https://www.kaggle.com/code/stpeteishii/lmsys-prompt-response-words-keybert
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install chardet  # chardetライブラリをインストールします。 これは、文字エンコーディングを自動的に検出するために使用されます。
```

---The following area is a Code cell (cell numver is 3)---
```python
# debug = False  # デバッグモードを無効にします。デバッグ情報は表示されません。
# debug2 = False  # さらなるデバッグモードを無効にします。追加のデバッグ情報は表示されません。
```

---The following area is a Code cell (cell numver is 4)---
```python
import numpy as np  # NumPyライブラリをインポートします。数値計算に使います。
import pandas as pd  # pandasライブラリをインポートします。データ操作に使います。
import os  # osモジュールをインポートします。ファイルやディレクトリ操作に使います。
from sklearn.metrics import mean_squared_error  # 平均二乗誤差を計算するための関数をインポートします。
from sklearn.model_selection import StratifiedKFold  # 層化K分割交差検証を行うためのクラスをインポートします。
import torch  # PyTorchライブラリをインポートします。深層学習に使います。
import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポートします。
from torch.utils.data import DataLoader, Dataset  # データローダーとデータセットのクラスをインポートします。
from tqdm import tqdm  # 進捗バーを表示するためのtqdmをインポートします。
import matplotlib.pyplot as plt  # グラフ描画のためのmatplotlibをインポートします。
import transformers  # transformersライブラリをインポートします。事前学習済みモデルにアクセスするために使用します。
from transformers import AutoTokenizer, AutoModelForSequenceClassification  # トークナイザーと分類用モデルをインポートします。
import random  # ランダム数生成のためのrandomモジュールをインポートします。
import chardet  # 文字エンコーディングを自動的に検出するためのchardetをインポートします。
import warnings  # 警告メッセージの制御のためにwarningsモジュールをインポートします。
warnings.simplefilter('ignore')  # 警告を無視するフィルターを設定します。
scaler = torch.cuda.amp.GradScaler()  # 自動混合精度トレーニングに使うスケーラーを初期化します。
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # 使用可能なデバイスを確認し、CUDA対応GPUがあればそれを使います。
device  # 現在使用しているデバイスを表示します。
```

---The following area is a Code cell (cell numver is 5)---
```python
def random_seed(SEED):  # ランダムシードを設定する関数を定義します。
    
    random.seed(SEED)  # randomモジュールのシードを設定します。
    os.environ['PYTHONHASHSEED'] = str(SEED)  # 環境変数にシードを設定します。ハッシュ値の決定に影響します。
    np.random.seed(SEED)  # NumPyの乱数生成器のシードを設定します。
    torch.manual_seed(SEED)  # PyTorch CPUのシードを設定します。
    torch.cuda.manual_seed(SEED)  # PyTorchのCUDA GPUのシードを設定します。
    torch.cuda.manual_seed_all(SEED)  # 全てのCUDAデバイスにシードを設定します。
    torch.backends.cudnn.deterministic = True  # cuDNNの決定論的動作を有効にします。これにより再現性が向上します。
    
SEED = 508  # 使用するシード値を設定します。
random_seed(SEED)  # 定義した関数を呼び出してシードを設定します。
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# ラベルはランキングです
```

---The following area is a Code cell (cell numver is 7)---
```python
train0 = pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/train_key.csv')  # 訓練データをCSVファイルから読み込みます。
train0 = train0[0:20000]  # 訓練データの最初の20,000行を取得します。
display(train0)  # 訓練データを表示します。
print(train0.columns.tolist())  # 訓練データのカラム名をリスト形式で表示します。

test0 = pd.read_csv('/kaggle/input/lmsys-prompt-response-words-keybert/test_key.csv')  # テストデータをCSVファイルから読み込みます。
display(test0)  # テストデータを表示します。
print(test0.columns.tolist())  # テストデータのカラム名をリスト形式で表示します。
```

---The following area is a Code cell (cell numver is 8)---
```python
trainA = train0[['res_a_kw', 'winner_model_a']]  # 訓練データからモデルAに関するカラムを選択します。
trainA.columns = ['text', 'label']  # カラム名を 'text' と 'label' に変更します。
trainB = train0[['res_b_kw', 'winner_model_b']]  # 訓練データからモデルBに関するカラムを選択します。
trainB.columns = ['text', 'label']  # カラム名を 'text' と 'label' に変更します。
data = pd.concat([trainA, trainB], axis=0)  # モデルAとモデルBのデータを縦に結合します。

testA = test0[['res_a_kw']]  # テストデータからモデルAに関するカラムを選択します。
testA['label'] = 0  # モデルAのラベルを0に設定します。
testA.columns = ['text', 'label']  # カラム名を 'text' と 'label' に変更します。
testB = test0[['res_b_kw']]  # テストデータからモデルBに関するカラムを選択します。
testB['label'] = 0  # モデルBのラベルを0に設定します。
testB.columns = ['text', 'label']  # カラム名を 'text' と 'label' に変更します。
TEST = pd.concat([testA, testB], axis=0)  # モデルAとモデルBのテストデータを縦に結合します。
```

---The following area is a Code cell (cell numver is 9)---
```python
from sklearn.model_selection import train_test_split  # データを訓練セットとテストセットに分割するための関数をインポートします。
train, test = train_test_split(data, test_size=0.2, random_state=42)  # データを80%の訓練セットと20%のテストセットに分割します。random_stateを設定することで再現性を持たせます。
```

---The following area is a Code cell (cell numver is 10)---
```python
# tokenizer = transformers.BertTokenizer.from_pretrained("../input/bert-base-uncased")  # BERTのトークナイザーを事前学習済みモデルから読み込みます。パスは指定された場所に応じて設定します。
tokenizer = transformers.AutoTokenizer.from_pretrained("roberta-base")  # RoBERTaのトークナイザーを事前学習済みモデルから読み込みます。
```

---The following area is a Code cell (cell numver is 11)---
```python
test_s = train['text'].iloc[0]  # 訓練データから最初のテキストを取得します。
result1 = tokenizer.encode_plus(test_s)  # テキストをトークン化し、入力IDと注意マスクなどを生成します。
tokenizer.decode(result1["input_ids"])  # トークン化された入力IDをデコードして元のテキストに戻します。
```

---The following area is a Code cell (cell numver is 12)---
```python
len(test_s.split(" "))  # テキストをスペースで分割し、単語の数をカウントします。
```

---The following area is a Code cell (cell numver is 13)---
```python
result2 = tokenizer.encode_plus(  # テキストをトークン化してエンコードします。
    test_s,  # エンコードするテキスト
    add_special_tokens=True,  # 特殊トークン（[CLS]や[SEP]）を追加します。
    max_length=20,  # 最大トークン数を20に設定します。
    pad_to_max_length=True,  # 最大トークン数に満たない場合はパディングを追加します。
    truncation=True  # 最大長を超える場合はトークンを切り捨てます。
)  # エンコードされた結果をresult2に格納します。
```

---The following area is a Code cell (cell numver is 14)---
```python
tokenizer.decode(result2["input_ids"])  # エンコードされた入力IDをデコードして、人間が読める形式のテキストに戻します。
```

---The following area is a Code cell (cell numver is 15)---
```python
max_sens = 20  # 最大文の長さを20に設定します。

train = train.sort_values("label").reset_index(drop=True)  # ラベルに基づいて訓練データをソートし、インデックスをリセットします。

train["kfold"] = train.index % 5  # K-Foldクロスバリデーションのために0から4のインデックスを生成します。

p_train = train[train["kfold"] != 0].reset_index(drop=True)  # K-Foldのインデックスが0でない訓練データを取得します。
p_valid = train[train["kfold"] == 0].reset_index(drop=True)  # K-Foldのインデックスが0の訓練データを取得し、検証データとします。

p_test = TEST.reset_index(drop=True)  # テストデータのインデックスをリセットします。
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
'>>token_type_ids' はRoBERTa/DeBERTaでは必要ありません。
```

---The following area is a Code cell (cell numver is 17)---
```python
class BERTDataSet(Dataset):  # BERT用のデータセットクラスを定義します。
    
    def __init__(self, sentences, targets):  # コンストラクタで文とターゲットを受け取ります。        
        self.sentences = sentences  # 文をインスタンス変数に格納します。
        self.targets = targets  # ターゲットをインスタンス変数に格納します。
        
    def __len__(self):  # データセットの長さを返すメソッドを定義します。        
        return len(self.sentences)  # 文の数を返します。
    
    def __getitem__(self, idx):  # インデックス指定でデータを取得するメソッドを定義します。        
        sentence = self.sentences[idx]  # 指定されたインデックスの文を取得します。    
        bert_sens = tokenizer.encode_plus(  # 文をトークン化し、BERT用の形式に変換します。
                                sentence,
                                add_special_tokens=True,  # 特殊トークンを追加します。
                                max_length=max_sens,  # 最大長を設定します。
                                pad_to_max_length=True,  # 最大長にパディングします。
                                return_attention_mask=True)  # 注意マスクも返します。

        ids = torch.tensor(bert_sens['input_ids'], dtype=torch.long)  # 入力IDをテンソルに変換します。
        mask = torch.tensor(bert_sens['attention_mask'], dtype=torch.long)  # 注意マスクをテンソルに変換します。

        target = torch.tensor(self.targets[idx], dtype=torch.float)  # ターゲットをテンソルに変換します。
        
        return {  # 辞書形式でデータを返します。
                'ids': ids,
                'mask': mask,
                'targets': target
            }
```

---The following area is a Code cell (cell numver is 18)---
```python
train_dataset = BERTDataSet(p_train["text"], p_train["label"])  # 訓練データセットをBERTDataSetクラスを使って作成します。
valid_dataset = BERTDataSet(p_valid["text"], p_valid["label"])  # 検証データセットを作成します。
test_dataset = BERTDataSet(p_test["text"], p_test["label"])  # テストデータセットを作成します。

train_batch = 16  # 訓練時のバッチサイズを16に設定します。
valid_batch = 32  # 検証時のバッチサイズを32に設定します。
test_batch = 32   # テスト時のバッチサイズを32に設定します。

train_dataloader = DataLoader(train_dataset, batch_size=train_batch, shuffle=True, num_workers=8, pin_memory=True)  # 訓練データローダーを作成します。
valid_dataloader = DataLoader(valid_dataset, batch_size=valid_batch, shuffle=False, num_workers=8, pin_memory=True)  # 検証データローダーを作成します。
test_dataloader = DataLoader(test_dataset, batch_size=test_batch, shuffle=False, num_workers=8, pin_memory=True)  # テストデータローダーを作成します。
```

---The following area is a Code cell (cell numver is 19)---
```python
model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)  # RoBERTaの事前学習済みモデルを読み込み、シーケンス分類用に設定します。ラベル数は1です。
# model = transformers.BertForSequenceClassification.from_pretrained("../input/bert-base-uncased", num_labels=1)  # BERTの事前学習済みモデルを読み込むためのコメントアウトされたコードです。こちらもラベル数は1です。
```

---The following area is a Code cell (cell numver is 20)---
```python
model.to(device)  # モデルを指定したデバイス（GPUまたはCPU）に移動させます。
model.train()  # モデルを訓練モードに設定します。これにより、ドロップアウトなどの訓練専用の機能が有効になります。
```

---The following area is a Code cell (cell numver is 21)---
```python
for a in train_dataloader:  # 訓練データローダーからデータを取得するループを開始します。
    ids = a["ids"].to(device)  # バッチ内の入力IDをデバイスに移動させます。
    mask = a["mask"].to(device)  # バッチ内の注意マスクをデバイスに移動させます。
    output = model(ids, mask)  # モデルにIDとマスクを入力し、出力を取得します。
    break  # 一度のループで処理を止めます。 (デバッグ目的についての一時停止)
```

---The following area is a Code cell (cell numver is 22)---
```python
output = output["logits"].squeeze(-1).shape  # モデルの出力からlogitsを抽出し、次元を1つ削減して、出力の形状を取得します。
```

---The following area is a Code cell (cell numver is 23)---
```python
from transformers import AdamW  # AdamWオプティマイザをインポートします。
LR = 2e-5  # 学習率を2e-5に設定します。
optimizer = AdamW(model.parameters(), LR, betas=(0.9, 0.999), weight_decay=1e-2)  # モデルのパラメータに対してAdamWオプティマイザを初期化します。ベータ値と重み減衰率も設定します。
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# エポック数を設定します
```

---The following area is a Code cell (cell numver is 25)---
```python
from transformers import get_linear_schedule_with_warmup  # ウォームアップ付きの線形スケジューラをインポートします。
epochs = 30  # エポック数を30に設定します。
# if debug:
#     epochs = 1  # デバッグモードの場合、エポック数を1に設定します（コメントアウトされています）。
train_steps = int(len(p_train) / train_batch * epochs)  # 訓練ステップ数を計算します。
print(train_steps)  # 計算された訓練ステップ数を表示します。
num_steps = int(train_steps * 0.1)  # ウォームアップ期間中のステップ数を全体の10%として計算します。
scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)  # オプティマイザ用にウォームアップ付きの線形スケジューラを初期化します。
```

---The following area is a Code cell (cell numver is 26)---
```python
def loss_fn(output, target):  # 出力とターゲットに基づいて損失を計算する関数を定義します。
    return torch.sqrt(nn.MSELoss()(output, target))  # 平均二乗誤差（MSE）を計算し、それに平方根を取ってロスを返します。これはRMSE（Root Mean Squared Error）を求めるためです。
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
# トレーニング（訓練）関数の定義
```

---The following area is a Code cell (cell numver is 28)---
```python
def training(  # 訓練を行う関数を定義します。
    train_dataloader,  # 訓練データローダー
    model,  # モデル
    optimizer,  # オプティマイザ
    scheduler  # スケジューラ
):
    
    model.train()  # モデルを訓練モードに設定します。
    torch.backends.cudnn.benchmark = True  # cuDNNのベンチマークを有効にします。動的な最適化のために使用されます。
    allpreds = []  # すべての予測を格納するリスト
    alltargets = []  # すべてのターゲットを格納するリスト

    for a in train_dataloader:  # 訓練データローダーからデータを取得します。

        losses = []  # 各バッチの損失を格納するリスト
        optimizer.zero_grad()  # 勾配を初期化します。

        with torch.cuda.amp.autocast():  # 自動混合精度を使用して計算します。
            
            ids = a["ids"].to(device, non_blocking=True)  # バッチ内の入力IDをデバイスに移動させます。
            mask = a["mask"].to(device, non_blocking=True)  # バッチ内の注意マスクをデバイスに移動させます。

            output = model(ids, mask)  # モデルにIDとマスクを入力し、出力を取得します。
            output = output["logits"].squeeze(-1)  # 出力からlogitsを抽出し、次元を1つ削減します。
            target = a["targets"].to(device, non_blocking=True)  # バッチ内のターゲットをデバイスに移動させます。
            loss = loss_fn(output, target)  # 出力とターゲットを用いて損失を計算します。

            losses.append(loss.item())  # 損失を記録します。
            allpreds.append(output.detach().cpu().numpy())  # 予測をCPUに移動させるとともにリストに追加します。
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())  # ターゲットをCPUに移動させ、リストに追加します。

        scaler.scale(loss).backward()  # 損失をスケールし、逆伝播を行います。
        scaler.step(optimizer)  # オプティマイザでステップ実行します。
        scaler.update()  # スケーラーを更新します。
        
        del loss  # 使用した損失のメモリを解放します。

        scheduler.step()  # スケジューラを1ステップ進めます。

    allpreds = np.concatenate(allpreds)  # すべての予測を結合します。
    alltargets = np.concatenate(alltargets)  # すべてのターゲットを結合します。
    losses = np.mean(losses)  # すべての損失の平均を計算します。
    train_rme_loss = np.sqrt(mean_squared_error(alltargets, allpreds))  # RMSEを計算します。

    return losses, train_rme_loss  # 損失とRMSEを返します。
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
# 検証（バリデーション）関数の定義
```

---The following area is a Code cell (cell numver is 30)---
```python
def validating(valid_dataloader, model):  # 検証を行う関数を定義します。
    
    model.eval()  # モデルを評価モードに設定します。
    allpreds = []  # すべての予測を格納するリスト
    alltargets = []  # すべてのターゲットを格納するリスト

    for a in valid_dataloader:  # 検証データローダーからデータを取得します。
        losses = []  # 各バッチの損失を格納するリスト
        with torch.no_grad():  # 勾配計算を行わないようにします。

            ids = a["ids"].to(device)  # バッチ内の入力IDをデバイスに移動させます。
            mask = a["mask"].to(device)  # バッチ内の注意マスクをデバイスに移動させます。

            output = model(ids, mask)  # モデルにIDとマスクを入力し、出力を取得します。
            output = output["logits"].squeeze(-1)  # 出力からlogitsを抽出し、次元を1つ削減します。
            target = a["targets"].to(device)  # バッチ内のターゲットをデバイスに移動させます。
            loss = loss_fn(output, target)  # 出力とターゲットを用いて損失を計算します。
            losses.append(loss.item())  # 損失を記録します。
            allpreds.append(output.detach().cpu().numpy())  # 予測をCPUに移動させるとともにリストに追加します。
            alltargets.append(target.detach().squeeze(-1).cpu().numpy())  # ターゲットをCPUに移動させ、リストに追加します。
            
            del loss  # 使用した損失のメモリを解放します。

    allpreds = np.concatenate(allpreds)  # すべての予測を結合します。
    alltargets = np.concatenate(alltargets)  # すべてのターゲットを結合します。
    losses = np.mean(losses)  # すべての損失の平均を計算します。
    valid_rme_loss = np.sqrt(mean_squared_error(alltargets, allpreds))  # RMSEを計算します。

    return allpreds, losses, valid_rme_loss  # 予測、損失、RMSEを返します。
```

---The following area is a Markdown cell (cell numver is 31)---
```markdown
if debug2 == False:  # debug2がFalseの場合の処理
        for a in range(epochs):  # エポック数分のループを開始します。
            for b in train_dataloader:  # 訓練データローダーからデータを取得します。
                break  # 一度のループで処理を止めます。 (デバッグ目的についての一時停止)

        losses, train_rme_loss = training(train_dataloader, model, optimizer, scheduler)  # 訓練を行い、損失とRMSEを取得します。

        for a in valid_dataloader:  # 検証データローダーからデータを取得します。
            break  # 一度のループで処理を止めます。 (デバッグ目的についての一時停止)

# 訓練と検証を行います
```

---The following area is a Code cell (cell numver is 32)---
```python
trainlosses = []  # 訓練時の損失を格納するリスト
vallosses = []  # 検証時の損失を格納するリスト
bestscore = None  # 最良スコアを初期化します。
trainscores = []  # 訓練スコアを格納するリスト
validscores = []  # 検証スコアを格納するリスト

for epoch in tqdm(range(epochs)):  # エポック数分のループを進行状況とともに実行します。
    
    print("---------------" + str(epoch) + " start -------------")  # 現在のエポックを表示します。
    
    trainloss, trainscore = training(train_dataloader, model, optimizer, scheduler)  # 訓練を行い、損失とスコアを取得します。
    trainlosses.append(trainloss)  # 訓練損失をリストに追加します。
    trainscores.append(trainscore)  # 訓練スコアをリストに追加します。
    
    print("trainscore is " + str(trainscore))  # 訓練スコアを表示します。
    
    preds, validloss, valscore = validating(valid_dataloader, model)  # 検証を行い、予測、損失とスコアを取得します。
    vallosses.append(validloss)  # 検証損失をリストに追加します。
    validscores.append(valscore)  # 検証スコアをリストに追加します。
    
    print("valscore is " + str(valscore))  # 検証スコアを表示します。
    
    if bestscore is None:  # 最良スコアが未設定の場合
        bestscore = valscore  # 最良スコアを更新します。
        
        print("Save first model")  # 最初のモデルを保存します。
        
        state = {  # モデルの状態を辞書に格納します。
                        'state_dict': model.state_dict(),  # モデルの状態辞書
                        'optimizer_dict': optimizer.state_dict(),  # オプティマイザの状態辞書
                        "bestscore": bestscore  # 最良スコア
                    }
            
        torch.save(state, "model0.pth")  # モデルをファイルに保存します。
        
    elif bestscore > valscore:  # 新しいスコアが最良スコアよりも良い場合
        bestscore = valscore  # 最良スコアを更新します。        
        print("found better point")  # より良いポイントが見つかったことを表示します。        
        state = {  # モデルの状態を辞書に格納します。
                        'state_dict': model.state_dict(),  # モデルの状態辞書
                        'optimizer_dict': optimizer.state_dict(),  # オプティマイザの状態辞書
                        "bestscore": bestscore  # 最良スコア
                    }
            
        torch.save(state, "model0.pth")  # モデルをファイルに保存します。
        
    else:  # それ以外の場合
        pass  # 何もしません。
```

---The following area is a Code cell (cell numver is 33)---
```python
plt.scatter(p_valid['label'], preds, alpha=0.2)  # 検証データの実際のラベルと予測値を散布図で表示します。
plt.title('Validation Prediction Result')  # グラフのタイトルを設定します。
plt.xlabel('Actual')  # x軸のラベルを設定します。
plt.ylabel('Prediction')  # y軸のラベルを設定します。
plt.show()  # グラフを表示します。

x = np.arange(epochs)  # エポック数の範囲を生成します。
plt.title('Validation Losses')  # グラフのタイトルを設定します。
plt.xlabel('Epoch')  # x軸のラベルを設定します。
plt.ylabel('Loss')  # y軸のラベルを設定します。
plt.plot(x, trainlosses)  # 訓練損失をプロットします。
plt.plot(x, vallosses)  # 検証損失をプロットします。
plt.show()  # グラフを表示します。

x = np.arange(epochs)  # エポック数の範囲を生成します。
plt.title('Validation Scores')  # グラフのタイトルを設定します。
plt.xlabel('Epoch')  # x軸のラベルを設定します。
plt.ylabel('Score')  # y軸のラベルを設定します。
plt.plot(x, trainscores)  # 訓練スコアをプロットします。
plt.plot(x, validscores)  # 検証スコアをプロットします。
plt.show()  # グラフを表示します。
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# モデルの保存
```

---The following area is a Code cell (cell numver is 35)---
```python
bestscores = []  # 最良スコアを格納するリストを初期化します。
bestscores.append(bestscore)  # 最初の最良スコアをリストに追加します。

for fold in range(1, 5):  # 1から4までのフォールドについてループします。
    
    # データを初期化します。
    p_train = train[train["kfold"] != fold].reset_index(drop=True)  # 現在のフォールド以外のデータを訓練データに設定します。
    p_valid = train[train["kfold"] == fold].reset_index(drop=True)  # 現在のフォールドのデータを検証データに設定します。

    train_dataset = BERTDataSet(p_train["text"], p_train["label"])  # 訓練データセットの作成。
    valid_dataset = BERTDataSet(p_valid["text"], p_valid["label"])  # 検証データセットの作成。
    
    model = transformers.AutoModelForSequenceClassification.from_pretrained("roberta-base", num_labels=1)  # RoBERTaの事前学習済みモデルを読み込みます。
    
    model.to(device)  # モデルを指定したデバイスに移動させます。
    LR = 2e-5  # 学習率を設定します。
    optimizer = AdamW(model.parameters(), LR, betas=(0.9, 0.999), weight_decay=1e-2)  # AdamWオプティマイザを初期化します。
    train_steps = int(len(p_train) / train_batch * epochs)  # 訓練ステップ数を計算します。
    num_steps = int(train_steps * 0.1)  # ウォームアップ期間中のステップ数を全体の10%として計算します。
    scheduler = get_linear_schedule_with_warmup(optimizer, num_steps, train_steps)  # スケジューラを初期化します。

    trainlosses = []  # 訓練損失を格納するリスト
    vallosses = []  # 検証損失を格納するリスト
    bestscore = None  # 最良スコアを初期化します。
    trainscores = []  # 訓練スコアを格納するリスト
    validscores = []  # 検証スコアを格納するリスト

    for epoch in tqdm(range(epochs)):  # エポック数分のループを進行状況とともに実行します。

        print("---------------" + str(epoch) + " start -------------")  # 現在のエポックを表示します。

        trainloss, trainscore = training(train_dataloader, model, optimizer, scheduler)  # 訓練を行い、損失とスコアを取得します。
        trainlosses.append(trainloss)  # 訓練損失をリストに追加します。
        trainscores.append(trainscore)  # 訓練スコアをリストに追加します。

        print("trainscore is " + str(trainscore))  # 訓練スコアを表示します。

        preds, validloss, valscore = validating(valid_dataloader, model)  # 検証を行い、予測、損失とスコアを取得します。
        vallosses.append(validloss)  # 検証損失をリストに追加します。
        validscores.append(valscore)  # 検証スコアをリストに追加します。

        print("valscore is " + str(valscore))  # 検証スコアを表示します。

        if bestscore is None:  # 最良スコアが未設定の場合
            bestscore = valscore  # 最良スコアを更新します。

            print("Save first model")  # 最初のモデルを保存します。

            state = {  # モデルの状態を辞書に格納します。
                            'state_dict': model.state_dict(),  # モデルの状態辞書
                            'optimizer_dict': optimizer.state_dict(),  # オプティマイザの状態辞書
                            "bestscore": bestscore  # 最良スコア
                        }

            torch.save(state, "model" + str(fold) + ".pth")  # モデルをファイルに保存します。 

        elif bestscore > valscore:  # 新しいスコアが最良スコアよりも良い場合
            bestscore = valscore  # 最良スコアを更新します。
            print("found better point")  # より良いポイントが見つかったことを表示します。

            state = {  # モデルの状態を辞書に格納します。
                            'state_dict': model.state_dict(),  # モデルの状態辞書
                            'optimizer_dict': optimizer.state_dict(),  # オプティマイザの状態辞書
                            "bestscore": bestscore  # 最良スコア
                        }
            torch.save(state, "model" + str(fold) + ".pth")  # モデルをファイルに保存します。

        else:  # それ以外の場合
            pass  # 何もしません。

    bestscores.append(bestscore)  # 最良スコアをリストに追加します。
```

---The following area is a Code cell (cell numver is 36)---
```python
bestscores  # 各フォールドの最良スコアを表示します。
```

---The following area is a Code cell (cell numver is 37)---
```python
np.mean(bestscores)  # 各フォールドの最良スコアの平均を計算します。
print("My CV is " + str(np.mean(bestscores)) + ".")  # クロスバリデーションの結果を表示します。
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# 予測関数の定義
保存されたモデルは使用しません。
```

---The following area is a Code cell (cell numver is 39)---
```python
def predicting(test_dataloader, model):  # 予測を行う関数を定義します。
    
    model.to(device)  # モデルを指定したデバイスに移動させます。
    model.eval()  # モデルを評価モードに設定します。   
    allpreds = []  # すべての予測を格納するリスト
    preds = []  # 個々の予測を格納するリスト
    allvalloss = 0  # 検証損失の合計を初期化します。

    with torch.no_grad():  # 勾配計算を行わないようにします。
        for a in test_dataloader:  # テストデータローダーからデータを取得します。

            ids = a["ids"].to(device)  # バッチ内の入力IDをデバイスに移動させます。
            mask = a["mask"].to(device)  # バッチ内の注意マスクをデバイスに移動させます。

            output = model(ids, mask)  # モデルにIDとマスクを入力し、出力を取得します。
            output = output["logits"].squeeze(-1)  # 出力からlogitsを抽出し、次元を1つ削減します。
            preds.append(output.cpu().numpy())  # 予測をCPUに移動させ、リストに追加します。

        preds = np.concatenate(preds)  # すべての予測を結合します。
        allpreds.append(preds)  # 予測を全体のリストに追加します。

    return allpreds  # すべての予測を返します。
```

---The following area is a Markdown cell (cell numver is 40)---
```markdown
# 予測を行います
```

---The following area is a Code cell (cell numver is 41)---
```python
tpreds = predicting(test_dataloader, model)  # テストデータローダーを使用して予測を実行し、結果をtpredsに格納します。
```

---The following area is a Markdown cell (cell numver is 42)---
```markdown
# 予測結果
```

---The following area is a Code cell (cell numver is 43)---
```python
test_pred = []  # 予測結果を格納するリストを初期化します。
for p in tpreds[0]:  # tpredsの最初の要素（予測結果）をループします。
    test_pred += [p]  # 各予測値をリストに追加します。
```

---The following area is a Code cell (cell numver is 44)---
```python
submit = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # 提出用のサンプルCSVファイルを読み込みます。
pa = test_pred[0:len(test0)]  # 予測結果の最初の部分をモデルAの結果として設定します。
pb = test_pred[len(test0):]  # 予測結果の残りの部分をモデルBの結果として設定します。
pc = []  # モデルの引き分け結果を格納するリストを初期化します。
for i in range(len(test0)):  # テストデータの長さ分ループします。
    pc += [np.clip(1 - (pa[i] + pb[i]), 0, 1)]  # モデルAとモデルBの結果から引き分けのスコアを計算し、0〜1の範囲にクリップします。
submit['winner_model_a'] = pa  # 提出データフレームにモデルAの結果を追加します。
submit['winner_model_b'] = pb  # 提出データフレームにモデルBの結果を追加します。
submit['winner_tie'] = pc  # 提出データフレームに引き分けの結果を追加します。
display(submit)  # 提出データフレームを表示します。
submit.to_csv('submission.csv', index=False)  # 提出データフレームをCSVファイルとして保存します。
```

---The following area is a Markdown cell (cell numver is 45)---
```markdown
[Caution] 'submission.csv'は提出用ではありません。このノートブックはインターネット接続の条件で実行されているためです。
```

** @@@ Jupyter Notebook numver 57, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間の好みの予測に焦点を当てています。具体的には、異なる大規模言語モデル（LLM）によって生成された応答のうち、どちらがユーザーに好まれるかを予測するための機械学習モデルを構築しています。

### 問題を解決するための手法とライブラリ:

1. **データ前処理**:
   - `pandas`を使用してCSVファイルから訓練データとテストデータを読み込みます。
   - プロンプトと応答を結合し、特徴量を抽出するために`TfidfVectorizer`を利用します。これにより、テキストを数値ベクトルに変換します。

2. **データの分割とスケーリング**:
   - `train_test_split`を使用してデータセットを訓練セットと検証セットに分割します。
   - `StandardScaler`を用いて特徴量を標準化します。

3. **モデル構築**:
   - TensorFlowとKerasを使用してニューラルネットワークモデルを構築します。Functional APIを利用して複数層の全結合層（Dense）を定義し、ドロップアウト層で過学習を防ぎます。
   - 出力層はソフトマックス関数を使用し、3つのクラス（モデルAの勝者、モデルBの勝者、引き分け）に対する確率を出力します。

4. **モデルの訓練と評価**:
   - 定義したモデルを訓練データで訓練し、検証データを使用してモデルの性能を評価します。
   - `log_loss`を用いて検証時のログ損失を計算し、その結果を出力します。

5. **予測と提出用ファイル作成**:
   - テストデータに対する予測を行い、`submission.csv`というファイル名で結果を保存します。このファイルには、各モデルの勝者確率と引き分けの確率が含まれています。

全体として、このノートブックは、チャットボットの応答の好みに関する人間の予測を高めるために、機械学習の手法を用いて問題にアプローチしています。使用する主なライブラリは、`numpy`、`pandas`、`sklearn`、`tensorflow`です。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な解析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例えば、いくつかの便利なパッケージを読み込むことができます

import numpy as np # 線形代数用
import pandas as pd # データ処理、CSVファイルの入出力用（例: pd.read_csv）

# 入力データファイルは、読み取り専用の"../input/"ディレクトリに存在します
# 例えば、これを実行すると（実行ボタンをクリックしたりShift+Enterを押すと）入力ディレクトリ内の全ファイルをリストアップします

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）に最大20GB書き込むことができ、"Save & Run All"を使用してバージョンを作成すると、その出力が保存されます
# 一時ファイルは/kaggle/temp/に書き込むこともできますが、それは現在のセッション外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # GPUを無効にする

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split  # データを訓練セットと検証セットに分割
from sklearn.feature_extraction.text import TfidfVectorizer  # テキストデータのためのTF-IDFベクトル化
from sklearn.preprocessing import StandardScaler  # 特徴量の標準化
from sklearn.metrics import log_loss  # ログ損失計算
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout  # Kerasレイヤーのインポート
from tensorflow.keras.models import Model  # Kerasモデルのインポート

# データセットを読み込む
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # 訓練データの読み込み
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータの読み込み

# プロンプトと応答を結合して特徴抽出を行う
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']  # 応答Aとプロンプトを結合
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']  # 応答Bとプロンプトを結合

# TF-IDFを使用してテキストデータをベクトル化
tfidf = TfidfVectorizer(max_features=5000)  # 最大5000特徴量でTF-IDFを設定
X_a = tfidf.fit_transform(train_data['text_a']).toarray()  # 応答Aのテキストをベクトル化
X_b = tfidf.transform(train_data['text_b']).toarray()  # 応答Bのテキストをベクトル化

# ベクトル化した応答を結合
X = np.hstack([X_a, X_b])  # 応答Aと応答Bを横に結合

# 目標変数
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # 勝者モデルの情報を取り出す

# データを訓練セットと検証セットに分割
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)  # 20%を検証用に

# 特徴量をスケーリング
scaler = StandardScaler()  # 標準化スケーラーを作成
X_train = scaler.fit_transform(X_train)  # 訓練データをフィッティングしてスケーリング
X_val = scaler.transform(X_val)  # 検証データをスケーリング

# Functional APIを使用してモデルを定義
input_layer = Input(shape=(X_train.shape[1],))  # 入力層の形状を設定
x = Dense(512, activation='relu')(input_layer)  # 512ユニットの全結合層を追加
x = Dropout(0.5)(x)  # ドロップアウト層を追加
x = Dense(256, activation='relu')(x)  # 256ユニットの全結合層を追加
x = Dropout(0.5)(x)  # ドロップアウト層を追加
x = Dense(128, activation='relu')(x)  # 128ユニットの全結合層を追加
x = Dropout(0.5)(x)  # ドロップアウト層を追加
output_layer = Dense(3, activation='softmax')(x)  # 出力層を追加

model = Model(inputs=input_layer, outputs=output_layer)  # モデルを定義
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  # モデルをコンパイル

# モデルを訓練
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val))  # モデルの訓練

# モデルを評価
val_predictions = model.predict(X_val)  # 検証データに対する予測
loss = log_loss(y_val, val_predictions)  # ログ損失を計算
print(f'Validation Log Loss: {loss}')  # 検証用ログ損失を出力

# テストデータの準備
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a']  # テストデータのためにプロンプトと応答Aを結合
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b']  # テストデータのためにプロンプトと応答Bを結合
X_test_a = tfidf.transform(test_data['text_a']).toarray()  # テストデータ応答Aをベクトル化
X_test_b = tfidf.transform(test_data['text_b']).toarray()  # テストデータ応答Bをベクトル化
X_test = np.hstack([X_test_a, X_test_b])  # 応答Aと応答Bを横に結合
X_test = scaler.transform(X_test)  # テストデータをスケーリング

# テストセットで予測
test_predictions = model.predict(X_test)  # テストデータに対する予測

# 提出用ファイルを作成
submission = pd.DataFrame(test_data['id'])  # 提出用データフレームを作成
submission['winner_model_a'] = test_predictions[:, 0]  # モデルAの勝者確率を追加
submission['winner_model_b'] = test_predictions[:, 1]  # モデルBの勝者確率を追加
submission['winner_tie'] = test_predictions[:, 2]  # 引き分けの確率を追加
submission.to_csv('submission.csv', index=False)  # 提出用CSVファイルとして保存
```

** @@@ Jupyter Notebook numver 58, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、選好予測のためのモデルを構築することを目的としています。具体的には、大規模言語モデル（LLM）を使用し、ユーザーの好みに基づいて2つの応答からどちらが好まれるかを予測するタスクに取り組んでいます。

Notebookでは、以下の手法やライブラリが使用されています：

1. **ライブラリのインストールとインポート**：
   - `peft`と`bitsandbytes`を利用し、特定のモデルやデータ処理に必要な機能を提供しています。
   - `pandas`を使用してデータを操作し、`datasets`ライブラリからデータを読み込んでいます。
   - `transformers`ライブラリからの様々なクラス（`AutoTokenizer`, `AutoModelForSequenceClassification`, `TrainingArguments`, `Trainer`など）を用いてモデルの構築とトレーニングを行っています。
   - `scikit-learn`から`log_loss`をインポートし、モデルの性能評価に使用しています。

2. **データ準備**：
   - トレーニングデータとして`train.csv`を読み込み、ベースとなるデータフレームを作成します。
   - 各サンプルに最も高い値を持つ応答をラベルとして追加し、データセットを必要な列のみに整理します。
   - データセットを5%の割合でトレーニングセットとテストセットに分割します。

3. **モデルの設定**：
   - `DeBERTa`モデルのファインチューニングに使用するチェックポイントを指定し、トークナイザーを初期化します。
   - `LoRa`（Low-Rank Adaptation）を利用して、効率的にモデルを訓練する設定を行います。

4. **トークナイゼーション**：
   - 各サンプルのプロンプトと応答をトークン化し、モデルが受け取れる形式に変換します。

5. **トレーニング**：
   - `TrainingArguments`を設定し、トレーニングの実行を管理する`Trainer`オブジェクトを初期化します。
   - ログロスや精度を評価指標として計算する関数を定義し、トレーニングを実行します。
   - トレーニング終了後、モデルとトークナイザーを保存します。

このNotebookは、LLMの強化学習タスクにおけるユーザーの選好を予測する機械学習モデルを構築し、データ準備からトレーニングまでの一連の流れを示しています。重要なポイントとして、再現性を考慮したシード値の設定や、最適化されたトレーニング戦略を採用していることが挙げられます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリ
```

---The following area is a Code cell (cell numver is 2)---
```python
# peftとbitsandbytesというライブラリをインストールします。
# これらのライブラリは、特定のモデルやデータ処理に必要な機能を提供します。
!pip install peft bitsandbytes
```

---The following area is a Code cell (cell numver is 3)---
```python
# pandasライブラリをインポートします。データの操作や分析を行うために使用します。
import pandas as pd

# datasetsライブラリから、Dataset、load_metric、load_datasetをインポートします。
# これらはデータの読み込みと評価指標の計算に使います。
from datasets import Dataset, load_metric, load_dataset

# transformersライブラリから、いくつかの重要なクラスをインポートします。
# AutoTokenizerはトークナイザーを自動的にロードするため、AutoModelForSequenceClassificationはシーケンス分類のモデルをロードするために使います。
# TrainingArgumentsはトレーニング設定を格納し、Trainerはトレーニングの実行を管理します。
# DataCollatorWithPaddingはバッチ内のデータを整形するために、EarlyStoppingCallbackはトレーニングの早期停止を設定します。
# AutoConfigはモデルの設定を自動的に取得するために使います。
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding,
    EarlyStoppingCallback,
    AutoConfig
)

# pathlibライブラリをインポートします。ファイルパスの操作を簡単に行うために使用します。
from pathlib import Path

# sklearn.metricsからlog_lossをインポートします。予測の精度を評価するために使用します。
from sklearn.metrics import log_loss

# numpyライブラリをインポートします。数値計算を行うために使います。
import numpy as np

# jsonライブラリをインポートします。JSONデータの読み書きに使用します。
import json

# peftライブラリから、複数のクラスをインポートします。
# これらは特定のモデル設定やトレーニングを行うために使います。
from peft import get_peft_config, get_peft_model, PeftModel, PeftConfig, LoraConfig, TaskType, LoftQConfig

# bitsandbytesライブラリをインポートします。効率的な計算を行うために利用します。
import bitsandbytes as bnb

# tqdmライブラリを自動的にインポートします。プログレスバーを表示するために使います。
from tqdm.auto import tqdm

# randomライブラリをインポートします。ランダムな数値生成を行うために使います。
import random
```

---The following area is a Code cell (cell numver is 4)---
```python
# kaggle_secretsライブラリからUserSecretsClientをインポートします。
# これにより、Kaggleのユーザーシークレットにアクセスできるようになります。
from kaggle_secrets import UserSecretsClient

# wandbライブラリをインポートします。これはWeights and Biasesを使用して実験を管理するためのライブラリです。
import wandb

# UserSecretsClientのインスタンスを作成します。このクライアントを使ってシークレット情報にアクセスします。
user_secrets = UserSecretsClient()

# "wandb_api_key"という名前のシークレット情報を取得します。これにはWandBのAPIキーが保存されています。
my_secret = user_secrets.get_secret("wandb_api_key") 

# 取得したAPIキーを使用してWandBにログインします。このステップが成功すると、WandBの機能が利用可能になります。
wandb.login(key=my_secret)
```

---The following area is a Code cell (cell numver is 5)---
```python
# 環境変数WANDB_LOG_MODELを"checkpoint"に設定します。
# これはWandB（Weights and Biases）がモデルのチェックポイントをログに記録することを指示します。
%env WANDB_LOG_MODEL="checkpoint"

# 環境変数WANDB_PROJECTを"LMSYS"に設定します。
# これはWandBでのプロジェクト名を指定し、実験をこのプロジェクトに関連付けます。
%env WANDB_PROJECT=LMSYS
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# データと設定
```

---The following area is a Code cell (cell numver is 7)---
```python
# すべてのランダム数生成器に同じシード値を設定する関数を定義します。
# この関数を使用することで、結果を再現可能にし、実験の一貫性を保ちます。
def seed_everything(seed):
    # randomライブラリをインポートします。Pythonのランダム生成器を操作するために使用します。
    import random
    # osライブラリをインポートします。オペレーティングシステムの機能にアクセスするために使用します。
    import os
    # numpyライブラリをインポートします。数値計算の際に使用します。
    import numpy as np
    # torchライブラリをインポートします。PyTorchを使った深層学習のために使用します。
    import torch
    
    # Pythonのランダム生成器のシード値を設定します。
    random.seed(seed)
    # 環境変数PYTHONHASHSEEDをシード値に設定します。これにより、ハッシュの一貫性が保たれます。
    os.environ['PYTHONHASHSEED'] = str(seed)
    # numpyのランダム生成器のシード値を設定します。
    np.random.seed(seed)
    # PyTorchのCPU用のランダム生成器のシード値を設定します。
    torch.manual_seed(seed)
    # PyTorchのGPU用のランダム生成器のシード値を設定します。
    torch.cuda.manual_seed(seed)
    # cuDNNの決定論的モードを有効にします。これにより、同じ入力に対して常に同じ出力が得られます。
    torch.backends.cudnn.deterministic = True
    
# 2024をシード値にして、seed_everything関数を呼び出します。これによってランダム生成器が初期化されます。
seed_everything(2024)
```

---The following area is a Code cell (cell numver is 8)---
```python
# 最大長（max_len）を1024に設定します。
# これは、モデルに入力するシーケンスの最大トークン数を指定します。
# この長さを超えるトークンは切り捨てられるか、適切な処理が行われます。
max_len = 1024
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
## ラベルの生成と列の削除
```

---The following area is a Code cell (cell numver is 10)---
```python
# フォルダーパスを設定します。
# '/kaggle/input/lmsys-chatbot-arena'はデータが格納されているディレクトリのパスです。
folder_path = Path('/kaggle/input/lmsys-chatbot-arena')
```

---The following area is a Code cell (cell numver is 11)---
```python
# 'train.csv'ファイルを読み込み、データセットとしてtrain_dfに格納します。
# load_dataset関数を使ってCSV形式でデータを読み込みます。
train_df = load_dataset('csv', data_files=str(folder_path / 'train.csv'))

# 以下の行は以前のデータセットを読み込むためのコメントアウトされた例です。
# 'external_train.csv'という別のトレーニングデータを読み込む場合。
# train_df = load_dataset('csv', data_files='external_train.csv')
# '/kaggle/input/lmsys-additional-33k-labelled-conversations/lmsys-33k-deduplicated.csv'からのデータを読み込む場合。
# train_df = load_dataset('csv', data_files='/kaggle/input/lmsys-additional-33k-labelled-conversations/lmsys-33k-deduplicated.csv')

# 'test.csv'ファイルを読み込み、データセットとしてtest_dfに格納します。
test_df = load_dataset('csv', data_files=str(folder_path / 'test.csv'))

# 'sample_submission.csv'ファイルを読み込み、データセットとしてsample_dfに格納します。
sample_df = load_dataset('csv', data_files=str(folder_path / 'sample_submission.csv'))
```

---The following area is a Code cell (cell numver is 12)---
```python
# ラベルを生成する関数get_labelを定義します。
# この関数は、各サンプルに対して最も高い値を持つインデックスをラベルとして追加します。
def get_label(sample):
    # winner_model_a、winner_model_b、winner_tieの値の中で最大値を持つインデックスを取得し、labelとして設定します。
    sample['label'] = np.argmax([sample['winner_model_a'], sample['winner_model_b'], sample['winner_tie']])
    return sample

# train_dfデータセットにget_label関数を適用します。
# これにより、各サンプルにラベルが追加されます。
train_df = train_df.map(get_label)
```

---The following area is a Code cell (cell numver is 13)---
```python
# train_dfデータセットから必要な列だけを選択し、新しいデータフレームを作成します。
# ここでは、'prompt'、'response_a'、'response_b'、'label'の4つの列を選択します。
train_df = train_df.select_columns(['prompt', 'response_a', 'response_b', 'label'])

# 現在のtrain_dfデータセットを表示します。
# これにより、選択した列のみが含まれるデータフレームが確認できます。
train_df
```

---The following area is a Code cell (cell numver is 14)---
```python
# train_dfデータセットを5%の割合でトレーニングセットとテストセットに分割します。
# train_test_split関数を使用して、データセットをシャッフルし、指定した割合で分割を行います。
dataset = train_df['train'].train_test_split(0.05)

# 現在のデータセットの分割結果を表示します。
# これにより、トレーニングセットとテストセットの構造を確認できます。
dataset
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# モデル
```

---The following area is a Code cell (cell numver is 16)---
```python
# 使用するモデルの名前を指定するための変数model_nameを定義します。
# このコメントアウトされた行は、microsoftのDeBERTa V3ベースモデルを指定するための例です。
# model_name = "microsoft/deberta-v3-base"

# アダプタートークナイザーのパスを指定します。
# このパスは、事前にファインチューニングされたDeBERTaモデルのトークナイザーが保存されている場所です。
adapther_tokenizer_path = "/kaggle/input/deberta-finetuned/DeBerta-base-5"

# 使用するモデルのチェックポイントのパスを指定します。
# これは、ファインチューニングされたモデルの保存場所を示しています。
model_name = "/kaggle/input/deberta-finetuned/LMSYS/checkpoint-2980"
```

---The following area is a Code cell (cell numver is 17)---
```python
# 指定したアダプタートークナイザーのパスからトークナイザーを初期化します。
# max_lenで定義した最大長に基づいて、トークナイザーが設定されます。
tokenizer = AutoTokenizer.from_pretrained(adapther_tokenizer_path, model_max_length=max_len)

# 指定したモデル名からシーケンス分類モデルを初期化します。
# num_labelsは分類するラベルの数を示し、ここでは3つのラベルがあることを指定します。
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=3,
)
```

---The following area is a Code cell (cell numver is 18)---
```python
# LoftQConfigを使用して、LoftQのビット数を4に設定します。
# これは、モデルの量子化設定に関連します。
loftq_config = LoftQConfig(loftq_bits=4)

# LoraConfigを初期化し、モデルのトレーニング設定を指定します。
peft_config = LoraConfig(
    # target_modulesには、特定のターゲットモジュールを指定することができますが、ここではコメントアウトされています。
    # target_modules=['query_proj', 'value_proj', 'key_proj'],
    
    # RSLORAを使用するかどうかを指定します。ここでは使用することを選択しています。
    use_rslora=True,
    
    # LoRAのランクを設定します。この場合は16です。
    r=16,
    
    # LoRAのスケーリングファクターを指定します。この場合は8です。
    lora_alpha=8,
    
    # LoRAによるドロップアウト率を指定します。ここでは0.1です。
    lora_dropout=0.1,
    
    # タスクのタイプをSEQ_CLS（シーケンス分類）に設定します。
    task_type=TaskType.SEQ_CLS,
    
    # 初期LoRAウェイトの設定を'loftq'とします。
    init_lora_weights='loftq',
)

# モデルにPEFT設定を適用します。
# これにより、トレーニングのためにモデルが更新されます。
model = get_peft_model(model, peft_config)
```

---The following area is a Code cell (cell numver is 19)---
```python
# モデルのトレーニング可能なパラメータを表示する関数を定義します。
# この関数は、トレーニング可能なパラメータの数と全パラメータの数、およびトレーニング可能な割合を計算して表示します。
def print_trainable_parameters(model):
    trainable_params = 0  # トレーニング可能なパラメータの初期値
    all_param = 0  # 全パラメータの初期値
    
    # モデルの各パラメータをイテレートします。
    for _, param in model.named_parameters():
        # 全パラメータの合計を更新します。
        all_param += param.numel()
        # パラメータがトレーニング可能な場合、そのカウントを増やします。
        if param.requires_grad:
            trainable_params += param.numel()
    
    # トレーニング可能なパラメータ、全パラメータ、トレーニング可能な割合を出力します。
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}"
    )

# モデルのトレーニング可能なパラメータを表示するために関数を呼び出します。
print_trainable_parameters(model)
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## トークナイズ
```

---The following area is a Code cell (cell numver is 21)---
```python
# トークナイザーからセパレータトークンとクラスストークンのIDを取得します。
sep_token = tokenizer.sep_token_id  # セパレータトークンのIDを取得
cls_token = tokenizer.cls_token_id  # クラスストークンのIDを取得

# トークナイザーにパディングトークンを追加します。
# ここでは、'<pad>'という文字列をパディングトークンとして追加します。
tokenizer.add_special_tokens({'pad_token': '<pad>'})

# モデルのトークン埋め込みのサイズをトークナイザーのトークン数の長さに合わせて調整することができますが、
# その行はコメントアウトされています。これを有効にすることで、トークン数に応じてモデルの埋め込み層が再調整されます。
# model.resize_token_embeddings(len(tokenizer))

# 各トークンの最大長を決定します。
# ここでは、max_lenを3で割って3を引き、各トークンが保持できる最大長を計算します。
len_to_each = max_len // 3 - 3
```

---The following area is a Code cell (cell numver is 22)---
```python
# トークナイザーを使用して入力データをトークナイズする関数を定義します。
# この関数は、各サンプルのプロンプトと応答をトークン化し、モデルに適した形式に変換します。
def tokenize_function(sample):
    # プロンプトをトークナイズし、最大長len_to_eachに制限し、トランケーションとパディングを適用します。
    # ここでは、入力IDとしてトークンのIDリストを取得します。
    prompt = tokenizer(sample['prompt'], max_length=len_to_each, truncation=True, padding=True).input_ids
    
    # 応答Aをトークナイズし、同様の処理を行います。
    response_a = tokenizer(sample['response_a'], max_length=len_to_each, truncation=True, padding=True).input_ids
    
    # 応答Bをトークナイズし、同様の処理を行います。
    response_b = tokenizer(sample['response_b'], max_length=len_to_each, truncation=True, padding=True).input_ids
    
    # トークンIDを結合し、モデルの入力形式を作成します。
    # ここでは、クラスストークン、プロンプト、セパレータトークン、応答A、セパレータトークン、応答Bを結合しています。
    sample['input_ids'] = [cls_token] + prompt + [sep_token] + response_a + [sep_token] + response_b
    return sample
```

---The following area is a Code cell (cell numver is 23)---
```python
# 定義したtokenize_functionを使用して、dataset内の各サンプルに対してトークナイズ処理を適用します。
# このmap関数により、各サンプルがトークン化され、モデルに適した形式に変換されます。
dataset = dataset.map(tokenize_function)
```

---The following area is a Code cell (cell numver is 24)---
```python
# 現在のdatasetデータセットを表示します。
# これにより、トークナイズ処理が適用された後のサンプルの構造を確認できます。
dataset
```

---The following area is a Code cell (cell numver is 25)---
```python
# datasetから必要な列だけを選択し、新しいデータセットを作成します。
# ここでは、'input_ids'と'label'の2つの列を選択します。
dataset = dataset.select_columns(['input_ids', 'label'])
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
## トレーニング
```

---The following area is a Code cell (cell numver is 27)---
```python
# トレーニングの引数を設定するためのTrainingArgumentsを初期化します。
args = TrainingArguments(
    "LMSYS",  # トレーニング結果の出力先ディレクトリ名
    eval_strategy="steps",  # 評価の実行戦略をステップごとに設定します。
    save_strategy="steps",  # モデルの保存戦略をステップごとに設定します。
    save_steps=500,  # 500ステップごとにモデルを保存します。
    learning_rate=3e-4,  # 学習率を設定します。
    fp16=True,  # 16ビット浮動小数点でトレーニングを行うかどうかを指定します。
    per_device_train_batch_size=4,  # 各デバイスのトレーニングバッチサイズを4に設定します。
    per_device_eval_batch_size=16,  # 各デバイスの評価バッチサイズを16に設定します。
    gradient_accumulation_steps=4,  # 勾配の累積ステップを4に設定します。
    optim='adamw_hf',  # 最適化アルゴリズムをAdamWに設定します。
    num_train_epochs=3,  # トレーニングエポック数を3に設定します。
    weight_decay=0.01,  # 重みの減衰を設定します。
    load_best_model_at_end=True,  # トレーニングの最後に最良のモデルをロードするかどうかを指定します。
    save_total_limit=2,  # 保存するモデルの最大数を2に設定します。
    metric_for_best_model='accuracy',  # 最良のモデルの評価基準を精度に設定します。
    report_to="wandb",  # 結果をWandBに報告します。
    run_name="DeBerta-base-train-data",  # 実行の名前を指定します。
    eval_steps=500,  # 評価を行うステップを500に設定します。
    logging_strategy="steps",  # ロギングの戦略をステップごとに指定します。
    logging_steps=100,  # 100ステップごとにログを出力します。
)
```

---The following area is a Code cell (cell numver is 28)---
```python
# 評価のためのメトリックを計算する関数を定義します。
# この関数は、モデルの出力とラベルを使用して評価指標を計算します。
def compute_metrics(eval_pred):
    logits, labels = eval_pred  # 評価予測からロジットとラベルを取得します。
    
    # ソフトマックス関数を適用して、ロジットを確率に変換します。
    probabilities = np.exp(logits) / np.sum(np.exp(logits), axis=1, keepdims=True)
    
    # 評価指標を辞書形式で返します。
    return {
        'eval_log_loss': log_loss(labels, probabilities),  # ログロスを計算します。
        'eval_accuracy': (np.argmax(logits, axis=1) == labels).mean()  # 精度を計算します。
    }
```

---The following area is a Code cell (cell numver is 29)---
```python
# データのバッチ処理を行うためのDataCollatorWithPaddingを初期化します。
# これにより、入力シーケンスが同じ長さにパディングされ、モデルへの入力形式が整えられます。
data_collator = DataCollatorWithPadding(tokenizer)
```

---The following area is a Code cell (cell numver is 30)---
```python
# トレーニングを実行するためのTrainerを初期化します。
trainer = Trainer(
    model=model,  # トレーニングするモデルを指定します。
    args=args,  # 設定したトレーニング引数を指定します。
    train_dataset=dataset['train'],  # トレーニングデータセットを指定します。
    eval_dataset=dataset['test'],  # 評価データセットを指定します。
    compute_metrics=compute_metrics,  # 評価指標を計算する関数を指定します。
    data_collator=data_collator,  # データのバッチ処理を行うコラレーターを指定します。
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # 早期停止のコールバックを設定します。
)
```

---The following area is a Code cell (cell numver is 31)---
```python
# トレーニングを開始します。
# これにより、指定された設定に従ってモデルのトレーニングが実行されます。
trainer.train()

# トレーニングが完了したら、WandBのログを終了します。
# これにより、現在の実行の情報が保存されます。
wandb.finish()
```

---The following area is a Code cell (cell numver is 32)---
```python
# トレーニング済みモデルとトークナイザーを保存するためのフォルダー名を設定します。
save_folder = 'DeBerta-base-trained'

# モデルをマージしてメモリからアンロードします。
# これにより、モデルの重みが最適化され、次のステップで保存できる状態になります。
model = model.merge_and_unload()

# 保存先フォルダーにモデルの重みを保存します。
model.save_pretrained(save_folder)

# 保存先フォルダーにトークナイザーの設定を保存します。
tokenizer.save_pretrained(save_folder)
```

---The following area is a Code cell (cell numver is 33)---
```python
# Trainerオブジェクトを使用して、トレーニング済みモデルを指定したフォルダーに保存します。
# ここでは、フォルダー名に"-trainer"を追加して保存先を指定します。
trainer.save_model(f"{save_folder}-trainer")
```

---The following area is a Code cell (cell numver is 34)---
```python
# コードセルが空のため、何も実行しません。必要に応じてコードを記述してください。
```

** @@@ Jupyter Notebook numver 59, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションのために、チャットボットの応答に対する人間の好みを予測するモデルを開発することを目的としています。具体的には、LLM（大規模言語モデル）を使用し、「winner_model_a」「winner_model_b」「winner_tie」といった分類を予測するために、ロジスティック回帰的な手法を用いています。

### 主な問題
- ユーザーが2つのモデルによる応答の中からどちらを好むかを予測する問題に取り組んでいます。データからは、各応答がどれだけ好まれるかを評価するためにログ損失（log loss）をメトリクスとして使用しています。

### 使用手法とライブラリ
- **Hugging Face Transformers**: モデルのトレーニングや推論に使用され、特に`LlamaModel`や`AutoTokenizer`が利用されています。
- **Pandas、NumPy**: データ処理に使用されており、データの読み込みや変換、数値計算を行っています。
- **PEFT (Parameter-Efficient Fine-Tuning)**: パラメータの効率的な調整を行うために、Low-Rank Adaptation（LoRA）を使用しています。これにより、モデルの性能を保ちながら、必要なメモリを削減しています。
- **Scikit-learn**: 評価指標としてのログロスと精度を計算するためにインポートされています。

### 処理の流れ
1. 必要なライブラリのインストールとインポート。
2. 訓練データの読み込みと前処理。各応答のラベル設定を行います。
3. トークナイザーの設定を行い、データセットをトークン化。
4. モデルの準備（`Llama3ForSFT`クラス）、LoRA設定を行い、事前学習されたモデルを読み込みます。
5. トレーニングの引数を設定し、トレーニングを実行。

最終的に、提案されたアプローチによって、与えられたプロンプトに対して最適な応答を予測する能力を持つモデルが訓練されています。評価結果として、ログロス値が0.9231、リーダーボードでのスコアが0.936となっています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 結果
- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| サブセット | ログロス |
| - | - |
| 評価 | 0.9231|
| LB | 0.936 |

## 注意事項
コードを再現したい場合は、以下の点に注意してください:
- すべてのデータを使用
- `per_device_train_batch_size=4`を設定
- A10を使用して1エポックで約15時間かかります
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install git+https://github.com/huggingface/transformers  # Hugging FaceのTransformersライブラリをインストールします。
!pip install -U bitsandbytes accelerate peft  # bitsandbytes、accelerate、peftを最新バージョンにアップグレードしてインストールします。
```

---The following area is a Code cell (cell numver is 3)---
```python
import os  # オペレーティングシステムに関連する機能をインポートします。
import copy  # オブジェクトのコピーをサポートする機能をインポートします。
from dataclasses import dataclass  # データクラスを作成するためのデコレーターをインポートします。

import torch  # PyTorchの基本ライブラリをインポートします。
import torch.nn as nn  # PyTorchのニューラルネットワークモジュールをインポートします。
import torch.nn.functional as F  # ニューラルネットワーク用の関数をインポートします。
import pandas as pd  # データ処理用のPandasライブラリをインポートします。
import numpy as np  # 数値計算用のNumPyライブラリをインポートします。
from datasets import Dataset  # データセットを扱うためのクラスをインポートします。
from scipy.special import softmax  # softmax関数をインポートします。
from sklearn.preprocessing import LabelEncoder  # ラベルエンコーディングを行うためのクラスをインポートします。
from transformers import (  # Transformersライブラリから必要なクラスをインポートします。
    BitsAndBytesConfig,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
    AutoModel
)
from transformers.modeling_outputs import CausalLMOutputWithPast  # 過去の出力を含む因果言語モデルの出力をインポートします。
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType  # PEFTに関するクラスや関数をインポートします。
from sklearn.metrics import log_loss, accuracy_score  # 評価指標としてのログロスと精度をインポートします。
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"  # 訓練データのCSVファイルのパスを指定します。
model_path = "unsloth/Mistral-Nemo-Instruct-2407-bnb-4bit"  # 使用するモデルのパスを指定します。
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能かどうかを確認してデバイスを設定します。
MAX_LENGTH = 1024  # 最大入力長を設定します。
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測対象の列を指定します。
columns_to_vectorize = ["prompt", "response_a", "response_b"]  # ベクトル化する列を指定します。

train = pd.read_csv(TRAIN_CSV)  # CSVファイルから訓練データを読み込みます。
train = train.head(100)  # 最初の100件を取得します。
train['label'] = train[target_columns].idxmax(axis=1)  # 各行のラベルを決定します。
label_encoder = LabelEncoder()  # ラベルエンコーダを初期化します。
train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコードします。
train = train[columns_to_vectorize + ['label']]  # 必要な列だけを保持します。
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
### トークナイザーとデータセットの準備、メトリクス
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path, force_download=True)  # 事前学習済みのトークナイザーをロードします。
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer.add_eos_token = True  # 終了トークンを追加します。
tokenizer.padding_side = 'right'  # パディングを右側に設定します。

# ラベルIDsをトークナイザーを使って作成します。
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

def tokenize(example, tokenizer):
    # プロンプト、応答a、および応答bをトークン化します。
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    # 入力が最大長を超える場合は入力を切り詰めます。
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    # ラベルに対応するトークンIDを取得します。
    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids) * [1]  # アテンションマスクを作成します。
    labels = [-100] * len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    # トークナイズされたデータを返します。
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Code cell (cell numver is 9)---
```python
def load_data(df, tokenizer):
    # DataFrameをデータセットに変換します。
    raw_datasets = Dataset.from_pandas(df)
    tokenized_datasets = raw_datasets.map(
        tokenize, 
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}  # トークナイザーを引数として渡します。
    )
    return tokenized_datasets

def compute_metrics(pred):
    logits, labels = pred  # 予測とラベルを取得します。
    preds = logits.argmax(axis=-1)  # 最大のロジット値を持つインデックスを取得します。
    label_tokens_ids = np.array(LABEL_IDS)  # ラベルIDsを配列に変換します。
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。
    labels = labels[np.isin(labels, label_tokens_ids)]  # ラベルがラベルIDのいずれかに含まれている場合にフィルタリングします。
    labels = np.array([index_mapping[label.item()] for label in labels])  # マッピングを適用してラベルを変換します。
    acc = accuracy_score(labels, preds)  # 精度を計算します。
    probs = softmax(logits, axis=-1)  # ソフトマックスを適用して確率を計算します。
    log_loss_ = log_loss(labels, probs)  # ログロスを計算します。
    return {'accuracy': acc, 'log_loss': log_loss_}  # 精度とログロスを返します。

n_splits = 5  # データセットを分割するためのスプリット数を設定します。
fold_idx = 0  # フォールドインデックスを初期化します。
ds = load_data(train, tokenizer)  # データセットをロードします。
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],  # 訓練データのインデックス
        [i for i in range(len(ds)) if i % n_splits == fold_idx]  # 評価データのインデックス
    ) 
    for fold_idx in range(n_splits)  # 各フォールドのインデックスに対して反復処理します。
]
train_idx, eval_idx = folds[fold_idx]  # 訓練インデックスと評価インデックスを取得します。
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
### モデル
```

---The following area is a Code cell (cell numver is 11)---
```python
from transformers import AutoModel, MistralPreTrainedModel, MistralModel  # モデル関連のクラスをインポートします。
class Llama3ForSFT(MistralPreTrainedModel):  # MistralPreTrainedModelを拡張したLlama3ForSFTクラスを定義します。
    _tied_weights_keys = ["lm_head.weight"]  # 重みの結合キーを定義します。

    def __init__(self, config):
        super().__init__(config)  # 親クラスの初期化を行います。
        self.model = MistralModel(config)  # Mistralモデルを初期化します。
        self.vocab_size = config.vocab_size  # 語彙サイズを取得します。
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # ロジット層を初期化します。
        self.post_init()  # モデルの初期化を完了します。

    def forward(
        self,
        input_ids=None,
        attention_mask=None,
        position_ids=None,
        past_key_values=None,
        inputs_embeds=None,
        labels=None,
        use_cache=None,
        output_attentions=None,
        output_hidden_states=None,
        return_dict=None,
        cache_position=None,
    ):
        outputs = self.model(
            input_ids=input_ids,  # 入力IDを渡します。
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        hidden_states = outputs[0]  # 隠れ状態を取得します。
        logits = self.lm_head(hidden_states)  # 隠れ状態からロジットを計算します。
        logits = logits.float()  # ロジットを浮動小数点型に変換します。

        loss = None  # 初期化
        if labels is not None:
            # ラベルがある場合の処理を行います。
            # シフトしてトークンがn未満を予測します。
            shift_logits = logits[..., :-1, :].contiguous()  # ロジットをシフトします。
            shift_labels = labels[..., 1:].contiguous()  # ラベルをシフトします。
            # トークンをフラット化します。
            loss_fct = nn.CrossEntropyLoss()  # クロスエントロピー損失関数を定義します。
            shift_logits = shift_logits.view(-1, self.config.vocab_size)  # ロジットを再成形します。
            shift_labels = shift_labels.view(-1)  # ラベルを再成形します。
            # モデル並行処理を有効にします。
            shift_labels = shift_labels.to(shift_logits.device)  # デバイスにラベルを移動します。

            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)  # ラベルIDsをテンソルに変換します。
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}  # インデックスマッピングを作成します。
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]  # 正しいラベルを取得します。
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)  # マッピングを適用します。
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]  # 正しいロジットを取得します。
            loss = loss_fct(true_logits, true_labels)  # 損失を計算します。

        return CausalLMOutputWithPast(  # 結果を返します。
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 12)---
```python
peft_config = LoraConfig(  # PEFT設定を作成します。
    r=16,  # ランクを設定します。
    lora_alpha=32,  # alpha値を設定します。
    lora_dropout=0.05,  # ドロップアウト率を設定します。
    bias='none',  # バイアスの設定を行います。
    inference_mode=False,  # 推論モードをオフにします。
    task_type=TaskType.CAUSAL_LM,  # タスクのタイプを因果言語モデルに設定します。
    target_modules=['q_proj', 'k_proj', 'v_proj'],  # 対象となるモジュールを設定します。
)

# 事前学習済みのモデルをロードし、kビット訓練の準備をします。
model = Llama3ForSFT.from_pretrained(
    model_path, 
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定します。
)
model.config.use_cache = False  # キャッシュを無効にします。
model = prepare_model_for_kbit_training(model)  # kビット訓練のための準備をします。
model = get_peft_model(model, peft_config)  # PEFTモデルを取得します。
print(model)  # モデルの情報を表示します。
model.print_trainable_parameters()  # 訓練可能なパラメータを表示します。
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
#### トレーニング引数
```

---The following area is a Code cell (cell numver is 14)---
```python
args = TrainingArguments(  # トレーニング引数を設定します。
    output_dir='output',  # 出力ディレクトリを指定します。
    overwrite_output_dir=True,  # 出力ディレクトリを上書きします。
    evaluation_strategy="epoch",  # 評価戦略をエポックごとに設定します。
    save_strategy="steps",  # 保存戦略をステップごとに設定します。
    save_steps=200,  # 200ステップごとに保存します。
    save_total_limit=1,  # 保存するファイル数の制限を設定します。
    logging_strategy="steps",  # ロギング戦略をステップごとに設定します。
    logging_steps=10,  # 10ステップごとにロギングします。
    warmup_steps=20,  # ウォームアップステップを設定します。
    optim="adamw_8bit",  # 最適化手法を設定します。
    learning_rate=2e-4,  # 学習率を設定します。
    per_device_train_batch_size=1,  # デバイスごとの訓練バッチサイズを設定します。
    per_device_eval_batch_size=1,  # デバイスごとの評価バッチサイズを設定します。
    gradient_accumulation_steps=3,  # 勾配蓄積ステップを設定します。
    num_train_epochs=1,  # 訓練エポック数を設定します。
    fp16=True,  # 半精度浮動小数点を使用します。
    metric_for_best_model="log_loss",  # 最良モデルのメトリックをログロスに設定します。
    greater_is_better=False,  # より良いのは小さい方の損失とします。
    report_to="none",  # ロギングの報告先を指定しません。
)
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
### トレーニング !
```

---The following area is a Code cell (cell numver is 16)---
```python
import transformers  # Transformersライブラリをインポートします。
transformers.__version__  # 現在のTransformersのバージョンを表示します。
```

---The following area is a Code cell (cell numver is 17)---
```python
trainer = Trainer(  # Trainerインスタンスを作成します。
    args=args,  # トレーニング引数を渡します。
    model=model,  # 使用するモデルを指定します。
    train_dataset=ds.select(train_idx),  # 訓練データセットを指定します。
    eval_dataset=ds.select(eval_idx),  # 評価データセットを指定します。
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータを指定します。
    compute_metrics=compute_metrics,  # メトリクス計算を指定します。
)
trainer.train()  # 訓練を開始します。
```

** @@@ Jupyter Notebook numver 60, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
以下は、提供されたJupyter Notebookの要約です。

---

## 要約

このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するためのもので、ユーザーが2つの異なるチャットボットの応答のどちらを好むかを予測するモデルの構築を目指しています。このコンペティションの目標は、チャットボットの応答を評価するためのデータセットを使用し、機械学習モデルを訓練することです。

### 主要な問題
- チャットボットからの2つの応答に対して、どちらの応答がより好まれるかを予測すること。具体的には、ユーザーによる選好データを用いてモデルを学習させます。

### 使用される手法とライブラリ
- **ライブラリのインストール**: `ftfy`（テキスト修正）および`textstat`（テキストの統計的評価）のライブラリがインストールされています。
- **データ処理**: Pandasを使用してデータの読み込み、欠損値の処理、特徴量の生成を行います。
- **特徴量の計算**: `textstat`を用いて、様々な可読性指標（フレスクの読みやすさやスモッグ指数など）を計算し、チャットボットの応答から特徴量を生成します。
- **モデル**: `CatBoostClassifier`と`LGBMClassifier`を使用して、データの分類モデルを構築します。また、交差検証のために`StratifiedKFold`を用いています。
- **メトリクス**: モデルの性能は、対数損失（log loss）によって評価されます。

### フロー
1. データの読み込みと前処理を行い、使用しない列を削除します。
2. チャットボットの応答から得た特徴量を生成し、学習用データセットを作成します。
3. `StratifiedKFold`を用いてクロスバリデーションを実施し、各モデルのトレーニングを行います。
4. テストデータに対して予測を行い、最終的な出力を`submission.csv`形式で保存します。

このNotebookは、効果的にユーザーの好みを予測するためのモデル構築プロセスを提供しており、機械学習におけるデータ前処理、特徴量エンジニアリング、モデル評価の実践的なアプローチが示されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリ
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install  /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
!pip install  /kaggle/input/textstat-dependencies/textstat-0.7.4-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import textstat as ts
import json
from ftfy import fix_encoding
from catboost import CatBoostClassifier,Pool
from lightgbm import LGBMClassifier
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss

from tqdm.notebook import tqdm
tqdm.pandas()
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# データ
```

---The following area is a Code cell (cell numver is 5)---
```python
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 6)---
```python
response_a = pd.read_csv("/kaggle/input/responses-textstat/response_a.csv")
response_b = pd.read_csv("/kaggle/input/responses-textstat/response_b.csv")
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
## 真のデータを取得
```

---The following area is a Code cell (cell numver is 8)---
```python
unused_columns = ['model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie', 'text_standard']
```

---The following area is a Code cell (cell numver is 9)---
```python
textstat_train = response_a.drop(columns=unused_columns).merge(response_b.drop(columns=unused_columns), on='id')
```

---The following area is a Code cell (cell numver is 10)---
```python
textstat_train = textstat_train.groupby('id').mean()
textstat_train.to_csv('textstat.csv', index=False)
```

---The following area is a Code cell (cell numver is 11)---
```python
X = textstat_train.copy()
```

---The following area is a Code cell (cell numver is 12)---
```python
y = response_a[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].groupby('id').agg('max').apply(pd.Series.argmax, axis=1)
y.head()
```

---The following area is a Code cell (cell numver is 13)---
```python
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)  # Stratified K-Foldの分割方法
clfs = []  # モデルを格納するリスト
scores = []  # スコアを格納するリスト
for train_index, test_index in tqdm(skf.split(X, y), total=5):  # 5分割のクロスバリデーション 

    X_train, X_test = X.iloc[train_index], X.iloc[test_index]  # 学習データとテストデータに分割
    y_train, y_test = y.iloc[train_index], y.iloc[test_index]  # ラベルデータも同様に分割

    # 学習を加速するための特殊なクラス
    train_dataset = Pool(data=X_train, label=y_train)  # 学習データセットの作成
    eval_dataset = Pool(data=X_test, label=y_test)  # 評価データセットの作成

    clf = CatBoostClassifier(
        depth=6,
        iterations=10000,
        learning_rate=0.06,
#         loss_function="MultiLogloss",  # マルチロス関数
        eval_metric = 'AUC', 
#         custom_metric=["Logloss"],  # 'AUC / Accuracy,
        
        # CatBoostの特長 - カテゴリカル特徴量の処理
#         cat_features=cat_features,
        # 無視する特徴量
#         ignored_features = ignored_features,
        
        # 正則化と学習の加速
#         colsample_bylevel=0.4,
#         subsample=0.95,
        l2_leaf_reg=10,  # L2 正則化
        min_data_in_leaf=50,  # リーフノードに必要な最小サンプル数
        max_bin=70,  # 最大ビンの数
        random_strength=1,  # ランダム強度
        
        # スコアリングパラメータ
        task_type="CPU",  # CPUを使用
        thread_count=-1,  # 全スレッドを使用
        bootstrap_type="Bernoulli",  # ブートストラップタイプ
        
        # 重要な設定
        random_seed=2024,  # 乱数シード
#         auto_class_weights="SqrtBalanced",  # 自動クラス重み
        early_stopping_rounds=200)  # 早期停止のラウンド数設定

    clfs.append(clf)  # モデルをリストに追加

    clf.fit(
        train_dataset,  # 学習データでフィット
        eval_set=eval_dataset,  # 評価データを指定
        verbose=200,  # 学習過程を200ステップごとに表示
        use_best_model=True,  # 最良のモデルを使用
        plot=False)  # プロットを無効にする

    # モデルのトレーニング後、スコアを保存
    scores.append(np.mean([v for k, v in clf.best_score_["validation"].items() if "Recall" in k], dtype="float16"))

assert len(clfs) == 5  # モデルの数が5であることを確認
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## テストを生成
```

---The following area is a Code cell (cell numver is 15)---
```python
df_test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
```

---The following area is a Code cell (cell numver is 16)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()  # データフレームのコピー
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))  # プロンプトのエンコーディングを修正
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))  # 応答Aのエンコーディングを修正
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))  # 応答Bのエンコーディングを修正

    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])  # カラムを展開
    return tmp
```

---The following area is a Code cell (cell numver is 17)---
```python
def get_features(df, column):
    df = df.copy()  # データフレームをコピー
    # 各種特徴量を計算
    df['flesch_reading_ease'] = df[column].progress_apply(lambda x: ts.flesch_reading_ease(str(x)))  # フレスクの読みやすさ
    df['flesch_kincaid_grade'] = df[column].progress_apply(lambda x: ts.flesch_kincaid_grade(str(x)))  # フレスク・キンケイド・グレード
    df['smog_index'] = df[column].progress_apply(lambda x: ts.smog_index(str(x)))  # スモッグ指数
    df['automated_readability_index'] = df[column].progress_apply(lambda x: ts.automated_readability_index(str(x)))  # 自動可読性指数
    df['dale_chall_readability_score'] = df[column].progress_apply(lambda x: ts.dale_chall_readability_score(str(x)))  # デール・チャールの可読性スコア
    df['difficult_words'] = df[column].progress_apply(lambda x: ts.difficult_words(str(x)))  # 難しい単語の数
    df['linsear_write_formula'] = df[column].progress_apply(lambda x: ts.linsear_write_formula(str(x)))  # リンセア・ライティング公式
    df['gunning_fog'] = df[column].progress_apply(lambda x: ts.gunning_fog(str(x)))  # ガニング・フォグ指数
    df['text_standard'] = df[column].progress_apply(lambda x: ts.text_standard(str(x)))  # テキスト標準
    df['fernandez_huerta'] = df[column].progress_apply(lambda x: ts.fernandez_huerta(str(x)))  # フェルナンデス・ウエルタ指数
    df['szigriszt_pazos'] = df[column].progress_apply(lambda x: ts.szigriszt_pazos(str(x)))  # シジリスツ・パソス指数
    df['gutierrez_polini'] = df[column].progress_apply(lambda x: ts.gutierrez_polini(str(x)))  # グティエレス・ポリニ指数
    df['crawford'] = df[column].progress_apply(lambda x: ts.crawford(str(x)))  # クロフォード指数
#     df['gulpease_index'] = df[column].progress_apply(lambda x: ts.gulpease_index(str(x)))  # ギュルペース指数
#     df['osman'] = df[column].progress_apply(lambda x: ts.osman(str(x)))  # オスマン指数
    return df
```

---The following area is a Code cell (cell numver is 18)---
```python
test_expl = get_exploded(df_test)  # テストデータを展開
test_a = get_features(test_expl, 'response_a')  # 応答Aの特徴を取得
test_b = get_features(test_expl, 'response_b')  # 応答Bの特徴を取得
```

---The following area is a Code cell (cell numver is 19)---
```python
unused_columns = ['prompt', 'response_a', 'response_b', 'text_standard']  # 使用しない列の指定
textstat_test = test_a.drop(columns=unused_columns).merge(test_b.drop(columns=unused_columns), on='id')  # 応答AとBの特徴量をマージ
test = textstat_test.groupby('id').mean()  # IDごとに平均を計算
test
```

---The following area is a Code cell (cell numver is 20)---
```python
y_pred = []  # 予測結果を格納するリスト
for clf in tqdm(clfs, total=len(clfs)):  # 学習したモデルを全て使用
    y_predict = clf.predict_proba(test)  # テストデータに対する予測確率を計算
    y_pred.append(y_predict)  # 予測結果をリストに追加
```

---The following area is a Code cell (cell numver is 21)---
```python
y_pred = sum(y_pred) / len(y_pred)  # 予測結果の平均を計算
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# 提出
```

---The following area is a Code cell (cell numver is 23)---
```python
sample_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 24)---
```python
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred  # 予測結果をサンプル提出ファイルに格納
```

---The following area is a Code cell (cell numver is 25)---
```python
sample_df.head()  # サンプル提出ファイルの先頭を表示
```

---The following area is a Code cell (cell numver is 26)---
```python
sample_df.to_csv('submission.csv', index=False)  # 提出ファイルとして保存
```

** @@@ Jupyter Notebook numver 61, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Chatbot Arenaコンペティションにおいて、チャットボットの応答の好みを予測するモデルを構築するためのものです。具体的には、与えられたプロンプトに対して2つの異なる応答のうちどちらが好まれるかを予測するタスクに取り組んでいます。

### 問題の概要
コンペティションのゴールは、ユーザーが好むチャットボットの応答を予測することであり、そのためにさまざまな大規模言語モデル（LLM）を使用して、選好のスコアを生成することが必要です。

### 使用した手法とライブラリ
1. **ライブラリのインストールとインポート**:
   - `pandas`: データ操作に使用。
   - `catboost`: 最終的なモデルの学習にはCatBoostClassifierを利用。
   - `transformers`: Hugging Faceライブラリからのモデルとトークナイザをインポート。
   - `nltk`: 自然言語処理に必要なストップワードの処理。
   - `tqdm`: プログレスバーの表示。
   - `torch`: Pytorchを使用してモデルを扱います。

2. **データ処理**:
   - 入力データの読み込みと前処理を行い、プロンプトや応答をJSON形式から辞書型に変換。
   - ストップワードリストを読み込み、不要な単語の削除を行うための準備。

3. **モデルの構築**:
   - Hugging FaceのPre-trainedモデル（RoBERTa, DeBERTa, DistilBERTなど）を用いて質問応答のパイプラインを構築。
   - それぞれのモデルに対して、データセットに基づいたスコアを計算。

4. **モデルの学習**:
   - 生成された特徴量とターゲット（勝者モデル）を使用して、CatBoostClassifierでモデルを訓練。

5. **予測と提出ファイルの作成**:
   - テストデータに対し、モデルからの予測確率を算出し、サンプル提出ファイルに追加。
   - 最終的にCSVファイルとして提出用のファイルを保存。

### 結果の出力
最終的には、予測結果を含むCSVファイルが生成されます。このノートブックはデータ処理、モデルの選定、訓練、評価、結果のフォーマットまでの一連の流れを示しており、コンペティションにおける優れたモデル構築のための実践的な手法を提供しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリ
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install /kaggle/input/ftfy-dependeces/ftfy-6.2.0-py3-none-any.whl
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
# ロジスティック回帰モデルをインポートするためのコードがコメントアウトされています
# from sklearn.linear_model import LogisticRegression
from catboost import CatBoostClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss
from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline
from ftfy import fix_encoding
import re
import nltk
from nltk.corpus import stopwords
# NLTKのストップワードをダウンロードするためのコードがコメントアウトされています
# nltk.download('stopwords')
from torch.utils.data import Dataset
import torch
import json
import optuna
import numpy as np

from tqdm.auto import tqdm
tqdm.pandas()  # プログレスバーの表示を可能にします
```

---The following area is a Code cell (cell numver is 4)---
```python
# ストップワードリストを読み込み、リスト形式で保存します
stop_words = pd.read_csv("/kaggle/input/nltk-english-stopwords/nltk_eng_stopwords.csv")["list_of_stopwords"].tolist()
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# データの読み込み
```

---The following area is a Code cell (cell numver is 6)---
```python
# 学習データ、テストデータ、サンプル提出ファイルを読み込みます
train_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
test_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")
sample_df = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/sample_submission.csv")
```

---The following area is a Code cell (cell numver is 7)---
```python
# テストデータの行数が10未満の場合、学習データを100行に制限するコードがコメントアウトされています
# if test_df.shape[0] < 10:
#     train_df = train_df[:100]
```

---The following area is a Code cell (cell numver is 8)---
```python
def get_exploded(df: pd.DataFrame) -> pd.DataFrame:
    tmp = df.copy()
    # prmopt列、response_a列、response_b列の各値をJSON形式から辞書型に変換し、進捗バーを表示します
    tmp["prompt"] = tmp["prompt"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_a"] = tmp["response_a"].progress_apply(lambda x: json.loads(fix_encoding(x)))
    tmp["response_b"] = tmp["response_b"].progress_apply(lambda x: json.loads(fix_encoding(x)))

    # 各列でexplodeを適用してデータを展開します
    tmp = tmp.explode(['prompt', 'response_a', 'response_b'])
    return tmp
```

---The following area is a Code cell (cell numver is 9)---
```python
# 学習データとテストデータを展開します
tmp_train = get_exploded(train_df)
tmp_test = get_exploded(test_df)
```

---The following area is a Code cell (cell numver is 10)---
```python
class MyDataset(Dataset):
    def __init__(self, df, col):
        self.col = col
        self.df = df.copy()
        
        # prompt列と指定した列の値をエンコード修正します
        self.df["prompt"] = self.df["prompt"].progress_apply(self.fix_encode)
        self.df[col] = self.df[col].progress_apply(self.fix_encode)
        
        # 各列でexplodeを適用してデータを展開します
        self.df = self.df.explode(['prompt', col])
    
    def fix_encode(self, x):
        return json.loads(fix_encoding(x))  # エンコードを修正します

    def __len__(self):
        return len(self.df)  # データフレームの長さを返します

    def __getitem__(self, i):
        QA_input = {}
        QA_input['question'] = str(self.df.iloc[i]["prompt"])  # 質問を取得します
        QA_input['context'] = str(self.df.iloc[i][self.col])  # コンテキストを取得します
        
        # 質問またはコンテキストが空の場合、ダミーテキストを設定します
        if not QA_input['question']:
            QA_input['question'] = 'empty_text' * 10
        if not QA_input['context']:
            QA_input['context'] = 'empty_text' * 10
        
        # 質問とコンテキストの長さを510文字に制限します
        QA_input['question'] = QA_input['question'][:510]
        QA_input['context'] = QA_input['context'][:510]

        return QA_input  # 質問とコンテキストの辞書を返します


# データセットのインスタンスを作成します
dataset_a = MyDataset(train_df, col='response_a')
dataset_b = MyDataset(train_df, col='response_b')

dataset_a_test = MyDataset(test_df, col='response_a')
dataset_b_test = MyDataset(test_df, col='response_b')
```

---The following area is a Code cell (cell numver is 11)---
```python
# データセットの長さを確認します
len(dataset_a)
```

---The following area is a Code cell (cell numver is 12)---
```python
# 出力結果を保存するための辞書を初期化します
outs_dict = {'dataset_a': [], 'dataset_b': []}
outs_dict_test = {'dataset_a': [], 'dataset_b': []}
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# モデル

## スコアの生成
```

---The following area is a Code cell (cell numver is 14)---
```python
# 使用するモデルのリストを定義します
model_list = [
    "deepset/roberta-base-squad2",
    "deepset/deberta-v3-base-squad2",
    "distilbert/distilbert-base-cased-distilled-squad"
#     "Palak/microsoft_deberta-large_squad"
#     'distilbert/distilbert-base-cased-distilled-squad',
#     'deepset/bert-large-uncased-whole-word-masking-squad2'
]

# Kaggleのパスを加えたモデルリストのコピーを作成します
model_list_kaggle = model_list.copy()
for i, model_name in enumerate(model_list_kaggle):
    model_list_kaggle[i] = '/kaggle/input/deberta-v3-base/' + model_name
model_list_kaggle
```

---The following area is a Code cell (cell numver is 15)---
```python
nlp_list = []

# それぞれのモデルに対して、モデルとトークナイザーを読み込み、パイプラインを作成します
for model_name in model_list_kaggle:
    model = AutoModelForQuestionAnswering.from_pretrained(model_name)
    tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True)
    
    nlp = pipeline('question-answering', model=model, tokenizer=tokenizer, device='cuda', torch_dtype=torch.float16)
    
    nlp_list.append(nlp)  # パイプラインをリストに追加します
```

---The following area is a Code cell (cell numver is 16)---
```python
# モデルのパイプラインを保存します
for model_name, pipeline in tqdm(zip(model_list, nlp_list), total=len(nlp_list)):
    pipeline.save_pretrained(model_name)
```

---The following area is a Code cell (cell numver is 17)---
```python
def get_outs(model_list):
    outs_dict = {}
    # 各モデルに対して出力辞書を初期化します
    for model_name in model_list:
        outs_dict[f'{model_name}-a'] = []
        outs_dict[f'{model_name}-b'] = []
    return outs_dict
```

---The following area is a Code cell (cell numver is 18)---
```python
# 出力用の辞書を作成します
outs_train = get_outs(model_list)
outs_test = get_outs(model_list)
```

---The following area is a Code cell (cell numver is 19)---
```python
# トークナイザーの設定がコメントアウトされています
# tokenizer_kwargs = {"truncation": True, 'max_length': 512, 'padding': True}
```

---The following area is a Code cell (cell numver is 20)---
```python
def get_score(nlp, dataset) -> list:
    scores = []
    # データセットの各サンプルに対してスコアを計算します
    for sample in tqdm(dataset, total=len(dataset)):
        try:
            out = nlp(sample, doc_stride=47)  # NLPパイプラインを使用してスコアを計算します
        except:
            print('omom')  # エラーが発生した場合の処理
            out = {}
            out['score'] = 0  # スコアを0に設定します
        scores.append(out['score'])  # スコアをリストに追加します
    return scores
```

---The following area is a Code cell (cell numver is 21)---
```python
# 各モデルに対してトレーニングとテストのスコアを取得します
for model_name, nlp in tqdm(zip(model_list, nlp_list), total=len(model_list)):
    outs_train[f'{model_name}-a'] = get_score(nlp, dataset_a)
    outs_train[f'{model_name}-b'] = get_score(nlp, dataset_b)
    
    outs_test[f'{model_name}-a'] = get_score(nlp, dataset_a_test)
    outs_test[f'{model_name}-b'] = get_score(nlp, dataset_b_test)
    
    del nlp  # 使用が終わったモデルのパイプラインを削除します
```

---The following area is a Code cell (cell numver is 22)---
```python
# 出力辞書をデータフレームに変換します
outs_train = pd.DataFrame(outs_train)
outs_test = pd.DataFrame(outs_test)
```

---The following area is a Code cell (cell numver is 23)---
```python
# 学習結果を表示します
outs_train.head()
```

---The following area is a Code cell (cell numver is 24)---
```python
# 学習結果とID、勝者モデルの情報を結合します
df_bert = pd.concat([outs_train, tmp_train[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']].reset_index()], axis=1).drop('index', axis=1)
df_bert_test = pd.concat([outs_test, tmp_test['id'].reset_index()], axis=1).drop('index', axis=1)
df_bert.head()
```

---The following area is a Code cell (cell numver is 25)---
```python
# モデルのデータセットを取得します
model_dataset = outs_train.columns
```

---The following area is a Code cell (cell numver is 26)---
```python
# 学習データをIDでグループ化し、平均を計算します
df_bert_train = df_bert.groupby('id').mean()

df_bert_test = df_bert_test.groupby('id').mean()
```

---The following area is a Code cell (cell numver is 27)---
```python
# 学習データとテストデータをCSVファイルに保存します
df_bert_train.to_csv('df_bert_train.csv', index=False)
df_bert_test.to_csv('df_bert_test.csv', index=False)
```

---The following area is a Code cell (cell numver is 28)---
```python
# 学習データの最初の数行を表示します
df_bert_train.head()
```

---The following area is a Markdown cell (cell numver is 29)---
```markdown
## 一つの列に集約
```

---The following area is a Code cell (cell numver is 30)---
```python
# 勝者を示す列を追加します
df_bert_train['winner'] = df_bert_train[['winner_model_a', 'winner_model_b', 'winner_tie']].apply(np.argmax, axis=1)
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 31)---
```python
# 不要な列を削除します
df_bert_train.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], inplace=True)
```

---The following area is a Code cell (cell numver is 32)---
```python
# 変換後のデータを表示します
df_bert_train.head(2)
```

---The following area is a Code cell (cell numver is 33)---
```python
# 最終的なデータをCSVファイルに保存します
df_bert_train.to_csv('deberts.csv', index=False)
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
## モデルのトレーニング
```

---The following area is a Code cell (cell numver is 35)---
```python
# CatBoostClassifierのモデルを定義します
model = CatBoostClassifier(verbose=False, random_state=2024)
```

---The following area is a Code cell (cell numver is 36)---
```python
target = 'winner'  # 予測するターゲット列を定義します
```

---The following area is a Code cell (cell numver is 37)---
```python
# 学習データの最初の数行を表示します
df_bert_train.head()
```

---The following area is a Code cell (cell numver is 38)---
```python
# 特徴量とターゲットを分割します
X_train = df_bert_train.drop(columns=target)  # 特徴量
y_train = df_bert_train[target]  # ターゲット
```

---The following area is a Code cell (cell numver is 39)---
```python
# モデルを学習させます
model.fit(X_train, y_train)
```

---The following area is a Code cell (cell numver is 40)---
```python
# テストデータに対して予測を行います
y_pred = model.predict_proba(df_bert_test)  # 予測確率を取得します
y_pred
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# 提出用ファイルの作成
```

---The following area is a Code cell (cell numver is 42)---
```python
# サンプル提出ファイルに予測結果を追加します
sample_df[['winner_model_a', 'winner_model_b', 'winner_tie']] = y_pred
```

---The following area is a Code cell (cell numver is 43)---
```python
# 提出ファイルの内容を表示します
sample_df
```

---The following area is a Code cell (cell numver is 44)---
```python
# 提出ファイルをCSV形式で保存します
sample_df.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 62, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、「LMSYS - Chatbot Arena人間による好み予測チャレンジ」に参加するための機械学習モデルの構築プロセスを示しています。主な目的は、2つの異なる言語モデル（LLM）が生成した応答の中から、どちらがユーザーに好まれるかを予測することです。

### 問題の取り組み
本Notebookでは、提供されたトレーニングデータを使用して、ユーザーが選択した応答の勝者を予測するモデルを構築しています。具体的には、ユーザーが選んだモデルの応答に基づいて、クラス分類問題を解決します。データの不均衡が存在するため、SMOTE（Synthetic Minority Over-sampling Technique）を用いて不均衡データのリサンプリングも行っています。

### 使用する手法とライブラリ
Notebookでは以下の手法とライブラリが使用されています：
- **NumPyとPandas**: 配列操作やデータフレームの操作に使用され、データの前処理と可視化に役立ちます。
- **Scikit-learn**: モデルの構築と評価に用いられ、特に以下が使用されています。
  - `TfidfVectorizer`: テキストデータをベクトル化するために使用。
  - `LogisticRegression`, `RandomForestClassifier`, `GradientBoostingClassifier`: 様々な分類モデルの構築に使用。
  - `GridSearchCV`: ハイパーパラメータの最適化に利用。
  - `SMOTE`: 不均衡データのオーバーサンプリングに使用。
  - `classification_report`, `log_loss`, `confusion_matrix`, `roc_curve`: モデルの評価指標。
  
- **MatplotlibとSeaborn**: データの可視化によく使われ、クラス分布のプロットや混同行列、ROC曲線などが表示されます。

### プロセスの概要
1. **データ収集**: トレーニングデータとテストデータをCSVファイルから読み込み。
2. **データ理解**: データの情報を確認し、特にクラスの分布などを可視化。
3. **データ準備**: テキストデータのクリーニング（URLやHTMLタグの除去、ストップワードの削除）を行い、テキストデータをTF-IDFベクトルに変換。
4. **特長エンジニアリング**: 応答の長さや自己強調に関する特徴量を追加。
5. **モデルの選択と評価**: モデルをトレーニング、バリデーションセットで評価、最適なハイパーパラメータを探索。
6. **予測**: 最良モデルを用いてテストデータに対する予測確率を生成し、最終的な提出ファイルを作成。

最終的に、テストデータの予測結果を含むCSVファイルを作成し、提出準備を完了します。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# たとえば、以下はいくつかの便利なパッケージのロード方法です

import numpy as np # 線形代数のためのライブラリ
import pandas as pd # データ処理とCSVファイルの入出力 (例: pd.read_csv)

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# たとえば、これを実行すると（クリックするかShift+Enterを押すことで）入力ディレクトリ下の全ファイルが一覧表示されます

import os
for dirname, _, filenames in os.walk('/kaggle/input'): # 指定したディレクトリ内を再帰的に探索します
    for filename in filenames: # 見つかったファイルのリストをループします
        print(os.path.join(dirname, filename)) # 各ファイルのフルパスを出力します

# 現在のディレクトリ (/kaggle/working/) には最大で20GBのデータを書き込むことができ、
# このデータは「保存してすべて実行」を使用してバージョンを作成するときに出力として保存されます
# また、一時ファイルを/kaggle/temp/に書き込むこともできますが、これは現在のセッションの外で保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
# 必要なライブラリをインポートします
import numpy as np  # 数値計算用のライブラリ
import pandas as pd  # データ処理用のライブラリ
import re  # 正規表現操作用のライブラリ
from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFベクトル化用
from sklearn.model_selection import train_test_split, GridSearchCV  # データ分割およびグリッドサーチ用
from sklearn.metrics import classification_report, log_loss, confusion_matrix, roc_curve, auc  # モデル評価用のメトリクス
from imblearn.over_sampling import SMOTE  # 不均衡データを扱うためのオーバーサンプリング手法
from sklearn.linear_model import LogisticRegression  # ロジスティック回帰モデル
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  # ランダムフォレストおよび勾配ブースティングモデル
import matplotlib.pyplot as plt  # グラフ描画用のライブラリ
import seaborn as sns  # データ可視化用のライブラリ

print('全てのライブラリがインポートされました')  # インポートが完了したことを表示します
```

---The following area is a Code cell (cell numver is 3)---
```python
# データ収集
train_file = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのファイルパス
test_file = '/kaggle/input/lmsys-chatbot-arena/test.csv'  # テストデータのファイルパス
train_data = pd.read_csv(train_file)  # トレーニングデータをCSVファイルから読み込みます
test_data = pd.read_csv(test_file)  # テストデータをCSVファイルから読み込みます
```

---The following area is a Code cell (cell numver is 4)---
```python
# データ理解
print("トレーニングデータセット情報:")
print(train_data.info())  # トレーニングデータセットの情報を表示します
print("\n最初の数行:")
print(train_data.head())  # トレーニングデータの最初の数行を表示します

print("\nテストデータセット情報:")
print(test_data.info())  # テストデータセットの情報を表示します
print("\n最初の数行:")
print(test_data.head())  # テストデータの最初の数行を表示します

# リサンプリング前のクラス分布を可視化
plt.figure(figsize=(12, 6))  # グラフのサイズを設定
sns.countplot(x=train_data['winner_model_a'])  # 'winner_model_a'のクラス分布をカウントプロットします
plt.title("リサンプリング前のクラス分布")  # グラフのタイトルを設定
plt.show()  # グラフを表示します
```

---The following area is a Code cell (cell numver is 5)---
```python
# データ準備
def clean_text(text, stop_words):  # テキストをクリーンアップする関数
    text = re.sub(r'\[.*?\]', '', text)  # 角括弧内のテキストを削除
    text = re.sub(r'http\S+|www.\S+', '', text)  # URLを削除
    text = re.sub(r'<.*?>+', '', text)  # HTMLタグを削除
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # アルファベットとスペース以外の文字を削除
    text = text.lower()  # すべての文字を小文字に変換
    text = ' '.join(word for word in text.split() if word not in stop_words)  # ストップワードを削除
    return text  # クリーンアップされたテキストを返す

stop_words = set()  # ストップワードのセットを初期化

# テキストデータをクリーンアップ
train_data['prompt'] = train_data['prompt'].apply(lambda x: clean_text(x, stop_words))  # 'prompt'列のテキストをクリーンアップ
train_data['response_a'] = train_data['response_a'].apply(lambda x: clean_text(x, stop_words))  # 'response_a'列のテキストをクリーンアップ
train_data['response_b'] = train_data['response_b'].apply(lambda x: clean_text(x, stop_words))  # 'response_b'列のテキストをクリーンアップ

test_data['prompt'] = test_data['prompt'].apply(lambda x: clean_text(x, stop_words))  # テストデータの'prompt'列をクリーンアップ
test_data['response_a'] = test_data['response_a'].apply(lambda x: clean_text(x, stop_words))  # テストデータの'response_a'列をクリーンアップ
test_data['response_b'] = test_data['response_b'].apply(lambda x: clean_text(x, stop_words))  # テストデータの'response_b'列をクリーンアップ
```

---The following area is a Code cell (cell numver is 6)---
```python
# テキストデータをベクトル化
vectorizer = TfidfVectorizer(max_features=1000)  # 最大1000特徴を持つTF-IDFベクトルライザーを作成
train_text = train_data['prompt'] + ' ' + train_data['response_a'] + ' ' + train_data['response_b']  # トレーニングテキストを結合
test_text = test_data['prompt'] + ' ' + test_data['response_a'] + ' ' + test_data['response_b']  # テストテキストを結合

X_train_text = vectorizer.fit_transform(train_text)  # トレーニングデータをTF-IDFベクトル化
X_test_text = vectorizer.transform(test_text)  # テストデータをTF-IDFベクトル化

# 表現のバイアス - 応答の長さと長さの差を追加
train_data['response_a_length'] = train_data['response_a'].apply(len)  # 'response_a'の長さを計算
train_data['response_b_length'] = train_data['response_b'].apply(len)  # 'response_b'の長さを計算
test_data['response_a_length'] = test_data['response_a'].apply(len)  # テストデータの'response_a'の長さを計算
test_data['response_b_length'] = test_data['response_b'].apply(len)  # テストデータの'response_b'の長さを計算
train_data['length_diff'] = train_data['response_a_length'] - train_data['response_b_length']  # 応答長の差を計算
test_data['length_diff'] = test_data['response_a_length'] - test_data['response_b_length']  # テストデータの応答長の差を計算

# ポジションバイアス - ポジションバイアスの特徴を追加
train_data['position_bias_a'] = 0  # 'response_a'が常に最初であると仮定
train_data['position_bias_b'] = 1  # 'response_b'が常に2番目であると仮定
test_data['position_bias_a'] = 0
test_data['position_bias_b'] = 1

# セルフエンハンスメントバイアス - セルフエンハンスメント検出の特徴を追加
def detect_self_enhancement(text):  # セルフエンハンスメントを検出する関数
    keywords = ['best', 'better', 'excellent', 'superior', 'number one']  # キーワードリスト
    for keyword in keywords:  # キーワードの各要素について
        if keyword in text:  # キーワードがテキストに含まれているかチェック
            return 1  # 見つかった場合は1を返す
    return 0  # 見つからなかった場合は0を返す

train_data['self_enhancement_a'] = train_data['response_a'].apply(detect_self_enhancement)  # 'response_a'のセルフエンハンスメントを検出
train_data['self_enhancement_b'] = train_data['response_b'].apply(detect_self_enhancement)  # 'response_b'のセルフエンハンスメントを検出
test_data['self_enhancement_a'] = test_data['response_a'].apply(detect_self_enhancement)  # テストデータの'response_a'のセルフエンハンスメントを検出
test_data['self_enhancement_b'] = test_data['response_b'].apply(detect_self_enhancement)  # テストデータの'response_b'のセルフエンハンスメントを検出
```

---The following area is a Code cell (cell numver is 7)---
```python
# カテゴリカル特徴のエンコーディング
categorical_columns = ['model_a', 'model_b']  # カテゴリカル列のリスト
for column in categorical_columns:  # 各カテゴリカル列について
    if column not in test_data.columns:  # テストデータに列が存在しない場合
        test_data[column] = 'missing'  # 存在しない列には'missing'という値を設定
train_data_encoded = pd.get_dummies(train_data, columns=categorical_columns)  # トレーニングデータをダミー変数にエンコード
test_data_encoded = pd.get_dummies(test_data, columns=categorical_columns)  # テストデータをダミー変数にエンコード
train_data_encoded, test_data_encoded = train_data_encoded.align(test_data_encoded, join='left', axis=1, fill_value=0)  # データセットの整合性を確保

# 欠損列の処理
test_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie'], errors='ignore', inplace=True)  # テストデータから無視可能な列を削除

# 非数値列の削除
non_numeric_columns = train_data_encoded.select_dtypes(exclude=[np.number]).columns  # 非数値列を選択
train_data_encoded.drop(columns=non_numeric_columns, inplace=True)  # トレーニングデータから非数値列を削除
test_data_encoded.drop(columns=non_numeric_columns, inplace=True)  # テストデータから非数値列を削除

# すべての特徴をトレーニングおよびテストセットに結合
X_train_combined = np.hstack((X_train_text.toarray(), train_data_encoded.drop(columns=['winner_model_a', 'winner_model_b', 'winner_tie']).values))  # トレーニングセットを結合
X_test_combined = np.hstack((X_test_text.toarray(), test_data_encoded.values))  # テストセットを結合
X = X_train_combined  # 特徴行列Xにトレーニングセットを設定
y = train_data_encoded['winner_model_a']  # 目的変数yにトレーニングデータの'winner_model_a'を設定
```

---The following area is a Code cell (cell numver is 8)---
```python
# モデリング
# データのリサンプリング
smote = SMOTE(random_state=42)  # SMOTEインスタンスを作成（ランダムシードを42に設定）
X_resampled, y_resampled = smote.fit_resample(X, y)  # データをリサンプリング

# リサンプリング後のクラス分布を可視化
plt.figure(figsize=(12, 6))  # グラフのサイズを設定
sns.countplot(x=y_resampled)  # リサンプリング後のクラス分布をカウントプロット
plt.title("リサンプリング後のクラス分布")  # グラフのタイトルを設定
plt.show()  # グラフを表示します
```

---The following area is a Code cell (cell numver is 9)---
```python
# リサンプルされたデータをトレーニングセットとバリデーションセットに分割
X_train, X_val, y_train, y_val = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)  # データを80%トレーニング、20%バリデーションに分割
```

---The following area is a Code cell (cell numver is 10)---
```python
# ハイパーパラメータの調整とモデル選択
models = {  # 使用するモデルの設定
    'Logistic Regression': {
        'model': LogisticRegression(random_state=42, max_iter=1000),  # ロジスティック回帰モデル
        'params': {'C': [0.01, 0.1, 1, 10, 100]}  # ハイパーパラメータの候補
    },
    'Random Forest': {
        'model': RandomForestClassifier(random_state=42),  # ランダムフォレストモデル
        'params': {'n_estimators': [50, 100, 200], 'max_depth': [10, 20, 30]}  # ハイパーパラメータの候補
    },
    'Gradient Boosting': {
        'model': GradientBoostingClassifier(random_state=42),  # 勾配ブースティングモデル
        'params': {'learning_rate': [0.01, 0.1, 0.2], 'n_estimators': [100, 200]}  # ハイパーパラメータの候補
    }
}

best_models = {}  # 最良モデルを格納する辞書

for model_name, config in models.items():  # 各モデルについて
    grid_search = GridSearchCV(estimator=config['model'], param_grid=config['params'], cv=5, scoring='neg_log_loss', verbose=2, n_jobs=-1)  # グリッドサーチを設定
    grid_search.fit(X_train, y_train)  # トレーニングデータでモデルを学習
    best_models[model_name] = grid_search.best_estimator_  # 最良モデルを保存
    print(f"最良の{model_name}モデル: {grid_search.best_params_}")  # 最良ハイパーパラメータを出力
```

---The following area is a Code cell (cell numver is 11)---
```python
# 評価
for model_name, model in best_models.items():  # 最良モデルの各名称とモデルについて
    y_val_pred = model.predict(X_val)  # バリデーションセットに対する予測を行う
    y_val_pred_proba = model.predict_proba(X_val)  # バリデーションセットに対する予測確率を計算
    print(f"バリデーションセットの分類レポート ({model_name}):")
    print(classification_report(y_val, y_val_pred, zero_division=1))  # 分類レポートを表示
    print(f"ログ損失 ({model_name}): {log_loss(y_val, y_val_pred_proba)}")  # ログ損失を表示

    # 混同行列
    cm = confusion_matrix(y_val, y_val_pred)  # 混同行列を計算
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')  # 混同行列をヒートマップで表示
    plt.title(f'混同行列 ({model_name})')  # タイトルを設定
    plt.xlabel('予測値')  # x軸のラベルを設定
    plt.ylabel('真の値')  # y軸のラベルを設定
    plt.show()  # グラフを表示

    # ROC曲線
    fpr, tpr, _ = roc_curve(y_val, y_val_pred_proba[:, 1])  # 偽陽性率と真陽性率を計算
    roc_auc = auc(fpr, tpr)  # ROC曲線の下の面積を計算
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC曲線 (面積 = %0.2f)' % roc_auc)  # ROC曲線をプロット
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')  # 対角線をプロット
    plt.xlim([0.0, 1.0])  # x軸の範囲を設定
    plt.ylim([0.0, 1.05])  # y軸の範囲を設定
    plt.xlabel('偽陽性率')  # x軸のラベルを設定
    plt.ylabel('真陽性率')  # y軸のラベルを設定
    plt.title(f'受信者動作特性 ({model_name})')  # タイトルを設定
    plt.legend(loc="lower right")  # 凡例を表示
    plt.show()  # グラフを表示
```

---The following area is a Code cell (cell numver is 12)---
```python
# 最良モデルを用いたデプロイメント (例: ロジスティック回帰)
best_lr_model = best_models['Logistic Regression']  # 最良のロジスティック回帰モデルを取得
test_predictions_proba = best_lr_model.predict_proba(X_test_combined)  # テストデータに対する予測確率を計算
submission_df = pd.DataFrame({
    'id': test_data['id'],  # テストデータのIDを追加
    'winner_model_a': test_predictions_proba[:, 0],  # モデルAの予測確率を追加
    'winner_model_b': test_predictions_proba[:, 1],  # モデルBの予測確率を追加
    'winner_tie': 0.0  # バイナリ分類であると仮定し、引き分けの列を0.0に設定
})
submission_df.to_csv('submission.csv', index=False)  # 提出用ファイルをCSVとして保存
print(submission_df.head())  # 提出用ファイルの先頭行を表示
print("提出ファイルが正常に保存されました。")  # 保存完了のメッセージを表示
```

** @@@ Jupyter Notebook numver 63, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaにおける人間による好みの予測に関するコンペティションに取り組んでいます。主な目的は、チャットボットの応答に対するユーザーの好みを予測し、どちらのモデルが勝者であるかを判断することです。

ノートブックでは、以下の手法やライブラリが使用されています：

1. **データの取り込みと前処理**：
   - `pandas`を使って訓練データを読み込み、`winner`列を作成するためのカスタム関数`get_winner`を定義しています。この関数は、各行の応答がどちらのモデルによって勝たれたのかを確認します。

2. **データの可視化**：
   - `seaborn`と`matplotlib`を用いて、`winner`列の分布や応答の文字数の相関を可視化しています。

3. **特徴量エンジニアリング**：
   - モデルAとモデルBのそれぞれの応答の長さを新たな特徴量として追加し、これをモデルの学習に使用します。

4. **モデルの学習**：
   - `scikit-learn`の`MultiOutputClassifier`と`RandomForestClassifier`を用いて、応答の長さを特徴量としてモデルを訓練し、予測を行っています。

5. **評価と予測**：
   - 検証データに対する予測結果を得て、`log_loss`を利用してモデルの性能を評価しています。

6. **テストデータに対する予測**：
   - テストセットを読み込み、事前に定義した特徴量エンジニアリングの手順を適用し、最終的な予測結果を生成して`submission.csv`形式で提出用ファイルを作成しています。

全体として、ノートブックは人間の好みを予測するために、ランダムフォレスト分類器を中心とした機械学習手法と特徴量加工を駆使しており、特に応答の長さに着目した解析を行っている点が特徴です。
```

---The following area is a Code cell (cell numver is 1)---
```python
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import log_loss
from sklearn.multioutput import MultiOutputClassifier
import numpy as np

# データセットを読み込む
train_df = pd.read_csv('/kaggle/input/my-files/train.csv')

# 列名を確認する
print(train_df.columns)

# 'winner' 列が正しく作成されているか確認する
def get_winner(row):
    # モデルAが勝者の場合
    if row['winner_model_a'] == 1:
        return 'a'
    # モデルBが勝者の場合
    elif row['winner_model_b'] == 1:
        return 'b'
    # 引き分けの場合
    else:
        return 'tie'

# 'winner' 列を新たに作成する
train_df['winner'] = train_df.apply(get_winner, axis=1)

# 'winner' 列が作成されたことを確認するため最初の数行を表示
print(train_df.head())

# DataFrameに'winner'列が存在するか確認する
print('winner' in train_df.columns)

# 'winner' 変数の分布を可視化する
sns.countplot(data=train_df, x='winner')
plt.title("勝者の分布")
plt.xlabel("勝者")
plt.ylabel("カウント")
plt.show()

# 特徴量エンジニアリング: 各応答の長さを特徴量として追加
train_df['response_a_length'] = train_df['response_a'].apply(len)
train_df['response_b_length'] = train_df['response_b'].apply(len)

# 相関行列を表示する
correlation_matrix = train_df[['response_a_length', 'response_b_length']].corr()
sns.heatmap(correlation_matrix, annot=True)
plt.title("相関行列")
plt.show()

# モデルの学習のためにデータを準備
X = train_df[['response_a_length', 'response_b_length']]
y = train_df['winner']

# 目的変数をワンホットエンコーディングする
y = pd.get_dummies(y)

# データを学習用と検証用に分割する
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# モデルを初期化して学習する
model = MultiOutputClassifier(RandomForestClassifier())
model.fit(X_train, y_train)

# 予測を行う
y_pred = model.predict_proba(X_val)

# 配列のリストを単一の配列に変換する
y_pred = np.array([pred[:, 1] for pred in y_pred]).T

# モデルを評価する
loss = log_loss(y_val, y_pred)
print(f'ログ損失: {loss}')

# 提出ファイルを準備する
# テストセットを読み込む
test_df = pd.read_csv('/kaggle/input/my-files/test.csv')

# テストセットに対して特徴量エンジニアリングの手順を追加
test_df['response_a_length'] = test_df['response_a'].apply(len)
test_df['response_b_length'] = test_df['response_b'].apply(len)

# テストセットで予測を行う
test_pred = model.predict_proba(test_df[['response_a_length', 'response_b_length']])

# 配列のリストを単一の配列に変換する
test_pred = np.array([pred[:, 1] for pred in test_pred]).T

# 提出用に予測結果をフォーマットする
submission = pd.DataFrame({
    'id': test_df['id'],
    'winner_model_a': test_pred[:, 0],
    'winner_model_b': test_pred[:, 1],
    'winner_tie': test_pred[:, 2]
})

# 提出ファイルを保存する
submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 64, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、ユーザーの応答選好を予測するためのモデルを開発することを目的としています。特に、事前トレーニング済みのLLM（大規模言語モデル）をファインチューニングするために、LoRA（Low-Rank Adaptation）を用いた手法が採用されています。

## 取り組んでいる問題
このノートブックは、Chatbot Arenaのデータセットを利用して、ユーザーがどちらのチャットボットの応答を好むかを予測するモデルの構築に焦点を当てています。データセットには、さまざまなプロンプトに対応する複数の応答が含まれており、ユーザーの好みを学習し予測することが目的です。

## 使用している手法
主な手法として、以下の技術が使用されています：
1. **LoRA**: モデルのファインチューニングのために用いられる低ランク適応手法。これにより、パラメータ数を削減しつつモデルの性能を維持し、効率的なトレーニングが可能になります。
2. **量子化**: モデルを4ビットで読み込み、メモリ使用量を削減し、計算効率を向上させるために`BitsAndBytesConfig`を使用しています。
3. **データ前処理**: ユーザープロンプトと応答を結合し、トークナイズ（テキストデータを数値に変換する処理）を行います。この処理にはHugging Faceの`transformers`ライブラリが利用されています。

## 使用しているライブラリ
- **Pandas**: データの操作と分析。
- **PyTorch**: モデルのトレーニングとデータローディングに使用。
- **Transformers**: 事前トレーニング済みモデルやトークナイザーの取得。
- **PEFT (Parameter Efficient Fine-Tuning)**: LoRAを使用するための機能。
- **Datasets**: データセットの管理と前処理を簡素化。
- **TRL (Transformers Reinforcement Learning)**: モデルのトレーニングとフィードバックのためのフレームワーク。

約1エポックでモデルをトレーニングし、エポックごとに評価を行いながら、最良のモデルを保存する仕組みが組まれています。

このノートブックは、最終的には、ユーザーが好む応答を予測する効果的なモデルを構築することを目指しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
!pip install -q -U bitsandbytes
!pip install -q -U transformers
!pip install -q -U peft
!pip install -q -U accelerate
!pip install -q -U datasets
!pip install -q -U trl
```

---The following area is a Code cell (cell numver is 2)---
```python
import pandas as pd
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
import os
import torch
from time import time
from datasets import load_dataset
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    AutoTokenizer,
    TrainingArguments,
    AutoModelForSequenceClassification,
    Trainer,
    EarlyStoppingCallback
)
from trl import SFTTrainer,setup_chat_format
import numpy as np
from transformers import BitsAndBytesConfig, AutoModelForSequenceClassification
from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model
from transformers import AutoTokenizer
from datasets import DatasetDict, Dataset
```

---The following area is a Code cell (cell numver is 3)---
```python
class CFG:
    VER = 1
    NUM_LABELS = 3
    BATCH_SIZE = 4
    EPOCHS = 1
    
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'

    
    SEED = 2024 
    MAX_LENGTH = 1024 
    
    OUTPUT_DIR = 'Llama 3 8b fine-tuned model'
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# データセットの準備
```

---The following area is a Code cell (cell numver is 5)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
train = train[:int(len(train)*0.01)]

# 入力文字列を処理する関数
def process(input_str):
    stripped_str = input_str.strip('[]')  # 角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文を取り出し、前後の空白を取り除く
    return  ' '.join(sentences)  # 文を結合して返す

# プロンプトと応答を処理
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# 'Null'の応答を削除して訓練データを整形
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)  # 'null'行を削除
train.reset_index(inplace=True, drop=True)  # インデックスをリセット
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # ラベルを設定

print(f"Total {len(indexes)} Null response rows dropped")  # 削除された行数を表示
print('Total train samples: ', len(train))  # 残りのサンプル数を表示

# 入力と応答をテキスト形式に変換
train['text'] = 'ユーザープロンプト: ' + train['prompt'] +  '\n\nモデルA :\n' + train['response_a'] +'\n\n--------\n\nモデルB:\n'  + train['response_b']
print(train['text'][4])  # 5番目のテキストを表示
```

---The following area is a Code cell (cell numver is 6)---
```python
from sklearn.model_selection import train_test_split

train_df, val_df = train_test_split(train, test_size=0.2, random_state=42)  # データを訓練データと検証データに分割

dataset_train = Dataset.from_pandas(train_df)  # 訓練データセットの作成
dataset_val = Dataset.from_pandas(val_df)  # 検証データセットの作成

# データセットを1つのDatasetDictに結合
dataset = DatasetDict({
    'train': dataset_train,
    'val': dataset_val,
})
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# モデルの読み込み - 量子化
```

---The following area is a Code cell (cell numver is 8)---
```python
quantization_config = BitsAndBytesConfig(
    load_in_4bit = True,  # 4ビットでモデルを読み込む設定
    bnb_4bit_quant_type = 'nf4',  # 量子化のタイプ
    bnb_4bit_use_double_quant = True,  # ダブル量子化を使用するか
    bnb_4bit_compute_dtype = torch.bfloat16  # 計算のデータ型
)

# 事前トレーニング済みモデルを読み込み
model = AutoModelForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,  # モデル名
    quantization_config=quantization_config,  # 量子化設定を適用
    num_labels=CFG.NUM_LABELS,  # ラベルの数
    device_map='auto'  # 自動でデバイスを設定
)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# LoRAの設定
```

---The following area is a Code cell (cell numver is 10)---
```python
lora_config = LoraConfig(
    r = 16,  # LoRAのrパラメータ
    lora_alpha = 8,  # 構成パラメータ
    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],  # ターゲットモジュールの指定
    lora_dropout = 0.05,  # ドロップアウト率
    bias = 'none',  # バイアス設定
    task_type = 'SEQ_CLS'  # タスクのタイプ
)

# モデルのkビットトレーニング用の準備
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)  # LoRAモデルを取得
```

---The following area is a Code cell (cell numver is 11)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME, add_prefix_space=True)  # トークナイザーを読み込み

# パディングトークンの設定
tokenizer.pad_token_id = tokenizer.eos_token_id  # パディングトークンIDを設定
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを設定
tokenizer.padding_side = 'right'  # パディング側を右に設定
tokenizer.add_eos_token = True  # EOSトークンを追加
```

---The following area is a Code cell (cell numver is 12)---
```python
model.config.pad_token_id = tokenizer.pad_token_id  # モデルにパディングトークンIDを設定
# model.config.use_cache = False  # キャッシュの使用を無効化する設定（コメントアウト）
model.config.pretraining_tp = 1  # プリトレーニングのタイポロジを設定
```

---The following area is a Code cell (cell numver is 13)---
```python
# データの前処理を行う関数
def data_preprocesing(row):
    return tokenizer(row['text'], padding='max_length', truncation=True, max_length=CFG.MAX_LENGTH, return_tensors='np')  # テキストをトークナイズ

# トークナイズされたデータを生成
tokenized_data = dataset.map(data_preprocesing, batched=True, remove_columns=['text'])  # データセットにマッピング
tokenized_data.set_format("torch")  # PyTorch形式に設定
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# Llama 3のファインチューニング: モデルのトレーニング
```

---The following area is a Code cell (cell numver is 15)---
```python
training_args = TrainingArguments(
    per_device_train_batch_size=CFG.BATCH_SIZE,  # デバイスごとの訓練バッチサイズ
    num_train_epochs=CFG.EPOCHS,  # 訓練エポック数
    logging_dir = f'./logs_v{CFG.VER}',  # ロギングディレクトリ
    output_dir = f'./output_v{CFG.VER}',  # 出力ディレクトリ
    logging_steps=10,  # ロギングのステップ数
    save_steps=10,  # モデルの保存のステップ数
    logging_first_step=True,  # 最初のステップのロギングを有効にする
    overwrite_output_dir=True,  # 出力ディレクトリの上書きを許可
    warmup_ratio=0.0,  # ウォームアップ比率
    learning_rate=5e-5,  # 学習率
    lr_scheduler_type='constant',  # 学習率スケジューラのタイプ
    weight_decay=0.01,  # 重みの減衰
    eval_steps=10,  # 評価のステップ数
    evaluation_strategy='steps',  # 評価戦略
    save_total_limit=2,  # 保存するトータル数の制限
    report_to='none',  # ロギングの報告先をなしに設定
    load_best_model_at_end = True  # 最後に最良モデルを読み込む
)
```

---The following area is a Code cell (cell numver is 16)---
```python
trainer = Trainer(
    model=model,  # 学習させるモデル
    args=training_args,  # 訓練の引数
    train_dataset=tokenized_data['train'],  # 訓練データセット
    eval_dataset=tokenized_data['val'],  # 評価データセット
    callbacks=[EarlyStoppingCallback(early_stopping_patience=10)]  # 早期終了のコールバック
)

train_result = trainer.train()  # モデルの学習を開始
```

** @@@ Jupyter Notebook numver 65, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、人間による好み予測に関連した問題に取り組んでいます。具体的には、大規模言語モデル（LLM）が生成したチャットボットの応答の中から、どちらの応答が人間に好まれるかを予測するための機械学習モデルを構築しています。

### 問題の背景
コンペティションの課題は、二つのチャットボットが提供する応答に対するユーザーの好みを予測することです。このため、提供されたデータセットから学習し、テストセットに対する予測を行う必要があります。

### 使用されている手法とライブラリ
1. **ライブラリのインストール**: コンピュータビジョンライブラリや機械学習、トランスフォーマーモデルのサポートをするため、以下のライブラリを使用しています。
   - `accelerate`, `aiohttp`, `bitsandbytes`, `triton`, `unsloth`, `peft`, `trl`, `transformers`, `datasets`など。
   
2. **PyTorch**及びCUDAの使用: GPU上での高速な計算を実現するために、NVIDIAのCUDA関連のライブラリもインストールされています。PyTorchフレームワークを使用して、深層学習モデルを実装しています。

3. **データ前処理**: データセットの読み込みと処理が行われ、プロンプト生成関数が定義されています。訓練データ、評価データ、テストデータが生成され、モデルに渡されます。

4. **モデルの設定**: トークナイザーとモデルが初期化され、LoRA（Low-Rank Adaptation）を通じて効率的な微調整が行われます。

5. **学習プロセス**: `SFTTrainer`クラスが用いられ、ハイパーパラメータやトレーニング戦略が設定され、モデルの訓練が行われます。

6. **推論と評価**: 学習後、テストデータに対する応答の予測が行われ、それぞれの予測結果をもとに評価関数が実行されます。また、ロジット（各モデルの応答に対する確率予測）の抽出も行われ、最終的に結果をCSVファイルとして出力する準備が整います。

このNotebookは、データの前処理からモデルの構築、学習、評価、そして最終的な予測結果の出力までを包含しており、機械学習フレームワークや自然言語処理（NLP）技術を幅広く実践しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# 必要なライブラリをインストールします
# accelerateパッケージをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/accelerate-0.29.3-py3-none-any.whl'
# aiohttpライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/aiohttp-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'
# bitsandbytesライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl'
# fsspecライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/fsspec-2024.3.1-py3-none-any.whl'
# tritonライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
# unslothライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/unsloth-2024.4-py3-none-any.whl'
# peftライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/peft-0.10.0-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 2)---
```python
# NVIDIAのCUDA関連ライブラリをインストールします
# nvrtcライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
# runtimeライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
# cuptiライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
# cublasライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl'
# cudnnライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl'
# cufftライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl'
# curandライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl'
# nvjitlinkライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvjitlink_cu12-12.3.52-py3-none-manylinux1_x86_64.whl'
# cusparseライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl'
# cusolverライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl'
# nvtxライブラリをインストール
!pip install '/kaggle/input/llm-detect-pip/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl'
# NCCLライブラリをインストール（コメントアウトされています）
#!pip install '/kaggle/input/nvidia-nccl-2-19-3/nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl'
# Tritonライブラリをインストール（コメントアウトされています）
#!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl'
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なパッケージをインストールします
# shtabライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/shtab-1.7.1-py3-none-any.whl'
# tyroライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/tyro-0.8.3-py3-none-any.whl'
# trlライブラリをインストール
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/trl-0.8.5-py3-none-any.whl'
```

---The following area is a Code cell (cell numver is 4)---
```python
# NVIDIAのNCCLライブラリをインストールします
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 5)---
```python
# Tritonライブラリをインストールします
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl'
```

---The following area is a Code cell (cell numver is 6)---
```python
# PyTorchライブラリをインストールします
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl'
```

---The following area is a Code cell (cell numver is 7)---
```python
# xformersライブラリをインストールします
!pip install '/kaggle/input/easy-to-use-unslot-wheel/pip-wheel/xformers-0.0.22.post7-cp310-cp310-manylinux2014_x86_64.whl'
```

---The following area is a Code cell (cell numver is 8)---
```python
# torchライブラリをインポートします
import torch
# 現在のPyTorchのバージョンを表示します
print(f"pytorch version {torch.__version__}")  # PyTorchのバージョンを出力します
```

---The following area is a Code cell (cell numver is 9)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 10)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 11)---
```python
# unslothライブラリからFastLanguageModelクラスをインポートします
from unsloth import FastLanguageModel  # 効率的な言語モデルを使用するためのクラスです
```

---The following area is a Code cell (cell numver is 12)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 13)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 14)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 15)---
```python
# CUDAのメモリ効率的なSDP（Scalable Data Parallel）を無効にします
torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率的SDPを無効にします
# CUDAのフラッシュSDPを無効にします
torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にします
```

---The following area is a Code cell (cell numver is 16)---
```python
# pandasライブラリをインポートします
import pandas as pd  # データ操作と分析を行うためのライブラリ
# numpyライブラリをインポートします
import numpy as np  # 配列操作や数値計算のためのライブラリ
```

---The following area is a Code cell (cell numver is 17)---
```python
# 必要なライブラリをインポートします
import os  # オペレーティングシステムに対する操作を行うためのライブラリ
from tqdm import tqdm  # 進捗バーを表示するためのライブラリ
import bitsandbytes as bnb  # メモリ効率を考慮したデータ型を扱うライブラリ
import torch  # PyTorchライブラリをインポート
import torch.nn as nn  # ニューラルネットワーク関連のモジュールを使用
import transformers  # Hugging Faceのtransformersライブラリをインポート
from datasets import Dataset  # データセットを扱うためのモジュール
from peft import LoraConfig, PeftConfig  # PEFT関連の設定をインポート
from peft import PeftModel  # PEFTモデルをインポート
from trl import SFTTrainer  # SFTTrainerをインポート
from trl import setup_chat_format  # チャットフォーマットを設定する関数をインポート
from transformers import (  # transformersライブラリから必要なコンポーネントをインポート
    AutoModelForCausalLM,  # 自己回帰型言語モデルのための自動モデル
    AutoTokenizer,  # 自動的にトークナイザーを取得するクラス
    BitsAndBytesConfig,  # メモリ効率の良い設定用のクラス
    TrainingArguments,  # 学習時の引数を定義するためのクラス
    pipeline,  # トークナイザーやモデルを簡単に使用するためのパイプライン
    logging  # ログ出力を扱うためのモジュール
)
from sklearn.metrics import (  # sklearnからメトリクスをインポート
    accuracy_score,  # 正確度を計算するための関数
    classification_report,  # 分類結果のレポートを生成するための関数
    confusion_matrix  # 混同行列を生成するための関数
)
from sklearn.model_selection import train_test_split  # トレーニングとテストのデータ分割を行うための関数
```

---The following area is a Code cell (cell numver is 18)---
```python
# テストデータセットを読み込みます
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))  # CSVファイルを読み込み、DataFrameに格納します
```

---The following area is a Code cell (cell numver is 19)---
```python
# テストプロンプトを生成する関数を定義します
def generate_test_prompt(data_point):
    # 与えられたデータポイントに基づいて、会話を分析するプロンプトを生成します
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred 
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}  # プロンプトを表示します
Model A Response: {data_point["response_a"]}  # モデルAの応答を表示します
Model B Response: {data_point["response_b"]}  # モデルBの応答を表示します
"""
```

---The following area is a Code cell (cell numver is 20)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 21)---
```python
# モデルとトークナイザーの設定を行います
'''
MODEL_PATH = "/kaggle/input/llama-3/transformers/8b-hf/1"  # 使用するモデルのパスを指定します

# 量子化設定を定義します
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビット量子化を使用
    bnb_4bit_quant_type="nf4",  # 量子化タイプを指定
    bnb_4bit_compute_dtype=torch.float16,  # 計算に使用するデータ型を指定
    bnb_4bit_use_double_quant=True,  # ダブル量子化を使用
)

# モデルを事前学習済みのパスから読み込む
model1 = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",  # 自動的にデバイスマッピングを設定
    trust_remote_code=True,  # リモートコードを信頼
    quantization_config=quantization_config,  # 先ほど定義した量子化設定を使用
)

# トークナイザーをモデルのパスから読み込む
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, dtype=torch.float16)  # データ型をfloat16に設定
'''
```

---The following area is a Code cell (cell numver is 22)---
```python
# モデルの設定を行います
max_seq_length = 2048  # シーケンスの最大長を指定します。内部でRoPEスケーリングを自動サポートします。
dtype = torch.float16  # データ型を指定。自動検出にしない場合はfloat16を使用。Tesla T4やV100の場合はfloat16、Ampere+の場合はBfloat16を選択。
load_in_4bit = True  # メモリ使用量を抑えるために4ビット量子化を使用します。Falseを選択すると使用しません。

# 4ビット事前量子化モデルのリスト（ダウンロードが4倍速く、OOM（メモリ不足）を回避できます）
fourbit_models = [
    "unsloth/mistral-7b-bnb-4bit",  # サポートされているモデル

    "unsloth/llama-2-7b-bnb-4bit",  # 他のモデルもここに追加できます
    "unsloth/llama-2-13b-bnb-4bit",
    "unsloth/codellama-34b-bnb-4bit",
    "unsloth/tinyllama-bnb-4bit",
    "unsloth/llama-3-8b-bnb-4bit"
]  # さらに多くのモデルは https://huggingface.co/unsloth で確認できます

# FastLanguageModelを使用してモデルとトークナイザーを事前学習済みのものから読み込みます
model, tokenizer = FastLanguageModel.from_pretrained(
    model_name="/kaggle/input/unslothmistral-7b-instruct-v0-2-bnb-4bit/unsloth-mistral-7b-instruct-v0.2-bnb-4bit",  # 任意のモデルを選択
    max_seq_length=max_seq_length,  # 最大シーケンス長を指定
    dtype=dtype,  # データ型を指定
    load_in_4bit=load_in_4bit,  # 4ビット量子化を使用するかどうかを指定
    # token="hf_...",  # gatedモデルを使用する際に必要なトークン（必要に応じてコメントを解除）
)
```

---The following area is a Code cell (cell numver is 23)---
```python
# PEFT（Parameter-Efficient Fine-Tuning）モデルを取得します
model = FastLanguageModel.get_peft_model(
    model,
    r=16,  # 任意の正の整数を指定します。推奨値は8, 16, 32, 64, 128です。
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj",  # PEFTを適用するモジュールを指定
                    "gate_proj", "up_proj", "down_proj",],
    lora_alpha=16,  # LoRAのα値を指定
    lora_dropout=0,  # LoRAのドロップアウト率。任意の値を指定できますが、0が最適化されています。
    bias="none",  # バイアス設定。任意の値をサポートしますが、"none"が最適化されています。
    # [NEW] "unsloth"は30%少ないVRAMを使用し、2倍大きなバッチサイズに対応します！
    use_gradient_checkpointing="unsloth",  # 長いコンテキストのためにTrueまたは"unsloth"を指定
    random_state=3407,  # 乱数の状態を指定
    use_rslora=False,  # ランク安定LoRAを使用するかどうかを指定
    loftq_config=None,  # LoftQの設定を指定
)
```

---The following area is a Code cell (cell numver is 24)---
```python
# 提出ファイルとデータセットを読み込みます
submission = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv'))  # サンプル提出ファイルを読み込み
test = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/test.csv'))  # テストデータセットを読み込み
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")  # トレーニングデータセットを読み込み
```

---The following area is a Code cell (cell numver is 25)---
```python
# トレーニングデータセットにPreference列を追加します
result = []  # 結果を格納するリストを初期化
# トレーニングデータセットの各行を反復処理
for index, row in train.iterrows():
    # 各勝者モデルの情報に基づいて結果を追加
    if(row['winner_model_a'] == 1): 
        result.append('winner_model_a')  # モデルAが勝者の場合
    elif(row['winner_model_b'] == 1): 
        result.append('winner_model_b')  # モデルBが勝者の場合
    elif(row['winner_tie'] == 1): 
        result.append('winner_tie')  # 引き分けの場合

# トレーニングデータフレームの6列目に新しいPreference列を挿入
train.insert(6, "Preference", result, True)  
train  # 更新されたトレーニングデータフレームを表示
```

---The following area is a Code cell (cell numver is 26)---
```python
# トレーニングデータセットの最初の1000行を選択します
train = train[:1000]  # 先頭1000行を抽出
train.shape  # データフレームの形状を表示（行数と列数）
```

---The following area is a Code cell (cell numver is 27)---
```python
# トレーニングデータとテストデータを分割します
train_size = int(train.shape[0] * 0.6)  # トレーニングデータのサイズを全体の60%に設定
test_val_size = train.shape[0] - train_size  # 残りのデータをテストとバリデーション用に設定

# トレーニングデータをトレーニングセットとテスト・バリデーションセットに分割
train_, test_val = train_test_split(train, train_size=train_size,
                                     test_size=test_val_size,
                                     random_state=42)  # ランダム状態を42に設定

# テスト・バリデーションセットをさらにテストセットとバリデーションセットに分割
test_, val_ = train_test_split(test_val, train_size=int(test_val_size / 2),
                                test_size=int(test_val_size / 2),
                                random_state=42)  # 同様にランダム状態を42に設定
```

---The following area is a Code cell (cell numver is 28)---
```python
# プロンプトと応答を分析するための関数を定義します

# 過去のデータポイントからプロンプトを生成する関数
def past_generate_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}  # プロンプトを表示
            ----------------------------------------------------------------------------------------------------------
            model_a: {data_point["model_a"]}  # モデルAの情報を表示
            response_a: {data_point["response_a"]}  # モデルAの応答を表示
            ----------------------------------------------------------------------------------------------------------
            model_b: {data_point["model_b"]}  # モデルBの情報を表示
            response_b: {data_point["response_b"]}  # モデルBの応答を表示
            ----------------------------------------------------------------------------------------------------------
            Preference= {data_point["Preference"]} """  # 人間の好みを表示

# 現在のデータポイントからプロンプトを生成する関数
def generate_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}  # プロンプトを表示
Model A Response: {data_point["response_a"]}  # モデルAの応答を表示
Model B Response: {data_point["response_b"]}  # モデルBの応答を表示
Human Preference Label: {data_point["Preference"]}  # 人間の好みラベルを表示
"""

# テストデータポイントからプロンプトを生成する関数
def past_generate_test_prompt(data_point):
    return f"""
            Analyze the prompt and responses(response_a, response_b) from two chatbots(model_a, model_b).
            Then predict the human preference of those responses- if it is "winner_model_a", "winner_model_b" or
            "winner_tie". Return the answer as the correspoding preference label "winner_model_a", "winner_model_b" or
            "winner_tie" and the logits for each label in the order of preference.
            ----------------------------------------------------------------------------------------------------------
            prompt: {data_point["prompt"]}  # プロンプトを表示
            ----------------------------------------------------------------------------------------------------------

            response_a: {data_point["response_a"]}  # モデルAの応答を表示
            ----------------------------------------------------------------------------------------------------------

            response_b: {data_point["response_b"]}  # モデルBの応答を表示
            ----------------------------------------------------------------------------------------------------------
            Preference: """  # 人間の好みを表示

# 現在のテストデータポイントからプロンプトを生成する関数
def generate_test_prompt(data_point):
    return f"""Analyze the conversation between two chatbots (model_a and model_b) and their corresponding responses (response_a and response_b) to a given prompt. Determine which model provided the more preferred
response based on the human preference label (Preference). Return the predicted preference as one of three labels: 'winner_model_a', 'winner_model_b', or 'winner_tie', along with the logits for each label.

Prompt: {data_point["prompt"]}  # プロンプトを表示
Model A Response: {data_point["response_a"]}  # モデルAの応答を表示
Model B Response: {data_point["response_b"]}  # モデルBの応答を表示
"""
```

---The following area is a Code cell (cell numver is 29)---
```python
# トレーニング、評価、テストデータセットを生成します

# トレーニングデータセットのプロンプトを生成し、DataFrameに変換
X_train = pd.DataFrame(train_.apply(generate_prompt, axis=1), columns=["text"])
# 評価データセットのプロンプトを生成し、DataFrameに変換
X_eval = pd.DataFrame(val_.apply(generate_prompt, axis=1), columns=["text"])
# テストデータセットのプロンプトを生成し、DataFrameに変換
X_test = pd.DataFrame(test_.apply(generate_test_prompt, axis=1), columns=["text"])

# テストデータの真のラベルを取得
y_true = test_.Preference

# pandas DataFrameからデータセットを生成
train_data = Dataset.from_pandas(X_train)  # トレーニングデータをDatasetに変換
eval_data = Dataset.from_pandas(X_eval)    # 評価データをDatasetに変換
```

---The following area is a Code cell (cell numver is 30)---
```python
# モデルの評価を行う関数を定義します
def evaluate(y_true, y_pred):
    labels = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ラベルを定義
    mapping = {'none': 0, 'winner_model_a': 1, 'winner_model_b': 2, 'winner_tie': 3}  # ラベルのマッピングを定義

    # ラベルを対応する数値にマッピングする関数
    def map_func(x):
        return mapping.get(x, 1)  # デフォルトは1（winner_model_a）

    # 真のラベルと予測ラベルをマッピングして変換
    y_true = np.vectorize(map_func)(y_true)
    y_pred = np.vectorize(map_func)(y_pred)

    # 正確度を計算
    accuracy = accuracy_score(y_true=y_true, y_pred=y_pred)
    print(f'Accuracy: {accuracy:.3f}')  # 正確度を出力

    # 各ラベルの正確度を生成
    unique_labels = set(y_true)  # ユニークなラベルを取得

    for label in unique_labels:
        # 各ラベルのインデックスを取得
        label_indices = [i for i in range(len(y_true)) if y_true[i] == label]
        label_y_true = [y_true[i] for i in label_indices]  # 真のラベル
        label_y_pred = [y_pred[i] for i in label_indices]  # 予測ラベル
        accuracy = accuracy_score(label_y_true, label_y_pred)  # 各ラベルの正確度を計算
        print(f'Accuracy for label {label}: {accuracy:.3f}')  # 各ラベルの正確度を出力

    # 分類レポートを生成
    class_report = classification_report(y_true=y_true, y_pred=y_pred)
    print('\nClassification Report:')
    print(class_report)  # 分類レポートを出力

    # 混同行列を生成
    conf_matrix = confusion_matrix(y_true=y_true, y_pred=y_pred, labels=[0, 1, 2])
    print('\nConfusion Matrix:')
    print(conf_matrix)  # 混同行列を出力
```

---The following area is a Code cell (cell numver is 31)---
```python
# SFT（Supervised Fine-Tuning）トレーナーをインポートします
from trl import SFTTrainer
from transformers import TrainingArguments

# SFTTrainerのインスタンスを作成します
trainer = SFTTrainer(
    model=model,  # 学習するモデル
    tokenizer=tokenizer,  # 使用するトークナイザー
    train_dataset=train_data,  # トレーニングデータセット
    eval_dataset=eval_data,  # 評価データセット
    dataset_text_field="text",  # テキストフィールド
    max_seq_length=max_seq_length,  # 最大シーケンス長
    dataset_num_proc=2,  # データセットの処理を並列で行うプロセス数
    packing=False,  # 短いシーケンスのトレーニングを5倍速くすることができます
    args=TrainingArguments(
        per_device_train_batch_size=1,  # デバイスあたりのトレーニングバッチサイズ
        gradient_accumulation_steps=2,  # 勾配蓄積ステップ
        warmup_steps=5,  # ウォームアップステップ
        #evaluation_strategy="steps",  # 各エポックの終わりに評価
        #save_strategy="steps",  # チェックポイントを保存する戦略
        #save_steps=10,  # チェックポイントを500ステップごとに保存
        #eval_steps=10,
        max_steps=100,  # 最大ステップ数
        learning_rate=2e-4,  # 学習率
        fp16=True,  # フロート16精度を使用
        bf16=False,  # Bfloat16精度を使用しない
        logging_steps=1,  # ログ出力のステップ数
        optim="adamw_8bit",  # オプティマイザー
        weight_decay=0.01,  # 重みの減衰
        #load_best_model_at_end=True,  # 最良モデルを最後に読み込む
        lr_scheduler_type="linear",  # 学習率スケジューラーのタイプ
        seed=3407,  # シード値
        output_dir="outputs",  # 出力ディレクトリ
    ),
)
```

---The following area is a Code cell (cell numver is 32)---
```python
# W&B（Weights & Biases）をインポートし、初期化します
import wandb
wandb.init(mode='disabled')  # W&Bのモードを無効にして初期化します（ログを送信しない）
```

---The following area is a Code cell (cell numver is 33)---
```python
# ガーベジコレクションを実行します
import gc  # ガーベジコレクションモジュールをインポート
gc.collect()  # 使用されていないメモリを解放するためにガーベジコレクションを実行
```

---The following area is a Code cell (cell numver is 34)---
```python
# モデルのトレーニングを開始します
trainer.train()  # 定義したtrainerオブジェクトを使用してトレーニングを実行
```

---The following area is a Code cell (cell numver is 35)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 36)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 37)---
```python
# テストデータにプロンプトを生成するテキスト列を追加します
test['text'] = test.apply(lambda x: generate_test_prompt(x), axis=1)  # 各テストデータポイントに対してプロンプトを生成し、'text'列に追加
```

---The following area is a Code cell (cell numver is 38)---
```python
# テストデータの特定のプロンプトを使用してモデルを推論します
'''
prompt = test.loc[2]["text"]  # テストデータのインデックス2からプロンプトを取得

#print(prompt)  # プロンプトを表示するためのコメントアウト

#FastLanguageModel.for_inference(model)  # ネイティブ2倍速の推論を有効にします

# トークナイザーを使用してプロンプトを入力テンソルに変換し、指定したデバイスに移動
inputs = tokenizer(prompt, return_tensors='pt').to(device)

# モデルを使用して出力を生成
outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)  # 最大64の新しいトークンを生成
out1 = tokenizer.batch_decode(outputs)  # 出力をデコード
print("*************")  # 出力のセパレータ
print(out1)  # 出力を表示
'''
```

---The following area is a Code cell (cell numver is 39)---
```python
# テキストからロジットを抽出する関数を定義します
def extract_logits(text):
    try:
        d = {'winner_a': [], 'winner_b': [], 'winner_tie': []}  # 各クラスのロジットを格納する辞書を初期化
        # テキストからロジットの部分を抽出
        logits = re.findall(r"Logits: \[(.*?)\]", text)[0]  # ロジットの情報を正規表現で取得
        # ロジットの辞書を生成
        logits_dict = {k.strip(): float(v) for k, v in [item.split(": ") for item in logits[1:-1].split(", ")]}

        # ロジットを各クラスに分類
        for k, v in logits_dict.items():
            if '_a' in k:
                d['winner_a'].append(v)  # モデルAのロジットを追加
            elif '_b' in k:
                d['winner_b'].append(v)  # モデルBのロジットを追加
            elif '_tie' in k:
                d['winner_tie'].append(v)  # 引き分けのロジットを追加
            
        df = pd.DataFrame.from_dict(d)  # 辞書からDataFrameを作成
    
    except:
        d = {'winner_a': [0.33], 'winner_b': [0.33], 'winner_tie': [0.33]}  # エラー時のデフォルト値を設定
        df = pd.DataFrame.from_dict(d)  # デフォルトのDataFrameを作成

    return df  # データフレームを返す
```

---The following area is a Code cell (cell numver is 40)---
```python
# テストデータの特定のプロンプトを使用してモデルを推論します
prompt = X_test.iloc[2]["text"]  # テストデータのインデックス2からプロンプトを取得

#print(prompt)  # プロンプトを表示するためのコメントアウト

FastLanguageModel.for_inference(model)  # ネイティブ2倍速の推論を有効にします

# トークナイザーを使用してプロンプトを入力テンソルに変換し、CUDAデバイスに移動
inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#print(inputs)  # 入力テンソルを表示するためのコメントアウト

print("*************")  # 出力のセパレータ
# モデルを使用して出力を生成
outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)  # 最大64の新しいトークンを生成
out1 = tokenizer.batch_decode(outputs)  # 出力をデコード

print(out1)  # 出力を表示
```

---The following area is a Code cell (cell numver is 41)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 42)---
```python
# 正規表現をインポートし、人間の好みラベルを抽出するためのパターンを定義します
# import re  # 正規表現モジュールをインポート
# label_pattern = re.compile(r'Human Preference Label:\s*(\w+)')  # 人間の好みラベルを抽出するための正規表現パターン
```

---The following area is a Code cell (cell numver is 43)---
```python
# テキストからロジットを取得する関数を定義します
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')  # ロジットを抽出するための正規表現パターンを定義
#     logits_match = logits_pattern.search(text[0])  # テキストからロジットを検索

#     if logits_match:
#         logits_str = logits_match.group(1)  # 見つかったロジットの文字列を取得
#         logits_list = [(x.strip()) for x in logits_str.split(',')]  # カンマで分割してリストに変換

#         # 結果を出力
#         #print("Logits:", logits_list)  # ロジットを表示（コメントアウトされています）
#     else:
#         # 一致する文字列が見つからなかった場合、デフォルト値を指定
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
#     return logits_list  # ロジットのリストを返す
```

---The following area is a Code cell (cell numver is 44)---
```python
# テストデータに対する推論を実行し、予測とロジットを取得するループを定義します
# y_pred = []  # 予測結果を格納するリストを初期化
# logits = pd.DataFrame()  # ロジットを格納するデータフレームを初期化

# for i in tqdm(range(len(X_test))):  # テストデータの各インデックスに対してループ
#     prompt = X_test.iloc[i]["text"]  # プロンプトを取得
#     FastLanguageModel.for_inference(model)  # ネイティブ2倍速の推論を有効にします

#     # トークナイザーを使用してプロンプトを入力テンソルに変換し、CUDAデバイスに移動
#     inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

#     # モデルを使用して出力を生成
#     outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)
#     out1 = tokenizer.batch_decode(outputs)  # 出力をデコード

#     try:
#         preferences = label_pattern.search(out1[0]).group(1)  # 人間の好みラベルを抽出
#         lgts = extract_logits(out1[0])  # ロジットを抽出
#     except Exception as e:
#         print(out1)  # 出力を表示（例外が発生した場合）
#         preferences = 'winner_tie'  # デフォルトの好みラベルを設定

#     print(preferences)  # 予測された好みを表示
#     print(lgts)  # 抽出されたロジットを表示
#     y_pred.append(preferences)  # 予測結果をリストに追加
    
#     logits = pd.concat([logits, lgts], axis=0)  # ロジットをデータフレームに追加
```

---The following area is a Code cell (cell numver is 45)---
```python
# モデルの予測結果を評価します
##evaluate(y_true, y_pred)  # 真のラベルと予測結果を使って評価関数を呼び出す（コメントアウトされています）
```

---The following area is a Code cell (cell numver is 46)---
```python
# テキストからロジットを取得する関数を定義します
# def get_logits(text):
#     logits_pattern = re.compile(r'Logits:\s*\[(.*?)\]')  # ロジットを抽出するための正規表現パターンを定義
#     logits_match = logits_pattern.search(text[0])  # テキストからロジットを検索

#     if logits_match:
#         logits_str = logits_match.group(1)  # 見つかったロジットの文字列を取得
#         logits_list = [(x.strip()) for x in logits_str.split(',')]  # カンマで分割してリストに変換

#         # 結果を出力
#         #print("Logits:", logits_list)  # ロジットを表示（コメントアウトされています）
#     else:
#         # 一致する文字列が見つからなかった場合、デフォルト値を指定
#         logits_list = ["'winner_model_a': 0.33", "'winner_model_b': 0.33", "'winner_tie': 0.33"]
    
#     return logits_list  # ロジットのリストを返す
```

---The following area is a Code cell (cell numver is 47)---
```python
# テストデータに対する推論を実行し、ロジットのリストを取得します
logits_list = pd.DataFrame()  # ロジットを格納するためのデータフレームを初期化
for i in tqdm(range(len(test))):  # テストデータの各インデックスに対してループ
    prompt = test.iloc[i]["text"]  # プロンプトを取得
    FastLanguageModel.for_inference(model)  # ネイティブ2倍速の推論を有効にします

    # トークナイザーを使用してプロンプトを入力テンソルに変換し、CUDAデバイスに移動
    inputs = tokenizer(prompt, return_tensors='pt').to("cuda")

    # モデルを使用して出力を生成
    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)
    out1 = tokenizer.batch_decode(outputs)  # 出力をデコード
    preferences = extract_logits(out1[0])  # ロジットを抽出
    #print(preferences)  # 抽出したロジットを表示（コメントアウトされています）
    
    # ロジットをデータフレームに追加
    logits_list = pd.concat([logits_list, preferences], axis=0)  # ロジットのデータフレームを結合
```

---The following area is a Code cell (cell numver is 48)---
```python
# pandasライブラリをインポートします
# import pandas as pd

# ロジットの文字列リストを辞書に変換する関数を定義します
# def logits_to_dict(logits):
#     return {item.split(':')[0].strip("' "): float(item.split(':')[1].strip()) for item in logits}  # 項目を分割して辞書を生成

# 各ロジットリストを辞書に変換します
# logits_dicts = [logits_to_dict(logits) for logits in logits_list]  # ロジットリストから辞書のリストを作成

# データフレームを作成します
# df = pd.DataFrame(logits_dicts)  # 辞書のリストからデータフレームを生成

# df  # データフレームを表示（コメントアウトされています）
```

---The following area is a Code cell (cell numver is 49)---
```python
# ロジットリストのインデックスをリセットして新しいデータフレームを作成します
df1 = logits_list.reset_index(drop=True)  # インデックスをリセットし、古いインデックスを削除
```

---The following area is a Code cell (cell numver is 50)---
```python
# テストデータの'id'列とロジットのデータフレームを結合します
df = pd.concat([test['id'], df1], axis=1)  # 'id'列を左側に、ロジットのデータを右側に結合
```

---The following area is a Code cell (cell numver is 51)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

---The following area is a Code cell (cell numver is 52)---
```python
# 結合されたデータフレームを表示します
df  # テストデータの'id'列とロジットデータを含むデータフレームを表示
```

---The following area is a Code cell (cell numver is 53)---
```python
# 結合されたデータフレームをCSVファイルとして保存します
df.to_csv('submission.csv', index=False)  # インデックスなしで'submission.csv'として保存
```

---The following area is a Code cell (cell numver is 54)---
```python
# このセルは空です。必要なコードをここに追加してください。
```

** @@@ Jupyter Notebook numver 66, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arena コンペティションにおいて、ユーザーが選好する応答を予測するための機械学習モデルの構築に取り組んでいます。具体的には、巨大な言語モデル（LLM）であるQwen2を用いて、与えられたプロンプトに対する2つのモデルからの応答のどちらが好まれるかを予測します。

### 主要な問題
- ユーザーのプロンプトに対して、2つの匿名化された応答から、どちらがより好まれるかを明らかにするための予測モデルを構築します。

### 使用されている手法とライブラリ
1. **ライブラリのインストールとインポート**：
   - `bitsandbytes`, `transformers`, `tokenizers`, `peft` などのライブラリを用いて、最新のLLMを利用可能にします。
   - PyTorch、scikit-learn、NumPy、Pandasなど、データ処理およびモデル構築に必要なライブラリがインポートされています。

2. **データの準備**：
   - トレーニングデータとテストデータが CSV ファイルから読み込まれ、応答Aと応答Bの処理が行われます。
   - 入力テキストは、ユーザーのプロンプトとそれに対するモデルの応答を包含するように整形されます。

3. **トークナイゼーション**：
   - `AutoTokenizer`を使用して、テキストデータをトークン化し、モデルの入力として適切な形式に変換しています。

4. **モデルのロード**：
   - Qwen2モデルが初期化され、4ビット量子化を利用して効率的にGPUに配置されています。これにより、計算リソースの使用が最適化されます。
   - LoRa (Low-Rank Adaptation) によるパラメータ効率の良いファインチューニングの設定が行われています。

5. **推論**：
   - 推論には、2つのスレッドを用いてモデルの予測を効率的に行う関数が実装されています。各モデルによる出力が確率として計算され、最終的にそれぞれのモデルA、B、およびタイの結果が生成されます。

6. **出力処理**：
   - 生成された予測結果は、提出フォーマットに合う形で整形され、最終的には "submission.csv" として保存されます。

このノートブック全体を通じて、複数のLLMを活用し、複雑なデータ処理や予測を実現するための工夫が凝らされています。基本的に、二つのモデルからの応答を比較し、ユーザーの選好を予測するために、最先端の機械学習技術が利用されている点が特徴です。
```

---The following area is a Code cell (cell numver is 1)---
```python
# bitsandbytesライブラリを最新のバージョンにインストールします。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
# transformersライブラリを最新のバージョンにインストールします。
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
# tokenizersライブラリを最新のバージョンにインストールします。
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
# peftライブラリを最新のバージョンにインストールします。
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
このノートブックでの作業は、以下のノートブックからインスピレーションを受けています：
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします。
import torch  # PyTorchライブラリをインポート
import sklearn  # scikit-learnライブラリをインポート
import numpy as np  # NumPyライブラリをインポート
import pandas as pd  # Pandasライブラリをインポート
import time  # 時間操作のためのライブラリをインポート

# Transformersライブラリから必要なモデルとトークナイザーをインポート
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, Qwen2ForSequenceClassification, BitsAndBytesConfig
# PEFT（Parameter Efficient Fine-Tuning）のための設定をインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast  # 自動混合精度のための機能をインポート
from threading import Thread  # スレッド処理のためのライブラリをインポート

# ガベージコレクションをインポート
import gc
import os  # OS関連の機能をインポート
import io  # 入出力操作のためのライブラリをインポート
import time  # 時間操作のためのライブラリをインポート（再度インポート、必要であれば削除可能）
import json  # JSONデータを扱うためのライブラリをインポート
import random  # ランダム操作のためのライブラリをインポート
import pickle  # Pythonオブジェクトをバイナリ形式で保存・読み込みするためのライブラリをインポート
import zipfile  # ZIPファイル操作のためのライブラリをインポート
import datetime  # 日付と時間を操作するためのライブラリをインポート
import matplotlib.pyplot as plt  # グラフ描画のためのライブラリをインポート
from IPython.display import display  # IPythonの表示機能をインポート
from collections import Counter  # 要素のカウントを行うためのCounterクラスをインポート
from collections import defaultdict  # デフォルト辞書を扱うためのdefaultdictクラスをインポート
import torch  # 再度、PyTorchライブラリをインポート
from torch import nn  # ニューラルネットワークの基盤を構築するためのライブラリをインポート
import torch.nn.functional as F  # ニューラルネットワークの機能的操作を簡単にするためのライブラリをインポート
import pytorch_lightning as pl  # PyTorch Lightningライブラリをインポート
from torch.utils.data import Dataset, DataLoader  # データセットとデータローダーのためのクラスをインポート
from sklearn.metrics import log_loss  # 対数損失を計算するための関数をインポート
import tokenizers  # トークナイザーを扱うためのライブラリをインポート
```

---The following area is a Code cell (cell numver is 4)---
```python
# メモリ効率の良いスパース分解（SDP）を無効にします。
torch.backends.cuda.enable_mem_efficient_sdp(False)
# フラッシュSDPを無効にします。
torch.backends.cuda.enable_flash_sdp(False)

# GPUが利用できない場合はメッセージを表示します。
if (not torch.cuda.is_available()): 
    print("申し訳ありませんが、GPUが必要です！")

# モデル名のパスを定義します。
MODEL_NAME = '/kaggle/input/qwen2/transformers/qwen2-7b-instruct/1'
# 重みのパスを定義します。
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'
# 最大入力長を定義します。
MAX_LENGTH = 1284
# バッチサイズを定義します。
BATCH_SIZE = 8
# 使用するデバイスをCUDAに設定します（GPU）。
DEVICE = torch.device("cuda")
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## データの準備
```

---The following area is a Code cell (cell numver is 6)---
```python
# テストデータを読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
# 訓練データを読み込みます。ファイルを読み取りモードで開きます。
train = pd.read_csv(open('/kaggle/input/lmsys-chatbot-arena/train.csv', 'r'))
# サンプルの提出ファイルを読み込みます。
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
```

---The following area is a Code cell (cell numver is 7)---
```python
# 文字列のリストを連結する関数を定義します。
def process(input_str):
    # 入力文字列の両端の'[]'を取り除きます。
    stripped_str = input_str.strip('[]')
    # 文字列を分割して、各文から余分な引用符を取り除きます。
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # 文をスペースで結合して返します。
    return  ' '.join(sentences)

# テストデータに対してプロンプト、応答A、応答Bを処理します。
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# サンプルの提出データとテストデータの先頭5行を表示します。
display(sample_sub)
display(test.head(5))

# モデル用にテキストを準備します。
test['text'] = 'ユーザープロンプト: ' + test['prompt'] +  '\n\nモデルA :\n' + test['response_a'] +'\n\n--------\n\nモデルB:\n'  + test['response_b']
# 処理されたテキストの最初の要素を表示します。
print(test['text'][0])
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## トークナイズ
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイザーを初期化します。
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# テキストデータをトークン化します。
tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

# トークン化された入力IDとアテンションマスクをデバイス（GPU）に移動させます。
INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# テンソルをCPUに移動し、リストに変換します。
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

# 新しいデータフレームを作成し、INPUT_IDSとATTENTION_MASKSを格納します。
data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu
data['ATTENTION_MASKS'] = attention_masks_cpu
# データフレームの最初の2行を表示します。
data[:2]
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## モデルのロード
> 各GPUに1つのモデルをロードします。
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytesの設定を行います。
# 8ビットでの読み込みを設定する場合（コメントアウトされています）
# bnb_config =  BitsAndBytesConfig(
#     load_in_8bit=True,
#     bnb_8bit_compute_dtype=torch.float16,
#     bnb_8bit_use_double_quant=False)

# 4ビットでの読み込みを設定します。
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビットで読み込み
    bnb_4bit_quant_type="nf4",  # 量子化のタイプを"nf4"に設定
    bnb_4bit_use_double_quant=True,  # 二重量子化を使用
    bnb_4bit_compute_dtype=torch.bfloat16,  # 計算のデータ型をbfloat16に設定
)

# GPU 0に基礎モデルをロードします。
device0 = torch.device('cuda:0')

base_model_0 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,  # モデル名を指定
    num_labels=3,  # ラベルの数を指定
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を指定
    device_map='cuda:0'  # モデルをGPU 0に配置
)

# パディングトークンIDをトークナイザーから設定します。
base_model_0.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Code cell (cell numver is 12)---
```python
# GPU 1に基礎モデルをロードします。
device1 = torch.device('cuda:1')

base_model_1 = Qwen2ForSequenceClassification.from_pretrained(
    MODEL_NAME,  # モデル名を指定
    num_labels=3,  # ラベルの数を指定
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を指定
    device_map='cuda:1'  # モデルをGPU 1に配置
)

# パディングトークンIDをトークナイザーから設定します。
base_model_1.config.pad_token_id = tokenizer.pad_token_id
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## 重みのロード
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRa（Low-Rank Adaptation）の設定を行います。
peft_config = LoraConfig(
    r=16,  # 決定係数レイテンシを指定
    lora_alpha=32,  # LoRaでのスケーリングファクターを指定
    lora_dropout=0.10,  # ドロップアウト率を指定
    bias='none',  # バイアスの設定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクのタイプをシーケンス分類に設定
    target_modules=['o_proj', 'v_proj']  # ターゲットモジュールを指定
)
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTモデルを取得し、GPU 0に配置します。
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 

# 重みをロードします（コメントアウトされています）
# model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
# モデルを評価モードに設定します。
model_0.eval()

# PEFTモデルを取得し、GPU 1に配置します。
model_1 = get_peft_model(base_model_1, peft_config).to(device1)

# 重みをロードします（コメントアウトされています）
# model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
# モデルを評価モードに設定します。
model_1.eval()

# 学習可能なパラメータを表示します。
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()
```

---The following area is a Code cell (cell numver is 16)---
```python
# ガベージコレクションを実行して、不要なメモリを解放します。
gc.collect()
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## 推論
```

---The following area is a Code cell (cell numver is 18)---
```python
# 推論を行う関数を定義します。
def inference(df, model, device, batch_size=BATCH_SIZE):
    # DataFrameから入力IDとアテンションマスクを張り込むテンソルを作成します。
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    # 各クラスの生成された確率を格納するリストを初期化します。
    generated_class_a = []
    generated_class_b = []
    generated_class_c = []

    model.eval()  # モデルを評価モードに設定します。
    
    # バッチサイズごとにデータを処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算します。
        
        # バッチの入力IDとアテンションマスクを取得します。
        batch_input_ids = input_ids[start_idx:end_idx].to(device)
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)
        
        with torch.no_grad():  # 勾配計算を無効にします。
            with autocast():  # 自動混合精度を使用します。
                outputs = model(
                    input_ids=batch_input_ids,  # モデルに入力IDを渡します。
                    attention_mask=batch_attention_mask  # 注意マスクを渡します。
                )
        
        # 出力のロジットをソフトマックス関数で確率に変換します。
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        # 各クラスの確率をリストに追加します。
        generated_class_a.extend(probabilities[:, 0])
        generated_class_b.extend(probabilities[:, 1])
        generated_class_c.extend(probabilities[:, 2])
    
    # DataFrameに生成されたクラスの確率を追加します。
    df['winner_model_a'] = generated_class_a
    df['winner_model_b'] = generated_class_b
    df['winner_tie'] = generated_class_c

    # GPUのキャッシュをクリアします。
    torch.cuda.empty_cache()  

    return df  # 処理されたDataFrameを返します。
```

---The following area is a Code cell (cell numver is 19)---
```python
# 処理時間の計測を開始します。
st = time.time()

N_SAMPLES = len(data)  # データサンプルの総数を取得します。

# データを2つのサブセットに分割します。
half = round(N_SAMPLES / 2)  # サンプル数の半分を計算します。
sub1 = data.iloc[0:half].copy()  # 最初の半分のデータを取得します。
sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りの半分のデータを取得します。

# スレッド内で推論を実行するための関数を定義します。
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)  # 推論結果を辞書に格納します。

# スレッドからの結果を保存するための辞書を初期化します。
results = {}
```

---The following area is a Code cell (cell numver is 20)---
```python
# スレッドを開始します。
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッドt0を作成
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッドt1を作成

t0.start()  # スレッドt0を開始
t1.start()  # スレッドt1を開始

# すべてのスレッドの終了を待ちます。
t0.join()
t1.join()

# 結果を元のDataFrameに結合します。
data = pd.concat([results[0], results[1]], axis=0)

# 処理が完了したことを表示します。
print(f"処理が完了しました。合計時間: {time.time() - st}")

# ターゲット列を定義します。
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

# サンプルの提出データフレームに結果を追加します。
sample_sub[TARGETS] = data[TARGETS]
```

---The following area is a Code cell (cell numver is 21)---
```python
# ターゲット列の値を取得し、予測結果を変数に格納します。
llama_preds = data[TARGETS].values
```

---The following area is a Code cell (cell numver is 22)---
```python
# 予測結果をDataFrameに変換し、元のテストデータのIDをインデックスとして設定します。
out = pd.DataFrame(llama_preds, 
                index=test.id,  # テストデータのIDをインデックスに設定
                columns=train.columns[-3:])  # 直近の3つの列をカラムとして設定
# 結果の先頭5行を表示します。
display(out.head())
```

---The following area is a Code cell (cell numver is 23)---
```python
# 予測結果をCSVファイルとして保存します。
out.to_csv('submission.csv')
```

** @@@ Jupyter Notebook numver 67, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」コンペティションにおいて、チャットボットの応答がどちらのモデルに好まれるかを予測する問題に取り組んでいます。具体的には、異なる大規模言語モデル（LLM）による応答の比較を行い、どちらの応答がユーザーに受け入れられるかを予測するための機械学習モデルを構築しています。

### 問題へのアプローチ
1. **データ準備**:
   - テストデータとサンプル提出ファイルをCSVから読み込み、必要なデータを整形します。
   - ユーザープロンプトおよびモデルの応答を連結し、モデルの入力形式に適したテキストを準備します。

2. **ライブラリの使用**:
   - `torch` (深層学習フレームワーク) と `sklearn` (機械学習ツール) のほか、`transformers`ライブラリを利用して、LLMのベースモデル（Llama）を読み込んでいます。
   - `peft` (Parameter-Efficient Fine-Tuning)ライブラリを用いてモデルを効率的にファインチューニングします。

3. **トークナイズ**:
   - `AutoTokenizer`を使用して、モデルへの入力データをトークン化し、適切な形状にパディングします。

4. **モデルの構築と推論**:
   - Chatbot Arenaでの競争用に、大規模なLLMをモデルとして設定し、複数のGPUを用いて並列処理を行います。
   - `LoraConfig`を通じてLoRA（Low-Rank Adaptation）によるパラメータの効率的な微調整を行います。
   - 推論関数を定義し、入力データに対してモデルの出力を早く計算します。

5. **LightGBMの使用**:
   - LightGBMを用いた新たな特徴量抽出と分類モデルによって、応答の選好予測も行います。
   - 以前に保存されたモデルを用いて推論を実施し、各応答のクラス確率を出力します。

6. **予測のブレンド**:
   - 最後に、LGBMモデルトとLLMモデルの予測を重み付けしてブレンドし、最終的な予測結果を生成します。

### 使用ライブラリ
- `torch`: 深層学習モデルのライブラリ。
- `transformers`: LLMの読み込みやトークナイズ用ライブラリ。
- `peft`: モデルのファインチューニングを効率化するためのライブラリ。
- `pandas`: データ処理ライブラリ。
- `sklearn`: 機械学習用ライブラリ（モデル評価、データ処理）。
- `lightgbm`: 勾配ブースティングフレームワーク。
- `matplotlib`: グラフ描画ライブラリ（必要に応じて使用）。

このノートブックは、最新の機械学習手法を用いて、ユーザーが最も好むチャットボットの応答を的確に予測するモデルを構築するプロセスを詳細に示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# bitsandbytesを最新バージョンにアップグレードしてインストールします。
# インデックスを使用せず、指定したリンクからパッケージを探します。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/

# transformersライブラリを最新バージョンにアップグレードしてインストールします。
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/

# tokenizersライブラリを最新バージョンにアップグレードしてインストールします。
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/

# peftライブラリを最新バージョンにアップグレードしてインストールします。
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Markdown cell (cell numver is 2)---
```markdown
このノートブックの作業は、以下のノートブックにインスパイアされています：
* https://www.kaggle.com/code/ivanvybornov/llama3-8b-lgbm-tfidf
* https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b

## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします。
import torch  # PyTorchをインポートして、深層学習モデルの構築に使用します。
import sklearn  # 機械学習のためのツールを提供するライブラリ
import numpy as np  # 数値計算を行うためのライブラリ
import pandas as pd  # データ操作と解析のためのライブラリ
import time  # 時間に関連する機能を扱うライブラリ

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
# Transformersライブラリからトークナイザーやモデルをインポートします。

from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
# PEFT（Parameter-Efficient Fine-Tuning）ライブラリから必要なクラスや関数をインポートします。

from torch.cuda.amp import autocast  # 自動混合精度トレーニングのための機能をインポートします。
from threading import Thread  # スレッド処理のためのライブラリをインポートします。

import gc  # ガベージコレクションを管理するためのライブラリ
import os  # オペレーティングシステムの機能を操作するライブラリ
import io  # 入出力関連の操作を行うライブラリ
import time  # 時間操作のためのライブラリ
import json  # JSONデータの操作を行うためのライブラリ
import random  # 乱数生成のためのライブラリ
import pickle  # オブジェクトのシリアライズを行うためのライブラリ
import zipfile  # ZIPアーカイブの読み書きを扱うライブラリ
import datetime  # 日付と時間の操作を行うためのライブラリ
import matplotlib.pyplot as plt  # グラフ作成のためのライブラリ
from IPython.display import display  # Jupyter Notebookでの表示を管理するための機能をインポートします。
from collections import Counter  # 頻度計算を行うためのクラス
from collections import defaultdict  # デフォルト値の付いた辞書を作成するためのクラス
import torch  # もう一度PyTorchをインポートします。
from torch import nn  # PyTorchのニューラルネットワーク関連モジュールをインポートします。
import torch.nn.functional as F  # ニューラルネットワークの機能的な操作を提供します。
import pytorch_lightning as pl  # PyTorchを使った機械学習モデルの訓練を簡素化するライブラリ
from torch.utils.data import Dataset, DataLoader  # データセットとデータローダーのクラスをインポートします。
from sklearn.metrics import log_loss  # ログ損失を計算するための関数をインポートします。
import tokenizers  # トークナイザー関連の機能を提供するライブラリ
```

---The following area is a Code cell (cell numver is 4)---
```python
# メモリ効率の良い分散処理およびフラッシュ分散処理を無効にします。
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# GPUが利用可能であるか確認し、利用できない場合はメッセージを表示します。
if (not torch.cuda.is_available()): 
    print("申し訳ありません - GPUが必要です！")

# モデル名、重みのパス、最大シーケンス長、バッチサイズ、デバイスの設定を定義します。
MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのディレクトリ
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'  # モデルの重みが保存されているパス
MAX_LENGTH = 1284  # 最大シーケンス長を1284に設定
BATCH_SIZE = 8  # 訓練時に使用するバッチサイズを8に設定
DEVICE = torch.device("cuda")  # 使用するデバイスとしてGPUを指定
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## データの準備
```

---The following area is a Code cell (cell numver is 6)---
```python
# テストデータをCSVファイルから読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータのデータフレームを作成
# サンプルの提出ファイルをCSVファイルから読み込みます。
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # 提出サンプルデータのデータフレームを作成
```

---The following area is a Code cell (cell numver is 7)---
```python
# 文字列のリストを連結するための関数を定義します。
def process(input_str):
    stripped_str = input_str.strip('[]')  # 最初と最後の角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 各文を処理し、余分な引用符を取り除く
    return ' '.join(sentences)  # 文を空白で結合して返す

# テストデータの各カラムに対してprocess関数を適用します。
test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'カラムを処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'カラムを処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'カラムを処理

# サンプル提出データとテストデータの先頭5行を表示します。
display(sample_sub)  # サンプル提出データを表示
display(test.head(5))  # 処理後のテストデータの最初の5行を表示

# モデル用にテキストを準備します。
test['text'] = 'User prompt: ' + test['prompt'] + '\n\nModel A :\n' + test['response_a'] + '\n\n--------\n\nModel B:\n' + test['response_b']
# 処理されたテキストの最初の要素を出力します。
print(test['text'][0])  # 最初のテキストを表示
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
## トークナイズ
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイザーを指定したパスから読み込みます。
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# テキストデータをトークン化し、最大長さでパディングします。
tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')

# トークンの入力IDとアテンションマスクをデバイスに移動し、データ型をint32に設定します。
INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)

# テンソルをCPUに移動し、リストに変換します。
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]  # 入力IDをリストに変換
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]  # アテンションマスクをリストに変換

# 新しいデータフレームを作成し、INPUT_IDSとATTENTION_MASKSカラムを追加します。
data = pd.DataFrame()
data['INPUT_IDS'] = input_ids_cpu  # 入力IDをDataFrameに追加
data['ATTENTION_MASKS'] = attention_masks_cpu  # アテンションマスクをDataFrameに追加

# 最初の2行を表示します。
data[:2]  # データフレームの最初の2行を表示
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
## モデルの読み込み
> 各GPUに1つのモデルを読み込みます。
```

---The following area is a Code cell (cell numver is 11)---
```python
# BitsAndBytesの設定を行います。
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットでモデルを読み込む設定
    bnb_8bit_compute_dtype=torch.float16,  # 8ビット計算のデータ型をfloat16に設定
    bnb_8bit_use_double_quant=False)  # ダブル量子化を使用しない設定

# GPU 0にベースモデルを読み込みます。
device0 = torch.device('cuda:0')

# LlamaForSequenceClassificationモデルを指定した設定で読み込みます。
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # 分類タスクのラベル数を3に設定
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化の設定を適用
    device_map='cuda:0')  # モデルをGPU 0に割り当て

# パディングトークンのIDをトークナイザーから取得し、モデルの設定に適用します。
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定
```

---The following area is a Code cell (cell numver is 12)---
```python
# GPU 1にベースモデルを読み込みます。
device1 = torch.device('cuda:1')

# LlamaForSequenceClassificationモデルを指定した設定で読み込みます。
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # 分類タスクのラベル数を3に設定
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化の設定を適用
    device_map='cuda:1')  # モデルをGPU 1に割り当て

# パディングトークンのIDをトークナイザーから取得し、モデルの設定に適用します。
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## 重みの読み込み
```

---The following area is a Code cell (cell numver is 14)---
```python
# LoRaの設定を行います。
peft_config = LoraConfig(
    r=16,  # LoRaのランク
    lora_alpha=32,  # LoRaのスケーリング係数
    lora_dropout=0.10,  # ドロップアウト率
    bias='none',  # バイアスの使用設定
    inference_mode=True,  # 推論モードの設定
    task_type=TaskType.SEQ_CLS,  # タスクの種類をシーケンス分類に設定
    target_modules=['o_proj', 'v_proj']  # 対象とするモジュール
)
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTモデルを取得し、GPU 0に移動します。
model_0 = get_peft_model(base_model_0, peft_config).to(device0) 
# 重みを読み込みます。
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # モデルに重みを適用
model_0.eval()  # 評価モードに設定

# GPU 1に対して同様の処理を行います。
model_1 = get_peft_model(base_model_1, peft_config).to(device1)
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # モデルに重みを適用
model_1.eval()  # 評価モードに設定

# 学習可能なパラメータを表示します。
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()  # 各モデルの学習可能なパラメータを出力
```

---The following area is a Code cell (cell numver is 16)---
```python
# ガベージコレクションを実行して、メモリを解放します。
gc.collect()  # 未使用のメモリを整理して、システムの効率を向上させるために呼び出します。
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## 推論
```

---The following area is a Code cell (cell numver is 18)---
```python
# 推論を行う関数を定義します。
def inference(df, model, device, batch_size=BATCH_SIZE):
    # DataFrameから入力IDとアテンションマスクをテンソルとして取得します。
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)
    
    # 各クラスの生成結果を保持するリストを初期化します。
    generated_class_a = []  # モデルAの結果を格納
    generated_class_b = []  # モデルBの結果を格納
    generated_class_c = []  # 引き分けの結果を格納

    model.eval()  # モデルを評価モードに設定
    
    # バッチ処理でデータを処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりを計算
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送
        
        with torch.no_grad():  # 勾配計算を無効にします。
            with autocast():  # 自動混合精度を使用して計算を行います。
                outputs = model(
                    input_ids=batch_input_ids,  # バッチの入力IDをモデルに渡す
                    attention_mask=batch_attention_mask  # バッチのアテンションマスクをモデルに渡す
                )
        
        # モデルの出力をソフトマックス関数を用いて確率に変換します。
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()
        
        # 各クラスの確率をリストに追加します。
        generated_class_a.extend(probabilities[:, 0])  # クラスAの確率を拡張
        generated_class_b.extend(probabilities[:, 1])  # クラスBの確率を拡張
        generated_class_c.extend(probabilities[:, 2])  # 引き分けの確率を拡張
    
    # DataFrameに生成した結果を追加します。
    df['winner_model_a'] = generated_class_a  # モデルAの勝者確率を追加
    df['winner_model_b'] = generated_class_b  # モデルBの勝者確率を追加
    df['winner_tie'] = generated_class_c  # 引き分け確率を追加

    torch.cuda.empty_cache()  # GPUのキャッシュをクリアしてメモリを解放

    return df  # 結果を含むDataFrameを返します。
```

---The following area is a Code cell (cell numver is 19)---
```python
# 時間計測を開始します。
st = time.time()

# データのサンプル数を取得します。
N_SAMPLES = len(data)

# データを2つのサブセットに分割します。
half = round(N_SAMPLES / 2)  # サンプル数の半分を計算
sub1 = data.iloc[0:half].copy()  # 最初の半分のデータをコピー
sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りのデータをコピー

# スレッドで推論を実行するための関数を定義します。
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)  # 指定したインデックスに推論結果を保存

# スレッドからの結果を格納するための辞書を初期化します。
results = {}  # 結果を格納する空の辞書を作成
```

---The following area is a Code cell (cell numver is 20)---
```python
# スレッドを開始します。
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # スレッド0を初期化
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # スレッド1を初期化

t0.start()  # スレッド0を開始
t1.start()  # スレッド1を開始

# すべてのスレッドが終了するのを待ちます。
t0.join()  # スレッド0の終了を待機
t1.join()  # スレッド1の終了を待機

# 結果を元のDataFrameに結合します。
data = pd.concat([results[0], results[1]], axis=0)  # 2つの結果を縦に結合

# 処理が完了したことを表示し、所要時間を出力します。
print(f"処理が完了しました。総時間: {time.time() - st}")

# ターゲットとするカラムをリスト化します。
TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']

# サンプル提出データに推論結果を追加します。
sample_sub[TARGETS] = data[TARGETS]  # ターゲットの結果を提出サンプルに追加
```

---The following area is a Code cell (cell numver is 21)---
```python
# ターゲットカラムから予測結果を抽出し、NumPy配列に変換します。
llama_preds = data[TARGETS].values  # 推論結果をNumPy配列として取得
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
## LGBM + TF-IDF
```

---The following area is a Code cell (cell numver is 23)---
```python
# コンペティションのタグを設定します。
TAG = 'lmsys-chatbot-arena'

import os  # osモジュールをインポート

# 実行環境がRUNPODかどうかをチェックします。
RUNPOD = os.path.exists('/workspace/')  # '/workspace/'が存在するか確認
KAGGLE = not RUNPOD  # RUNPODでない場合はKAGGLE環境とする
if KAGGLE: 
    print('kaggle')  # KAGGLE環境であればメッセージを表示
```

---The following area is a Code cell (cell numver is 24)---
```python
# pandasをインポートしてみます。
try:
    import pandas as pd  # pandasをインポート
except:  # インポートに失敗した場合
    # Kaggle APIと必要なライブラリをインストールします。
    !pip install -q kaggle  # Kaggle APIをインストール
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # データ処理ライブラリをインストール
    !pip install -q protobuf  # protobufライブラリをインストール
    !pip install -q numba  # numbaライブラリをインストール
```

---The following area is a Code cell (cell numver is 25)---
```python
# データのパスを設定します。
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)  # 環境に応じてデータのパスを決定

import os  # osモジュールを再度インポートします。

# 実行環境がRUNPODの場合の処理
if RUNPOD:
    # Kaggle認証ファイルが存在しない場合
    if not os.path.exists('~/.kaggle/kaggle.json'):
        !mkdir -p ~/.kaggle  # .kaggleディレクトリを作成
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # kaggle.jsonをコピー
        !chmod 600 /root/.kaggle/kaggle.json  # 権限を設定
    
    # コンペティションのZIPファイルが存在しない場合
    if not os.path.exists('/workspace/' + TAG + '.zip'):
        !kaggle competitions download $TAG -p /workspace/  # Kaggleからデータをダウンロード
        
    # データを格納するディレクトリが存在しない場合
    if not os.path.exists('/data/'):
        import zipfile  # zipファイル処理用のライブラリをインポート
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍してデータを/data/に展開
```

---The following area is a Code cell (cell numver is 26)---
```python
# 入力、モデル、ログ用のパスを設定します。
INPUT_PATH = '/kaggle/input/'  # Kaggleの入力データパス
MODEL_PATH = '/workspace/models/'  # モデル保存用のパス
LOGITS_PATH = '/workspace/logits/'  # ログ保存用のパス

# モデルパスをKAGGLE環境に応じて設定します。
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'  # 'lsys-models'を含むディレクトリを見つけてそのパスを設定

# MODEL_PATHを表示します。
print(MODEL_PATH)

# コードのパスを設定します。KAGGLE環境によって異なります。
CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパス
SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パス
```

---The following area is a Code cell (cell numver is 27)---
```python
# トークナイザーの並列処理を無効にします。
os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # 複数のトークナイザーの並列処理を無効にして、競合を防ぎます。
```

---The following area is a Code cell (cell numver is 28)---
```python
# 訓練データ、テストデータ、サンプル提出ファイルをCSVから読み込みます。
train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # 訓練データを読み込み
test = pd.read_csv(open(DATA + 'test.csv', 'r'))  # テストデータを読み込み
sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込み

# 訓練データとテストデータのサンプル数を表示します。
print(len(train), len(test))  # 訓練データとテストデータの行数を表示
```

---The following area is a Code cell (cell numver is 29)---
```python
# パラメータの辞書を初期化します。
params = {}
if False:#len(test) < 10: 
    pass;  # 条件がFalseの場合は何もしない
    params['subsample'] = 30  # サブサンプル数を30に設定
else:
    # サブサンプル数を設定しない
    params['fold'] = -1  # フォールドの設定を-1にする

# その他のモデルに関するパラメータを設定します。
params['n_epochs'] = 1  # エポック数を1に設定
params['n_lgb'] = 1  # LightGBMのモデル数を1に設定
params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデルをDeBERTaに設定
```

---The following area is a Code cell (cell numver is 30)---
```python
# フォールド数、シード値、サブサンプルサイズを設定します。
FULL = params.get('fold', 0) < 0  # フォールドが-1未満の場合はFULLをTrueに設定
N_FOLDS = int(params.get('n_folds', 3))  # フォールドの数を取得、デフォルトは3
FOLD = int(params.get('fold', 0))  # 現在のフォールドを取得、デフォルトは0
SEED = int(params.get('seed', 3))  # シードの値を取得、デフォルトは3
SS = int(params.get('subsample', 1))  # サブサンプル数を取得、デフォルトは1

# 設定されたフォールド数、現在のフォールド、シード、サブサンプルサイズを表示します。
print(N_FOLDS, FOLD, SEED, SS)  # 各設定値を出力
```

---The following area is a Code cell (cell numver is 31)---
```python
from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldをインポート

# フォールドを取得するための関数を定義します。
def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state=SEED, shuffle=True)  # StratifiedKFoldを使用してフォールドを生成
                .split(X=np.zeros(len(train)), y=train.iloc[:, -3:].idxmax(1)))  # ラベルのインデックスを用いて分割

# 現在のフォールドに応じて訓練データとテストデータのIDを取得します。
train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]
# サブサンプル数が1より大きい場合、IDをサブサンプリングします。
if SS > 1: 
    train_ids, test_ids = train_ids[::SS], test_ids[::SS]

# 訓練データとテストデータのIDの数を表示し、重複していないことを確認します。
print(len(train_ids), len(test_ids))  # 訓練IDとテストIDの数を表示
assert set(train_ids) & set(test_ids) == set()  # 訓練とテストIDに重複がないことを確認
```

---The following area is a Code cell (cell numver is 32)---
```python
# リスト内の文字列を結合するための関数を定義します。
def join_strings(x):
    # xがリストである場合、要素を結合します。Noneの場合は空文字に置き換えます。
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x  # リストをスペースで結合
    return x  # 結合した文字列を返します
```

---The following area is a Code cell (cell numver is 33)---
```python
# 結合された文字列の長さを返す関数を定義します。
def len_join_strings(x):
    # join_strings関数を使用して文字列を結合し、その結果をスペースで分割して単語数を返します。
    return len(join_strings(x).split())  # 結合した文字列の単語数を返す
```

---The following area is a Code cell (cell numver is 34)---
```python
# JSON形式の文字列を解析し、結合された文字列の長さを返す関数を定義します。
def len_join_strings_j(x):
    x = json.loads(x)  # JSON文字列をPythonオブジェクトに変換
    return len_join_strings(x)  # 結合された文字列の長さを返す
```

---The following area is a Code cell (cell numver is 35)---
```python
# シードを設定して再現性を確保します。
torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchのシードを設定
random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールのシードを設定
np.random.seed(datetime.datetime.now().microsecond)  # NumPyのシードを設定
```

---The following area is a Code cell (cell numver is 36)---
```python
# 学習モードや推論モード、保存モードを設定します。
# TRAINはTrueかつKAGGLEでない場合に学習を行うかを示します。
TRAIN = False  # 学習を行うかどうかを設定（現在はFalse）
INFER = True  # 推論を行うかどうかを設定（現在はTrue、KAGGLE環境でも使用可能）
SAVE = False  # モデルや結果を保存するかどうかを設定（現在はFalse）
```

---The following area is a Code cell (cell numver is 37)---
```python
# LightGBMとカウントベクトライザーをインポートします。
import lightgbm as lgb  # LightGBMライブラリをインポート
from sklearn.feature_extraction.text import CountVectorizer  # テキストデータの特徴量抽出に使用するカウントベクトライザーをインポート
```

---The following area is a Code cell (cell numver is 38)---
```python
# LightGBMを使用するかどうかを設定します。
LGB = True  # LightGBMを使用するかどうかのフラグを設定
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # 学習モードでLightGBMを使用するかのフラグ
INFER_LGB = not TRAIN and LGB  # 学習していない場合にLightGBMによる推論を行うかのフラグ
```

---The following area is a Code cell (cell numver is 39)---
```python
# 事前に保存したカウントベクトライザーを読み込みます。
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # cvecモデルを読み込む
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # ccvecモデルを読み込む
```

---The following area is a Code cell (cell numver is 40)---
```python
# シンメトリック対数変換を行う関数を定義します。
def symlog(x): 
    return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # 対数変換と符号付き変換を行う

# 密な行列を変換する関数を定義します。
def dense(x):
    x = np.asarray(x.astype(np.float32).todense())  # 行列を密な形式に変換し、float32型にする
    x = symlog(x)  # シンメトリック対数変換を適用
    return x  # 変換した値を返す

# 特徴量を取得するための関数を定義します。
def get_features(df):
    # プロンプトに対する特徴を取得
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])  # cvecとccvecを用いてtransform

    # モデルAに対する特徴を取得
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ] 
                    for v in [cvec, ccvec]
                ])  # response_aに対してtransform

    # モデルBに対する特徴を取得
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ] 
                    for v in [cvec, ccvec]
                ])  # response_bに対してtransform
    
    # 特徴量を計算
    v = np.hstack([
    # pfeat, 
          afeat - bfeat, np.abs(afeat - bfeat), 
    # afeat + bfeat
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # 投票モデルが存在する場合、特徴量を正規化
    except: 
        pass  # エラーが発生しても何もしない

    # 追加の特徴量を計算
    extras = []
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']  # 確認したい追加の文字列
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)  # それぞれのカラムで指定した文字列の出現回数をカウント
            
    extras.append(df[c].str.len())  # 各カラムの文字列長を追加
    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 各カラムの単語数を取得及び追加
    
    extras = np.stack(extras, axis=1)  # 追加特徴量をスタック
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 特徴量を根号と対数変換

    return np.hstack([v, extras])  # すべての特徴量を結合して返す
    # return v  # vだけを返す選択肢もある
```

---The following area is a Code cell (cell numver is 41)---
```python
# 事前に保存したLightGBMモデルを読み込みます。
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込む
```

---The following area is a Code cell (cell numver is 42)---
```python
# 推論を行うための処理を実行します。
if INFER and params.get('n_lgb', 1) > 0:
    df = test  # テストデータをdfに設定
    yps = []  # モデルの予測結果を格納するリストを初期化
    b = 1000  # バッチサイズを設定
    
    # テストデータをバッチ処理で繰り返す
    for i in range(0, len(df), b):
        arr = get_features(df.iloc[i: i + b])  # 特徴量を取得
        ypms = []  # 個々のモデルの予測結果を格納するリストを初期化
        
        # 各LightGBMモデルについて推論を実施
        for model in lgb_models:
            ypms.append(model.predict_proba(arr))  # 各モデルの予測確率を追加
        
        # 平均を取った確率をypsに追加
        yps.append(np.stack(ypms).mean(0))  # 各モデルの予測をスタックし、平均を計算
        # break;  # デバッグのためにループを中断する行（コメントアウト）

        print('.', end='')  # 進行状況の表示
        
        # ypsの長さが偶数のときにガベージコレクションを実行
        if len(yps) % 2 == 0:
            gc.collect()  # メモリを整理
    print()  # 改行

    # すべての予測結果を結合
    yp = np.concatenate(yps)  # 予測確率を一つの配列に結合
```

---The following area is a Code cell (cell numver is 43)---
```python
# LightGBMモデルからの予測結果をlgb_predsに格納します。
lgb_preds = yp  # 推論結果をlgb_predsに設定
```

---The following area is a Markdown cell (cell numver is 44)---
```markdown
## 予測のブレンド

$\operatorname{preds} = 0.2 \cdot \operatorname{LGBMブースティング予測} + 0.8 \cdot \operatorname{ラマ予測}$
```

---The following area is a Code cell (cell numver is 45)---
```python
# LGBMモデルとラマモデルの予測をブレンドします。
lgb_wt = 0.2  # LGBMモデルの重みを設定
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # ブレンドした予測結果を計算
```

---The following area is a Code cell (cell numver is 46)---
```python
# ブレンドした予測結果をデータフレームに変換します。
out = pd.DataFrame(preds, 
                index=df.id,  # データフレームのインデックスとしてIDを設定
                columns=train.columns[-3:])  # カラム名として訓練データの最後の3カラムを設定
display(out.head())  # データフレームの先頭5行を表示
```

---The following area is a Code cell (cell numver is 47)---
```python
# 予測結果をCSVファイルとして保存します。
out.to_csv('submission.csv')  # 予測結果を'submission.csv'という名前で保存
```

** @@@ Jupyter Notebook numver 68, the number of votes :1 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arena コンペティションにおいて、ユーザーが好む応答を予測するための機械学習モデルを構築することを目的としています。特に、チャットボットの応答の中から、どちらがユーザーに好まれるかを判定するための特徴量を抽出し、予測モデルを訓練する方法を示しています。

### 主な問題:
- コンペティションのデータを用いて、二者間の選好（どちらの応答が好まれるか）を予測する。

### 使用している手法:
1. **データの前処理**:
   - プロンプトや応答をクリーニングし、'null'応答を持つ行を削除する。

2. **特徴量エンジニアリング**:
   - **TF-IDF**ベクトル化（単語と文字レベル）を用いてテキストデータを数値化し、各応答の特徴を抽出。
   - 応答の長さに基づく特徴を作成し、応答の長さの差や比も考慮。
   - パープレキシティを算出することで、応答の自然さを測定し、IFD（Input-Output Frequency Discrepancy）に基づいた特徴量を生成。

3. **モデルの構築**:
   - LightGBMを用いた多クラス分類器を使用、一対多の選好予測を行う。交差検証を通じて汎用性を向上させ、評価指標として対数損失（Log Loss）を使用してモデルの性能を測定。

### 使用されているライブラリ:
- **numpy, pandas**: データの操作と分析。
- **regex**: テキスト処理のための正規表現。
- **scikit-learn**: 特徴量エンジニアリングやモデル評価。
- **lightgbm**: 高速で効率的なブースティング決定木。
- **transformers**: DeBERTaとGPTモデルを活用し、自然言語の理解を深めるためのパープレキシティ計算。
- **tqdm**: 進捗バーの表示。

このノートブックは、ユーザーの選好を予測するために、データの前処理からモデル構築、予測までの一連のステップを網羅しており、特にTF-IDFやパープレキシティなどのテキスト特徴量を活用している点が特徴です。最終的に、予測された結果はCSV形式で出力され、コンペティションに提出可能な形式が整えられています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 参照: [IFD](https://github.com/tianyi-lab/Superfiltering)
$$\mathrm{IFD}_\theta(Q,A)=\frac{\mathrm{PPL}_\theta(A|Q)}{\mathrm{PPL}_\theta(A)}$$
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import numpy as np 
import pandas as pd 
import regex as re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold, StratifiedGroupKFold, train_test_split
from sklearn.metrics import classification_report, f1_score, recall_score, precision_score, accuracy_score, roc_auc_score, log_loss
from sklearn.preprocessing import LabelEncoder
from scipy.sparse import csr_matrix, save_npz, load_npz, hstack
import lightgbm as lgb
from tqdm import tqdm
import gensim
import itertools
from gensim.utils import simple_preprocess
from gensim.models import Word2Vec
from transformers import DebertaV2Tokenizer, DebertaV2Model
from transformers import set_seed, AutoModelForCausalLM, AutoTokenizer
import torch
import joblib
import unicodedata
import re
import time
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings("ignore")
```

---The following area is a Code cell (cell numver is 3)---
```python
MAX_LENGTH = 1024  # モデルの最大入力長を設定
deberta_path = "/kaggle/input/debertav3base"  # DeBERTaモデルのパス
gpt_path = "/kaggle/input/qwen2-1-5b-instruct"  # GPTモデルのパス
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能か確認し、デバイスを設定
SEED = 42  # 乱数シードを設定
set_seed(SEED)  # 乱数シードを設定する関数を呼び出し
```

---The following area is a Code cell (cell numver is 4)---
```python
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")  # トレーニングデータを読み込む
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")  # テストデータを読み込む
vectorize_on_train_and_test = True  # トレーニングデータとテストデータのベクトル化を実行するかのフラグ
# トレーニングデータの小さい部分でのクイックテスト用（提出時に多くのGPUを使用しないように）
# (これがオンの場合、保存されたモデルは完全には訓練されない)
quick_test = True
quick_test_items = 1000  # クイックテストで使用するデータの数
# 実際のテストデータが検出される場合、自動的にクイックテストを無効にする...（スコアリング時に完全なトレーニングを保証する）
if (len(test)) > 3: quick_test = False
if quick_test: train = train.head(quick_test_items)  # クイックテストの場合、トレーニングデータを制限する

def process(input_str):
    stripped_str = input_str.strip('[]')  # 入力文字列の前後の中括弧を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 各文をリストに分割し、余分な引用符を削除
    return ' '.join(sentences)  # 文を結合して一つの文字列にする

# テストデータのプロンプト、応答A、応答Bを処理
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)
# トレーニングデータのプロンプト、応答A、応答Bを処理
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 応答Aと応答Bが両方'null'のインデックスを取得
train.drop(indexes, inplace=True)  # 'null'のインデックスをトレーニングデータから削除
train.reset_index(inplace=True, drop=True)  # インデックスをリセット
print(f"Total {len(indexes)} Null response rows dropped")  # 削除された行数を表示
print('Total train samples: ', len(train))  # トレーニングデータのサンプル数を表示

target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']  # ターゲットとなるカラムを定義
columns_to_vectorize = ["prompt", "response_a", "response_b"]  # ベクトル化するカラムを定義
train['label'] = train[target_columns].idxmax(axis=1)  # 各行の勝者モデルをラベルとして設定
label_encoder = LabelEncoder()  # ラベルエンコーダを作成
train['label'] = label_encoder.fit_transform(train['label'])  # ラベルをエンコード
train = train[columns_to_vectorize + ['label']]  # 必要なカラムのみを保持
train.head(3)  # トレーニングデータの最初の3行を表示
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
### TF-IDF
```

---The following area is a Code cell (cell numver is 6)---
```python
train_text = train[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # トレーニングデータの全テキストを結合
test_text = test[['prompt', 'response_a', 'response_b']].astype(str).apply(lambda x: ' '.join(x), axis=1)  # テストデータの全テキストを結合

if vectorize_on_train_and_test:  # トレーニングとテスト両方をベクトル化する場合
    vector_fit_text = pd.concat([train_text, test_text], axis=0).reset_index(drop=True)  # トレーニングデータとテストデータを連結
else:
    vector_fit_text = train_text  # そうでなければトレーニングデータのみを使用
```

---The following area is a Code cell (cell numver is 7)---
```python
def custom_tokenizer(text):
    return re.findall(r'[^\W]+', text)  # 特殊文字を除外して単語を抽出するカスタムトークナイザー

# word-level vectorizer
tfidf_word_vectorizer = TfidfVectorizer(
    ngram_range=(1, 5),  # n-gramの範囲を設定（1から5）
    tokenizer=custom_tokenizer,  # カスタムトークナイザーを指定
    token_pattern=None,  # デフォルトのトークンパターンを無効に
    strip_accents='unicode',  # アクセントを削除
    min_df=4,  # 最小出現回数
    max_features=300  # 最大特徴数
)

# char-level vectorizer
tfidf_char_vectorizer = TfidfVectorizer(
    analyzer='char',  # 文字単位のベクトル化を実行
    ngram_range=(1, 5),  # n-gramの範囲
    max_features=1000,  # 最大特徴数
    min_df=4  # 最小出現回数
)

def batch_process(texts, batch_size):
    for i in range(0, len(texts), batch_size):  # バッチサイズごとにテキストを処理
        yield texts[i:i + batch_size]  # 各バッチを返す

# バッチごとに進行状況を表示しながら処理
batch_size = 1000  # バッチサイズを設定
for batch in tqdm(batch_process(vector_fit_text, batch_size), total=np.ceil(len(vector_fit_text) / batch_size)):  # 進捗バーを表示
    if len(batch) >= tfidf_word_vectorizer.min_df:  # 最小出現回数を考慮して単語ベクトルをフィット
        tfidf_word_vectorizer.fit(batch)
    if len(batch) >= tfidf_char_vectorizer.min_df:  # 最小出現回数を考慮して文字ベクトルをフィット
        tfidf_char_vectorizer.fit(batch)
        
def get_tfidf_vectors(df):
    vectorized_columns = []  # ベクトル化されたカラムのリスト
    for column in columns_to_vectorize:  # 各列に対して
        vectorized_columns.append(tfidf_word_vectorizer.transform(df[column]))  # 単語ベクトルを付加
        vectorized_columns.append(tfidf_char_vectorizer.transform(df[column]))  # 文字ベクトルを付加
    return hstack(vectorized_columns)  # スパース行列として返す

tfidf_train_vectors = get_tfidf_vectors(train)  # トレーニングデータのTF-IDFベクトルを取得
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
### LEN
```

---The following area is a Code cell (cell numver is 9)---
```python
def has_none(vals) -> int:
    # 応答にnullが含まれる場合、予測において役立つ可能性がある
    return int(any(val is None for val in vals))  # 一つでもnullがあれば1を返す

def str_length(vals) -> int:
    length = 0  # 文字列の長さを保持する変数
    for val in vals:  # 各値に対して
        if isinstance(val, str):  # 値が文字列であれば
            length += len(val)  # その長さを加算
    return length  # 総長を返す

def get_length_features(data: pd.DataFrame):
    length_feature_array = []  # 長さ特徴の配列を作成
    length_feature_array.append(data["response_a"].apply(str_length))  # 応答Aの長さを取得
    length_feature_array.append(data["response_b"].apply(str_length))  # 応答Bの長さを取得
    length_feature_array.append(length_feature_array[0] - length_feature_array[1])  # 応答Aと応答Bの長さの差
    length_feature_array.append((length_feature_array[0] + length_feature_array[1]) / 2)  # 応答の平均長
    length_feature_array.append((length_feature_array[0] / length_feature_array[1]))  # 応答Aと応答Bの長さの比
    length_feature_array.append(data["response_a"].apply(has_none))  # 応答Aにnullが含まれているか
    length_feature_array.append(data["response_b"].apply(has_none))  # 応答Bにnullが含まれているか
    length_feature_array.append(data["response_a"].apply(has_none) - data["response_b"].apply(has_none))  # nullの差
    length_feature_array = np.array(length_feature_array).reshape(len(length_feature_array), -1)  # 配列を整形
    length_feature_array = np.transpose(length_feature_array, (1, 0))  # 行と列を入れ替え
    return length_feature_array  # 最終結果を返す

train_length_features = get_length_features(train)  # トレーニングデータの長さ特徴を取得
print(train_length_features.shape)  # 形状を表示
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
### IFD
```

---The following area is a Code cell (cell numver is 11)---
```python
def get_ifd_features(data: pd.DataFrame):
    model = AutoModelForCausalLM.from_pretrained(gpt_path, torch_dtype=torch.bfloat16).to(device)  # GPTモデルを読み込んでデバイスに移動
    tokenizer = AutoTokenizer.from_pretrained(gpt_path)  # トークナイザーを読み込む
    model.eval()  # 評価モードに設定
    
    def get_ppl_features(output, instruct=''):
        try:
            answer_start_index = 0  # 応答の開始位置を初期化
            if instruct != '':  # 指示があれば
                answer_start_index = tokenizer.encode(instruct, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).shape[1]  # 指示のトークン数を取得
            input_ids = tokenizer.encode(instruct + output, return_tensors="pt", truncation=True, max_length=MAX_LENGTH).to(device)  # 入力IDを生成
            labels = input_ids.clone()  # ラベルをクローン
            labels[:, :answer_start_index] = -100  # 指示部分のラベルを無視
            with torch.no_grad():  # 勾配計算を無視
                outputs = model(input_ids, labels=labels)  # モデルへの入力とラベルを指定して出力を取得
            perplexity = torch.exp(outputs.loss)  # パープレキシティを計算
            return perplexity.to('cpu').item()  # CPUに戻して値を返す
        except:
            return 0  # エラーが発生した場合は0を返す
        
    ppl_ifd_feature_array = []  # 特徴の配列を初期化
    for i in tqdm(range(len(data)), desc="Scoring ifd"):  # データに対して進捗を表示しながらループ
        instruct = data['prompt'][i]  # プロンプトを取得
        output_a = data['response_a'][i]  # 応答Aを取得
        output_b = data['response_b'][i]  # 応答Bを取得
        
        # 応答Aのパープレキシティを計算
        ppl_ca_a, ppl_da_a = get_ppl_features(output_a, instruct), get_ppl_features(output_a)
        # 応答Bのパープレキシティを計算
        ppl_ca_b, ppl_da_b = get_ppl_features(output_b, instruct), get_ppl_features(output_b)
        try:
            ifd_a = ppl_ca_a / ppl_da_a  # 応答AのIFDを計算
        except ZeroDivisionError:
            ifd_a = 0  # ゼロ除算が発生した場合は0とする
        try:
            ifd_b = ppl_ca_b / ppl_da_b  # 応答BのIFDを計算
        except ZeroDivisionError:
            ifd_b = 0  # ゼロ除算が発生した場合は0とする
        ppl_ifd_feature_array.append([ppl_ca_a, ppl_da_a, ifd_a, ppl_ca_b, ppl_da_b, ifd_b, ifd_a - ifd_b, ifd_a - ifd_b > 0])  # 特徴を追加
        
    ppl_ifd_feature_array = np.array(ppl_ifd_feature_array).reshape(len(ppl_ifd_feature_array), -1)  # 特徴の配列を整形
    return ppl_ifd_feature_array  # 結果を返す

ppl_ifd_features = get_ifd_features(train)  # トレーニングデータのIFD特徴を取得
print(ppl_ifd_features.shape)  # 形状を表示
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
### Combine
```

---The following area is a Code cell (cell numver is 13)---
```python
tfidf_train_vectors = csr_matrix(tfidf_train_vectors)*0.2  # TF-IDFベクトルをスケーリング
ppl_ifd_features_csr = csr_matrix(ppl_ifd_features)*0.7  # IFD特徴をスケーリング
train_length_features_csr = csr_matrix(train_length_features)*0.1  # 長さ特徴をスケーリング
combined_train_vectors = hstack([tfidf_train_vectors, train_length_features_csr, ppl_ifd_features_csr])  # ベクトルを結合
print(combined_train_vectors.shape)  # 結合後の形状を表示
print("Vectorizing test text...")  # テストテキストをベクトル化中...
tfidf_test_vectors = get_tfidf_vectors(test)  # テストデータのTF-IDFベクトルを取得
tfidf_test_vectors_csr = csr_matrix(tfidf_test_vectors)*0.2  # テストデータのTF-IDFベクトルをスケーリング
test_ppl_ifd_features = get_ifd_features(test)  # テストデータのIFD特徴を取得
test_ppl_ifd_features_csr = csr_matrix(test_ppl_ifd_features)*0.7  # テストデータのIFD特徴をスケーリング
test_length_features = get_length_features(test)  # テストデータの長さ特徴を取得
test_length_features_csr = csr_matrix(test_length_features)*0.1  # テストデータの長さ特徴をスケーリング
combined_test_vectors = hstack([tfidf_test_vectors_csr, test_length_features_csr, test_ppl_ifd_features_csr])  # テストデータのベクトルを結合
print("Done!")  # 完了メッセージを表示
```

---The following area is a Code cell (cell numver is 14)---
```python
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import log_loss, accuracy_score
from sklearn.model_selection import train_test_split
import joblib

max_estimators = 1000  # 最大推定器の数を設定
early_stopping_limit = 100  # 早期停止の制限を設定

# データ準備
X = combined_train_vectors  # 特徴ベクトル
y_encoded = train['label'].values  # ラベルを取得

# LightGBMのパラメータ
params = {
    'n_estimators': max_estimators,  # 推定器の数
    'max_depth': 4,  # 最大深さ
    'subsample': 0.8,  # サンプリングフラクション
    'colsample_bytree': 0.8,  # 特徴のサンプリングフラクション
    'objective': 'multiclass',  # 目的関数
    'num_class': 3,  # クラス数
    'metric': 'multi_logloss',  # 評価指標
    'random_state': 42,  # 乱数シード
    'learning_rate': 0.03,  # 学習率
    'verbose': -1  # ログの出力を抑制
}

# モデルの作成
model = lgb.LGBMClassifier(**params)  # LightGBMクラス分類器を生成

# 5フォールドの交差検証
stratified_k_fold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)  # StratifiedKFoldを設定
logloss_scores = []  # loglossスコアを保持するリスト
accuracy_scores = []  # 精度スコアを保持するリスト
test_pred_list = []  # テストの予測結果を保持するリスト

for fold, (train_indices, val_indices) in enumerate(stratified_k_fold.split(X, y_encoded)):  # 各フォールドに対して
    print(f"\nFold {fold + 1}")  # フォールドの番号を表示
    X_train_fold, X_val_fold = X[train_indices], X[val_indices]  # フォールドのトレーニングデータとバリデーションデータ
    y_train_fold, y_val_fold = y_encoded[train_indices], y_encoded[val_indices]  # フォールドのトレーニングラベルとバリデーションラベル

    def callback(env):
        if env.iteration % 10 == 0: print("Iteration:", env.iteration, "\tLog Loss:", env.evaluation_result_list[0][2])  # ログ出力のコールバック関数

    model.fit(
        X_train_fold, y_train_fold,  # トレーニングデータとトレーニングラベルでフィット
        eval_set=[(X_val_fold, y_val_fold)],  # バリデーションデータを指定
        eval_metric='multi_logloss',  # 評価指標を指定
        callbacks=[lgb.early_stopping(stopping_rounds=early_stopping_limit), callback]  # 早期停止とコールバックを設定
    )

    y_pred_proba_fold = model.predict_proba(X_val_fold)  # バリデーションデータに対する予測確率
    logloss_fold = log_loss(y_val_fold, y_pred_proba_fold)  # loglossを計算
    logloss_scores.append(logloss_fold)  # スコアを追加
    print(f"Log Loss: {logloss_fold}")  # loglossを表示
    
    y_pred_fold = np.argmax(y_pred_proba_fold, axis=1)  # 最大の予測確率を持つクラスを取得
    accuracy_fold = accuracy_score(y_val_fold, y_pred_fold)  # 精度を計算
    accuracy_scores.append(accuracy_fold)  # スコアを追加
    print(f"Accuracy: {accuracy_fold}")  # 精度を表示

    test_pred_list.append(model.predict_proba(combined_test_vectors[-test.shape[0]:]))  # テストデータの予測確率を追加

# 平均スコアを計算して表示
average_logloss = np.mean(logloss_scores)  # 平均loglossを計算
average_accuracy = np.mean(accuracy_scores)  # 平均精度を計算
print(f"\nAverage Log Loss: {average_logloss}")  # 平均loglossを表示
print(f"Average Accuracy: {average_accuracy}")  # 平均精度を表示
```

---The following area is a Code cell (cell numver is 15)---
```python
preds_test = np.mean(test_pred_list, axis=0)  # テスト予測の平均を計算
submission = pd.DataFrame({
    'id': test["id"],  # テストデータのID
    'winner_model_a': preds_test[:, 0],  # モデルAの勝者予測
    'winner_model_b': preds_test[:, 1],  # モデルBの勝者予測
    'winner_tie': preds_test[:, 2]  # タイの勝者予測
})
submission.to_csv('submission.csv', index=False)  # 提出ファイルをCSVとして保存
display(submission)  # 提出内容を表示
```

** @@@ Jupyter Notebook numver 69, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleコンペティション「LMSYS - Chatbot Arena」における人間による好みの予測を行うための機械学習モデルの開発を目的としています。具体的には、異なる大規模言語モデル（LLM）が生成した応答のうち、ユーザーに最も好まれるものを予測するモデルを設計しています。

## 問題の取り組み
このNotebookでは、特に以下の課題に取り組んでいます：
- LLMの応答の中から、どのモデルがユーザーの好みに合致するかを予測する。
- LoRA（Low-Rank Adaptation）を使用したモデルの微調整。
- 最大シーケンス長を768に設定し、メモリ制約を考慮する。

## 使用手法とライブラリ
Notebookは多くのPyTorchとTransformersライブラリのモジュールを活用しています。以下に主な手法とライブラリを示します：

- **データ前処理**: サンプリング、欠損値の除去、ラベルの変換などを行っています。データはCSVファイルから読み込まれ、必要に応じてJSON形式に変換されます。
- **カスタムデータセット**:
  - `CustomDataset`クラスを使用して、トークン化されたプロンプトと応答をデータローダーに組み込みます。
  - `create_dataloaders`関数を介して、訓練と評価用のデータローダーを作成します。
  
- **モデル構築**: 
  - MicrosoftのPhi-3-mini-4k-instructモデルをベースに、LoRAアダプターを追加したカスタムモデルを定義しています。これにより、少ないパラメータでモデルのパフォーマンスを向上させることができます。
  - `Phi3Attention`クラスをカスタマイズして、LoRAアダプターが適用されたマルチヘッダーアテンションを実装しています。

- **トレーニングプロセス**:
  - 半精度トレーニングを行い、モデルのトレーニングと評価をパラレルに処理します。
  - 最適化手法にはAdamWが使用され、学習率スケジューラも設定されています。
  - 評価指標として、精度とF1スコアを計算・表示しています。

- **量子化**: 
  - モデルの重みを量子化する関数が実装されており、モデルのサイズを削減し、効率的な推論を行うことが可能です。

このNotebookでは、最終的にトレーニングしたモデルを保存し、後で利用できるようにしています。このプロセス全体を通じて、特にユーザーの好みに基づく予測を行うための効果的な機械学習手法を実装しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# インストール不要

microsoft/Phi-3-mini-4k-instruct + LoRA > GPUでの並列トレーニング

最大シーケンス長はモデルのパフォーマンスに大きな影響を与えますが、メモリ不足のため、最大長は768に設定されました。


## ライブラリの読み込み
```

---The following area is a Code cell (cell numver is 2)---
```python
import multiprocessing as mp
mp.set_start_method('spawn')
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import DataLoader

import datasets
from datasets import load_dataset, load_metric, Dataset

from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, log_loss

from accelerate import notebook_launcher, Accelerator, PartialState
from accelerate.utils import write_basic_config
from accelerate.inference import prepare_pippy

import transformers
from transformers import (
    AdamW,
    get_linear_schedule_with_warmup,
    set_seed,
    AutoTokenizer,
    AutoModel,
    AutoModelForSequenceClassification,
    DataCollatorWithPadding,
    AutoConfig
)

import os
import shutil
import math
import json
from tqdm import tqdm
import gc
import pandas as pd
import numpy as np
from typing import Optional, Tuple
```

---The following area is a Code cell (cell numver is 4)---
```python
# パラメータ設定
model_name = "/kaggle/input/microsoftphi-3-mini-4k-instruct/transformers/default/1"  # モデルの名前
model_path = "model_checkpoint.pth"  # モデルのチェックポイントのパス
seed = 42  # 乱数シード
lora_r = 2  # LoRAのランク
quantize_bit = 16  # 量子化ビット数
learning_rate = 5e-4  # 学習率
weight_decay = 0.1  # 重み減衰
beta1 = 0.9  # Adamオプティマイザのβ1
beta2 = 0.999  # Adamオプティマイザのβ2
eps = 1e-9  # 許容誤差
l1_rate = 1e-10  # L1正則化率
batch_size = 1  # バッチサイズ
max_len = 256  # 最大シーケンス長
n_sample = 0.10  # サンプリング比率
n_epoch = 2  # エポック数
device = "cuda"  # 使用するデバイス
file_path = '/kaggle/input/lmsys-chatbot-arena/train.csv'  # トレーニングデータのファイルパス
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## 前処理
```

---The following area is a Code cell (cell numver is 6)---
```python
def cl(x):
  if x == [1,0,0]:
    return 0  # モデルAが勝者の場合
  elif x == [0,1,0]:
    return 1  # モデルBが勝者の場合
  else:
    return 2  # 引き分けの場合
```

---The following area is a Code cell (cell numver is 7)---
```python
def preprocess_data(file_path, sample=False):
    train = pd.read_csv(file_path)  # 訓練データをCSVから読み込む
    clf_train = train[['prompt','response_a','response_b','winner_model_a','winner_model_b','winner_tie']]

    # JSON形式から最初の要素を抽出
    clf_train.loc[:, "prompt"] = clf_train["prompt"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_a"] = clf_train["response_a"].apply(lambda x: json.loads(x)[0])
    clf_train.loc[:, "response_b"] = clf_train["response_b"].apply(lambda x: json.loads(x)[0])

    clf_train = clf_train.dropna()  # 欠損値を削除
    clf_train = clf_train.reset_index(drop=True)  # インデックスをリセット

    # 各勝者を含むターゲット列を作成
    clf_train['target'] = [[clf_train['winner_model_a'][x],clf_train['winner_model_b'][x],clf_train['winner_tie'][x]] for x in range(len(clf_train))]

    clf_train = clf_train[['prompt','response_a','response_b','target']]

    # ターゲットをラベルに変換
    clf_train['labels'] = clf_train['target'].apply(lambda x : cl(x))

    # 各要素の長さを計算
    clf_train['p_len'] = clf_train['prompt'].apply(lambda x : len(x))
    clf_train['a_len'] = clf_train['response_a'].apply(lambda x : len(x))
    clf_train['b_len'] = clf_train['response_b'].apply(lambda x : len(x))

    clf_train['len'] = clf_train['p_len'] + clf_train['a_len'] + clf_train['b_len']
    
    if sample:  # サンプリングを行う場合
        clf_train = clf_train.sample(int(len(clf_train) * n_sample), weights="len", random_state=seed).reset_index(drop=True)

    t_dat, v_dat = train_test_split(clf_train, test_size=0.2, random_state=42, stratify=clf_train['labels'])  # データセットを訓練と検証に分割

    t_dat = t_dat.reset_index(drop=True)
    v_dat = v_dat.reset_index(drop=True)

    t_dat = t_dat.drop(labels='target', axis=1)  # 'target'列を削除
    v_dat = v_dat.drop(labels='target', axis=1)  # 'target'列を削除
    return t_dat, v_dat  # 訓練データと検証データを返す
```

---The following area is a Code cell (cell numver is 8)---
```python
class CustomDataset(torch.utils.data.Dataset):

    def __init__(self, df, tokenizer, max_len):
        self.tokenizer = tokenizer
        self.prompt = df['prompt']
        self.response_a = df['response_a']
        self.response_b = df['response_b']
        self.max_len = max_len
        self.targets = df.get('labels', None)  # ラベルを取得

    def __len__(self):
        return len(self.prompt)  # プロンプトの数を返す

    def __getitem__(self, index):
        # インデックスに基づいてプロンプトおよび応答を取得
        prompt = str(self.prompt[index])
        response_a = str(self.response_a[index])
        response_b = str(self.response_b[index])

        # 各テキストのトークナイズされた長さを計算
        prompt_len = len(self.tokenizer("##prompt: " + prompt, add_special_tokens=True)['input_ids'])
        response_a_len = len(self.tokenizer("##response_a: " + response_a, add_special_tokens=True)['input_ids'])
        response_b_len = len(self.tokenizer("##response_b: " + response_b, add_special_tokens=True)['input_ids'])

        # 最大長を超えないように長さを制限
        final_prompt_len = min(self.max_len, prompt_len)
        final_a_len = min(self.max_len, response_a_len)
        final_b_len = min(self.max_len, response_b_len)

        # トークナイズ
        prompt_token = self.tokenizer("##prompt: " + prompt, add_special_tokens=True, max_length=final_prompt_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_a_token = self.tokenizer("##response_a: " + response_a, add_special_tokens=True, max_length=final_a_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')
        response_b_token = self.tokenizer("##response_b: " + response_b, add_special_tokens=True, max_length=final_b_len, truncation=True, padding='max_length', return_attention_mask=True, return_tensors='pt')

        # 入力IDとアテンションマスクを結合
        input_ids = torch.cat([prompt_token['input_ids'], response_a_token['input_ids'], response_b_token['input_ids']], dim=1)
        attention_mask = torch.cat([prompt_token['attention_mask'], response_a_token['attention_mask'], response_b_token['attention_mask']], dim=1)

        if self.targets is not None:
            labels = torch.LongTensor([self.targets[index]])
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten(), 'labels': labels}  # 入力ID、アテンションマスク、ラベルを返す
        else:
            return {'input_ids': input_ids.flatten(), 'attention_mask': attention_mask.flatten()}  # 入力IDとアテンションマスクだけを返す
```

---The following area is a Code cell (cell numver is 9)---
```python
def custom_collate_fn(batch, tokenizer):

    input_ids = [item['input_ids'] for item in batch]
    attention_masks = [item['attention_mask'] for item in batch]
    labels = torch.cat([item['labels'] for item in batch], dim=0) if 'labels' in batch[0] else None

    # バッチ内のシーケンスの最大長を見つける
    max_len = max([input_id.size(0) for input_id in input_ids])

    # 新しい最大長で再トークナイズ
    new_input_ids = []
    new_attention_masks = []

    for item in batch:
        input_ids = item['input_ids'][:max_len]  # 最大長で切り取る
        attention_mask = item['attention_mask'][:max_len]  # 最大長で切り取る

        new_input_ids.append(input_ids)  # 新しい入力IDに追加
        new_attention_masks.append(attention_mask)  # 新しいアテンションマスクに追加

    new_input_ids = pad_sequence(new_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)  # パディングを適用
    new_attention_masks = pad_sequence(new_attention_masks, batch_first=True, padding_value=0)  # パディングを適用

    output = {
    'input_ids': new_input_ids,
    'attention_mask': new_attention_masks}

    if labels is not None:
        output['labels'] = labels  # ラベルがある場合は追加

    return output
```

---The following area is a Code cell (cell numver is 10)---
```python
def create_dataloaders(df, tokenizer, max_len, batch_size, shuffle=True):
    # データローダーを作成するための関数
    dataloader = DataLoader(
        CustomDataset(df, tokenizer, max_len), shuffle=shuffle, batch_size=batch_size , collate_fn=lambda x: custom_collate_fn(x, tokenizer)
    )
    return dataloader
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
## モデルの読み込み
```

---The following area is a Code cell (cell numver is 12)---
```python
def quantize_tensor(tensor, num_bits=quantize_bit):
    qmin = 0.
    qmax = 2.**num_bits - 1.  # 量子化の最大値を計算

    min_val, max_val = tensor.min(), tensor.max()  # テンソルの最小値と最大値を取得
    scale = (max_val - min_val) / (qmax - qmin)  # スケールを計算
    zero_point = qmin - min_val / scale  # ゼロポイントを計算

    quantized_tensor = torch.round(tensor / scale + zero_point)  # テンソルを量子化
    quantized_tensor = torch.clamp(quantized_tensor, qmin, qmax)  # 最大・最小値でクリップ
    quantized_tensor = (quantized_tensor - zero_point) * scale  # 再スケーリング

    return quantized_tensor

def quantize_model(model, num_bits=8):
    for name, module in model.named_modules():
        if isinstance(module, nn.Linear):  # 線形層の場合
            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化
        elif isinstance(module, nn.Conv2d):  # 畳み込み層の場合
            module.weight.data = quantize_tensor(module.weight.data, num_bits)  # 重みを量子化
            if module.bias is not None:
                module.bias.data = quantize_tensor(module.bias.data, num_bits)  # バイアスを量子化

    return model


# import torch.quantization

# def quantize_model_dynamic(model):
#     model.qconfig = torch.quantization.default_dynamic_qconfig
#     torch.quantization.prepare(model, inplace=True)
#     torch.quantization.convert(model, inplace=True)
#     return model
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
## アダプタレイヤーの追加
transformers.models.phi3.modeling_phi3.Phi3Attentionからコピー

[GitHubのURL](https://github.com/huggingface/transformers/blob/main/src/transformers/models/phi3/modeling_phi3.py)
```

---The following area is a Code cell (cell numver is 14)---
```python
class LoRA(nn.Module):
    def __init__(self, in_features, out_features, rank=lora_r, alpha=1.0, lora_dropout=0.05):
        super(LoRA, self).__init__()
        self.alpha = alpha  # Alphaパラメータ
        self.rank = rank  # LoRAのランク
        self.lora_a = nn.Linear(in_features, rank, bias=False)  # LoRA A行列
        self.lora_b = nn.Linear(rank, out_features, bias=False)  # LoRA B行列
        self.dropout = nn.Dropout(lora_dropout)  # ドロップアウト層

    def forward(self, x):
        lora_out = self.alpha * self.lora_b(self.lora_a(x))  # LoRAの出力を計算
        lora_out = self.dropout(lora_out)  # ドロップアウトを適用
        return lora_out
```

---The following area is a Code cell (cell numver is 15)---
```python
from transformers.models.phi3.modeling_phi3 import (
Phi3RotaryEmbedding, 
# Phi3LongRoPEScaledRotaryEmbedding,
apply_rotary_pos_emb,
repeat_kv
)
```

---The following area is a Code cell (cell numver is 16)---
```python
class Phi3Attention(nn.Module):
    """ 'Attention Is All You Need' 論文に基づくマルチヘッダーアテンション """

    def __init__(self, config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"{self.__class__.__name__} を layer_idx を指定せずにインスタンス化するのは推奨されず、"
                "キャッシュを使用している場合にフォワード呼び出し時にエラーが発生する可能性があります。"
                "このクラスを作成する際は、layer_idx を指定してください。"
            )

        # 各種設定を初期化
        self.attention_dropout = config.attention_dropout
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads
        self.max_position_embeddings = config.max_position_embeddings
        self.original_max_position_embeddings = config.original_max_position_embeddings
        self.rope_theta = config.rope_theta
        self.rope_scaling = config.rope_scaling
        self.is_causal = True

        # hidden_size が num_heads で割り切れない場合にエラーを発生させる
        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size は num_heads で割り切れる必要があります (得られた `hidden_size`: {self.hidden_size}"
                f" と `num_heads`: {self.num_heads})。"
            )

        # 線形変換のためのサイズを計算
        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)
        self._init_rope()
        
        ########################## LoRAアダプタ ##########################
        self.qkv_lora = LoRA(self.hidden_size, op_size, lora_r)  # LoRAアダプタの初期化
        self.o_lora = LoRA(self.num_heads * self.head_dim, self.hidden_size, lora_r)  # LoRAアダプタの初期化
        ########################## LoRAアダプタ ##########################
        
    def _init_rope(self):
        if self.rope_scaling is None:
            self.rotary_emb = Phi3RotaryEmbedding(
                self.head_dim,
                max_position_embeddings=self.max_position_embeddings,
                base=self.rope_theta,
            )
        else:
            scaling_type = self.config.rope_scaling["type"]
            if scaling_type == "longrope":
                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)
            else:
                raise ValueError(f"未知の RoPE スケーリングタイプ {scaling_type}")

    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value=None,
        output_attentions: bool = False,
        use_cache: bool = False,
        cache_position: Optional[torch.LongTensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        # logger.warning_once("フラッシュアテンションの実装を実行していないため、数値的な差異が予想されます。")

        bsz, q_len, _ = hidden_states.size()
        ########################## LoRAアダプタ ##########################
        qkv = self.qkv_proj(hidden_states) + self.qkv_lora(hidden_states)  # LoRAアダプタを適用
        ########################## LoRAアダプタ ##########################
        query_pos = self.num_heads * self.head_dim
        query_states = qkv[..., :query_pos]
        key_states = qkv[..., query_pos:query_pos + self.num_key_value_heads * self.head_dim]
        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim:]

        # 各状態の形状を整形
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2]
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"キャッシュ構造はバージョンv4.36以降変更されました。{self.__class__.__name__} を使用して自動回帰デコーディングを行っている場合、"
                    "レイヤーインデックスで初期化されていることを確認してください。"
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)

        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos, "cache_position": cache_position}  # RoPEモデル専用の引数
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # k/v ヘッドが n_kv_heads < n_heads の場合は重複
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights は {(bsz, self.num_heads, q_len, kv_seq_len)} サイズであるべきですが、"
                f" {attn_weights.size()} です。"
            )

        if attention_mask is not None:
            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]
            attn_weights += causal_mask

        # atención を fp32 にアップキャスト
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)

        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` は {(bsz, self.num_heads, q_len, self.head_dim)} サイズであるべきですが、"
                f" {attn_output.size()} です。"
            )

        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
        ########################## LoRAアダプタ ##########################
        attn_output = self.o_proj(attn_output) + self.o_lora(attn_output)  # LoRAを適用
        ########################## LoRAアダプタ ##########################
        if not output_attentions:
            attn_weights = None

        return attn_output, attn_weights, past_key_value
```

---The following area is a Code cell (cell numver is 17)---
```python
def replace_attention_module(config, layer, layer_idx):
    if hasattr(layer, 'self_attn') and layer_idx > 12:

        new_attention = Phi3Attention(config, layer_idx)  # 新しいPhi3Attentionを作成

        # 重みをコピー
        new_attention.qkv_proj.weight.data.copy_(layer.self_attn.qkv_proj.weight.data)
        new_attention.o_proj.weight.data.copy_(layer.self_attn.o_proj.weight.data)

        layer.self_attn = new_attention  # 置き換え
```

---The following area is a Code cell (cell numver is 18)---
```python
loss_fn = nn.CrossEntropyLoss()  # クロスエントロピー損失関数の初期化

class LoraModelForClassification(nn.Module):
    def __init__(self, lora_model):  # LoRAモデルを受け取る
        super(LoraModelForClassification, self).__init__()
        self.config = lora_model.config  # 設定を保存
        self.peft_model = lora_model  # LoRAモデルを保存
        self.dropout = nn.Dropout(0.1)  # ドロップアウト層
        self.classifier = nn.Linear(self.config.hidden_size, 3)  # 最終分類層の初期化
#         self.classifier.weight.data = self.classifier.weight.data.to(torch.float16)  # 重みのデータ型を変更
#         self.classifier.bias.data = self.classifier.bias.data.to(torch.float16)  # バイアスのデータ型を変更

    def forward(self, input_ids, attention_mask, labels=None):
        outputs = self.peft_model(input_ids, attention_mask=attention_mask)  # モデルからの出力
        pooled_output = outputs.last_hidden_state.mean(dim=1)  # 出力の平均値を計算
        output_dropout = self.dropout(pooled_output)  # ドロップアウトを適用
        logits = self.classifier(output_dropout)  # 最終的なロジットを取得
        loss = None
        if labels is not None:
            labels = labels
            loss = loss_fn(logits, labels)  # 損失を計算
        return loss, logits  # 損失とロジットを返す
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## パラレルトレーニング
```

---The following area is a Code cell (cell numver is 20)---
```python
def parallel_function(model_name, attention_name, file_path):
    mp.set_start_method('spawn', force=True)  # プロセスの開始方法を設定

    accelerator = Accelerator(mixed_precision="fp16")  # 半精度トレーニングを使用
    if accelerator.is_main_process:  # メインプロセスの場合
        datasets.utils.logging.set_verbosity_warning()  # 警告のログレベルを設定
        transformers.utils.logging.set_verbosity_info()  # 情報のログレベルを設定
    else:
        datasets.utils.logging.set_verbosity_error()  # エラーログレベルを設定
        transformers.utils.logging.set_verbosity_error()  # エラーログレベルを設定

    set_seed(seed)  # 乱数シードを設定

    tokenizer = AutoTokenizer.from_pretrained(model_name)  # トークナイザを読み込む
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを設定
    tokenizer.padding_side = "right"  # fp16トレーニングの問題を修正
    
    model = AutoModel.from_pretrained(model_name, torch_dtype=torch.float16)  # モデルを読み込み
    model = quantize_model(model)  # モデルを量子化
    for idx, layer in enumerate(model.layers):
        replace_attention_module(model.config, layer, idx)  # アテンションモジュールを置き換え
    model = LoraModelForClassification(model)  # LoRAモデルを初期化

    # 学習可能なパラメータの設定
    for param in model.peft_model.parameters():
        param.requires_grad = False
    for param in model.classifier.parameters():
        param.requires_grad = True

    # LoRAの重みを学習可能にする
    for name, module in model.named_modules():
        if isinstance(module, attention_name):
            module.qkv_lora.lora_a.weight.requires_grad = True  # LoRA A層の重み
            module.qkv_lora.lora_b.weight.requires_grad = True  # LoRA B層の重み
            module.o_lora.lora_a.weight.requires_grad = True  # 出力LoRA A層の重み
            module.o_lora.lora_b.weight.requires_grad = True  # 出力LoRA B層の重み

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"総学習可能パラメータ数: {total_params}")  # 学習可能パラメータ数を表示

    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)  # オプティマイザの初期化

    # データを前処理し、データローダーを作成
    t_dat, v_dat = preprocess_data(file_path, sample=True)
    train_dataloader = create_dataloaders(t_dat, tokenizer, max_len, batch_size=batch_size, shuffle=True)
    eval_dataloader = create_dataloaders(v_dat, tokenizer, max_len, batch_size=batch_size, shuffle=True)

    # モデルとオプティマイザを準備
    model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(
        model, optimizer, train_dataloader, eval_dataloader)

    lr_scheduler = get_linear_schedule_with_warmup(  # 学習率スケジューラを設定
        optimizer=optimizer,
        num_warmup_steps=100,
        num_training_steps=len(train_dataloader) * n_epoch,
    )

    progress_bar = tqdm(range(n_epoch * len(train_dataloader)), disable=not accelerator.is_main_process)  # プログレスバーの初期化

    # トレーニングと評価のループ
    train_loss = 0
    valid_loss = 0
    
    for epoch in range(n_epoch):
        model.train()  # モデルをトレーニングモードに設定
        for step, batch in enumerate(train_dataloader):
            loss, _ = model(**batch)  # バッチをモデルに渡して損失を計算
            accelerator.backward(loss)  # バックプロパゲーション

            optimizer.step()  # オプティマイザのステップ
            lr_scheduler.step()  # 学習率スケジューラのステップ
            optimizer.zero_grad()  # 勾配をゼロにリセット
            progress_bar.update(1)  # プログレスバーを更新

            train_loss += loss.item()  # トレーニングロスを累積
            
        all_predictions = []
        all_labels = []
        model.eval()  # モデルを評価モードに設定
        for step, batch in enumerate(eval_dataloader):
            with torch.no_grad():
                loss, logits = model(**batch)  # 評価バッチに対する損失とロジットを計算
            predictions = logits.argmax(dim=-1)  # 最大のロジットを予測として抽出
            all_predictions.append(accelerator.gather(predictions))  # 予測を収集
            all_labels.append(accelerator.gather(batch["labels"]))  # ラベルを収集
            
            valid_loss += loss.item()  # 検証ロスを累積

        all_predictions = torch.cat(all_predictions)[:len(eval_dataloader)].cpu()  # すべての予測を結合
        all_labels = torch.cat(all_labels)[:len(eval_dataloader)].cpu()  # すべてのラベルを結合

        acc_metric = accuracy_score(all_labels, all_predictions)  # 精度を計算
        eval_metric = f1_score(all_labels, all_predictions, average="macro")  # F1スコアを計算
        train_loss_avg = train_loss / len(train_dataloader)  # トレーニングロスの平均を計算
        valid_loss_avg = valid_loss / len(eval_dataloader)  # 検証ロスの平均を計算
        
        # 結果を表示
        accelerator.print(f"エポック: {epoch} \n 精度: {acc_metric:.3f} \n F1スコア: {eval_metric:.3f} \n トレーニングロス: {train_loss_avg:.3f} \n 検証ロス: {valid_loss_avg:.3f}")

    model = accelerator.unwrap_model(model)  # モデルのラッピングを解除
    accelerator.wait_for_everyone()  # すべてのプロセスが完了するのを待つ

    # モデルの状態を保存
    if accelerator.is_main_process:
        torch.save(model.state_dict(), model_path)  # モデルの状態を保存

    # 同期完了のメッセージ
    accelerator.wait_for_everyone()
```

---The following area is a Code cell (cell numver is 21)---
```python
notebook_launcher(parallel_function, args=(model_name, Phi3Attention, file_path,), num_processes=2)  # パラレルファンクションを起動
```

** @@@ Jupyter Notebook numver 70, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、チャットボットの応答に対するユーザーの好みを予測するための提出ファイルを作成することを目的としています。

### 取り組む問題
ノートブックは、二つの異なるLLM（Large Language Models）による応答の間で、どちらのモデルがユーザーに好まれるかを予測し、その結果をCSVファイル形式で提出するためのデータフレームを生成します。具体的には、テストデータに基づいて、モデルA、モデルBの勝者となる確率、および引き分けの確率を含む提出ファイルを作成します。

### 使用されている手法とライブラリ
- **NumPy**: 線形代数やランダムな値の生成、正規化に使用されます。`np.random.rand`を用いてランダムな浮動小数点数を生成し、`np.isclose`を用いて合計が1に近いことを確認しています。
- **Pandas**: データフレームの操作やCSVファイルの入出力のために使用されています。`pd.read_csv`を使ってテストデータを読み込み、`pd.DataFrame`を使用して新しいデータフレームを作成しています。また、`to_csv`メソッドを使用して、最終的なデータフレームを「submission.csv」という名前で保存します。

### ノートブックの流れ
1. テスト用CSVファイルを読み込む。
2. 新しいデータフレームを作成し、テストデータの行数に応じたランダムな値を生成。
3. 生成したランダムな値を正規化して、各行の合計が1になるように調整。
4. 正規化した値をデータフレームに追加し、勝者の確率を設定。
5. 確率が合計1に近いことを確認するためにアサーションを行う。
6. 最初の数行を表示し、結果を確認後、ファイルとして保存。

このノートブックは、過程におけるデータ生成と基本的な確率モデルを使用しており、提出形式に適した結果を効率的に生成する方法を示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
import numpy as np # 線形代数を行うためのnumpyライブラリをインポートします
import pandas as pd # データ処理やCSVファイルの入出力を行うためのpandasライブラリをインポートします（例: pd.read_csv）

# テスト用のCSVファイルを読み込みます
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# 'test'と同じ'id'列を持つ新しいデータフレームを作成します
sample_submission = pd.DataFrame({'id': test['id']})

# 各列に対してランダムな浮動小数点数を生成します
n_rows = len(test) # テストデータの行数を取得します
random_values = np.random.rand(n_rows, 3) # 行数と3列にわたるランダムな値を生成します

# ランダムな値を正規化して、各行の合計が1になるようにします
normalized_values = random_values / random_values.sum(axis=1)[:, np.newaxis]

# 正規化したランダムな値をデータフレームに追加します
sample_submission['winner_model_a'] = normalized_values[:, 0] # モデルAの勝者の確率を追加
sample_submission['winner_model_b'] = normalized_values[:, 1] # モデルBの勝者の確率を追加
sample_submission['winner_tie'] = normalized_values[:, 2] # 引き分けの確率を追加

# 勝者列の合計が各行で1に非常に近いことを確認します
# (浮動小数点精度の問題に対処するためにnp.iscloseを使用します)
assert np.allclose(sample_submission[['winner_model_a', 'winner_model_b', 'winner_tie']].sum(axis=1), 1) # Assert文を使用して条件を確認

# 新しいデータフレームの最初の数行を表示します
print(sample_submission.head()) # sample_submissionの最初の5行を表示します
```

---The following area is a Code cell (cell numver is 2)---
```python
# データフレームをCSVファイルとして保存します
# 'submission.csv'という名前で保存し、行のインデックスは含めません
sample_submission.to_csv('submission.csv', index=False)
```

** @@@ Jupyter Notebook numver 71, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaのコンペティションにおいて、どちらのチャットボット応答がユーザーに好まれるかを予測するためのモデルをトレーニングするプロセスを示しています。具体的には、2つの大規模言語モデル（LLM）から生成された応答を比較し、選好を予測するための一連の手法を適用しています。

主な取り組み内容としては、以下の手順が含まれています：

1. **ライブラリのインポート**: `transformers`, `torch`, `peft`, `datasets`などのライブラリを用いて、モデルのトレーニング、推論、およびデータ処理を行います。

2. **データの準備**: CSVファイルからトレーニングデータとテストデータを読み込み、JSON形式のテキストデータを処理してプロンプトと応答を結合する関数を作成します。この段階で、欠損値も処理されています。

3. **トークナイザーの準備**: 指定したトークナイザーを用いてテキストをトークナイズし、入力IDとアテンションマスクを生成します。

4. **モデル設定とロード**: GPUを2つ使ってモデルを設定し、LoRA（Low-Rank Adaptation）技術を利用してメモリ効率を最大化しながら8ビットの計算でモデルをロードします。その後、トレーニング済みの重みを読み込みます。

5. **推論の実行**: スレッドプールを使用し、バッチごとにデータを処理しながらモデルの推論を行います。結果として、Aモデルが勝つ確率、Bモデルが勝つ確率、引き分けの確率を計算します。

6. **提出ファイルの生成**: 最終的な予測結果を新しいデータフレームにまとめ、"submission.csv"という名前でCSVファイルとして保存します。

このNotebookでは、特に`transformers`ライブラリの`AutoModelForSequenceClassification`やLoRA設定、また`PeftModel`を活用してモデルのメモリ効率とトレーニングの性能を向上させる努力がなされています。ユーザーの好みを効果的に予測するために、複数のモデルの出力を比較するアプローチが取られています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# 注記
- [トレーニングスクリプト](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)

# インポート
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install transformers -U --no-index --find-links /kaggle/input/lmsys-transformers/lmsys_transformers
```

---The following area is a Code cell (cell numver is 3)---
```python
!pip install peft accelerate bitsandbytes -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 4)---
```python
import json
import time
import sklearn
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from torch.cuda.amp import autocast
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor
from threading import Thread
from datasets import load_dataset, Dataset
from transformers import AutoTokenizer, AutoModel, AutoConfig, DataCollatorForSeq2Seq
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding, AutoModelForSequenceClassification
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType 
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from transformers.modeling_outputs import CausalLMOutputWithPast
from transformers import BitsAndBytesConfig, LlamaForCausalLM, LlamaModel, LlamaPreTrainedModel
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from transformers import set_seed

# 高度なメモリ効率を利用するための設定
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)
# 利用可能なGPUが2つあることを確認します
assert torch.cuda.device_count() == 2, "申し訳ありませんが、マルチGPUが必要です！"
import warnings
warnings.filterwarnings('ignore')
```

---The following area is a Code cell (cell numver is 5)---
```python
# モデルの設定
MODEL_NAME = '/kaggle/input/meta-llama-3-8b/LLM-Research/Meta-Llama-3-8B'  # モデル名を指定
WEIGHTS_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/checkpoint-550'  # 重みのパス
TOKENIZER_PATH = '/kaggle/input/llama31-sample5500-cls/llama31-sample5500-cls/tokenizer'  # トークナイザーのパス
MAX_LENGTH = 2400  # 最大シーケンス長
BATCH_SIZE = 4  # バッチサイズ設定
DEVICE = torch.device("cuda")  # CUDAデバイスを指定
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 7)---
```python
def process_text(text: str) -> list:
    # JSON形式のテキストを解析し、欠損値を'none'で置き換えます
    x = json.loads(text)
    x = ['none' if pd.isna(i) else i for i in x]
    return x

def merge_text(x):
    # プロンプトとレスポンスを結合します
    prompt = x['prompt']
    response_a = x['response_a']
    response_b = x['response_b']
    res = ''
    for i in range(len(prompt)):
        if i == len(prompt) - 1:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}'
        else:
            res += f'<prompt>: {prompt[i]}' + f'\n\n<response_a>: {response_a[i]}' + f'\n\n<response_b>: {response_b[i]}' + '\n\n'
    return res
```

---The following area is a Code cell (cell numver is 8)---
```python
# トレーニングデータとテストデータを読み込みます
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# テストデータの行数が3のときは、トレーニングデータの最初の100行を使用します
if test.shape[0] == 3:
    test = train.head(100)

# 各列を処理します
test['prompt'] = test['prompt'].map(lambda x: process_text(x))
test['response_a'] = test['response_a'].map(lambda x: process_text(x))
test['response_b'] = test['response_b'].map(lambda x: process_text(x))

# プロンプトとレスポンスを結合して新しいテキスト列を作ります
test['text'] = test.apply(lambda x: merge_text(x), axis=1)
# テストデータの最初の行を表示します
test.head()
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(tokenizer, x):
    # テキストをトークナイズして、入力IDとアテンションマスクを生成します
    tokenized = tokenizer(x, max_length=MAX_LENGTH, truncation=True)
    return tokenized['input_ids'], tokenized['attention_mask']
```

---The following area is a Code cell (cell numver is 11)---
```python
# トークナイザーをロードします
llama31_tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_PATH)

# 空のデータフレームを作成
llama31_data = pd.DataFrame()
llama31_data["id"] = test["id"]
# トークナイズした結果をデータフレームに保存します
llama31_data["input_ids"], llama31_data["attention_mask"] = tokenize(llama31_tokenizer, list(test['text']))
# 入力IDの長さを計算します
llama31_data["length"] = llama31_data["input_ids"].apply(len)
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# モデルをロード 
それぞれのGPUに1つのモデルをロードします。
```

---The following area is a Code cell (cell numver is 13)---
```python
# 8ビットの計算に関する設定を定義します
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,
    bnb_8bit_compute_dtype=torch.float16,
    bnb_8bit_use_double_quant=False,
)

device_0 = torch.device('cuda:0')  # 最初のGPUを指定
llama31_model_0 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # 出力ラベルの数
    torch_dtype=torch.float16,  # 16ビット浮動小数点でモデルをロード
    quantization_config=bnb_config,
    use_cache=False,  # キャッシュを無効に
    device_map=device_0  # デバイスの設定
)
# パディングトークンIDを設定します
llama31_model_0.config.pad_token_id = llama31_tokenizer.pad_token_id

device_1 = torch.device('cuda:1')  # 2つ目のGPUを指定
llama31_model_1 = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    use_cache=False,
    device_map=device_1
)
# パディングトークンIDを設定します
llama31_model_1.config.pad_token_id = llama31_tokenizer.pad_token_id

# LoRA設定の作成
lora_config = LoraConfig(
    r=4,
    lora_alpha=8,
    # 自己注意のターゲットモジュールのみ
    target_modules=['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'],
    lora_dropout=0,  # ドロップアウト率を0に設定
    bias='none',  # バイアスを無効に
    task_type=TaskType.SEQ_CLS,  # タスクタイプをシーケンス分類に設定
)

# モデルのトレーニング準備についてのコードはコメントアウトされています
# llama31_model_0 = prepare_model_for_kbit_training(llama31_model_0)
# llama31_model_0 = get_peft_model(llama31_model_0, lora_config)
# llama31_model_0.print_trainable_parameters()

# llama31_model_1 = prepare_model_for_kbit_training(llama31_model_1)
# llama31_model_1 = get_peft_model(llama31_model_1, lora_config)
# llama31_model_1.print_trainable_parameters()
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 重みのロード
```

---The following area is a Code cell (cell numver is 15)---
```python
# PEFTを取得します
llama31_model_0 = PeftModel.from_pretrained(llama31_model_0, model_id=WEIGHTS_PATH).to(device_0)
llama31_model_0.eval()  # 評価モードに設定

llama31_model_1 = PeftModel.from_pretrained(llama31_model_1, model_id=WEIGHTS_PATH).to(device_1)
llama31_model_1.eval();  # 評価モードに設定
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 17)---
```python
@torch.no_grad()  # 勾配を計算しないようにします
@torch.cuda.amp.autocast()  # 自動混合精度を使用します
def inference(df, model, tokenizer, device, batch_size=BATCH_SIZE, max_length=MAX_LENGTH):
    a_win, b_win, tie = [], [], []  # 各モデルの勝者を格納するリスト
    
    # バッチごとにデータを処理
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最も長いシーケンスに合わせてパディング
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchテンソルとして返す
        )
        outputs = model(**inputs.to(device))  # モデルへの入力
        proba = outputs.logits.softmax(-1).cpu()  # ソフトマックス関数を適用して確率を計算
        
        # 各モデルの勝者の確率を収集
        a_win.extend(proba[:, 0].tolist())
        b_win.extend(proba[:, 1].tolist())
        tie.extend(proba[:, 2].tolist())
    
    df["winner_model_a"] = a_win  # 勝者モデルAの確率をデータフレームに追加
    df["winner_model_b"] = b_win  # 勝者モデルBの確率をデータフレームに追加
    df["winner_tie"] = tie  # 引き分けの確率をデータフレームに追加
    
    return df  # データフレームを返す
```

---The following area is a Code cell (cell numver is 18)---
```python
st = time.time()  # 処理開始の時間を記録

# テストデータを長さでソート
llama31_data = llama31_data.sort_values("length", ascending=False).reset_index(drop=True)
sub_1 = llama31_data.iloc[0::2].copy()  # 偶数の行を取得
sub_2 = llama31_data.iloc[1::2].copy()  # 奇数の行を取得

# スレッドプールを使用して推論を並列処理
with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (llama31_model_0, llama31_model_1), (llama31_tokenizer, llama31_tokenizer), (device_0, device_1))

# 結果を結合
result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 各モデルの勝者確率を取得

print(f"経過時間: {time.time() - st}")  # 処理にかかった時間を表示
```

---The following area is a Code cell (cell numver is 19)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝者確率を設定
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝者確率を設定
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分け確率を設定
# 提出用データフレームを作成
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]
# CSVファイルとして保存
submission_df.to_csv('submission.csv', index=False)
# 提出データフレームを表示
display(submission_df)
```

** @@@ Jupyter Notebook numver 72, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、Kaggleのコンペティション「LMSYS - Chatbot Arena」において、ユーザーの好みに基づくチャットボット応答の優劣を予測するための推論プロセスの最適化を目指したものです。具体的には、LLM（大規模言語モデル）であるLlamaを使用し、応答確率を計算する際の推論速度を38%向上させています。

### 取り組んでいる問題
ノートブックは、主に次の問題に焦点を当てています:
1. **推論時間の短縮**: 元のスクリプトの65分から、40分に推論時間を短縮することに成功。
2. **動的パディングの実装**: 入力データの長さに基づいてバッチ内でパディングを動的に適用し、冗長なパディングを減少させています。
3. **入力シーケンスの最大長の設定**: モデルが長い入力に対処できるように、`max_length`を1024から2048に拡張。

### 使用している手法・ライブラリ
- **フレームワーク**: PyTorchが使用され、特にGPUリソースを活用したモデルの推論が行われています。
- **Transformersライブラリ**: Hugging FaceのTransformersライブラリを利用してLLMを読み込み、トークナイズ、モデルの推論を実施。
- **テストタイム拡張（TTA）**: モデルの出力を改善するために、異なる応答の順序を入れ替える手法も試されていますが、効果は限定的でした。
- **メモリ効率の良い注意機構**: メモリ使用量を削減するために、Memory-Efficient Attentionを有効化しています。

### 実装の流れ
1. **データの準備**: テストデータをトークナイズし、整形します。
2. **モデルの設定と読み込み**: 複数のGPUにモデルを読み込み、LoRA（Low-Rank Adaptation）による微調整を設定します。
3. **推論プロセス**: データフレームをバッチ処理し、各モデルによる出力を計算し、結果を結合して最終的な確率を算出します。
4. **結果の保存**: モデルによる勝者の確率を提出用CSVファイルに保存します。

このノートブックは、チャットボットの応答を評価するための効率的な推論手法の実践的なケーススタディを示しており、その中で最善のプラクティスが紹介されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 🦙🦙🦙 このノートブックの内容
このノートブックは、@kishanvavdaraによる[Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)を基に作成されています。リンク先のノートブックをまだ確認していない場合は、ぜひチェックして評価をお願いします。
私は、@kishanvavdaraの作業をもとにいくつかの改善を加えました：

### 38%速い推論
このスクリプトを使って、トレーニングセットの最初の10,000サンプルに対する推論時間は40分で、一方で元のスクリプトは65分かかるため、精度に影響を与えずに38%速くなっています。主に次の2つを追加しました：

#### 1. 動的パディング
すべての入力を事前に固定長にパディングする代わりに、各ミニバッチ内の最長シーケンスまで動的にパディングが適用されます。

#### 2. テストデータを入力長でソート
動的パディングの利点を最大限に活かすため、テストデータは入力長でソートされます。これにより、各ミニバッチ内の入力がほぼ同じ長さになり、冗長なパディングを減らすことができます。

### より長い入力シーケンス
訓練データの99%は1024以内に収まっていますが、残りの1%は収まっていません。さらに、テストセットにはより長いシーケンスが含まれる可能性があるため、`max_length`は可能な限り長く設定する方が安全だと思います。
`max_length`を1024から1280に変更することでLBが0.989から0.983に改善されました。

## 試してみたが効果がなかったこと

### テストタイム拡張（TTA）
私は、response_aとresponse_bの順序を入れ替えるシンプルなTTAを試しました。これにより、サンプルごとにモデルが2回呼び出されるため推論時間が2倍になります。
二つのソフトマックス確率を平均化するか、二つのロジットを平均化してからソフトマックス確率を計算できます。両方のアプローチではLBが改善されませんでしたが、ソフトマックスの平均化がより良い結果を示しました。
TTAはサンプルごとにモデルを2回呼び出すため、推論時間が2倍になります。`max_length=1280`とTTAを有効にして提出が完了しましたが、効率的な推論により9時間以内に収まりました。

### 各入力を切り詰める
元の実装では、プロンプト + response_a + response_bという連結シーケンスが切り詰められます。直截的な切り詰めを適用すると、一部の（稀ではありますが）プロンプトが1280トークンを超えるため、モデルは勝者をランダムに推測するしかなくなります。
私は、最初に各入力を固定長に切り詰めてから、三つを連結しようとしましたが、LBは改善されませんでした。

## 🆕 バージョン4の更新
効果的な推論のおかげで入力シーケンスの長さを増やす時間が十分にあるため、`max_length`を2048に変更しました。また、ミニバッチサイズは8から4に減少させました。
さらに、メモリ使用量を減らすために[Memory-Efficient Attention](https://github.com/facebookresearch/xformers)を有効にしました。
これによりLBが0.983から0.979に改善され、提出はTTAなしで4時間未満で済みました。
ミニバッチサイズを1に減らすことでさらに長くできますが、まだテストしていません。

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
PyTorchの[ドキュメント](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html?highlight=scaled_dot_product#torch.nn.functional.scaled_dot_product_attention)によると、`scaled_dot_product_attention`は、自動的に最も最適な実装を選択します：
1. Flash Attention
2. メモリ効率の良い注意機構
3. PyTorchの（ナイーブな）実装

デフォルトでは、これらすべてが有効ですが、特定のバックエンドを手動で有効化/無効化することもできます。
```

---The following area is a Code cell (cell numver is 5)---
```python
assert torch.cuda.device_count() == 2, "申し訳ありませんが、マルチGPUが必要です！"
torch.backends.cuda.enable_mem_efficient_sdp(True)
torch.backends.cuda.enable_flash_sdp(True)  # Flash AttentionはT4/P100をサポートしていないため、効果はありません。
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルの名前
    weights_path = '/kaggle/input/lmsys-model/model'  # 重みのパス
    max_length = 2048  # 最大長
    batch_size = 4  # バッチサイズ
    device = torch.device("cuda")  # 使用するデバイス
    tta = False  # テストタイム拡張。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # max_length//3を各入力に適用するか、連結入力にmax_lengthを適用するか

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# リスト内の文字列を連結する関数
def process(input_str):
    stripped_str = input_str.strip('[]')  # 文字列の先頭と末尾の角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文字列を分割してトリミング
    return  ' '.join(sentences)  # 文を結合して返す

test.loc[:, 'prompt'] = test['prompt'].apply(process)  # 'prompt'列を処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # 'response_a'列を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # 'response_b'列を処理

display(test.head(5))  # 上位5行を表示
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 10)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["User prompt: " + p for p in prompt]  # ユーザーのプロンプトを整形
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]  # Model Aの応答を整形
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]  # Model Bの応答を整形
    if spread_max_length:
        # max_lengthを各入力に均等に分配する場合
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 入力IDの生成
        attention_mask = [[1]* len(i) for i in input_ids]  # 注意マスクの生成
    else:
        # max_lengthを全体の入力に適用する場合
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイザでトークナイズ
        input_ids = tokenized.input_ids  # 入力IDを取得
        attention_mask = tokenized.attention_mask  # 注意マスクを取得
    return input_ids, attention_mask  # 入力IDと注意マスクを返す
```

---The following area is a Code cell (cell numver is 11)---
```python
%%time

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーを読み込み

data = pd.DataFrame()
data["id"] = test["id"]  # ID列を作成
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークナイズを適用
data["length"] = data["input_ids"].apply(len)  # 入力IDの長さを計算

aug_data = pd.DataFrame()
aug_data["id"] = test["id"]  # ID列を作成
# response_aとresponse_bを入れ替える
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)  # 入力IDの長さを計算
```

---The following area is a Code cell (cell numver is 12)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # 最初の入力IDをデコードして表示
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  # 最初の入れ替えた入力IDをデコードして表示
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# モデルの読み込み 
各GPUに1モデルを読み込みます。
```

---The following area is a Code cell (cell numver is 15)---
```python
# BitsAndBytesの設定
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,  # モデルを8ビットで読み込む設定
    bnb_8bit_compute_dtype=torch.float16,  # 計算のデータ型をfloat16に設定
    bnb_8bit_use_double_quant=False,  # 二重量子化を無効にする設定
)

# GPU 0にベースモデルを読み込む
device_0 = torch.device('cuda:0')
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:0')  # GPU 0に割り当て
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定

# GPU 1にベースモデルを読み込む
device_1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,
    num_labels=3,
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,  # 量子化設定を適用
    device_map='cuda:1')  # GPU 1に割り当て
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定
```

---The following area is a Markdown cell (cell numver is 16)---
```markdown
# 重みの読み込み
```

---The following area is a Code cell (cell numver is 17)---
```python
# LoRAの設定
peft_config = LoraConfig(
    r=16,  # 縮小次元
    lora_alpha=32,  # LoRAのスケーリング係数
    lora_dropout=0.10,  # ドロップアウト率
    bias='none',  # バイアスの設定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクの種類
    target_modules=['o_proj', 'v_proj']  # 対象モジュール
)
```

---The following area is a Code cell (cell numver is 18)---
```python
# PEFTの取得
model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # PEFTモデルを取得してGPU 0に配置
# 重みを読み込む
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)
model_0.eval()  # 評価モードに設定

model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # PEFTモデルを取得してGPU 1に配置
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを読み込む
model_1.eval()  # 評価モードに設定
```

---The following area is a Code cell (cell numver is 19)---
```python
# 学習可能なパラメータ
model_0.print_trainable_parameters()  # モデル0の学習可能なパラメータを表示
model_1.print_trainable_parameters()  # モデル1の学習可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 21)---
```python
@torch.no_grad()
@torch.cuda.amp.autocast()
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝ちと引き分けのカウントを初期化
    
    # バッチサイズに基づいてデータを分割して推論を行う
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを決定
        tmp = df.iloc[start_idx:end_idx]  # データフレームのサブセットを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換
        attention_mask = tmp["attention_mask"].to_list()  # 注意マスクをリストに変換
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",  # 最長の長さでパディング
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchテンソルとして返す
        )
        outputs = model(**inputs.to(device))  # モデルに入力を渡して出力を計算
        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックス関数で処理
        
        a_win.extend(proba[:, 0].tolist())  # Model Aの勝ちの確率を追加
        b_win.extend(proba[:, 1].tolist())  # Model Bの勝ちの確率を追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率を追加
    
    df["winner_model_a"] = a_win  # データフレームにModel Aの勝ちの確率を追加
    df["winner_model_b"] = b_win  # データフレームにModel Bの勝ちの確率を追加
    df["winner_tie"] = tie  # データフレームに引き分けの確率を追加
    
    return df  # 結果のデータフレームを返す
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

# 动的パディングを最大限に活用するために入力長でソート
data = data.sort_values("length", ascending=False)
# sub_1とsub_2のトークン数がほぼ同じである必要があります
sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー
sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー

with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝者の確率を取得

print(f"経過時間: {time.time() - st}")  # 経過時間を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
st = time.time()

if cfg.tta:
    data = aug_data.sort_values("length", ascending=False)  # 入力長でソートして速度を向上させる
    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをコピー
    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをコピー

    with ThreadPoolExecutor(max_workers=2) as executor:  # スレッドプールを使って並列処理
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 並列に推論を実行

    tta_result_df = pd.concat(list(results), axis=0)  # 結果を結合
    # TTAの結果は順序が逆であることに注意
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values  # TTAの確率を取得
    # 元の結果とTTAの結果を平均化する
    proba = (proba + tta_proba) / 2  # 確率を平均化

print(f"経過時間: {time.time() - st}")  # 経過時間を表示
```

---The following area is a Code cell (cell numver is 24)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # データフレームにModel Aの勝ちの確率を追加
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # データフレームにModel Bの勝ちの確率を追加
result_df.loc[:, "winner_tie"] = proba[:, 2]  # データフレームに引き分けの確率を追加
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用データフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルに書き出し
display(submission_df)  # 提出データを表示
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
---

# コメント 

> ## Crystal Veil
> 
> 優れたLLMファインチューニングコード
> 
> 

---

> ## Zac Wing
> 
> 印象的な作業です！
> 
> 

---

> ## toolman
> 
> ありがとう、素晴らしい作品です
> 
> 

---

> ## Matt McDonagh
> 
> 動的パディングがあることすら知らなかった。
> 
> 新しいトリックを学ぶことが好きです。大きなものもあれば、小さな工夫もあります。それらはすべて私たちをより良くします。
> 
> この作業を共有してくれてありがとう。
> 
> 

---

> ## Kishan Vavdara
> 
> 素晴らしい仕事[@emiz6413](https://www.kaggle.com/emiz6413)！ありがとうございます、これが必要でした！
> 
> 

---

> ## jiangli59
> 
> メモリ効率の良い注意機構を有効にする方法について教えてもらえますか？
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私が行ったのは、以下のコードを呼び出してメモリ効率の良い注意機構を有効にするためのグローバルフラグを設定することでした。
> > 
> > ```
> > torch.backends.cuda.enable_mem_efficient_sdp(True)
> > 
> > ```
> > 
> > 注意してください、mem_efficient_sdpはデフォルトで有効になっているので、これは冗長です。
> > 
> > 

---

> ## Lorry Zou
> 
> 素晴らしい仕事[@emiz6413](https://www.kaggle.com/emiz6413)！モデルのトレーニングについて少し共有してもらえますか？Kaggleでトレーニングしましたか、それとも別のプラットフォームで？どれくらいの時間がかかり、いくつかのハイパーパラメータの工夫についても教えていただけますか？ありがとうございます！
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > [@kishanvavdara](https://www.kaggle.com/kishanvavdara)の別の[ノートブック](https://www.kaggle.com/code/kishanvavdara/lmsys-llama-3-tpu-train)でモデルをトレーニングしました。50%のトレーニングデータを使用し、Kaggle TPUを使って約2時間かかります。
> > 
> > 
> > 
> > > ## Lorry Zou
> > > 
> > > お返事ありがとうございます！つまり、ファインチューニングせずに[@kishanvavdara](https://www.kaggle.com/kishanvavdara)のモデルの重みを使用して推論したのですか？
> > > 
> > > 
> > > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > はい、その通りです。
> > > 
> > > 
> > > 

---

> ## bao
> 
> 共有してくれてありがとう。質問がありますが、なぜトークナイザを呼び出すときにpadding=Trueを使用せず、pad_without_fast_tokenizer_warningを使用してパディングするのですか？
> 
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > それが動的パディングを行っている部分です。
> > 
> >
```

** @@@ Jupyter Notebook numver 73, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおいて、ユーザーのプロンプトに対する2つの異なる応答のいずれが好まれるかを予測する問題に取り組んでいます。具体的には、機械学習モデルを用いて、人工知能チャットボットの応答の選好を学習します。

### 問題の概要
Notebook内での主な目的は、与えられたプロンプトに対して2つの応答モデル（応答Aと応答B）のいずれが好まれるかの確率を予測することです。予測は、訓練データセットとテストデータセットを使用して行われます。

### 手法とライブラリ
この問題を解決するために、以下の手法とライブラリが使用されています：

1. **ライブラリ**
   - **PyTorch**: 深層学習フレームワークで、モデルの構築と訓練に使用されています。
   - **Transformers**: 事前訓練済みの言語モデルを取得するためのライブラリで、チャットボットの応答生成に利用されています。
   - **Pandas**: データ操作と処理のためのライブラリで、データセットの読み込みや前処理に使用されています。
   - **NumPy**: 数値計算を補助するためのライブラリで、ベクトル計算などに活用されています。

2. **モデルアーキテクチャ**
   - **EmbeddingModel**: プロンプトと応答を埋め込みベクトルに変換し、それらのベクトルを統合するためのクラス。
   - **Model**: 埋め込み出力をLSTMとCNNで処理し、その後全結合層を通してクラス（勝者モデル）を出力するニューラルネットワークの定義。
   - **Datasetクラス**: データのサンプリングを行うためのカスタムデータセットクラス。

3. **トレーニングプロセス**
   - データセットの読み込み後、トレーニングデータをDataLoaderを用いてバッチ処理し、モデルの訓練を行います。
   - 損失関数には交差エントロピー損失が使用され、Adamオプティマイザーでモデルのパラメータの更新を行います。
   - トレーニングはエポック数に基づいて繰り返され、各エポックの損失と精度が記録されます。

4. **テストと評価**
   - 学習済みモデルを用いてテストデータに対する予測を実施し、予測結果をデータフレームにまとめて出力します。

このNotebookは、機械学習アルゴリズムを用いて言語モデルの応答に対するユーザーの好みを科学的に予測し、人間とAIのインタラクションを改善するための重要なステップを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# tqdmをインポートします。これはプログレスバーを提供するライブラリです。
from tqdm import tqdm
# データ処理のためにpandasをインポートします。
import pandas as pd
# JSONファイルを読み込むためのjsonライブラリをインポートします。
import json
# PyTorchライブラリをインポートします。深層学習を行うためのフレームワークです。
import torch
# 事前訓練済みのモデルを取り扱うためにtransformersライブラリからAutoModelをインポートします。
from transformers import AutoModel
# numpyの線形代数機能からnorm関数をインポートします。ベクトルのノルム計算に使用します。
from numpy.linalg import norm
# PyTorchのニューラルネットワークモジュールをインポートします。
import torch.nn as nn
# numpyをインポートします。数値計算に使用されるライブラリです。
import numpy as np
# PyTorchのデータセットおよびデータローダーをインポートします。データを扱うための便利なツールです。
from torch.utils.data import Dataset, DataLoader
# PyTorchの関数型APIをインポートします。ニューラルネットワークの活性化関数などに使用されます。
import torch.nn.functional as F
# 正規表現を扱うためのreライブラリをインポートします。
import re
```

---The following area is a Code cell (cell numver is 2)---
```python
# 使用可能なデバイスを確認し、CUDA（GPU）が利用可能であればそれを使用します。
# そうでない場合は、CPUを使用します。
device = "cuda" if torch.cuda.is_available() else "cpu"
# 現在のデバイス（GPUまたはCPU）を表示します。
device
```

---The following area is a Code cell (cell numver is 3)---
```python
# 事前訓練済みのモデルを指定したパスからロードします。
# AutoModelを使用して、指定されたディレクトリのモデルを取得し、トークナイザーとして設定します。
tokenizer = AutoModel.from_pretrained('/kaggle/input/jinaai/pytorch/default/4')
```

---The following area is a Code cell (cell numver is 4)---
```python
# 埋め込みモデルを定義するクラス
class EmbeddingModel(nn.Module):
    def __init__(self, embedding_model, max_sequences):
        super(EmbeddingModel, self).__init__()
        self.embedding = embedding_model  # 埋め込みモデルを指定
        self.max_seq_length = max_sequences  # 最大シーケンス長を設定
        self.device = device  # 使用するデバイスを指定

    def forward(self, prompts, responses_a, responses_b):
        batch_features_a = []  # 応答Aの特徴を格納するリスト
        batch_features_b = []  # 応答Bの特徴を格納するリスト

        # 各プロンプトと応答を処理するループ
        for prompt, response_a, response_b in zip(prompts, responses_a, responses_b):
            prompt = json.loads(prompt)  # プロンプトをJSONから読み込む
            response_a = json.loads(response_a)  # 応答AをJSONから読み込む
            response_b = json.loads(response_b)  # 応答BをJSONから読み込む
            
            prompt = ["" if p is None else p for p in prompt]  # Noneの場合は空文字に変換
            response_a = ["" if r is None else r for r in response_a]  # Noneの場合は空文字に変換
            response_b = ["" if r is None else r for r in response_b]  # Noneの場合は空文字に変換
            
            # プロンプトを埋め込む
            embedded_prompt = torch.from_numpy(self.embedding.encode(prompt)).to(self.device)
           
            # 応答Aを埋め込む
            embedded_response_a = torch.from_numpy(self.embedding.encode(response_a)).to(self.device)
            # 応答Bを埋め込む
            embedded_response_b = torch.from_numpy(self.embedding.encode(response_b)).to(self.device)

            features_a = []  # 応答Aの特徴を格納するリスト
            features_b = []  # 応答Bの特徴を格納するリスト
            # 各埋め込みを結合して特徴を作成
            for i in range(len(embedded_prompt)):
                combined_a = torch.cat((embedded_prompt[i], embedded_response_a[i]), dim=0)
                combined_b = torch.cat((embedded_prompt[i], embedded_response_b[i]), dim=0)

                features_a.append(combined_a)  # 応答Aの特徴を追加
                features_b.append(combined_b)  # 応答Bの特徴を追加

            features_a = torch.stack(features_a) if features_a else torch.tensor([]).to(self.device)
            features_b = torch.stack(features_b) if features_b else torch.tensor([]).to(self.device)

            # 特徴を指定の形状にパディングする
            features_a = self.pad_to_shape(features_a, (self.max_seq_length, 768 * 2))
            features_b = self.pad_to_shape(features_b, (self.max_seq_length, 768 * 2))

            batch_features_a.append(features_a)  # バッチに応答Aの特徴を追加
            batch_features_b.append(features_b)  # バッチに応答Bの特徴を追加

        return torch.stack(batch_features_a).to(self.device), torch.stack(batch_features_b).to(self.device)

    def pad_to_shape(self, tensor, shape):
        current_shape = tensor.shape  # 現在のテンソルの形状を取得
        padding = [(0, max(s - cs, 0)) for cs, s in zip(current_shape, shape)]  # パディングサイズを計算
        # テンソルにパディングを適用
        padded_tensor = F.pad(tensor, pad=[p for pair in reversed(padding) for p in pair], mode='constant', value=0)
        return padded_tensor[:shape[0], :shape[1]]  # 指定した形状に合わせてトリミング

# 全体のモデルを定義するクラス
class Model(nn.Module):
    def __init__(self, embedding_model, max_sequences=64, hidden_dim=512, dropout=0.3):
        super(Model, self).__init__()
        self.device = device  # デバイスを指定
        self.embedding = EmbeddingModel(embedding_model, max_sequences)  # 埋め込みモデルを作成
        # LSTMレイヤーを初期化
        self.lstm_input_a = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)
        self.lstm_input_b = nn.LSTM(768 * 2, hidden_dim, batch_first=True).to(self.device)

        # 畳み込みレイヤーを初期化
        self.conv_input_a = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)
        self.conv_input_b = nn.Conv1d(768 * 2, hidden_dim, kernel_size=3, padding=1).to(self.device)

        # 全結合層の定義
        self.fc = nn.Sequential(
            nn.Linear(hidden_dim * 2 + hidden_dim * 2, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 3),  # 出力は3クラス
            nn.Softmax()
        ).to(self.device)

    def forward(self, prompts, responses_a, responses_b):
        batch_a, batch_b = self.embedding(prompts, responses_a, responses_b)  # 埋め込みを計算

        # LSTMを通す
        batch_a_lstm, _ = self.lstm_input_a(batch_a)  # (バッチ, 64, hidden_dim)
        batch_b_lstm, _ = self.lstm_input_b(batch_b)  # (バッチ, 64, hidden_dim)

        # 畳み込み層を通す
        batch_a_cnn = self.conv_input_a(batch_a.permute(0, 2, 1)).permute(0, 2, 1)  # (バッチ, 64, hidden_dim)
        batch_b_cnn = self.conv_input_b(batch_b.permute(0, 2, 1)).permute(0, 2, 1)  # (バッチ, 64, hidden_dim)

        # LSTMの最終出力を取得
        batch_a_lstm = batch_a_lstm[:, -1, :] 
        batch_b_lstm = batch_b_lstm[:, -1, :]  
        # 畳み込みの最終出力を取得
        batch_a_cnn = batch_a_cnn[:, -1, :]    
        batch_b_cnn = batch_b_cnn[:, -1, :]
        
        # 特徴を結合
        combined = torch.cat([batch_a_lstm, batch_a_cnn, batch_b_lstm, batch_b_cnn], dim=1)
        flattened = combined.view(combined.size(0), -1)  # 転置して線形層に適した形状にする

        output = self.fc(flattened)  # 全結合層を通す
        return output  # 出力を返す

# 学習データセット用のクラス
class DatasetLMSYS(Dataset):
    def __init__(self, data):
        self.data = data  # データを保存

    def __len__(self):
        return len(self.data)  # データの長さを返す

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]  # データフレームからサンプルを取得
        prompt = sample['prompt']  # プロンプトを取得
        response_a = sample['response_a']  # 応答Aを取得
        response_b = sample['response_b']  # 応答Bを取得
        label = sample['model_result']  # ラベルを取得
        return prompt, response_a, response_b, label  # プロンプト、応答A、応答B、およびラベルを返す
    
# テストデータセット用のクラス
class DatasetLMSYSTest(Dataset):
    def __init__(self, data):
        self.data = data  # データを保存

    def __len__(self):
        return len(self.data)  # データの長さを返す

    def __getitem__(self, idx):
        sample = self.data.iloc[idx]  # データフレームからサンプルを取得
        _id = sample['id']  # IDを取得
        _prompt = sample['prompt']  # プロンプトを取得
        _response_a = sample['response_a']  # 応答Aを取得
        _response_b = sample['response_b']  # 応答Bを取得
        return _id, _prompt, _response_a, _response_b  # ID、プロンプト、応答A、および応答Bを返す
```

---The following area is a Code cell (cell numver is 5)---
```python
# 定義したモデルをインスタンス化します。
# 事前訓練済みのトークナイザーを渡し、指定したデバイス（GPUまたはCPU）に移動させます。
model = Model(tokenizer).to(device)
```

---The following area is a Code cell (cell numver is 6)---
```python
# ミニバッチのサイズを設定します。これは一度に処理するデータの数です。
batch_size = 128
# 学習率を設定します。これはモデルの重みの更新に使用されるステップサイズです。
learning_rate = 0.001
# エポック数を設定します。エポックとは、全データセットを1回学習させることを指します。
epochs = 5
```

---The following area is a Code cell (cell numver is 7)---
```python
# トレーニングデータをCSVファイルから読み込みます。
file_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
# モデルの結果を計算します。winner_model_aが1なら0、winner_model_bが1なら1、どちらでもなければ2を設定します。
file_data['model_result'] = file_data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)
# 使用するカラムを指定します。プロンプト、応答A、応答B、モデルの結果のみを残します。
file_data = file_data[['prompt', 'response_a', 'response_b', 'model_result']]
# データローダーを作成します。これによりミニバッチでデータを効率的に処理できます。
train_loader = DataLoader(
    dataset=DatasetLMSYS(file_data),  # 自作のDatasetを使用
    batch_size=batch_size,  # バッチサイズを指定
    shuffle=True  # データをランダムにシャッフルします
)

# テストデータをCSVファイルから読み込みます。
file_test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
# テストデータ用のデータローダーを作成します。
test_loader = DataLoader(
    dataset=DatasetLMSYSTest(file_test),  # 自作のテストDatasetを使用
    batch_size=batch_size,  # バッチサイズを指定
    shuffle=False  # テストデータはシャッフルしません
)
```

---The following area is a Code cell (cell numver is 8)---
```python
# 損失関数を設定します。ここでは交差エントロピー損失を使用します。
criterion = nn.CrossEntropyLoss()
# 最適化手法としてAdamを選択し、モデルのパラメータに対して学習率を設定します。
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```

---The following area is a Code cell (cell numver is 9)---
```python
# エポック数分ループします。各エポックは1回の学習サイクルを示します。
for epoch in range(epochs):
    print(f"Epoch {epoch + 1}/{epochs}")  # 現在のエポック数を表示
    running_loss = 0  # 現在の損失の累積値
    total_train = 0  # 総トレーニングサンプル数
    correct_train = 0  # 正しく分類されたサンプルの数

    # モデルをトレーニングモードに設定
    model.train()
    # トレーニングデータローダーからバッチを取得
    for batch in tqdm(train_loader):
        prompts, responses_a, responses_b, labels = batch  # バッチからデータを取得
        labels = labels.to(device)  # ラベルをデバイスに移動

        # モデルの出力を計算
        outputs = model(prompts, responses_a, responses_b)
        # 最大値を持つインデックスを取得
        _, predicted_idx = torch.max(outputs.data, 1)

        # 損失を計算
        loss = criterion(outputs, labels)
        # 勾配をゼロに設定
        optimizer.zero_grad()

        # 損失に基づいて勾配を計算
        loss.backward()
        # パラメータを更新
        optimizer.step()

        # 損失の累積値を更新
        running_loss += loss.item()
        total_train += labels.size(0)  # トレーニングサンプル数を増加
        correct_train += (predicted_idx == labels).sum().item()  # 正解をカウント

        # メモリ管理のためにラベルと出力を削除
        del labels, outputs
        
    # トレーニング精度を計算
    train_accuracy = 100 * correct_train / total_train
    # トレーニング損失と精度を表示
    print(f"\nTraining Loss: {running_loss/len(train_loader):.4f} | Training Accuracy: {train_accuracy:.2f}%")
print("\n==> Training finished!")  # トレーニング終了のメッセージを表示
```

---The following area is a Code cell (cell numver is 10)---
```python
# モデルのテストを行う関数を定義します。
def test(model, test_loader, device):
    model.eval()  # モデルを評価モードに設定
    results = []  # 結果を格納するリスト
    with torch.no_grad():  # 勾配計算を無効にしてメモリを節約
        # テストデータローダーからバッチを取得
        for batch in tqdm(test_loader):
            ids, prompts, responses_a, responses_b = batch  # バッチからデータを取得
            # モデルの出力を計算
            outputs = model(prompts, responses_a, responses_b)
            # 最大値のインデックスを取得
            _, predicted_idx = torch.max(outputs.data, 1)
            
            # 結果をリストに追加
            for idx, output, prediction in zip(ids, outputs, predicted_idx):
                results.append({
                    'id': idx.item(),  # IDを追加
                    'winner_model_a': output[0].item(),  # モデルAの勝者確率を追加
                    'winner_model_b': output[1].item(),  # モデルBの勝者確率を追加
                    'winner_tie': output[2].item()  # 引き分けの確率を追加
                })
    df_results = pd.DataFrame(results)  # 結果をデータフレームに変換
    return df_results  # データフレームを返す

# テスト関数を呼び出し、結果を取得
df_results = test(model, test_loader, device)
# 結果を表示
df_results
```

** @@@ Jupyter Notebook numver 74, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、「LMSYS Keras Gemma 2B」というタイトルで、LMSYS - Chatbot Arenaのコンペティションにおけるチャットボット応答の評価問題に取り組んでいます。具体的には、与えられたユーザーのプロンプトに対して複数の応答モデルから勝者を予測するためのモデルを構築およびトレーニングしています。

### 実装手法:
1. **ライブラリと環境の設定:**
   - `keras-nlp`および`tensorflow-text`を使用して自然言語処理のためのKeras機能を強化し、`tensorflow-cpu`をインストールしてTPUに悪影響を与えないよう設定しています。
   - JAXライブラリを利用し、高速な数値計算が可能な環境を整えています。

2. **デバイスの管理:**
   - KerasのディストリビューションAPIを使用して、TPUリソースを効率的に利用するためのデバイスメッシュを設定し、モデルの重みを8つのTPUに分散させています。

3. **データの前処理:**
   - 入力プロンプトとレスポンスのペアをCSVファイルから読み込み、適切に整形して新しいデータフレームを作成しています。このデータには、応答の勝利モデル情報も含まれています。
   - テキスト内の非表示文字（サロゲートペア）を取り除く関数を定義し、データクレンジングを行っています。

4. **モデルの構築:**
   - Kerasの`GemmaCausalLM`モデルを利用して、因果モデルをベースとした構造を設計しています。そして、独自の全結合層を追加して、最終的な出力を3クラスのsoftmax出力にセットしています。
   - LoRA（Low-Rank Adaptation）を使用してモデルの知識を効率的に適応させ、計算資源の無駄を減らしています。

5. **モデルのコンパイルとトレーニング:**
   - AdamWオプティマイザを利用し、学習率と重みの減衰を設定してモデルをコンパイルしています。
   - TensorFlowのデータセットAPIを利用してトレーニングデータを準備し、モデルを1エポック訓練しています。

6. **重みの保存:**
   - トレーニング後、LoRAで適応したモデルの重みを保存して、後で再利用できるようにしています。

このノートブックは、KerasとTensorFlowを利用して、複数の応答モデル間の優劣を予測するための効率的な機械学習モデルを構築するための枠組みを提供しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# LMSYS Keras Gemma 2B
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U keras-nlp tensorflow-text
# keras-nlp と tensorflow-text をアップグレードしてインストールします。
# これにより、NLP用のKerasライブラリとテキスト処理用のTensorFlowライブラリが使用可能になります。

!pip install -q -U tensorflow-cpu
# tensorflow-cpu をインストールします。
# これにより、TensorFlow がTPUにアクセスしようとしないようにします。
# TPU は特別なハードウェアで、通常のCPUの代わりに使われることがあるため、ここではCPU版を指定します。
```

---The following area is a Code cell (cell numver is 3)---
```python
import jax

# JAXライブラリをインポートします。
# JAXは、高速な数値計算や自動微分を提供するライブラリです。

jax.devices()
# 使用可能なデバイスを表示します。
# この関数は、JAXが使用できるCPUやGPU、TPUなどのデバイスをリストします。
```

---The following area is a Code cell (cell numver is 4)---
```python
import os

# Keras 3のディストリビューションAPIは、現時点ではJAXバックエンドでのみ実装されています。
os.environ["KERAS_BACKEND"] = "jax"
# TPUのメモリを事前に確保して、メモリの断片化と割り当てのオーバーヘッドを最小化します。
os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
# XLA（Accelerated Linear Algebra）Pythonクライアントのメモリ使用率を1.0に設定し、
# すべてのTPUメモリを確保します。これにより、メモリの効率的な使用が促進されます。
```

---The following area is a Code cell (cell numver is 5)---
```python
import keras
import keras_nlp

# Kerasライブラリをインポートします。
# Kerasは、深層学習モデルを簡単に構築・トレーニングできる高水準なAPIです。

import keras_nlp
# keras_nlpライブラリをインポートします。
# keras_nlpは、自然言語処理（NLP）タスクに特化したKerasの拡張機能であり、
# モデルの構築やデータ処理を支援するための豊富なツールやモジュールを提供します。
```

---The following area is a Code cell (cell numver is 6)---
```python
# (1, 8) の形状のデバイスメッシュを作成し、すべての8つのTPUに重みを分散させます。
device_mesh = keras.distribution.DeviceMesh(
    (8, 1),  # デバイスメッシュの形状を指定します。ここでは8つのTPUを使います。
    ["batch", "model"],  # バッチ次元とモデル次元を指定します。
    devices=keras.distribution.list_devices(),  # 使用可能なデバイスのリストを取得します。
) 
# このデバイスメッシュは、TPUのリソースを効果的に利用するために、
# モデルの重みを複数のTPUにまたがって分散させるために使用されます。
```

---The following area is a Code cell (cell numver is 7)---
```python
model_dim = "model"

layout_map = keras.distribution.LayoutMap(device_mesh)

# 'token_embedding/embeddings'に一致する重みは、8つのTPUに分割されます。
layout_map["token_embedding/embeddings"] = (model_dim, None)

# Attentionレイヤー内のクエリ、キー、値の行列に対して一致する正規表現
layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)

# Attention出力のカーネルに対するレイアウトマップの設定
layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)

# フィードフォワードゲーティングのカーネルに対するレイアウトマップの設定
layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)

# フィードフォワード線形変換のカーネルに対するレイアウトマップの設定
layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)

# このレイアウトマップは、モデルの重みがTPU間で適切に分散されるように配置を設定します。
# 各重みに対して適切なデバイスを指定することで、計算効率を向上させます。
```

---The following area is a Code cell (cell numver is 8)---
```python
def remove_surrogates(text):
    # テキスト内のサロゲートペア（Unicodeの範囲0xD800から0xDFFF）を削除する関数です。
    return ''.join(char for char in text if not (0xD800 <= ord(char) <= 0xDFFF))
    # 上記の条件に基づいて、サロゲートペアではない文字のみを結合して新しい文字列を作成し、
    # サロゲートペアを排除したテキストを返します。
```

---The following area is a Code cell (cell numver is 9)---
```python
from pandas import read_csv, DataFrame

input_columns = ['prompt', 'response_a', 'response_b']
label_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']

# CSVファイルから生のトレーニングデータセットを読み込みます。
raw_train_dataset = read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')

# nullでない場合にevalを使って最初の要素を取得し、入力カラムに適用します。
raw_train_dataset[input_columns] = raw_train_dataset[input_columns].map(lambda x: eval(x)[0] if 'null' not in x else None)

# 欠損値を持つ行を削除し、不要なカラム（'model_a', 'model_b'）を削除してインデックスをリセットします。
raw_train_dataset = raw_train_dataset.dropna().drop(['model_a', 'model_b'], axis=1).reset_index(drop=True)

# 新しいデータフレームを作成し、テキストとラベルを設定します。
train_dataset = DataFrame({
    'text' : raw_train_dataset[input_columns].apply(lambda x: '<start_of_turn>user\nFind which one is the best answer for the question:\n'+x['prompt']+'\n\nA:\n'+x['response_a']+'\n\nB\n:'+x['response_b']+'\n\nC:\n both right (or) both wrong<end_of_turn>\n<start_of_turn>model\n', axis=1).apply(lambda x: remove_surrogates(x)),
    # テキスト列には、ユーザーからのプロンプトとモデルの応答が含まれます。
    'label' : raw_train_dataset[label_columns].apply(lambda x: x.values.tolist(), axis=1)
    # ラベル列には、各応答の勝者をリストとして格納します。
    # 'label' : raw_train_dataset[label_columns].apply(lambda x: 'A' if x.values.tolist()[0] == 1 else 'B' if x.values.tolist()[1] == 1 else 'C', axis=1)
})

# データセットを最初の4000行に制限します。
train_dataset = train_dataset[:4000]
raw_train_dataset = raw_train_dataset[:4000]
```

---The following area is a Code cell (cell numver is 10)---
```python
len(train_dataset)
# train_datasetのデータフレームの行数を取得します。
# この行数はトレーニングデータセットに含まれるサンプルの数を示します。
```

---The following area is a Code cell (cell numver is 11)---
```python
model_parallel = keras.distribution.ModelParallel(
    layout_map=layout_map,  # 先に定義したレイアウトマップを使用します。
    batch_dim_name="batch",  # バッチ次元の名前を指定します。
)

# モデルの並列処理の設定を行います。
keras.distribution.set_distribution(model_parallel)
# これにより、モデルがTPUでのトレーニングのために適切に分散されるようになります。
# 同時に複数のTPUを利用してトレーニングが行えるようになります。
```

---The following area is a Code cell (cell numver is 12)---
```python
keras.config.set_floatx("float16")
# Kerasの浮動小数点数の精度をfloat16に設定します。
# これにより、メモリ使用量が削減され、高速な計算が可能になります。

gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("/kaggle/input/gemma/keras/gemma_instruct_2b_en/2")
# GemmaCausalLMモデルを指定されたプレセットから読み込みます。
# このモデルは因果言語モデルであり、自然言語の生成タスクに使用されます。

gemma_lm.summary()
# モデルの概要を表示します。
# これにより、モデルの構造、パラメータの数、レイヤーの情報などが確認できます。
```

---The following area is a Code cell (cell numver is 13)---
```python
gemma_lm.backbone.enable_lora(rank=8)
# GemmaモデルのバックボーンにLoRA（Low-Rank Adaptation）を有効にします。
# rank=8は、LoRAのランクを8に設定することを意味します。
# LoRAは、モデルのパラメータ数を増やさずに、特定のタスクに対するモデルの適応能力を向上させるための手法です。
# これにより、少ない計算リソースで効果的にモデルを微調整できます。
```

---The following area is a Code cell (cell numver is 14)---
```python
# gemma_lm.backbone.layers[:16]の各レイヤーについて、学習可能な状態を無効化します。
# for layer in gemma_lm.backbone.layers[:16]:
#     layer.trainable = False
# 最初の16層の重みを固定し、トレーニング中にこれらのパラメータが更新されないようにします。
# これにより、モデル全体のトレーニング時間を短縮し、過学習を防ぐことができます。
```

---The following area is a Code cell (cell numver is 15)---
```python
gemma_lm.summary()
# モデルの概要を再表示します。
# これにより、モデルの構造、レイヤーの情報、トレーニング可能なパラメータの数などが確認でき、
# 変更後のモデルの状態を確認できます。
```

---The following area is a Code cell (cell numver is 16)---
```python
def preprocess_fn(text, label=None):
    # テキストとラベルを前処理する関数です。
    preprocessed = gemma_lm._preprocessor(text, sequence_length=3072)[0]
    # Gemmaモデルのプリプロセッサを使用してテキストを3072のシーケンス長に前処理します。
    
    # 前処理関数が必要な入力のみを返すようにします。
    return (
        {'token_ids': preprocessed['token_ids'], 'padding_mask': preprocessed['padding_mask']}, 
        label if label is not None else {'token_ids': preprocessed['token_ids'], 'padding_mask': preprocessed['padding_mask']}
    )
    # token_idsとpadding_maskからなる辞書を返します。
    # ラベルがNoneでない場合は、ラベルも一緒に返します。そうでなければ、token_idsとpadding_maskの辞書を返します。
```

---The following area is a Code cell (cell numver is 17)---
```python
gemma_lm.layers[-1]
# Gemmaモデルの最後のレイヤーを取得します。
# これにより、モデルの出力層や最後の層の詳細を確認できます。
# モデルのアーキテクチャを理解するために役立ちます。
```

---The following area is a Code cell (cell numver is 18)---
```python
import gc

# Gemmaモデルの最後のレイヤーを削除します。
del gemma_lm.layers[-1]

# ガーベジコレクションを実行して、不要なメモリを解放します。
gc.collect()
# これにより、削除されたレイヤーに関連するメモリが解放され、
# メモリ使用量を最適化します。
```

---The following area is a Code cell (cell numver is 19)---
```python
import tensorflow as tf
from keras.layers import Input, Dense, Flatten, GlobalAveragePooling1D
from keras import Model

# モデルの入力を定義します。
inputs = {
    "token_ids": keras.Input(shape=(3072,), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(3072,), dtype=tf.int32, name="padding_mask"),
}

# Gemmaモデルのバックボーンを通して入力を処理します。
x = gemma_lm.backbone(inputs)
print(x.shape)  # Gemmaモデルの出力の形状を表示します。

# 出力をグローバル平均プールします。
x = GlobalAveragePooling1D()(x)
print(x.shape)  # プーリング後の形状を表示します。

# 最後に、3クラスのsoftmax出力を持つ全結合層を定義します。
outputs = Dense(3, 'softmax')(x)
# 入力と出力を使ってモデルを構築します。
model = Model(inputs, outputs)
```

---The following area is a Code cell (cell numver is 20)---
```python
optimizer = keras.optimizers.AdamW(
                learning_rate=5e-5,  # 学習率を5e-5に設定します。
                weight_decay=0.01,   # 重みの減衰を0.01に設定します。
)

# 重み減衰を適用しない変数の名前を指定します。
optimizer.exclude_from_weight_decay(var_names=["bias", "scale"])
# これにより、バイアスとスケールに関連するパラメータには重み減衰が適用されません。
# これは、過学習を防ぎつつモデルの学習パフォーマンスを向上させるために行われます。
```

---The following area is a Code cell (cell numver is 21)---
```python
model.compile(optimizer, loss=tf.keras.losses.CategoricalCrossentropy(),)
# モデルをコンパイルします。
# 指定したオプティマイザ（optimizer）を使用し、損失関数としてカテゴリカルクロスエントロピーを設定します。
# カテゴリカルクロスエントロピーは、多クラス分類の問題でよく使用される損失関数であり、
# モデルの出力と教師データのクラスラベルとの間の不一致を測定します。
```

---The following area is a Code cell (cell numver is 22)---
```python
import tensorflow as tf

# トレーニングデータセットからテキストとラベルを含むデータセットを作成します。
ds = tf.data.Dataset.from_tensor_slices((train_dataset.text.values, raw_train_dataset[label_columns].values))\
    .batch(8)  # バッチサイズを8に設定します。
    
# 前処理関数をデータセットに適用します。
ds = ds.map(preprocess_fn)

# データセットをシャッフルします。
ds = ds.shuffle(ds.cardinality())
# cardinality()はデータセットのサンプル数を返します。
# シャッフルにより、トレーニングデータの順序がランダム化され、モデルの学習が改善されることが期待されます。
```

---The following area is a Code cell (cell numver is 23)---
```python
train_split = ds.take(int(len(ds) * 0.9))
# データセットの90%を訓練用分割として取得します。

val_split = ds.skip(int(len(ds) * 0.9)).take(int(len(ds) * 0.1))
# データセットの残りの10%を検証用分割として取得します。
# skip()で90%をスキップし、take()で次の10%を取得します。

# モデルを訓練します。
histories = model.fit(train_split, validation_data=[val_split], epochs=1, batch_size=8)
# 訓練を1エポック行い、バッチサイズを8に設定します。
# validation_data引数を用いて、検証用データを設定し、モデルの性能を評価します。
```

---The following area is a Code cell (cell numver is 24)---
```python
model.get_layer("gemma_backbone").save_lora_weights('/kaggle/working/lora19.lora.h5')
# Gemmaモデルのバックボーンレイヤーに対してLoRA（Low-Rank Adaptation）重みを保存します。
# 指定されたパス（'/kaggle/working/lora19.lora.h5'）に重みをHDF5形式で保存します。
# これにより、後でこの重みを再利用したり、試験したりすることができます。
```

** @@@ Jupyter Notebook numver 75, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Kaggleの「LMSYS - Chatbot Arena」における人間による好み予測の問題に取り組んでいます。目的は、異なる大規模言語モデル（LLM）が生成した応答のどちらがユーザーに好まれるかを予測することです。具体的には、プロンプトとそれに対する応答を基に、各モデルの勝者を予測するための機械学習モデルを構築します。

ノートブックでは、以下の手法やライブラリを使用しています：

1. **データ処理**:
   - `pandas`を利用して、CSVファイルからトレーニングデータとテストデータを読み込みます。
   - テキストデータの前処理として、プロンプトと応答を結合し、新たな特徴量（text_a, text_b）を生成します。

2. **特徴抽出**:
   - `TfidfVectorizer`を使って、テキストデータをベクトル化します。これにより、テキストの重要度に基づく数値表現が得られ、機械学習モデルの入力として使用されます。
   
3. **データ分割とスケーリング**:
   - `train_test_split`を使って、データセットを訓練用と検証用に分割します。
   - 特徴量のスケーリングには`StandardScaler`を使用し、データの標準化を行います。

4. **モデル構築**:
   - `tensorflow`と`Keras`を用いて、シンプルなニューラルネットワークモデルを構築します。モデルは3つの出力を持ち、それぞれモデルA、モデルB、そしてタイの勝者の確率を予測します。

5. **モデル訓練と評価**:
   - モデルは10エポックで訓練され、バリデーションデータを利用した評価として対数損失（log loss）を計算します。

6. **予測と提出ファイル作成**:
   - テストデータに対する予測を行い、その結果を基に提出用のCSVファイル（submission.csv）を生成します。

最終的に、このノートブックは、異なる応答からどのモデルが好まれるかを予測し、その結果をKaggleに提出するための全体的なフローを提供しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 以下は、いくつかの便利なパッケージの読み込み例です

import numpy as np # 線形代数用ライブラリ
import pandas as pd # データ処理用ライブラリ、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは、読み取り専用の "../input/" ディレクトリにあります
# 例えば、これを実行することで（クリックするかShift+Enterを押す）、入力ディレクトリ内のすべてのファイルをリスト表示します

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ（/kaggle/working/）に最大20GBのデータを書き込むことができます
# これは、「保存してすべてを実行」機能を使用してバージョンを作成する際に出力として保持されます
# また、一時ファイルは/kaggle/temp/に書き込むことができますが、現在のセッションが終了すると保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
#pip install torch torchvision torchaudio
```

---The following area is a Code cell (cell numver is 3)---
```python
#pip install pandas numpy scikit-learn tensorflow
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import log_loss
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout

# データセットを読み込む
train_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
test_data = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# プロンプトと応答を結合して特徴抽出を行う
train_data['text_a'] = train_data['prompt'] + ' ' + train_data['response_a']
train_data['text_b'] = train_data['prompt'] + ' ' + train_data['response_b']

# TF-IDFを使用してテキストデータをベクトル化する
tfidf = TfidfVectorizer(max_features=5000) # 特徴数を5000に制限
X_a = tfidf.fit_transform(train_data['text_a']).toarray() # text_aをベクトル化
X_b = tfidf.transform(train_data['text_b']).toarray() # text_bをベクトル化

# ベクトル化された応答を結合する
X = np.hstack([X_a, X_b]) # 二つのベクトルを横に結合

# 目標変数（ターゲット）
y = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].values # 勝者モデルの情報を取得

# データをトレーニングセットとバリデーションセットに分割する
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42) # 80%をトレーニング、20%をバリデーション

# 特徴量をスケーリングする
scaler = StandardScaler() # 標準化のためのスケーラーを作成
X_train = scaler.fit_transform(X_train) # トレーニングデータをスケーリング
X_val = scaler.transform(X_val) # バリデーションデータをスケーリング

# シンプルなニューラルネットワークモデルを構築する
model = Sequential([
    Dense(512, activation='relu', input_shape=(X_train.shape[1],)), # 入力層
    Dropout(0.5), # ドロップアウト層（過学習を防ぐ）
    Dense(256, activation='relu'), # 隠れ層
    Dropout(0.5), # ドロップアウト層
    Dense(128, activation='relu'), # 隠れ層
    Dropout(0.5), # ドロップアウト層
    Dense(3, activation='softmax') # 出力層、3つのクラスに対する確率を出力
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # モデルをコンパイル

# モデルを訓練する
history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val)) # 学習を実行

# モデルを評価する
val_predictions = model.predict(X_val) # バリデーションデータで予測
loss = log_loss(y_val, val_predictions) #ロスを計算
print(f'Validation Log Loss: {loss}') # ロスの結果を表示

# テストデータを準備する
test_data['text_a'] = test_data['prompt'] + ' ' + test_data['response_a'] # テストデータのためにtext_aを作成
test_data['text_b'] = test_data['prompt'] + ' ' + test_data['response_b'] # テストデータのためにtext_bを作成
X_test_a = tfidf.transform(test_data['text_a']).toarray() # テストデータのtext_aをベクトル化
X_test_b = tfidf.transform(test_data['text_b']).toarray() # テストデータのtext_bをベクトル化
X_test = np.hstack([X_test_a, X_test_b]) # テストデータのベクトルを結合
X_test = scaler.transform(X_test) # テストデータをスケーリング

# テストセットに対して予測を行う
test_predictions = model.predict(X_test) # テストデータで予測

# 提出ファイルを作成する
submission = pd.DataFrame(test_data['id']) # 提出用のデータフレームを作成
submission['winner_model_a'] = test_predictions[:, 0] # モデルAの勝者確率を追加
submission['winner_model_b'] = test_predictions[:, 1] # モデルBの勝者確率を追加
submission['winner_tie'] = test_predictions[:, 2] # タイの勝者確率を追加
submission.to_csv('submission.csv', index=False) # CSVファイルとして保存
```

** @@@ Jupyter Notebook numver 76, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、「LMSYS - Chatbot Arena」コンペティションに関連するタスクに取り組んでいます。目的は、異なる大規模言語モデル（LLM）が生成した応答に基づいて、どちらのモデルの応答がユーザーに好まれるかを予測することです。

## 問題の取り組み
Notebookでは、ユーザーのプロンプトとそれに対する二つのモデルの応答を組み合わせたテキストデータを使用して、モデルの応答の好みを予測します。具体的には、トレーニングデータを取得し、トークン化してラベル付きデータを準備し、最終的に深層学習モデルをトレーニングしてテストデータに対する予測を行います。

## 使用された手法とライブラリ
以下の手法とライブラリが使用されています：

1. **ライブラリのインポート**:
   - 基本的なオペレーションのために`os`, `gc`, `re`, `time`, `random`などの標準ライブラリを使用。
   - 数値計算とデータ操作のために`NumPy`と`Pandas`を使用。
   - ビジュアライゼーションのために`Matplotlib`をインポート。
   - トークン化と深層学習モデルのために`PyTorch`と`Transformers`ライブラリを使用。
   - モデル評価のために`sklearn.metrics`から`accuracy_score`をインポート。

2. **データの前処理**:
   - トレーニングデータの読み込み、ユーザーのプロンプトとモデルの応答を組み合わせた新しいテキスト列を作成。
   - 入力テキストをトークン化し、入力IDとアテンションマスクを生成。

3. **モデルの構築**:
   - Kerasを用いてLSTM（長短期記憶）とCNN（畳み込みニューラルネットワーク）のアーキテクチャを組み合わせた複合モデルを定義。バッチ正規化やドロップアウトなどのテクニックを使用して、過学習を防止。
   - モデルのコンパイルとトレーニングが行われ、精度や損失といったメトリクスで評価されます。

4. **予測と提出ファイルの作成**:
   - テストデータに対してモデルを適用し、予測を行った後、結果をCSVフォーマットで保存します。

このNotebookは、深層学習を利用して人間の好みに基づく応答の予測を行うための包括的な手法を示しており、実際の機械学習パイプラインにおける重要な工程を構成しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import os  # オペレーティングシステムとのやり取りをするためのライブラリをインポートします
import gc  # ガーベジコレクションを扱うためのライブラリをインポートします
import re  # 正規表現を使用するためのライブラリをインポートします
from time import time  # 時間を計測するためのtime関数をインポートします
import random  # ランダムな数値を生成するためのライブラリをインポートします
import warnings  # 警告を管理するためのライブラリをインポートします
import numpy as np  # 数値計算を行うためのNumPyライブラリをインポートします
import pandas as pd  # データ操作と解析を行うためのPandasライブラリをインポートします
import matplotlib.pyplot as plt  # グラフを描画するためのMatplotlibライブラリをインポートします
from tqdm.auto import tqdm  # 進捗状況を表示するためのtqdmライブラリをインポートします

import torch  # PyTorchライブラリをインポートします
import transformers  # トランスフォーマモデルを扱うためのライブラリをインポートします
from sklearn.metrics import accuracy_score  # 精度を計算するための関数をインポートします
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, AutoModel  # 自動トークナイザーやLlamaモデルをインポートします
from transformers import LlamaForCausalLM, LlamaTokenizer  # 原因モデルとトークナイザーをインポートします
import torch.nn.functional as F  # PyTorchの関数を扱うためのモジュールをインポートします

np.random.seed(1337)  # NumPyの乱数生成のシードを設定します。これにより、結果の再現性が確保されます。
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## トークナイザー
```

---The following area is a Code cell (cell numver is 4)---
```python
model = "/kaggle/input/llama-3/transformers/70b-chat-hf/1/llama3-70b-chat-hf"  # 使用するモデルのパスを指定します

tokenizer = AutoTokenizer.from_pretrained(model)  # モデルからトークナイザーを読み込みます
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークンと同じに設定します
tokenizer.padding_side = 'right'  # パディングを右側に適用するよう設定します
tokenizer.add_eos_token = True  # 終了トークンを追加することを許可します

# 推論中にオフラインでトークナイザーをロードするために保存します
tokenizer.save_pretrained('tokenizer')  # トークナイザーの設定を指定したフォルダに保存します
```

---The following area is a Code cell (cell numver is 5)---
```python
# Utility function giving token length
# トークンの長さを返すユーティリティ関数を定義します
def get_token_lengths(texts):
    # 各テキストに対してトークン化し、input_idsを取得します
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']  # テキストをトークン化し、NumPy形式の入力IDを取得します
    # 各テキストに対するinput_idsの長さを返します
    return [len(t) for t in input_ids]  # 各トークンの長さをリストとして返します
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
## トレーニングセットの準備
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込みます
train.head(5)  # トレーニングセットの最初の5行を表示します
```

---The following area is a Code cell (cell numver is 8)---
```python
def put_text(train):
    # 新しいテキスト列を作成し、ユーザーのプロンプトとモデルAおよびモデルBの応答を組み合わせます
    train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A:\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
    return train  # 変更を加えたトレーニングデータを返します

train = put_text(train)  # トレーニングデータに新しいテキスト列を追加します
print(train['text'][0])  # 新しく作成したテキスト列の最初の要素を表示します
```

---The following area is a Code cell (cell numver is 9)---
```python
def process(input_str):
    # 入力文字列から角括弧を取り除きます
    stripped_str = input_str.strip('[]')
    # 文字列を分割して、各文から囲いを除きます
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return ' '.join(sentences)  # 文をスペースで結合して返します

# 'prompt', 'response_a', 'response_b'列の各値にprocess関数を適用します
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

train = put_text(train)  # 新しいテキスト列をトレーニングデータに再度追加します
print(train['text'][0])  # 更新されたテキスト列の最初の要素を表示します
```

---The following area is a Code cell (cell numver is 10)---
```python
train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # トレーニングデータの'text'列に基づいてトークンのカウントを追加します

# モデル用のラベルを準備します
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # 各行の勝者モデルをラベルとして取得します

# データを表示します
display(train.head())  # 最初の数行のトレーニングデータを表示します
```

---The following area is a Code cell (cell numver is 11)---
```python
train.label.value_counts()  # 各ラベルの出現回数をカウントして表示します
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークンのカウント
display(train['token_count'].describe().to_frame().astype(int))  # トークンのカウントに関する統計情報を計算し、整数型のデータフレームとして表示します
```

---The following area is a Code cell (cell numver is 13)---
```python
# データの90%をカバーするトークンの長さを取得しますが、1024の長さを使用します
np.percentile(train['token_count'], 90)  # トークンのカウントの90パーセンタイルを計算します
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
## トークン化
```

---The following area is a Code cell (cell numver is 15)---
```python
# データをトークン化します
tokens = tokenizer(
    train['text'].tolist(),  # テキストをリストに変換してトークン化します
    max_length=1024,  # 最大長を1024に設定します
    truncation=True,  # 長すぎるトークンは切り詰めます
    return_tensors='np')  # NumPy形式のテンソルを返します

# 入力IDはトークンIDです
INPUT_IDS = tokens['input_ids']  # トークン化された入力IDを取得します
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得します
# テキストのラベル
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values  # ラベルを取得します

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を表示します
print(f'LABELS shape: {LABELS.shape}')  # ラベルの形状を表示します
```

---The following area is a Code cell (cell numver is 16)---
```python
max_features = 14300  # モデルで使用する最大特徴数を設定します
max_len = 1024  # トークンの最大長を設定します
maxlen = max_len  # max_lenをmaxlenに割り当てます
batch_size = 16  # バッチサイズを設定します
embedding_dims = 100  # 埋め込み次元の数を設定します
nb_filter = 150  # フィルタの数を設定します
filter_length = 3  # フィルタの長さを設定します
hidden_dims = 100  # 隠れ層の次元の数を設定します
nb_epoch = 100  # エポック数を設定します
```

---The following area is a Code cell (cell numver is 17)---
```python
from __future__ import print_function  # print_functionを将来のバージョンからインポートして、Python 2と3の互換性を持たせます
import numpy as np  # NumPyライブラリをインポートします

from keras.preprocessing import sequence  # シーケンスデータを処理するためのKerasモジュールをインポートします
from keras.models import Sequential  # シーケンシャルモデルをインポートします
from keras.layers import Dense, Dropout, Activation, Lambda  # 完全結合層などのレイヤーをインポートします
from keras.layers import Embedding  # 埋め込み層をインポートします
from keras.layers import Convolution1D, LSTM  # 1次元畳み込み層とLSTM層をインポートします
from keras.datasets import imdb  # IMDBデータセットをインポートします
from keras import backend as K  # Kerasのバックエンドをインポートします
from keras.optimizers import Adadelta, Adamax  # オプティマイザーをインポートします
from keras.preprocessing import sequence as sq  # シーケンス処理モジュールをインポートし、sqの別名を付けます

from keras.layers import Dense, Dropout, Activation, Lambda, Input, TimeDistributed, Flatten  # その他のレイヤーをインポートします
from keras.models import Model  # モデル推定に使用するKerasのモデルをインポートします
from keras.callbacks import ModelCheckpoint  # モデルのチェックポイントを管理するためのコールバックをインポートします

from tensorflow.python.keras.backend import set_session as K  # TensorFlowの適切なセッション管理用のKerasバックエンドをインポートします
num_samples = INPUT_IDS.shape[0]  # サンプルの総数を取得します

# X_validのサンプル数（X_trainの20%）
num_valid_samples = int(num_samples * 0.2)  # バリデーション用サンプル数を計算します

# X_trainのインデックスをシャッフルします
indices = np.random.permutation(num_samples)  # サンプルのインデックスをランダムに並べ替えます

# 最初の20%のインデックスをX_valid用として選択します
valid_indices = indices[:num_valid_samples]  # バリデーション用のインデックスを取得します

# 残りのインデックスをX_train用として選択します
train_indices = indices[num_valid_samples:]  # トレーニング用のインデックスを取得します

# 選択したインデックスから新しいX_validとX_trainを作成します
X_train = sq.pad_sequences(INPUT_IDS[train_indices], maxlen=max_len)  # トレーニングデータのパディングを行います
X_train_attention = sq.pad_sequences(ATTENTION_MASKS[train_indices], maxlen=max_len)  # アテンションマスクのパディングを行います
y_train = LABELS[train_indices]  # トレーニングラベルを設定します

X_valid = sq.pad_sequences(INPUT_IDS[valid_indices], maxlen=max_len)  # バリデーションデータのパディングを行います
X_valid_attention = sq.pad_sequences(ATTENTION_MASKS[valid_indices], maxlen=max_len)  # バリデーションのアテンションマスクのパディングを行います
y_valid = LABELS[valid_indices]  # バリデーションラベルを設定します
```

---The following area is a Code cell (cell numver is 18)---
```python
X_train = np.array(X_train)  # トレーニングデータをNumPy配列に変換します
y_train = np.array(y_train)  # トレーニングラベルをNumPy配列に変換します
X_valid = np.array(X_valid)  # バリデーションデータをNumPy配列に変換します
y_valid = np.array(y_valid)  # バリデーションラベルをNumPy配列に変換します
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
## モデルの定義
```

---The following area is a Code cell (cell numver is 20)---
```python
from tensorflow.keras.layers import Layer  # Kerasのレイヤーをインポートします
from keras.layers import concatenate, Dropout, BatchNormalization, LSTM, Conv1D  # 各種レイヤーをインポートします
from keras.layers import GlobalMaxPooling1D  # 1次元の全体最大プーリングレイヤーをインポートします
import tensorflow as tf  # TensorFlowをインポートします

class ApplyAttentionMask(Layer):
    def call(self, inputs):
        # 埋め込みとアテンションマスクを適用する関数を定義します
        embeddings, attention_mask = inputs
        return embeddings * tf.expand_dims(attention_mask, -1)  # アテンションマスクを埋め込みに適用します

input_layer = Input(shape=(max_len,), dtype='int32', name='main_input')  # メイン入力レイヤーを定義します
attention_masks = Input(shape=(max_len,), dtype='float32', name="attention_masks")  # アテンションマスクの入力レイヤーを定義します

# 埋め込み層
emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=max_len)(input_layer)  # 埋め込み層を定義します

masked_embeddings = ApplyAttentionMask(name='apply_attention_mask')([emb_layer, attention_masks])  # アテンションマスクを適用します

# LSTMブランチ（バッチ正規化とドロップアウトを含む）
lstm_out = LSTM(128, return_sequences=True)(masked_embeddings)  # LSTM層を追加します
lstm_out = BatchNormalization()(lstm_out)  # バッチ正規化を適用します
lstm_out = Dropout(0.5)(lstm_out)  # ドロップアウトを適用して過学習を防ぎます
lstm_out = LSTM(64, return_sequences=True)(lstm_out)  # さらにLSTM層を追加
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)
lstm_out = LSTM(32)(lstm_out)  # 最後のLSTM層を追加
lstm_out = BatchNormalization()(lstm_out)
lstm_out = Dropout(0.5)(lstm_out)

# CNNレイヤーブランチ（バッチ正規化とドロップアウトを含む）
cnn_out = Conv1D(64, 5, activation='relu')(masked_embeddings)  # 1次元畳み込み層を追加
cnn_out = BatchNormalization()(cnn_out)  # バッチ正規化を適用します
cnn_out = Dropout(0.5)(cnn_out)  # ドロップアウトを適用します
cnn_out = Conv1D(32, 5, activation='relu')(cnn_out)  # さらに1次元畳み込み層を追加
cnn_out = BatchNormalization()(cnn_out)
cnn_out = Dropout(0.5)(cnn_out)
cnn_out = GlobalMaxPooling1D()(cnn_out)  # グローバル最大プーリングを適用します

# LSTMとCNNの出力を連結します
merged = concatenate([lstm_out, cnn_out])  # 両者の出力を結合します
merged = Dense(32, activation='sigmoid')(merged)  # 隠れ層を追加します
merged = BatchNormalization()(merged)  # バッチ正規化を適用します
merged = Dropout(0.5)(merged)  # ドロップアウトを適用します
pred = Dense(3, activation='softmax')(merged)  # 出力層を追加します

# モデルを構築します
model = Model(inputs=[input_layer, attention_masks], outputs=[pred])  # モデルとして入力と出力を指定します
adadelta = Adadelta(learning_rate=1.0, rho=0.75, epsilon=1e-06)  # Adadeltaオプティマイザーを設定します
adamax = Adamax(learning_rate=0.001)  # Adamaxオプティマイザーを設定します
model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy'])  # モデルをコンパイルします
model.summary()  # モデルの概要を表示します
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
## トレーニング
```

---The following area is a Code cell (cell numver is 22)---
```python
def clip_indices(data, max_index):
    # インデックスが最大のインデックスを超えた場合、最大インデックス-1にクリップします
    return np.where(data >= max_index, max_index - 1, data)

X_train = clip_indices(X_train, 14300)  # トレーニングデータのインデックスをクリップします
X_train_attention = clip_indices(X_train_attention, 14300)  # トレーニングアテンションマスクのインデックスをクリップします
X_valid = clip_indices(X_valid, 14300)  # バリデーションデータのインデックスをクリップします
X_valid_attention = clip_indices(X_valid_attention, 14300)  # バリデーションアテンションマスクのインデックスをクリップします
```

---The following area is a Code cell (cell numver is 23)---
```python
from keras.callbacks import EarlyStopping  # 早期停止のためのコールバックをインポートします
checkpoint = ModelCheckpoint('/kaggle/working/model.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')  # モデルのチェックポイントを設定します
early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1, restore_best_weights=True)  # 早期停止を設定します

# モデルをトレーニングします
model.fit([X_train, X_train_attention], y_train,
          batch_size=16,  # バッチサイズを設定します
          epochs=nb_epoch,  # エポック数を設定します
#           callbacks=[checkpoint, early_stopping],  # チェックポイントと早期停止のコールバック（コメントアウト）
          callbacks=[early_stopping],  # 早期停止のコールバックのみ使用します
          validation_data=([X_valid, X_valid_attention], y_valid))  # バリデーションデータを指定します
```

---The following area is a Code cell (cell numver is 24)---
```python
model.compile(loss='categorical_crossentropy',  # 損失関数をカテゴリカルクロスエントロピーとして設定します
              optimizer='adadelta',  # オプティマイザーをAdadeltaとして設定します
              metrics=['accuracy'])  # 評価指標として精度を設定します

model.save('model.keras', overwrite=True)  # モデル全体を指定したファイル名で保存します
model.save_weights("model.weights.h5", overwrite=True)  # モデルの重みのみを保存します
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
## モデルのテスト
```

---The following area is a Code cell (cell numver is 26)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータをCSVファイルから読み込みます

# プロンプトと応答を処理します
test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプト列にprocess関数を適用します
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答列にprocess関数を適用します
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答列にprocess関数を適用します

# トレーニングにおいて'null'を含む行を削除します
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index  # 'null'応答を持つインデックスを取得します
test.drop(indexes, inplace=True)  # 該当行をデータフレームから削除します
test.reset_index(inplace=True, drop=True)  # インデックスをリセットします

print(f"Total {len(indexes)} Null response rows dropped")  # 削除された'null'応答の行数を表示します
print('Total train samples: ', len(test))  # 残ったトレーニングサンプルの総数を表示します
```

---The following area is a Code cell (cell numver is 27)---
```python
test.head()  # テストデータの最初の5行を表示します
```

---The following area is a Code cell (cell numver is 28)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A:\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + test['response_b']  # テストデータに新しいテキスト列を追加します
print(test['text'])  # 新しく作成したテキスト列の内容を表示します
```

---The following area is a Code cell (cell numver is 29)---
```python
# テストデータをトークン化します
tokens_test = tokenizer(
    test['text'].tolist(),  # テキストをリストに変換してトークン化します
    max_length=1024,  # 最大長を1024に設定します
    truncation=True,  # 長すぎるトークンは切り詰めます
    return_tensors='np')  # NumPy形式のテンソルを返します

# 入力IDはトークンIDです
INPUT_test = tokens_test['input_ids']  # トークン化された入力IDを取得します
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS2 = tokens_test['attention_mask']  # アテンションマスクを取得します

print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')  # 入力IDとアテンションマスクの形状を表示します
```

---The following area is a Code cell (cell numver is 30)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=max_len)  # テストデータの入力IDをパディングして最大長に揃えます
X_test_attention = sq.pad_sequences(ATTENTION_MASKS2, maxlen=max_len)  # テストデータのアテンションマスクをパディングして最大長に揃えます
```

---The following area is a Code cell (cell numver is 31)---
```python
test  # テストデータフレームの内容を表示します
```

---The following area is a Code cell (cell numver is 32)---
```python
y_predict = model.predict([X_test, X_test_attention])  # テストデータに対する予測を行います
y_predict  # 予測結果を表示します
```

---The following area is a Code cell (cell numver is 33)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # 予測結果をデータフレームに変換します
result_df = pd.concat([test['id'], winner_df], axis=1)  # テストデータのIDと予測結果を結合して新しいデータフレームを作成します
```

---The following area is a Code cell (cell numver is 34)---
```python
result_df.to_csv('submission.csv', index=False)  # 予測結果をCSVファイルとして保存します
result_df  # 最終的な結果データフレームを表示します
```

---The following area is a Markdown cell (cell numver is 35)---
```markdown
____________________
```

---The following area is a Code cell (cell numver is 36)---
```python
# # 必要なライブラリをインポートします
# import pandas as pd  # データ操作用ライブラリのインポート
# import numpy as np  # 数値計算用ライブラリのインポート
# from sklearn.model_selection import train_test_split  # データ分割用モジュールのインポート
# from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFベクトライザーをインポート
# from sklearn.ensemble import RandomForestClassifier  # ランダムフォレスト分類器をインポート
# from sklearn.metrics import log_loss  # ログロスを計算する関数をインポート
# from sklearn.preprocessing import LabelEncoder  # ラベルエンコーダをインポート

# # データをロードします
# train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータの読み込み
# test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータの読み込み

# # データを検査します
# print(train.head())  # トレーニングデータの最初の5行を表示
# print(test.head())  # テストデータの最初の5行を表示

# # データの前処理
# # 応答を1つのテキスト特徴として結合します
# train['response_combined'] = train['response_a'] + ' ' + train['response_b']  # 応答の結合
# test['response_combined'] = test['response_a'] + ' ' + test['response_b']  # テストデータの応答も結合

# # ターゲットラベルをエンコードします
# label_encoder = LabelEncoder()  # ラベルエンコーダのインスタンスを作成
# train['winner'] = label_encoder.fit_transform(train[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1))  # 勝者のラベルをエンコード

# # 特徴量エンジニアリング
# # 結合された応答をTF-IDFを使用してベクトル化します
# tfidf = TfidfVectorizer(max_features=1000)  # TF-IDFベクトライザーを作成
# X_train_tfidf = tfidf.fit_transform(train['response_combined'])  # トレーニングデータをベクトル化
# X_test_tfidf = tfidf.transform(test['response_combined'])  # テストデータをベクトル化

# # モデル学習のためのデータを準備します
# X_train, X_val, y_train, y_val = train_test_split(X_train_tfidf, train['winner'], test_size=0.2, random_state=42)  # データをトレーニングとバリデーションに分割

# # モデルをトレーニングします
# model = RandomForestClassifier(n_estimators=100, random_state=42)  # ランダムフォレストモデルを作成
# model.fit(X_train, y_train)  # モデルのトレーニング

# # モデルをバリデーションします
# y_val_pred_proba = model.predict_proba(X_val)  # バリデーションデータに対する予測確率を計算
# val_log_loss = log_loss(y_val, y_val_pred_proba)  # バリデーションロスを計算
# print(f'Validation Log Loss: {val_log_loss}')  # バリデーションロスを表示

# # テストセットに対して予測します
# test_pred_proba = model.predict_proba(X_test_tfidf)  # テストデータに対する予測確率を計算

# # 提出ファイルを準備します
# submission = pd.DataFrame(test['id'], columns=['id'])  # 提出用データフレームを作成
# submission['winner_model_a'] = test_pred_proba[:, label_encoder.transform(['winner_model_a'])]  # モデルAの勝者確率を加える
# submission['winner_model_b'] = test_pred_proba[:, label_encoder.transform(['winner_model_b'])]  # モデルBの勝者確率を加える
# submission['winner_tie'] = test_pred_proba[:, label_encoder.transform(['winner_tie'])]  # 引き分けの勝者確率を加える
# submission.to_csv('submission.csv', index=False)  # 提出ファイルをCSV形式で保存

# # 提出ファイルを検査します
# print(submission.head())  # 提出ファイルの最初の5行を表示
```

** @@@ Jupyter Notebook numver 77, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYSのChatbot Arenaコンペティションにおける人間の好み予測のための機械学習モデルのトレーニングに取り組んでいます。具体的には、Llama-3言語モデルを基にしたファインチューニングを行い、与えられたプロンプトに対してユーザーが好む応答のモデルを構築しています。

### 問題の背景
本コンペティションでは、ユーザーが提示したプロンプトに対する異なる応答の中から、どちらの応答が好まれるかを予測することが求められます。この予測は、ログ損失を用いて評価されます。

### 手法
このNotebookでは以下の手法とライブラリを使用しています：

- **ライブラリ**:
  - `transformers`: 大規模言語モデルやトークナイザーを用いるために使用。
  - `datasets`: データの読み込みと前処理を行うために利用。
  - `scikit-learn`: ログ損失や精度などのメトリクス計算を行うために使用。
  - `torch`: PyTorchによるニューラルネットワークの構築および訓練に使用。

- **モデル**:
  - Llama-3ベースモデル（`llama-3-8b-Instruct-bnb-4bit`）を利用し、これをファインチューニングして人間の選好を予測するモデルを構築しています。

- **データ処理**:
  - データセットからプロンプトと応答を抽出し、それらをトークン化。
  - 各応答に対するラベル（「winner_model_a」「winner_model_b」「winner_tie」）を生成。

- **トレーニング戦略**:
  - 交差検証を用いてモデルの性能を評価。
  - 勾配蓄積やバッチサイズ、エポック数を調整し、効率的にトレーニング。

- **評価**:
  - 訓練後、ログ損失と精度を検証し、最終的なパフォーマンスを評価しています。

### 結果
Notebookの結果セクションでは、推論コードへのリンクと評価データセットおよびリーダーボードでのログ損失（Eval: 0.9231, LB: 0.936）が示されています。また、成功裏に再現可能な設定として、使用するデータやハイパーパラメータ（例：トレーニング中のバッチサイズ、エポック数など）が明記されています。

このNotebookは、提供された情報に基づいて、ユーザーに好まれるチャットボットの応答を予測するための効率的な機械学習アプローチを示しており、実用的な応用が期待されます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 結果
- [推論コード](https://www.kaggle.com/code/shelterw/sft-llama-3-8b-inference)    

- [ベースモデル: llama-3-8b-Instruct-bnb-4bit](https://huggingface.co/unsloth/llama-3-8b-Instruct-bnb-4bit)

| サブセット | log loss |
| - | - |
| Eval | 0.9231 |
| LB | 0.936 |

## 注意
コードを再現したい場合は、以下の点に注意してください：
- すべてのデータを使用する
- per_device_train_batch_size=4を設定する
- 1エポックのトレーニングにはA10を使用して約15時間かかりました
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from datasets import Dataset
from scipy.special import softmax
from sklearn.preprocessing import LabelEncoder
from transformers import (
    BitsAndBytesConfig,
    LlamaPreTrainedModel,
    LlamaModel,
    AutoTokenizer,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorForSeq2Seq,
)
from transformers.modeling_outputs import CausalLMOutputWithPast
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
TRAIN_CSV = "/kaggle/input/lmsys-chatbot-arena/train.csv"
model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
MAX_LENGTH = 1024
target_columns = ['winner_model_a', 'winner_model_b', 'winner_tie']
columns_to_vectorize = ["prompt", "response_a", "response_b"]

train = pd.read_csv(TRAIN_CSV)
train = train.head(100)
train['label'] = train[target_columns].idxmax(axis=1) 
label_encoder = LabelEncoder()
train['label'] = label_encoder.fit_transform(train['label'])
train = train[columns_to_vectorize + ['label']]
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
### トークナイザーとデータセットの準備、メトリクス
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(model_path)
tokenizer.add_eos_token = True
tokenizer.padding_side = 'right'

# ラベルに対応するトークンIDを取得
LABEL_IDS = [tokenizer(i, add_special_tokens=False)["input_ids"][0] for i in ['a', 'b', 'tie']]

# トークン化の関数を定義
def tokenize(example, tokenizer):
    # プロンプトと応答をトークン化
    prompt = tokenizer('<prompt>: ' + " ".join(eval(example['prompt'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_a = tokenizer('\n\n<response_a>: ' + " ".join(eval(example['response_a'], {"null": ""})), add_special_tokens=False)["input_ids"]
    response_b = tokenizer('\n\n<response_b>: ' + " ".join(eval(example['response_b'], {"null": ""})), add_special_tokens=False)["input_ids"]
    
    # トークンIDの長さが最大長を超える場合、トリミング
    if len(prompt+response_a+response_b) > MAX_LENGTH:
        prompt = tokenizer('<prompt>: ' + eval(example['prompt'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:256]
        response_a = tokenizer('\n\n<response_a>: ' + eval(example['response_a'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        response_b = tokenizer('\n\n<response_b>: ' + eval(example['response_b'], {"null": ""})[-1], add_special_tokens=False)["input_ids"][:512]
        
    # 追加のプロンプトを定義
    extra_prompt = tokenizer('\n\n---------\nWhich is the better response for the prompt ? a or b or tie ?\n\nAnswer: ', add_special_tokens=False)["input_ids"]

    # ラベルのトークンIDを取得
    label_token_id = LABEL_IDS[int(example['label'])]
    input_ids = [tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt + [label_token_id] + [tokenizer.eos_token_id]
    attention_mask = len(input_ids)*[1]
    labels = [-100]* len([tokenizer.bos_token_id] + prompt + response_a + response_b + extra_prompt) + [label_token_id] + [tokenizer.eos_token_id]
    
    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "labels": labels
    }
```

---The following area is a Code cell (cell numver is 8)---
```python
# データを読み込み、トークン化する関数を定義
def load_data(df, tokenizer):
    raw_datasets = Dataset.from_pandas(df)  # pandas DataFrameをDatasetに変換
    tokenized_datasets = raw_datasets.map(
        tokenize,  # トークナイズ関数を適用
        remove_columns=raw_datasets.column_names,
        fn_kwargs={'tokenizer': tokenizer}
    )
    return tokenized_datasets

# メトリクスを計算する関数を定義
def compute_metrics(pred):
    logits, labels = pred  # 予測とラベルを取得
    preds = logits.argmax(axis=-1)  # 最も高いロジットを持つインデックスを予測
    label_tokens_ids = np.array(LABEL_IDS)
    index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
    labels = labels[np.isin(labels, label_tokens_ids)]
    labels = np.array([index_mapping[label.item()] for label in labels])  # ラベルをインデックスにマッピング
    
    acc = accuracy_score(labels, preds)  # 精度を計算
    probs = softmax(logits, axis=-1)  # ソフトマックスを計算
    log_loss_ = log_loss(labels, probs)  # log lossを計算
    return {'accuracy': acc, 'log_loss': log_loss_}

n_splits = 5
fold_idx = 0
ds = load_data(train, tokenizer)  # データの読み込み
folds = [
    (
        [i for i in range(len(ds)) if i % n_splits != fold_idx],
        [i for i in range(len(ds)) if i % n_splits == fold_idx]
    ) 
    for fold_idx in range(n_splits)  # n分割交差検証のインデックスを生成
]
train_idx, eval_idx = folds[fold_idx]  # トレーニングと評価のインデックスを分ける
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
### モデル
```

---The following area is a Code cell (cell numver is 10)---
```python
class Llama3ForSFT(LlamaPreTrainedModel):
    _tied_weights_keys = ["lm_head.weight"]  # 重みを共有するためのキー
    def __init__(self, config):
        super().__init__(config)
        self.model = LlamaModel(config)  # Llamaモデルの初期化
        self.vocab_size = config.vocab_size  # 語彙サイズの取得
        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)  # 線形層の初期化
        self.post_init()

    def forward(
        self,
        input_ids= None,
        attention_mask= None,
        position_ids = None,
        past_key_values= None,
        inputs_embeds= None,
        labels= None,
        use_cache= None,
        output_attentions= None,
        output_hidden_states = None,
        return_dict= None,
        cache_position = None,
    ):
        outputs = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_values=past_key_values,
            inputs_embeds=inputs_embeds,
            use_cache=use_cache,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
            cache_position=cache_position,
        )
        
        hidden_states = outputs[0]  # 隠れ状態を取得
        if self.config.pretraining_tp > 1:
            lm_head_slices = self.lm_head.weight.split(self.vocab_size // self.config.pretraining_tp, dim=0)
            logits = [F.linear(hidden_states, lm_head_slices[i]) for i in range(self.config.pretraining_tp)]
            logits = torch.cat(logits, dim=-1)
        else:
            logits = self.lm_head(hidden_states)  # 隠れ状態からロジットを取得
        logits = logits.float()  # ロジットを浮動小数点に変換

        loss = None
        if labels is not None:
            # トークンをずらして予測に使用
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()
            # トークンをフラット化
            loss_fct = nn.CrossEntropyLoss()
            shift_logits = shift_logits.view(-1, self.config.vocab_size)
            shift_labels = shift_labels.view(-1)
            # モデルの並列処理を有効にする
            shift_labels = shift_labels.to(shift_logits.device)

            label_tokens_ids = torch.tensor(LABEL_IDS, device=shift_labels.device)
            index_mapping = {value.item(): idx for idx, value in enumerate(label_tokens_ids)}
            true_labels = shift_labels[torch.isin(shift_labels, label_tokens_ids)]
            true_labels = torch.tensor([index_mapping[label.item()] for label in true_labels], device=true_labels.device)
            true_logits = shift_logits[torch.isin(shift_labels, label_tokens_ids)][:, label_tokens_ids]
            loss = loss_fct(true_logits, true_labels)  # ロスを計算

        return CausalLMOutputWithPast(
            loss=loss,
            logits=true_logits,
        )
```

---The following area is a Code cell (cell numver is 11)---
```python
peft_config = LoraConfig(
    r=16,
    lora_alpha=32,
    lora_dropout=0.05,
    bias='none',
    inference_mode=False,
    task_type=TaskType.CAUSAL_LM,
    target_modules=['q_proj', 'k_proj', 'v_proj',], 
)
```

---The following area is a Code cell (cell numver is 12)---
```python
model = Llama3ForSFT.from_pretrained(
    model_path, 
    load_in_8bit=True,
    torch_dtype=torch.float16,
    cache_dir="/kaggle/working/model"
)
model.config.use_cache = False  # キャッシュを使用しないよう設定
model = prepare_model_for_kbit_training(model)  # k-bitトレーニングのためにモデルを準備
model = get_peft_model(model, peft_config)  # Loraモデルを取得
print(model)  # モデルの構造を表示
model.print_trainable_parameters()  # 訓練可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
#### 学習引数
```

---The following area is a Code cell (cell numver is 14)---
```python
args = TrainingArguments(
    output_dir='output',
    overwrite_output_dir = True,  # 出力ディレクトリを上書き
    evaluation_strategy = "epoch",  # 評価戦略
    save_strategy = "steps",  # 保存戦略
    save_steps=200,  # 保存するステップ数
    save_total_limit=1,  # 保存するモデルの最大数
    logging_strategy="steps",  # ロギング戦略
    logging_steps=10,  # ロギングするステップ数
    warmup_steps=20,  # ウォームアップステップ数
    optim="adamw_8bit",  # オプティマイザ
    learning_rate=2e-4,  # 学習率
    per_device_train_batch_size=2,  # デバイスごとの訓練バッチサイズ
    per_device_eval_batch_size=4,  # デバイスごとの評価バッチサイズ
    gradient_accumulation_steps=2,  # 勾配蓄積ステップ数
    num_train_epochs=1,  # 訓練エポック数
    fp16=True,  # 16ビット浮動小数点の使用
    metric_for_best_model="log_loss",  # ベストモデルのメトリック
    greater_is_better = False,  # メトリックが大きい方が良いかどうか
    report_to="none",
)
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
### 学習中 !
```

---The following area is a Code cell (cell numver is 16)---
```python
trainer = Trainer(
    args=args,
    model=model,
    train_dataset=ds.select(train_idx),  # トレーニングデータセット
    eval_dataset=ds.select(eval_idx),  # 評価データセット
    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer),  # データコレータ
    compute_metrics=compute_metrics,  # メトリクス計算の関数
)
trainer.train()  # 学習を開始
```

---The following area is a Code cell (cell numver is 17)---
```python
model.save_pretrained('pretrained_model')  # 学習したモデルを保存
```

---The following area is a Code cell (cell numver is 18)---
```python
# from transformers import AutoModelForCausalLM, AutoTokenizer
# from peft import LoraConfig, get_peft_model, TaskType

# # 事前に学習されたモデルとLoraアダプタのパス
# model_path = "unsloth/llama-3-8b-Instruct-bnb-4bit"
# lora_adapter_path = "/kaggle/input/model-1"

# # ベースモデルをロード
# model_1 = AutoModelForCausalLM.from_pretrained(model_path)

# # トークナイザーをロード
# tokenizer = AutoTokenizer.from_pretrained(model_path)

# # Loraの設定
# lora_config = LoraConfig(
#     r=8,            # Loraのランク
#     lora_alpha=16,  # Loraのスケーリングファクター
#     task_type=TaskType.CAUSAL_LM  # モデルのタスクタイプ
# )

# # モデルをk-bitトレーニング用に準備
# model_1 = prepare_model_for_kbit_training(model_1)

# # モデルにLoraアダプタを適用
# model_1 = get_peft_model(model_1, lora_config)

# # 保存されたLoraアダプタのパラメータをロード
# model_1.load_adapter(lora_adapter_path, adapter_name="test")

# # モデルが使用可能になった
# model_1.eval()  # 評価モードに設定

# # 例文をトークン化してテキストを生成
# sentence = "Hello, how are you?"
# inputs = tokenizer(sentence, return_tensors="pt")
# outputs = model_1.generate(**inputs)

# print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```

---The following area is a Code cell (cell numver is 19)---
```python
# !zip -r model_2.zip /kaggle/working/saved_model_2
```

** @@@ Jupyter Notebook numver 78, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、チャットボット応答の好みを予測するための機械学習モデルを構築するためのものです。具体的には、提供されたデータセットを使用して、モデルA、モデルB、または引き分けとしてどの応答がユーザーに好まれるかを分類します。

### 主要な手法とライブラリ
1. **ライブラリの使用**:
   - `pandas`と`numpy`: データ操作や数値計算のために使用。
   - `matplotlib`と`seaborn`: データの可視化を行うために利用。
   - `scikit-learn`: データの分割や評価指標の計算に使用。
   - `tensorflow.keras`: ニューラルネットワークモデルの構築に使用します。

2. **データの前処理**:
   - 訓練データとテストデータをCSVファイルから読み込み、データフレームに格納。
   - プロンプトと応答を結合し、テキストデータをトークン化します。この作業では`Tokenizer`を使用し、シーケンスをパディングすることで、全ての入力データが同じ長さになるように整形します。
   - ターゲットラベル（勝者モデル）を数値形式からワンホットエンコーディングに変換します。

3. **モデル構築**:
   - Kerasの`Sequential`モデルを使用し、埋め込み層、LSTM層、および全結合層を組み合わせて、テキストデータを処理するモデルを構築します。
   - モデルは、3つのクラスに対する確率を出力するように設計されています（モデルA、モデルB、引き分け）。
   - 損失関数には`categorical_crossentropy`を使用し、最適化アルゴリズムには`adam`を設定しています。

4. **モデルの訓練**:
   - 訓練データを用いて10エポックの間モデルを学習させ、訓練過程の損失と精度を可視化します。
   - 訓練セットと検証セットに分けることで、モデルの過学習を防ぎつつ性能を評価します。

5. **予測と提出ファイルの作成**:
   - テストデータに対してモデルを用いて予測を実施し、結果はバイナリカラムとしてテストデータフレームに追加します。
   - 最終的に、予測結果を「submission.csv」というCSVファイルに保存し、Kaggleコンペティションに投稿可能な形式に整えます。

このNotebookは、与えられたデータセットをもとに、言語モデルの応答の好みを効果的に予測する機械学習フレームワークを実装するための全体的なプロセスを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# 必要なライブラリをインポートします
import pandas as pd  # データ操作用のライブラリ
import numpy as np  # 数値計算用のライブラリ
import matplotlib.pyplot as plt  # グラフ描画用のライブラリ
import seaborn as sns  # より美しいグラフを描くためのライブラリ
from sklearn.model_selection import train_test_split  # データを訓練用とテスト用に分割するための関数
from sklearn.metrics import confusion_matrix, classification_report  # 評価指標の計算に使う関数
from tensorflow.keras.preprocessing.text import Tokenizer  # テキストデータのトークン化を行うクラス
from tensorflow.keras.preprocessing.sequence import pad_sequences  # シーケンスデータを同じ長さにパディングするための関数
from tensorflow.keras.models import Sequential  # シーケンシャルモデルを作成するためのクラス
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout  # ニューラルネットワークのレイヤーをインポート
from tensorflow.keras.utils import to_categorical  # クラスラベルをカテゴリカルに変換するための関数

# これらのライブラリはデータの前処理やモデルの構築に使われます。
```

---The following area is a Code cell (cell numver is 2)---
```python
# データを読み込みます
train_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # 訓練データのCSVファイルを読み込み、train_dfに格納します
test_df = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータのCSVファイルを読み込み、test_dfに格納します

# 読み込んだデータフレームには、訓練用とテスト用のデータがそれぞれ格納されています。これらのデータはモデルの学習や評価に使用されます。
```

---The following area is a Code cell (cell numver is 3)---
```python
# カラム名を確認します
print("Train Data Columns:", train_df.columns)  # 訓練データのカラム名を表示します
print("\nTest Data Columns:", test_df.columns)  # テストデータのカラム名を表示します

# ここでは、訓練データとテストデータの各カラム名を確認して、データの構造を理解します。これにより、どの特徴量（フィーチャー）が使用されるかを把握することができます。
```

---The following area is a Code cell (cell numver is 4)---
```python
# データの小さなサブセットを使用します
train_df = train_df.head(2000)  # 訓練データから最初の2000行を抽出し、train_dfに格納します
test_df = test_df.head(2000)  # テストデータから最初の2000行を抽出し、test_dfに格納します

# これにより、データセットが小さくなり、処理時間を短縮し、モデルの実験を迅速に行うことができます。最初は限られたデータでテストし、その後、必要に応じて全体のデータを使用することが推奨されます。
```

---The following area is a Code cell (cell numver is 5)---
```python
# 勝者のカラムを一つのターゲットカラムに統合します
train_df['winner'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  
# winner_model_a、winner_model_b、winner_tieの各カラムで値が最大のインデックスを取得し、それを新しいwinnerカラムに格納します。

# これにより、勝者の情報が1つのカラムに集約され、モデルの学習や評価が容易になります。winnerカラムには、どのモデルが選ばれたかを示す数値が入ります。
```

---The following area is a Code cell (cell numver is 6)---
```python
# トークン化のためにすべてのテキストデータを結合します
all_text = pd.concat([train_df['prompt'], train_df['response_a'], train_df['response_b'],  # 訓練データのプロンプトと応答を結合
                      test_df['prompt'], test_df['response_a'], test_df['response_b']])  # テストデータのプロンプトと応答も結合します

# これにより、トークン化の処理を一度に行うためにすべてのテキストデータが一つのシリーズにまとめられます。この操作は、モデルが自然言語を理解するための前処理において重要なステップとなります。
```

---The following area is a Code cell (cell numver is 7)---
```python
# テキストデータをトークン化します
tokenizer = Tokenizer()  # Tokenizerオブジェクトを初期化します
tokenizer.fit_on_texts(all_text)  # 結合したテキストデータに基づいて単語のインデックスを作成します
vocab_size = len(tokenizer.word_index) + 1  # 語彙のサイズを取得します（インデックスが0から始まるので1を加えます）

# トークン化は、テキストデータを数値の配列に変換するプロセスです。これにより、機械学習モデルがテキストを理解し、処理できるようになります。vocab_sizeは使用する単語の総数を示し、モデルを構築する際に必要な情報となります。
```

---The following area is a Code cell (cell numver is 8)---
```python
def tokenize_and_pad(text_series, tokenizer, max_len):
    # テキストシリーズをトークン化し、パディングを行う関数
    sequences = tokenizer.texts_to_sequences(text_series)  # テキストデータを数値のシーケンスに変換します
    padded_sequences = pad_sequences(sequences, maxlen=max_len)  # シーケンスの長さをmax_lenに揃えます
    return padded_sequences  # パディングされたシーケンスを返します

# この関数は、与えられたテキストデータをトークン化し、指定された長さにパディングを行います。これにより、すべてのシーケンスが同じ長さになり、ニューラルネットワークに入力する際に扱いやすくなります。
```

---The following area is a Code cell (cell numver is 9)---
```python
# 最大シーケンス長を定義します
max_len = 100  # トークン化されたテキストシーケンスの最大長を100に設定します

# この設定により、全てのシーケンスは長さ100に揃えられ、短いシーケンスにはパディングが追加されます。長すぎるシーケンスは切り捨てられます。これにより、一貫した入力サイズが確保され、モデルのトレーニングが容易になります。
```

---The following area is a Code cell (cell numver is 10)---
```python
# 訓練データをトークン化し、パディングを行います
x_prompt = tokenize_and_pad(train_df['prompt'], tokenizer, max_len)  # プロンプトをトークン化してパディングする
x_response_a = tokenize_and_pad(train_df['response_a'], tokenizer, max_len)  # 応答Aをトークン化してパディングする
x_response_b = tokenize_and_pad(train_df['response_b'], tokenizer, max_len)  # 応答Bをトークン化してパディングする
x_train = np.concatenate([x_prompt, x_response_a, x_response_b], axis=1)  # トークン化されたプロンプトと応答を結合して訓練データを作成する

# この処理により、プロンプトとそれに対する2つの応答（response_aおよびresponse_b）が一つの特徴行列としてまとめられ、モデルの入力データとして利用できるようになります。次のステップでモデルを構築する際に、これらのデータが必要になります。
```

---The following area is a Code cell (cell numver is 11)---
```python
# ターゲット変数をエンコードします
y_train = to_categorical(train_df['winner'])  # winnerカラムをカテゴリカル形式に変換します

# to_categorical関数を使用することで、ターゲット変数（勝者）が整数ラベルからワンホットエンコーディング形式に変換されます。これにより、モデルがマルチクラスの分類タスクを適切に学習できるようになります。各クラスは、ベクトルの中で関連するインデックスに1が設定されている形式で表現されます。
```

---The following area is a Code cell (cell numver is 12)---
```python
# データを訓練セットと検証セットに分割します
x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)  
# 訓練データを80%の訓練セットと20%の検証セットに分割します。random_stateを指定することで、再現性のある分割が行われます。

# この分割により、訓練セットはモデルの学習に、検証セットはモデルの性能を評価するために使用されます。検証セットを用いて過学習を防ぐための調整も行います。
```

---The following area is a Code cell (cell numver is 13)---
```python
# Kerasモデルを構築します
model = Sequential()  # シーケンシャルモデルを初期化します
model.add(Embedding(vocab_size, 128))  # 埋め込みレイヤーを追加し、語彙サイズに基づいて128次元のベクトルを生成します
model.add(LSTM(64, return_sequences=True))  # LSTMレイヤーを追加し、64ユニットの出力を生成します（シーケンスを持つため、return_sequences=True）
model.add(Dropout(0.5))  # ドロップアウトレイヤーを追加し、50%の確率でニューロンを無効化します
model.add(LSTM(64))  # もう一つのLSTMレイヤーを追加し、最後の64ユニットの出力を生成します
model.add(Dense(3, activation='softmax'))  # 出力層を追加し、3クラスの確率を出力します（softmax活性化関数を使用）

# このモデルは、埋め込み、LSTM、ドロップアウト層を利用して、テキストデータからパターンを学習し、最終的に3つのクラス（モデルA、モデルB、引き分け）のいずれかを予測します。構築した後にモデルをコンパイルして学習させます。
```

---The following area is a Code cell (cell numver is 14)---
```python
# モデルをコンパイルします
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])  
# 最適化アルゴリズムとしてAdamを使用し、損失関数にはカテゴリカルクロスエントロピーを指定します。また、モデルの評価指標として精度（accuracy）を使用します。

# モデルをコンパイルすることで、学習プロセスを開始する準備が整います。この設定により、モデルは訓練データに対して学習を行い、誤差を最小化するように調整されます。
```

---The following area is a Code cell (cell numver is 15)---
```python
# モデルを訓練します
history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_val, y_val))  
# 訓練データ（x_train, y_train）を使用してモデルを10エポック（epoch）学習させます。バッチサイズは32に設定され、検証データ（x_val, y_val）も指定して、検証精度を確認します。

# このプロセスでは、モデルは訓練データから学習し、逐次エポックごとにパフォーマンスを向上させることを目指します。検証データを使って、過学習を避けつつモデルの性能を評価します。訓練の進行状況は、historyオブジェクトに保持され、後で可視化や分析に使用できます。
```

---The following area is a Code cell (cell numver is 16)---
```python
# 図とサブプロットのセットを作成します
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 6))  # 1行2列のサブプロットを作成し、サイズを指定します

# 訓練および検証の損失値をプロットします
ax1.plot(history.history['loss'], label='Train')  # 訓練データの損失をプロット
ax1.plot(history.history['val_loss'], label='Validation')  # 検証データの損失をプロット
ax1.set_title('Model Loss')  # グラフのタイトルを設定
ax1.set_xlabel('Epoch')  # x軸のラベルを設定
ax1.set_ylabel('Loss')  # y軸のラベルを設定
ax1.legend(loc='upper right')  # 凡例を右上に配置

# 訓練および検証の精度値をプロットします
ax2.plot(history.history['accuracy'], label='Train')  # 訓練データの精度をプロット
ax2.plot(history.history['val_accuracy'], label='Validation')  # 検証データの精度をプロット
ax2.set_title('Model Accuracy')  # グラフのタイトルを設定
ax2.set_xlabel('Epoch')  # x軸のラベルを設定
ax2.set_ylabel('Accuracy')  # y軸のラベルを設定
ax2.legend(loc='upper left')  # 凡例を左上に配置

# レイアウトを調整して重なりを防ぎます
plt.tight_layout()  # 自動的にレイアウトを調整します
plt.show()  # グラフを表示します

# これにより、訓練中のモデルの損失と精度の推移が可視化され、トレーニングの進行状況を確認することができます。訓練と検証の結果を比較することで、モデルの過学習や性能の適切性を評価する手助けになります。
```

---The following area is a Code cell (cell numver is 17)---
```python
# テストデータを準備します
x_test_prompt = tokenize_and_pad(test_df['prompt'], tokenizer, max_len)  # テストデータのプロンプトをトークン化してパディング
x_test_response_a = tokenize_and_pad(test_df['response_a'], tokenizer, max_len)  # テストデータの応答Aをトークン化してパディング
x_test_response_b = tokenize_and_pad(test_df['response_b'], tokenizer, max_len)  # テストデータの応答Bをトークン化してパディング
x_test = np.concatenate([x_test_prompt, x_test_response_a, x_test_response_b], axis=1)  # トークン化されたプロンプトと応答を結合してテストデータを作成

# この処理により、テストデータも訓練データと同様の方法で前処理され、モデルに入力できる形式になります。モデルの推論を行うための準備が整います。テストデータは、最終的にモデルの性能を評価するために使用されます。
```

---The following area is a Code cell (cell numver is 18)---
```python
# テストデータに対して予測を行います
y_test_pred = model.predict(x_test)  # テストデータに対してモデルを用いて予測を行います
y_test_pred_labels = np.argmax(y_test_pred, axis=1)  # 各サンプルの最大値のインデックスを取得し、予測ラベルを決定します

# この予測により、テストデータに対するモデルの出力が得られ、予測されるクラス（勝者）がラベルとして取得されます。このラベルは、その後の評価や結果の分析に使用されます。
```

---The following area is a Code cell (cell numver is 19)---
```python
# 予測をバイナリカラムに変換します
test_df['winner_model_a'] = (y_test_pred_labels == 0).astype(float)  # モデルAの予測が当たった場合は1、そうでなければ0を設定
test_df['winner_model_b'] = (y_test_pred_labels == 1).astype(float)  # モデルBの予測が当たった場合は1、そうでなければ0を設定
test_df['winner_tie'] = (y_test_pred_labels == 2).astype(float)  # 引き分けの予測が当たった場合は1、そうでなければ0を設定

# ここでは、各モデルの勝者としての予測結果を表すバイナリカラムがtest_dfに追加されます。これにより、最終的な結果を容易に解釈できる形式となり、各モデルが選ばれた確率が明確に示されます。
```

---The following area is a Code cell (cell numver is 20)---
```python
# 予測結果をsubmission.csvに保存します
submission_df = test_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用のデータフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルとして保存（インデックスは含めない）

# ここでは、テストデータのIDと予測された勝者モデルを含む予測結果をsubmission.csvというファイルに保存します。このファイルは、コンペティションに提出するためのフォーマットとして使用されます。
```

** @@@ Jupyter Notebook numver 79, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena」コンペティションに参加するために、人間による好みを予測する機械学習モデルを構築することを目的としています。具体的には、異なる大規模言語モデル（LLM）からの応答の中で、どちらの応答がユーザーに好まれるかを予測するタスクに取り組んでいます。

### 問題設定
このNotebookは、ユーザーが提示したプロンプトに対して、2つのモデルが生成した応答のどちらが好まれるか（モデルAの応答、モデルBの応答、または同点）を予測することを目指しています。訓練データには、各プロンプトに対してユーザーからの選好結果が含まれており、これを用いてモデルを訓練します。

### 使用されている手法とライブラリ
1. **データ処理**:
   - Pandas: データの読み込み、加工、前処理を行うために使用。
   - NumPy: 数値計算や線形代数の処理に利用。

2. **データ前処理**:
   - カテゴリ変数のラベルエンコーディング: `LabelEncoder`を用いて応答モデル名を数値に変換。
   - テキストデータからの特徴量作成: プロンプトと応答の単語数をカウントし、新たな特徴量として追加。
   - 特徴量の正規化: `StandardScaler`を用いて特徴量をスケーリング。

3. **モデル構築**:
   - Keras: 深層学習モデルを構築するために使用。順伝播型のニューラルネットワークを設計し、隠れ層に`Dropout`を用いて過学習を防止。
   - Adamオプティマイザと`categorical_crossentropy`損失関数でモデルをコンパイルし、訓練を行う。アーリーストッピングを利用して、バリデーションロスの低下が見られなくなったときに訓練を早期に終了。

4. **評価と予測**:
   - 訓練したモデルの精度をテストデータで評価し、結果を表示。
   - テストデータに対して予測を行い、その結果を提出形式のDataFrameに整形し、CSVファイルとしてエクスポート。

このNotebookは、モデルの訓練から評価、予測、さらには提出ファイルの生成までの一連のプロセスを網羅しており、最終的な目的であるKaggleへの提出準備を行っています。運用するにあたり、TensorFlowやKeras、Pandasなどのライブラリを活用し、データサイエンスの実践的な応用を示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージで定義されており: https://github.com/kaggle/docker-python
# 例えば、以下の便利なパッケージを読み込むことができます

import numpy as np # 線形代数用
import pandas as pd # データ処理、CSVファイルの入出力用 (例: pd.read_csv)

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると (実行をクリックするかShift+Enterを押すと)、入力ディレクトリ内の全ファイルのリストが表示されます

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

# 現在のディレクトリ (/kaggle/working/) に最大20GBまで書き込むことができ、 
# バージョンを作成する際に "Save & Run All" を使用すると出力として保存されます
# また、一時ファイルを/kaggle/temp/に書き込むこともできますが、 
# 現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
data=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.utils import to_categorical

# データを読み込む

# データの前処理
# ラベルを作成する
data['winner'] = data.apply(lambda row: 0 if row['winner_model_a'] == 1 else (1 if row['winner_model_b'] == 1 else 2), axis=1)

# カテゴリ変数のエンコード
label_encoder = LabelEncoder()
data['model_a'] = label_encoder.fit_transform(data['model_a'])
data['model_b'] = label_encoder.fit_transform(data['model_b'])

# 特徴量とターゲットの分割
X = data[['model_a', 'model_b', 'prompt', 'response_a', 'response_b']]
y = data['winner']

# テキストのベクトル化（単純なアプローチ：単語数をカウント）
X['prompt_len'] = X['prompt'].apply(lambda x: len(str(x).split()))
X['response_a_len'] = X['response_a'].apply(lambda x: len(str(x).split()))
X['response_b_len'] = X['response_b'].apply(lambda x: len(str(x).split()))
X = X.drop(['prompt', 'response_a', 'response_b'], axis=1)

# 特徴量を正規化する
scaler = StandardScaler()
X = scaler.fit_transform(X)

# ラベルをワンホットエンコード
y = to_categorical(y, num_classes=3)

# データを訓練セットとテストセットに分割する
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# モデルを構築する
model = Sequential()
model.add(Dense(128, input_dim=X_train.shape[1], activation='relu')) # 入力層
model.add(Dropout(0.1)) # ドロップアウト層
model.add(Dense(32, activation='relu')) # 隠れ層
model.add(Dropout(0.1)) # ドロップアウト層
model.add(Dense(16, activation='relu')) # 隠れ層
model.add(Dense(3, activation='softmax')) # 出力層（3クラスのソフトマックス）

# モデルをコンパイルする
model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])

# アーリーストッピングを定義する
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# モデルを訓練する
history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=100, batch_size=32, callbacks=[early_stopping])

# モデルを評価する
loss, accuracy = model.evaluate(X_test, y_test)
print(f"テスト精度: {accuracy * 100:.2f}%")

# モデルを保存する
model.save('chatbot_preference_model.h5')

# 訓練中の検証ロスを出力する
val_loss = history.history['val_loss']
print(f"検証ロス: {val_loss[-1]:.4f}")
```

---The following area is a Code cell (cell numver is 4)---
```python
import pandas as pd
import numpy as np
from tensorflow.keras.models import load_model
from sklearn.preprocessing import LabelEncoder, StandardScaler

# テストデータを読み込む
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

# 'model_a' と 'model_b' にデフォルト値を追加する（プレースホルダとして）
test['model_a'] = 'gpt-3.5-turbo-0613'  # 例のデフォルト値
test['model_b'] = 'gpt-3.5-turbo-0613'  # 例のデフォルト値

# 訓練データで使用したのと同じラベルエンコーダを使ってカテゴリ変数をエンコード
label_encoder = LabelEncoder()
test['model_a'] = label_encoder.fit_transform(test['model_a'])
test['model_b'] = label_encoder.fit_transform(test['model_b'])

# テキストのベクトル化（単純なアプローチ：単語数をカウント）
test['prompt_len'] = test['prompt'].apply(lambda x: len(str(x).split()))
test['response_a_len'] = test['response_a'].apply(lambda x: len(str(x).split()))
test['response_b_len'] = test['response_b'].apply(lambda x: len(str(x).split()))

# 元のテキストカラムを削除する
X_test = test.drop(['prompt', 'response_a', 'response_b'], axis=1)

# 訓練データで使用したのと同じスケーラーを使用して特徴量を正規化
# 訓練時にフィットさせたスケーラーを再利用
scaler = StandardScaler()
X_test_scaled = scaler.fit_transform(X_test[['model_a', 'model_b', 'prompt_len', 'response_a_len', 'response_b_len']])

# 訓練済みのモデルを読み込む
model = load_model('chatbot_preference_model.h5')

# 予測を行う
probs = model.predict(X_test_scaled)

# 提出ファイルの準備
submission = pd.DataFrame(probs, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
submission['id'] = test['id']

# 'id'を最初に持ってくるように列の順序を変更する
submission = submission[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

submission.to_csv('submission.csv', index=False)
print("提出ファイルが正常に生成されました！")
```

---The following area is a Code cell (cell numver is 5)---
```python
print(submission)
```

** @@@ Jupyter Notebook numver 80, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」に参加するために設計されており、2つの異なる言語モデルの応答の好ましさを予測する問題に取り組んでいます。

## 問題へのアプローチ
Notebookでは、CatBoostを用いて、チャットボットの応答の優劣を予測するモデルを構築します。このモデルにより、2つの異なるチャットボットの応答の中で、どちらがユーザーによって好まれるかを確率として出力します。データセットには、与えられたプロンプトに対してモデルが生成した複数の応答と、それに対するユーザーの選好が含まれています。

## 使用されている手法とライブラリ
1. **ライブラリ**:
   - **bitsandbytes**および**transformers**: 大規模言語モデルを効率的に扱うためのライブラリとして使用されています。
   - **CatBoost**: 決定木を基にした機械学習アルゴリズムであり、特にカテゴリカルデータに強いモデルを構築します。
   - **PyTorch**: 深層学習モデルやテンソル操作を行うためのフレームワークです。
   - **Pandas**および**NumPy**: データ操作や数値計算に使用されます。

2. **主な手順**:
   - 必要なライブラリのインストールとインポート。
   - 訓練データの読み込みと埋め込み特徴の取得。
   - ターゲット変数の整備と訓練データとテストデータの分割。
   - CatBoostClassifierのモデルをロードし、予測を行う準備。
   - Gemmaモデルの初期化とトークナイザーの設定。
   - テストデータの整形およびトークン化を行い、モデルに入力するための形式に変換。
   - スレッドを使用して埋め込みを計算し、最終的にCatBoostモデルを使って予測確率を取得。
   - 提出用のCSVファイルを生成するために、予測結果をサンプルデータフレームに設定し保存。

このNotebookは、最終的に提出用ファイル「submission.csv」を生成し、コンペティションで使用する予測確率を出力します。この一連の処理を通じて、2つのチャットボットの応答に対して、ユーザーの好みを予測するモデルを効果的に構築しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# bitsandbytesライブラリをインストールします。
# --no-indexオプションは、PyPIリポジトリを使用せず、指定したファイルシステムのリンクからのみインストールします。
!pip install -q -U bitsandbytes --no-index --find-links ../input/libs-install

# transformersライブラリをインストールします。
# 同様に、--no-indexオプションを使用して、指定したリンクからのみインストールします。
!pip install -q -U transformers --no-index --find-links ../input/libs-install
```

---The following area is a Code cell (cell numver is 2)---
```python
# 必要なライブラリをインポートします。
import os  # OSに関連する機能を提供するライブラリ
import gc  # ガーベジコレクタ（不要なオブジェクトをメモリから削除するためのライブラリ）
import re  # 正規表現を使用するためのライブラリ
from time import time  # 時間に関する機能を提供するライブラリ

import torch  # PyTorchライブラリ
import transformers  # Hugging Faceのtransformersライブラリ
import sklearn  # 機械学習のためのライブラリ
import random  # ランダムな数値を生成するためのライブラリ
import numpy as np  # 数値計算を行うためのライブラリ（NumPy）
import pandas as pd  # データ分析を行うためのライブラリ（Pandas）
import matplotlib.pyplot as plt  # グラフ描画を行うためのライブラリ

# モデルとトークナイザーをインポートします。
from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig

import time  # 再度timeライブラリをインポート（必要ない場合は削除可能）
from catboost import CatBoostClassifier, Pool  # CatBoostライブラリをインポート（決定木による分類器）
from sklearn.model_selection import train_test_split  # データを訓練セットとテストセットに分割する機能
from sklearn.metrics import accuracy_score, log_loss  # モデルの評価指標を計算するための機能

from torch.cuda.amp import autocast  # 自動混合精度を使用するための機能をインポート
from threading import Thread  # スレッドの管理を行うためのライブラリ

# CUDAのメモリアイデアを有効にする設定
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# GPUが使用可能か確認し、使用できない場合はメッセージを表示します。
if (not torch.cuda.is_available()):
    print("Sorry - GPU required!")  # GPUが必要であることを知らせるメッセージを表示します。
```

---The following area is a Code cell (cell numver is 3)---
```python
# 訓練データフレームをCSVファイルから読み込みます。
train_df = pd.read_csv('/kaggle/input/embedding/train_embed.csv')

# 訓練データの埋め込み（特徴量）をNumpy配列として読み込みます。
train_embed = np.load('/kaggle/input/embedding/gemma2_train_embed.npy')

# 各行のラベルを設定します。
# winner_model_a, winner_model_b, winner_tie の中で最大の値を持つ列を見つけ、その列のインデックスをラベルとして使用します。
train_df.loc[:, 'label'] = np.argmax(train_df[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  # どのモデルが勝つかを示すラベルを作成します。
```

---The following area is a Code cell (cell numver is 4)---
```python
# ターゲット変数の列名を定義します。
Targets = ['winner_model_a', 'winner_model_b', 'winner_tie']

# 予測対象のラベルを取得します。
y = train_df['label'].values

# 訓練データとテストデータに分割します。
# train_test_split関数を使用して、インデックスを訓練用とテスト用に分けます。
train_idx, test_idx = train_test_split(train_df.index, test_size=0.1, random_state=42, stratify=y)

# 訓練データとテストデータの特徴量とラベルを設定します。
X_train, y_train = train_embed[train_idx], train_df.iloc[train_idx]['label'].values  # 訓練データ
X_test, y_test = train_embed[test_idx], train_df.iloc[test_idx]['label'].values  # テストデータ

# 訓練データとテストデータの形状を出力します。
print(X_train.shape, y_train.shape)  # 訓練データの形状を出力
print(X_test.shape, y_test.shape)      # テストデータの形状を出力
```

---The following area is a Code cell (cell numver is 5)---
```python
# CatBoostClassifierのモデルを初期化します。
model_cb = CatBoostClassifier()

# 保存されたCatBoostモデルを読み込みます。
# 指定されたパスからモデルをロードします。
model_cb.load_model('/kaggle/input/catboost-mike/catboost.cbm')
```

---The following area is a Code cell (cell numver is 6)---
```python
# CatBoostモデルの情報を表示します。
# これにより、モデルの構造やパラメータ、トレーニング状況などの詳細を確認することができます。
model_cb
```

---The following area is a Code cell (cell numver is 7)---
```python
# モデルのパスと設定を定義します。
MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'  # Gemmaモデルの保存先パス
MAX_LENGTH = 1024  # モデルが処理する最大のシーケンス長
BATCH_SIZE = 2  # バッチサイズ

# 使用するGPUデバイスを指定します。
device0 = torch.device('cuda:0')  # 1つ目のGPUデバイス
device1 = torch.device('cuda:1')  # 2つ目のGPUデバイス

# トークナイザーをモデルからロードします。
tokenizer = GemmaTokenizer.from_pretrained(MODEL_PATH)

# 量子化の設定を定義します。
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビットモードで読み込みます。
    bnb_4bit_compute_dtype=torch.float16,  # 計算精度としてfloat16を使用します。
    bnb_4bit_use_double_quant=False)  # 二重量子化を使用しない設定

# 1つ目のGPUデバイスにモデルをロードします。
model_0 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",  # float16バージョンを使用
                                        device_map='cuda:0',  # GPUデバイス0にマッピング
                                        quantization_config=bnb_config_4bit)        

# 2つ目のGPUデバイスにモデルをロードします。
model_1 = Gemma2ForCausalLM.from_pretrained(MODEL_PATH,
                                        revision="float16",  # float16バージョンを使用
                                        device_map='cuda:1',  # GPUデバイス1にマッピング
                                        quantization_config=bnb_config_4bit)
```

---The following area is a Code cell (cell numver is 8)---
```python
# 入力文字列を処理する関数を定義します。
def process(input_str):
    # 文字列の先頭と末尾のブラケットを削除します。
    stripped_str = input_str.strip('[]')
    # カンマで区切られた文をリストに分割し、前後の空白を削除します。
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # 文が存在する場合は最後の文を返し、存在しない場合は空文字を返します。
    return sentences[-1] if sentences else ''

# テストデータをCSVファイルから読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# 'prompt', 'response_a', 'response_b' 列に対してprocess関数を適用します。
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# テキスト列を構成します。モデルの応答を整理した形式です。
test['text'] = '<start_of_turn>User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n----\n\nModel B:\n'  + test['response_b'] + '<end_of_turn><eos>'

# 最初のテキストを出力します。
print(test['text'][0])  # 生成された最初のテキストの形式を表示します。
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイザーを使用してテキストをトークン化します。
tokens = tokenizer(test['text'].tolist(),
                   padding='max_length',  # 最大長にパディングを行います。
                   max_length=MAX_LENGTH,  # 定義した最大長を適用します。
                   truncation=True,  # 長さが超過する場合は切り捨てます。
                   return_tensors='pt')  # PyTorchテンソルとして返します。

# テキストデータをDataFrameに変換します。
data = pd.DataFrame()
# トークンの入力IDと注意マスクをリストとしてDataFrameに追加します。
data['INPUT_IDS'] = [tensor.tolist() for tensor in tokens['input_ids']]
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in tokens['attention_mask']]

# 最初の2行を表示します。
data[:2]  # データの最初の2行を表示して内容を確認します。
```

---The following area is a Code cell (cell numver is 10)---
```python
# 埋め込みを取得する関数を定義します。
def get_embeddings(df, model, device, batch_size=BATCH_SIZE):  
    # INPUT_IDSとATTENTION_MASKSをテンソルに変換します。
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []  # 埋め込みのリストを初期化します。

    # バッチごとに処理を行います。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # 最後のバッチのインデックスを設定
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチの注意マスクをデバイスに転送
        
        # ガーベジコレクションを実施し、メモリをクリアします。
        gc.collect() 
        torch.cuda.empty_cache()

        # 勾配計算を行わずにモデルを通して埋め込みを取得します。
        with torch.no_grad():
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)
            embed = outputs.hidden_states[-1]  # 最後の隠れ層の出力を取得
            embed_mean = torch.mean(embed, dim=1).cpu()  # 平均プールを行い、CPUに戻します。
            embed_list.append(embed_mean)  # 得られた埋め込みをリストに追加
            
            torch.cuda.empty_cache()  # 再度メモリをクリア
        
    # リストの埋め込みを1つのテンソルに結合して返します。
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings  # 取得した埋め込みを返します。

# 埋め込みを計算し結果を保存する関数を定義します。
def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)  # 指定されたインデックスに埋め込み結果を格納します。
```

---The following area is a Code cell (cell numver is 11)---
```python
# 処理開始時刻を記録します。
st = time.time()

# データのサンプル数を取得します。
N_SAMPLES = len(data)
half = round(N_SAMPLES / 2)  # データを2つのサブセットに分けるためのインデックスを計算
sub1 = data.iloc[0:half].copy()  # 最初の半分のデータをコピー
sub2 = data.iloc[half:N_SAMPLES].copy()  # 残りの半分のデータをコピー

# 結果を格納する辞書を初期化します。
results = {}

# それぞれのスレッドを作成し、compute_embed関数を呼び出します。
t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))  # スレッド0
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))  # スレッド1

# スレッドを開始します。
t0.start()
t1.start()

# 両方のスレッドが完了するのを待ちます。
t0.join()
t1.join()

# 処理の完了時刻を表示します。
print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")  # 処理にかかった合計時間を表示します。
```

---The following area is a Code cell (cell numver is 12)---
```python
# 結果の埋め込みを結合します。
# 先ほどのスレッドで計算した埋め込みを1つのテンソルにまとめます。
test_embeddings = torch.cat([results[0], results[1]], dim=0)

# 結合した埋め込みの形状を出力します。
test_embeddings.shape  # テンソルの形状を表示して、埋め込みのサイズを確認します。
```

---The following area is a Code cell (cell numver is 13)---
```python
# ガーベジコレクションを実行して、不要なオブジェクトをメモリから削除します。
gc.collect()

# 使用し終わったモデルを削除します。
del model_1  # モデル1を削除
del model_0  # モデル0を削除

# GPUメモリをクリアします。
torch.cuda.empty_cache()  # 不要なメモリを解放します。
```

---The following area is a Code cell (cell numver is 14)---
```python
# CatBoostモデルを使用して予測確率を計算します。
# テストデータの埋め込みに対して、モデルが各クラスが選ばれる確率を予測します。
preds = model_cb.predict_proba(test_embeddings.numpy())

# 予測結果を表示します。
preds  # 各モデルの応答が選ばれる確率の配列を出力します。
```

---The following area is a Code cell (cell numver is 15)---
```python
# 提出用サンプルファイルをCSVから読み込みます。
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')

# 予測結果をサンプル提出用データフレームに設定します。
sample_sub[Targets] = preds  # 予測確率をターゲット列に代入します。

# サンプル提出用データを表示します。
display(sample_sub)  # 最終的な提出用データフレームを画面に表示します。
```

---The following area is a Code cell (cell numver is 16)---
```python
# 提出用データフレームをCSVファイルとして保存します。
sample_sub.to_csv('submission.csv', index=False)  # インデックスなしで'submission.csv'という名前で保存します。
```

---The following area is a Code cell (cell numver is 17)---
```python

```

** @@@ Jupyter Notebook numver 81, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおいて、ユーザーが生成した応答のどちらが好まれるかを予測するための機械学習モデルを構築することに取り組んでいます。具体的には、Siamese LSTM（Long Short-Term Memory）ネットワークを使用して、二つのチャットボットの応答を比較し、どちらのモデルが好まれるかを分類するタスクを行います。

### 問題の概要
- 目標は、ユーザーがどちらの応答（モデルAまたはモデルB）を選ぶか、または引き分けかを予測することです。この目的のために、提供されたトレーニングデータセットを使用して、3種類の出力（モデルAの勝利、モデルBの勝利、引き分け）を分類します。

### 使用されている手法とライブラリ
- **トランスフォーマーモデル**: `transformers`ライブラリを使用してBERTトークナイザーを利用しています。このトークナイザーは、テキストをトークン化してモデルに入力する際の前処理を行います。
- **データセットの準備**: `datasets`ライブラリを使用して、トレーニングデータセットとテストデータセットの読み込みと処理が行われます。
- **Siamese LSTMモデル**: PyTorchを用いてSiameseネットワークを実装し、二つの応答をLSTMで処理し、最終的に全結合層でクラスを予測します。モデルの構成には、埋め込み層、LSTM層、および出力層が含まれます。
- **データローダー**: DataLoaderを使用して、バッチ処理を行い、トレーニングデータとテストデータを効率的に処理します。
- **損失関数とオプティマイザー**: モデルの学習にはクロスエントロピー損失を用い、Adamオプティマイザーでパラメータを最適化します。

### トレーニングプロセス
- モデルは指定されたエポック数でトレーニングされ、各エポックで損失が計算され、学習が進む様子が表示されます。
- トレーニングの後、テストデータに対して推論を行い、各応答の勝者の確率を予測します。

### 結果の保存
- 最終的に、テストデータに対する予測結果をCSV形式で保存し、提出フォーマットに従った結果を生成します。予測結果には、各応答に対する確率（モデルAが勝つ、モデルBが勝つ、引き分け）が含まれています。

このNotebookは、機械学習モデルの設計、実装、トレーニング、評価、及び提出のための一連の作業を示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# 特にコードが記載されていないため、翻訳する内容がありません。
```

---The following area is a Code cell (cell numver is 2)---
```python
# transformersとdatasetsライブラリをインストールします。
# transformersライブラリは、トランスフォーマーベースのモデルの使用を容易にします。
# datasetsライブラリは、さまざまなデータセットを簡単にダウンロードして利用できるようにします。
pip install transformers datasets
```

---The following area is a Code cell (cell numver is 3)---
```python
import pandas as pd
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import BertTokenizer
import torch.nn as nn
from tqdm import tqdm
from sklearn.metrics import log_loss

# SiameseLSTMクラスを定義します
class SiameseLSTM(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers):
        super(SiameseLSTM, self).__init__()
        self.embedding = nn.Embedding(input_size, hidden_size)  # 入力層の埋め込み層
        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)  # LSTM層
        self.fc = nn.Linear(hidden_size, 3)  # 出力層：3クラス（モデルAが勝つ、モデルBが勝つ、引き分け）

    def forward_one(self, x):
        x = self.embedding(x)  # 入力を埋め込みに変換
        _, (h, _) = self.lstm(x)  # LSTMを通して隠れ状態を取得
        return h[-1]  # 最後の隠れ状態を返す

    def forward(self, x1, x2):
        h1 = self.forward_one(x1)  # 最初の入力の出力を取得
        h2 = self.forward_one(x2)  # 二つ目の入力の出力を取得
        return self.fc(torch.abs(h1 - h2))  # 二つの隠れ状態の絶対差を計算し、全結合層に渡す

# ステップ1：トレーニングデータセットを読み込む
df_train = pd.read_csv('/kaggle/input/datasetcomp/train.csv')

# 無効なケースをフィルタリングし、データを準備する
data = df_train[['response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie']].values

def determine_label(row):
    if row[2] == 1:
        return 0  # モデルAが勝つ
    elif row[3] == 1:
        return 1  # モデルBが勝つ
    elif row[4] == 1:
        return 2  # 引き分け
    else:
        return -1  # 無効または不明なケース

labels = [determine_label(row) for row in data if determine_label(row) != -1]  # ラベルをリストに格納
data = [row[:2] for row in data if determine_label(row) != -1]  # 有効なデータペアを取得

# ステップ2：カスタムDatasetクラスを定義します
class SiameseDataset(Dataset):
    def __init__(self, data, labels, tokenizer, max_length):
        self.data = data
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)  # データの長さを返す

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]
        label = self.labels[idx]

        # トークナイザーを使用してテキストをトークン化
        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク
            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク
            'label': torch.tensor(label, dtype=torch.long)  # ラベル
        }

# ステップ3：BERTトークナイザーを初期化します
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased/')
max_length = 128  # データセットに応じて調整

# ステップ4：トレーニング用のDatasetとDataLoaderのインスタンスを作成します
train_dataset = SiameseDataset(data, labels, tokenizer, max_length)
batch_size = 32  # バッチサイズを32に設定
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)  # データローダーを作成

# ステップ5：Siameseネットワークモデルを定義します
input_size = len(tokenizer)  # トークナイザーのサイズを入力サイズに設定
hidden_size = 300  # 隠れ層のサイズを300に設定
num_layers = 1  # LSTMのレイヤー数
model = SiameseLSTM(input_size, hidden_size, num_layers)

# ステップ6：損失関数とオプティマイザーを定義します
criterion = nn.CrossEntropyLoss()  # クロスエントロピー損失
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  # Adamオプティマイザー

# ステップ7：トレーニングループ
num_epochs = 5  # 必要に応じて調整
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  # GPUが利用可能であればGPUを使用
model.to(device)  # モデルをデバイスに転送

for epoch in range(num_epochs):
    model.train()  # モデルを訓練モードに設定
    total_loss = 0
    for batch in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()  # 勾配を初期化
        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得
        loss = criterion(outputs, labels)  # 損失を計算
        loss.backward()  # バックプロパゲーションで勾配を計算
        optimizer.step()  # オプティマイザーで重みを更新

        total_loss += loss.item()  # 累積損失を加算

    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}')  # エポックごとの損失を表示

# テストデータセットを読み込む
df_test = pd.read_csv('/kaggle/input/datasetcomp/test.csv')

# テスト用のカスタムDatasetクラスを定義します
class SiameseTestDataset(Dataset):
    def __init__(self, data, tokenizer, max_length):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)  # データの長さを返す

    def __getitem__(self, idx):
        pair = self.data[idx]
        response_a = pair[0]
        response_b = pair[1]

        # トークナイザーを使用してテキストをトークン化
        tokens_a = self.tokenizer(response_a, padding="max_length", truncation=True, max_length=self.max_length)
        tokens_b = self.tokenizer(response_b, padding="max_length", truncation=True, max_length=self.max_length)

        return {
            'input_ids_a': torch.tensor(tokens_a['input_ids']),  # モデルAの入力ID
            'attention_mask_a': torch.tensor(tokens_a['attention_mask']),  # モデルAのアテンションマスク
            'input_ids_b': torch.tensor(tokens_b['input_ids']),  # モデルBの入力ID
            'attention_mask_b': torch.tensor(tokens_b['attention_mask']),  # モデルBのアテンションマスク
        }

# テストデータを準備する
test_data = df_test[['response_a', 'response_b']].values.tolist()

# SiameseTestDatasetのインスタンスを作成
test_dataset = SiameseTestDataset(test_data, tokenizer, max_length)

# テストデータセットのデータローダーを作成
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# 保存したモデルを読み込む
model.load_state_dict(torch.load('/kaggle/input/datasetcomp/siamese_model.pth'))
model.eval()  # モデルを評価モードに設定

# テストデータに対して推論を行い、予測を生成する
all_preds = []
with torch.no_grad():  # 勾配計算を無効にする
    for batch in tqdm(test_loader, desc='Testing'):
        input_ids_a = batch['input_ids_a'].to(device)
        attention_mask_a = batch['attention_mask_a'].to(device)
        input_ids_b = batch['input_ids_b'].to(device)
        attention_mask_b = batch['attention_mask_b'].to(device)

        outputs = model(input_ids_a, input_ids_b)  # モデルに入力を渡して出力を取得
        probabilities = nn.Softmax(dim=1)(outputs)  # ソフトマックス関数を適用して確率を計算
        all_preds.extend(probabilities.cpu().numpy().tolist())  # 確率をリストに追加

# 予測結果のDataFrameを作成
pred_df = pd.DataFrame(all_preds, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])
pred_df['id'] = df_test['id']  # テストデータのIDを追加

# 必要な形式に列を再配置
pred_df = pred_df[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']]

# 提出用に予測結果をCSVに保存
pred_df.to_csv('submission.csv', index=False)
print(pred_df.head())  # 予測結果の先頭部分を表示
```

---The following area is a Code cell (cell numver is 4)---
```python
# 特にコードが記載されていないため、翻訳する内容がありません。
```

---The following area is a Code cell (cell numver is 5)---
```python
# 特にコードが記載されていないため、翻訳する内容がありません。
```

** @@@ Jupyter Notebook numver 82, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックでは、4ビット量子化された「Gemma-2 9b Instruct」モデルに基づくLoRAアダプターを使用した推論の手法が示されています。主な目的は、Chatbot Arenaコンペティションにおける、2つの言語モデル間のユーザー応答の選好予測を行うことです。

### 問題の扱い
ノートブックは、LoRAアダプターを用いて、量子化による誤差の影響を軽減しつつ、推論を迅速化する方法を探求しています。モデルをマージすることで誤差が生じる可能性があるため、LoRAアダプターを維持した状態での推論を推奨しています。また、モデルの性能は評価セットでの対数損失が0.9371、公開リーダーボードでの対数損失が0.941であると報告しています。

### 使用ライブラリと手法
ノートブックでは、主に以下のライブラリが使用されています:
- **Transformers**: 「Gemma2ForSequenceClassification」と「GemmaTokenizerFast」を使用してモデルの呼び出しやトークン化を実施。
- **Peft**: LoRAアダプターの適用に用いられます。
- **Torch**: GPU利用や自動混合精度計算をサポートし、モデルの推論が行われます。
- **PandasとNumPy**: データ処理と操作のためにデータフレームを使用し、結果の格納に役立てています。

### データ処理と推論
データはCSVファイルから読み込まれ、テキストの前処理を行った後、トークナイズされます。推論は、2つのGPUを用いてバッチごとに行われ、モデルAとモデルBの各々の勝率を計算します。結果として、モデルごとの勝率や引き分け確率をデータフレームに格納し、最終的に提出用のCSVファイルとして成果物を保存します。

全体として、このノートブックは、量子化されたモデルを最適に利用するための手法と細かな設定を提供し、効率的に予測を行うフレームワークを構築しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## このノートブックについて

これは、4ビット量子化された [Gemma-2 9b Instruct](https://blog.google/technology/developers/google-gemma-2/) と、私がアップロードしたスクリプトを使用してトレーニングしたLoRAアダプターを利用した推論ノートブックです [ここ](https://www.kaggle.com/code/emiz6413/gemma-2-9b-4-bit-qlora-finetune)で確認できます。
LoRAアダプターをベースモデルにマージすることで推論を速くすることもできますが、安易にそうすると無視できない量子化誤差が発生する可能性があります。そのため、私はLoRAアダプターをマージせずに維持することにしました。

## 結果

| サブセット | 対数損失 |
| - | - |
| 評価セット | 0.9371 |
| 公開LB | 0.941 |

提出には約4時間かかります。`max_length=2048`でTTAは使用していません。
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install transformers peft accelerate bitsandbytes \
    -U --no-index --find-links /kaggle/input/lmsys-wheel-files
```

---The following area is a Code cell (cell numver is 3)---
```python
import time
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor

import torch
import sklearn
import numpy as np
import pandas as pd
from transformers import Gemma2ForSequenceClassification, GemmaTokenizerFast, BitsAndBytesConfig
from transformers.data.data_collator import pad_without_fast_tokenizer_warning
from peft import PeftModel
```

---The following area is a Code cell (cell numver is 4)---
```python
assert torch.cuda.device_count() == 2
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
@dataclass
class Config:
    gemma_dir = '/kaggle/input/gemma-2/transformers/gemma-2-9b-it-4bit/1/gemma-2-9b-it-4bit'
    lora_dir = '/kaggle/input/73zap2gx/checkpoint-5748'
    max_length = 2048
    batch_size = 4
    device = torch.device("cuda")    
    tta = False  # テスト時のデータ拡張。<prompt>-<model-bの応答>-<model-aの応答>
    spread_max_length = False  # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するか

cfg = Config()
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# データの読み込みと前処理
```

---The following area is a Code cell (cell numver is 8)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')
```

---The following area is a Code cell (cell numver is 9)---
```python
def process_text(text: str) -> str:
    return " ".join(eval(text, {"null": ""}))

test.loc[:, 'prompt'] = test['prompt'].apply(process_text)
test.loc[:, 'response_a'] = test['response_a'].apply(process_text)
test.loc[:, 'response_b'] = test['response_b'].apply(process_text)

display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 11)---
```python
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    prompt = ["<prompt>: " + p for p in prompt]  # プロンプトに"<prompt>: "を追加
    response_a = ["\n\n<response_a>: " + r_a for r_a in response_a]  # 応答Aにプレフィックスを追加
    response_b = ["\n\n<response_b>: " + r_b for r_b in response_b]  # 応答Bにプレフィックスを追加
    if spread_max_length:  # spread_max_lengthがTrueの場合
        # 各要素をmax_length//3でトークナイズ
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # 各リストを結合
        attention_mask = [[1]* len(i) for i in input_ids]  # 各入力の長さに応じたアテンションマスクを作成
    else:
        # 各要素を結合してトークン化
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)  # トークナイズ
        input_ids = tokenized.input_ids  # トークンIDを取得
        attention_mask = tokenized.attention_mask  # アテンションマスクを取得
    return input_ids, attention_mask  # トークンIDとアテンションマスクを返す
```

---The following area is a Code cell (cell numver is 12)---
```python
%%time

tokenizer = GemmaTokenizerFast.from_pretrained(cfg.gemma_dir)  # Gemmaトークナイザーを読み込む
tokenizer.add_eos_token = True  # 終了トークンを追加
tokenizer.padding_side = "right"  # パディングの位置を右に設定

data = pd.DataFrame()
data["id"] = test["id"]
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])  # トークン化した結果をデータフレームに格納
data["length"] = data["input_ids"].apply(len)  # 各入力の長さを計算して追加

aug_data = pd.DataFrame()  # 拡張データ用のデータフレームを作成
aug_data["id"] = test["id"]
# response_aとresponse_bを入れ替える
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])  # トークナイズして格納
aug_data["length"] = aug_data["input_ids"].apply(len)  # 長さを計算
```

---The following area is a Code cell (cell numver is 13)---
```python
print(tokenizer.decode(data["input_ids"][0]))  # トークナイズしたデータの最初の要素をデコードして表示
```

---The following area is a Code cell (cell numver is 14)---
```python
print(tokenizer.decode(aug_data["input_ids"][0]))  # 拡張データの最初の要素をデコードして表示
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# モデルを読み込む
```

---The following area is a Code cell (cell numver is 16)---
```python
# GPU 0にベースモデルを読み込む
device_0 = torch.device('cuda:0')
model_0 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_0,
    use_cache=False,
)

# GPU 1にベースモデルを読み込む
device_1 = torch.device('cuda:1')
model_1 = Gemma2ForSequenceClassification.from_pretrained(
    cfg.gemma_dir,
    device_map=device_1,
    use_cache=False,
)
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
#### LoRAアダプターを読み込む
```

---The following area is a Code cell (cell numver is 18)---
```python
model_0 = PeftModel.from_pretrained(model_0, cfg.lora_dir)  # LoRAアダプターをモデル0に適用
model_1 = PeftModel.from_pretrained(model_1, cfg.lora_dir)  # LoRAアダプターをモデル1に適用
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
@torch.no_grad()  # 勾配計算を無効にする
@torch.cuda.amp.autocast()  # 自動混合精度を使って演算を行う
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝率と引き分けを記録するリスト
    
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終端インデックスを計算
        tmp = df.iloc[start_idx:end_idx]  # データフレームからバッチを取得
        input_ids = tmp["input_ids"].to_list()  # 入力IDを取得
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクを取得
        # トークナイザーを使ってデータをパディング
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding="longest",
            pad_to_multiple_of=None,
            return_tensors="pt",
        )
        outputs = model(**inputs.to(device))  # モデルで出力を計算
        proba = outputs.logits.softmax(-1).cpu()  # ロジットをソフトマックスで確率に変換
        
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝率をリストに追加
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝率をリストに追加
        tie.extend(proba[:, 2].tolist())  # 引き分けの確率をリストに追加
    
    df["winner_model_a"] = a_win  # モデルAの勝率をデータフレームに追加
    df["winner_model_b"] = b_win  # モデルBの勝率をデータフレームに追加
    df["winner_tie"] = tie  # 引き分けの確率をデータフレームに追加
    
    return df  # 更新されたデータフレームを返す
```

---The following area is a Code cell (cell numver is 21)---
```python
st = time.time()  # 処理開始時間を記録

# 入力の長さでソートして動的パディングを最大限に活用する
data = data.sort_values("length", ascending=False)
# sub_1とsub_2のトークン数がほぼ同じになるように分ける
sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータを選択
sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータを選択

with ThreadPoolExecutor(max_workers=2) as executor:
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

result_df = pd.concat(list(results), axis=0)  # 結果を結合
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝率の配列を取得

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 22)---
```python
st = time.time()

if cfg.tta:  # TTAが有効な場合
    data = aug_data.sort_values("length", ascending=False)  # 入力の長さでソートしてスピードを向上させる
    sub_1 = data.iloc[0::2].copy()  # 偶数番目のデータ
    sub_2 = data.iloc[1::2].copy()  # 奇数番目のデータ

    with ThreadPoolExecutor(max_workers=2) as executor:
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))  # 2つのモデルで推論実行

    tta_result_df = pd.concat(list(results), axis=0)  # TTAの結果を結合
    # TTAの順序が反転するので調整
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values 
    # 元の結果とTTA結果を平均
    proba = (proba + tta_proba) / 2

print(f"経過時間: {time.time() - st}")  # 処理時間を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝率を更新
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝率を更新
result_df.loc[:, "winner_tie"] = proba[:, 2]  # 引き分けの勝率を更新
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  # 提出用のデータフレームを作成
submission_df.to_csv('submission.csv', index=False)  # CSVファイルに保存
display(submission_df)  # 提出データを表示
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
---

# コメント 

> ## Cody_Null
> 
> 推論時間を速めるアイデアはありますか？パフォーマンスを失わずに。
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > LoRA_A x LoRA_Bを最初に計算したときにキャッシュするのは簡単な方法ですが、それほど速度向上は見込めないかもしれません。
> > 
> > TensorRTやvLLMのような最適化ライブラリを使えるのかも気になります。
> > 
> > 
> > > ## Cody_Null
> > > 
> > > vLLMを試したことがありますか？私は試しましたが、うまく動かす方法がわかりませんでした。 
> > > 
> > > 
> > > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > まだ試していません。max_lengthを増やすと対数損失が減少することは認識していますが、2048を越えると改善は非常に小さいです。私のケースでは2048から4096にすると対数損失が0.002減少しました。残りの時間で最適化できる他の方法を検討しています。
> > > 
> > > 

---

> ## carvingfate
> 
> 私は以前30位でしたが、このコードのおかげで私の努力が無駄に思えてしまいます。しかし、共有する精神を尊重しており、これがインターネットの精神であると思います。
> 
> 
> > ## jointcc2
> > 
> > 業界の状況もそうだと思います、一つのモデルが全ての過去の努力を上回りますね。
> > 
> > 

---

> ## Van chrn
> 
> なぜvLLMではなく？それはもっと速いかもしれません！
> 
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私はそれに取り組んでいます！
> > 
> > 
> > > ## Cody_Null
> > > 
> > > この使用方法を実現しましたか？私はvLLMを使ったことがなく、動作しているのを見てみたいです！
> > > 
> > > 

---

> ## Dai LinLing
> 
> 共有してくれてありがとう。これは私にとって非常に助けになり、理解も深まりました。
> 
> 

---

> ## Turbo
> 
> [@emiz6413](https://www.kaggle.com/emiz6413)   ノートブックを共有してくれてありがとう。
> 
> 

---

> ## Vitalii Bozheniuk
> 
> なぜシルバーティアの解決策を公開するのかわかりません。この0.88のノートブックを公開すれば、全員が1位になれるのですか？人々がアイデアやノートブックを共有するのは理解できますが、30位のノートブックを共有するのは意味がありません。競争とチャレンジの雰囲気が消えてしまいます。
> 
> 
> > ## G John Rao
> > 
> > まだ1ヶ月残っていますが、初心者にとってはブーストになります。経験豊富な専門家にとっては、1ヶ月は新しいアイデアを構築したり実装するには十分な時間です。 
> > 
> > 

---

> ## Korey Ma
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！私はいくつかの追加パラメータを微調整し、cv&lb(0.912&0.924)を達成しました。さらに良くするために他のトリックを試したいです😆
> 
> 
> > ## Yichuan Gao
> > 
> > もう少し詳細を共有していただけますか？ハイパーパラメータを調整しているのか、LoRAレイヤーやランクを増やしているのか？
> > 
> > > ## Korey Ma
> > > 
> > > "o_proj"と"gate_proj"を追加しただけです。 
> > > 

---

> ## samson
> 
> かなり良いノートブックを作成されていて、コメントに対する見解も妥当です。
> 
> ですが、なぜ重みを共有したのでしょう？真剣に学びたいと思っている人は、あなたの方法を使ったりそれを自分に適応したりするでしょう。しかし、あなたのせいで100以上のチームが盲目的にコピー+提出することになりました。これにより、中間にいる人々が適切なチームメートを見つけるのは不可能です。
> 
> 

---

> ## superferg
> 
> すごいですね。
> 
> 

---

> ## yuanzhe zhou
> 
> よくやりました！つまり、LLMを使うことが鍵ですか？BERTタイプのモデルは収束するには小さすぎるようです。
> 
> > ## Valentin Werner
> > 
> > まさにそうです。私もLLMがシーケンス生成で十分に微調整されていると思います。AI生成テキストをより認識し、テキストがどうあるべきかに最適化されているため、このタスクに適しています。これにより、基本的に火に火をもって戦うことになります。
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 実際、deberta-v3-smallを完全に微調整したところ、約1.1になりました。
> > > > BERTスタイルのエンコーダアーキテクチャは、理論的にはこれらの分類タスクにより適していると思います。
> > > しかし、あなたが指摘したように、実際にはLLMははるかに大きい（deberta v2 xxlargeは1.5B）ため、過学習を避けることができ、より多くのメモリ容量を持つことができます。
> > > もう一つの理由は、指示微調整という、非常に競技に似たデータを使用しているからかもしれません。
> > > 私はバニラgemma-2-9bでテストしたことはありませんが、どのように動作するかを見るのは興味深いです。
> > > 

---

> ## ano
> 
> [@emiz6413](https://www.kaggle.com/emiz6413) ノートブックをありがとう！微調整したモデルの検証データとcvスコアについて教えていただけますか？あなたのトレーニングノートブックに基づいて、私は行数を5で割った数に基づいて、約20％のデータを検証用に使用しました。その後、対数損失を計算しましたが、cvスコアは0.9未満でした。明らかに、検証データに間違いがあったため、cvスコアはあなたのトレーニングノートブックで書かれた0.9371よりも低くなりました。微調整モデルの検証データをどのように作成すればよいか教えていただけますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 私のトレーニングノートブックで検証データを作成するには、次の行が実行されないことを確認してください。
> > 
> > ```
> > # ds = ds.select(torch.arange(100))
> > 
> > ```
> > 
> > 次に、最後のセルにあるこの行で検証セットを選択する必要があります。
> > 
> > ```
> > ds.select(eval_idx)
> > 
> > ```
> > 
> > > ## ano
> > > 
> > > 返信ありがとうございます。もちろん、データを減らすための行を削除しました。
> > > 
> > > では、検証データはあなたのトレーニングノートブックでのn_splits = 5およびfold_idx = 0で選択されるのですね。うーん、そうすると、私のコードにCVスコアを計算する上での間違いがあったかもしれません。 
> > > > [UPDATE] バグを見つけました。ありがとうございます！
> > > 

---

> ## Guillermo Perez G
> 
> 素晴らしい！しかし、スコアをさらに下げることはできないと思います。それとも、ノートブックの質によるのでしょうか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > 現在のトップスコアは0.9未満です。スコアを改善するためのアイデアが残っていると思います。
> > 
> > > ## floriandev
> > > 
> > > Eisukeさん、素晴らしいノートブックをありがとう！あなたのノートブックを使うことで0.9未満になる可能性はありますか？
> > > 

---

> ## Sparsh Tewatia
> 
> 終了までにどれくらいの時間がかかりますか？時間が許せば、LLAMA3とGemma 2の2つのLLMのアンサンブルを行うことができます。
> 
> > ## Lorry Zou
> > 
> > ノートブックでは約4時間と記載されていますので、llama3とgemma2のアンサンブルは実行可能のようです。 
> > 
> > > ## Eisuke Mizutaniトピック作成者
> > > 
> > > 私はllama3とgemma2のアンサンブルを9時間以内で実行できました。max_length=2048およびper_device_batch_size=4を使用しました。
> > > 

---

> ## Lorry Zou
> 
> 私が行った全ての作業が無駄になりました…悲しい😅 でも素晴らしい作品です。
> 
> 

---

> ## Sam
> 
> 私はこのノートブックで提供されているのと同じ推論パラメータ（batch_size、max_length）を使ってGemmaモデルを試すことに決めました。（llamaではなくBert-likeモデルを使用している以外はすべて同じです）。このノートブックはT4x2で9時間以上かかっても終わらず、途中で止まってしまいます。
> 
> 25,000の例をGemmaモデルで処理すると、約17時間かかることがわかりました。
> 
> 何が問題かアドバイスをいただけますか？Gemmaモデルの推論を速くするにはどうすればいいですか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > [@andreysemenovmax](https://www.kaggle.com/andreysemenovmax) 
> > 
> > 8ビット重量とfp16アクティベーションでのGemma2 9bの動的量子化を行うと、提出のために4.5時間以内に実行されます。
> > 
> > 

---

> ## capyun007
> 
> Kaggleの初心者で質問があります。次のコードを実行してDataFrameをCSVファイルとして保存した後：
> 
> submission_df.to_csv('submission.csv', index=False)
> 
> submission.csvファイルはどこにありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > ノートブックをコミット&保存すると、出力タブに表示されるはずです。インタラクティブセッションで実行した場合は、作業ディレクトリ（./submission.csv）で確認できます。
> > 
> > > ## capyun007
> > > 
> > > 自分のノートブックをコミット&保存しましたが、出力タブに表示されません。
> > > 

---

> ## kanishka sriramoju
> 
> こんにちは、私はここで初心者です。あなたがKaggle入力ディレクトリから事前にトレーニングされたモデルを読み込んだことを見ました。この競技では、ノートブックが独立した環境で実行され、これらの作成されたディレクトリにアクセスできないということはありますか？
> 
> > ## Eisuke Mizutaniトピック作成者
> > 
> > それらのデータセットは公開にしているので、提出時にはアクセスできるはずです。
> > 
> >
```

** @@@ Jupyter Notebook numver 83, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、「Llama-3 8b」を使用してTPU上で大規模言語モデル（LLM）のトレーニングを実行することを目的としています。コンペティションの文脈では、異なるLLMの応答の好みを予測するために必要なデータ処理やモデルのトレーニング手法に焦点を当てています。

### 主な内容と手法:
1. **ライブラリのインポート**: 
   - トレーニングには、`transformers`, `torch`, `peft`, `torch_xla`などのライブラリが使用されます。これらは、MLモデルの構築、データ処理、およびTPUでの効率的なトレーニングに役立ちます。
  
2. **TPU環境の設定**:
   - TPU上での作業を行うため、TPU関連のモジュールがインポートされ、デバイスを初期化しています。

3. **データの前処理**:
   - KaggleのデータセットをCSVから読み込み、プロンプトと応答を整形します。無効なデータ（両方の応答が`null`の場合）は削除され、トークン長が計算されてデータフレームに追加されます。

4. **トークナイゼーション**:
   - トークン化に`AutoTokenizer`を利用し、テキストの長さを設定、パディングを施し、ラベルを取得します。

5. **モデルの構築**:
   - `LlamaForSequenceClassification`を使用して、3つのターゲットラベルに基づく分類モデルを構築します。また、LoRA（Low-Rank Adaptation）の手法を用いてモデルのパラメータ最適化を図ります。

6. **トレーニング実行**:
   - モデルを指定されたエポックでトレーニングし、損失関数として交差エントロピーを使用。学習率の調整やメトリクスの記録を行いながら、各エポックの進捗を追うことができます。

7. **モデルの保存**:
   - トレーニング後、モデルとオプティマイザの状態を保存し、後の利用や再トレーニングに備えます。

8. **ビジュアライゼーション**:
   - トレーニング過程での損失をプロットし、モデルの学習の進捗を評価する手段を提供しています。

### 結論:
このノートブックは、TPUを使用したLLMトレーニングに特化しており、効率的なデータ処理、モデル構築、トレーニング手法を整備しています。トレーニングのスピードや最適化の余地があることを指摘し、さらなる改善のための方向性を示しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Llama-3 8b [TPUトレーニング]

TPU上での大規模言語モデル（LLM）のトレーニングを学びます。これがあなたにも役に立つことを願っています！

このノートブックは以下のリソースに触発されました：

* [LLM detect AI competition Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [トレーニング]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (トレーニング)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)

前提条件：Llama-3を使用するためのアクセス

注意：これはトレーニング用のノートブックであり、推論用のノートブックは[こちら](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)で見つけることができます。

学びがあったり、役立つと感じた場合は、ぜひ投票してください！

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
# ライブラリのインストール
!pip install -qq peft==0.6.0  # peftライブラリのインストール
!pip install -qq bitsandbytes==0.41.1  # bitsandbytesライブラリのインストール
!pip install -qq accelerate==0.24.1  # accelerateライブラリのインストール
!pip install -qq transformers==4.35.0  # transformersライブラリのインストール
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q  # CPU版のPyTorchのインストール
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q  # TPU用のtorch_xlaのインストール
!pip uninstall -qq tensorflow -y  # TensorFlowがTPUを占有して権限エラーを引き起こすため、アンインストールします
!cp /kaggle/input/utils-xla/spmd_util.py .  # このリポジトリからのファイルコピー：https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 3)---
```python
import os  # osモジュールのインポート
import gc  # ガーベジコレクション用モジュールのインポート
import re  # 正規表現操作用モジュールのインポート
from time import time  # 時間関連の関数用にtimeをインポート
import random  # 乱数生成用モジュールのインポート
import warnings  # 警告メッセージの管理用モジュールのインポート
import numpy as np  # 数値計算用のライブラリNumPyのインポート
import pandas as pd  # データ操作用のライブラリPandasのインポート
import matplotlib.pyplot as plt  # グラフ描画用のmatplotlibのインポート
from tqdm.auto import tqdm  # プログレスバー表示用のtqdmをインポート

import torch  # PyTorchのインポート
import transformers  # transformersライブラリのインポート
from sklearn.metrics import accuracy_score  # 精度スコア評価のための関数インポート
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification  # トークナイザーとモデルのインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # Peft関連のモジュールをインポート
import torch.nn.functional as F  # PyTorchの関数型APIをインポート

import torch_xla.debug.profiler as xp  # XLAのデバッグ用プロファイラーインポート
import torch_xla.core.xla_model as xm  # XLAモデル用モジュールをインポート
import torch_xla.experimental.xla_sharding as xs  # XLAシャーディング用モジュールのインポート
import torch_xla.runtime as xr  # XLA実行時間用モジュールのインポート

xr.use_spmd()  # SPMD（Single Program Multiple Data）を使用するように設定

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor  # XLAシャーディングされたテンソル用のインポート
from torch_xla.experimental.xla_sharding import Mesh  # XLAシャーディングのためのメッシュをインポート
from spmd_util import partition_module  # モジュールのパーティション分割用ユーティリティのインポート

tqdm.pandas()  # Pandas DataFrameとの互換性を持たせるための設定

print(f'Torch Version: {torch.__version__}')  # 現在のTorchのバージョンを表示します
```

---The following area is a Code cell (cell numver is 4)---
```python
# ライブラリのインポート
# import os  # osモジュールのインポート
# import gc  # ガーベジコレクション用モジュールのインポート
# import re  # 正規表現操作用モジュールのインポート
# from time import time  # 時間関連の関数用にtimeをインポート
# import random  # 乱数生成用モジュールのインポート
# import warnings  # 警告メッセージの管理用モジュールのインポート
# import numpy as np  # 数値計算用のライブラリNumPyのインポート
# import pandas as pd  # データ操作用のライブラリPandasのインポート
# import matplotlib.pyplot as plt  # グラフ描画用のmatplotlibのインポート
# from tqdm.auto import tqdm  # プログレスバー表示用のtqdmをインポート

# import torch  # PyTorchのインポート
# import transformers  # transformersライブラリのインポート
# from sklearn.metrics import accuracy_score  # 精度スコア評価のための関数インポート
# from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification  # トークナイザーとモデルのインポート
# from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # Peft関連のモジュールをインポート
# import torch.nn.functional as F  # PyTorchの関数型APIをインポート

# import torch_xla.debug.profiler as xp  # XLAのデバッグ用プロファイラーインポート
# import torch_xla.core.xla_model as xm  # XLAモデル用モジュールをインポート
# import torch_xla.experimental.xla_sharding as xs  # XLAシャーディング用モジュールのインポート
# import torch_xla.runtime as xr  # XLA実行時間用モジュールのインポート

# xr.use_spmd()  # SPMD（Single Program Multiple Data）を使用するように設定

# from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor  # XLAシャーディングされたテンソル用のインポート
# from torch_xla.experimental.xla_sharding import Mesh  # XLAシャーディングのためのメッシュをインポート
# from spmd_util import partition_module  # モジュールのパーティション分割用ユーティリティのインポート

# tqdm.pandas()  # Pandas DataFrameとの互換性を持たせるための設定

# print(f'Torch Version: {torch.__version__}')  # 現在のTorchのバージョンを表示します
# /usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html
#   from .autonotebook import tqdm as notebook_tqdm
# /usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.
#   warn("The installed version of bitsandbytes was compiled without GPU support. "
# /usr/local/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cpu.so: undefined symbol: cadam32bit_grad_fp32
# Torch Version: 2.1.2+cpu

# 設定
# class CFG:
#     NUM_EPOCHS = 1  # エポック数
#     BATCH_SIZE = 1  # バッチサイズ（以前は16）
#     DROPOUT = 0.05  # ドロップアウト率
#     MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのパス
#     SEED = 2024  # 乱数シード
#     MAX_LENGTH = 1024  # 最大シーケンス長
#     NUM_WARMUP_STEPS = 128  # ウォームアップステップ数
#     LR_MAX = 5e-5  # 最大学習率
#     NUM_LABELS = 3  # ラベル数
#     LORA_RANK = 4  # LoRAのランク
#     LORA_ALPHA = 8  # LoRAのアルファ値
#     LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモジュール
    
# DEVICE = xm.xla_device()  # TPUデバイスの初期化
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    NUM_EPOCHS = 1  # エポック数
    BATCH_SIZE = 1  # バッチサイズ（以前は16だった）
    DROPOUT = 0.05  # ドロップアウト率
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのパス
    SEED = 2024  # 乱数シード
    MAX_LENGTH = 1024  # 最大シーケンス長
    NUM_WARMUP_STEPS = 128  # ウォームアップステップ数
    LR_MAX = 5e-5  # 最大学習率
    NUM_LABELS = 3  # ラベルの数
    LORA_RANK = 4  # LoRAのランク
    LORA_ALPHA = 8  # LoRAのアルファ値
    LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモジュールのリスト
    
DEVICE = xm.xla_device()  # TPUデバイスの初期化
```

---The following area is a Code cell (cell numver is 7)---
```python
def set_seeds(seed):
    """再現性のためのシードを設定します"""
    os.environ['PYTHONHASHSEED'] = str(seed)  # Pythonのハッシュシードを設定
    random.seed(seed)  # randomモジュールのシードを設定
    np.random.seed(seed)  # NumPyのシードを設定
    torch.manual_seed(seed)  # PyTorchのシードを設定
    if torch.cuda.is_available():  # GPUが利用可能な場合
        torch.cuda.manual_seed(seed)  # CUDA用のシードを設定
        torch.cuda.manual_seed_all(seed)  # 全てのCUDAデバイスにシードを設定
        
    # 全TPUコアのためにシードを設定
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)  # 設定したシードを使ってシードを設定
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# トークナイザー
```

---The following area is a Code cell (cell numver is 9)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)  # 指定したモデル名からトークナイザーをロード
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークンに設定
tokenizer.padding_side = 'right'  # パディングを右側に設定
tokenizer.add_eos_token = True  # 終了トークンを追加する設定

# 推論時にオフラインでロードできるようにトークナイザーを保存
tokenizer.save_pretrained('tokenizer')  # トークナイザーを指定したディレクトリに保存します
```

---The following area is a Code cell (cell numver is 10)---
```python
# トークン長を取得するユーティリティ関数
def get_token_lengths(texts):
    # テキストをトークナイズし、各テキストのinput_idsを取得
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # 各テキストのinput_idsの長さを返す
    return [len(t) for t in input_ids]  # 各トークンの長さをリストとして返します
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# トレーニングデータの準備
```

---The following area is a Code cell (cell numver is 12)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータのCSVファイルを読み込む
def process(input_str):
    stripped_str = input_str.strip('[]')  # 角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文章を分割し、各文章の余分な引用符を取り除く
    return ' '.join(sentences)  # 文章をスペースで結合して返す

train.loc[:, 'prompt'] = train['prompt'].apply(process)  # プロンプト列を処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # 応答A列を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # 応答B列を処理

# トレーニングに向けて 'Null' を削除
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 応答Aと応答Bが両方 'null' の行を見つける
train.drop(indexes, inplace=True)  # 該当する行を削除
train.reset_index(inplace=True, drop=True)  # インデックスをリセット

print(f"総計 {len(indexes)} 行のNull応答が削除されました")
print('トレーニングサンプルの合計: ', len(train))  # 残ったトレーニングサンプルの数を表示
```

---The following area is a Code cell (cell numver is 13)---
```python
train.head(5)  # トレーニングデータの最初の5行を表示します
```

---The following area is a Code cell (cell numver is 14)---
```python
train['text'] = 'User prompt: ' + train['prompt'] + '\n\nModel A:\n' + train['response_a'] + '\n\n--------\n\nModel B:\n' + train['response_b']  # 各行にユーザープロンプトとモデルの応答を結合
print(train['text'][4])  # 4番目のテキストを表示します
```

---The following area is a Code cell (cell numver is 15)---
```python
# トレーニングデータセットの50%のみを使用してトレーニング
# train = train[:int(len(train) * 0.5)]  # 最初の50%を使用する場合（コメントアウト）
train = train[int(len(train) * 0.5):]  # データセットの後半を使用

# データを2つの半分に分割
first_half = train[:int(len(train) * 0.5)]  # 最初の半分を取得

first_half.loc[:, 'token_count'] = get_token_lengths(first_half['text'])  # 最初の半分のテキストのトークン数をカウント
first_half.loc[:, 'label'] = np.argmax(first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  # 勝者モデルのインデックスを取得

train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # トレーニングデータ全体のトークン数をカウント

# モデルのラベルを準備し、最大インデックスを取得
train.loc[:, 'label'] = np.argmax(train[['winner_model_a', 'winner_model_b', 'winner_tie']].values, axis=1)  # 勝者モデルのインデックスを取得

# データを表示
display(train.head())  # トレーニングデータの最初の数行を表示します
```

---The following area is a Code cell (cell numver is 16)---
```python

```

---The following area is a Code cell (cell numver is 17)---
```python
train.label.value_counts()  # ラベルの値カウントを表示します
```

---The following area is a Code cell (cell numver is 18)---
```python
# トークン数の統計を表示
display(train['token_count'].describe().to_frame().astype(int))  # トークン数の統計情報を整数型のデータフレームとして表示します
```

---The following area is a Code cell (cell numver is 19)---
```python
# データの90%をカバーするトークンの長さを取得しますが、1024の長さを維持します！
np.percentile(train['token_count'], 90)  # トークン数の90パーセンタイルを取得します
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 21)---
```python
# データをトークナイズします
tokens = tokenizer(
    train['text'].tolist(),  # テキストをリストとしてトークナイズ
    padding='max_length',  # 最大長でパディング
    max_length=CFG.MAX_LENGTH,  # 最大長を設定
    truncation=True,  # 長すぎるテキストは切り詰める
    return_tensors='np')  # NumPyテンソルとして返す

# 入力IDはトークンIDです
INPUT_IDS = tokens['input_ids']  # トークンIDを取得
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得
# テキストのラベル
LABELS = train[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # ラベルを取得

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を表示
print(f'LABELS shape: {LABELS.shape}')  # ラベルの形状を表示
```

---The following area is a Code cell (cell numver is 22)---
```python
tokens = tokenizer(
    first_half['text'].tolist(),  # 最初の半分のテキストをリストとしてトークナイズ
    padding='max_length',  # 最大長でパディングを行う
    max_length=CFG.MAX_LENGTH,  # 最大長を設定
    truncation=True,  # 長すぎるテキストは切り捨てる
    return_tensors='np'  # NumPyテンソルとして返す
)

INPUT_IDS = tokens['input_ids']  # トークンIDを取得
ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得
LABELS = first_half[['winner_model_a', 'winner_model_b', 'winner_tie']].values  # ラベルを取得
print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を表示
print(f'LABELS shape: {LABELS.shape}')  # ラベルの形状を表示
```

---The following area is a Code cell (cell numver is 23)---
```python
def train_dataset(batch_size):
    N_SAMPLES = LABELS.shape[0]  # サンプル数を取得
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))  # バッチサイズに基づいてインデックスを作成
    while True:
        # インデックスをシャッフル
        np.random.shuffle(IDXS)
        # 全インデックスを一度だけイテレート
        for idxs in IDXS.reshape(-1, batch_size):  # バッチサイズに分割
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)  # 入力IDをデバイスに送る
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)  # アテンションマスクをデバイスに送る
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # ラベルをデバイスに送る（マルチラベル出力）
            
            # TPUノードにシャーディングを行う（適用可能であれば、メッシュを適切に定義する必要があります）
            xs.mark_sharding(input_ids, mesh, (0, 1))  # 入力IDのシャーディングマーク
            xs.mark_sharding(attention_mask, mesh, (0, 1))  # アテンションマスクのシャーディングマーク
            xs.mark_sharding(labels, mesh, (0, 1))  # ラベルのシャーディングマーク
            
            yield input_ids, attention_mask, labels  # バッチを生成

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)  # トレーニングデータセットを作成
```

---The following area is a Code cell (cell numver is 24)---
```python
# 最初の半分のデータセット生成器を定義します
def train_dataset_first_half(batch_size):
    N_SAMPLES = LABELS.shape[0]  # サンプル数を取得
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))  # バッチサイズに基づいてインデックスを作成
    while True:
        np.random.shuffle(IDXS)  # インデックスをシャッフル
        for idxs in IDXS.reshape(-1, batch_size):  # バッチサイズに分割
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)  # 入力IDをデバイスに送る
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)  # アテンションマスクをデバイスに送る
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # ラベルをデバイスに送る
            xs.mark_sharding(input_ids, mesh, (0, 1))  # 入力IDのシャーディングマーク
            xs.mark_sharding(attention_mask, mesh, (0, 1))  # アテンションマスクのシャーディングマーク
            xs.mark_sharding(labels, mesh, (0, 1))  # ラベルのシャーディングマーク
            yield input_ids, attention_mask, labels  # バッチを生成

TRAIN_DATASET = train_dataset_first_half(CFG.BATCH_SIZE)  # 最初の半分のトレーニングデータセットを作成

# 最初の半分のためのエポックごとのステップ数を計算
STEPS_PER_EPOCH = len(first_half) // CFG.BATCH_SIZE  # エポックごとのステップ数を計算
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# モデルの読み込み
```

---The following area is a Code cell (cell numver is 26)---
```python
# 3つのターゲットラベルを用いた分類用のモデルを読み込みます
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,  # 指定したモデル名から読み込む
    num_labels=CFG.NUM_LABELS,  # ラベルの数を指定
    torch_dtype=torch.bfloat16)  # PyTorchのデータ型を指定

base_model.config.pretraining_tp = 1  # 事前トレーニングのTPを1に設定

# パディングトークンを設定
base_model.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定
```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# ローレンジ適応（LORA）
```

---The following area is a Code cell (cell numver is 29)---
```python
lora_config = LoraConfig(
    r=CFG.LORA_RANK,  # ローランク行列の次元
    lora_alpha=CFG.LORA_ALPHA,  # LoRAのアクティベーションと事前トレーニングされた重みアクティベーションのスケーリング係数
    lora_dropout=CFG.DROPOUT,  # ドロップアウト率
    bias='none',  # バイアスの設定
    inference_mode=False,  # 推論モードの設定
    task_type=TaskType.SEQ_CLS,  # タスクタイプをシーケンス分類に設定
    target_modules=CFG.LORA_MODULES  # 出力と値のプロジェクションのみを使用
)
```

---The following area is a Code cell (cell numver is 30)---
```python
# LoRAモデルを作成
model = get_peft_model(base_model, lora_config)  # 基本モデルとLoRA設定からLoRAモデルを生成
# 学習可能なパラメータを表示
model.print_trainable_parameters()  # 学習可能なパラメータの情報を表示します
```

---The following area is a Code cell (cell numver is 31)---
```python
# TPUノードの数を取得
num_devices = xr.global_runtime_device_count()  # グローバルなランタイムデバイスの数を取得
mesh_shape = (1, num_devices, 1)  # メッシュの形状を定義
device_ids = np.array(range(num_devices))  # デバイスIDの配列を作成
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))  # メッシュを作成

# モデルを分割
partition_module(model, mesh)  # モデルをメッシュに分配

print(f'num_devices: {num_devices}')  # 使用するデバイスの数を表示
```

---The following area is a Code cell (cell numver is 32)---
```python
# 学習可能な層を確認します
MODEL_LAYERS_ROWS = []  # モデル層情報を格納するリスト
TRAINABLE_PARAMS = []  # 学習可能なパラメータを格納するリスト
N_TRAINABLE_PARAMS = 0  # 学習可能なパラメータの総数

# モデルの各パラメータをループ
for name, param in model.named_parameters():
    # レイヤーのパラメータ数を計算
    n_parameters = int(torch.prod(torch.tensor(param.shape)))
    # 学習可能な層のみ
    if param.requires_grad:
        # 層の情報を追加
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,  # パラメータ数
            'name': name,  # レイヤー名
            'dtype': param.data.dtype,  # データ型
        })
        # 学習可能なパラメータを追加
        TRAINABLE_PARAMS.append({'params': param})
        # 学習可能なパラメータの数を加算
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))  # モデル層情報のデータフレームを表示

print(f"""
===============================
N_TRAINABLE_PARAMS: {N_TRAINABLE_PARAMS:,}  # 学習可能なパラメータの総数を表示
N_TRAINABLE_LAYERS: {len(TRAINABLE_PARAMS)}  # 学習可能な層の数を表示
===============================
""")
```

---The following area is a Markdown cell (cell numver is 33)---
```markdown
# トレーニング
```

---The following area is a Code cell (cell numver is 34)---
```python
# 学習率とオプティマイザーの設定
N_SAMPLES = len(train)  # トレーニングサンプルの数を取得
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE  # エポックごとのステップ数を計算

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)  # AdamWオプティマイザーを設定

# ウォームアップ付きコサイン学習率スケジューラ
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,  # オプティマイザーを指定
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,  # ウォームアップステップ数を指定
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS  # 総トレーニングステップ数を計算
)

print(f'BATCH_SIZE: {CFG.BATCH_SIZE}, N_SAMPLES: {N_SAMPLES}, STEPS_PER_EPOCH: {STEPS_PER_EPOCH}')  # バッチサイズ、サンプル数、エポックごとのステップ数を表示
```

---The following area is a Code cell (cell numver is 35)---
```python
# オプティマイザーの状態（例えば、モーメンタムバッファ）のデータ型を設定
for state in OPTIMIZER.state.values():  # オプティマイザーの状態をループ
    for k, v in state.items():  # 各状態のキーと値をループ
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:  # 値がテンソルで、データ型がfloat32でない場合
            state[v] = v.to(dtype=torch.float32)  # データ型をfloat32に変換
```

---The following area is a Code cell (cell numver is 36)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)  # トレーニングデータセットから次のバッチを取得

print(f'input_ids shape: {input_ids.shape}, dtype: {input_ids.dtype}')  # 入力IDの形状とデータ型を表示
print(f'attention_mask shape: {attention_mask.shape}, dtype: {attention_mask.dtype}')  # アテンションマスクの形状とデータ型を表示
print(f'labels shape: {labels.shape}, dtype: {labels.dtype}')  # ラベルの形状とデータ型を表示
```

---The following area is a Code cell (cell numver is 37)---
```python
%%time
# ダミー予測
with torch.no_grad():  # 勾配計算を行わない設定
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # モデルに入力IDとアテンションマスクを与えて出力を取得
    
print(f'logits: {outputs.logits}, dtype: {outputs.logits.dtype}')  # ロジットとそのデータ型を表示
```

---The following area is a Code cell (cell numver is 38)---
```python
# モデルをトレーニングモードに設定
model.train()

# 損失関数、交差エントロピー
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)  # 交差エントロピー損失関数をfloat32型に設定
```

---The following area is a Code cell (cell numver is 39)---
```python
st = time()  # 開始時間を記録
warnings.filterwarnings("error")  # 警告をエラーとして表示
METRICS = {
    'loss': [],  # 損失を格納するリスト
    'accuracy': {'y_true': [], 'y_pred': []}  # 正解ラベルと予測ラベルを格納する辞書
}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):  # エポックをループ
    ste = time()  # 各エポックの開始時間を記録
    for step in range(STEPS_PER_EPOCH):  # 各エポックのステップをループ
        # 勾配をゼロに設定
        OPTIMIZER.zero_grad()
        
        # バッチを取得
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # フォワードパス
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # ロジットをfloat32に設定
        logits = outputs.logits.to(dtype=torch.float32)
        
        # バックワードパス
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))  # 損失を計算
        loss.backward()  # 勾配を計算
        
        # オプティマイザステップ
        OPTIMIZER.step()  # パラメータを更新
        xm.mark_step()  # TPUステップをマーク
        
        # 学習率スケジューラを更新
        lr_scheduler.step()
        
        # メトリクスとプログレスバーを更新
        METRICS['loss'].append(float(loss))  # 損失を追加
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()  # 正解ラベルを追加
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()  # 予測ラベルを追加
        
        if (step + 1) % 200 == 0:  # 200ステップごとにメトリクスを表示
            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])  # 最新の損失を追加
            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))  # 精度を計算
            lr = OPTIMIZER.param_groups[0]['lr']  # 学習率を取得
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nSteps per epoch: {step+1} complete | Time elapsed: {time()- st}')  # 経過時間を表示
    
    print(f'\nEpoch {epoch+1} Completed | Total time for epoch: {time() - ste} ' )  # エポックの完了時間を表示

    # 停止した場合、TPUで将来的にトレーニングを続けるためにモデルとオプティマイザを保存
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')  # モデルを保存
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')  # オプティマイザを保存
    
    print(f'Model saved at epoch {epoch+1}| Elapsed time: {time() - st} ')  # モデル保存のメッセージを表示
```

---The following area is a Code cell (cell numver is 40)---
```python
plt.figure(figsize=(15, 6))  # グラフのサイズを設定
plt.plot(METRICS['loss'])  # 損失のプロット
plt.xlabel('Step per epoch')  # x軸のラベル
plt.ylabel('Loss')  # y軸のラベル
plt.title('Loss Plot step per epoch')  # グラフのタイトル
plt.show()  # グラフを表示
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# モデルの保存
```

---The following area is a Code cell (cell numver is 42)---
```python
model = model.cpu()  # モデルをCPUに移動
torch.save(dict([(k, v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')  # 学習可能なパラメータを保存
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
# 結論

トレーニングの速度を上げて最適化する余地はまだたくさんあります！ より多くのデータや異なるバッチサイズ、学習率を試してみてください... すべての成功を祈ります！
```

** @@@ Jupyter Notebook numver 84, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、ユーザーの好みを予測する問題に取り組んでいます。具体的には、複数の大規模言語モデル（LLM）からの応答に対して、どの応答がより好まれるかを予測するためのモデルを構築しています。

### 主な内容と手法

1. **データの前処理**: 
   - `pandas`を用いてテストデータを読み込み、各プロンプトと応答のテキストを処理する関数を定義します。この処理では、特定の文字を除去し、テキストをクレンジングしてから新しいカラムに格納しています。

2. **カスタムデータセットの作成**:
   - `torch.utils.data.Dataset`を継承した`Senmamtic_news`クラスを作成し、プロンプトと応答をトークナイズ（BERTトークナイザーを使用）して、モデルへの入力形式に整えています。
   - 応答が長すぎる場合には適切にトリミングを行う処理を実装しています。

3. **データローダーの取得**:
   - データローダーは、バッチ処理およびデータのシャッフルを行うための関数`get_semantic_data_loader`を通じて取得され、トレーニングとテストの両方のモードに対応しています。

4. **モデルの定義と推論**:
   - `BertPET`クラスとして、事前学習済みのBERTモデルを利用したマスク言語モデルを定義しています。
   - モデルを使用して推論を行い、得られた確率的出力を`inference`関数で計算します。

5. **出力結果の生成**:
   - 最終的に、テストデータに対する予測結果を保存するために、提出用のCSVファイル`submission.csv`を作成しています。出力フォーマットはコンペティションで要求される形式に従っています。

### 使用ライブラリ
- `transformers`（BERTモデル用）
- `torch`（PyTorch）
- `pandas`（データ処理）
- `tqdm`（進捗表示）

このノートブックは、言語モデルとユーザーの選好を結びつけるための強力な機械学習技術を活用しており、チャットボットの応答の質を向上させるための具体的なアプローチを示しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
from itertools import zip_longest
from tqdm import tqdm
from sklearn.model_selection import StratifiedKFold
from transformers import BertTokenizer, AutoTokenizer
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence
import torch
import pandas as pd
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# データを処理する関数
def process(example):
    # プロンプトの文をリストに変換し、特定の文字を取り除く
    sentences = [s.strip('"').replace('[MASK]','') for s in example['prompt'].strip('[]').split('","')]
    # 応答Aの文をリストに変換し、特定の文字を取り除く
    sentences_a = [s.strip('"').replace('[MASK]','') for s in example['response_a'].strip('[]').split('","')]
    # 応答Bの文をリストに変換し、特定の文字を取り除く
    sentences_b = [s.strip('"').replace('[MASK]','') for s in example['response_b'].strip('[]').split('","')]
    # プロンプトと応答Aの文を組み合わせ、空の文は除外
    texts_a = [p for pair in zip_longest(sentences, sentences_a, fillvalue='') for p in pair if p]
    # プロンプトと応答Bの文を組み合わせ、空の文は除外
    texts_b = [p for pair in zip_longest(sentences, sentences_b, fillvalue='') for p in pair if p]
    # プロセス結果をデータシリーズとして返す
    return pd.Series([' '.join(sentences), ' '.join(sentences_a), ' '.join(sentences_b), '\n'.join(texts_a), '\n'.join(texts_b)], index=['prompt', 'response_a', 'response_b', 'text_a','text_b'])

# データフレームに対して処理関数を適用
test[['prompt', 'response_a', 'response_b', 'text_a','text_b']] = test.apply(process, axis=1)
# 欠損値を含む行を削除
test = test.dropna()

# 自作データセットクラス
class Senmamtic_news(Dataset):
    def __init__(self, data, tokenizer):
        self.data = data
        self.tokenizer = tokenizer
        self.sep_token = tokenizer.sep_token
        self.mask_token = tokenizer.mask_token

        prompt_new = []
        # プロンプトのトークナイジング
        tk0 = tqdm(data['prompt'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            # プロンプトが長すぎる場合、前半と後半を結合する
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            prompt_new.append(text)
        
        print(f'== response_a ==')
        response_a_new = []
        # 応答Aのトークナイジング
        tk0 = tqdm(data['response_a'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_a_new.append(text)

        print(f'== response_b ==')
        response_b_new = []
        # 応答Bのトークナイジング
        tk0 = tqdm(data['response_b'].fillna("").values, total=len(data))
        for text in tk0:
            length = len(tokenizer(text, add_special_tokens=False)['input_ids'])
            if (length > 512):
                text = tokenizer.convert_tokens_to_string(
                    tokenizer.tokenize(text)[:256] + tokenizer.tokenize(text)[-256:])
            response_b_new.append(text)
        
        # 新しいプロンプトと応答をデータフレームに格納
        self.data['prompt'] = prompt_new
        self.data['response_a'] = response_a_new
        self.data['response_b'] = response_b_new

    def __len__(self):
        # データの長さを返す
        return len(self.data)

    def __getitem__(self, idx):
        # データポイントの取得
        prompt = self.data['prompt'][idx]
        response_a = self.data['response_a'][idx]
        response_b = self.data['response_b'][idx]
        system_prompt = f"""{prompt}:
                        Response A: {response_a}
                        Response B: {response_b}
                        Which is better? Choose 'A', 'B', or 'both'.
""" + self.mask_token

        # モデルの入力用にトークン化
        inputs = self.tokenizer(system_prompt, truncation=True, max_length=1600)
        input_ids  = torch.tensor(inputs['input_ids'], dtype=torch.long)
        mask_index = torch.where(input_ids == self.tokenizer.mask_token_id)[0]

        return {
            'input_ids': input_ids,
            'attention_mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),
            'mask_index': mask_index
        }

# データをバッチにするためのカスタム関数
def collate_fn_semantic(batch):
    input_ids = [item['input_ids'] for item in batch]
    attention_mask = [item['attention_mask'] for item in batch]
    mask_index = [item['mask_index'] for item in batch]

    # パディングを施し、バッチ処理
    input_ids = pad_sequence(input_ids, batch_first=True)
    attention_mask = pad_sequence(attention_mask, batch_first=True,)
    mask_index = torch.stack(mask_index)
    return {
        'input_ids': input_ids,
        'attention_mask': attention_mask,
        'mask_index': mask_index
    }

# データローダーを取得する関数
def get_semantic_data_loader(batch_size=32, mode='train', shuffle=True):
    tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/deberta-small/deberta')
    if mode == 'train':
        dataset = Senmamtic_news(train, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=collate_fn_semantic)
        return dataloader
    else:
        dataset = Senmamtic_news(test, tokenizer)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn_semantic, drop_last=False)
        return dataloader

import torch
from torch import nn
from transformers import AutoModelForMaskedLM, AutoConfig
from tqdm import tqdm

# モデルクラスの定義
class BertPET(nn.Module):
    def __init__(self, model_path):
        super().__init__()
        self.bert = AutoModelForMaskedLM.from_pretrained(model_path)
        self.config = AutoConfig.from_pretrained(model_path)

    def forward(self, input_ids, attention_mask, mask_index, labels=None, return_logits=False):
        output = self.bert(input_ids=input_ids, attention_mask=attention_mask).logits
        mask_index = mask_index.unsqueeze(-1).expand(-1, -1, output.size(-1))
        output = torch.gather(output, 1, mask_index).squeeze(1)
        if return_logits:
            # 特定インデックスのlogitsを返す
            logits = output[:,[336, 732, 462]]
            return logits

# 推論用の関数
def inference(model, dataloader, device):
    model.eval()
    softmax = nn.Softmax(dim=-1)
    all_probs = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Inference", leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            mask_index = batch['mask_index'].to(device)

            logits = model(input_ids=input_ids, attention_mask=attention_mask, mask_index=mask_index, return_logits=True)
            probs = softmax(logits)
            probs_list = probs.cpu().numpy().tolist()
            for i in probs_list:
                all_probs.append(i)

    return all_probs

# デバイス設定
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# モデルと重みをロード
model_path = '/kaggle/input/deberta-small/deberta'  # 事前学習済みモデルのパス
checkpoint_path = '/kaggle/input/deberta-v3/model_checkpoint_epoch_2.pt'  # 最良重みファイルのパス
model = BertPET(model_path)
model.load_state_dict(torch.load(checkpoint_path, map_location=device))
model.to(device)

# テストデータを取得
test_loader = get_semantic_data_loader(2, mode='test')

# 推論を実施
test_probs = inference(model, test_loader, device)

# 提出ファイルの作成
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')
test[['winner_model_a', 'winner_model_b', 'winner_tie']] = test_probs
sample_sub = sample_sub[['id']].merge(test[['id', 'winner_model_a', 'winner_model_b', 'winner_tie']], on='id', how='left')
sample_sub.to_csv('submission.csv', index=False)
display(sample_sub.head())
```

** @@@ Jupyter Notebook numver 85, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測の問題に取り組んでいます。この目的のために、ユーザーが二つの異なる言語モデルからの応答の中でどちらを好むかを予測するためのモデルを構築しています。

### 問題の概要
与えられたデータセットからユーザーのプロンプトと応答を処理し、各応答の優劣を予測することが課題です。具体的には、ユーザーがどちらのモデルの応答を選ぶか、または両方を同等に評価するか（引き分け）を予測します。

### 使用されている手法とライブラリ
1. **データの前処理**: pandasを用いてトレーニングデータを読み込み、プロンプトや応答のテキストを整形。この際、不要な'null'応答を削除し、応答を連結して新しいテキストフォーマットを作成しています。

2. **トークナイザー**: Hugging Faceの`BertTokenizer`を利用して、テキストをトークン化します。トークン化には、パディングや特別なトークンの設定が含まれています。

3. **モデルの構築**: Kerasを用いて、畳み込みニューラルネットワーク（CNN）と長短期記憶（LSTM）を組み合わせたモデルを設計しています。具体的には、1D畳み込み層とLSTM層を用いて、テキスト分類のための深層学習モデルを構築し、最終的にソフトマックスで出力する多クラス分類器を定義します。

4. **モデルトレーニング**: 定義したモデルに対して、トレーニングデータを用いて学習を行い、モデルのパラメータを最適化します。また、学習過程でモデルの性能を評価し、最良のモデルを保存しています。

5. **予測と提出ファイルの作成**: テストデータを処理し、同様にトークン化及びパディングを行った後、トレーニングしたモデルを使用して予測を行い、最終的に提出用のCSVファイルを生成しています。

### 結論
このノートブックは、言語モデルの応答の優劣を予測するための深層学習モデルを構築し、詳細なデータ前処理と評価手法を適用しています。また、最適化の余地が残っており、さらなるパフォーマンス向上が期待されます。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
import gc
import re
from time import time
import random
import warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tqdm.auto import tqdm

import torch
import transformers
from sklearn.metrics import accuracy_score
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification
import torch.nn.functional as F
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
# トークナイザー
```

---The following area is a Code cell (cell numver is 4)---
```python
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert/tensorflow2/bert-en-uncased-l-10-h-128-a-2/2')

tokenizer.pad_token = tokenizer.eos_token  # パディングトークンをEOSトークンに設定
tokenizer.padding_side = 'right'  # 右側にパディング
tokenizer.add_eos_token = True  # EOSトークンを追加
# オフラインで推論時にロードするためにトークナイザーを保存
tokenizer.save_pretrained('tokenizer')
```

---The following area is a Code cell (cell numver is 5)---
```python
# トークンの長さを返すユーティリティ関数
def get_token_lengths(texts):
    # テキストをトークン化して各テキストのinput_idsを取得
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']
    # 各テキストのinput_idsの長さを返す
    return [len(t) for t in input_ids]
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# トレーニングデータの準備
```

---The following area is a Code cell (cell numver is 7)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')
def process(input_str):
    stripped_str = input_str.strip('[]')
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    return  ' '.join(sentences)

train.loc[:, 'prompt'] = train['prompt'].apply(process)  # プロンプトを処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # モデルAの応答を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # モデルBの応答を処理

# トレーニングのために'Null'を削除
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)  # 'null'応答の行を削除
train.reset_index(inplace=True, drop=True)  # インデックスをリセット

print(f"Total {len(indexes)} Null response rows dropped")  # 削除した'Null'の行数を表示
print('Total train samples: ', len(train))  # トレーニングデータのサンプル数を表示
```

---The following area is a Code cell (cell numver is 8)---
```python
train.head(5)  # トレーニングデータの最初の5行を表示
```

---The following area is a Code cell (cell numver is 9)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(train['text'][4])  # 4番目のテキストを表示
```

---The following area is a Code cell (cell numver is 10)---
```python
# データセットの50%を使用してトレーニング
train = train[:int(len(train) * 1)]  # 指定された割合までのデータを使用

train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # トークン数をカウントする

# モデル用にラベルを準備
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)

# データを表示
display(train.head())
```

---The following area is a Code cell (cell numver is 11)---
```python
train.label.value_counts()  # 各ラベルの数をカウント
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークン数
display(train['token_count'].describe().to_frame().astype(int))  # トークン数の統計を表示
```

---The following area is a Code cell (cell numver is 13)---
```python
# データの90%をカバーするトークンの長さを取得、長さは1024を使用
np.percentile(train['token_count'], 90)  # 90パーセンタイルのトークン数を出力
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# トークン化
```

---The following area is a Code cell (cell numver is 15)---
```python
# データをトークン化
tokens = tokenizer(
    train['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')  # トークン化の設定

# 入力IDはトークンのID
INPUT_IDS = tokens['input_ids']
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS = tokens['attention_mask']
# テキストのラベル
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values

print(f'INPUT_IDS shape: {INPUT_IDS.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS.shape}')  # 入力とアテンションマスクの形状を表示
print(f'LABELS shape: {LABELS.shape}')  # ラベルの形状を表示
```

---The following area is a Code cell (cell numver is 16)---
```python
max_features = 21540  # 最大特徴数
maxlen = 1024  # 最大長さ
batch_size = 16  # バッチサイズ
embedding_dims = 200  # 埋め込み次元
nb_filter = 150  # フィルター数
filter_length = 3  # フィルターの長さ
hidden_dims = 100  # 隠れ層次元
nb_epoch = 14  # エポック数
```

---The following area is a Code cell (cell numver is 17)---
```python
from __future__ import print_function
import numpy as np

from keras.preprocessing import sequence
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation, Lambda
from keras.layers import Embedding
from keras.layers import Convolution1D, LSTM
from keras.datasets import imdb
from keras import backend as K
from keras.optimizers import Adadelta
from keras.preprocessing import sequence as sq

from keras.layers import Dense, Dropout, Activation, Lambda, Input, TimeDistributed, Flatten
from keras.models import Model
from keras.callbacks import ModelCheckpoint

from tensorflow.python.keras.backend import set_session as K
X_train = sq.pad_sequences(INPUT_IDS, maxlen=maxlen)  # 入力IDをパディング
y_train = LABELS  # ラベル
```

---The following area is a Code cell (cell numver is 18)---
```python
X_train = np.array(X_train)  # 入力をNumPy配列に変換
y_train = np.array(y_train)  # ラベルをNumPy配列に変換
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# モデルの定義
```

---The following area is a Code cell (cell numver is 20)---
```python
'''この例はテキスト分類のためのConvolution1Dの使用を示しています。
2エポック後にテスト精度0.88に達します。
Intel i5 2.4Ghz CPUで90秒/エポック、Tesla K40 GPUで10秒/エポック。'''
from keras.layers import Concatenate
from keras.layers import GlobalMaxPooling1D

#config = K.tf.ConfigProto(intra_op_parallelism_threads=16, inter_op_parallelism_threads=16, \
#                        allow_soft_placement=True, device_count = {'CPU': 1})

# tf_config = K.tf.ConfigProto()
# tf_config.gpu_options.allow_growth = True
# session = K.tf.Session(config=tf_config)
# K.set_session(session)

# config = K.tf.ConfigProto(intra_op_parallelism_threads=4, inter_op_parallelism_threads=4, \
#                         allow_soft_placement=True, device_count = {'CPU': 4})
# session = K.tf.Session(config=config)
# K.set_session(session)

model = Sequential()  # シーケンシャルモデルを作成

input_layer = Input(shape=(maxlen,),dtype='int64', name='main_input')  # 入力レイヤー
emb_layer = Embedding(max_features,
                      embedding_dims,
                      input_length=maxlen
                      )(input_layer)  # 埋め込み層を追加
def max_1d(X):
    return K.max(X, axis=1)  # 1次元の最大値を求める関数

# 3のサイズのフィルターを持つConvolution1Dを追加
con3_layer = Convolution1D(filters=nb_filter,
                    padding='valid',
                    activation='relu',
                    kernel_size=3,
                    strides=1)(emb_layer)

pool_con3_layer = GlobalMaxPooling1D()(con3_layer)  # グローバル最大プーリング

# 4のサイズのフィルターを持つConvolution1Dを追加
con4_layer = Convolution1D(filters=nb_filter,
                    kernel_size=5,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con4_layer = GlobalMaxPooling1D()(con4_layer)  # グローバル最大プーリング

# 5のサイズのフィルターを持つConvolution1Dを追加
con5_layer = Convolution1D(filters=nb_filter,
                    kernel_size=7,
                    padding='valid',
                    activation='relu',
                    strides=1)(emb_layer)

pool_con5_layer = GlobalMaxPooling1D()(con5_layer)  # グローバル最大プーリング

cnn_layer = Concatenate()([pool_con3_layer, pool_con5_layer, pool_con4_layer])  # 畳み込み層の出力を結合

# LSTM層を追加
x = Embedding(max_features, embedding_dims, input_length=maxlen)(input_layer)  # 埋め込み層
lstm_layer = LSTM(128)(x)  # LSTM層

cnn_lstm_layer = Concatenate()([lstm_layer, cnn_layer])  # LSTM層とCNN層を結合

dense_layer = Dense(hidden_dims*2, activation='sigmoid')(cnn_lstm_layer)  # 密なレイヤー
output_layer = Dropout(0.2)(dense_layer)  # ドロップアウト層
output_layer = Dense(3, trainable=True, activation='softmax')(output_layer)  # 出力層


model = Model(inputs=[input_layer], outputs=[output_layer])  # モデルを定義
adadelta = Adadelta(learning_rate=1.0, rho=0.95, epsilon=1e-06)  # Adadeltaオプティマイザーを定義

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])  # モデルをコンパイル
model.summary()  # モデルの概要を表示
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# トレーニング
```

---The following area is a Code cell (cell numver is 22)---
```python
checkpoint = ModelCheckpoint('CNN-LSTM-weights/weights.keras',
                                 monitor='val_acc', verbose=0, save_best_only=True,
                                 mode='max')  # モデルのチェックポイントを定義
model.fit(X_train, y_train,
          batch_size=16,
          epochs=nb_epoch,
          callbacks=[checkpoint])  # モデルのトレーニング

model.compile(loss='categorical_crossentropy',
              optimizer="adamax",
              metrics=['accuracy'])  # モデルを再コンパイル
```

---The following area is a Code cell (cell numver is 23)---
```python
model.save('model_LSTM_mix_CNN.keras')  # モデル全体を保存
```

---The following area is a Code cell (cell numver is 24)---
```python
model.predict(X_train)  # トレーニングデータに対する予測を行う
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
# モデルのテスト
```

---The following area is a Code cell (cell numver is 26)---
```python
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')


test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプトを処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答を処理

# トレーニングのために'Null'を削除
indexes = test[(test.response_a == 'null') & (test.response_b == 'null')].index
test.drop(indexes, inplace=True)  # 'null'応答の行を削除
test.reset_index(inplace=True, drop=True)  # インデックスをリセット

print(f"Total {len(indexes)} Null response rows dropped")  # 削除した'Null'の行数を表示
print('Total train samples: ', len(test))  # テストデータのサンプル数を表示
```

---The following area is a Code cell (cell numver is 27)---
```python
test.head()  # テストデータの最初の5行を表示
```

---The following area is a Code cell (cell numver is 28)---
```python
test['text'] = 'User prompt: ' + test['prompt'] +  '\n\nModel A :\n' + test['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']
print(test['text'])  # テキストを表示
```

---The following area is a Code cell (cell numver is 29)---
```python
# データをトークン化
tokens_test = tokenizer(
    test['text'].tolist(), 
    max_length=1024, 
    truncation=True, 
    return_tensors='np')  # トークン化の設定

# 入力IDはトークンのID
INPUT_test = tokens_test['input_ids']
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS2 = tokens_test['attention_mask']

print(f'INPUT_IDS shape: {INPUT_test.shape}, ATTENTION_MASKS shape: {ATTENTION_MASKS2.shape}')  # 入力とアテンションマスクの形状を表示
```

---The following area is a Code cell (cell numver is 30)---
```python
X_test = sq.pad_sequences(INPUT_test, maxlen=maxlen)  # テストデータの入力をパディング
```

---The following area is a Code cell (cell numver is 31)---
```python
test  # テストデータを表示
```

---The following area is a Code cell (cell numver is 32)---
```python
y_predict = model.predict(X_test)  # テストデータに対する予測を行う
y_predict  # 予測結果を表示
```

---The following area is a Code cell (cell numver is 33)---
```python
winner_df = pd.DataFrame(y_predict, columns=['winner_model_a', 'winner_model_b', 'winner_tie'])  # 予測結果をデータフレームに変換
result_df = pd.concat([test['id'], winner_df], axis=1)  # テストデータのIDと予測結果を結合
```

---The following area is a Code cell (cell numver is 34)---
```python
result_df.to_csv('submission.csv', index=False)  # 結果をCSVファイルに保存
```

---The following area is a Code cell (cell numver is 35)---
```python
result_df  # 結果データフレームを表示
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
# 結論

トレーニングの速度を上げて最適化する余地がまだたくさんあります！より多くのデータや異なるバッチサイズ、学習率を試してみてください... 頑張ってください！
```

** @@@ Jupyter Notebook numver 86, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyterノートブックは、Gemma-2 9bモデルを使用して、人間の好みに基づくチャットボットの応答を予測するタスクに取り組んでいます。コンペティションにおいて、参加者はどちらのモデルの応答が好まれるかを予測するための機械学習モデルを構築する必要があります。

主な手法として、ノートブックではLoRA（Low-Rank Adaptation）およびQLoRA（Quantized Low-Rank Adaptation）を用いてモデルをファインチューニングしています。これは、トレーニング中に元の重みを凍結し、LoRAアダプタの重みだけを更新することで、メモリを効率的に使用しつつ、優れたパフォーマンスを実現する手法です。QLoRAでは、モデルの重みを量子化して計算リソースを最小化し、高精度の計算を維持します。

使用しているライブラリには、`transformers`、`bitsandbytes`、および`peft`が含まれ、これらを用いてモデル設定、トークナイザー、データセットにアクセスし、トレーニングを実行します。評価指標としては、対数損失（log loss）と精度（accuracy）が用いられ、特にKaggleのルールに基づいてパフォーマンスが計測されます。

ノートブックは、Gemma-2モデルのトレーニングを1エポックで実施し、得られた結果は評価セットに対して0.9371のログロス、リーダーボード上で0.941に達しました。このデモには、100サンプルのデータセットを使用しており、全体の設定やパラメータはカスタマイズ可能です。トレーニング結果はTensorBoardで記録され、リアルタイムでメモリ使用量や損失、精度を観察できるようになっています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## このノートブックについて
このノートブックでは、Gemma-2 9bをトレーニングしてLB: 0.941を取得する方法を示します。推論コードは[こちら](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)で見つけることができます。
私は、unslothチームがアップロードした4ビット量子化された[Gemma 2 9b Instruct](https://huggingface.co/unsloth/gemma-2-9b-it-bnb-4bit)をベースモデルとして使用し、LoRAアダプタを追加して1エポックでトレーニングしました。

## 結果

評価セットとして `id % 5 == 0` を使用し、残りをすべてトレーニングに使用しました。

| サブセット | ログロス |
| - | - |
| 評価 | 0.9371 |
| LB | 0.941 |

## QLoRAファインチューニングとは？

従来のファインチューニングでは、重み ($\mathbf{W}$) の更新が以下のように行われます：

$$
\mathbf{W} \leftarrow \mathbf{W} - \eta \frac{{\partial L}}{{\partial \mathbf{W}}} = \mathbf{W} + \Delta \mathbf{W}
$$

ここで、$L$ はこのステップでの損失値、$\eta$ は学習率です。

[LoRA](https://arxiv.org/abs/2106.09685)は、$\Delta \mathbf{W} \in \mathbb{R}^{\text{d} \times \text{k}}$ を、$r \ll \text{min}(\text{d}, \text{k})$ の2つの (はるかに) 小さい行列、$\mathbf{B} \in \mathbb{R}^{\text{d} \times \text{r}}$ と$\mathbf{A} \in \mathbb{R}^{\text{r} \times \text{k}}$ に因子分解して近似しようとします。

$$
\Delta \mathbf{W}_{s} \approx \mathbf{B} \mathbf{A}
$$

<img src="https://storage.googleapis.com/pii_data_detection/lora_diagram.png">

トレーニング中、元の重みは凍結されるため、$\mathbf{A}$ と $\mathbf{B}$ のみが更新されます。これにより、トレーニング中に更新する必要のある元の重みの割合はごくわずか (例えば <1%) に抑えられます。この方法で、トレーニング中のGPUメモリ使用量を大幅に削減しながら、通常の (フル) ファインチューニングと同等のパフォーマンスを達成できます。

[QLoRA](https://arxiv.org/abs/2305.14314)は、LLMの量子化により効率をさらに向上させます。例えば、8Bパラメータモデルは32ビットで32GBのVRAMを必要としますが、量子化された8ビット/4ビット8Bモデルはそれぞれ8GB/4GBで済みます。
QLoRAは、低精度 (例えば8ビット) でLLMの重みを量子化する一方で、フォワード/バックワードの計算は高精度 (例えば16ビット) で行い、LoRAアダプタの重みも高精度で保持されていることに注意してください。

A6000を使用した1エポックは、4ビットで約15時間、8ビットで約24時間かかり、ログロスの差は大きくありませんでした。

## 注意
Kaggleカーネルでのフルトレーニングには非常に長い時間がかかります。フルトレーニングを実行するには外部の計算リソースを使用することをお勧めします。
このノートブックではデモ目的で100サンプルのみを使用していますが、その他はすべて私の設定と同じです。
```

---The following area is a Code cell (cell numver is 2)---
```python
# gemma-2はtransformers>=4.42.3から利用可能です
!pip install -U "transformers>=4.42.3" bitsandbytes accelerate peft
```

---The following area is a Code cell (cell numver is 3)---
```python
import os
import copy
from dataclasses import dataclass
# 必要なライブラリをインポート
from torch.utils.tensorboard import SummaryWriter
import psutil  # システムとプロセスのリソース使用情報を取得するためのライブラリ
import numpy as np
import torch
from datasets import Dataset
from transformers import (
    BitsAndBytesConfig,
    Gemma2ForSequenceClassification,
    GemmaTokenizerFast,
    Gemma2Config,
    PreTrainedTokenizerBase, 
    EvalPrediction,
    Trainer,
    TrainingArguments,
    DataCollatorWithPadding,
    TrainerCallback,
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, TaskType
from sklearn.metrics import log_loss, accuracy_score
```

---The following area is a Code cell (cell numver is 4)---
```python
# !pip install --upgrade kaggle
# # まず、これを実行してから作成する必要があります
# !kaggle datasets list
```

---The following area is a Code cell (cell numver is 5)---
```python
# import json

# token = {
#     "username": "qinhaoyang",
#     "key": "02c6cdf132dabb5ddd9de0d37d8a7777"
# }

# with open('/root/.kaggle/kaggle.json', 'w') as file:
#     json.dump(token, file)
# with open('/kaggle/working/kaggle.json', 'w') as file:
#     json.dump(token, file)
```

---The following area is a Code cell (cell numver is 6)---
```python
# !chmod 600 /root/.kaggle/kaggle.json

# # JSONデータを定義
# data ={
#   "title": "LMSYS-model", # データセットのタイトル
#   "subtitle": "",
#   "description": "",
#   "id": "qinhaoyang/LMSYS-model",
#   "licenses": [
#         {
#             "name": "unknown"
#         }
#     ],
#     "keywords": [],
#     "collaborators": [],
#     "data": []
# }


# with open('/kaggle/working/dataset-metadata.json', 'w') as file:
#     json.dump(data, file)
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
### 設定
```

---The following area is a Code cell (cell numver is 8)---
```python
#!kaggle kernels output qinhaoyang/training-gemma-2-9b-4-bit-qlora-fine-tuning -p /kaggle/working
```

---The following area is a Code cell (cell numver is 9)---
```python
#!kaggle datasets version -p /kaggle/working/output/ -m "データセットの変更内容についての説明" --dir-mode tar
```

---The following area is a Code cell (cell numver is 10)---
```python
@dataclass
class Config:
    output_dir: str = "output"
    checkpoint: str = "unsloth/gemma-2-9b-it-bnb-4bit"  # 4ビット量子化されたgemma-2-9b-instruct
    max_length: int = 1024
    n_splits: int = 5
    fold_idx: int = 0
    optim_type: str = "adamw_8bit"
    per_device_train_batch_size: int = 4
    gradient_accumulation_steps: int = 1  # グローバルバッチサイズは8
    per_device_eval_batch_size: int = 4
    n_epochs: int = 1
    freeze_layers: int = 16  # 合計42層があるため、最初の16層にはアダプタを追加しない
    lr: float = 2e-4
    warmup_steps: int = 20
    lora_r: int = 16
    lora_alpha: float = lora_r * 2
    lora_dropout: float = 0.05
    lora_bias: str = "none"
    
config = Config()
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
#### トレーニング引数
```

---The following area is a Code cell (cell numver is 12)---
```python
training_args = TrainingArguments(
    output_dir="output",
    overwrite_output_dir=True,
    report_to="none",
    num_train_epochs=config.n_epochs,
    per_device_train_batch_size=config.per_device_train_batch_size,
    gradient_accumulation_steps=config.gradient_accumulation_steps,
    per_device_eval_batch_size=config.per_device_eval_batch_size,
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="steps",
    save_steps=200,
    optim=config.optim_type,
    fp16=True,
    learning_rate=config.lr,
    warmup_steps=config.warmup_steps,
)
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
#### LoRA設定
```

---The following area is a Code cell (cell numver is 14)---
```python
lora_config = LoraConfig(
    r=config.lora_r,
    lora_alpha=config.lora_alpha,
    # 自自己注意メカニズムのみをターゲットする
    target_modules=["q_proj", "k_proj", "v_proj"],
    layers_to_transform=[i for i in range(42) if i >= config.freeze_layers],
    lora_dropout=config.lora_dropout,
    bias=config.lora_bias,
    task_type=TaskType.SEQ_CLS,
)
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
### トークナイザーとモデルのインスタンス化
```

---The following area is a Code cell (cell numver is 16)---
```python
tokenizer = GemmaTokenizerFast.from_pretrained(config.checkpoint)
tokenizer.add_eos_token = True  # <eos>を末尾に追加
tokenizer.padding_side = "right"
```

---The following area is a Code cell (cell numver is 17)---
```python
model = Gemma2ForSequenceClassification.from_pretrained(
    config.checkpoint,
    num_labels=3,
    torch_dtype=torch.float16,
    device_map="auto",
)
model.config.use_cache = False  # キャッシュの使用を無効にする
model = prepare_model_for_kbit_training(model)  # 低ビットでのトレーニングの準備
model = get_peft_model(model, lora_config)  # LoRAモデルの取得
model
```

---The following area is a Code cell (cell numver is 18)---
```python
model.print_trainable_parameters()  # トレーニング可能なパラメータを表示
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
### データセットのインスタンス化
```

---The following area is a Code cell (cell numver is 20)---
```python
# ds = Dataset.from_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
ds = Dataset.from_csv("/kaggle/input/lmsys-72k-dataset/lmsys-7.2k.csv")  # データセットの読み込み
# ds = ds.select(torch.arange(100))  # デモ目的で最初の100データを使用
```

---The following area is a Code cell (cell numver is 21)---
```python
class CustomTokenizer:
    def __init__(
        self, 
        tokenizer: PreTrainedTokenizerBase, 
        max_length: int
    ) -> None:
        self.tokenizer = tokenizer
        self.max_length = max_length
        
    def __call__(self, batch: dict) -> dict:
        prompt = ["<prompt>: " + self.process_text(t) for t in batch["prompt"]]  # プロンプトを処理
        response_a = ["\n\n<response_a>: " + self.process_text(t) for t in batch["response_a"]]  # 応答Aを処理
        response_b = ["\n\n<response_b>: " + self.process_text(t) for t in batch["response_b"]]  # 応答Bを処理
        texts = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]  # テキストを組み合わせる
        tokenized = self.tokenizer(texts, max_length=self.max_length, truncation=True)  # トークナイザーでトークン化
        labels=[]
        for a_win, b_win in zip(batch["winner_model_a"], batch["winner_model_b"]):
            if a_win:
                label = 0  # モデルAが勝った場合
            elif b_win:
                label = 1  # モデルBが勝った場合
            else:
                label = 2  # 引き分けの場合
            labels.append(label)  # ラベルを追加
        return {**tokenized, "labels": labels}  # トークン化された結果とラベルを返す
        
    @staticmethod
    def process_text(text: str) -> str:
        return " ".join(eval(text, {"null": ""}))  # テキストを処理する
```

---The following area is a Code cell (cell numver is 22)---
```python
encode = CustomTokenizer(tokenizer, max_length=config.max_length)  # カスタムトークナイザーのインスタンス化
ds = ds.map(encode, batched=True)  # データセットにトークナイザーを適用
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
### メトリックの計算

LBで使用されるログロスと精度を補助的なメトリックとして計算します。
```

---The following area is a Code cell (cell numver is 24)---
```python
def compute_metrics(eval_preds: EvalPrediction) -> dict:
    preds = eval_preds.predictions  # 予測結果を取得
    labels = eval_preds.label_ids  # ラベルを取得
    probs = torch.from_numpy(preds).float().softmax(-1).numpy()  # 予測値を確率に変換
    loss = log_loss(y_true=labels, y_pred=probs)  # ログロスを計算
    acc = accuracy_score(y_true=labels, y_pred=preds.argmax(-1))  # 精度を計算
    return {"acc": acc, "log_loss": loss}  # 精度とログロスを返す
```

---The following area is a Markdown cell (cell numver is 25)---
```markdown
### 分割

ここでは、`id % 5` に従ってトレーニングと評価を分割します。
```

---The following area is a Code cell (cell numver is 26)---
```python
folds = [
    (
        [i for i in range(len(ds)) if i % config.n_splits != fold_idx],  # トレーニング用
        [i for i in range(len(ds)) if i % config.n_splits == fold_idx]  # 評価用
    ) 
    for fold_idx in range(config.n_splits)
]
```

---The following area is a Code cell (cell numver is 27)---
```python
# チェックポイントをロード
checkpoint = "/kaggle/input/lmsys-gemma-checkpoint/output/checkpoint-10200"
```

---The following area is a Code cell (cell numver is 28)---
```python
#%tensorboard --logdir=/kaggle/working
```

---The following area is a Code cell (cell numver is 29)---
```python
# 現在のプロセスのメモリ使用状況を記録し、TensorBoardに書き込む関数を定義します
def log_memory_usage(step, writer):  # step は記録のステップ識別子、writer はSummaryWriterのインスタンス
    # 現在のプロセスを取得
    process = psutil.Process(os.getpid())
    # プロセスのメモリ情報を取得
    mem_info = process.memory_info()
    # RSS（常駐セットサイズ）とVMS（仮想メモリサイズ）をMBに変換し、TensorBoardに記録
    writer.add_scalar('Memory Usage/RSS (MB)', mem_info.rss / (1024 * 1024), step)  # 物理メモリ使用量
    writer.add_scalar('Memory Usage/VMS (MB)', mem_info.vms / (1024 * 1024), step)  # 仮想メモリ使用量
    print(f"メモリ使用量がステップ {step} で記録されました")

# 各エポック終了時にメモリ使用状況を記録するカスタムのTrainerCallbackサブクラスを作成
class MemoryUsageLoggingCallback(TrainerCallback):
    def on_epoch_end(self, args, state, control, **kwargs):  # 各エポック終了時に呼び出されるコールバックメソッド
        # 現在のエポック数を記録のステップとして使用
        current_epoch = state.epoch
        # 前に定義した関数を呼び出し、メモリ使用状況を記録
        log_memory_usage(current_epoch, tb_writer)
    
    def on_log(self, args, state, control, **kwargs):
        logs = kwargs.get("logs", {})
        for key, value in logs.items():
            if isinstance(value, (int, float)):
                tb_writer.add_scalar(f"{key.capitalize()}", value, state.global_step)
                print(f"{key}: {value}")

        # トレーニングと評価の損失と精度も追加で記録
        if "loss" in logs:
            tb_writer.add_scalar("Loss/train", logs["loss"], state.global_step)
        if "eval_loss" in logs:
            tb_writer.add_scalar("Loss/eval", logs["eval_loss"], state.global_step)
        if "accuracy" in logs:
            tb_writer.add_scalar("Accuracy/train", logs["accuracy"], state.global_step)
        if "eval_accuracy" in logs:
            tb_writer.add_scalar("Accuracy/eval", logs["eval_accuracy"], state.global_step)

# SummaryWriterを初期化
tb_writer = SummaryWriter(log_dir="/kaggle/working/Gemma/tensorboard_logs")

# トレーニングと評価のデータセットを定義
train_idx, eval_idx = folds[config.fold_idx]

# Trainerを初期化し、モデル、データセットなどの設定を含め、メモリ使用記録のカスタムコールバックも追加
trainer = Trainer(
    args=training_args,  # トレーニング引数
    model=model,  # トレーニングするモデル
    tokenizer=tokenizer,  # トークナイザー
    train_dataset=ds.select(train_idx),  # トレーニングデータセット
    eval_dataset=ds.select(eval_idx),  # 評価データセット
    compute_metrics=compute_metrics,  # 評価指標を計算するメソッド
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),  # データ処理
    callbacks=[MemoryUsageLoggingCallback()],  # Trainerにカスタムコールバックを追加
)

# チェックポイントがあるか確認してトレーニングを再開
if checkpoint:
    trainer.train(resume_from_checkpoint=checkpoint)
else:
    trainer.train()

# トレーニング後、SummaryWriterを閉じてリソースを解放
tb_writer.close()
```

---The following area is a Code cell (cell numver is 30)---
```python
model.save_pretrained("/kaggle/working/Gemma") # モデルを保存するローカルパスを指定
tokenizer.save_pretrained("/kaggle/working/Gemma") # 必要に応じて、トークナイザーも同時に保存
```

---The following area is a Code cell (cell numver is 31)---
```python
# !kaggle datasets version -p /kaggle/working -m "データセットの変更内容についての説明" --dir-mode tar
```

** @@@ Jupyter Notebook numver 87, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおける人間の好みを予測するためのモデルを構築することを目的としています。特に、このノートブックは、2つの大規模言語モデル（モデルAとモデルB）の応答がユーザーに好まれる確率を分析し、予測するための手法を実装しています。

### 主な内容と手法

1. **ライブラリのインポート**: 
   - `pandas`: データ操作に使用。
   - `scikit-learn`からの複数のモジュール: データセットの分割、TF-IDFによるテキストのベクトル化、ロジスティック回帰モデル、ログ損失の計算。

2. **データセットの読み込み**: 
   - 訓練データ（`train.csv`）とテストデータ（`test.csv`）をPandasを使用して読み込みます。

3. **ターゲット変数の作成**: 
   - モデルAが好まれた場合は1、それ以外は0として新しいカラム`target`を生成します。

4. **テキストデータの準備**: 
   - プロンプトとそれに対応するモデルAおよびモデルBの応答を結合した新しいテキストデータカラムを作成します。

5. **TF-IDFベクトル化**: 
   - テキストデータを最大5000の特徴量にベクトル化します。

6. **データセットの分割**: 
   - 訓練データを訓練用と検証用に80:20の割合で分割します。

7. **ロジスティック回帰モデルの実装**: 
   - 2つのロジスティック回帰モデル（モデルAとモデルB）を設定し、訓練データを用いて学習させます。

8. **検証データの予測と評価**: 
   - 検証データに対する予測を行い、ログ損失を計算してモデルのパフォーマンスを評価します。

9. **テストデータの予測と提出ファイルの作成**: 
   - テストデータに対して学習したモデルを用いて予測を行い、最終的な提出ファイル（`submission.csv`）を作成します。

このノートブックでは、データ前処理からモデル訓練、評価、予測結果の出力までの一連の流れが示されており、機械学習を用いたテキストデータの解析における基本的な技術を適用しています。
```

---The following area is a Code cell (cell numver is 1)---
```python
# ライブラリのインポート
import pandas as pd  # データ操作のためのPandasライブラリをインポートします
from sklearn.model_selection import train_test_split  # データセットを分割するための関数をインポートします
from sklearn.feature_extraction.text import TfidfVectorizer  # TF-IDFベクトル化のためのライブラリをインポートします
from sklearn.linear_model import LogisticRegression  # ロジスティック回帰モデルをインポートします
from sklearn.metrics import log_loss  # ログ損失の計算のための関数をインポートします

# データセットの読み込み
train_file_path = '/kaggle/input/lmsysdataset/train.csv'  # 訓練データのファイルパス
test_file_path = '/kaggle/input/lmsysdataset/test.csv'  # テストデータのファイルパス

train_df = pd.read_csv(train_file_path)  # 訓練データを読み込みます
test_df = pd.read_csv(test_file_path)  # テストデータを読み込みます

# ターゲット変数の作成
# 'winner_model_a'が1の行には1を、それ以外は0をターゲットとします
train_df['target'] = train_df.apply(lambda row: 1 if row['winner_model_a'] == 1 else 0, axis=1)

# テキストデータの結合
# プロンプトとモデルA及びモデルBの応答を結合した新しいカラムを作成します
train_df['text_a'] = train_df['prompt'] + ' ' + train_df['response_a']
train_df['text_b'] = train_df['prompt'] + ' ' + train_df['response_b']
test_df['text_a'] = test_df['prompt'] + ' ' + test_df['response_a']
test_df['text_b'] = test_df['prompt'] + ' ' + test_df['response_b']

# TF-IDFベクトライザの設定
vectorizer = TfidfVectorizer(max_features=5000)  # 最大5000の特徴量を持つTF-IDFベクトライザを作成します

# テキストデータのベクトル化
# 訓練データのテキストをベクトル化します
X_a = vectorizer.fit_transform(train_df['text_a'])  
# 訓練データの別のテキストをベクトル化します
X_b = vectorizer.fit_transform(train_df['text_b'])  
# テストデータのテキストをベクトル化します
X_test_a = vectorizer.transform(test_df['text_a'])  
X_test_b = vectorizer.transform(test_df['text_b'])  

# 訓練データと検証データの分割
# 訓練データを訓練用と検証用に分割します（8:2の割合）
X_train_a, X_valid_a, y_train, y_valid = train_test_split(X_a, train_df['target'], test_size=0.2, random_state=42)  
X_train_b, X_valid_b, _, _ = train_test_split(X_b, train_df['target'], test_size=0.2, random_state=42)  

# ロジスティック回帰モデルの設定
model_a = LogisticRegression(max_iter=1000)  # モデルAのロジスティック回帰を設定します
model_b = LogisticRegression(max_iter=1000)  # モデルBのロジスティック回帰を設定します

# モデルの訓練
model_a.fit(X_train_a, y_train)  # モデルAを訓練データで学習させます
model_b.fit(X_train_b, y_train)  # モデルBを訓練データで学習させます

# 検証データの予測
valid_preds_a = model_a.predict_proba(X_valid_a)[:, 1]  # モデルAによる検証データの予測確率を計算します
valid_preds_b = model_b.predict_proba(X_valid_b)[:, 1]  # モデルBによる検証データの予測確率を計算します

# ログ損失の計算
loss_a = log_loss(y_valid, valid_preds_a)  # モデルAのログ損失を計算します
loss_b = log_loss(y_valid, valid_preds_b)  # モデルBのログ損失を計算します

print(f'Log Loss for model_a: {loss_a}')  # モデルAのログ損失を表示します
print(f'Log Loss for model_b: {loss_b}')  # モデルBのログ損失を表示します

# テストデータの予測
test_preds_a = model_a.predict_proba(X_test_a)[:, 1]  # モデルAによるテストデータの予測確率を計算します
test_preds_b = model_b.predict_proba(X_test_b)[:, 1]  # モデルBによるテストデータの予測確率を計算します

# 提出ファイルの作成
submission = pd.DataFrame({  # 提出用のDataFrameを作成します
    'id': test_df['id'],  # テストデータのIDを追加します
    'winner_model_a': test_preds_a,  # モデルAの予測確率を追加します
    'winner_model_b': test_preds_b,  # モデルBの予測確率を追加します
    'winner_tie': 0.0  # 同点の場合は0とします（必要に応じて調整）
})

# 提出ファイルの保存
submission.to_csv('/kaggle/working/submission.csv', index=False)  # 提出ファイルをCSV形式で保存します
print("Submission file created successfully!")  # 提出ファイル作成の成功メッセージを表示します
```

---The following area is a Code cell (cell numver is 2)---
```python
# このセルは現在何もコードがありません。必要なコードを挿入してください。
```

** @@@ Jupyter Notebook numver 88, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Kaggleの「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」において、ユーザーが2つのチャットボットの応答からどちらを好むかを予測する問題に取り組んでいます。具体的には、与えられたプロンプトに対する2つの異なるモデル（モデルAおよびモデルB）の応答を評価し、どのモデルが選好されるか、または引き分けになるかを予測するための機械学習モデルを構築しています。

### 主要な手法とライブラリ:
1. **ライブラリの使用**:
   - **Pandas**: データセットの読み込みや処理に使用。
   - **NumPy**: 数値計算をサポート。
   - **TensorFlow**: 深層学習モデルの構築とトレーニングに使用。
   - **Transformers**: 自然言語処理のためのBERTモデルを利用するライブラリ。

2. **データ処理**:
   - データセットに含まれるプロンプトと2つの応答から特徴量を抽出し、モデルのトレーニングに必要な形式に変換します。
   - 特徴量として、プロンプト、モデルAおよびモデルBの応答、そして勝者を示すラベルを使用します。

3. **モデル構築**:
   - BERTトークナイザーを用いて、テキストデータをトークン化します。
   - BERTモデル自体を使用して、プロンプトおよび応答を埋め込み（embedding）として変換します。
   - これらの埋め込みを結合し、その後に全結合層（Dense Layer）を通じて最終的な予測を行うモデルを構築します。

4. **トレーニングと評価**:
   - トレーニングデータとバリデーションデータに分割し、モデルのトレーニングを実施します。
   - 最後に、テストデータに対して予測を行い、その結果をCSVファイルとして出力します。

このNotebookは、自然言語処理における先進的な技術（BERTなど）を用いた深層学習の適用例を示しており、ユーザーの嗜好を具体的な数値に落とし込むためのプロセスを包括的に扱っています。最終的な出力は、各応答の勝者モデル（モデルA、モデルB、引き分け）の確率を示したCSVファイル「submission.csv」として保存されます。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な解析ライブラリがインストールされています
# これはkaggle/pythonのDockerイメージによって定義されています: https://github.com/kaggle/docker-python
# 例として、いくつかの便利なパッケージを読み込むことができます

import numpy as np # 線形代数のためのライブラリ
import pandas as pd # データ処理のためのライブラリ、CSVファイルの入出力 (例: pd.read_csv)

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# 例えば、これを実行すると (クリックして実行したりShift+Enterを押すことで) 入力ディレクトリ内のすべてのファイルをリストします

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename)) # 各ファイルのパスを表示します

# 現在のディレクトリ (/kaggle/working/) に最大20GBまで書き込むことができます
# このディレクトリは「Save & Run All」を使用してバージョンを作成した際に出力として保存されます
# 一時ファイルは/kaggle/temp/に書き込むこともできますが、現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
train=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv') # トレーニングデータをCSVファイルから読み込みます
test=pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv') # テストデータをCSVファイルから読み込みます
```

---The following area is a Code cell (cell numver is 3)---
```python
import tensorflow as tf # TensorFlowライブラリをインポートします
# TPUを検出して初期化します
# TPU（Tensor Processing Unit）は、特にディープラーニングのトレーニングと推論に最適化されたハードウェアアクセラレーターです
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
## ライブラリのインポート
```

---The following area is a Code cell (cell numver is 5)---
```python
import pandas as pd # データ処理のためのライブラリをインポートします
import tensorflow as tf # TensorFlowライブラリをインポートします
import numpy as np # 数値計算のためのライブラリをインポートします
import transformers # Transformersライブラリをインポートします。自然言語処理で使われるモデルのためのライブラリです
from sklearn.model_selection import train_test_split # データをトレーニングセットとテストセットに分割するための関数をインポートします
from transformers import BertTokenizer # BERTモデル用のトークナイザーをインポートします。テキストをトークンに変換する役割があります
```

---The following area is a Code cell (cell numver is 6)---
```python
tokenizer = BertTokenizer.from_pretrained('/kaggle/input/bert-base-uncased') # 事前学習済みのBERTトークナイザーを指定されたパスからロードします
# このトークナイザーは、テキストをBERTモデルの入力形式に変換するために使用されます
# 'bert-base-uncased'は小文字のみのBERTモデルで、事前学習済みの重みを含んでいます
```

---The following area is a Code cell (cell numver is 7)---
```python
from transformers import TFBertModel # TensorFlow用のBERTモデルをインポートします

# BERTモデルをロードします
bert_model = TFBertModel.from_pretrained('/kaggle/input/bert-base-uncased') # 指定されたパスから事前学習済みのBERTモデルをロードします
# このモデルは、自然言語処理タスクにおいて特徴量抽出や埋め込みを生成するために使用されます
```

---The following area is a Code cell (cell numver is 8)---
```python
for i in train.index: # トレーニングデータのすべてのインデックスに対してループします
    if train.loc[i,'winner_model_a'] == 1: # モデルAが勝者である場合
        train.loc[i,'winner'] = 0 # 勝者の列に0を設定します（モデルAが勝った場合）
    elif train.loc[i,'winner_model_b'] == 1: # モデルBが勝者である場合
        train.loc[i,'winner'] = 1 # 勝者の列に1を設定します（モデルBが勝った場合）
    else:
        train.loc[i,'winner'] = 2 # いずれのモデルも勝者でない場合は、勝者の列に2を設定します（引き分けと見なす）
```

---The following area is a Code cell (cell numver is 9)---
```python
features = ['prompt', 'response_a', 'response_b', 'winner'] # 使用する特徴量のリストを定義します
# 'prompt'はユーザーからの入力文、'response_a'はモデルAの応答、'response_b'はモデルBの応答、'winner'は勝者を示します
```

---The following area is a Code cell (cell numver is 10)---
```python
train_data = train[features] # トレーニングデータから指定した特徴量のみを抽出して、新しいデータフレームtrain_dataを作成します
# train_dataには'prompt', 'response_a', 'response_b', 'winner'の情報が含まれます
```

---The following area is a Code cell (cell numver is 11)---
```python
X_train, X_val = train_test_split(train_data, test_size=0.2, random_state=42) # トレーニングデータをトレーニングセットとバリデーションセットに分割します
# test_size=0.2は全体の20%をバリデーションセットとし、random_state=42は再現性のための乱数シードを設定します
# X_trainにはトレーニングデータが、X_valにはバリデーションデータが含まれます
```

---The following area is a Code cell (cell numver is 12)---
```python
# トークナイズ関数
def tokenize_function(df): # 引数としてデータフレームを受け取るトークナイズ関数を定義します
    prompt_encodings = tokenizer( # プロンプトのトークン化
        df['prompt'].tolist(), # 'prompt'列の内容をリストに変換します
        padding='max_length', # 最大長までパディングを行います
        truncation=True, # 長すぎる入力を切り捨てます
        max_length=128, # 最大長を128に設定します
        return_tensors='tf' # TensorFlowテンソル形式で返します
    )
    response_a_encodings = tokenizer( # モデルAの応答のトークン化
        df['response_a'].tolist(), 
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    response_b_encodings = tokenizer( # モデルBの応答のトークン化
        df['response_b'].tolist(),
        padding='max_length',
        truncation=True,
        max_length=128,
        return_tensors='tf'
    )
    return prompt_encodings, response_a_encodings, response_b_encodings # トークン化されたプロンプト、応答A、応答Bを返します

# トークナイズを実行し、トレーニングデータとバリデーションデータを処理します
train_prompt_encodings, train_response_a_encodings, train_response_b_encodings = tokenize_function(X_train)
val_prompt_encodings, val_response_a_encodings, val_response_b_encodings = tokenize_function(X_val)
```

---The following area is a Code cell (cell numver is 13)---
```python
import tensorflow as tf # TensorFlowライブラリをインポートします
from tensorflow.keras.utils import to_categorical # カテゴリカルラベルに変換するためのユーティリティをインポートします

# 入力特徴量とラベルを準備します
train_labels = to_categorical(X_train['winner'].tolist(), num_classes=3) # トレーニングラベルをカテゴリカル形式に変換します
val_labels = to_categorical(X_val['winner'].tolist(), num_classes=3) # バリデーションラベルをカテゴリカル形式に変換します

# TensorFlowデータセットを作成します
train_dataset = tf.data.Dataset.from_tensor_slices(( # トレーニングデータセットをテンソルスライスから作成します
    {
        'input_ids_prompt': train_prompt_encodings['input_ids'], # プロンプトの入力ID
        'attention_mask_prompt': train_prompt_encodings['attention_mask'], # プロンプトのアテンションマスク
        'input_ids_response_a': train_response_a_encodings['input_ids'], # 応答Aの入力ID
        'attention_mask_response_a': train_response_a_encodings['attention_mask'], # 応答Aのアテンションマスク
        'input_ids_response_b': train_response_b_encodings['input_ids'], # 応答Bの入力ID
        'attention_mask_response_b': train_response_b_encodings['attention_mask'], # 応答Bのアテンションマスク
    },
    train_labels # 対応するトレーニングラベル
)).shuffle(1000).batch(1) # データをシャッフルし、バッチサイズ1で処理します

val_dataset = tf.data.Dataset.from_tensor_slices(( # バリデーションデータセットも同様に作成します
    {
        'input_ids_prompt': val_prompt_encodings['input_ids'],
        'attention_mask_prompt': val_prompt_encodings['attention_mask'],
        'input_ids_response_a': val_response_a_encodings['input_ids'],
        'attention_mask_response_a': val_response_a_encodings['attention_mask'],
        'input_ids_response_b': val_response_b_encodings['input_ids'],
        'attention_mask_response_b': val_response_b_encodings['attention_mask'],
    },
    val_labels # 対応するバリデーションラベル
)).batch(1) # バッチサイズ1で処理します
```

---The following area is a Code cell (cell numver is 14)---
```python
# 入力を定義します
input_ids_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_prompt") # プロンプトの入力IDを受け取るための層を定義します
attention_mask_prompt = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_prompt") # プロンプトのアテンションマスクを受け取るための層を定義します

input_ids_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_a") # 応答Aの入力IDを受け取るための層を定義します
attention_mask_response_a = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_a") # 応答Aのアテンションマスクを受け取るための層を定義します

input_ids_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="input_ids_response_b") # 応答Bの入力IDを受け取るための層を定義します
attention_mask_response_b = tf.keras.layers.Input(shape=(128,), dtype=tf.int32, name="attention_mask_response_b") # 応答Bのアテンションマスクを受け取るための層を定義します
```

---The following area is a Code cell (cell numver is 15)---
```python
import tensorflow as tf # TensorFlowライブラリをインポートします
from transformers import TFBertModel # TensorFlow用BERTモデルをインポートします

class BertEmbeddingLayer(tf.keras.layers.Layer): # カスタムBERT埋め込み層の定義
    def __init__(self, bert_model_name='bert-base-uncased', **kwargs): # コンストラクタ
        super(BertEmbeddingLayer, self).__init__(**kwargs) # 親クラスのコンストラクタを呼び出します
        self.bert = bert_model # BERTモデルを初期化します
        
    def call(self, inputs): # 層が呼び出されたときの処理を定義します
        input_ids, attention_mask = inputs # 入力の分解
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask) # BERTモデルに入力を渡して出力を取得します
        return outputs.last_hidden_state[:, 0, :]  # CLSトークンの埋め込みを返します（最初のトークンに対応）

# カスタムBERT層を初期化します
bert_layer = BertEmbeddingLayer()
```

---The following area is a Code cell (cell numver is 16)---
```python
prompt_embeddings = bert_layer([input_ids_prompt, attention_mask_prompt]) # プロンプトの埋め込みを取得します
response_a_embeddings = bert_layer([input_ids_response_a, attention_mask_response_a]) # 応答Aの埋め込みを取得します
response_b_embeddings = bert_layer([input_ids_response_b, attention_mask_response_b]) # 応答Bの埋め込みを取得します

# 埋め込みを結合します
combined_embeddings = tf.keras.layers.Concatenate()([prompt_embeddings, response_a_embeddings, response_b_embeddings]) # プロンプト、応答A、応答Bの埋め込みを結合します
```

---The following area is a Code cell (cell numver is 17)---
```python
dense_layer = tf.keras.layers.Dense(256, activation='relu')(combined_embeddings) # 結合された埋め込みを入力として、256ユニットの全結合層を定義します
dropout_layer = tf.keras.layers.Dropout(0.2)(dense_layer) # ドロップアウト層を追加して過学習を防ぎます（ドロップアウト率は20%）
output_layer = tf.keras.layers.Dense(3, activation='softmax')(dropout_layer) # 出力層を定義します。クラス数は3（勝者モデルA、モデルB、引き分け）

# モデルを構築し、コンパイルします
model = tf.keras.Model(inputs=[ # モデルの入力を指定します
    input_ids_prompt, attention_mask_prompt,
    input_ids_response_a, attention_mask_response_a,
    input_ids_response_b, attention_mask_response_b
], outputs=output_layer) # モデルの出力を指定します

model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5), loss='categorical_crossentropy', metrics=['accuracy']) # モデルをコンパイルします。オプティマイザーはAdam、損失関数はカテゴリカルクロスエントロピー、評価指標は精度を使用します
```

---The following area is a Code cell (cell numver is 18)---
```python
history = model.fit( # モデルのトレーニングを開始します
    train_dataset, # トレーニングデータセットを指定します
    validation_data=val_dataset, # バリデーションデータセットを指定します
    epochs=1 # エポック数は1に設定します（1回のトレーニングで全データを1周します）
)
```

---The following area is a Code cell (cell numver is 19)---
```python
def encode_text(texts, max_length): # テキストをエンコードする関数を定義します
    return tokenizer( # トークナイザーを使用してテキストをトークン化します
        texts,
        truncation=True, # 長すぎるテキストを切り捨てます
        padding='max_length', # 最大長までパディングします
        max_length=max_length, # 最大長を指定します
        return_tensors='tf' # TensorFlowテンソル形式で返します
    )
```

---The following area is a Code cell (cell numver is 20)---
```python
test # テストデータセットを表示します。これによりテストデータの内容を確認できます。
```

---The following area is a Code cell (cell numver is 21)---
```python
max_length = 128  # モデルの最大シーケンス長に応じて調整します
input_ids_prompt = encode_text(test['prompt'].tolist(), max_length) # テストセットのプロンプトをエンコードします
input_ids_response_a = encode_text(test['response_a'].tolist(), max_length) # テストセットの応答Aをエンコードします
input_ids_response_b = encode_text(test['response_b'].tolist(), max_length) # テストセットの応答Bをエンコードします
```

---The following area is a Code cell (cell numver is 22)---
```python
predictions = model.predict({ # モデルを使って予測を行います
    'input_ids_prompt': input_ids_prompt['input_ids'], # プロンプトの入力ID
    'attention_mask_prompt': input_ids_prompt['attention_mask'], # プロンプトのアテンションマスク
    'input_ids_response_a': input_ids_response_a['input_ids'], # 応答Aの入力ID
    'attention_mask_response_a': input_ids_response_a['attention_mask'], # 応答Aのアテンションマスク
    'input_ids_response_b': input_ids_response_b['input_ids'], # 応答Bの入力ID
    'attention_mask_response_b': input_ids_response_b['attention_mask'] # 応答Bのアテンションマスク
}) # これにより、テストデータに対してモデルが予測を行います
```

---The following area is a Code cell (cell numver is 23)---
```python
print("done till here") # ここまでの処理が完了したことを表示します
```

---The following area is a Code cell (cell numver is 24)---
```python
results = pd.DataFrame({ # 予測結果をデータフレームにまとめます
    'ID': test['id'], # テストセットのID列
    'winner_model_a': predictions[0], # モデルAの勝者確率
    'winner_model_b': predictions[1], # モデルBの勝者確率
    'winner_tie': predictions[2], # 引き分けの確率
})
```

---The following area is a Code cell (cell numver is 25)---
```python
results.to_csv('/kaggle/working/submission.csv', index=False) # 予測結果をCSVファイルとして保存します
# ファイル名は'submission.csv'で、インデックスは保存しません
```

---The following area is a Code cell (cell numver is 26)---
```python

```

---The following area is a Code cell (cell numver is 27)---
```python

```

---The following area is a Code cell (cell numver is 28)---
```python

```

---The following area is a Code cell (cell numver is 29)---
```python

```

** @@@ Jupyter Notebook numver 89, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Gemma 2 9bモデルを使用して埋め込みを計算し、それに基づいて分類器をトレーニングする準備を行うものです。ノートブックは、主に埋め込みの取得に焦点を当てています。

## 問題設定
ユーザーは、Chatbot Arenaのデータを用いて、異なるモデルが生成した応答に基づいてユーザーの好みを予測するための埋め込みを計算することを目的としています。具体的には、Gemma 2モデルを利用して、プロンプトと2つのモデルの応答を組み合わせたテキストから埋め込みを生成します。これにより、機械学習モデルのトレーニングデータとして使えるベクトル表現を作成します。

## 使用する手法やライブラリ
このノートブックでは、以下のライブラリと手法が用いられています。

1. **ライブラリのインポート**:
   - `bitsandbytes`: メモリ効率を向上させるためのライブラリ。
   - `transformers`: Hugging Faceのライブラリを使用し、Gemma 2モデルおよびトークナイザーを利用。
   - `torch`: PyTorchライブラリを用いた深層学習。
   - `pandas`、`numpy`、`matplotlib`: データ操作や可視化のためのライブラリ。

2. **モデルの設定**:
   - Gemma 2モデルを4ビット量子化し、2つのGPUデバイスを使用してモデルの異なるインスタンスをロードします。

3. **データ前処理**:
   - 提供されたトレーニングデータを読み込み、プロンプトと応答を整形します。

4. **トークン化**:
   - 整形したテキストをトークナイズし、PyTorchのテンソル形式に変換します。

5. **埋め込みの計算**:
   - モデルを用いてトークン化されたデータから埋め込みを計算。
   - 並行処理を利用し、2つのスレッドで異なるデバイスを活用して埋め込みを取得します。

6. **結果の保存**:
   - 計算された埋め込みを保存し、さらにトレーニングデータをCSV形式で保存します。

このノートブックは、Chatbot Arenaコンペティションのデータを活用し、LLM（大規模言語モデル）を使用したユーザーの好み予測へ向けた埋め込み計算を効率的に行うための基盤を提供しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## Gemma 2 - 9b 

Gemma 2 9bモデルを使用して埋め込みを取得し、それに基づいて分類器をトレーニングします。これはその第一部であり、ここでは埋め込みのみを計算します。他のモデルを使用することもできます。それでは始めましょう！

役に立ったら、ぜひアップボートしてください！

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
# bitsandbytesライブラリをインストールします
!pip install -q -U bitsandbytes 

# Hugging FaceのtransformersライブラリをGitHubからインストールします
!pip install -q git+https://github.com/huggingface/transformers

# SentencePieceライブラリをインストールします（テキスト処理に使用します）
!pip install sentencepiece
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします
import os  # OS関連の機能を提供します
import gc  # ガーベジコレクションを扱うためのライブラリです
import re  # 正規表現操作用ライブラリです
from time import time  # 時間を計測するための機能をインポートします

import torch  # PyTorchライブラリをインポートします
import transformers  # transformersライブラリをインポートします
import sklearn  # scikit-learnライブラリをインポートします（機械学習用）
import random  # ランダム数生成用ライブラリをインポートします
import numpy as np  # 数値計算用ライブラリをインポートします
import pandas as pd  # データ操作用ライブラリをインポートします
import matplotlib.pyplot as plt  # プロット作成用ライブラリをインポートします

from transformers import Gemma2ForCausalLM, GemmaTokenizer, BitsAndBytesConfig  # Gemmaモデルとトークナイザーをインポート

import time  # 時間計測用ライブラリを再度インポート
from torch.cuda.amp import autocast  # 自動混合精度計算用のautocast機能をインポート
from threading import Thread  # スレッド処理用ライブラリをインポート

# CUDAのメモリ効率的なSDPを有効化します。この設定により、GPUのメモリ使用効率が改善されます
torch.backends.cuda.enable_mem_efficient_sdp(False)
torch.backends.cuda.enable_flash_sdp(False)

# CUDAが利用可能でない場合、GPUが必要であることを知らせるメッセージを表示します
# if (not torch.cuda.is_available()): print("Sorry - GPU required!")
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
# 設定クラスを定義します
class CFG:
    # モデルのパスを指定します（Kaggle上のGemma 2 9bモデルのパス）
    MODEL_PATH = '/kaggle/input/gemma-2-9b-hf'
    # 最大入力シーケンスの長さを設定します
    MAX_LENGTH = 1024
    # バッチサイズを設定します
    BATCH_SIZE = 2
    
# 使用するデバイスを指定します（GPUの0番目のデバイス）
device0 = torch.device('cuda:0')
# 使用するデバイスを指定します（GPUの1番目のデバイス）
device1 = torch.device('cuda:1')
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# モデルの読み込み
```

---The following area is a Code cell (cell numver is 7)---
```python
# トークナイザーを指定したモデルパスから読み込みます
tokenizer = GemmaTokenizer.from_pretrained(CFG.MODEL_PATH)

# 4ビットの量子化設定を定義します
bnb_config_4bit = BitsAndBytesConfig(
    load_in_4bit=True,  # 4ビットでモデルを読み込みます
    bnb_4bit_compute_dtype=torch.float16,  # 計算データ型をfloat16に指定します
    bnb_4bit_use_double_quant=False)  # 二重量子化を使用するかどうかを指定します（ここでは無効にします）

# GPUの0番目のデバイスにモデルを読み込みます
model_0 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",  # モデルのバージョンをfloat16に指定
                                        device_map='cuda:0',  # モデルをデバイス0にマッピング
                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用します

# GPUの1番目のデバイスにモデルを読み込みます
model_1 = Gemma2ForCausalLM.from_pretrained(CFG.MODEL_PATH,
                                        revision="float16",  # モデルのバージョンをfloat16に指定
                                        device_map='cuda:1',  # モデルをデバイス1にマッピング
                                        quantization_config=bnb_config_4bit)  # 量子化設定を適用します
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# トレーニングデータの準備
```

---The following area is a Code cell (cell numver is 9)---
```python
# 入力文字列を処理する関数を定義します
def process(input_str):
    # 角括弧を剥がし、文字列を分割します
    stripped_str = input_str.strip('[]')
    # 文章を抽出し、各文の前後のダブルクォートを削除します
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # 最後の文を返します。もし文がなければ空文字を返します
    return sentences[-1] if sentences else ''
  
# トレーニングデータをCSVファイルから読み込みます
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena-additional-data-90k-columns/Merged_data.csv')

# 各列のプロンプトと応答を処理して整形します
train.loc[:, 'prompt'] = train['prompt'].apply(process)  # プロンプトを処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # モデルAの応答を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # モデルBの応答を処理

# モデルのインプットテキストを整形します
train['text'] = '<start_of_turn>User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n----\n\nModel B:\n'  + train['response_b'] + '<end_of_turn><eos>'  # モデルのプロンプトと応答の形式を設定
```

---The following area is a Code cell (cell numver is 10)---
```python
# データセットからサンプルを40,000件のみ取得します
train = train[:40000]

# トレーニングデータの最初の1行を表示します
train.head(1)  # データフレームの最初の1行を表示して中身を確認します
```

---The following area is a Code cell (cell numver is 11)---
```python
# インデックス10のトレーニングデータテキストを表示します
print(train['text'][10])  # 特定のテキストの内容を確認するために出力します
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# トークナイズ（トークン化）
```

---The following area is a Code cell (cell numver is 13)---
```python
# トレーニングテキストをトークナイズします
tokens = tokenizer(train['text'].tolist(),
                   padding='max_length',  # 最大長さにパディングを施します
                   max_length=CFG.MAX_LENGTH,  # 最大シーケンス長を設定します
                   truncation=True,  # 長すぎるテキストは切り捨てます
                   return_tensors='pt')  # PyTorchテンソルとして戻します

# トークン化された入力IDとアテンションマスクを取得します
INPUT_IDS = tokens['input_ids']  # 入力ID
ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスク

# 新しいDataFrameを作成します
data = pd.DataFrame()
data['INPUT_IDS'] = [tensor.tolist() for tensor in INPUT_IDS]  # 入力IDをリストに変換
data['ATTENTION_MASKS'] = [tensor.tolist() for tensor in ATTENTION_MASKS]  # アテンションマスクをリストに変換

# 最初の2行を表示して内容を確認します
data[:2]
```

---The following area is a Markdown cell (cell numver is 14)---
```markdown
# 埋め込みの計算
```

---The following area is a Code cell (cell numver is 15)---
```python
# 埋め込みを計算する関数を定義します
def get_embeddings(df, model, device, batch_size=CFG.BATCH_SIZE):  
    # INPUT_IDSとATTENTION_MASKSをTensorに変換します
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)

    embed_list = []  # 埋め込みを格納するリストを初期化

    # バッチサイズに基づいてデータフレームを処理します
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを計算
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送
        gc.collect()  # ガーベジコレクションを実行してメモリを解放
        torch.cuda.empty_cache()  # GPUメモリのキャッシュをクリア

        with torch.no_grad():  # 勾配を計算しないコンテキストで実行
            outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_mask, output_hidden_states=True)  # モデルに入力
            embed = outputs.hidden_states[-1]  # 最後の隠れ状態を取得
            embed_mean = torch.mean(embed, dim=1).cpu()  # 平均プーリングを行いCPUに戻す
            embed_list.append(embed_mean)  # 埋め込みをリストに追加
            
            torch.cuda.empty_cache()  # GPUメモリのキャッシュをクリア
        
    # リスト内のすべての埋め込みを結合します
    embeddings = torch.cat(embed_list, dim=0)
    return embeddings  # 計算された埋め込みを返します

# 埋め込みを計算するための関数を定義します
def compute_embed(df, model, device, results, index):
    results[index] = get_embeddings(df, model, device)  # 結果を指定されたインデックスに保存します
```

---The following area is a Code cell (cell numver is 16)---
```python
# 処理開始のタイムスタンプを記録します
st = time.time()

# データのサンプル数を取得します
N_SAMPLES = len(data)
# サンプル数の半分を計算します
half = round(N_SAMPLES / 2)
# データフレームを2つの部分に分割します
sub1 = data.iloc[0:half].copy()  # 前半のサブセット
sub2 = data.iloc[half:N_SAMPLES].copy()  # 後半のサブセット

results = {}  # 埋め込みを格納するための辞書を初期化

# スレッドを使用して並行処理を行います
t0 = Thread(target=compute_embed, args=(sub1, model_0, device0, results, 0))  # モデル0を使用するスレッド
t1 = Thread(target=compute_embed, args=(sub2, model_1, device1, results, 1))  # モデル1を使用するスレッド

# スレッドを開始します
t0.start()
t1.start()

# スレッドの終了を待ちます
t0.join()
t1.join()

# 処理が完了したことを知らせるメッセージをプリントします
print(f"Processing complete. Total time: {time.time() - st:.2f} seconds")  # 処理にかかった合計時間を表示
```

---The following area is a Code cell (cell numver is 17)---
```python
# 2つのスレッドから得られた埋め込みを結合します
embeddings = torch.cat([results[0], results[1]], dim=0)

# 結合された埋め込みの形状を表示します
embeddings.shape  # 埋め込みのテンソルの形状を出力して確認します
```

---The following area is a Code cell (cell numver is 18)---
```python
# ガーベジコレクションを実行してメモリを解放します
gc.collect()

# 使用が終了したモデルを削除してメモリを解放します
del model_1  # モデル1を削除
del model_0  # モデル0を削除

# GPUメモリのキャッシュをクリアします
torch.cuda.empty_cache()  # メモリの効率的な使用のためにキャッシュをクリアします
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 埋め込みの保存
```

---The following area is a Code cell (cell numver is 20)---
```python
# 埋め込みを保存するパスを指定します
save_path = 'gemma2_train_embed.npy'

# 埋め込みを.npyファイルとして保存します
np.save(save_path, embeddings.numpy())  # NumPy形式で埋め込みを保存

# 完全性のためにトレーニングデータも保存します
train.to_csv('train_embed.csv', index=False)  # トレーニングデータをCSVファイルとして保存

# 保存完了メッセージを表示します
print(f"Concatenated embeddings saved to {save_path}")  # 保存されたファイルのパスを表示します
```

** @@@ Jupyter Notebook numver 90, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のためのDeberta-v3 xsmallモデルを使用したスターターコードです。以下に、取り組んでいる問題や使用している手法、ライブラリについての要約を示します。

### 問題
このノートブックでは、異なる分岐によるLLMの応答の優劣を予測することを目的としています。具体的には、与えられたプロンプトに対して生成された応答のうち、どちらのモデルが優れているか（または引き分けか）を予測するモデルを訓練します。

### 手法
1. **モデルの選定**: Deberta-v3 xsmallモデルを使用しています。このモデルは、テキストの埋め込みを計算するために利用され、文脈情報を捉えるのに効果的です。

2. **データ処理**: プロンプトと応答を含むデータセットを処理し、必要なラベルを追加する処理を行っています。「winner_model_a」「winner_model_b」「winner_tie」に基づいてターゲットラベルを生成します。

3. **トークナイジング**: Hugging Faceの`AutoTokenizer`を使用してテキストをトークナイズし、特殊トークンを追加して、応答形式を整えています。

4. **データセットとデータローダー**: `TrainDataset`クラスを定義し、`DataLoader`を利用してミニバッチ処理を行います。

5. **モデルの定義**: `CustomModel`クラスを定義し、Deberta-v3モデルの出力を平均プーリングし、最終的な出力を得るための全結合層を追加しています。

6. **訓練と評価**: 訓練ループを設け、訓練データとバリデーションデータを使用してモデルを訓練しています。損失関数にはクロスエントロピーを使用し、訓練及び評価時にログを取るように設定されています。

### 使用ライブラリ
- **PyTorch**: モデルの構築、訓練、評価に主要に使用。
- **Transformers**: Hugging Faceのライブラリを使用してトークナイジングとモデルを利用。
- **NumPy**および**Pandas**: データ処理や操作に使用。
- **Scikit-learn**: 評価指標としてログロスを計算するために使用。

### 結論
本ノートブックは、自然言語モデルに基づいたチャットボットの応答を比較し、人間の好みに基づく優劣を予測するための基盤を提供しています。Deberta-v3を用いたアプローチは、対応するプロンプトと応答を効果的に処理し、最終的に人間の選好を学習するモデル作りのための重要なステップとされています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# このノートブックについて
- Deberta-v3 xsmallのスターターコード
- 元のノートブックは[こちら](https://www.kaggle.com/code/yasufuminakama/fb3-deberta-v3-base-baseline-train)
- 推論ノートブックは後ほど更新されます。
- このノートブックでは、プロンプトのみを使用していますので、response_a、response_bなどを使用できます。

このノートブックが役立った場合は、自由にアップボートしてください。

そして、元のノートブックにもアップボートしてください :)

`V1` - テスト用のデバッグモードを実行
- プロンプトのみを使用しています。さらにテキストを追加できます。

`V2` - 既存の前処理を削除（この部分にはさらなるアイデアが必要です）
- [こちら](https://www.kaggle.com/competitions/learning-agency-lab-automated-essay-scoring-2/discussion/497832)を参照してください。
- `max_length = 1024`と`lr = 1e-5`に変更しました。

`V3` - すべてのテキストまたはHuggingFaceなどに対して更新されます。
```

---The following area is a Code cell (cell numver is 2)---
```python
# ====================================================
# ディレクトリ設定
# ====================================================
import os

OUTPUT_DIR = './'
if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
```

---The following area is a Code cell (cell numver is 3)---
```python
!nvidia-smi
```

---The following area is a Code cell (cell numver is 4)---
```python
# ====================================================
# CFG
# ====================================================
class CFG:
    wandb=False
    competition='LMSYS'
    _wandb_kernel='test'
    debug=True
    apex=True
    print_freq=20
    num_workers=4
    model="microsoft/deberta-v3-xsmall" # ["microsoft/deberta-v3-small", "microsoft/deberta-v3-base"]
    gradient_checkpointing=False
    scheduler='cosine' # ['linear', 'cosine']
    batch_scheduler=True
    num_cycles=0.5
    num_warmup_steps=0
    epochs=4
    encoder_lr=1e-5
    decoder_lr=1e-5
    min_lr=1e-5
    eps=1e-6
    betas=(0.9, 0.999)
    batch_size=4
    max_len=2048
    weight_decay=0.01
    gradient_accumulation_steps=1
    max_grad_norm=1000
    target_label=['target']
    target_cols=['winner_model_a', 'winner_model_b', 'winner_tie']
    seed=42
    train=True
    
if CFG.debug:
    CFG.epochs = 2
    CFG.trn_fold = [0, 1]
```

---The following area is a Code cell (cell numver is 5)---
```python
# ====================================================
# ライブラリ
# ====================================================
import os
import gc
import re
import ast
import sys
import copy
import json
import time
import math
import string
import pickle
import random
import joblib
import itertools
import warnings
warnings.filterwarnings("ignore")

import scipy as sp
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)
from tqdm.auto import tqdm
from sklearn.metrics import log_loss
from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold

import torch
import torch.nn as nn
from torch.nn import Parameter
import torch.nn.functional as F
from torch.optim import Adam, SGD, AdamW
from torch.utils.data import DataLoader, Dataset

os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels transformers')
os.system('python -m pip install --no-index --find-links=../input/lmsys-pip-wheels tokenizers')

import tokenizers
import transformers
print(f"tokenizers.__version__: {tokenizers.__version__}")
print(f"transformers.__version__: {transformers.__version__}")
from transformers import AutoTokenizer, AutoModel, AutoConfig
from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup
%env TOKENIZERS_PARALLELISM=true

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
```

---The following area is a Code cell (cell numver is 6)---
```python
# ====================================================
# ユーティリティ
# ====================================================
def get_score(y_trues, y_preds):
    # ログロスを計算します
    score = log_loss(y_trues, y_preds, labels=[0, 1, 2])
    return score


def get_logger(filename=OUTPUT_DIR+'train'):
    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter
    logger = getLogger(__name__)
    logger.setLevel(INFO)
    handler1 = StreamHandler() 
    handler1.setFormatter(Formatter("%(message)s"))  # コンソールに出力されるフォーマット
    handler2 = FileHandler(filename=f"{filename}.log")  # ファイルに出力されるフォーマット
    handler2.setFormatter(Formatter("%(message)s"))
    logger.addHandler(handler1)
    logger.addHandler(handler2)
    return logger

LOGGER = get_logger()

def seed_everything(seed=42):
    # シード値を設定して再現性を持たせるための関数
    random.seed(seed)
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    
seed_everything(seed=CFG.seed)
```

---The following area is a Code cell (cell numver is 7)---
```python
# =====================================
# データフレームにターゲットを追加します
# =====================================
def add_label(df):
    labels = np.zeros(len(df), dtype=np.int32)  # ラベル用の配列を初期化
    labels[df['winner_model_a'] == 1] = 0  # モデルAが勝った場合のラベル
    labels[df['winner_model_b'] == 1] = 1  # モデルBが勝った場合のラベル
    labels[df['winner_tie'] == 1] = 2  # 引き分けの場合のラベル
    df['target'] = labels  # データフレームにターゲット列を追加
    return df
```

---The following area is a Code cell (cell numver is 8)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # 訓練データを読み込む
def process(input_str):
    # 入力文字列を処理する関数
    stripped_str = input_str.strip('[]')  # 先頭と末尾の[]を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 各文を抽出
    return sentences

# プロンプトと応答を処理する
train.loc[:, 'prompt'] = train['prompt'].apply(process)
train.loc[:, 'response_a'] = train['response_a'].apply(process)
train.loc[:, 'response_b'] = train['response_b'].apply(process)

# 訓練用に'Null'を削除
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index
train.drop(indexes, inplace=True)  # Nullインデックスを削除
train.reset_index(inplace=True, drop=True)

print(f"Total {len(indexes)} Null response rows dropped")  # 削除されたNull行の数を表示
print('Total train samples: ', len(train))  # 訓練サンプルの合計数を表示
```

---The following area is a Code cell (cell numver is 9)---
```python
add_label(train).head(5)  # ターゲットを追加した訓練データの最初の5行を表示
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トークナイザー
```

---The following area is a Code cell (cell numver is 11)---
```python
# ====================================================
# トークナイザー
# ====================================================
# 特殊トークンを定義
special_tokens = ['[R_STRAT]', '[R_END]', '<PROMPT>', '<RESPONSE>', '[NL]', '[NLNL]']

def preprocess_text(text):
    # テキストを前処理する関数
    text = text.replace('\n\n', ' [NLNL] ')  # 2つの改行を[nl NL]に変換
    text = text.replace('\n', ' [NL] ')  # 1つの改行を[nl]に変換
    return text

def format_conversation(row):
    # 会話形式にフォーマットする関数
    conversations = []
    num_turns = min(len(row['prompt']), len(row['response_a']), len(row['response_b']))  # 最小のターン数を取得
    
    for i in range(num_turns):
        # プロンプトと応答をフォーマット
        prompt = f"<PROMPT> {row['prompt'][i]}"
        response_a = f"<RESPONSE> [R_STRAT] {preprocess_text(row['response_a'][i])} [R_END]"
        response_b = f"[R_STRAT] {preprocess_text(row['response_b'][i])} [R_END]"
        conversations.append(f"{prompt} {response_a} {response_b}")  # 会話を追加
        
    return ' [NLNL] '.join(conversations)  # 会話を連結して返す

# 訓練データに形式を適用
train['text'] = train.apply(format_conversation, axis=1)

# トークナイザーに特殊トークンを追加
tokenizer = AutoTokenizer.from_pretrained(CFG.model)
special_tokens_dict = {'additional_special_tokens': special_tokens}
num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)  # 特殊トークンをトークナイザーに追加
CFG.tokenizer = tokenizer
```

---The following area is a Code cell (cell numver is 12)---
```python
tokenizer.decode(
    tokenizer(
        train['text'][1]
    ).input_ids
)  # トークン化されたテキストをデコードして表示
```

---The following area is a Code cell (cell numver is 13)---
```python
# ====================================================
# データセット
# ====================================================
def prepare_input(cfg, text):
    # 入力を準備する関数
    inputs = cfg.tokenizer.encode_plus(
        text, 
        return_tensors=None, 
        add_special_tokens=True, 
        max_length=CFG.max_len,
        pad_to_max_length=True,
        truncation=True
    )
    for k, v in inputs.items():
        inputs[k] = torch.tensor(v, dtype=torch.long)  # テンソルに変換
    return inputs


class TrainDataset(Dataset):
    def __init__(self, cfg, df):
        self.cfg = cfg
        self.texts = df['text'].values  # テキストを取得
        self.labels = df[cfg.target_label].values.squeeze().tolist()  # ラベルを取得

    def __len__(self):
        return len(self.texts)  # テキストの長さを返す

    def __getitem__(self, item):
        inputs = prepare_input(self.cfg, self.texts[item])  # 入力を準備
        label = torch.tensor(self.labels[item], dtype=torch.long)  # ラベルをテンソルに変換
        return inputs, label  # 入力とラベルを返す
    
    
def collate(inputs):
    mask_len = int(inputs["attention_mask"].sum(axis=1).max())  # 最大のマスク長を取得
    for k, v in inputs.items():
        inputs[k] = inputs[k][:,:mask_len]  # マスク長に合わせて切り詰める
    return inputs
```

---The following area is a Code cell (cell numver is 14)---
```python
# ====================================================
# モデル
# ====================================================
class MeanPooling(nn.Module):
    def __init__(self):
        super(MeanPooling, self).__init__()
        
    def forward(self, last_hidden_state, attention_mask):
        # 最後の隠れ状態を平均プーリングする関数
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()  # マスクを拡張
        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)  # 重み付きの埋め込みを合計
        sum_mask = input_mask_expanded.sum(1)
        sum_mask = torch.clamp(sum_mask, min=1e-9)  # マスクの合計を制限
        mean_embeddings = sum_embeddings / sum_mask  # 平均埋め込みを計算
        return mean_embeddings
    

class CustomModel(nn.Module):
    def __init__(self, cfg, config_path=None, pretrained=False):
        super().__init__()
        self.cfg = cfg
        if config_path is None:
            # モデルの設定を読み込む
            self.config = AutoConfig.from_pretrained(cfg.model, output_hidden_states=True)
            self.config.hidden_dropout = 0.  # ドロップアウトを無効にする
            self.config.hidden_dropout_prob = 0. 
            self.config.attention_dropout = 0.
            self.config.attention_probs_dropout_prob = 0.
            LOGGER.info(self.config) 
        else:
            self.config = torch.load(config_path)  # 設定をファイルから読み込む
        if pretrained:
            self.model = AutoModel.from_pretrained(cfg.model, config=self.config)  # 事前学習済みモデルを読み込む
        else:
            self.model = AutoModel(self.config)  # 新しいモデルを作成
        if self.cfg.gradient_checkpointing:
            self.model.gradient_checkpointing_enable()  # 勾配チェックポイントを有効にする
        self.pool = MeanPooling()  # 平均プーリングの定義
        self.fc = nn.Linear(self.config.hidden_size, 3)  # 出力層の定義
        self._init_weights(self.fc)  # 重みを初期化
        
    def _init_weights(self, module):
        # モデルの重みを初期化するメソッド
        if isinstance(module, nn.Linear):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)  # 初期化を設定
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.Embedding):
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.padding_idx is not None:
                module.weight.data[module.padding_idx].zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        
    def feature(self, inputs):
        # 特徴を抽出するメソッド
        outputs = self.model(**inputs)  # モデルを実行
        last_hidden_states = outputs[0]  # 最後の隠れ状態を取得
        feature = self.pool(last_hidden_states, inputs['attention_mask'])  # 平均プーリングを行う
        return feature

    def forward(self, inputs):
        # フォワードパスを実行するメソッド
        feature = self.feature(inputs)  # 特徴を抽出
        output = self.fc(feature)  # 出力を計算
        return output  # 出力を返す
```

---The following area is a Code cell (cell numver is 15)---
```python
# ====================================================
# ヘルパー関数
# ====================================================
class AverageMeter(object):
    """平均と現在の値を計算し、保存します"""
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0  # 現在の値
        self.avg = 0  # 平均値
        self.sum = 0  # 合計
        self.count = 0  # カウント

    def update(self, val, n=1):
        self.val = val  # 現在の値を更新
        self.sum += val * n  # 合計を更新
        self.count += n  # カウントを更新
        self.avg = self.sum / self.count  # 平均を再計算


def asMinutes(s):
    # 秒を分と秒に変換します
    m = math.floor(s / 60)  # 分を計算
    s -= m * 60  # 残りの秒を計算
    return '%dm %ds' % (m, s)  # 形式を返します


def timeSince(since, percent):
    # 経過時間と残り時間を計算します
    now = time.time()
    s = now - since  # 経過時間
    es = s / (percent)  # 予想される経過時間
    rs = es - s  # 残り時間
    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))  # 結果を返します


def train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device):
    # 訓練を実行する関数
    model.train()  # モデルを訓練モードに設定
    scaler = torch.cuda.amp.GradScaler(enabled=CFG.apex)  # 自動混合精度のスケーラー
    losses = AverageMeter()  # 損失を追跡
    start = end = time.time()  # 時間を記録
    global_step = 0  # グローバルステップ
    for step, (inputs, labels) in enumerate(train_loader):
        inputs = collate(inputs)  # 入力をコラテート
        for k, v in inputs.items():
            inputs[k] = v.to(device)  # デバイスに移動
        labels = labels.to(device)  # ラベルをデバイスに移動
        batch_size = labels.size(0)  # バッチサイズを取得
        with torch.cuda.amp.autocast(enabled=CFG.apex):
            y_preds = model(inputs)  # モデルの予測を取得
            loss = criterion(y_preds, labels)  # 損失を計算
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps  # 勾配蓄積の場合、損失をスケーリング
        losses.update(loss.item(), batch_size)  # 損失を更新
        scaler.scale(loss).backward()  # 後方伝播
        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG.max_grad_norm)  # 勾配のクリッピング
        if (step + 1) % CFG.gradient_accumulation_steps == 0:  # 勾配の更新条件
            scaler.step(optimizer)  # オプティマイザのステップ
            scaler.update()  # スケーラーを更新
            optimizer.zero_grad()  # 勾配をゼロに
            global_step += 1  # グローバルステップを増加
            if CFG.batch_scheduler:
                scheduler.step()  # スケジューラをステップ
                
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(train_loader)-1):  # ログ出力の条件
            print('Epoch: [{0}][{1}/{2}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  'Grad: {grad_norm:.4f}  '
                  'LR: {lr:.8f}  '
                  .format(epoch+1, step, len(train_loader), 
                          remain=timeSince(start, float(step+1)/len(train_loader)),
                          loss=losses,
                          grad_norm=grad_norm,
                          lr=scheduler.get_lr()[0]))  # ログ出力
    return losses.avg  # 平均損失を返す


def valid_fn(valid_loader, model, criterion, device):
    # 評価を実行する関数
    losses = AverageMeter()  # 損失を追跡
    model.eval()  # モデルを評価モードに設定
    preds = []  # 予測を保存
    start = end = time.time()  # 時間を記録
    for step, (inputs, labels) in enumerate(valid_loader):
        inputs = collate(inputs)  # 入力をコラテート
        for k, v in inputs.items():
            inputs[k] = v.to(device)  # デバイスに移動
        labels = labels.to(device)  # ラベルをデバイスに移動
        batch_size = labels.size(0)  # バッチサイズを取得
        with torch.no_grad():  # 勾配計算を無効に
            y_preds = model(inputs)  # モデルの予測を取得
            loss = criterion(y_preds, labels)  # 損失を計算
        if CFG.gradient_accumulation_steps > 1:
            loss = loss / CFG.gradient_accumulation_steps  # 勾配蓄積の場合、損失をスケーリング
        losses.update(loss.item(), batch_size)  # 損失を更新
        preds.append(y_preds.softmax(1).to('cpu').numpy())  # ソフトマックスの予測を保存
        end = time.time()
        if step % CFG.print_freq == 0 or step == (len(valid_loader)-1):  # ログ出力の条件
            print('EVAL: [{0}/{1}] '
                  'Elapsed {remain:s} '
                  'Loss: {loss.val:.4f}({loss.avg:.4f}) '
                  .format(step, len(valid_loader),
                          loss=losses,
                          remain=timeSince(start, float(step+1)/len(valid_loader))))  # ログ出力
    predictions = np.concatenate(preds)  # すべての予測を連結
    return losses.avg, predictions  # 平均損失と予測を返す
```

---The following area is a Code cell (cell numver is 16)---
```python
# ====================================================
# 訓練ループ
# ====================================================
def train_loop(train_df, valid_df):
    
    LOGGER.info(f"========== 訓練 ==========")

    # ====================================================
    # ローダー
    # ====================================================
    valid_labels = valid_df[CFG.target_label].values  # バリデーションラベルを取得
    
    train_dataset = TrainDataset(CFG, train_df)  # 訓練データセットを準備
    valid_dataset = TrainDataset(CFG, valid_df)  # バリデーションデータセットを準備

    train_loader = DataLoader(train_dataset,
                              batch_size=CFG.batch_size,
                              shuffle=True,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=True)  # 訓練ローダー
    valid_loader = DataLoader(valid_dataset,
                              batch_size=CFG.batch_size * 2,
                              shuffle=False,
                              num_workers=CFG.num_workers, pin_memory=True, drop_last=False)  # バリデーションローダー

    # ====================================================
    # モデルとオプティマイザ
    # ====================================================
    model = CustomModel(CFG, config_path=None, pretrained=True)  # モデルを初期化
    torch.save(model.config, OUTPUT_DIR+'config.pth')  # モデルの設定を保存
    model.to(device)  # モデルをデバイスに移動
    
    def get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):
        # オプティマイザのパラメータを取得する関数
        param_optimizer = list(model.named_parameters())  # パラメータをリストに格納
        no_decay = ["bias", "LayerNorm.bias", "LayerNorm.weight"]
        optimizer_parameters = [
            {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': weight_decay},
            {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],
             'lr': encoder_lr, 'weight_decay': 0.0},
            {'params': [p for n, p in model.named_parameters() if "model" not in n],
             'lr': decoder_lr, 'weight_decay': 0.0}
        ]
        return optimizer_parameters

    optimizer_parameters = get_optimizer_params(model,
                                                encoder_lr=CFG.encoder_lr, 
                                                decoder_lr=CFG.decoder_lr,
                                                weight_decay=CFG.weight_decay)  # オプティマイザのパラメータを取得
    optimizer = AdamW(optimizer_parameters, lr=CFG.encoder_lr, eps=CFG.eps, betas=CFG.betas)  # オプティマイザの初期化
    
    # ====================================================
    # スケジューラ
    # ====================================================
    def get_scheduler(cfg, optimizer, num_train_steps):
        # スケジューラを取得する関数
        if cfg.scheduler == 'linear':
            scheduler = get_linear_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps
            )
        elif cfg.scheduler == 'cosine':
            scheduler = get_cosine_schedule_with_warmup(
                optimizer, num_warmup_steps=cfg.num_warmup_steps, num_training_steps=num_train_steps, num_cycles=cfg.num_cycles
            )
        return scheduler
    
    num_train_steps = int(len(train_df) / CFG.batch_size * CFG.epochs)  # トレーニングステップ数を計算
    scheduler = get_scheduler(CFG, optimizer, num_train_steps)  # スケジューラの初期化

    # ====================================================
    # ループ
    # ====================================================
    criterion = nn.CrossEntropyLoss()  # 損失関数を定義
    
    best_score = np.inf  # 最良スコアを無限大で初期化

    for epoch in range(CFG.epochs):

        start_time = time.time()  # 開始時間を記録

        # 訓練
        avg_loss = train_fn(train_loader, model, criterion, optimizer, epoch, scheduler, device)  # 訓練を実行

        # 評価
        avg_val_loss, predictions = valid_fn(valid_loader, model, criterion, device)  # バリデーションを実行
        
        # スコアリング
        score = get_score(valid_labels, predictions)  # スコアを計算

        elapsed = time.time() - start_time  # 経過時間を計算

        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')
        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')  # ログ出力
        if CFG.wandb:
            wandb.log({"epoch": epoch+1, 
                       "avg_train_loss": avg_loss, 
                       "avg_val_loss": avg_val_loss,
                       "score": score})  # wandbにログを記録
            
        if best_score > score:  # 新しいベストスコアかどうか
            best_score = score  # ベストスコアを更新
            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')  # モデルを保存
            torch.save({'model': model.state_dict(),
                        'predictions': predictions},  # モデルの状態を保存
                        OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth")  # 最良モデルを保存

    predictions = torch.load(OUTPUT_DIR+f"{CFG.model.replace('/', '-')}_best.pth", 
                             map_location=torch.device('cpu'))['predictions']  # 最良モデルの予測をロード
    valid_df[[f"pred_{c}" for c in CFG.target_cols]] = predictions  # バリデーションデータフレームに予測を追加

    torch.cuda.empty_cache()  # CUDAメモリを空に
    gc.collect()  # ガーベジコレクションを実行
    
    return valid_df  # バリデーションデータフレームを返す
```

---The following area is a Code cell (cell numver is 17)---
```python
if __name__ == '__main__':
    
    def get_result(oof_df):
        # 結果を取得する関数
        labels = oof_df[CFG.target_cols].values  # ラベルを取得
        labels = np.argmax(labels, axis=1)  # ラベルの最大値のインデックスを取得
        preds = oof_df[[f"pred_{c}" for c in CFG.target_cols]].values  # 予測を取得
        score = get_score(labels, preds)  # スコアを計算
        LOGGER.info(f'Score: {score:<.4f}')  # ログ出力
    
    if CFG.train:
        # データを訓練とバリデーションに分割（80%訓練、20%バリデーション）
        train_df = train.sample(frac=0.8, random_state=CFG.seed).reset_index(drop=True)  # 訓練データをサンプリング
        valid_df = train.drop(train_df.index).reset_index(drop=True)  # 残りをバリデーションデータに設定

        _oof_df = train_loop(train_df, valid_df)  # 訓練ループを実行
        LOGGER.info(f"========== 結果 ==========")
        get_result(_oof_df)  # 結果を取得
        _oof_df.to_pickle(OUTPUT_DIR+'oof_df.pkl')  # 結果を保存
        
    if CFG.wandb:
        wandb.finish()  # wandbの終了
```

** @@@ Jupyter Notebook numver 91, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このノートブックは、KaggleのLMSYS - Chatbot Arenaコンペティションにおいて、大規模言語モデル（LLM）の応答の好ましさを予測するタスクに取り組んでいます。特に、LLMの応答がどちらのモデル（モデルAまたはモデルB）がより好まれるかを判定するための推論を行っています。

### 主要な取り組みと手法

1. **高速化された推論**:
   - 元のスクリプトに対して38%の推論時間短縮を達成しました。具体的には、トレーニングセットの最初の10,000サンプルにおける推論時間を65分から40分に短縮しました。
   - これを実現するために、二つの主要な技術を導入しました：
     - **動的パディング**: 各ミニバッチの最長シーケンスに合わせてオンザフライでパディングを行う手法。
     - **テストデータ長によるソート**: 入力の長さでソートすることで各ミニバッチ内のトークン数を揃え、余分なパディングを避けています。

2. **長い入力シーケンスの扱い**: 
   - `max_length`を1024から1280に変更することで、モデルのパフォーマンスが向上しました。

3. **使用ライブラリ**:
   - **PyTorch**、**Transformers**、**Tokenizers**、および**PEFT**（Parameter-Efficient Fine-Tuning）などの機械学習用のライブラリを使用しています。
   - 特に、Transformersライブラリを使ってLLMのトークナイゼーションやモデルの初期化を行っています。

4. **推論プロセスの実装**:
   - 複数のGPUを使用し、並列処理により推論を行うことで効率を向上させています。
   - モデルからの出力を用いて、各モデルの勝利の確率を計算し、最終的な結果を出力します。

5. **提出用ファイルの作成**:
   - 出力された確率をもとに「submission.csv」という形式でCSVファイルを生成し、Kaggleに提出可能なフォーマットに適合させています。

このノートブックは、効率的な推論手法を取り入れ、特にモデルのパフォーマンスを維持しつつ迅速に結果を生成することに焦点を当てています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## 🦙🦙🦙 このノートブックについて
このノートブックは、@kishanvavdaraによる [Inference - llama-3 8b](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b) を元に作成されています。リンクされたノートブックをまだ確認していない方は、ぜひチェックして高評価をつけることをお勧めします。
私は、@kishanvavdaraの作品に対していくつかの改善を加えました：

### 38%高速化された推論
トレーニングセットの最初の10,000サンプルを使用した推論時間は、このスクリプトを使うと40分かかります（TTAなし）。一方、元のスクリプトでは65分かかるため、精度に劣化がないまま38%高速化されました。私が主に追加したのは2つの機能です：

#### 1. 動的パディング
すべての入力を事前に固定長にパディングするのではなく、各ミニバッチの最長のシーケンスに合わせてオンザフライでパディングを適用します。

#### 2. テストデータを入力の長さでソート
動的パディングの利点を最大限に活用するために、テストデータは入力の長さでソートされます。こうすることで、各ミニバッチ内の入力がほぼ同じ長さになり、不要なパディングが減ります。

### より長い入力シーケンス
トレーニングデータの99%は1024トークン以内に収まっていますが、残りの1%はそれを超えています。また、テストセットにはさらに長いシーケンスが存在する可能性があるため、`max_length`をできるだけ長く設定する方が安全だと考えました。
`max_length`を1024から1280に変更すると、LBは0.989から0.983に改善されました。

## 試したが効果がなかったこと

### テスト時間の拡張 (TTA)
私は、response_aとresponse_bの順序を入れ替える簡単なTTAを試みました。この方法では、サンプルごとにモデルが2回呼び出されるため、推論時間が2倍に増加することに注意してください。
2つのソフトマックス確率を平均化するか、2つのロジットを平均してからソフトマックス確率を計算することができます。どちらのアプローチもLBを改善しませんでしたが、ソフトマックスを平均化する方がパフォーマンスが良かったです。
TTAは推論時間を2倍に増加させるため、サンプルごとにモデルを2回呼び出します。効率的な推論のおかげで、`max_length=1280`でTTAを有効にした状態でも、9時間以内に提出が完了しました。

### 各入力の切り捨て
元の実装では、プロンプト + response_a + response_bとして連結されたシーケンスを切り捨てています。切り捨てを単純に適用すると、一部の（稀ではありますが）プロンプトが1280トークンを超えるため、プロンプトのみの入力が生成され、モデルは勝者をランダムに推測するしかなくなります。
私は各入力を固定長にまず切り捨て、その後3つを連結する方法を試しましたが、LBは改善されませんでした。

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
# bitsandbytesライブラリを最新バージョンにインストールします。 
# -qオプションは出力を抑制し、--no-indexオプションはPyPIインデックスを無視してローカルのリンクを使用します。
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/

# transformersライブラリを最新バージョンにインストールします。
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/

# tokenizersライブラリを最新バージョンにインストールします。
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/

# peftライブラリを最新バージョンにインストールします。
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
# 必要なライブラリをインポートします

import time  # 時間計測のためのライブラリ
from dataclasses import dataclass  # データクラスを使用するためのライブラリ
from concurrent.futures import ThreadPoolExecutor  # スレッドプールを使用して並行処理を行うためのライブラリ

import torch  # PyTorchライブラリ
import sklearn  # 機械学習ライブラリ
import numpy as np  # 数値計算ライブラリ
import pandas as pd  # データ操作ライブラリ
from transformers import AutoTokenizer, LlamaForSequenceClassification, BitsAndBytesConfig  # Hugging FaceのTransformersライブラリから必要なクラスをインポート
from transformers.data.data_collator import pad_without_fast_tokenizer_warning  # パディングを行う関数をインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # PEFT関連のクラスや関数をインポート
```

---The following area is a Code cell (cell numver is 4)---
```python
# 使用可能なGPUの数が2つであることを確認します。
# 要求された条件を満たさない場合、エラーメッセージが表示されます。
assert torch.cuda.device_count() == 2, "申し訳ありませんが、マルチGPUが必要です！"

# CUDAのメモリ効率的なスパースドット製品（SDP）を無効にします。
torch.backends.cuda.enable_mem_efficient_sdp(False)

# CUDAのフラッシュスパースドット製品（SDP）を無効にします。
torch.backends.cuda.enable_flash_sdp(False)
```

---The following area is a Code cell (cell numver is 5)---
```python
@dataclass
class Config:
    # 使用するモデルの名前（またはファイルパス）を指定します。
    model_name = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'
    
    # モデルの重みの保存場所を指定します。
    weights_path = '/kaggle/input/lmsys-model/model'
    
    # 最大入力シーケンスの長さを指定します。
    max_length = 1280
    
    # バッチサイズを指定します。1回の処理で扱うサンプルの数です。
    batch_size = 8
    
    # 使用するデバイスを指定します。ここではCUDA（GPU）を指定しています。
    device = torch.device("cuda")    
    
    # テスト時間の拡張（TTA）を使用するかどうかを指定します。
    # <prompt>-<model-bの応答>-<model-aの応答>の形式で使用されます。
    tta = False  
    
    # 各入力にmax_length//3を適用するか、連結した入力にmax_lengthを適用するかを指定します。
    spread_max_length = False  

# Configクラスのインスタンスを作成します。
cfg = Config()
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
# データの準備
```

---The following area is a Code cell (cell numver is 7)---
```python
# テストデータをCSVファイルから読み込みます。
test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')

# 文字列のリストを連結する関数を定義します。
def process(input_str):
    # 引数として渡された文字列の両端のブラケットを取り除きます。
    stripped_str = input_str.strip('[]')
    # 各文を取り出し、ダブルクオーテーションを取り除いてリストを作成します。
    sentences = [s.strip('"') for s in stripped_str.split('","')]
    # リスト内の文を空白で結合して1つの文字列にします。
    return ' '.join(sentences)

# 'prompt'カラム、'response_a'カラム、'response_b'カラムに対してprocess関数を適用します。
test.loc[:, 'prompt'] = test['prompt'].apply(process)
test.loc[:, 'response_a'] = test['response_a'].apply(process)
test.loc[:, 'response_b'] = test['response_b'].apply(process)

# 処理したデータを表示します。最初の5行を表示します。
display(test.head(5))
```

---The following area is a Markdown cell (cell numver is 8)---
```markdown
# トークナイズ（単語分割）
```

---The following area is a Code cell (cell numver is 9)---
```python
# トークナイザーを使用してテキストをトークンに変換する関数を定義します。
def tokenize(
    tokenizer, prompt, response_a, response_b, max_length=cfg.max_length, spread_max_length=cfg.spread_max_length
):
    # ユーザーのプロンプトに接頭辞を追加します。
    prompt = ["User prompt: " + p for p in prompt]
    # モデルAの応答に接頭辞を追加します。
    response_a = ["\n\nModel A :\n" + r_a for r_a in response_a]
    # モデルBの応答に接頭辞を追加します。
    response_b = ["\n\n--------\n\nModel B:\n" + r_b for r_b in response_b]
    
    if spread_max_length:
        # max_lengthを3で割った値を使用して個々のトークンの長さを設定します。
        prompt = tokenizer(prompt, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_a = tokenizer(response_a, max_length=max_length//3, truncation=True, padding=False).input_ids
        response_b = tokenizer(response_b, max_length=max_length//3, truncation=True, padding=False).input_ids
        
        # プロンプトと応答のトークンを結合してinput_idsを作成します。
        input_ids = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        # attention_maskを作成します。各トークンの有効性を示します。
        attention_mask = [[1] * len(i) for i in input_ids]
    else:
        # プロンプトと応答を結合して1つのテキストリストを作成します。
        text = [p + r_a + r_b for p, r_a, r_b in zip(prompt, response_a, response_b)]
        # 一括でトークナイザーを使ってトークン化します。
        tokenized = tokenizer(text, max_length=max_length, truncation=True, padding=False)
        # input_idsとattention_maskを取得します。
        input_ids = tokenized.input_ids
        attention_mask = tokenized.attention_mask
    
    return input_ids, attention_mask  # トークンIDとアテンションマスクを返します。
```

---The following area is a Code cell (cell numver is 10)---
```python
%%time  # このセルの実行時間を計測します。

# トークナイザーを事前学習済みモデルからロードします。
tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')

# データフレームを初期化します。
data = pd.DataFrame()
data["id"] = test["id"]  # テストデータのIDを追加します。
# トークナイズした情報をdataフレームに追加します。
data["input_ids"], data["attention_mask"] = tokenize(tokenizer, test["prompt"], test["response_a"], test["response_b"])
data["length"] = data["input_ids"].apply(len)  # 各入力の長さを計算します。

# 拡張データフレームを初期化します。
aug_data = pd.DataFrame()
aug_data["id"] = test["id"]  # テストデータのIDを追加します。
# response_aとresponse_bを入れ替えたトークナイズを行います。
aug_data['input_ids'], aug_data['attention_mask'] = tokenize(tokenizer, test["prompt"], test["response_b"], test["response_a"])
aug_data["length"] = aug_data["input_ids"].apply(len)  # 各入力の長さを計算します。
```

---The following area is a Code cell (cell numver is 11)---
```python
# トークンIDからデコードして、最初の入力を人間が読める形式に変換します。
print(tokenizer.decode(data["input_ids"][0]))
```

---The following area is a Code cell (cell numver is 12)---
```python
# 拡張データフレームにおける最初の入力をトークンIDからデコードし、人間が読める形式に変換します。
print(tokenizer.decode(aug_data["input_ids"][0]))
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# モデルの読み込み
各GPUに1つのモデルを読み込みます。
```

---The following area is a Code cell (cell numver is 14)---
```python
# BitsAndBytesの設定を行います。
bnb_config = BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットでモデルをロードします。
    bnb_8bit_compute_dtype=torch.float16,  # 計算時のデータ型をfloat16に設定します。
    bnb_8bit_use_double_quant=False,  # ダブル量子化を使用しない設定です。
)

# GPU 0にベースモデルをロードします。
device_0 = torch.device('cuda:0')  # 使用するデバイスを指定します。
base_model_0 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,  # モデルの名前を指定します。
    num_labels=3,  # 出力ラベルの数を指定します。
    torch_dtype=torch.float16,  # トークンのデータ型をfloat16に設定します。
    quantization_config=bnb_config,  # 量子化構成を設定します。
    device_map='cuda:0'  # モデルをCUDAデバイス0にマッピングします。
)
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定します。

# GPU 1にベースモデルをロードします。
device_1 = torch.device('cuda:1')  # 使用するデバイスを指定します。
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    cfg.model_name,  # モデルの名前を指定します。
    num_labels=3,  # 出力ラベルの数を指定します。
    torch_dtype=torch.float16,  # トークンのデータ型をfloat16に設定します。
    quantization_config=bnb_config,  # 量子化構成を設定します。
    device_map='cuda:1'  # モデルをCUDAデバイス1にマッピングします。
)
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンのIDを設定します。
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
# 重みのロード
```

---The following area is a Code cell (cell numver is 16)---
```python
# LoRAの設定を行います。
peft_config = LoraConfig(
    r=16,  # LoRAのランクを指定します。
    lora_alpha=32,  # LoRAのアルファ値を設定します。
    lora_dropout=0.10,  # LoRAに使用するドロップアウト率を設定します。
    bias='none',  # バイアスの設定を指定します。
    inference_mode=True,  # 推論モードを有効にします。
    task_type=TaskType.SEQ_CLS,  # タスクのタイプをシーケンス分類に設定します。
    target_modules=['o_proj', 'v_proj']  # 対象モジュールを指定します。
)
```

---The following area is a Code cell (cell numver is 17)---
```python
# PEFTモデルを取得します。
model_0 = get_peft_model(base_model_0, peft_config).to(device_0)  # base_model_0にPEFT設定を適用し、デバイス0に移動します。
# 重みをロードします。
model_0.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを指定されたパスからロードします。
model_0.eval()  # モデルを評価モードに設定します。

# PEFTモデルを取得します。
model_1 = get_peft_model(base_model_1, peft_config).to(device_1)  # base_model_1にPEFT設定を適用し、デバイス1に移動します。
model_1.load_state_dict(torch.load(cfg.weights_path), strict=False)  # 重みを指定されたパスからロードします。
model_1.eval()  # モデルを評価モードに設定します。
```

---The following area is a Code cell (cell numver is 18)---
```python
# モデルの学習可能なパラメータを表示します。
model_0.print_trainable_parameters()  # model_0の学習可能なパラメータを表示します。
model_1.print_trainable_parameters()  # model_1の学習可能なパラメータを表示します。
```

---The following area is a Markdown cell (cell numver is 19)---
```markdown
# 推論
```

---The following area is a Code cell (cell numver is 20)---
```python
# 推論を行う関数を定義します。
@torch.no_grad()  # 勾配計算を無効にします。推論時は必要ないため効率的です。
@torch.cuda.amp.autocast()  # 自動混合精度を使用して計算を高速化します。
def inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
    a_win, b_win, tie = [], [], []  # 各モデルの勝者の確率を格納するリストを初期化します。
    
    # データフレームをバッチサイズごとに処理します。
    for start_idx in range(0, len(df), batch_size):
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終了インデックスを計算します。
        tmp = df.iloc[start_idx:end_idx]  # 現在のバッチのデータを取得します。
        input_ids = tmp["input_ids"].to_list()  # 入力IDをリストに変換します。
        attention_mask = tmp["attention_mask"].to_list()  # アテンションマスクをリストに変換します。
        
        # パディングを行い、モデルの入力形式に変換します。
        inputs = pad_without_fast_tokenizer_warning(
            tokenizer,
            {"input_ids": input_ids, "attention_mask": attention_mask},
            padding=True,
            max_length=max_length,
            pad_to_multiple_of=None,
            return_tensors="pt",  # PyTorchテンソルとして返します。
        )
        
        # モデルを使って推論を行います。
        outputs = model(**inputs.to(device))  # デバイスに入力を移動させてモデルに渡します。
        proba = outputs.logits.softmax(-1).cpu()  # ロジットにソフトマックスを適用し、確率を計算します。

        # 各モデルの勝者の確率をリストに追加します。
        a_win.extend(proba[:, 0].tolist())  # モデルAの勝者の確率を追加します。
        b_win.extend(proba[:, 1].tolist())  # モデルBの勝者の確率を追加します。
        tie.extend(proba[:, 2].tolist())    # 引き分けの確率を追加します。
    
    # データフレームに勝者の情報を追加します。
    df["winner_model_a"] = a_win
    df["winner_model_b"] = b_win
    df["winner_tie"] = tie
    
    return df  # 更新されたデータフレームを返します。
```

---The following area is a Code cell (cell numver is 21)---
```python
# 時間計測を開始します。
st = time.time()

# 動的パディングを最大限活用するために入力の長さでソートします。
data = data.sort_values("length", ascending=False)
# サブセット1とサブセット2ではトークンの総数がほぼ同じである必要があります。
sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピーします。
sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピーします。

# スレッドプールエグゼキュータを使用して並行処理を行います。
with ThreadPoolExecutor(max_workers=2) as executor:
    # inference関数をサブセット1とサブセット2に対してモデル0とモデル1で実行します。
    results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

# 結果をデータフレームとして結合します。
result_df = pd.concat(list(results), axis=0)
proba = result_df[["winner_model_a", "winner_model_b", "winner_tie"]].values  # 勝者の確率を抽出します。

# 経過時間を表示します。
print(f"経過時間: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 22)---
```python
# 時間計測を開始します。
st = time.time()

# テスト時間の拡張（TTA）が有効な場合の処理です。
if cfg.tta:
    # 入力の長さでソートし、処理速度を向上させます。
    data = aug_data.sort_values("length", ascending=False)
    sub_1 = data.iloc[0::2].copy()  # 偶数インデックスのデータをサブセット1にコピーします。
    sub_2 = data.iloc[1::2].copy()  # 奇数インデックスのデータをサブセット2にコピーします。

    # スレッドプールエグゼキュータを使用して並行処理を行います。
    with ThreadPoolExecutor(max_workers=2) as executor:
        # inference関数をサブセット1とサブセット2に対してモデル0とモデル1で実行します。
        results = executor.map(inference, (sub_1, sub_2), (model_0, model_1), (device_0, device_1))

    # 結果をデータフレームとして結合します。
    tta_result_df = pd.concat(list(results), axis=0)
    # TTAの順序が反転していることを考慮します。
    tta_proba = tta_result_df[["winner_model_b", "winner_model_a", "winner_tie"]].values  
    # 元の結果とTTA結果を平均化します。
    proba = (proba + tta_proba) / 2

# 経過時間を表示します。
print(f"経過時間: {time.time() - st}")
```

---The following area is a Code cell (cell numver is 23)---
```python
# 結果データフレームに、勝者の確率を更新します。
result_df.loc[:, "winner_model_a"] = proba[:, 0]  # モデルAの勝者の確率を追加します。
result_df.loc[:, "winner_model_b"] = proba[:, 1]  # モデルBの勝者の確率を追加します。
result_df.loc[:, "winner_tie"] = proba[:, 2]      # 引き分けの確率を追加します。

# 提出用データフレームを作成します。
submission_df = result_df[["id", 'winner_model_a', 'winner_model_b', 'winner_tie']]  
# 提出用データフレームをCSVファイルとして保存します。
submission_df.to_csv('submission.csv', index=False)  
# 提出用データフレームを表示します。
display(submission_df)
```

---The following area is a Code cell (cell numver is 24)---
```python

```

** @@@ Jupyter Notebook numver 92, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、LMSYS - Chatbot Arenaコンペティションにおけるトピックモデリングを目的としたもので、主要な問題はテキストデータから有意義なトピックを抽出し、それを視覚化することです。特に、ユーザーからのプロンプトとそれに対する応答を解析し、各プロンプトの潜在的なパターンを特定することで、チャットボットの応答を改善するための洞察を得ることを目指しています。

### 使用されている手法およびライブラリ

1. **テキスト処理とデータ前処理**:
   - **Pandas**: データフレームを操作し、トレーニングとテストデータを読み込み、空の応答を除外するために使用。
   - **literal_eval**: 文字列をPythonのリテラルに変換するために使用。

2. **トピックモデリング**:
   - **BERTopic**: トピックモデリングを行うためのライブラリで、以下の一連の処理を実施:
     - **SentenceTransformer**: 文を埋め込みベクトルに変換。
     - **UMAP**: 次元削減手法で、データの次元数を減少させるために使用。
     - **HDBSCAN**: 様々な形状のクラスタを見つけるための密度ベースのクラスタリング手法。
     - **CountVectorizer / TfidfVectorizer**: テキストデータを数値ベクトルに変換。
     - **c-TF-IDF**: 重み付けされたトピック表現を生成するために使われる。

3. **視覚化手法**:
   - トピックの重要度や関係性を示すために、バーチャートやヒートマップを生成。具体的には `visualize_barchart` や `visualize_heatmap` を使用。

4. **ガイド付きトピックモデリング**:
   - 修正されたタスクキーワードを用いて、ガイド付きトピックモデリングを実施し、特定のNLPタスクに関連するテーマを強化。

このノートブックでは、具体的なトピックモデリングの手続きや視覚化に重点を置いており、同時にデータの探索的分析（EDA）の結果も考慮しながら、ユーザーの好みに影響を与えうる要素（バイアスや冗長性など）を考察しています。最終的に、特定のトピックがどのように分布し、各トピックがどの文脈において重要であるのかを示すことで、ほしい応答を導く意思決定を支援することを目指しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# インポート（ライブラリの読み込み）
```

---The following area is a Code cell (cell numver is 2)---
```python
# IPython.displayモジュールからclear_output関数をインポートします。
# この関数は、Jupyter Notebookの出力をクリアするために使用されます。
```

---The following area is a Code cell (cell numver is 3)---
```python
# bertopicというライブラリをインストールします。
# -qオプションは、インストール中の進行状況を表示しないようにするためのものです。
# これにより、インストールのメッセージが少なくなり、ノートブックの出力がスッキリします。
```

---The following area is a Code cell (cell numver is 4)---
```python
# numpyライブラリを、バージョン1.23.5にアップグレードしてインストールします。
# -Uオプションは、指定したバージョンがインストールされていない場合や古いバージョンがインストールされている場合に更新することを指示します。
# NumPyは、数値計算や配列操作に非常に便利なライブラリです。
```

---The following area is a Code cell (cell numver is 5)---
```python
# pandasライブラリをpdという略称でインポートします。
# literal_eval関数をastモジュールからインポートします。これは、文字列をPythonのリテラルに評価するために使用します。
# numpyライブラリをnpという略称でインポートします。数値計算用のライブラリです。
# osモジュールをインポートします。これは、オペレーティングシステムとのインタラクションに使用します。
# randomモジュールをインポートします。これは、ランダムな数値や選択を生成するために使用します。
# reモジュールをインポートします。これは、正規表現操作を行うために使用します。

# UMAPとHDBSCANは、データの次元削減およびクラスタリングに使用されるライブラリです。
from umap import UMAP
from hdbscan import HDBSCAN

# CountVectorizerとTfidfVectorizerは、テキストデータを数値ベクトルに変換するために使用される方法です。
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

# SentenceTransformerは、文章をベクトルに変換するためのモデルを提供するライブラリです。
from sentence_transformers import SentenceTransformer

# BERTopicは、トピックモデリングを行うためのライブラリです。
from bertopic import BERTopic
from bertopic.representation import KeyBERTInspired, MaximalMarginalRelevance
from bertopic.vectorizers import ClassTfidfTransformer
```

---The following area is a Markdown cell (cell numver is 6)---
```markdown
**始める前に、以下はこのノートブックで行われた素晴らしい探索的データ分析（EDA）から得られた概要です：**https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda

**一般的な発見**

- トレーニングセットには64種類の異なるモデルがあります。
- 各プロンプトに対して3つの応答があり、それぞれ異なるモデルからのもので、人間の好みに基づいてランク付けされています。
- トレーニングデータで最も一般的（頻出）なモデルは以下の通りです：

    - gpt-4-1106-preview 
    - gpt-3.5-turbo-0613
    - gpt-4-0613 
    - claude-2.1 
    - (gpt-4-0314, claude-instant-1)
    
    
- ターン数：トレーニングデータのプロンプト/応答ペアの数。

    - 約86.88％の会話はシングルターンです。
    - 99.19％以上の会話は6ターン未満です。
    - 最大ターン数は36です。
    
    

**モデルの応答の好み**

- gpt-3.5-turbo-0314、gpt-4-0125-preview、gpt-4-1106-previewを含む3つのLLMが勝率50％を超えています。
- 低いタイ率は勝者をより決定的に判断できることを意味します。


**応答/プロンプトにおけるバイアスと相関関係**

- 人間の判断者には位置バイアスがありません。つまり、トレーニングデータにおける位置A、B、Cには、アノテータの好みに関する位置バイアスが存在しません。
 
- 同じプロンプトに対するモデルの応答の長さの相関：同じプロンプトに対する応答の長さには強い相関があります。
 
- プロンプトの長さと応答の長さの相関：プロンプトの長さと応答の長さの間には線形関係があるが、相関はかなり弱いようです。

- 冗長性バイアス（応答の冗長さが人間の好みに与える影響）：
    - データには明確な冗長性バイアスがあります。
    - 「平均応答長」と「勝率」の相関 = 0.488
    - gpt-4-0125-previewとgpt-4-1106-previewは、平均応答長が最も長いトップ2のモデルです。
    

**ヌル/空の応答またはプロンプト**

- 空のプロンプトを持つサンプルが5つあります。
- 空のプロンプトはすべて単一のスペース " " で、会話の最後のプロンプトに現れます。
- モデルは空のプロンプトが送信されても応答を続けることができます。
- 欠落した応答は空またはNoneである可能性があります。

- 判断者への影響：
    - タイ率は約0.15に低下し、これは非常に妥当です。
    - もし1つのモデルの応答が欠落している場合、判断者は通常応答する別のモデルに投票したり、タイにする傾向があります。
    

**トレーニングデータとテストデータの確認**
```

---The following area is a Code cell (cell numver is 7)---
```python
# 応答オブジェクトを解析するための関数を定義します。
def parse_response(response_object:str)->[str,]:
    # response_objectが文字列として与えられ、それをPythonリテラルに評価しようとします。
    try:
        resp = literal_eval(response_object)
        
    except Exception as e:
        # もし評価に失敗した場合、'null'をNoneに置き換えます。
        try:
            response_object = response_object.replace('null',None)
            resp = literal_eval(response_object)
        except Exception as e:
            # 評価失敗時、デバッグ用に応答オブジェクトの型と内容を出力しますが、コメントアウトされています。
            # print(type(response_object),response_object)
            # 空のリストを返します。
            resp = []

    # 解析された応答を返します。
    return resp
```

---The following area is a Code cell (cell numver is 8)---
```python
# トレーニングデータとテストデータを読み込みます。
train = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/train.csv")
# 'prompt'列の各要素に対してliteral_evalを適用し、文字列をPythonのリテラルに変換します。
train['prompt'] = train['prompt'].apply(literal_eval)
# 'response_a'列と'response_b'列の各要素に対してparse_response関数を適用し、応答を解析します。
train['response_a'] = train['response_a'].apply(parse_response)
train['response_b'] = train['response_b'].apply(parse_response)

# テストデータを読み込みます。
test = pd.read_csv("/kaggle/input/lmsys-chatbot-arena/test.csv")

# トレーニングデータとテストデータの形状（行数と列数）を出力します。
_ = print(train.shape),print(test.shape)
```

---The following area is a Code cell (cell numver is 9)---
```python
# トレーニングデータの最初の5行を表示します。
# これにより、データの構造や内容を確認することができます。
train.head()
```

---The following area is a Code cell (cell numver is 10)---
```python
# 空の応答を持つ行を削除します。
# response_aとresponse_bの両方が非空である行のみをフィルタリングします。
train_ss = train[(train.response_a.apply(len)>0) & (train.response_b.apply(len)>0)]

# フィルタリング後のデータの形状（行数と列数）を表示します。
train_ss.shape
```

---The following area is a Code cell (cell numver is 11)---
```python
# テストデータの最初の5行を表示します。
# これにより、テストデータの構造や内容を確認することができます。
test.head()
```

---The following area is a Markdown cell (cell numver is 12)---
```markdown
# トピックモデリング

**仕組み (出典: https://maartengr.github.io/BERTopic/algorithm/algorithm.html)**

![image.png](attachment:60ac0df8-5866-4948-9240-7e6453976a10.png)

1. ドキュメントの埋め込み：トランスフォーマーベースのモデル埋め込みを使用して、ドキュメントを数値表現に変換することから始めます。

2. 次元削減：ドキュメントの数値表現を作成した後、これらの表現の次元を削減する必要があります。クラスターモデルは、高次元データを扱う際に「次元の呪い」のために困難を抱えることがよくあります。

3. ドキュメントのクラスタリング：埋め込みを減少させた後、データのクラスタリングを始めることができます。そのために、密度ベースのクラスタリング手法であるHDBSCANを利用します。HDBSCANは、さまざまな形状のクラスターを見つけることができ、可能な場合には外れ値を特定する優れた機能を持っています。

4. Bag-of-words：BERTopicのアルゴリズムでは、モジュラリティを許可しながらトピック表現を作成するためにHDBSCANをクラスタリングモデルとして使用します。これは、異なる密度と形状のクラスターを収容するからです。重心ベースの手法を使用するのではなく、クラスター内のすべてのドキュメントを単一のドキュメントに統合し、各単語の頻度をカウントしてbag-of-words表現を形成します。この表現は、クラスターサイズの違いに対して正規化され、特定のクラスター構造を仮定せずにクラスターレベルでの単語に焦点を当てます。

5. トピック表現：生成されたbag-of-words表現から、1つのクラスターを他のクラスターと区別する要素を知りたいと思います。クラスター1に典型的な単語は何で、他のすべてのクラスターにはあまり典型的でないのはどれでしょうか？これを解決するために、ドキュメントではなくトピック（すなわちクラスター）を考慮するようにTF-IDFを修正する必要があります。

6. トピック表現の微調整：生成されたc-TF-IDFトピックを候補トピックとして考えることができます。それぞれに一連のキーワードと、トピック表現をさらに微調整するために使用できる代表的なドキュメントが含まれます。各トピックに対して代表的なドキュメントのセットを持つことは、大きな利点です。これにより、限られた数のドキュメントの微調整が可能になります。これにより、大規模モデルの計算コストが削減され、各トピックごとにその小さな代表的なドキュメントのセットでのみ操作を行う必要があります。

次に、教師なしトピックモデリングアプローチを使用して、プロンプト内の潜在的なパターンを見つけてみましょう。これにより、プロンプトに含まれるトピックの大まかな理解が得られます。
```

---The following area is a Code cell (cell numver is 13)---
```python
# モジュールのセットアップ

# ステップ1 - 埋め込みを抽出する
embedding_model = SentenceTransformer("all-MiniLM-L6-v2")

# ステップ2 - 次元を削減する
umap_model = UMAP(
    n_neighbors=20,  # 各点の近傍の数
    n_components=5,  # 出力する次元数
    min_dist=0.0,    # 最小距離の制約
    metric="cosine", # コサイン距離を使用
    random_state=7,  # 再現性のためのランダムシード
)

# ステップ3 - 減少した埋め込みをクラスタリングする
hdbscan_model = HDBSCAN(
    min_cluster_size=32,  # 最小クラスタサイズ
    min_samples=1,        # 最小サンプル数
    metric="euclidean",   # ユークリッド距離を使用
    cluster_selection_method="eom", # クラスタ選択メソッド
    prediction_data=True   # クラスタリング結果を予測するためのデータを保持
)

# ステップ4 - トピックをトークナイズする
vectorizer_model = CountVectorizer(stop_words="english") # 英語のストップワードを取り除く

# ステップ5 - トピック表現を作成する
ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True,) # 頻出単語を減少させるオプション

# `bertopic.representation`モデルを使用してトピック表現を微調整
representation_model = MaximalMarginalRelevance(diversity=0.4,   # 多様性の調整
                                                top_n_words=15    # トピック表現に含める単語の数
                                               )
```

---The following area is a Code cell (cell numver is 14)---
```python
# トピックモデリングのパイプラインを構築します。
topic_model = BERTopic(
    embedding_model=embedding_model,  # 埋め込みモデルを指定
    umap_model=umap_model,            # 次元削減モデルを指定
    hdbscan_model=hdbscan_model,      # クラスタリングモデルを指定
    vectorizer_model=vectorizer_model, # トークナイザーを指定
    ctfidf_model=ctfidf_model,        # c-TF-IDFモデルを指定
    representation_model=representation_model, # 表現モデルを指定
    n_gram_range=(1,5),               # n-gramの範囲を設定（1から5）
    language="english"                # 言語を英語に設定
)
```

---The following area is a Code cell (cell numver is 15)---
```python
# トレーニングプロンプトを結合します。
# 各プロンプトの要素を2つの改行で結合し、リストに変換します。
train_prompt_concatenated = train.prompt.apply(lambda x: "\n\n".join(x)).to_list()

# 結合されたプロンプトの数を表示します。
len(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 16)---
```python
# トピックモデリングの適合を行います。
# train_prompt_concatenatedに基づいてトピックをフィットさせ、トピックとその確率を得ます。
# %%timeマジックコマンドは、このセルの実行時間を計測します。
%%time
topics, topic_proba = topic_model.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 17)---
```python
# トピックモデルを保存します。
# 保存時に"safetensors"形式を使用します。
topic_model.save("topic_model_unguided", serialization="safetensors")
```

---The following area is a Code cell (cell numver is 18)---
```python
# ユニークなトピックの数を表示します。
# np.uniqueを使用して、トピックの配列からユニークな値の数を取得します。
print(f"ユニークなトピックの数: {len(np.unique(topics))}")
```

---The following area is a Code cell (cell numver is 19)---
```python
# トピック情報を取得します。
# トピックに関する詳細情報を含むデータフレームを取得します。
topic_info = topic_model.get_topic_info()

# 最初の10行を表示します。
topic_info.head(10)
```

---The following area is a Code cell (cell numver is 20)---
```python
# 特定のトピックの表現を取得します。
# トピック0に関する情報を得ます。
topic_model.get_topic(0)
```

---The following area is a Code cell (cell numver is 21)---
```python
# トピック表現を視覚化します。
# 上位30のトピックについて、各トピックの上位10単語を表示する棒グラフを作成します。
topic_model.visualize_barchart(top_n_topics=30, n_words=10)
```

---The following area is a Code cell (cell numver is 22)---
```python
# トピックの用語ランクを視覚化します。
# トピックにおける単語の重要度のランクを示すプロットを作成します。
topic_model.visualize_term_rank()
```

---The following area is a Code cell (cell numver is 23)---
```python
# トピックのヒートマップを視覚化します。
# 上位20トピックの関係性を示すヒートマップを作成します。
topic_model.visualize_heatmap(top_n_topics=20)
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
# ガイド付きトピックモデリング

以下の図は、ガイド付きトピックモデリングの基本的な考え方を示しています（出典: https://maartengr.github.io/BERTopic/getting_started/guided）。

![image.png](attachment:e350c127-4025-4b0a-9942-3768abb34873.png)
```

---The following area is a Code cell (cell numver is 25)---
```python
# タスクトピックを定義します。
# さまざまな自然言語処理タスクに関連するテーマをカンマで区切ってリスト化します。
task_topics = "Code to text,,text to code,Named entity recognition,Sentiment Analysis,Translation,Question Answering,Program Execution, Miscallenous tasks,Text Categorization,Language Identification, Information Extraction,Text Quality,Summarization,text completion,essay writing,poem writing,creative writing,fact verification,reasoning,mathematical,grammer task,rephrasing,style transfer,paraphrasing,natural language inference,question generation,text matching,dialogue generation,harmfullness detection,toxic language detection,fact verification,keyword tagging".split(",")

# 定義したタスクトピックを表示します。
print(task_topics)
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
**CHATGPTを使用して修正されたタスクキーワード**
```

---The following area is a Code cell (cell numver is 27)---
```python
# シードトピックを定義します。
# 修正されたタスクに関連するキーワードのリストを作成します。
task_topics_modified = [
    ['Code to text', 'source code', 'comments', 'explanation', 'description', 'documentation'],
    ['Text to code', 'programming', 'syntax', 'function', 'script', 'automation'],
    ['Named entity recognition', 'NER', 'entities', 'classification', 'annotation', 'identification'],
    ['Sentiment Analysis', 'emotion', 'opinion', 'polarity', 'attitude', 'mood'],
    ['Translation', 'bilingual', 'language pair', 'conversion', 'interpretation', 'localization'],
    ['Question Answering', 'QA', 'response', 'inquiry', 'knowledge', 'retrieval'],
    ['Program Execution', 'run', 'execute', 'compile', 'script', 'process'],
    ['Miscellaneous tasks', 'varied', 'general', 'diverse', 'assorted', 'multiple'],
    ['Text Categorization', 'classification', 'labeling', 'sorting', 'grouping', 'organization'],
    ['Language Identification', 'detection', 'recognition', 'classification', 'language', 'dialect'],
    ['Information Extraction', 'data mining', 'retrieval', 'extraction', 'parsing', 'harvesting'],
    ['Text Quality', 'clarity', 'readability', 'coherence', 'accuracy', 'precision'],
    ['Summarization', 'abstract', 'condense', 'overview', 'digest', 'outline'],
    ['Text completion', 'autocomplete', 'fill-in', 'predictive', 'continuation', 'suggestion'],
    ['Essay writing', 'composition', 'argument', 'thesis', 'structure', 'drafting'],
    ['Poem writing', 'verse', 'rhyme', 'stanza', 'meter', 'lyric'],
    ['Creative writing', 'story', 'imagination', 'narrative', 'fiction', 'expression'],
    ['Fact verification', 'truth', 'validation', 'accuracy', 'confirmation', 'authenticity'],
    ['Reasoning', 'logic', 'deduction', 'inference', 'rationale', 'analysis'],
    ['Mathematical', 'calculation', 'formula', 'equation', 'computation', 'arithmetic'],
    ['Grammar task', 'syntax', 'rules', 'correction', 'structure', 'editing'],
    ['Rephrasing', 'paraphrase', 'reword', 'rewrite', 'restatement', 'alteration'],
    ['Style transfer', 'transformation', 'conversion', 'adaptation', 'modification', 'recasting'],
    ['Paraphrasing', 'rewording', 'restating', 'rephrasing', 'altering', 'modifying'],
    ['Natural language inference', 'NLI', 'hypothesis', 'entailment', 'contradiction', 'inference'],
    ['Question generation', 'inquiry', 'query', 'interrogative', 'ask', 'question'],
    ['Text matching', 'similarity', 'comparison', 'alignment', 'correlation', 'matching'],
    ['Dialogue generation', 'conversation', 'interaction', 'exchange', 'communication', 'chatbot'],
    ['Harmfulness detection', 'toxicity', 'abuse', 'malice', 'danger', 'risk'],
    ['Toxic language detection', 'abusive', 'offensive', 'harmful', 'inappropriate', 'insulting'],
    ['Fact verification', 'validation', 'authenticity', 'accuracy', 'truth', 'confirmation'],
    ['Keyword tagging', 'labeling', 'annotation', 'classification', 'indexing', 'tagging'],
    ['Topic modeling', 'themes', 'topics', 'clustering', 'segmentation', 'grouping'],
    ['Contextual embedding', 'context', 'representation', 'vectors', 'embeddings', 'contextualization'],
    ['Coreference resolution', 'pronouns', 'anaphora', 'antecedents', 'referents', 'binding'],
    ['Semantic similarity', 'meaning', 'relation', 'comparison', 'equivalence', 'likeness'],
    ['Document summarization', 'overview', 'digest', 'abstract', 'compendium', 'condensation'],
    ['Speech recognition', 'transcription', 'audio', 'voice', 'ASR', 'spoken'],
    ['Optical character recognition', 'OCR', 'text', 'image', 'scanning', 'extraction'],
    ['Text generation', 'creation', 'synthesis', 'generation', 'writing', 'production'],
    ['Dialogue summarization', 'conversation', 'overview', 'recap', 'condensation', 'summary'],
    ['Data anonymization', 'privacy', 'masking', 'obfuscation', 'anonymity', 'de-identification']
]

# 修正されたタスクキーワードの数を表示します。
len(task_topics_modified)
```

---The following area is a Code cell (cell numver is 28)---
```python
# ガイド付きトピックモデリングのパイプラインを構築します。
# 修正されたタスクキーワードを使ってガイド付きトピックモデルを定義します。
topic_model_guided = BERTopic(
    embedding_model=embedding_model,       # 埋め込みモデルを指定
    umap_model=umap_model,                 # 次元削減モデルを指定
    hdbscan_model=hdbscan_model,           # クラスタリングモデルを指定
    vectorizer_model=vectorizer_model,     # トークナイザーを指定
    ctfidf_model=ctfidf_model,             # c-TF-IDFモデルを指定
    representation_model=representation_model, # 表現モデルを指定
    n_gram_range=(1,5),                    # n-gramの範囲を設定（1から5）
    language="english",                    # 言語を英語に設定
    seed_topic_list=task_topics_modified    # ガイド付きのシードトピックを指定
)
```

---The following area is a Code cell (cell numver is 29)---
```python
# ガイド付きトピックモデリングの適合を行います。
# train_prompt_concatenatedに基づいてトピックをフィットさせ、トピックとその確率を得ます。
# %%timeマジックコマンドは、このセルの実行時間を計測します。
%%time
topics, topic_proba = topic_model_guided.fit_transform(train_prompt_concatenated)
```

---The following area is a Code cell (cell numver is 30)---
```python
# ガイド付きトピックモデリングのトピック表現を視覚化します。
# 上位30のトピックについて、各トピックの上位10単語を表示する棒グラフを作成します。
topic_model_guided.visualize_barchart(top_n_topics=30, n_words=10)
```

---The following area is a Code cell (cell numver is 31)---
```python
# ガイド付きトピックモデルを保存します。
# 保存時に"safetensors"形式を使用します。
topic_model_guided.save("topic_model_guided", serialization="safetensors")
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
# リソース

* https://research.google/pubs/large-language-models-are-effective-text-rankers-with-pairwise-ranking-prompting/
* https://magazine.sebastianraschka.com/p/tips-for-llm-pretraining-and-evaluating-rms
* https://magazine.sebastianraschka.com/p/llm-training-rlhf-and-its-alternatives
* https://www.kaggle.com/code/abaojiang/lmsys-detailed-eda
* https://www.kaggle.com/code/robikscube/lmsys-chatbot-arena-data-anaylsis#Response-Length-Baseline
* https://medium.com/data-reply-it-datatech/bertopic-topic-modeling-as-you-have-never-seen-it-before-abb48bbab2b2
```

** @@@ Jupyter Notebook numver 93, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、KerasおよびKerasNLPを使用して、LMSYS - Chatbot Arenaコンペティションにおける人間による好み予測のためのモデルを構築するスターターノートブックです。具体的には、LLM（大規模言語モデル）によって生成されたチャットボットの応答に対して、どの応答がユーザーに好まれるかを予測する問題に取り組んでいます。

## 問題
コンペティションの目的は、ユーザーが提示するプロンプトに対して複数の応答から選んだ場合に、その選ばれる可能性の高い応答を予測することです。これは、多数のLLMからの応答に基づいて行われます。

## 使用している手法
このノートブックでは、**DebertaV3**というモデルをファインチューニングし、選択肢問題の形式で応答を処理しています。具体的には、各プロンプトと応答をペアで使用し、LLMがどの応答を好ましく評価するかを学習します。モデルにはKerasNLPを使用し、混合精度トレーニングを利用することでトレーニング時間を短縮し、GPUのメモリ使用量を減らしています。

## 使用しているライブラリ
- **Keras**: モデルの構築とトレーニングに使用され、ユーザーが指定したバックエンド（TensorFlow、PyTorch、JAX）を選択して使用できます。
- **KerasNLP**: NLPのために設計されたKerasのサブライブラリで、特にDebertaV3のような様々な事前学習モデルの実装を提供しています。
- **TensorFlow**: モデルのトレーニングと評価を行うためのエコシステム。
- **NumPy/Pandas**: データ操作および分析に使用。
- **Matplotlib/Plotly**: データの可視化に使用。

## その他
ノートブックは、データの読み込みから前処理、モデル構築、訓練、予測、提出ファイルの作成までのプロセスが詳細に示されており、各ステップでのコードが含まれています。多段階のアプローチを取り入れており、最良のモデル評価を行うためのチェックポイントも設定されています。また、今後の改善ポイントとして、大規模モデルへの拡大、クロスバリデーション、応答の順序シャッフル、エポック数の増加などが提案されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
<center><img src="https://keras.io/img/logo-small.png" alt="Kerasのロゴ" width="100"><br/>
このスターターノートブックはKerasチームによって提供されています。</center>

# LMSYS - Chatbot Arena 人間による好み予測 [KerasNLP](https://github.com/keras-team/keras-nlp) と [Keras](https://github.com/keras-team/keras) を使用

<div align="center">
    <img src="https://i.ibb.co/wJMF5HL/lmsys.png">
</div>

このコンペティションでは、我々の目的は、LLM（大規模言語モデル）によって動かされるチャットボット同士の対戦において、どのLLMの応答がユーザーに好まれるかを予測することです。言い換えれば、このコンペティションの目標は、審査員の好みを予測し、特定のプロンプト/応答ペアが勝者として選ばれる可能性を決定することです。このノートブックでは、KerasNLPを使用して、このコンペティションのために**DebertaV3**モデルをファインチューニングするプロセスを案内します。この戦略は、選択肢問題（MCQ）モデルがトレーニングされる方法に似ています。さらに、トレーニングと推論を迅速化するために混合精度を使用します。

**ご存知でしたか**: このノートブックはバックエンドに依存しないため、TensorFlow、PyTorch、JAXのいずれのバックエンドにも対応しています。ただし、最良のパフォーマンスを達成するには `JAX` を使用することをお勧めします。KerasNLPとKerasは、好みのバックエンドを選択することを可能にしています。さらなる詳細は[Keras](https://keras.io/keras_3/)で確認してください。

**注意**: KerasNLPについてのより深い理解を得るためには、[KerasNLPガイド](https://keras.io/keras_nlp/)を参照してください。

# 📚 | ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
import os
# Kerasのバックエンドを"jax"に設定します。
# "tensorflow" または "torch" を使用することもできます。
os.environ["KERAS_BACKEND"] = "jax"  # または "tensorflow" または "torch"

# KerasNLPライブラリをインポートします。
import keras_nlp
# Kerasライブラリをインポートします。
import keras
# TensorFlowライブラリをインポートします。
import tensorflow as tf

# 数値計算のためのNumPyライブラリをインポートします。
import numpy as np 
# データ操作のためのPandasライブラリをインポートします。
import pandas as pd
# プログレスバーの表示のためのtqdmライブラリをインポートします。
from tqdm import tqdm
# JSON操作のためのjsonライブラリをインポートします。
import json

# グラフ描画のためのMatplotlibライブラリをインポートします。
import matplotlib.pyplot as plt
# Matplotlibの設定を行うためのmplモジュールをインポートします。
import matplotlib as mpl
# Plotlyでインタラクティブなグラフを作成するためのexpressモジュールをインポートします。
import plotly.express as px
```

---The following area is a Markdown cell (cell numver is 3)---
```markdown
## ライブラリのバージョン
```

---The following area is a Code cell (cell numver is 4)---
```python
# 現在のTensorFlowのバージョンを出力します。
print("TensorFlow:", tf.__version__)
# 現在のKerasのバージョンを出力します。
print("Keras:", keras.__version__)
# 現在のKerasNLPのバージョンを出力します。
print("KerasNLP:", keras_nlp.__version__)
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
# ⚙️ | 設定
```

---The following area is a Code cell (cell numver is 6)---
```python
class CFG:
    # ランダムシードを設定します。
    seed = 42  # ランダムシード
    # 使用する事前トレーニングモデルの名前を指定します。
    preset = "deberta_v3_extra_small_en" # 事前トレーニングモデルの名前
    # 入力シーケンスの長さを設定します。
    sequence_length = 512  # 入力シーケンスの長さ
    # トレーニングエポックの数を設定します。
    epochs = 3 # トレーニングエポック数
    # バッチサイズを指定します。
    batch_size = 16  # バッチサイズ
    # 学習率スケジューラのタイプを設定します。
    scheduler = 'cosine'  # 学習率スケジューラ
    # ラベルとその名前の対応を定義します。
    label2name = {0: 'winner_model_a', 1: 'winner_model_b', 2: 'winner_tie'}
    # 名前からラベルを取得する辞書を作成します。
    name2label = {v:k for k, v in label2name.items()}
    # クラスラベルのリストを作成します。
    class_labels = list(label2name.keys())
    # クラス名のリストを作成します。
    class_names = list(label2name.values())
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# ♻️ | 再現性
ランダムシードの値を設定して、各実行で類似した結果を得られるようにします。
```

---The following area is a Code cell (cell numver is 8)---
```python
# Kerasで使用するランダムシードを設定します。
keras.utils.set_random_seed(CFG.seed)
```

---The following area is a Markdown cell (cell numver is 9)---
```markdown
# 🧮 | 混合精度

このノートブックでは、トレーニングと推論の際にfloat32精度の代わりに混合精度を使用して、GPUメモリの使用量を削減します。これにより、より大きなバッチサイズを使用できるようになり、トレーニングと推論の時間を短縮することができます。
```

---The following area is a Code cell (cell numver is 10)---
```python
# Kerasの全体的なポリシーを混合精度（mixed_float16）に設定します。
keras.mixed_precision.set_global_policy("mixed_float16")
```

---The following area is a Markdown cell (cell numver is 11)---
```markdown
# 📁 | データセットのパス
```

---The following area is a Code cell (cell numver is 12)---
```python
# データセットのベースパスを設定します。
BASE_PATH = '/kaggle/input/lmsys-chatbot-arena'
```

---The following area is a Markdown cell (cell numver is 13)---
```markdown
# 📖 | メタデータ

コンペティションのデータセットは、ChatBot Arenaからのユーザーのインタラクションで構成されています。各インタラクションでは、審査員が2つの異なる大規模言語モデルに対して1つ以上のプロンプトを提示し、どのモデルがより満足のいく応答を提供したかを示します。トレーニングデータには`55,000`行が含まれており、テストセットには約`25,000`行が期待されています。

## ファイル

### `train.csv`
- `id`: 各行の一意の識別子。
- `model_[a/b]`: モデルの識別子。train.csvには存在しますが、test.csvには存在しません。
- `prompt`: 両方のモデルに与えられる入力プロンプト。
- `response_[a/b]`: モデル_[a/b]のプロンプトに対する応答。
- `winner_model_[a/b/tie]`: 審査員の選択（正解ターゲット）を示すバイナリ列。

### `test.csv`
- `id`: 各行の一意の識別子。
- `prompt`: 両方のモデルに与えられる入力プロンプト。
- `response_[a/b]`: モデル_[a/b]のプロンプトに対する応答。

> 注意: 各インタラクションには複数のプロンプトと応答がある場合がありますが、このノートブックでは**各インタラクションにつき1つのプロンプト**のみを使用します。すべてのプロンプトと応答を使用することもできます。また、データフレーム内のプロンプトと応答は文字列形式のリストとして提供されているため、`eval()`を使用してリテラルリストに変換する必要があります。

## トレーニングデータ
```

---The following area is a Code cell (cell numver is 14)---
```python
# トレーニングデータを読み込みます。
df = pd.read_csv(f'{BASE_PATH}/train.csv') 

# サンプルデータ
# df = df.sample(frac=0.10)

# 最初のプロンプトとその関連する応答を取得します。
df["prompt"] = df.prompt.map(lambda x: eval(x)[0])
df["response_a"] = df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
df["response_b"] = df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# ラベルの変換を行います。
df["class_name"] = df[["winner_model_a", "winner_model_b" , "winner_tie"]].idxmax(axis=1)
df["class_label"] = df.class_name.map(CFG.name2label)

# サンプルを表示します。
df.head()
```

---The following area is a Markdown cell (cell numver is 15)---
```markdown
## テストデータ
```

---The following area is a Code cell (cell numver is 16)---
```python
# テストデータを読み込みます。
test_df = pd.read_csv(f'{BASE_PATH}/test.csv')

# 最初のプロンプトと応答を取得します。
test_df["prompt"] = test_df.prompt.map(lambda x: eval(x)[0])
test_df["response_a"] = test_df.response_a.map(lambda x: eval(x.replace("null","''"))[0])
test_df["response_b"] = test_df.response_b.map(lambda x: eval(x.replace("null", "''"))[0])

# サンプルを表示します。
test_df.head()
```

---The following area is a Markdown cell (cell numver is 17)---
```markdown
## プロンプトによる応答の文脈化

我々のアプローチでは、すべての応答に対して単一のプロンプトを使用するのではなく、各応答をプロンプトで文脈化します。つまり、各応答に対して、モデルにそれぞれの応答と組み合わせた同じセットのプロンプトを提供することになります（例: `(P + R_A)`、`(P + R_B)`など）。このアプローチは、NLPにおける選択肢問題のタスクに類似しています。

> 注意: 一部のプロンプトや応答が`utf-8`でエンコードされていない場合、データローダーを作成する際にエラーが発生することがあります。その場合、空の文字列に置き換えます。
```

---The following area is a Code cell (cell numver is 18)---
```python
# プロンプトと選択肢に基づいてオプションを作成する関数を定義します。
def make_pairs(row):
    row["encode_fail"] = False
    try:
        # プロンプトをUTF-8でエンコードし、デコードします。
        prompt = row.prompt.encode("utf-8").decode("utf-8")
    except:
        # エンコードに失敗した場合は空の文字列にします。
        prompt = ""
        row["encode_fail"] = True

    try:
        # モデルAの応答をUTF-8でエンコードし、デコードします。
        response_a = row.response_a.encode("utf-8").decode("utf-8")
    except:
        # エンコードに失敗した場合は空の文字列にします。
        response_a = ""
        row["encode_fail"] = True

    try:
        # モデルBの応答をUTF-8でエンコードし、デコードします。
        response_b = row.response_b.encode("utf-8").decode("utf-8")
    except:
        # エンコードに失敗した場合は空の文字列にします。
        response_b = ""
        row["encode_fail"] = True
        
    # プロンプトと応答を組み合わせたオプションを作成します。
    row['options'] = [f"Prompt: {prompt}\n\nResponse: {response_a}",  # モデルAからの応答
                      f"Prompt: {prompt}\n\nResponse: {response_b}"  # モデルBからの応答
                     ]
    return row
```

---The following area is a Code cell (cell numver is 19)---
```python
# make_pairs関数をdfの各行に適用します。
df = df.apply(make_pairs, axis=1)  
# dfの最初の2行を表示します。
display(df.head(2))  

# make_pairs関数をtest_dfの各行に適用します。
test_df = test_df.apply(make_pairs, axis=1)  
# test_dfの最初の2行を表示します。
display(test_df.head(2))
```

---The following area is a Markdown cell (cell numver is 20)---
```markdown
## エンコーディング失敗統計

エンコーディングの問題があるサンプル数を確認してみましょう。以下のコードから、エンコーディングに失敗したサンプルはわずか$1\%$であり、$99\%$のサンプルには問題がないことがわかります。テストデータでも同様の傾向が期待できます。このため、データのごく一部を空の文字列として扱っても、トレーニングや推論に大きな影響はないでしょう。
```

---The following area is a Code cell (cell numver is 21)---
```python
# エンコーディング失敗の統計をカウントします。
df.encode_fail.value_counts(normalize=False)
```

---The following area is a Markdown cell (cell numver is 22)---
```markdown
# 🎨 | 探索的データ分析 (EDA)

## LLMの分布
```

---The following area is a Code cell (cell numver is 23)---
```python
# モデルAとモデルBのデータを結合します。
model_df = pd.concat([df.model_a, df.model_b])
# 各モデルのカウントを取得します。
counts = model_df.value_counts().reset_index()
counts.columns = ['LLM', 'Count']

# Plotlyを使用してカスタムスタイリングの棒グラフを作成します。
fig = px.bar(counts, x='LLM', y='Count',
             title='LLMの分布',
             color='Count', color_continuous_scale='viridis')

# x軸ラベルを回転させて、読みやすくします。
fig.update_layout(xaxis_tickangle=-45)  

# グラフを表示します。
fig.show()
```

---The following area is a Markdown cell (cell numver is 24)---
```markdown
## 勝利の分布
```

---The following area is a Code cell (cell numver is 25)---
```python
# 勝者のカウントを取得します。
counts = df['class_name'].value_counts().reset_index()
counts.columns = ['Winner', 'Win Count']

# トレーニングデータの勝者分布を示す棒グラフを作成します。
fig = px.bar(counts, x='Winner', y='Win Count',
             title='トレーニングデータの勝者分布',
             labels={'Winner': '勝者', 'Win Count': '勝利の数'},
             color='Winner', color_continuous_scale='viridis')

# x軸とy軸のタイトルを設定します。
fig.update_layout(xaxis_title="勝者", yaxis_title="勝利の数")

# グラフを表示します。
fig.show()
```

---The following area is a Markdown cell (cell numver is 26)---
```markdown
# 🔪 | データ分割

以下のコードスニペットでは、`class_label`列の層別化を使用して、既存のデータをトレーニングデータと検証データに分割します。
```

---The following area is a Code cell (cell numver is 27)---
```python
# パッケージをインポートします。
from sklearn.model_selection import train_test_split  # パッケージをインポート

# データをトレーニングデータと検証データに分割します。
train_df, valid_df = train_test_split(df, test_size=0.2, stratify=df["class_label"])
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# 🍽️ | 前処理

**何をするのか:** 前処理器は入力文字列を受け取り、それを前処理されたテンソルを含む辞書（`token_ids`, `padding_mask`）に変換します。このプロセスはトークン化から始まり、入力文字列がトークンIDのシーケンスに変換されます。

**なぜ重要なのか:** 生のテキストデータは、その高次元性のためにモデル化が難しく、複雑です。テキストをトークンのコンパクトなセットに変換することで、例えば `"The quick brown fox"` を `["the", "qu", "##ick", "br", "##own", "fox"]` に変換することで、データを単純化します。多くのモデルは、特別なトークンや追加のテンソルを使用して入力を理解します。これらのトークンは、入力を分割し、パディングを特定するなどのタスクに役立ちます。すべてのシーケンスをパディングを使って同じ長さにすることで、計算効率が向上し、その後のステップがスムーズになります。

**KerasNLP**で利用可能な前処理およびトークナイザー層にアクセスするには、以下のページを確認してください:
- [前処理](https://keras.io/api/keras_nlp/preprocessing_layers/)
- [トークナイザー](https://keras.io/api/keras_nlp/tokenizers/)
```

---The following area is a Code cell (cell numver is 29)---
```python
# DebertaV3モデルの前処理器を設定します。
preprocessor = keras_nlp.models.DebertaV3Preprocessor.from_preset(
    preset=CFG.preset, # モデルの名前
    sequence_length=CFG.sequence_length, # 最大シーケンス長、短い場合はパディングされる
)
```

---The following area is a Markdown cell (cell numver is 30)---
```markdown
次に、前処理層の出力形状がどのようになるかを確認しましょう。層の出力形状は $(num\_responses, sequence\_length)$ として表されます。
```

---The following area is a Code cell (cell numver is 31)---
```python
# 最初の行のオプションを処理します。
outs = preprocessor(df.options.iloc[0])  

# 各処理された出力の形状を表示します。
for k, v in outs.items():
    print(k, ":", v.shape)
```

---The following area is a Markdown cell (cell numver is 32)---
```markdown
`preprocessing_fn`関数を使用して、`dataset.map(preprocessing_fn)`メソッドを介して各テキストオプションを変換します。
```

---The following area is a Code cell (cell numver is 33)---
```python
# テキストを前処理するための関数を定義します。
def preprocess_fn(text, label=None):
    # テキストを前処理します。
    text = preprocessor(text)  
    # ラベルが利用可能であれば、処理されたテキストとラベルを返します。
    return (text, label) if label is not None else text
```

---The following area is a Markdown cell (cell numver is 34)---
```markdown
# 🍚 | データローダー

以下のコードは、データ処理のために`tf.data.Dataset`を使用して堅牢なデータフローパイプラインを構築します。`tf.data`の主な特徴は、パイプラインの構築を簡素化し、コンポーネントをシーケンスで表現できる点です。

`tf.data`に関する詳細は、この[ドキュメント](https://www.tensorflow.org/guide/data)を参照してください。
```

---The following area is a Code cell (cell numver is 35)---
```python
# テキストとラベルを使用してデータセットを構築する関数を定義します。
def build_dataset(texts, labels=None, batch_size=32,
                  cache=True, shuffle=1024):
    AUTO = tf.data.AUTOTUNE  # AUTOTUNEオプション
    # ラベルが指定されていない場合、slicesをテキストのタプルに設定します。
    slices = (texts,) if labels is None else (texts, keras.utils.to_categorical(labels, num_classes=3))  # スライスを作成
    # スライスからデータセットを作成します。
    ds = tf.data.Dataset.from_tensor_slices(slices)  
    # キャッシュが有効な場合、データセットをキャッシュします。
    ds = ds.cache() if cache else ds  
    # 前処理関数をマッピングします。
    ds = ds.map(preprocess_fn, num_parallel_calls=AUTO)  
    opt = tf.data.Options()  # データセットオプションを作成
    if shuffle: 
        # シャッフルが有効な場合、データセットをシャッフルします。
        ds = ds.shuffle(shuffle, seed=CFG.seed)  
        opt.experimental_deterministic = False
    # データセットオプションを設定します。
    ds = ds.with_options(opt)  
    # データセットをバッチ化します。
    ds = ds.batch(batch_size, drop_remainder=False)  
    # 次のバッチをプリフェッチします。
    ds = ds.prefetch(AUTO)  
    return ds  # 構築されたデータセットを返します
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
## トレーニング/検証データローダーの構築
```

---The following area is a Code cell (cell numver is 37)---
```python
# トレーニングデータのテキストとラベルを抽出します。
train_texts = train_df.options.tolist()  # トレーニングテキストを抽出
train_labels = train_df.class_label.tolist()  # トレーニングラベルを抽出
# トレーニングデータローダーを構築します。
train_ds = build_dataset(train_texts, train_labels,
                         batch_size=CFG.batch_size,
                         shuffle=True)

# 検証データのテキストとラベルを抽出します。
valid_texts = valid_df.options.tolist()  # 検証テキストを抽出
valid_labels = valid_df.class_label.tolist()  # 検証ラベルを抽出
# 検証データローダーを構築します。
valid_ds = build_dataset(valid_texts, valid_labels,
                         batch_size=CFG.batch_size,
                         shuffle=False)
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# ⚓ | 学習率スケジュール

学習率スケジューラーの実装は、転移学習において非常に重要です。学習率は`lr_start`から始まり、さまざまな手法を用いて徐々に`lr_min`まで減少します。手法には以下が含まれます:
- `step`: ステップ状に学習率を下げる方法で、階段のように見えます。
- `cos`: コサインカーブを利用して、学習率を徐々に減少させる方法です。
- `exp`: 学習率を指数的に減少させる方法です。

**重要性:** 適切に構築された学習率スケジュールは、効率的なモデルのトレーニングに不可欠であり、最適な収束を保証し、オーバーシュートや停滞といった問題を避けるのに役立ちます。
```

---The following area is a Code cell (cell numver is 39)---
```python
import math

# 学習率コールバックを取得する関数を定義します。
def get_lr_callback(batch_size=8, mode='cos', epochs=10, plot=False):
    lr_start, lr_max, lr_min = 1.0e-6, 0.6e-6 * batch_size, 1e-6  # 学習率の開始、最大、最小値を設定
    lr_ramp_ep, lr_sus_ep, lr_decay = 2, 0, 0.8  # 学習率ランプのエポック数、持続エポック数、減衰率を設定

    def lrfn(epoch):  # 学習率更新関数
        if epoch < lr_ramp_ep: 
            lr = (lr_max - lr_start) / lr_ramp_ep * epoch + lr_start  # ランプアップ
        elif epoch < lr_ramp_ep + lr_sus_ep: 
            lr = lr_max  # 最大値で維持
        elif mode == 'exp': 
            lr = (lr_max - lr_min) * lr_decay**(epoch - lr_ramp_ep - lr_sus_ep) + lr_min  # 指数減衰
        elif mode == 'step': 
            lr = lr_max * lr_decay**((epoch - lr_ramp_ep - lr_sus_ep) // 2)  # ステップ減衰
        elif mode == 'cos':
            decay_total_epochs, decay_epoch_index = epochs - lr_ramp_ep - lr_sus_ep + 3, epoch - lr_ramp_ep - lr_sus_ep
            phase = math.pi * decay_epoch_index / decay_total_epochs  # コサイン減衰
            lr = (lr_max - lr_min) * 0.5 * (1 + math.cos(phase)) + lr_min
        return lr

    if plot:  # plotがTrueの場合、学習率曲線をプロットします。
        plt.figure(figsize=(10, 5))
        plt.plot(np.arange(epochs), [lrfn(epoch) for epoch in np.arange(epochs)], marker='o')
        plt.xlabel('epoch'); plt.ylabel('lr')
        plt.title('学習率スケジューラ')
        plt.show()

    return keras.callbacks.LearningRateScheduler(lrfn, verbose=False)  # 学習率コールバックを作成する
```

---The following area is a Code cell (cell numver is 40)---
```python
# 学習率コールバックを取得します。プロットを表示します。
lr_cb = get_lr_callback(CFG.batch_size, plot=True)
```

---The following area is a Markdown cell (cell numver is 41)---
```markdown
# 💾 | モデルチェックポイント

以下のコードは、トレーニング中にモデルの最良のチェックポイントを保存するコールバックを作成します。このチェックポイントは、提出時の推論に使用します。
```

---The following area is a Code cell (cell numver is 42)---
```python
# モデルチェックポイントコールバックを取得します。
ckpt_cb = keras.callbacks.ModelCheckpoint(f'best_model.weights.h5',
                                          monitor='val_log_loss',  # 検証ロスを監視
                                          save_best_only=True,  # 最良のみを保存
                                          save_weights_only=True,  # 重みのみを保存
                                          mode='min')  # 最小値で監視
```

---The following area is a Markdown cell (cell numver is 43)---
```markdown
# 📏 | メトリック

このコンペティションのメトリックは**ログロス**です。このメトリックは数学的に次のように表されます。

$$
\text{Log Loss} = -\frac{1}{N} \sum_{i=1}^{N} \left( y_i \log(p_i) + (1 - y_i) \log(1 - p_i) \right)
$$

ここで、$ N $はサンプルの数、$ y_i $は真のラベル、$ p_i $はサンプルが正のクラスに属する予測確率です。

このメトリックは、分類タスクで広く使用されるカテゴリカルクロスエントロピーに似ています。したがって、ロスをゼロから実装する必要はありません。Kerasライブラリにはこのメトリックの実装が既にあるため、単にこのメトリックを使用してモデルの性能を監視します。
```

---The following area is a Code cell (cell numver is 44)---
```python
# ログロスメトリックをKerasのCategoricalCrossentropyとして定義します。
log_loss = keras.metrics.CategoricalCrossentropy(name="log_loss")
```

---The following area is a Markdown cell (cell numver is 45)---
```markdown
# 🤖 | モデリング

`KerasNLP`ライブラリは、`Bert`、`Roberta`、`DebertaV3`などのさまざまなNLPモデルアーキテクチャを提供します。このノートブックでは`DebertaV3`に焦点を当てていますが、他のモデルは[KerasNLPのドキュメント](https://keras.io/api/keras_nlp/models/)で探索できます。より深い理解のためには、[入門ガイド](https://keras.io/guides/keras_nlp/getting_started/)を参照してください。

我々のアプローチでは、`keras_nlp.models.DebertaV3Classifier`を利用して各プロンプトと応答のペアを処理し、出力埋め込みを生成します。その後、これらの埋め込みを連結し、プーリング層と分類器を通してログitsを取得し、最終的な出力のために`softmax`関数を適用します。

複数の応答を扱う際には、ウェイトシェアリング戦略を使用します。つまり、モデルにはプロンプトとともに1つの応答をずつ提供し、`(P + R_A)`, `(P + R_B)`などの形で全ての応答に対して同じモデルの重みを使用します。全ての応答の埋め込みを取得した後、それらを連結し、平均プーリングを適用します。次に、`Linear/Dense`層と`Softmax`関数を分類器として最終結果を取得します。一度に全ての応答を提供すると、テキストの長さが増加し、モデルの扱いが複雑になります。分類器では`winner_model_a`、`winner_model_b`、および`draw`ケースの3つのクラスを使用することに注意してください。

下の図はこのアプローチを示しています：

<div align="center">
    <img src="https://i.postimg.cc/g0gcvy3f/Kaggle-drawio.png">
</div>

コーディングの観点からは、図に示されているように別々のモデルではなく、共有重みを持つ同一のモデルをすべての応答に使用することに注意してください。
```

---The following area is a Code cell (cell numver is 46)---
```python
# 入力層を定義します。
inputs = {
    "token_ids": keras.Input(shape=(2, None), dtype=tf.int32, name="token_ids"),
    "padding_mask": keras.Input(shape=(2, None), dtype=tf.int32, name="padding_mask"),
}
# DebertaV3Classifierバックボーンを作成します。
backbone = keras_nlp.models.DebertaV3Backbone.from_preset(
    CFG.preset,
)

# 最初の応答: (P + R_A) の埋め込みをバックボーンを使用して計算します。
response_a = {k: v[:, 0, :] for k, v in inputs.items()}
embed_a = backbone(response_a)

# 2番目の応答: (P + R_B) の埋め込みを、同じバックボーンを使用して計算します。
response_b = {k: v[:, 1, :] for k, v in inputs.items()}
embed_b = backbone(response_b)

# 最終的な出力を計算します。
embeds = keras.layers.Concatenate(axis=-1)([embed_a, embed_b])
embeds = keras.layers.GlobalAveragePooling1D()(embeds)
outputs = keras.layers.Dense(3, activation="softmax", name="classifier")(embeds)
model = keras.Model(inputs, outputs)

# オプティマイザー、損失、メトリックとともにモデルをコンパイルします。
model.compile(
    optimizer=keras.optimizers.Adam(5e-6),
    loss=keras.losses.CategoricalCrossentropy(label_smoothing=0.02),
    metrics=[
        log_loss,
        keras.metrics.CategoricalAccuracy(name="accuracy"),
    ],
)
```

---The following area is a Markdown cell (cell numver is 47)---
```markdown
### モデルサマリー
```

---The following area is a Code cell (cell numver is 48)---
```python
# モデルの概要を表示します。
model.summary()
```

---The following area is a Markdown cell (cell numver is 49)---
```markdown
### モデルのプロット

以下のモデルグラフでは、**4つ**の入力があるように見えるかもしれませんが、実際には前述の通り**2つ**の入力があります。我々の入力は2つの部分から構成されており、それぞれの応答に対するものです。しかし、各入力には`token_ids`と`padding_mask`があるため、4つの入力があるように見えますが、実際には2つの入力となります。
```

---The following area is a Code cell (cell numver is 50)---
```python
# 現在エラーが発生しています!! [おそらくライブラリや環境の問題ですので、近いうちに修正されることを願っています]

# モデルのプロットを試みますが、エラーが発生する可能性があります。
# keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)
```

---The following area is a Markdown cell (cell numver is 51)---
```markdown
# 🚂 | トレーニング
```

---The following area is a Code cell (cell numver is 52)---
```python
# モデルのトレーニングを開始します。
history = model.fit(
    train_ds,
    epochs=CFG.epochs,
    validation_data=valid_ds,
    callbacks=[lr_cb, ckpt_cb]
)
```

---The following area is a Markdown cell (cell numver is 53)---
```markdown
## 最良モデルの読み込み

トレーニングが完了した後、最良の結果を得るために重みを読み込み、最良のパフォーマンスを得ましょう。
```

---The following area is a Code cell (cell numver is 54)---
```python
# 最良の重みを読み込みます。
model.load_weights('/kaggle/working/best_model.weights.h5')
```

---The following area is a Markdown cell (cell numver is 55)---
```markdown
# 🧪 | 予測
```

---The following area is a Code cell (cell numver is 56)---
```python
# テストデータセットを構築します。
test_texts = test_df.options.tolist()
test_ds = build_dataset(test_texts,
                         batch_size=min(len(test_df), CFG.batch_size),
                         shuffle=False)
```

---The following area is a Code cell (cell numver is 57)---
```python
# トレーニングされたモデルを使用してテストデータに対して予測を行います。
test_preds = model.predict(test_ds, verbose=1)
```

---The following area is a Markdown cell (cell numver is 58)---
```markdown
# 📬 | 提出

以下のコードは、提出ファイルの準備をします。
```

---The following area is a Code cell (cell numver is 59)---
```python
# 提出用のデータフレームを作成します。
sub_df = test_df[["id"]].copy()
# 予測結果を追加します。
sub_df[CFG.class_names] = test_preds.tolist()
# 提出ファイルをCSV形式で保存します。
sub_df.to_csv("submission.csv", index=False)
# 最初の数行を表示します。
sub_df.head()
```

---The following area is a Markdown cell (cell numver is 60)---
```markdown
# 🔭 | 今後の方向性

このノートブックでは、小さなモデルと控えめなトークン長で良いスコアを達成しましたが、改善の余地はまだたくさんあります。以下の方法でさらなる向上を図ることができます：

1. `Deberta-Base`や`Deberta-Small`のような大きなモデル、あるいは`Gemma`のようなLLMを試してみる。
2. 最大トークン長を増やしてデータの損失を減らす。
3. 5フォールドクロスバリデーションとアンサンブルを使用してモデルを堅牢にし、より良いスコアを得る。
4. 応答の順序をシャッフルするなどの拡張を追加して、より堅牢なパフォーマンスを得る。
5. より多くのエポックでトレーニングする。
6. 学習率スケジューラーを調整する。

# 📌 | 参考文献

* [LLM Science Exam: KerasCore + KerasNLP [TPU]](https://www.kaggle.com/code/awsaf49/llm-science-exam-kerascore-kerasnlp-tpu)
* [AES 2.0: KerasNLP Starter](https://www.kaggle.com/code/awsaf49/aes-2-0-kerasnlp-starter)
```

** @@@ Jupyter Notebook numver 94, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
### Jupyter Notebook 要約

このJupyter Notebookは、「LMSYS - Chatbot Arena」コンペティションにおいて、大規模言語モデル（LLM）であるLlama 3を使用し、ユーザーの好みを予測する問題に取り組んでいます。具体的には、異なるモデルの応答の優劣を評価し、その結果に基づいて予測を行うプロセスを含むノートブックです。

#### 主要な問題:
- ユーザーから提供されたプロンプトに対する2つのチャットボット（モデルAとモデルB）の応答を比較し、どちらが好まれるかを予測すること。

#### 使用した手法:
1. **ライブラリとフレームワーク**:
   - `transformers`: Llamaモデルのトークナイジングやシーケンス分類のために使用。
   - `peft`: 軽量なファインチューニングを可能にするために使用され、LoRa設定を通じてモデルの重みを管理。
   - `torch`: PyTorchを用いて深層学習モデルを構築、GPUでの計算を行う。

2. **データの処理**:
   - Pandasを用いてデータを読み込み、トークナイズすることで、モデルが処理しやすい形式に変換。
   - 特徴量を作成するために、プロンプトや応答の文字列情報を結合して特長抽出を実施。

3. **モデルの構築と推論**:
   - Llamaモデルを2つの異なるGPU（デバイス）に読み込み、それぞれのモデルで予測を行う。
   - データの半分を異なるスレッドで処理することで、推論の効率を向上。

4. **LightGBMとのハイブリッドアプローチ**:
   - LightGBMを使って別途予測を実施し、最終的な予測はLlamaによる予測とLightGBMによる予測の加重平均で得られる。

5. **出力**:
   - 最終的に得られた予測結果をCSVファイル（`submission.csv`）として保存。

このNotebookは、高度な機械学習技術を活用し、ユーザーの好みをより正確に予測するための基盤を提供することを目的としています。特に、異なるモデルの応答を効果的に比較する手法が強調されています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
## llama3-8b

作成者への感謝:
https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b
```

---The following area is a Code cell (cell numver is 2)---
```python
!pip install -q -U bitsandbytes --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U transformers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U tokenizers --no-index --find-links ../input/llm-detect-pip/
!pip install -q -U peft --no-index --find-links ../input/llm-detect-pip/
```

---The following area is a Code cell (cell numver is 3)---
```python
import torch
import sklearn
import numpy as np
import pandas as pd
import time

from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification, BitsAndBytesConfig
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType
from torch.cuda.amp import autocast
from threading import Thread

torch.backends.cuda.enable_mem_efficient_sdp(False)  # メモリ効率の良いSDPを有効にする
torch.backends.cuda.enable_flash_sdp(False)  # フラッシュSDPを無効にする

if (not torch.cuda.is_available()): print("申し訳ありませんが、GPUが必要です！")  # GPUが利用できない場合のメッセージ

MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # モデルのパス
WEIGHTS_PATH = '/kaggle/input/lmsys-model/model'  # 重みのパス
MAX_LENGTH = 1024  # トークンの最大長
BATCH_SIZE = 8  # バッチサイズ
DEVICE = torch.device("cuda")  # デバイスをCUDAに設定

# # データの準備

test = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込む
sample_sub = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/sample_submission.csv')  # サンプル提出データを読み込む

# リスト内の文字列を連結する関数
def process(input_str):
    stripped_str = input_str.strip('[]')  # 角括弧を削除
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 文章を分割してクォーテーションを削除
    return  ' '.join(sentences)  # 文章をスペースで連結して返す

test.loc[:, 'prompt'] = test['prompt'].apply(process)  # プロンプトを処理
test.loc[:, 'response_a'] = test['response_a'].apply(process)  # モデルAの応答を処理
test.loc[:, 'response_b'] = test['response_b'].apply(process)  # モデルBの応答を処理

display(sample_sub)  # サンプル提出データを表示
display(test.head(5))  # テストデータの最初の5行を表示

# モデル用のテキストを準備
test['text'] = 'ユーザーのプロンプト: ' + test['prompt'] +  '\n\nモデルA :\n' + test['response_a'] +'\n\n--------\n\nモデルB:\n'  + test['response_b']
print(test['text'][0])  # 最初のテキストを表示

# # トークナイズ

tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/lmsys-model/tokenizer')  # トークナイザーの読み込み

tokens = tokenizer(test['text'].tolist(), padding='max_length',
                   max_length=MAX_LENGTH, truncation=True, return_tensors='pt')  # テキストをトークナイズ

INPUT_IDS = tokens['input_ids'].to(DEVICE, dtype=torch.int32)  # 入力IDをデバイスに転送
ATTENTION_MASKS = tokens['attention_mask'].to(DEVICE, dtype=torch.int32)  # アテンションマスクをデバイスに転送

# テンソルをCPUに移動し、リストに変換
input_ids_cpu = [tensor.cpu().tolist() for tensor in INPUT_IDS]
attention_masks_cpu = [tensor.cpu().tolist() for tensor in ATTENTION_MASKS]

data = pd.DataFrame()  # 新しいデータフレームを作成
data['INPUT_IDS'] = input_ids_cpu  # 入力IDを格納
data['ATTENTION_MASKS'] = attention_masks_cpu  # アテンションマスクを格納
data[:2]  # 最初の2行を表示

# # モデルをロード
# 各GPUに1つのモデルを読み込む

# BitsAndBytesの設定
bnb_config =  BitsAndBytesConfig(
    load_in_8bit=True,  # 8ビットで読み込む
    bnb_8bit_compute_dtype=torch.float16,  # 8ビット計算のデータ型を指定
    bnb_8bit_use_double_quant=False)  # 二重量子化を無効にする

# GPU 0にベースモデルを読み込む
device0 = torch.device('cuda:0')

base_model_0 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,  # ラベルの数
    torch_dtype=torch.float16,  # モデルのデータ型をfloat16に設定
    quantization_config=bnb_config,
    device_map='cuda:0')  # モデルをGPU 0にマップ
base_model_0.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定

# GPU 1にベースモデルを読み込む
device1 = torch.device('cuda:1')
base_model_1 = LlamaForSequenceClassification.from_pretrained(
    MODEL_NAME,
    num_labels=3,
    torch_dtype=torch.float16,
    quantization_config=bnb_config,
    device_map='cuda:1')  # モデルをGPU 1にマップ
base_model_1.config.pad_token_id = tokenizer.pad_token_id  # パディングトークンIDを設定

# これで、各GPUにモデルが正常に読み込まれました！

# # 重みを読み込む

# LoRa設定
peft_config = LoraConfig(
    r=16,  # LoRaのパラメータ
    lora_alpha=32,  # LoRaのアルファパラメータ
    lora_dropout=0.10,  # LoRaのドロップアウト率
    bias='none',  # バイアスの設定
    inference_mode=True,  # 推論モードを有効にする
    task_type=TaskType.SEQ_CLS,  # タスクの種類
    target_modules=['o_proj', 'v_proj'])  # 対象モジュール

# PEFTを取得
model_0 = get_peft_model(base_model_0, peft_config).to(device0)  # モデルをGPUに転送
# 重みを読み込む
model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み
model_0.eval()  # 評価モードに設定

model_1 = get_peft_model(base_model_1, peft_config).to(device1)  # モデルをGPUに転送
model_1.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)  # 重みの読み込み
model_1.eval()  # 評価モードに設定

# 学習可能なパラメータの表示
model_0.print_trainable_parameters(), model_1.print_trainable_parameters()

# # 推論
# 

import gc
gc.collect()  # ガーベジコレクションを実行

def inference(df, model, device, batch_size=BATCH_SIZE):
    input_ids = torch.tensor(df['INPUT_IDS'].values.tolist(), dtype=torch.long)  # 入力IDをテンソルに変換
    attention_mask = torch.tensor(df['ATTENTION_MASKS'].values.tolist(), dtype=torch.long)  # アテンションマスクをテンソルに変換
    
    generated_class_a = []  # モデルAの結果を格納するリスト
    generated_class_b = []  # モデルBの結果を格納するリスト
    generated_class_c = []  # 同点を格納するリスト

    model.eval()  # 評価モードに設定
    
    for start_idx in range(0, len(df), batch_size):  # バッチごとに繰り返す
        end_idx = min(start_idx + batch_size, len(df))  # バッチの終わりのインデックスを計算
        batch_input_ids = input_ids[start_idx:end_idx].to(device)  # バッチの入力IDをデバイスに転送
        batch_attention_mask = attention_mask[start_idx:end_idx].to(device)  # バッチのアテンションマスクをデバイスに転送
        
        with torch.no_grad():  # 勾配計算を無効にする
            with autocast():  # 自動混合精度を使用
                outputs = model(
                    input_ids=batch_input_ids,
                    attention_mask=batch_attention_mask
                )
        
        probabilities = torch.softmax(outputs.logits, dim=-1).cpu().numpy()  # 出力の確率を計算
        
        generated_class_a.extend(probabilities[:, 0])  # モデルAの結果を追加
        generated_class_b.extend(probabilities[:, 1])  # モデルBの結果を追加
        generated_class_c.extend(probabilities[:, 2])  # 同点の結果を追加
    
    df['winner_model_a'] = generated_class_a  # データフレームにモデルAの結果を保存
    df['winner_model_b'] = generated_class_b  # データフレームにモデルBの結果を保存
    df['winner_tie'] = generated_class_c  # データフレームに同点の結果を保存

    torch.cuda.empty_cache()  # CUDAのメモリを空にする

    return df  # 処理したデータフレームを返す

st = time.time()  # 処理開始時間を記録

N_SAMPLES = len(data)  # データのサンプル数

# データを二つのサブセットに分割
half = round(N_SAMPLES / 2)  # サンプルの半分のサイズを計算
sub1 = data.iloc[0:half].copy()  # 最初のサブセット
sub2 = data.iloc[half:N_SAMPLES].copy()  # 2つ目のサブセット

# スレッドで推論を実行する関数
def run_inference(df, model, device, results, index):
    results[index] = inference(df, model, device)  # 結果を保存

# スレッドからの結果を保存するための辞書
results = {}

# スレッドを開始
t0 = Thread(target=run_inference, args=(sub1, model_0, device0, results, 0))  # モデルAを使用するスレッド
t1 = Thread(target=run_inference, args=(sub2, model_1, device1, results, 1))  # モデルBを使用するスレッド

t0.start()  # スレッドの開始
t1.start()  # スレッドの開始

# すべてのスレッドが終了するのを待つ
t0.join()
t1.join()

# 結果を元のデータフレームに統合
data = pd.concat([results[0], results[1]], axis=0)

print(f"処理が完了しました。合計時間: {time.time() - st}")  # 処理時間を表示

# 推論が約4.5時間で完了しました。改善の余地があるため、異なる後処理を試して共有することをお勧めします。Kaggleのやり方 :)

TARGETS = ['winner_model_a', 'winner_model_b', 'winner_tie']  # 予測のターゲット列

sample_sub[TARGETS] = data[TARGETS]  # サンプル提出に結果を追加
```

---The following area is a Code cell (cell numver is 4)---
```python
llama_preds = data[TARGETS].values  # ターゲットの値を取得
```

---The following area is a Markdown cell (cell numver is 5)---
```markdown
## LGBM + tfidf
```

---The following area is a Code cell (cell numver is 6)---
```python
TAG = 'lmsys-chatbot-arena'  # コンペティションのタグ

import os
RUNPOD = os.path.exists('/workspace/')  # 実行環境がWORKSPACEかどうか確認
KAGGLE = not RUNPOD  # Kaggleでの実行かどうか確認
if KAGGLE: print('kaggle')  # Kaggleでの実行メッセージ
```

---The following area is a Code cell (cell numver is 7)---
```python
try:
    import pandas as pd  # pandasをインポート
except:
    !pip install -q kaggle  # kaggleがインストールされていない場合インストール
    !pip install -q pandas matplotlib scipy joblib scikit-learn lightgbm  # 必要なパッケージをインストール
    !pip install -q protobuf  # protobufをインストール
    !pip install -q numba  # numbaをインストール
```

---The following area is a Code cell (cell numver is 8)---
```python
DATA = '/data/' if RUNPOD else 'data/' \
        if not os.path.exists('/kaggle/') \
            else '/kaggle/input/{}/'.format(TAG)  # データのパスを設定

import os

if RUNPOD:
    if not os.path.exists('~/.kaggle/kaggle.json'):  # kaggle.jsonが存在しない場合
        !mkdir -p ~/.kaggle  # .kaggleディレクトリを作成
        !cp /workspace/kaggle.json ~/.kaggle/kaggle.json  # kaggle.jsonをコピー
        !chmod 600 /root/.kaggle/kaggle.json  # permissionsを変更

    if not os.path.exists('/workspace/' + TAG + '.zip'):  # ZIPファイルが存在しない場合
        !kaggle competitions download $TAG -p /workspace/  # コンペティションデータをダウンロード
        
    if not os.path.exists('/data/'):  # /data/ディレクトリが存在しない場合
        import zipfile
        zipfile.ZipFile('/workspace/' + TAG + '.zip').extractall('/data/')  # ZIPファイルを解凍
```

---The following area is a Code cell (cell numver is 9)---
```python
INPUT_PATH = '/kaggle/input/'  # 入力パス
MODEL_PATH = '/workspace/models/'; LOGITS_PATH = '/workspace/logits/'  # モデルとロジットのパス
MODEL_PATH = MODEL_PATH if not KAGGLE else '/kaggle/input/' \
                + [e for e in os.listdir('/kaggle/input') if 'lsys-models' in e][0] + '/'  # モデルパスを設定
# MODEL_PATH = MODEL_PATH if not KAGGLE else ''#MODEL_PATH + os.listdir(MODEL_PATH)[0] + '/'
print(MODEL_PATH)  # モデルパスを表示

CODE_PATH = MODEL_PATH if KAGGLE else '/workspace/'  # コードパスを設定
SAVE_PATH = MODEL_PATH if not KAGGLE else ''  # 保存パスを設定
```

---The following area is a Code cell (cell numver is 10)---
```python
import os
import io
import gc
import time
import json
import random
import pickle
import zipfile
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from IPython.display import display
from collections import Counter
from collections import defaultdict
import torch
from torch import nn
import torch.nn.functional as F
import pytorch_lightning as pl
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import log_loss
import tokenizers

os.environ['TOKENIZERS_PARALLELISM'] = 'false'  # TOKENIZERSの並列処理を無効にする
```

---The following area is a Code cell (cell numver is 11)---
```python
train = pd.read_csv(open(DATA + 'train.csv', 'r'))  # 訓練データを読み込む
test = pd.read_csv(open(DATA + 'test.csv', 'r'))  # テストデータを読み込む
sample = pd.read_csv(DATA + 'sample_submission.csv')  # サンプル提出データを読み込む

print(len(train), len(test))  # 訓練データとテストデータの長さを表示
```

---The following area is a Code cell (cell numver is 12)---
```python
params = {}  # パラメータの初期化
if False:  # もしテストデータの長さが10未満の場合
    pass;
    params['subsample'] = 30  # サブサンプルの数を設定
else:
    # params['subsample'] = 2
    params['fold'] = -1  # フォールドを設定


params['n_epochs'] = 1  # 学習のエポック数
params['n_lgb'] = 1  # LightGBMの数
params['model'] = 'microsoft/deberta-v3-small'  # 使用するモデル
```

---The following area is a Code cell (cell numver is 13)---
```python
# params = {}
FULL = params.get('fold', 0) < 0  # 完全に学習するかどうか
N_FOLDS = int(params.get('n_folds', 3));  # フォールド数を設定
FOLD = int(params.get('fold', 0))  # 現在のフォールドを設定
SEED = int(params.get('seed', 3))  # 乱数シードを設定
SS = int(params.get('subsample', 1))  # サブサンプルの数を設定

print(N_FOLDS, FOLD, SEED, SS)  # フォールド数、現在のフォールド、シード、サブサンプル数を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
from sklearn.model_selection import StratifiedKFold  # StratifiedKFoldをインポート

def get_folds(train): 
    return list(StratifiedKFold(N_FOLDS, random_state = SEED, shuffle = True)\
                    .split(X = np.zeros(len(train)), y = train.iloc[:, -3:].idxmax(1)))  # フォールドを取得

train_ids, test_ids = get_folds(train)[FOLD] if not FULL else [list(range(len(train))), []]  # IDを取得
if SS > 1: train_ids, test_ids = train_ids[::SS], test_ids[::SS]  # サブサンプルを取得

print(len(train_ids), len(test_ids));  assert set(train_ids) & set(test_ids) == set()  # 確認
```

---The following area is a Code cell (cell numver is 15)---
```python
def join_strings(x, ):
    x = ' '.join(['' if e is None else e for e in x]) if isinstance(x, list) else x  # リスト内の要素を結合
    return x  # 結合した文字列を返す
```

---The following area is a Code cell (cell numver is 16)---
```python
def len_join_strings(x, ):
    return len(join_strings(x).split())  # 結合した文字列の長さを返す
```

---The following area is a Code cell (cell numver is 17)---
```python
def len_join_strings_j(x):
    x = json.loads(x)  # JSON文字列をロード
    return len_join_strings(x)  # 結合した文字列の長さを返す
```

---The following area is a Code cell (cell numver is 18)---
```python
torch.manual_seed(datetime.datetime.now().microsecond)  # PyTorchの乱数シードを現在のマイクロ秒で設定
random.seed(datetime.datetime.now().microsecond)  # Pythonのrandomモジュールの乱数シードを設定
np.random.seed(datetime.datetime.now().microsecond)  # NumPyの乱数シードを設定
```

---The following area is a Code cell (cell numver is 19)---
```python
# TRAIN = True and not KAGGLE
TRAIN = False  # 訓練フラグを設定
INFER = True  # 推論フラグを設定（KAGGLEの場合も推論）
SAVE = False  # 保存フラグを設定
```

---The following area is a Code cell (cell numver is 20)---
```python
import lightgbm as lgb  # LightGBMをインポート
from sklearn.feature_extraction.text import CountVectorizer  # CountVectorizerをインポート
```

---The following area is a Code cell (cell numver is 21)---
```python
LGB = True  # LightGBMフラグを設定
TRAIN_LGB = TRAIN and LGB and params.get('n_lgb', 1) > 0  # LightGBMの訓練フラグを設定
INFER_LGB = not TRAIN and LGB  # LightGBMの推論フラグを設定
```

---The following area is a Code cell (cell numver is 22)---
```python
cvec  = pickle.load(open(MODEL_PATH + 'cvec.pkl', 'rb'))  # CountVectorizerを読み込み
ccvec = pickle.load(open(MODEL_PATH + 'ccvec.pkl', 'rb'))  # カスタムCountVectorizerを読み込み
```

---The following area is a Code cell (cell numver is 23)---
```python
def symlog(x): return (np.sign(x) * np.log1p(np.abs(x))).astype(np.float32)  # 対称対数変換の関数

def dense(x):
    x = np.asarray(x.astype(np.float32).todense())  # 疎行列を密行列に変換
    x = symlog(x)  # 対称対数変換を適用
    return x  # 変換した値を返す

def get_features(df):
    # プロンプトに対する特徴を抽出
    pfeat = np.hstack([dense(v.transform(df[c])) 
                for v in [cvec, ccvec]
                    for c in ['prompt', ]])
    
    # モデルAに対する特徴を抽出
    afeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_a', ]
                    for v in [cvec, ccvec]
                ])
    
    # モデルBに対する特徴を抽出
    bfeat = np.hstack([dense(v.transform(df[c])) 
                for c in ['response_b', ]
                    for v in [cvec, ccvec]
                ])
    
    # 特徴の組み合わせ
    v = np.hstack([
          afeat - bfeat, np.abs(afeat - bfeat),  # モデルAとBの差分
        ])
    try: 
        v = v / (len(all_vote_models) if len(df) < len(train) else 1)  # モデル数で割る（条件付き）
    except: pass

    extras = []  # 追加の特徴を格納するリスト
    EXTRAS = ['\n', '\n\n', '.', ' ', '","']  # カウントする文字のリスト
    for e in EXTRAS:
        for c in ['prompt', 'response_a', 'response_b']:
            extras.append(df[c].str.count(e).values)  # 特徴量としてカウントを追加
            
    extras.append(df[c].str.len())  # 文字列の長さを追加
    extras.append(df[c].str.split().apply(lambda x: len(x)))  # 単語数を追加
    
    extras = np.stack(extras, axis = 1)  # 配列に変換
    extras = np.hstack([extras ** 0.5, np.log1p(extras)])  # 追加特徴の変換
    return np.hstack([v, extras])  # 特徴を結合して返す
    # return v
```

---The following area is a Code cell (cell numver is 24)---
```python
lgb_models = pickle.load(open(MODEL_PATH + 'lgb_models.pkl', 'rb'))  # LightGBMモデルを読み込み
```

---The following area is a Code cell (cell numver is 25)---
```python
if INFER and params.get('n_lgb', 1) > 0:  # 推論が有効で、LightGBMのモデルが存在する場合
    df = test  # テストデータを指定
    yps = []  # 予測を保存するリスト
    b = 1000  # バッチサイズ
    for i in range(0, len(df), b):  # データをバッチごとに処理
        arr = get_features(df.iloc[i: i + b])  # 特徴を取得
        ypms = []  # 各モデルの予測を保存するリスト
        for model in lgb_models:  # すべてのモデルに対して予測を実行
            ypms.append(model.predict_proba(arr))  # モデルからの予測を追加
        yps.append(np.stack(ypms).mean(0))  # モデルの平均を計算してリストに追加
        print('.', end = '')  # 進行状況を表示
        
        if len(yps) % 2 == 0:  # 2つのバッチごとにガーベジコレクションを実行
            gc.collect()
    print()  # 改行

    yp = np.concatenate(yps)  # すべての予測を結合
```

---The following area is a Code cell (cell numver is 26)---
```python
lgb_preds = yp  # LightGBMの予測を保存
```

---The following area is a Markdown cell (cell numver is 27)---
```markdown
## 予測のブレンド

$\operatorname{preds} = 0.2 \cdot \operatorname{lgbm \ boosting \ preds} + 0.8 \cdot \operatorname{llama \ preds}$
```

---The following area is a Code cell (cell numver is 28)---
```python
lgb_wt = 0.2  # LightGBMの重みを設定
preds = lgb_wt * lgb_preds + (1 - lgb_wt) * llama_preds  # 予測をブレンド
```

---The following area is a Code cell (cell numver is 29)---
```python
out = pd.DataFrame(preds, 
                index = df.id,  # インデックスに元のIDを設定
                    columns = train.columns[-3:])  # 予測列の列名を設定
display(out.head())  # 結果を表示
```

---The following area is a Code cell (cell numver is 30)---
```python
out.to_csv('submission.csv')  # 結果をCSVファイルに保存
```

** @@@ Jupyter Notebook numver 95, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookは、Llama-3 8bモデルをTPU(Super TPU)でトレーニングすることに焦点を当てています。特に、大規模言語モデル(LLM)のトレーニング過程を詳細に説明し、実際にトレーニングを行うための実装方法を示しています。

### 取り組んでいる問題
ノートブックは、Llama-3のLPF（Low-Rank Adaptation、LoRA）と呼ばれる技術を使用して、TPU上でのモデルのトレーニングを効率化する問題に取り組んでいます。具体的には、チャットボットの応答に対するユーザーの好みを予測するためのモデルをトレーニングし、ユーザインタラクションの改善を目指しています。

### 使用している手法とライブラリ
- **ライブラリ**：
  - `transformers`: Hugging Faceのライブラリを用いて、LLMのモデルやトークナイザーを扱っています。
  - `torch`: PyTorchを使用して、モデルのトレーニングとテストを行います。
  - `torch_xla`: TPUでの計算をサポートするために、XLA（Accelerated Linear Algebra）関連の機能を利用しています。
  - `peft`: 特にLoRAに関連するパラメータの効率的な調整を行います。

- **手法**:
  - **LoRA (Low-Rank Adaptation)**: 低ランク行列の近似を使用して、モデルのトレーニングを効率的に行い、パラメータの数を削減します。
  - **クロスエントロピー損失関数**: モデルの損失を評価するために使用されます。
  - **コサイン学習率スケジュール**: 学習率を調整しつつトレーニングを進めるために用いられます。
  
ノートブックは、トレーニングデータを読み込み、データ前処理を行い、トークナイザーを使ってデータをトークナイズします。その後、モデルを定義し、TPU環境におけるデータのシャーディングとパラメータの設定を行います。最終的に、トレーニングループ内でモデルを訓練し、損失の履歴をプロットして視覚的に訓練状況を確認します。

### 結論
全体として、このノートブックはLlama-3モデルをTPUでトレーニングするための具体的な手順と実装を示し、さらなるデータやハイパーパラメータの調整によるトレーニングの最適化の可能性についても言及しています。
```

---The following area is a Markdown cell (cell numver is 1)---
```markdown
# Llama-3 8b [TPUトレーニング]

TPU上でLLMをトレーニングする方法を学ぶことができます。これがあなたにも役立つことを願っています！

ノートブックは以下からインスパイアを受けています：

* [LLM検出AIコンペ Mistral-7B](https://www.kaggle.com/code/hotchpotch/train-llm-detect-ai-comp-mistral-7b/notebook)
* [DAIGT Mistral-7B TPU BFloat16 [トレーニング]](https://www.kaggle.com/code/markwijkhuizen/daigt-mistral-7b-tpu-bfloat16-train)
* [LLAMA 2 13B on TPU (トレーニング)](https://www.kaggle.com/code/defdet/llama-2-13b-on-tpu-training)

前提条件：Llama-3の使用権限

注意：これはトレーニング専用のノートブックです、推論ノートブックは[こちら](https://www.kaggle.com/code/kishanvavdara/inference-llama-3-8b)から見つけることができます。

学びや役立つことがあれば、投票していただけると嬉しいです！

# ライブラリのインポート
```

---The following area is a Code cell (cell numver is 2)---
```python
# ライブラリをインストール
!pip install -qq peft==0.6.0  # PEFTライブラリのインストール
!pip install -qq bitsandbytes==0.41.1  # BitsAndBytesライブラリのインストール
!pip install -qq accelerate==0.24.1  # Accelerateライブラリのインストール
!pip install -qq transformers==4.35.0  # Transformersライブラリのインストール
!pip install -qq torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q  # CPU用のPyTorchのインストール
!pip install -qq torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q  # TPU用のTorch XLAのインストール
!pip uninstall -qq tensorflow -y # テンソルフローをアンインストールします。これを行わないと、TFがTPUを占有し、PyTorchに対する権限エラーが発生する可能性があります。
!cp /kaggle/input/utils-xla/spmd_util.py . # このリポジトリからファイルをコピーします: https://github.com/HeegyuKim/torch-xla-SPMD
```

---The following area is a Code cell (cell numver is 3)---
```python
import os  # osモジュールをインポート（オペレーティングシステムに関連する機能を提供）
import gc  # gcモジュールをインポート（ガーベジコレクション、不要なメモリの解放を行う）
import re  # reモジュールをインポート（正規表現を扱うための機能を提供）
from time import time  # timeモジュールからtime関数をインポート（時間計測を行うため）
import random  # randomモジュールをインポート（ランダムな数の生成を行う）
import warnings  # warningsモジュールをインポート（警告メッセージを表示するため）
import numpy as np  # NumPyライブラリをnpというエイリアスでインポート（数値計算を行うため）
import pandas as pd  # pandasライブラリをpdというエイリアスでインポート（データ操作と分析のためのライブラリ）
import matplotlib.pyplot as plt  # Matplotlibライブラリをpltというエイリアスでインポート（データの可視化を行うため）
from tqdm.auto import tqdm  # tqdmライブラリから自動的にプログレスバーをインポート

import torch  # PyTorchライブラリをインポート（機械学習のためのライブラリ）
import transformers  # Hugging FaceのTransformersライブラリをインポート（NLPモデルを使用するため）
from sklearn.metrics import accuracy_score  # scikit-learnから精度計算のための関数をインポート
from transformers import AutoTokenizer, LlamaModel, LlamaForSequenceClassification  # 自動トークナイザーとモデルをインポート
from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType  # PEFT関連のモジュールをインポート
import torch.nn.functional as F  # PyTorchのニューラルネットワークの関数をFというエイリアスでインポート

import torch_xla.debug.profiler as xp  # XLAのプロファイラをインポート（パフォーマンスの分析に使用）
import torch_xla.core.xla_model as xm  # XLAモジュールをインポート（TPUのための処理をサポート）
import torch_xla.experimental.xla_sharding as xs  # XLAのシャーディング機能をインポート（データ分散のため）
import torch_xla.runtime as xr  # XLAの実行時機能をインポート

xr.use_spmd()  # SPMD（Single Program Multiple Data）を使用する設定

from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor  # XLAシャーディングテンソルをインポート（シャーディングされたテンソルを扱う）
from torch_xla.experimental.xla_sharding import Mesh  # メッシュ構造をインポート（データ分散のため）
from spmd_util import partition_module  # モジュールをパーティションするための関数をインポート

tqdm.pandas()  # pandasの進捗バーを設定

print(f'Torch Version: {torch.__version__}')  # 現在のPyTorchのバージョンを出力
```

---The following area is a Markdown cell (cell numver is 4)---
```markdown
# 設定
```

---The following area is a Code cell (cell numver is 5)---
```python
class CFG:  # 設定を管理するクラス
    NUM_EPOCHS = 1  # エポック数（モデルをトレーニングする回数）
    BATCH_SIZE = 16  # バッチサイズ（一度に処理するデータの数）
    DROPOUT = 0.05  # ドロップアウト率（過学習を防ぐための手法）
    MODEL_NAME = '/kaggle/input/llama-3/transformers/8b-chat-hf/1'  # 使用するモデルのパス
    SEED = 2024  # 再現性を確保するための乱数の種
    MAX_LENGTH = 1024  # 最大シーケンス長（モデルに入力する最大の長さ）
    NUM_WARMUP_STEPS = 128  # ウォームアップステップの数（学習率を調整するための初期ステップ）
    LR_MAX = 5e-5  # 最大学習率（モデルをトレーニングする際の最大の学習率）
    NUM_LABELS = 3  # ラベルの数（分類するクラスの数）
    LORA_RANK = 4  # LoRAのランク（低ランク近似のためのパラメーター）
    LORA_ALPHA = 8  # LoRAのアルファ値（スケーリングファクター）
    LORA_MODULES = ['o_proj', 'v_proj']  # LoRAを適用するモジュールのリスト
    
DEVICE = xm.xla_device()  # TPUデバイスを初期化する
```

---The following area is a Code cell (cell numver is 6)---
```python
def set_seeds(seed):  # 再現性を確保するための乱数の種を設定する関数
    """再現性を確保するために種を設定します"""
    os.environ['PYTHONHASHSEED'] = str(seed)  # Pythonのハッシュシードを設定
    random.seed(seed)  # randomモジュールの種を設定
    np.random.seed(seed)  # NumPyの乱数生成器の種を設定
    torch.manual_seed(seed)  # PyTorchの手動シードを設定
    if torch.cuda.is_available():  # CUDAが利用可能な場合
        torch.cuda.manual_seed(seed)  # GPUのシードを設定
        torch.cuda.manual_seed_all(seed)  # 全てのGPUのシードを設定
        
    # すべてのTPUコアに対してシードを設定
    xm.set_rng_state(seed, device=xm.xla_device())  

set_seeds(seed=CFG.SEED)  # 定義した関数を呼び出し、設定したシードを使用して乱数の種を設定
```

---The following area is a Markdown cell (cell numver is 7)---
```markdown
# トークナイザー
```

---The following area is a Code cell (cell numver is 8)---
```python
tokenizer = AutoTokenizer.from_pretrained(CFG.MODEL_NAME)  # 設定したモデル名から事前トレーニングされたトークナイザーをロード
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークン（eos_token）に設定
tokenizer.padding_side = 'right'  # パディングを右側に追加
tokenizer.add_eos_token = True  # 終了トークンを追加する設定

# 推論時にオフラインでロードできるようにトークナイザーを保存
tokenizer.save_pretrained('tokenizer')  # トークナイザーを指定したディレクトリに保存
```

---The following area is a Code cell (cell numver is 9)---
```python
# テキストのトークン長を取得するユーティリティ関数
def get_token_lengths(texts):  # テキストのリストを引数に取る関数
    # テキストをトークナイズし、各テキストに対するinput_idsを取得
    input_ids = tokenizer(texts.tolist(), return_tensors='np')['input_ids']  
    # 各テキストのinput_idsの長さを返す
    return [len(t) for t in input_ids]  # 各トークンの長さをリストとして返す
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
# トレーニングの準備
```

---The following area is a Code cell (cell numver is 11)---
```python
train = pd.read_csv('/kaggle/input/lmsys-chatbot-arena/train.csv')  # トレーニングデータをCSVファイルから読み込む

def process(input_str):  # 入力文字列を処理する関数
    stripped_str = input_str.strip('[]')  # 角括弧を取り除く
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 各文をトリミングし、リストに分割
    return ' '.join(sentences)  # 文をスペースで結合して返す

# 各列に対してprocess関数を適用
train.loc[:, 'prompt'] = train['prompt'].apply(process)  # 'prompt'列を処理
train.loc[:, 'response_a'] = train['response_a'].apply(process)  # 'response_a'列を処理
train.loc[:, 'response_b'] = train['response_b'].apply(process)  # 'response_b'列を処理

# トレーニングのために'Null'をドロップ
indexes = train[(train.response_a == 'null') & (train.response_b == 'null')].index  # 'response_a'と'response_b'が両方'null'の行のインデックスを取得
train.drop(indexes, inplace=True)  # 取得したインデックスの行を削除
train.reset_index(inplace=True, drop=True)  # インデックスをリセット

print(f"合計 {len(indexes)} 行のNullレスポンスが削除されました")
print('トレーニングサンプルの合計: ', len(train))  # トレーニングサンプルの数を出力
```

---The following area is a Code cell (cell numver is 12)---
```python
train.head(5)  # トレーニングデータの最初の5行を表示する
```

---The following area is a Code cell (cell numver is 13)---
```python
train['text'] = 'User prompt: ' + train['prompt'] +  '\n\nModel A :\n' + train['response_a'] +'\n\n--------\n\nModel B:\n'  + train['response_b']  
# トレーニング用のテキストを形成するために、各列のデータを結合して新しい'text'列を作成

print(train['text'][4])  # 'text'列の5行目（インデックスは4）の内容を表示
```

---The following area is a Code cell (cell numver is 14)---
```python
# トレーニングにはデータセットの50％のみを使用
train = train[:int(len(train) * 0.5)]  # トレーニングデータの最初の50％を選択

train.loc[:, 'token_count'] = get_token_lengths(train['text'])  # 'text'列のトークン数を計算し、'token_count'列に保存

# モデル用のラベルを準備
train.loc[:, 'label'] = np.argmax(train[['winner_model_a','winner_model_b','winner_tie']].values, axis=1)  # 勝者モデルのインデックスをラベルとして設定

# データを表示
display(train.head())  # トレーニングデータの最初の数行を表示
```

---The following area is a Code cell (cell numver is 15)---
```python
train.label.value_counts()  # 各ラベルの出現頻度をカウントして表示する
```

---The following area is a Code cell (cell numver is 16)---
```python
# トークン数の概要を表示
display(train['token_count'].describe().to_frame().astype(int))  # 'token_count'列の統計情報を整数型のデータフレームとして表示する
```

---The following area is a Code cell (cell numver is 17)---
```python
# データの90％をカバーするトークンの長さを取得しますが、1024の長さを維持します！
np.percentile(train['token_count'], 90)  # 'token_count'列の90パーセンタイルを計算して返す
```

---The following area is a Markdown cell (cell numver is 18)---
```markdown
# トークナイズ
```

---The following area is a Code cell (cell numver is 19)---
```python
# データをトークナイズ
tokens = tokenizer(
    train['text'].tolist(),  # トークナイズするテキストのリスト
    padding='max_length',  # 最大長にパディング
    max_length=CFG.MAX_LENGTH,  # 設定した最大長を使用
    truncation=True,  # 超過するトークンを切り捨てる
    return_tensors='np'  # NumPy形式で返す
)

# 入力IDはトークンIDに対応
INPUT_IDS = tokens['input_ids']  # トークンIDを取得
# パディングトークンを無視するためのアテンションマスク
ATTENTION_MASKS = tokens['attention_mask']  # アテンションマスクを取得
# テキストのラベル
LABELS = train[['winner_model_a','winner_model_b','winner_tie']].values  # 各テキストのラベルを取得

print(f'入力IDの形状: {INPUT_IDS.shape}, アテンションマスクの形状: {ATTENTION_MASKS.shape}')  # 入力IDとアテンションマスクの形状を出力
print(f'ラベルの形状: {LABELS.shape}')  # ラベルの形状を出力
```

---The following area is a Code cell (cell numver is 20)---
```python
def train_dataset(batch_size):  # トレーニング用データセットを生成する関数
    N_SAMPLES = LABELS.shape[0]  # サンプルの数を取得
    IDXS = np.arange(N_SAMPLES - (N_SAMPLES % batch_size))  # バッチサイズで割り切れるインデックスを作成
    while True:  # 無限ループでデータを生成
        # インデックスをシャッフル
        np.random.shuffle(IDXS)  
        # すべてのインデックスを一度だけ反復
        for idxs in IDXS.reshape(-1, batch_size):  # バッチサイズのインデックスで形を変更
            input_ids = torch.tensor(INPUT_IDS[idxs]).to(DEVICE)  # 入力IDをテンソルにしてデバイスに移動
            attention_mask = torch.tensor(ATTENTION_MASKS[idxs]).to(DEVICE)  # アテンションマスクをテンソルにしてデバイスに移動
            labels = torch.tensor(LABELS[idxs]).to(DEVICE)  # ラベルをテンソルにしてデバイスに移動（マルチラベル出力）
            
            # TPUノードに対してシャーディングを行う（メッシュを適切に定義する必要があります）
            xs.mark_sharding(input_ids, mesh, (0, 1))  # 入力IDのシャーディングをマーク
            xs.mark_sharding(attention_mask, mesh, (0, 1))  # アテンションマスクのシャーディングをマーク
            xs.mark_sharding(labels, mesh, (0, 1))  # ラベルのシャーディングをマーク
            
            yield input_ids, attention_mask, labels  # 入力ID、アテンションマスク、ラベルを返す

TRAIN_DATASET = train_dataset(CFG.BATCH_SIZE)  # 定義したバッチサイズでトレーニングデータセットを作成
```

---The following area is a Markdown cell (cell numver is 21)---
```markdown
# モデルをロード
```

---The following area is a Code cell (cell numver is 22)---
```python
# 3つのターゲットラベルの分類用モデルをロード
base_model = LlamaForSequenceClassification.from_pretrained(
    CFG.MODEL_NAME,  # 設定したモデル名からロード
    num_labels=CFG.NUM_LABELS,  # ラベルの数を設定
    torch_dtype=torch.bfloat16)  # 使用するデータ型を指定

base_model.config.pretraining_tp = 1  # プリトレーニングのTPを設定

# パディングトークンのIDを設定
base_model.config.pad_token_id = tokenizer.pad_token_id  # トークナイザーのパディングトークンIDをモデルに設定
```

---The following area is a Markdown cell (cell numver is 23)---
```markdown
# 低ランク適応 [LORA]
```

---The following area is a Code cell (cell numver is 24)---
```python
lora_config = LoraConfig(  # LoRAの設定を定義
    r=CFG.LORA_RANK,  # 低ランクマトリックスの次元
    lora_alpha=CFG.LORA_ALPHA,  # LoRAアクティベーションと事前トレーニングされた重みアクティベーションのスケーリングファクター
    lora_dropout=CFG.DROPOUT,  # ドロップアウト率
    bias='none',  # バイアスの設定
    inference_mode=False,  # 推論モードの設定
    task_type=TaskType.SEQ_CLS,  # タスクの種類（シーケンス分類）
    target_modules=CFG.LORA_MODULES  # 出力と値の投影のみを使用
)
```

---The following area is a Code cell (cell numver is 25)---
```python
# LoRAモデルを作成
model = get_peft_model(base_model, lora_config)  # 基本モデルからLoRAモデルを取得
# 訓練可能なパラメーターを表示
model.print_trainable_parameters()  # 訓練可能なパラメーターの数を出力
```

---The following area is a Code cell (cell numver is 26)---
```python
# TPUノードの数を取得
num_devices = xr.global_runtime_device_count()  # 使用可能なTPUデバイスの数を取得
mesh_shape = (1, num_devices, 1)  # メッシュの形状を定義
device_ids = np.array(range(num_devices))  # デバイスIDの配列を作成
mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))  # メッシュを作成（データ並列、フルモデルの分散、モデル並列）

# モデルを分散させる
partition_module(model, mesh)  # モジュールをメッシュに基づいてパーティショニング

print(f'デバイスの数: {num_devices}')  # 使用するデバイスの数を出力
```

---The following area is a Code cell (cell numver is 27)---
```python
# 学習可能なレイヤーを確認
MODEL_LAYERS_ROWS = []  # モデルのレイヤー情報を格納するリスト
TRAINABLE_PARAMS = []  # 学習可能なパラメータを格納するリスト
N_TRAINABLE_PARAMS = 0  # 学習可能なパラメータの合計数

for name, param in model.named_parameters():  # モデルの各パラメータについてループ
    # レイヤーのパラメータ数を計算
    n_parameters = int(torch.prod(torch.tensor(param.shape)))  
    # 学習可能なレイヤーのみ確認
    if param.requires_grad:  
        # レイヤー情報を追加
        MODEL_LAYERS_ROWS.append({
            'param': n_parameters,  # パラメータ数
            'name': name,  # レイヤー名
            'dtype': param.data.dtype,  # データ型
        })
        # 学習可能なパラメータを追加
        TRAINABLE_PARAMS.append({'params': param})  
        # 学習可能なパラメータの数を増加
        N_TRAINABLE_PARAMS += n_parameters
        
display(pd.DataFrame(MODEL_LAYERS_ROWS))  # レイヤー情報のデータフレームを表示

print(f"""
===============================
学習可能なパラメータの数: {N_TRAINABLE_PARAMS:,}
学習可能なレイヤーの数: {len(TRAINABLE_PARAMS)}
===============================
""")  # 学習可能なパラメータの総数とレイヤーの数を出力
```

---The following area is a Markdown cell (cell numver is 28)---
```markdown
# トレーニング
```

---The following area is a Code cell (cell numver is 29)---
```python
# 学習率 & オプティマイザーの設定
N_SAMPLES = len(train)  # トレーニングサンプルの数を取得
STEPS_PER_EPOCH = N_SAMPLES // CFG.BATCH_SIZE  # エポックごとのステップ数を計算

OPTIMIZER = torch.optim.AdamW(model.parameters(), lr=CFG.LR_MAX)  # AdamWオプティマイザーを設定

# コサイン学習率スケジュール（ウォームアップ付き）
lr_scheduler = transformers.get_cosine_schedule_with_warmup(
    optimizer=OPTIMIZER,  # 使用するオプティマイザー
    num_warmup_steps=CFG.NUM_WARMUP_STEPS,  # ウォームアップのステップ数
    num_training_steps=STEPS_PER_EPOCH * CFG.NUM_EPOCHS)  # 総トレーニングステップ数

print(f'バッチサイズ: {CFG.BATCH_SIZE}, サンプル数: {N_SAMPLES}, エポックごとのステップ数: {STEPS_PER_EPOCH}')  # バッチサイズ、サンプル数、エポックごとのステップ数を出力
```

---The following area is a Code cell (cell numver is 30)---
```python
# オプティマイザーの状態（例：モーメンタムバッファ）に対するデータ型を設定
for state in OPTIMIZER.state.values():  # オプティマイザーのすべての状態についてループ
    for k, v in state.items():  # 各状態のキーと値についてループ
        if isinstance(v, torch.Tensor) and state[k].dtype is not torch.float32:  # テンソルであり、データ型がfloat32でない場合
            state[v] = v.to(dtype=torch.float32)  # テンソルのデータ型をfloat32に変換
```

---The following area is a Code cell (cell numver is 31)---
```python
input_ids, attention_mask, labels = next(TRAIN_DATASET)  # トレーニングデータセットから次のバッチを取得

print(f'入力IDの形状: {input_ids.shape}, データ型: {input_ids.dtype}')  # 入力IDの形状とデータ型を出力
print(f'アテンションマスクの形状: {attention_mask.shape}, データ型: {attention_mask.dtype}')  # アテンションマスクの形状とデータ型を出力
print(f'ラベルの形状: {labels.shape}, データ型: {labels.dtype}')  # ラベルの形状とデータ型を出力
```

---The following area is a Code cell (cell numver is 32)---
```python
%%time
# ダミー予測
with torch.no_grad():  # 勾配計算を無効にする
    outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # モデルの出力を取得
    
print(f'ロジット: {outputs.logits}, データ型: {outputs.logits.dtype}')  # ロジットの値とデータ型を出力
```

---The following area is a Code cell (cell numver is 33)---
```python
# モデルをトレーニングモードに設定
model.train()  # モデルをトレーニングモードに切り替え

# 損失関数、クロスエントロピー
LOSS_FN = torch.nn.CrossEntropyLoss().to(dtype=torch.float32)  # クロスエントロピー損失を定義し、データ型をfloat32に設定
```

---The following area is a Code cell (cell numver is 34)---
```python
st = time()  # 開始時刻を記録
warnings.filterwarnings("error")  # 警告をエラーとして扱う
METRICS = {
    'loss': [],  # 損失を記録するリスト
    'accuracy': {'y_true': [], 'y_pred': [] }  # 正解ラベルと予測ラベルを記録する辞書
}

for epoch in tqdm(range(CFG.NUM_EPOCHS)):  # エポック数だけループ
    ste = time()  # 各エポックの開始時刻を記録
    for step in range(STEPS_PER_EPOCH):  # 各エポック内のステップ数だけループ
        # 勾配をゼロに設定
        OPTIMIZER.zero_grad()
        
        # バッチを取得
        input_ids, attention_mask, labels = next(TRAIN_DATASET)
        
        # フォワードパス
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)
       
        # ロジットをfloat32に変換
        logits = outputs.logits.to(dtype=torch.float32)
        
        # バックワードパス
        loss = LOSS_FN(logits, labels.to(dtype=torch.float32))  # 損失を計算
        loss.backward()  # バックワードパスの実行
        
        # オプティマイザーのステップ
        OPTIMIZER.step()  # オプティマイザーを更新
        xm.mark_step()  # TPUデバイスのステップをマーク
        
        # 学習率スケジューラーを更新
        lr_scheduler.step()
        
        # メトリクスと進行状況バーを更新
        METRICS['loss'].append(float(loss))  # 損失を記録
        METRICS['accuracy']['y_true'] += labels.squeeze().tolist()  # 正解ラベルを記録
        METRICS['accuracy']['y_pred'] += torch.argmax(F.softmax(logits, dim=-1), dim=1).cpu().tolist()  # 予測ラベルを記録
        
        if (step + 1) % 200 == 0:  # 200ステップごとに進捗を表示
            metrics = 'µ_loss: {:.3f}'.format(np.mean(METRICS['loss']))  # 平均損失を計算
            metrics += ', step_loss: {:.3f}'.format(METRICS['loss'][-1])  # 最後の損失を表示
            metrics += ', µ_auc: {:.3f}'.format(accuracy_score(torch.argmax(torch.tensor(METRICS['accuracy']['y_true']), axis=-1), \
                                                               METRICS['accuracy']['y_pred']))  # 正確度を計算
            lr = OPTIMIZER.param_groups[0]['lr']  # 現在の学習率を取得
            print(f'{epoch+1:02}/{CFG.NUM_EPOCHS:02} | {step+1:04}/{STEPS_PER_EPOCH} lr: {lr:.2E}, {metrics}', end='')
            print(f'\nエポック内のステップ: {step+1} 完了 | 経過時間: {time()- st}')  # 経過時間を表示
    
    print(f'\nエポック {epoch+1} 完了 | エポック全体の経過時間: {time() - ste} ' )

    # 停止した場合、将来再トレーニングを行うためにモデルとオプティマイザーを保存
    xm.save({k: v.cpu() for k, v in model.named_parameters() if v.requires_grad}, f'model_llama_3_cp_{epoch+1}_v1.pth')  # モデルを保存
    xm.save(OPTIMIZER.state_dict(), f'optimizer_llama_3_cp_{epoch+1}_v1.pth')  # オプティマイザーを保存    
    
    print(f'エポック {epoch+1} でモデルが保存されました | 経過時間: {time() - st} ')
```

---The following area is a Code cell (cell numver is 35)---
```python
plt.figure(figsize=(15, 6))  # グラフのサイズを設定
plt.plot(METRICS['loss'])  # 損失の変化をプロット
plt.xlabel('エポック内のステップ')  # x軸のラベル
plt.ylabel('損失')  # y軸のラベル
plt.title('エポック内のステップに対する損失プロット')  # グラフのタイトル
plt.show()  # グラフを表示
```

---The following area is a Markdown cell (cell numver is 36)---
```markdown
# モデルを保存
```

---The following area is a Code cell (cell numver is 37)---
```python
model = model.cpu()  # モデルをCPUに移動
torch.save(dict([(k, v) for k, v in model.named_parameters() if v.requires_grad]), 'llama_3_finetuned_model.pth')  
# 学習可能なパラメータのみを保存し、'llama_3_finetuned_model.pth'というファイル名で保存
```

---The following area is a Markdown cell (cell numver is 38)---
```markdown
# 結論

トレーニングを加速させ、最適化する余地がまだたくさんあります！もっと多くのデータ、異なるバッチサイズ、学習率などを試してみてください。成功を祈っています！
```

** @@@ Jupyter Notebook numver 96, the number of votes :0 @@@ **

---The following area is a Markdown cell (cell numver is 0)---
```markdown
# 要約 
このJupyter Notebookでは、Kaggleの「LMSYS - Chatbot Arena」コンペティションにおいて、人間の好みに基づいたモデルのトレーニングおよび評価を行うための準備と実行が取り扱われています。具体的には、異なる応答を提示するチャットボットのモデルを比較し、どちらが好まれるかを予測するための機械学習モデルの構築を目指しています。

### 取り組んでいる問題
Notebookは、ユーザープロンプトに対する2つの異なる応答の好みを判定する「報酬モデル」を構築するという課題に取り組んでいます。具体的には、トレーニングデータを処理し、必要な特徴量を抽出し、最終的にモデルのトレーニングを行う過程を示しています。

### 主な手法とライブラリ
1. **データ読み込みと前処理**:
   - Pandasを使用してトレーニングデータおよびテストデータをCSVファイルから読み込み、各応答に関する特徴を処理するカスタム関数が定義されています。
   - Hugging Faceの`datasets`ライブラリを利用してデータセットを管理します。

2. **モデルの選定と設定**:
   - Hugging FaceのTransformersライブラリから、Mistralという事前学習済みの因果言語モデルを使用し、量子化設定を行った上でモデルを読み込みます。
   - LoRA（Low-Rank Adaptation）技術を用いて、モデルにファインチューニングする設定が適用されます。

3. **トレーニング**:
   - `Trainer`クラスを拡張してカスタムトレーナーを定義し、損失計算にクラス重みを用いるなどの工夫がされています。
   - モデルのトレーニングは、PyTorchを使用し、勾配チェックポイント機能や、評価メトリクスを定義して行います。

4. **評価**:
   - トレーニング後、ユーザーが提示したプロンプトに基づいてモデルがどのように応答するかを生成し、その性能を評価します。

このNotebookは、データの前処理からモデルのトレーニングと評価に至る一連の流れを詳細に記述し、実践的な機械学習プロジェクトの進め方を示しています。使用するライブラリには、NumPy、Pandas、Transformers、Hugging FaceのDatasets、PyTorch、PEFT（Parameter-Efficient Fine-Tuning）などがあります。
```

---The following area is a Code cell (cell numver is 1)---
```python
# このPython 3環境には、多くの便利な分析ライブラリがインストールされています
# これはkaggle/python Dockerイメージによって定義されています: https://github.com/kaggle/docker-python
# たとえば、いくつかの便利なパッケージを読み込むことができます

import numpy as np # 線形代数用
import pandas as pd # データ処理、CSVファイルの入出力（例: pd.read_csv）

# 入力データファイルは、読み取り専用の"../input/"ディレクトリにあります
# たとえば、これを実行することで（実行をクリックするか、Shift + Enterを押すことで）入力ディレクトリ内のすべてのファイルをリスト表示できます

import os # オペレーティングシステムに関する機能を使用するためのモジュール
for dirname, _, filenames in os.walk('/kaggle/input'): # "/kaggle/input"ディレクトリ内のファイルやフォルダを再帰的に巡回します
    for filename in filenames: # ファイル名のリストをループします
        print(os.path.join(dirname, filename)) # 各ファイルのフルパスを出力します

# あなたは現在のディレクトリ（/kaggle/working/）に最大20GBを保存できます。これは「すべて保存して実行」機能を使用してバージョンを作成したときに出力として保持されます
# また、一時ファイルを/kaggle/temp/に書き込むこともできますが、現在のセッションの外では保存されません
```

---The following area is a Code cell (cell numver is 2)---
```python
# !pip install cupy-cuda11x>=12.0.0  # GPUを利用したCuPyライブラリのインストール（CUDA 11.x以上）
# !pip install packaging>=22  # パッケージ管理用ライブラリのインストール（バージョン22以上）
# !pip install shapely>=2.0.1  # 幾何学的操作を行うためのShapelyライブラリのインストール（バージョン2.0.1以上）
# !pip install numpy<1.26  # 数値計算用のNumPyライブラリのインストール（バージョン1.26未満）
# !pip install scipy<1.12  # 科学計算用のSciPyライブラリのインストール（バージョン1.12未満）
!pip install transformers datasets accelerate peft  # Transformers、データセット、加速化、PEFTライブラリのインストール
```

---The following area is a Code cell (cell numver is 3)---
```python
# このコマンドは、マシンごとに一度だけ実行すれば十分です
!pip install -q -U bitsandbytes  # bitsandbytesライブラリのインストール（省略モードで、アップグレードする）
# !pip install -q -U git+https://github.com/huggingface/transformers.git  # Hugging FaceのTransformersライブラリをGitHubからインストール（アップグレード）
# !pip install -q -U git+https://github.com/huggingface/peft.git  # Hugging FaceのPEFTライブラリをGitHubからインストール（アップグレード）
# !pip install -q -U git+https://github.com/huggingface/accelerate.git  # Hugging FaceのAccelerateライブラリをGitHubからインストール（アップグレード）
# !pip install -q -U datasets scipy ipywidgets  # データセット、SciPy、およびipywidgetsライブラリのインストール（アップグレード）
!pip install datasets  # datasetsライブラリのインストール
```

---The following area is a Code cell (cell numver is 4)---
```python
from datasets import load_dataset  # datasetsモジュールからload_dataset関数をインポートします

# 'csv'形式のデータセットを読み込みます。特に'test.csv'ファイルを指定しています。
train_dataset = load_dataset('csv', data_files='/kaggle/input/lmsys-chatbot-arena/test.csv')  # テストデータを含むCSVファイルを読み込み、train_datasetに格納します
```

---The following area is a Code cell (cell numver is 5)---
```python
from huggingface_hub import notebook_login  # Hugging Face Hubからnotebook_login関数をインポートします

# Hugging Face Hubへのログインプロンプトを表示します
notebook_login()  # ユーザーがHugging Face Hubにログインできるようにします。これにより、モデルやデータセットにアクセスできるようになります。
```

---The following area is a Code cell (cell numver is 6)---
```python
import torch  # PyTorchライブラリをインポートします
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # Hugging FaceのTransformersライブラリから必要なクラスをインポートします

base_model_id = "mistralai/Mistral-7B-v0.1"  # 使用する基本モデルのIDを指定します
bnb_config = BitsAndBytesConfig(  # 量子化に関する設定を作成します
    load_in_4bit=True,  # 4ビットでモデルを読み込む設定をします
    bnb_4bit_use_double_quant=True,  # ダブル量子化を使用する設定を行います
    bnb_4bit_quant_type="nf4",  # 量子化のタイプを指定します（nf4を使用）
    bnb_4bit_compute_dtype=torch.bfloat16  # 計算データ型をbfloat16に設定します
)

# モデルを事前学習済みの状態から読み込みます。量子化設定とデバイスマップを指定します。
model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map="auto")  # 自動デバイスマッピングでモデルを読み込みます
```

---The following area is a Code cell (cell numver is 7)---
```python
tokenizer = AutoTokenizer.from_pretrained(  # 基本モデルのトークナイザーを読み込みます
    base_model_id,  # 使用するモデルIDを指定します
    model_max_length=512,  # モデルの最大長を512トークンに設定します
    padding_side="left",  # パディングを左側に設定します
    add_eos_token=True  # 終了トークンを追加する設定をします
)

# トークナイザーのパディングトークンを終了トークンに設定します
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークンと同じに設定することで、モデル入力の整合性を保ちます
```

---The following area is a Code cell (cell numver is 8)---
```python
# Pandasライブラリを使用して、CSVファイルを読み込みます
train_data = pd.read_csv('../input/lmsys-chatbot-arena/train.csv')  # トレーニングデータを読み込みます
test_data = pd.read_csv('../input/lmsys-chatbot-arena/test.csv')  # テストデータを読み込みます
submission_data = pd.read_csv('../input/lmsys-chatbot-arena/sample_submission.csv')  # 提出用サンプルデータを読み込みます
```

---The following area is a Code cell (cell numver is 9)---
```python
# 各データセットの形状（行数と列数）を表示します
train_data.shape, test_data.shape, submission_data.shape  # トレーニングデータ、テストデータ、および提出データのサイズを確認します
```

---The following area is a Markdown cell (cell numver is 10)---
```markdown
```python
def process(input_str):  # 入力文字列を処理する関数
    stripped_str = input_str.strip('[]')  # 角括弧を削除します
    sentences = [s.strip('"') for s in stripped_str.split('","')]  # 各文を分離し、両端の引用符を削除します
    return  ' '.join(sentences)  # 文をスペースで結合して返します

def trim_endings(custom_string):  # 文字列の末尾をトリミングする関数
    return custom_string[:-2][2:]  # 最後の2文字を削除し、最初の2文字を除去して返します

def count_newlines(custom_string):  # 改行文字のカウントを行う関数
    return custom_string.count('\\n')  # 文字列内の改行数を返します

def word_counts(custom_string):  # 単語のカウントを行う関数
    return len(custom_string.split())  # 文字列を単語に分割し、その数を返します


def apply_transformations(df):  # データフレームに変換を適用する関数
    
    df['prompt'] =  df['prompt'].map(process)  # 'prompt'列にprocess関数を適用します
    df['response_a'] =  df['response_a'].map(process)  # 'response_a'列にprocess関数を適用します
    df['response_b'] =  df['response_b'].map(process)  # 'response_b'列にprocess関数を適用します
    
    df['prompt'] =  df['prompt'].map(trim_endings)  # 'prompt'列の末尾をトリミングします
    df['response_a'] =  df['response_a'].map(trim_endings)  # 'response_a'列の末尾をトリミングします
    df['response_b'] =  df['response_b'].map(trim_endings)  # 'response_b'列の末尾をトリミングします
    
    df['res_a_line_count'] = df['response_a'].map(count_newlines).astype(str)  # 'response_a'の改行数を計算し、新しい列に格納します
    df['res_b_line_count'] = df['response_b'].map(count_newlines).astype(str)  # 'response_b'の改行数を計算し、新しい列に格納します
    
    df['res_a_word_count'] = df['response_a'].map(word_counts).astype(str)  # 'response_a'の単語数を計算し、新しい列に格納します
    df['res_b_word_count'] = df['response_b'].map(word_counts).astype(str)  # 'response_b'の単語数を計算し、新しい列に格納します
    
    return df  # 変換後のデータフレームを返します

def create_summary(df):  # サマリーを作成する関数
    df['summary'] = 'User prompt: ' + df['prompt'] +  '\n\n Model A :\n' + df['response_a'] +'\n\n Model A length:\n' + df['res_a_word_count'] +'\n\n Model A Line Count:\n' + df['res_a_line_count'] +'\n\nModel B:\n'  + df['response_b'] +'\n\n \n\nModel B length:\n'  + df['res_b_word_count'] +'\n\n \n\nModel B Line Count:\n'  + df['res_b_line_count']
    return df  # データフレームにサマリー列を追加して返します
```
```

---The following area is a Code cell (cell numver is 11)---
```python
# トレーニングデータに変換を適用し、サマリーを作成します
train_data = apply_transformations(train_data)  # トレーニングデータに変換を適用します
train_data = create_summary(train_data)  # トレーニングデータにサマリーを作成します

# テストデータにも同様に変換を適用し、サマリーを作成します
test_data = apply_transformations(test_data)  # テストデータに変換を適用します    
test_data = create_summary(test_data)  # テストデータにサマリーを作成します
```

---The following area is a Code cell (cell numver is 12)---
```python
# トレーニングデータのサマリー列の3番目の要素を表示します
train_data['summary'][2]  # インデックス2のサマリーを取得して表示します
```

---The following area is a Code cell (cell numver is 13)---
```python
# トレーニングデータにラベル列を追加します。最高スコアのモデルを特定します
train_data['labels'] = train_data[['winner_model_a', 'winner_model_b', 'winner_tie']].idxmax(axis=1)  # 各行で最大値を持つ列のインデックスを取得して'labels'列に格納します

train_data['labels'] = train_data['labels'].astype('category')  # 'labels'列をカテゴリ型に変換します
train_data['target'] = train_data['labels'].cat.codes  # 'labels'のカテゴリコードを' target'列に格納します
```

---The following area is a Code cell (cell numver is 14)---
```python
# 'labels'列のカテゴリに対するコードを作成します
category_map = {code: category for code, category in enumerate(train_data['labels'].cat.categories)}  # 各カテゴリに対するコードを辞書形式で作成します
category_map  # カテゴリマップを表示します
```

---The following area is a Code cell (cell numver is 15)---
```python
# 再現性を持たせるためのランダムシードを設定します
SEED = 7920  # シード値を7920に設定します。これにより、結果が一貫性を持つようになります
```

---The following area is a Code cell (cell numver is 16)---
```python
from sklearn.datasets import make_classification  # データセットを生成するためのsklearnの関数をインポートします
from sklearn.model_selection import train_test_split  # データを訓練用と検証用に分割するための関数をインポートします
from sklearn.linear_model import LogisticRegression  # ロジスティック回帰モデルをインポートします
from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score  # 評価指標を計算するための関数をインポートします

# 使用するカラムを選択してトレーニングデータとテストデータを準備します
train_df = train_data[['id', 'summary', 'target']]  # トレーニングデータから'id', 'summary', 'target'を選択します
test_df = train_data[['id', 'summary']]  # テストデータから'id'と'summary'を選択します

# トレーニングデータを訓練セットと検証セットに分割します
df_train, df_val = train_test_split(train_df, test_size=0.2, random_state=SEED, stratify=train_df['target'])  # 20%を検証データとして分割します。分類のバランスを保ちます
```

---The following area is a Code cell (cell numver is 17)---
```python
from datasets import Dataset, DatasetDict  # datasetsモジュールからDatasetおよびDatasetDictをインポートします

# PandasデータフレームからHugging FaceのDataset形式に変換します
dataset_train = Dataset.from_pandas(df_train, preserve_index=False)  # トレーニングデータをDatasetに変換します（インデックスは保存しません）
dataset_val = Dataset.from_pandas(df_val, preserve_index=False)  # 検証データをDatasetに変換します（インデックスは保存しません）
dataset_test = Dataset.from_pandas(test_df, preserve_index=False)  # テストデータをDatasetに変換します（インデックスは保存しません）
```

---The following area is a Code cell (cell numver is 18)---
```python
# トレーニングデータセットをシャッフルして、新しいデータセットを作成します
dataset_train_shuffled = dataset_train.shuffle(seed=SEED)  # 指定したシードを使用してデータをランダムに並び替えます
```

---The following area is a Code cell (cell numver is 19)---
```python
# データセットを辞書形式でまとめて、トレーニング、検証、テストのデータセットを管理します
dataset = DatasetDict({  # データセットの辞書を作成します
    'train': dataset_train_shuffled,  # シャッフルしたトレーニングデータを追加します
    # 'val': dataset_val,  # 検証データを追加します（コメントアウト中）
    # 'test': dataset_test  # テストデータを追加します（コメントアウト中）
})
dataset  # 作成したデータセット辞書を表示します
```

---The following area is a Code cell (cell numver is 20)---
```python
# トレーニングデータの'target'列の値のカウントを正規化して表示します
df_train.target.value_counts(normalize=True)  # 各クラスの割合を表示します。これにより、データのバランスを確認できます。
```

---The following area is a Code cell (cell numver is 21)---
```python
# クラスの重みを計算します。これにより、クラス間の不均衡を扱います
class_weights = (1 / df_train.target.value_counts(normalize=True).sort_index()).tolist()  # 正規化されたターゲットの値を使って、それらの逆数を計算します
class_weights = torch.tensor(class_weights)  # リストをPyTorchのテンソルに変換します
class_weights = class_weights / class_weights.sum()  # 重みを合計で割って、正規化された重みにします
class_weights  # 計算したクラス重みを表示します
```

---The following area is a Code cell (cell numver is 22)---
```python
# トークナイザーを再度設定します。これによりトークン化が可能になります
tokenizer = AutoTokenizer.from_pretrained(  # 基本モデルのトークナイザーを読み込みます
    base_model_id,  # 使用するモデルIDを指定します
    model_max_length=512,  # モデルの最大長を512トークンに設定します
    padding_side="left",  # パディングを左側に設定します
    add_eos_token=True  # 終了トークンを追加する設定をします
)

# トークナイザーのパディングトークンを終了トークンに設定します
tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークンと同じに設定します
```

---The following area is a Code cell (cell numver is 23)---
```python
MAX_LEN = 512  # モデルの最大入力長を512トークンに設定します
col_to_delete = ['summary']  # 削除する列のリストを定義します

def llama_preprocessing_function(examples):  # データを前処理する関数を定義します
    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)  # 'summary'カラムをトークン化し、長さを最大値でトランケーションします

# データセット全体に前処理関数を適用し、バッチ処理を行い、不要な列を削除します
tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)

# 'target'列を'labels'列に名前を変更します
tokenized_datasets = tokenized_datasets.rename_column("target", "labels")

# データセットのフォーマットをPyTorch形式に設定します
tokenized_datasets.set_format("torch")  # トレーニング時にPyTorchが使用できる形式に設定します
```

---The following area is a Code cell (cell numver is 24)---
```python
from peft import prepare_model_for_kbit_training  # PEFTからkビットトレーニング用のモデル準備関数をインポートします

model.gradient_checkpointing_enable()  # 勾配チェックポイントを有効にして、メモリを節約します
model = prepare_model_for_kbit_training(model)  # モデルをkビットトレーニング用に準備します
```

---The following area is a Code cell (cell numver is 25)---
```python
def print_trainable_parameters(model):  # モデルのトレーニング可能なパラメータを表示する関数
    """
    モデル内のトレーニング可能なパラメータの数を表示します。
    """
    trainable_params = 0  # トレーニング可能なパラメータの初期化
    all_param = 0  # すべてのパラメータの初期化
    for _, param in model.named_parameters():  # モデル内のすべてのパラメータをループします
        all_param += param.numel()  # 総パラメータ数を計算します
        if param.requires_grad:  # パラメータが勾配計算を必要とする場合
            trainable_params += param.numel()  # トレーニング可能なパラメータ数を累積します
    print(
        f"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}"
    )  # トレーニング可能なパラメータ数、すべてのパラメータ数、トレーニング可能なパラメータの割合を表示します
```

---The following area is a Code cell (cell numver is 26)---
```python
# モデルの詳細を表示します
print(model)  # モデルの構造とパラメータの詳細を出力します
```

---The following area is a Code cell (cell numver is 27)---
```python
from accelerate import FullyShardedDataParallelPlugin, Accelerator  # Accelerateライブラリから関連するクラスをインポートします
from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig  # FSDPのための設定クラスをインポートします

# Fully Sharded Data Parallelプラグインを設定します
fsdp_plugin = FullyShardedDataParallelPlugin(
    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),  # チェックポイントをCPUにオフロードする設定
    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),  # 最適化ステート情報もCPUにオフロードする設定
)

# Acceleratorインスタンスを作成します
accelerator = Accelerator(fsdp_plugin=fsdp_plugin)  # FSDPプラグインを使用してAcceleratorを初期化します
```

---The following area is a Code cell (cell numver is 28)---
```python
from peft import LoraConfig, get_peft_model  # PEFTライブラリからLoraConfigとモデル取得関数をインポートします

# LoRAの設定を定義します
config = LoraConfig(
    r=8,  # rankの値
    lora_alpha=16,  # LoRAのスケーリングファクター
    target_modules=[  # 対象モジュールのリスト
        "q_proj",
        "k_proj",
        "v_proj",
        "o_proj",
        "gate_proj",
        "up_proj",
        "down_proj",
        "lm_head",
    ],
    bias="none",  # バイアスの設定
    lora_dropout=0.05,  # LoRAのドロップアウト率
    task_type="CAUSAL_LM",  # タスクタイプを指定します（因果言語モデル）
)

# モデルにLoRA設定を適用します
model = get_peft_model(model, config)  # LoRAの設定を持つモデルを取得します
print_trainable_parameters(model)  # トレーニング可能なパラメータを表示します

# アクセラレーターを適用します。必要に応じてコメントアウトできます。
model = accelerator.prepare_model(model)  # Accelerateを使用してモデルを準備します
```

---The following area is a Code cell (cell numver is 29)---
```python
# Weights & Biases（WandB）ライブラリをインストールします
!pip install -q wandb -U  # WandBをアップグレードします

import wandb, os  # WandBとOSモジュールをインポートします
wandb.login()  # WandBにログインします

wandb_project = "mistral-finetune"  # プロジェクト名を設定します
if len(wandb_project) > 0:  # プロジェクト名が空でない場合
    os.environ["WANDB_PROJECT"] = wandb_project  # 環境変数にプロジェクト名を設定します
```

---The following area is a Code cell (cell numver is 30)---
```python
# GPUの数が1つより多い場合の処理を行います
if torch.cuda.device_count() > 1:  # 使用可能なGPUが1つ以上の場合
    model.is_parallelizable = True  # モデルが並列化可能であることを指定します
    model.model_parallel = True  # モデル並列を有効にします
```

---The following area is a Code cell (cell numver is 31)---
```python
# NVIDIAのCUDAコンパイラのバージョンを表示します
!nvcc --version  # nvccのバージョン情報を表示します

# 古いバージョンのPyTorch関連ライブラリをアンインストールします
!pip uninstall torch torchvision torchaudio -y  # PyTorch、TorchVision、およびTorchAudioを強制的にアンインストールします

# CUDA 12.1対応の指定バージョンのPyTorch関連ライブラリをインストールします
!pip install torch==2.0.0+cu121 torchvision==0.15.0+cu121 torchaudio==2.0.0+cu121 -f https://download.pytorch.org/whl/nightly/cu121/torch_nightly.html  # PyTorchの特定バージョンをインストールします
```

---The following area is a Code cell (cell numver is 32)---
```python
# 既存のtorch、torchvision、torchaudioのバージョンをアンインストールします
!pip uninstall torch torchvision torchaudio -y  # PyTorch、TorchVision、およびTorchAudioを強制的にアンインストールします

# CUDA 11.2に対応した互換性のあるバージョンをインストールします
!pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cu112/torch_stable.html  # PyTorchの安定したバージョンをインストールします
```

---The following area is a Code cell (cell numver is 33)---
```python
import os  # OSモジュールをインポートします

# CUDAの起動ブロッキングを無効にします
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # CUDAカーネルの起動時にブロッキングを無効にします。これにより、実行時のパフォーマンスが向上する可能性があります。
```

---The following area is a Code cell (cell numver is 34)---
```python
import torch  # PyTorchライブラリをインポートします

seed = 7920  # 使用するシード値を設定します

# CUDAが使用可能であれば、CUDAデバイス用の乱数生成器を設定します
if torch.cuda.is_available():
    generator = torch.Generator(device='cuda').manual_seed(seed)  # CUDAデバイス用にシードを設定します
else:
    generator = torch.Generator().manual_seed(seed)  # CPU用にシードを設定します
```

---The following area is a Code cell (cell numver is 35)---
```python
# CUDAが利用可能であれば、CUDAデバイス用の乱数生成器を設定します
if torch.cuda.is_available():  
    generator = torch.Generator('cuda').manual_seed(seed)  # CUDAデバイスの乱数生成器をシードで初期化します
else:
    generator = torch.Generator().manual_seed(seed)  # CPU用の乱数生成器をシードで初期化します
```

---The following area is a Code cell (cell numver is 36)---
```python
import transformers  # Transformersライブラリをインポートします

class CustomTrainer(transformers.Trainer):  # Trainerクラスを継承したカスタムトレーナークラスを定義します
    def __init__(self, *args, class_weights=None, **kwargs):  # コンストラクタを定義します
        super().__init__(*args, **kwargs)  # 親クラスのコンストラクタを呼び出します
        # クラス重みが与えられた場合、テンソルに変換します
        if class_weights is not None:
            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)  # クラス重みをデバイスに移動します
        else:
            self.class_weights = None  # クラス重みがない場合はNoneとします

    def compute_loss(self, model, inputs, return_outputs=False):  # 損失を計算する関数を定義します
        # ラベルを抽出し、cross_entropy用にlong型に変換します
        labels = inputs.pop("labels").long()  # 入力からラベルを取り出します

        # フォワードパス
        outputs = model(**inputs)  # モデルを介して入力を処理します

        # 出力されたロジットを抽出します
        logits = outputs.get('logits')  # モデル出力からロジットを取り出します

        # 不均衡データ処理のためのクラス重みを用いたカスタム損失を計算します
        if self.class_weights is not None:
            loss = F.cross_entropy(logits, labels, weight=self.class_weights)  # クラス重みを使った損失計算
        else:
            loss = F.cross_entropy(logits, labels)  # クラス重みなしで損失計算

        return (loss, outputs) if return_outputs else loss  # 出力の条件に応じて損失と出力を返します
```

---The following area is a Code cell (cell numver is 37)---
```python
def compute_metrics(eval_pred):  # 評価予測をもとにメトリクスを計算する関数を定義します
    predictions, labels = eval_pred  # 評価予測から予測値とラベルを取得します
    predictions = np.argmax(predictions, axis=1)  # 予測の中で最大値のインデックスを取得してクラスラベルを決定します
    # 精度とバランス精度を計算して返します
    return {
        'balanced_accuracy': balanced_accuracy_score(predictions, labels),  # バランス精度を計算します
        'accuracy': accuracy_score(predictions, labels)  # 純粋な精度を計算します
    }
```

---The following area is a Code cell (cell numver is 38)---
```python
# トレーニングに関する引数を設定します
training_args = transformers.TrainingArguments(
    output_dir='model_classification',  # モデルの出力先ディレクトリ
    learning_rate=1e-4,  # 学習率
    per_device_train_batch_size=8,  # デバイスごとのトレーニングバッチサイズ
    per_device_eval_batch_size=8,  # デバイスごとの評価バッチサイズ
    num_train_epochs=1,  # トレーニングエポック数
    weight_decay=0.01,  # 重み減衰率
    evaluation_strategy='epoch',  # 評価戦略をエポック単位に設定
    save_strategy='epoch',  # モデルの保存戦略をエポック単位に設定
    load_best_model_at_end=True,  # 最良のモデルを最後にロード
	logging_steps=10,  # ロギングの頻度
	max_steps=100,  # 最大ステップ数
    fp16=True,  # 16ビット浮動小数点でトレーニングを実行
    push_to_hub=False  # モデルをHugging Face Hubにプッシュしない設定
)
```

---The following area is a Code cell (cell numver is 39)---
```python
# データのコラテーションを行うための関数を設定します
collate_fn = transformers.DataCollatorWithPadding(tokenizer=tokenizer)  # トークナイザーを用いてパディング付きのデータコレータを作成します
```

---The following area is a Code cell (cell numver is 40)---
```python
# カスタムトレーナーのインスタンスを作成します
trainer = CustomTrainer(
    model=model,  # 使用するモデル
    args=training_args,  # トレーニング引数
    train_dataset=tokenized_datasets['train'],  # トレーニングデータセット
    tokenizer=tokenizer,  # トークナイザー
    data_collator=collate_fn,  # データコレータ
    compute_metrics=compute_metrics,  # メトリクス計算関数
    class_weights=class_weights,  # クラス重み
)
```

---The following area is a Code cell (cell numver is 41)---
```python
# PyTorchライブラリをインストールします
!pip install torch  # 最新のPyTorchをインストールします
```

---The following area is a Code cell (cell numver is 42)---
```python
import transformers  # Transformersライブラリをインポートします
from datetime import datetime  # 日付時刻を扱うためのライブラリをインポートします
os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # CUDAカーネルの起動をブロッキングします

# CUDAの利用可能性を確認します
print("CUDA Available:", torch.cuda.is_available())  # CUDAが使用可能かどうかを表示します
project = "mistral-finetune"  # プロジェクト名を設定します
base_model_name = "mistral"  # 基本モデル名を設定します
run_name = base_model_name + "-" + project  # 実行名を設定します
output_dir = "./" + run_name  # 出力ディレクトリを設定します

tokenizer.pad_token = tokenizer.eos_token  # パディングトークンを終了トークンに設定します

# Trainerインスタンスを作成します
trainer = transformers.Trainer(
    model=model,  # 使用するモデル
    train_dataset=tokenized_datasets['train'],  # トレーニングデータセット
    args=transformers.TrainingArguments(
        output_dir=output_dir,  # 出力ディレクトリ
        warmup_steps=5,  # ウォームアップステップ数
        per_device_train_batch_size=2,  # デバイスごとのトレーニングバッチサイズ
        gradient_checkpointing=True,  # 勾配チェックポイントを有効にします
        gradient_accumulation_steps=4,  # 勾配蓄積ステップ数
        max_steps=1000,  # 最大ステップ数
        learning_rate=2.5e-5,  # 学習率
        logging_steps=50,  # ロギングの頻度
        fp16=False,  # 16ビット浮動小数点を無効にします
        optim="paged_adamw_8bit",  # 最適化手法を指定します
        logging_dir="./logs",  # ログの保存先ディレクトリ
        save_strategy="steps",  # ステップごとにモデルチェックポイントを保存
        save_steps=50,  # 50ステップごとにチェックポイントを保存
        evaluation_strategy="steps",  # ステップごとに評価
        eval_steps=50,  # 50ステップごとに評価しチェックポイントを保存
        do_eval=True,  # トレーニングの終了時に評価を実行
        report_to="wandb",  # Weights & Biasesにレポートするかどうか
        run_name=f"{run_name}-{datetime.now().strftime('%Y-%m-%d-%H-%M')}",  # W&Bランの名前の設定
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),  # 言語モデル用のデータコラレータを設定
)

model.config.use_cache = False  # 警告を抑制します。推論時には再度有効にしてください
trainer.train()  # トレーニングを開始します
```

---The following area is a Code cell (cell numver is 43)---
```python
import torch  # PyTorchライブラリをインポートします
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig  # 必要なクラスをインポートします

base_model_id = "mistralai/Mistral-7B-v0.1"  # 使用する基本モデルのIDを指定します
bnb_config = BitsAndBytesConfig(  # 量子化に関する設定を作成します
    load_in_4bit=True,  # 4ビットでモデルを読み込む設定
    bnb_4bit_use_double_quant=True,  # 2重量子化を使用する設定
    bnb_4bit_quant_type="nf4",  # 量子化のタイプを指定します（nf4）
    bnb_4bit_compute_dtype=torch.bfloat16  # 計算データ型をbfloat16に設定
)

# 事前学習済みのモデルを読み込みます
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_id,  # Mistralモデル
    quantization_config=bnb_config,  # 量子化設定
    device_map="auto",  # 自動デバイスマッピング
    trust_remote_code=True,  # リモートコードを信頼する設定
)

# 評価用のトークナイザーを読み込みます
eval_tokenizer = AutoTokenizer.from_pretrained(
    base_model_id,  # 使用するモデルIDを指定します
    add_bos_token=True,  # 始まりのトークンを追加する設定
    trust_remote_code=True,  # リモートコードを信頼する設定
)
```

---The following area is a Code cell (cell numver is 44)---
```python
from peft import PeftModel  # PEFTライブラリからPeftModelをインポートします

# 事前学習済みのモデルにファインチューニングされたモデルをロードします
ft_model = PeftModel.from_pretrained(base_model, "mistral-viggo-finetune/checkpoint-1000")  # 指定されたチェックポイントからファインチューニングモデルを読み込みます
```

---The following area is a Code cell (cell numver is 45)---
```python
# 評価用のプロンプトを定義します
eval_prompt = """Given a target sentence construct the underlying meaning representation of the input sentence as a single function with attributes and attribute values.
This function should describe the target string accurately and the function must be one of the following ['inform', 'request', 'give_opinion', 'confirm', 'verify_attribute', 'suggest', 'request_explanation', 'recommend', 'request_attribute'].
The attributes must be one of the following: ['name', 'exp_release_date', 'release_year', 'developer', 'esrb', 'rating', 'genres', 'player_perspective', 'has_multiplayer', 'platforms', 'available_on_steam', 'has_linux_release', 'has_mac_release', 'specifier']

### Target sentence:
Earlier, you stated that you didn't have strong feelings about PlayStation's Little Big Adventure. Is your opinion true for all games which don't have multiplayer?

### Meaning representation:
""" 

# トークナイザーを使用してモデル入力を準備します
model_input = eval_tokenizer(eval_prompt, return_tensors="pt").to("cuda")  # CUDAデバイスに転送します

# モデルを評価モードに設定します
ft_model.eval()
with torch.no_grad():  # 勾配計算を無効にします
    # モデルを使って生成した出力を表示します
    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))  # 生成されたトークンをデコードし、特殊トークンをスキップして表示します
```

