# Llama 3.1 7b vs Gemma 9b (sft)?
**SeshuRaju 🧘‍♂️** *2024年7月28日 日曜日 02:44:32 GMT+0900 (日本標準時)* (5 votes)
GemmaのローカルCVはLlama 3.1よりも良い結果が出ていますが、皆さんも同じですか？
- 設定はsft、qlora、4bit、バッチサイズは同じです。
Gemma 9b:  
  ステップ10: 損失 = 2.3923
  ステップ20: 損失 = 2.0361
  ステップ30: 損失 = 1.4534
  ステップ40: 損失 = 1.6852
  ステップ50: 損失 = 1.3092
LLama 3.1 7b:
  ステップ10: 損失 = 2.6542
  ステップ20: 損失 = 3.2993
  ステップ30: 損失 = 2.4278
  ステップ40: 損失 = 2.0152
  ステップ50: 損失 = 2.3515
---
 # 他のユーザーからのコメント
> ## Helmut12
> 
> コードページを見る限り、Gemmaはこのコンペティションに適していると思います。
> 
> 
> 
---
> ## sayoulala
> 
> 学習損失だけでは、どちらがうまく機能していないかを判断するのに十分ではありません。
> 
> 
> 
---
> ## Ashwani
> 
> 私の限られた実験では、gemma9bはllama3.1とllama3よりも良いパフォーマンスを発揮しています。
> 
> llama3.1とllama3はどちらも似たようなパフォーマンスで、llama3.1がわずかに優れています。
> 
> 
> 
> > ## Merlyn Wang
> > 
> > 同感です。
> > 
> > 
> > 
---
> ## CPMP
> 
> これは学習損失ですか、それとも検証損失ですか？
> 
> 
> 
> > ## SeshuRaju 🧘‍♂️トピック作成者
> > 
> > これは投稿[@cpmpml](https://www.kaggle.com/cpmpml) にある学習損失です。
> > 
> > エポックごとの検証損失です。
> > 
> >   ローカルCVの場合 - Llama 3.1 - 1.097、Gemma - 0.981
> > 
> > 
> > 
> > > ## CPMP
> > > 
> > > 1.09は学習していないモデルです。私の意見では、何かが間違っています。
> > > 
> > > 
> > > 
---

