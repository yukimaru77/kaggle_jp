# 要約 
## ディスカッション要約

このディスカッションは、KaggleコンペティションでLlama3-8bモデルをファインチューニングしたユーザーが、オフライン環境でモデルの重みをロードできない問題に直面したことから始まりました。

ユーザーは、Hugging Faceから取得したベースモデルとKaggleのデータセットからロードしたベースモデルの両方で、モデルの重みをロードしようとしましたが、どちらも失敗しました。

ユーザーは、Hugging FaceとKaggleのデータセットのモデルが互換性がないために、ロードが失敗しているのではないかと推測しました。

しかし、ユーザーは後に、トレーニングフローの変数名に矛盾があり、バックワードに必要な正しいラベルが取得できないことが原因であることを発見しました。

他のユーザーからのコメントでは、ファインチューニングにかかった時間や、Colabでのファインチューニングに関する質問が寄せられました。

**要約:**

* ユーザーは、KaggleでファインチューニングしたLlama3-8bモデルをオフライン環境でロードできない問題に直面しました。
* 問題の原因は、トレーニングフローの変数名に矛盾があったことでした。
* ユーザーは問題を解決し、ファインチューニングを続けることができました。
* 他のユーザーは、ファインチューニングにかかった時間や、Colabでのファインチューニングに関する質問をしました。 


---
# Llama3のファインチューニングモデルをオフラインで使う方法

**nahyat** *2024年6月20日 16:00:07 (日本標準時)* (1票)

Kishan Vavdaraさんの素晴らしいノートブックにインスパイアされて、ColabでLlama3-8bモデルをファインチューニングしました。ベースモデルはKaggleのデータセットではなく、Hugging FaceのLlama3-8bモデル（MODEL_NAME = 'meta-llama/Meta-Llama-3-8B'）から取得しました。

Kaggleでファインチューニングしたモデルを使おうとしたところ、オフライン環境ではHugging Faceの認証ができないため、作成したモデルの重みをロードできませんでした。

Kaggleのデータセットからベースモデルをロードした後も、モデルの重みをロードできませんでした。

これは、Hugging FaceとKaggleのデータセットのモデルが互換性がないために、ロードが失敗しているのでしょうか？

注：Kaggleのデータセットでは、Llama3ライセンスの申請も許可されています。

ご視聴ありがとうございました。

---
# 他のユーザーからのコメント

> ## Kishan Vavdara
> 
> ノートブックが役に立って嬉しいです。発生しているエラーを教えていただけますか？また、ファインチューニングにLoRAを使用しましたか？
> 
> 
> 
> > ## nahyatトピック作成者
> > 
> > ご視聴いただきありがとうございます！
> > 
> > エラーは出ていませんでしたが、ファインチューニングしたモデルの重みをベースモデルにロードする際、処理が速すぎる（あなたのモデルと比べて）のではないかと心配していました。 ↓
> > 
> > model_0.load_state_dict(torch.load(WEIGHTS_PATH), strict=False)
> > 
> > 推論の結果が驚くほど低かったので、モデルのパラメータをロードする際に何か問題があったのではないかと推測しました。
> > 
> > しかし、ローカル環境でコードを見直したところ、トレーニングフローの変数名に矛盾があり、バックワードに必要な正しいラベルが取得できないことがわかりました。（なぜtrainer.train()がエラーなしで動作したのかはわかりません。）
> > 
> > 時間を取らせてしまい申し訳ありません…
> > 
> > 
> > 
> > > ## Kishan Vavdara
> > > 
> > > 問題を見つけて解決できてよかったです。ファインチューニング頑張ってください！
> > > 
> > > 
> > > 
---
> ## Lorry Zou
> 
> Colabでのファインチューニングにかかった時間はどのくらいですか？私もColabでファインチューニングしようとしていますが、A100を使用しても20時間以上かかります。これはColabの最大セッション時間（12時間）よりもはるかに長いです。どのようにして12時間よりも短くできたのですか？どうもありがとうございます。
> 
> 
> 
---


