# Gemma2 9b の推論時間が Llama3 8b よりも大幅に長いのはなぜですか？

**Dylan Liu** *2024年7月17日 水曜日 15:55:30 GMT+0900 (日本標準時)* (2票)

同じ提出コードで、Llama3 8b モデルの推論に約4時間かかりますが、Gemma2 9b は約8時間かかります。同じような経験をしていますか？
---
# 他のユーザーからのコメント
> ## Ashwani
> 
> 私はそのような違いを見たことがありません。私の場合、Gemma は Llama よりも 25% ほど時間がかかります。
> 
> 推論時間をさらに短縮したい場合は、各バッチの動的パディングを確認してください。😀
> 
> 
> 
---
> ## Sparsh Tewatia
> 
> 20億のパラメータが追加されているのです。
> 
> 
> 
> > ## Dylan Liu トピック作成者
> > 
> > 20億のパラメータですか？10億のパラメータの違いだと思っていました。しかし、推論時間が2倍になるのは、まだ説明がつきません。
> > 
> > 
> > 
> > > ## Sparsh Tewatia
> > > 
> > > Gemma は常にパラメータ数を少なく見積もっており、102億のパラメータを示しています。また、LLAMA 3 はグループ化されたクエリアテンションを使用しており、トークナイザーに約12万トークンがありますが、Gemma はセルフアテンションを使用しており、トークナイザーに25万トークンがあるため、速度の違いを説明できます。
> > > 
> > > 
> > > 
---
> ## Yichuan Gao
> 
> 重みと compute_dtype のデータ型を確認することをお勧めします。compute で bfloat16 を使用している場合、T4 は bfloat16 をサポートしていないため、他の方法でエミュレートする必要があるため、大幅に遅くなります。私の経験では、Gemma2 9b と Mistral 7b の推論時間はほとんど変わりません (3～4時間程度)。4ビットの重みと float16 のデータ型を使用しています。
> 
> 
> 
---
> ## Valentin Werner
> 
> 私の場合、同じパラメータでのトレーニング時間も Llama3-8b よりも 50% 遅く、信じられないほどです。しかし、Sparsh が指摘したように、すべてアーキテクチャに依存します。
> 
> 
> 
> > ## Robert0921
> > 
> > LoRa の場合、Gemma2 は Llama3 よりも正確ですが、9時間の時間制限のため、より良い結果を得ることができませんでした。
> > 
> > 
> > 
---
> ## Robert0921
> 
> 推論だけでなく、トレーニング時間も長くなります。なぜなら、9b > 8b だからですか？
> 
> 
> 
---

