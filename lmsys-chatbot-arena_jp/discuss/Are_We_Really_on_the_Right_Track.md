# 私たちは本当に正しい道を歩んでいるのでしょうか？
**Lorry Zou** *2024年7月21日 日曜日 23:33:24 日本標準時* (2票)

コンペティションの説明から：
「このチャレンジは、人間のフィードバックからの強化学習（RLHF）における「報酬モデル」または「選好モデル」の概念と一致しています。以前の研究では、既存のLLMに直接プロンプトを与えて選好予測を行うことには限界があることが分かっています。これらの限界は、最初に提示された応答を好む傾向（位置バイアス）、過度に冗長になる傾向（冗長性バイアス）、自己宣伝を行う傾向（自己強化バイアス）などのバイアスに起因することがよくあります。」

コンペティション主催者は、強化学習を試すように促していますが、誰もが既存のLLMをファインチューニングしていますね。🙂🙃

---
# 他のユーザーからのコメント
> ## CPMP
> 
> RLHFは教師あり学習の方法です。ラベルは人間によって提供され、このコンペティションで私たちが持っているラベルと非常に似ています。
> 
> 私たちが何か違うことをすべきだと提案しているのか、よくわかりません。
> 
> 
> 
---
> ## Dlond Mike
> 
> ええ、だって本当に素晴らしいパフォーマンスを発揮するんだもん。金持ちのためのゲームだよ。（GPU :))
> 
> 
> 
---
> ## chan peter
> 
> RLHFモデルを試して、報酬スコアを入力として使用してシンプルな分類器を構築してみました。うまくいきましたが、RLHFモデルの実行には時間がかかりすぎ、コンペに少し遅れて参加したので、時間制限内に最適化して通過する時間がありませんでした。
> 
> 
> 
---

