# 使用可能なLLMモデル

**superferg** *2024年7月6日 土曜日 21:28:26 JST* (26票)

皆さん、どのモデルを試しましたか？私は以下のモデルを試しました。サンプルの20%をランダムに選択して検証セットとして使用しました。

| モデル | ローカル検証 | パブリックリーダーボード |
|---|---|---|
| Llama3-8B-instruct | 0.9419 | 0.954 |
| Llama3-8B | 0.9818 | 0.987 |
| Gemma2-9B-instruct | 0.9262 | 1.206 |
| Gemma2-9B | 0.9499 | 1.299 |

Gemma2-9Bは異常な結果を得ています。推論に問題があるのかもしれません。同様の問題を抱えている人はいますか？

**更新:**

[新しいパブリックノートブック](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)を使用すると、正しい結果が得られました。

| モデル | ローカル検証 | パブリックリーダーボード |
|---|---|---|
| Llama3-8B-instruct | 0.9419 | 0.954 |
| Llama3-8B | 0.9818 | 0.987 |
| Gemma2-9B-instruct | 0.9262 | 0.930 |
| Gemma2-9B | 0.9499 | TODO… |

---
# 他のユーザーからのコメント

> ## Valentin Werner
> 
> これだけはお伝えしておきます 😉
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 現在のローカル検証セットは0.91Xです。まだLBに移行できません。LoL
> > 
> > 
> > 
> > ## SAY WHAT
> > 
> > 面白いですね！！！
> > 
> > 
> > 
---
> ## Valentin Werner
> 
> Gemma2-9Bは最近登場しました。9Bはトレーニングをさらに難しくしますが、これらのモデルの中でパフォーマンスベンチマークでトップに立っています。
> 
> 
> 
> > ## Cody_Null
> > 
> > Gemma2-9BをHugging FaceからKaggleに引っ張ってくることができましたか？それともKaggleモデルのGemma 2 · gemma-2-9b-pt · V1を使用していますか？
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > Gemma2-9BをHugging FaceからKaggleに引っ張ってきました。
> > > 
> > > 
> > > 
> > ## s111mple
> > 
> > ファインチューニングされたモデルは良い結果が得られません。試しましたか？
> > 
> > 
> > 
---
> ## xiaotingting
> 
> 検証セットのインデックスはパブリックスコアと正の相関関係にあるようで、インデックスをさらに改善する余地があります。
> 
> 
> 
---
> ## Xiot1206
> 
> この重要な情報を提供していただきありがとうございます。
> 
> 
> 
---
> ## lllleeeo
> 
> NLP初心者なので、もしかしたらばかげた質問かもしれませんが、ファインチューニングに参加するために必要なパラメータ数をどのように決定しましたか？一つずつ試しましたか？経験に基づいて一般的にどれくらいが最適ですか？モデルによって異なりますか？私はLlama 8bのパブリックラップトップのファインチューニングでパラメータの0.02%しか使用していないことに気づきましたが、これは少なすぎませんか？
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 十分なコンピューティングパワーがない場合は、LoRAファインチューニング方法を使用するしかないかもしれません。
> > 
> > 
> > 
> > > ## lllleeeo
> > > 
> > > 返信ありがとうございます！A100と4090をレンタルして、並行して実験を行いたいのですが、そのコンピューティングパワーに基づいてより多くのパラメータを試せるかどうかが気になっています。しかし、どのくらいから試すべきか分かりません。
> > > 
> > > 
> > > 
> > > ## superfergトピック作成者
> > > 
> > > 最初はトップレベルのパブリックノートブックを試すことができます。
> > > 
> > > 
> > > 
> > > ## lllleeeo
> > > 
> > > ありがとうございます！うまくいきました！
> > > 
> > > 
> > > 
---
> ## Mr.T
> 
> 推論中にGemma 2-9bをどのようにロードしますか？
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 以下のノートブックを参照してください。
> > 
> > [https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)
> > 
> > 
> > 
---
> ## EISLab_hwlee
> 
> Gemma2-27B-instructモデルはパフォーマンスが向上しますか？
> 
> 
> 
> > ## EISLab_hwlee
> > 
> > 実験の結果、パフォーマンスが低いことが分かりました。
> > 
> > 
> > 
> > > ## superfergトピック作成者
> > > 
> > > 9時間以内に27Bの推論を完了することができません。理論的には、27Bはより良い結果を達成するはずです。
> > > 
> > > 
> > > 
> > > ## EISLab_hwlee
> > > 
> > > 私も提出できませんでした。
> > > 
> > > しかし、トレーニングでは損失が1.0を下回らず、評価損失も1.0を下回りませんでした。
> > > 
> > > 
> > > 
---
> ## hn
> 
> 単なる好奇心ですが、Gemma2の推論結果が不十分だった原因は何でしたか？パブリックノートブックで修正されたと書いてありました。
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 時間がないので理由は分かりませんが、以下の2つのノートブックを比較することで原因を分析できます。
> > 
> > [https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference](https://www.kaggle.com/code/emiz6413/llama-3-8b-38-faster-inference)
> > 
> > [https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora](https://www.kaggle.com/code/emiz6413/inference-gemma-2-9b-4-bit-qlora)
> > 
> > 
> > 
---
> ## Mukatai
> 
> 最近のパブリックノートブックでは、Gemma2のファインチューニングで0.941のスコアが記録されていますが、この表ではGemma2-9B-instructで0.930のスコアが表示されています。違いはありますか？
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 私は独自のトレーニングスクリプトを使用しているので、多少の違いがあるはずです。コンペティション終了後に公開できます。
> > 
> > 
> > 
> > > ## Mukatai
> > > 
> > > ありがとうございます。GemmaのトレーニングはKaggleで行われていますか？パブリックノートブックでは、単一のデータセットでのトレーニングは週ごとの30時間の制限を超えてしまいます。
> > > 
> > > 
> > > 
---
> ## Femca7
> 
> 取得した結果は、事前トレーニング済みモデルですか？それともファインチューニング済みモデルですか？
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > 提供した表に詳細が記載されています。'instruct'というサフィックスが付いているものは、ファインチューニング済みモデルです。
> > 
> > 
> > 
---
> ## yechenzhi1
> 
> Instructモデルはベースモデルよりも優れているのでしょうか？私はInstructモデルしか試していません。
> 
> 
> 
> > ## superfergトピック作成者
> > 
> > ローカルテストによると、Llama3-8B instructはLlama3-8Bよりも優れています。しかし、Llama3-8Bに適したハイパーパラメータが見つかっていないのかもしれません。
> > 
> > 
> > 
> > ## ducnh279
> > 
> > テキスト分類のためにデコーダーのみのモデルのファインチューニングを始めたばかりの頃、私も同様の質問をしました！
> > 
> > Twitterで[@rasbtn](https://www.kaggle.com/rasbtn)（著名な研究者/教育者）に質問しました！彼はこう答えました。
> > 
> > 私もいくつかの実験を行い、その結果、インストラクションチューニングされたバージョンを使用すると、ベースモデルと比較してパフォーマンスが向上し、収束が速くなることがよくあります。
> > 
> > 
> > 
> > > ## yechenzhi1
> > > 
> > > ありがとうございます！とても役に立ちます！
> > > 
> > > 
> > > 
---

