# 要約 
このディスカッションは、Kaggleコンペティション「LMSYS - Chatbot Arena 人間による好み予測チャレンジ」における、モデル実行に関する問題と解決策についてです。

ユーザーのKeShuang Liuは、CPUでモデルを実行した際に19GBのメモリを使用し、GPU p100では16GBしかなく、モデルが実行できない状況に直面していました。そこで、2つのt4ブロックを使用することで合計30GBのメモリが確保できることに気づき、モデルを2つのt4ブロックに展開する方法を質問しました。

ユーザーのMinato Ryanは、transformersライブラリを使用している場合は、`device_map="auto"`を使用することで、モデルが自動的に複数のGPUに分散されることを提案しました。KeShuang Liuは、この方法で問題を解決できたと報告しました。

このディスカッションは、Kaggleコンペティションにおける、GPUリソースの有効活用とモデルの分散実行に関する有益な情報提供となっています。


---
# 2枚のGPUでモデルを実行する方法

**KeShuang Liu** *2024年6月17日 月曜日 17:22:30 JST* (0票)

CPUにモデルをロードしたところ、19GBのメモリを使用しました。一方、GPU p100は16GBしかありませんでした。しかし、2つのt4ブロックを使用すると合計30GBになることがわかりました。この場合、モデルを2つのt4ブロックに展開できますか？どうすればよいですか？

---
# 他のユーザーからのコメント

> ## Minato Ryan
> 
> transformersライブラリを使用している場合は、`device_map="auto"`を使用してください。
> 
> 例えば、
> 
> ```
> AutoModelForCausalLM.from_pretrained("google-bert/bert-base-cased", device_map="auto")
> 
> ```
> 
> 
> 
> > ## KeShuang Liuトピック作成者
> > 
> > ご回答ありがとうございます。この方法で成功しました。
> > 
> > 
> > 
---

