# 生成ヘッダーを使用した予測
**Takamichi Toda** *2024年7月16日 火曜日 16:07:36 GMT+0900 (日本標準時)* (33票)

これはまだ試行錯誤中のアイデアであり、現在の環境ではまだ成功していませんが、共有したいと思います。

現在、公開されているコードからわかる限り、主流のアプローチはLlamaやその他のLLMをベースにしてLoRAで分類ヘッドを訓練することのようです。しかし、LLMはもともと次のトークンを予測するように訓練されているため、この方法はLLMの元の訓練から逸脱しているため、非効率的だと思います。

私のアイデアは、元のLLMと同じ生成ヘッダー（CausalLM）を使用することです。

プロンプトを調整し、トークンA、B、tieの生成確率を使用します。予測はsoftmaxを使用して後処理され、合計が1になるようにします。

以下は簡単なサンプルコードです。

```
text = """
### Instruction
Which model's answer is appropriate for the prompt?　If both are appropriate, answer `tie`.
### Prompt
{prompt text}
### A
{response A}
### B
{response B}
### Answer
"""
inputs = tokenizer(text)
out = model(inputs)
pred_token_id = tokenizer.encode("A") + tokenizer.encode("B") + tokenizer.encode("tie")
pred = out.logits[0, -1, pred_token_id].softmax(0)
```

これは、Llama3 8Bを使用してこの方法を評価するコードです。
[https://www.kaggle.com/code/takamichitoda/lmsys-zeroshot-prediction](https://www.kaggle.com/code/takamichitoda/lmsys-zeroshot-prediction)

Llama3は特別なファインチューニングを受けておらず、Kaggleモデルからロードされたまま使用されています。評価データはコンペティションデータの1/5を使用しており、これは現在の検証戦略（パブリックリーダーボードとよく相関しています）に相当します。

その結果、1.234のスコアを得ました。ZeroShotでこのような結果を得ることができるとは驚きでした。

現在、コンペティションデータでSFTを実行し、プロンプトを調整しています。しかし、分類ヘッドを学習するモデルの方がまだスコアが良いです。

同様のアプローチに取り組んでいる人はいますか？

---
# 他のユーザーからのコメント
> ## James Day
> 
> 私は同様の実験を試みました。主な理由は、HuggingFace transformersライブラリよりも高速でメモリ効率の高い、因果言語モデルの訓練と推論ライブラリ（unslothやvLLMなど）を活用したいと思ったからです。しかし、[@takamichitoda](https://www.kaggle.com/takamichitoda)の初期の実験のようにゼロショット推論を行うのではなく、実際にはLLMをファインチューニングしました。
> 
> Llama 3 8B Instructで次のトークン予測を行い、0.902（CV）を取得しました。これは、私が「通常の」Llama 3ベースの分類モデルで得ているスコアとほぼ同じです。しかし、同じアプローチはGemma 2 9B（0.990 CV 🤮）ではうまく機能しませんでした。これは、Gemmaのタイド埋め込みが原因である可能性があります。私のCVスコアは、対応するLBスコアよりも常に約0.03低いため、これらの結果はLBで約0.93と1.02になります。これは、提出するほど良くありません。
> 
> 
> 
> > ## Takamichi Todaトピック作成者
> > 
> > 共有していただきありがとうございます。
> > 
> > 0.9は私にとって素晴らしいスコアです ;)
> > 
> > ところで、可能であれば、ファインチューニングにどのようなプロンプトを使用したか教えていただけますか？
> > 
> > 私はtrlのSFTTrainerを使用しており、DataCollatorForCompletionOnlyLMを使用して出力のみを学習しています。
> > 
> > 
> > > ## James Day
> > > 
> > > 次のようなプロンプトを使用しました。
> > > 
> > > ```
> > > 以下のチャットボットのどちらが、ユーザーのリクエストへの応答をより適切に行いましたか？それとも同等でしたか？
> > > 
> > > ~~~~~~~~~~ BOT Aとの会話 ~~~~~~~~~~
> > > 
> > > ### ユーザー: "{初期プロンプト}"
> > > 
> > > ### BOT Aの応答: "{初期応答}"
> > > 
> > > ### ユーザー: "{フォローアッププロンプトがある場合は、3kトークンのコンテキストウィンドウに収まる会話ターンをできるだけ多く含め、必要に応じて各会話の最初の部分を破棄します}"
> > > 
> > > ### BOT Aの応答: "{フォローアップ応答}"
> > > 
> > > ~~~~~~~~~~ BOT Bとの会話 ~~~~~~~~~~
> > > 
> > > ### ユーザー: "{...}"
> > > 
> > > ### BOT Bの応答: "{...}"
> > > 
> > > ### ユーザー: "{...}"
> > > 
> > > ### BOT Bの応答: "{...}"
> > > 
> > > ### 最良の応答:
> > > 
> > > ```
> > > 
> > > これは、" A"、" B"、または" Tie"を出力するように訓練されました。スペースは応答トークンの一部でした。
> > > 
> > > 
> > > 
> > ## Valentin Werner
> > 
> > この実験のみ、またはすべてのモデルでCVのずれが発生していますか？私たちは、ずれが0.01をはるかに下回る実験をいくつか行い、あなたのものと同様のずれが発生する実験もいくつか行いました。
> > 
> > 
> > > ## James Day
> > > 
> > > すべての実験です。
> > > 
> > > また、頭の中で計算できるようにするために、単純化していました。リーダーボードで0.902 CVがどの程度になるかのより正確な推定値は、0.902*0.890 + 0.125 = 0.928です。この推定値は、以下のCV-LB相関データに基づいています。
> > > 
> > > 
> > > 
> > > ## ShelterW
> > > 
> > > 私は、CVとLBの間に比較的大きな差が生じているのは、追加のプロンプトが原因だと思います。
> > > 
> > > ところで、LLMのファインチューニングにqloraまたはloraを使用していますか？
> > > 
> > > 
> > > 
> > > ## James Day
> > > 
> > > qloraを使用しています。
> > > 
> > > CVとLBのスコアの差については、コンペティション主催者によって提供されていない外部訓練データの使用（「追加のプロンプト」の意味だと思います）が原因であるとは考えていません。これは、追加データを追加しても、既存のトレンドラインから有意な偏差が観察されなかったためです。より可能性の高い説明は、コンペティション主催者によって提供されたデータが、彼らのテストデータと完全に一致していないということです。たとえば、彼らは各会話が行われた日付に基づいてデータを分割している可能性があり、その結果、テストデータには、訓練データ（または私の交差検証データ）には存在しない新しいモデルからの応答が含まれることになります。
> > > 
> > > また、トレンドラインの傾きが1未満であることの1つの結果は、CVとLBのスコアが向上するにつれて、ずれが大きくなる傾向があるということです。ばかげたまでに外挿すると、交差検証で完全な精度を持つモデル（CV 0）は、リーダーボードで約0.125のスコアになる可能性があり、これは非常に大きなスコアずれです。
> > > 
> > > 
> > > 
---
> ## AbaoJiang
> 
> [@takamichitoda](https://www.kaggle.com/takamichitoda)さん、こんにちは。
> 
> あなたが言及したように、ゼロショット予測のパフォーマンスは1.234であり、グローバル平均を予測することによる1.098のスコアを上回っていません。
> 
> しかし、それでも試してみる価値のある興味深いアイデアです。共有していただきありがとうございます！
> 
> 
> 
> > ## Valentin Werner
> > 
> > 私は、モデルを最初に訓練せずにsoftmaxを仮定することが問題だと思います。モデルは、最初にファインチューニングされて、"A"、"B"、または"tie"を予測する必要があることを認識していない限り、"A"、"B"、または"tie"を予測する意図はほとんどありません。そのため、ロジットもほとんどナンセンスです。
> > 
> > 彼らが行ったこの単純なベースライン実験は、実際の実験が機能するかどうか、またはどの程度うまく機能するのかについて、何も語っていません。
> > 
> > 私にとって、一見したところ、シーケンス分類アプローチよりもメリットは見られません。なぜなら、おそらく自動回帰生成などを無効にする必要があるからです。しかし、間違いなく興味深いアイデアです。
> > 
> > 
> > > ## Takamichi Todaトピック作成者
> > > 
> > > SFTとプロンプトチューニングでスコアを1.037に改善することができました。分類ヘッドの方がまだ優れていますが、検証を続けるつもりです。
> > > 
> > > 
> > > > ## ShelterW
> > > > 
> > > > Llama3-8bをファインチューニングするためにSFTを使用し、LBスコアを0.935に改善しました。[こちら](https://www.kaggle.com/code/shelterw/training-llama3-8b-4-bit-qlora-sft)を参照して、ご意見をお寄せください。
> > > > 
> > > > 
> > > > 
---


