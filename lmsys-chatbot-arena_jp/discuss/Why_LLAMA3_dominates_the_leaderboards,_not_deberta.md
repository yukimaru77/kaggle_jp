# なぜLLAMA3がDebertaよりもリーダーボードで優勢なのか

**kagglethebest** *2024年7月5日 金曜日 22:41:58 JST* (6票)

パブリックノートブックを見たとき、DebertaではなくLLAMA3が最高スコアを記録していることに驚きました。テキスト分類タスクに関するコンペティション（このコンペティションもテキスト分類タスクと言えるでしょう）では、Debertaが最適なソリューションであるという印象を持っていました。少なくとも、大きな差はありませんでした。

この理由として、考えられるのは2つです。

* Debertaに適したカテゴリカル損失関数がまだ見つかっていない。
* LLAMAのようなデコーダーのみのモデルは、LLMによって出力されるテキストに対してより敏感である。

補足：Debertaを使ってLLAMAのベストノートブックのスコアを上回る人がいれば教えてください。

---
# 他のユーザーからのコメント

> ## Valentin Werner
> 
> 2番目の理由は確かに当てはまると思います。しかし、Llama3-8BはDeBERTaと比べて20倍のパラメータ数を持っており、それに応じて事前学習されていることも認識する必要があります。そのため、言語をはるかにうまく表現できるでしょう。分類ヘッドを追加するだけで、エンコーダーとデコーダーの違いを埋め合わせることができます。
> 
> 間違っていなければ、シーケンス分類のためのエンコーダーのみ（DeBERTa）とデコーダーのみ（LLama）のアーキテクチャの違いはわずかです。なぜなら、デコーダーはもはや自己回帰的に次のトークンを生成する必要がなくなり、エンコーダーのように分類を生成するからです。
> 
> 多くの場合、パラメータ数はスコア向上にわずかな影響しか与えません。しかし、この問題は非常に微妙なため（人間でさえデータセットを正確に予測することはできません）、膨大なパラメータ数はこれらの微妙な点を学習するのに役立ちます。私の意見では、この問題はDeBERTaには複雑すぎるのです。
> 
> 
> 
> > ## Cristóbal Mackenzie
> > 
> > これは理にかなっています。なぜなら、TinyLlamaを使って何度か試してみましたが、完全に失敗したからです。この問題では、パラメータ数は何かを学習する上で重要な要素のようです。
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > 聞いたところによると、一部の人々はDeberta XSで「何かを学習する」ことに成功したそうです。しかし、私の最高のDeBERTa（Large）は、いくつかの秘密のソースを含めても1.0をわずかに下回りました。
> > > 
> > > 
> > > 
> > > ## justin1357
> > > 
> > > Llamaはもっと優れているのでしょうか？
> > > 
> > > 
> > > 
---

