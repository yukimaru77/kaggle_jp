# 要約 
このKaggleコンペティションのディスカッションは、参加者たちが、特に学生にとって、コンペティションで成功するために必要な計算リソースの不足に苦労していることを示しています。

**主なポイント:**

* **計算リソースの不足:** 参加者は、大規模言語モデル（LLM）をトレーニングするために必要な計算能力が不足しているため、コンペティションで成功するために苦労しています。
* **DeBERTa vs. LLM:** 参加者は、DeBERTaのようなより小さなモデルよりもLLMの方がパフォーマンスが優れている可能性があると考えていますが、LLMのトレーニングには膨大な計算リソースが必要になります。
* **費用:** LLMのトレーニングには、学生にとって高価なクラウドコンピューティングサービスの利用が必要になります。
* **Kaggleの無料GPU:** 参加者は、Kaggleの無料GPUに頼っていますが、それらは限られており、コンペティションで成功するために必要な計算能力を提供するのに十分ではありません。
* **解決策:** 参加者は、クラウドコンピューティングサービスのレンタル、大学や教授からのリソースの利用、またはKaggleがより高性能なハードウェアに投資することを提案しています。

**結論:**

このディスカッションは、Kaggleコンペティションが、特に学生にとって、計算リソースの不足によってますます困難になっていることを示しています。参加者は、コンペティションで成功するために必要なリソースへのアクセスを改善するための解決策を求めています。


---
# このコンペティションに圧倒されている

**ducnh279** *2024年7月4日 木曜日 04:37:33 GMT+0900 (日本標準時)* (13 votes)
皆さん、こんにちは！

これまで参加してきた他のNLPコンペティションとは異なり、このコンペティションでは、デコーダーのみのモデルがDeBERTaよりも優れた性能を発揮する可能性があると信じています。特にこのコンペティションでは、LLMを使った実験は、私にとって計算量と費用面で非常に負担が大きいです。

- DeBERTa（large）の場合、Kaggleの2 x T4を使って6時間のトレーニングでLB: 0.993（チューニング済み）を達成できます。
- LLMの場合、Mistral 7B（4ビット量子化 + LoRA）で1回だけ実験を行ったところ、LB: 0.991でした。
LLMのファインチューニングでは、Lightning Studiosの1 A10Gを使って1つのフォールドを完了するのに15時間もかかり、1回の実験が非常に遅く、費用がかかります。もし、0.9から0.95に到達するための特別な魔法がなければ、バッチサイズ、学習率、ウォーミングアップステップ、プロンプトのチューニング、トレーニングを安定させ、早期のパフォーマンス飽和を回避するためのトレーニングトリックを試したり、単に1エポック以上を実行したりすることで、LB: <= 0.95に近づけることができると考えています。

学生である私は、特に無料のハードウェアへのアクセスが限られているため、Kaggleコンペティションがますます困難で計算量も増大していると感じています。KaggleとColabの無料GPUに頼っているため、競争する際にしばしば制約を感じ、圧倒されてしまいます。

このコンペティションで大きなお金を投資する必要があると思いますか？お金のない学生として、"戦略的投資"のために親に頼らなければならないかもしれません。😂

---
# 他のユーザーからのコメント

> ## kagglethebest
> 
> 同じ気持ちです。😂 KaggleのGPUを使ってDeberta Baseで良いスコアを出す方法を探しています。もし試行がうまくいかなかったら、このコンペティションは諦めます。
> 
> 
> 
> > ## ducnh279トピック作成者
> > 
> > 諦めないでください [@judith007](https://www.kaggle.com/judith007)! 最後まで戦いましょう！
> > 
> > ちなみに、DeBERTa largeを試して、2つのGPUの使い方を学ぶことをお勧めします。DeBERTa largeで0.988のスコアを達成しましたが、これは8ビット量子化されたLLaMA 8Bを使った最初の公開ノートブックと同じスコアです。
> > 
> > 
> > 
---
> ## Valentin Werner
> 
> 
> 
> 
> 
> > ## Valentin Werner
> > 
> > 冗談はさておき、高価なGPUを購入するよりも、ホストされたレンタルサービスでモデルをトレーニングすることを常に優先してください。レンタルに移行する前に、まずKaggleのGPU / TPUやGoogle Collabなどでゆっくりと検証することができます。データサイエンスのために3090TI / 4080 / 4090を購入する計算は、実際にはうまくいきません。私は4090を持っていますが、実験には最適ですが、それでもKaggleのTPUと同じ実験にはスケールできません。
> > 
> > 計算リソースによって制限されるのは、本当に気分が悪いです。レンタルができない場合に試せること：一部のクラウドプロバイダーは、期間限定でリサーチ用のコンピューティングを提供しています。大学や教授に、利用できるコンピューティングがあるかどうか尋ねてみてください（もしかしたら、課外活動として売り込み、結果を最後に発表してボーナス点を獲得できるかもしれません。私の大学には、ほとんどアイドル状態の4x V100セットアップ（合計128GB）があり、教授はほとんど私にお願いして、誰も研究をしていないときに何かをトレーニングするように頼んでいました）。
> > 
> > 
> > 
> > > ## ducnh279トピック作成者
> > > 
> > > あなたのミームは私の物語を語っています！
> > > 
> > > このコンペティションが終わったら、TPUトレーニングについて学びます！経験を共有してくれて本当にありがとう！
> > > 
> > > 
> > > 
---
> ## Cody_Null
> 
> 他の誰かがMistralのような7Bモデルを4ビットで動作させることができたのは嬉しいです。私のモデルにはバグがありましたが、それでもLLamaモデルを上回ることはなさそうでした。でも、あなたの立場はよく分かります。時々そう感じます。
> 
> 
> 
> > ## ducnh279トピック作成者
> > 
> > ご理解ありがとうございます！確かに、4ビットで量子化された7Bモデルは、パフォーマンスが確実に低下します。スケーリング則を使ってハイパラムを設定し、トレーニングテクニックを試してから、トレーニングセット全体で実行することができます。コンペティションが終わるまでは、これ以上は話せません。
> > 
> > 
> > 
---
> ## Taimo
> 
> Kaggleは学生にとって良い出発点です。
> 
> モデルのサイズが大きくなり続けても、教育目的のために、Kaggleはそういう場所であり続けるべきです。
> 
> Google（Alphabet）は、Kaggleのためにより高性能なハードウェアに投資すべきです。
> 
> 
> 
> > ## ducnh279トピック作成者
> > 
> > 教育目的のために、Kaggleはモデルのサイズが大きくなり続けても、そういう場所であり続けるべきです。
> > 
> > 同意です！KaggleコンペティションとKagglersからの共有を通して、多くのことを学びました！"large"というモデルとデータセットは、私にとってそれほど大きな問題ではありません！これからも学び続け、競争を続けていきます。
> > 
> > Google（Alphabet）は、Kaggleのためにより高性能なハードウェアに投資すべきです。
> > 
> > みんなそう願っています！hahaha
> > 
> > 
> > 
---
> ## xiaotingting
> 
> 大学院生になってからは、ラボのサーバーを使えて問題ありませんでした。しかし、最近論文を提出する必要があり、追加の実験を行う必要があったため、ラボの他のメンバーがサーバーを使っていたため、彼らが使っていないときしか使えませんでした。大きなモデルをファインチューニングしたい場合は、本当にカードが必要です。現在、この実験の準備のために2枚のA100カードをレンタルしており、トレーニングにはそれぞれ少なくとも2日かかります。1日中レンタルする方が費用対効果が高く、2枚のカードで1日約200元、1週間レンタルすると1000元以上かかります。
> 
> 
> 
> > ## KeShuang Liu
> > 
> > 私は会社でインターンをしていて、2枚のA800を提供されましたが、技術的な問題のため、良い結果を得ることができませんでした。
> > 
> > 
> > 
---

