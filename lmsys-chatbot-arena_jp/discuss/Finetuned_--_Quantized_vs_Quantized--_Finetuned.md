# ファインチューニングしてから量子化 vs 量子化してからファインチューニング

**Varun Jagannath** *2024年7月26日 23:22:44 (日本標準時)* (1票)

このコンペティションでは、Llama 3のようなモデルをファインチューニングしてから量子化するアプローチと、低ビット量子化されたモデルをデータセットでファインチューニングするアプローチのどちらが優れているのでしょうか？

---
# 他のユーザーからのコメント

> ## Valentin Werner
> 
> 実際にテストした人がいれば、ぜひ教えてほしいです。私の直感では、ファインチューニングしてから量子化の方が良いと思います。ファインチューニングの方が精度が高いからです。もちろん、後で量子化されるので、その精度が失われるという議論もあります。もしかしたら、量子化された状態でトレーニングすることで、検証データとリーダーボードのスコアの一貫性を高めることができるかもしれません。
> 
> 
> 
> > ## Pranshu Bahadur
> > 
> > このシナリオを少しテストしてみたのですが、あなたの仮説に同意します。bfloat16でGemma 2 9Bをトレーニングしたところ、トレーニング損失が0.44まで下がりました（明らかに過学習の兆候です）。量子化はトレーニング後に実施すべきだと思います。
> > 
> > 
> > 
> > ## Maksim Metelskii
> > 
> > 量子化されたモデルでトレーニングされたLoRaアダプター（16ビットまたは32ビット）は、量子化によって生じた不正確さを修正するのに役立つ可能性があります。LoRaアダプターは、量子化と新しい特定のタスクの不正確さの両方を解決します。ChatGPTは、量子化してからファインチューニングの方が精度が向上する可能性があると述べています。しかし、実際にテストする必要があります。
> > 
> > 
> > 
> > ## Varun Jagannath（トピック作成者）
> > 
> > 私の観察では、以前公開されたTPUトレーニングノートブックはリーダーボードで約0.98、最新のUnsloth Gemma 2のトレーニングと推論では0.94でした。そのため、量子化してからファインチューニングを行うアプローチがこのコンペティションでうまく機能しているのかを理解したかったのです。
> > 
> > 
> > 
---
> ## xiaotingting
> 
> 量子化後にファインチューニングを行う方が良いと思います。なぜなら、量子化によって生じた損失を補うことができるからです。量子化されたモデルをファインチューニングする場合と、トレーニング後に量子化する場合では、異なる学習率が必要になるかもしれません。
> 
> 
> 
--- 

