# プロンプトとレスポンスを効率的にトークン化する

**irishu** *2024年7月28日 13:56:19 (日本標準時)* (6票)

# 実験

これまで以下の3つの方法を試してみました。その中で、最初の方法がLBで最も良い結果を出しました。

### 方法

1. プロンプト + レスポンスA + レスポンスB を結合して、最大トークン数までトークン化する。
2. 各文に最大トークン数の1/3を割り当て、その制限までトークン化する。
3. トークン数を適切な比率（例：1:2:2）で割り当てる。

### 条件

- Gemma-2 9b 4-bit QLoRAを使用
- 最大トークン数 = 1024
- 最後のプロンプトとレスポンスのみを使用
- 全ての訓練データを使用して1エポック
- 優れた作品「[Training] Gemma-2 9b 4-bit QLoRA fine-tuning」([https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning](https://www.kaggle.com/code/emiz6413/training-gemma-2-9b-4-bit-qlora-fine-tuning))を参照

# 質問

### シンプルな方法1よりも効率的な方法はないでしょうか？

プロンプトとレスポンス（最後のもののみ）のトークン数の分布を見ると、約10%が合計で1024トークンを超えているようです。つまり、場合によっては、レスポンスBに十分な情報が含まれていない可能性があります。

### 最大トークン数を増やすとスコアはどのくらい向上するでしょうか？

計算リソースの関係で、まだテストできていません。

---

# 他のユーザーからのコメント

> ## irishuTopic 作成者
> 
> 学習と推論で最大トークン数を2048に変更したところ、スコアが向上しました。
> 
> これで、パディングを使ってトークン長を調整すべきかどうかが疑問です。
> 
> 
> 
---

