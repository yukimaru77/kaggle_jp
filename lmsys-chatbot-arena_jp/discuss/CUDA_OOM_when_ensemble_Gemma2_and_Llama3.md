# CUDA OOM ãŒç™ºç”Ÿã™ã‚‹å•é¡Œ: Gemma2 ã¨ Llama3 ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«

**Lorry Zou** *2024å¹´7æœˆ16æ—¥ ç«æ›œæ—¥ 00:39:47 GMT+0900 (æ—¥æœ¬æ¨™æº–æ™‚)* (6ç¥¨)

çš†ã•ã‚“ã€ã“ã‚“ã«ã¡ã¯ã€‚Gemma2 ã¨ Llama3 ã‚’ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã—ã‚ˆã†ã¨ã—ã¦ã„ã¾ã™ã€‚ç§ã®æˆ¦ç•¥ã¯ã€ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ -> Gemma2 ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ -> Gemma2 æŽ¨è«– -> Llama3 ãƒ¢ãƒ‡ãƒ«ã®èª­ã¿è¾¼ã¿ -> Llama3 æŽ¨è«– -> ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã§ã™ã€‚T4*2 ã‚’ä½¿ç”¨ã—ã¦ãŠã‚Šã€ã‚³ãƒ¼ãƒ‰ã¯ä¸»ã« [@kishanvavdara](https://www.kaggle.com/kishanvavdara) ã•ã‚“ã®æŽ¨è«–ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã«åŸºã¥ã„ã¦ã„ã¾ã™ã€‚

å•é¡Œç‚¹ã¯ã€Gemma2 ã®æŽ¨è«–å¾Œã« Llama3 ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚‚ã†ã¨ã™ã‚‹ã¨ã€CUDA OOM ãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã§ã™ã€‚Gemma ãƒ¢ãƒ‡ãƒ«ã‚’ 2 ã¤ã® GPU ã‹ã‚‰å‰Šé™¤ã—ã¦ãƒ¡ãƒ¢ãƒªã‚’ã‚¯ãƒªã‚¢ã—ã‚ˆã†ã¨ã—ã¾ã—ãŸ (å„ GPU ã« 1 ã¤ã® Gemma ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã¾ã™) ãŒã€`gemma_model.cpu(); del gemma_model; torch.cuda.empty_cache()` ã‚’ä½¿ç”¨ã—ã¦ã‚‚åŠ¹æžœãŒã‚ã‚Šã¾ã›ã‚“ã€‚GPU 0 ã®ã¿ãŒè§£æ”¾ã•ã‚Œã€GPU 1 ã¯ä¾ç„¶ã¨ã—ã¦ 8.9GB ã®ãƒ¡ãƒ¢ãƒªã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã€‚

ä¸¡æ–¹ã® GPU ã‹ã‚‰ã™ã¹ã¦ã®ãƒ¡ãƒ¢ãƒªã‚’è§£æ”¾ã™ã‚‹æ–¹æ³•ã€ã¾ãŸã¯ãƒ¢ãƒ‡ãƒ«ã®ã‚µã‚¤ã‚ºã‚’ç¸®å°ã™ã‚‹æ–¹æ³•ã¯ã‚ã‚‹ã§ã—ã‚‡ã†ã‹ï¼Ÿ

---
# ä»–ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®ã‚³ãƒ¡ãƒ³ãƒˆ

> ## å˜ãªã‚‹é‹ã§ã¯ãªãã€é©åˆ‡ãªæ–¹æ³•
> 
> ç°¡å˜ãªæ–¹æ³•ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚`%%writefile` ã‚’ä½¿ç”¨ã—ã¦ `.py` ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã—ã€`!python file_name.py` ã§ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œã—ã¦æå‡ºçµæžœã‚’ç”Ÿæˆã§ãã¾ã™ã€‚å…·ä½“çš„ã«ã¯ã€Gemma ã¨ Llama ç”¨ã« 2 ã¤ã® py ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆã§ãã¾ã™ã€‚å„ãƒ•ã‚¡ã‚¤ãƒ«ã§ã€ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ csv ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜ã§ãã¾ã™ã€‚æœ€å¾Œã«ã€ãã‚Œã‚‰ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’å®Ÿè¡Œã§ãã¾ã™ã€‚
> 
> é‡è¦ãªç‚¹ã¯ã€`!python file_name.py` ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã€ãƒ¡ãƒ¢ãƒªãŒã‚¯ãƒªã‚¢ã•ã‚Œã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚ŒãŒå•é¡Œã®è§£æ±ºã«å½¹ç«‹ã¤ã“ã¨ã‚’é¡˜ã£ã¦ã„ã¾ã™ã€‚
> 
> 
> 
> > ## Lorry Zouãƒˆãƒ”ãƒƒã‚¯ä½œæˆè€…
> > 
> > ã¯ã„ã€ãƒŽãƒ¼ãƒˆãƒ–ãƒƒã‚¯å…¨ä½“ã‚’ Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«å¤‰æ›ã—ãŸã¨ã“ã‚ã€ãƒ¡ãƒ¢ãƒªè§£æ”¾ãŒã†ã¾ãã„ãã¾ã—ãŸã€‚Python ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚’ç›´æŽ¥æå‡ºã§ãã‚‹ã“ã¨ã‚’çŸ¥ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚LOLã€‚
> > 
> > 
> > 
---
> ## Priyanshu Joshi
> 
> ãƒ¢ãƒ‡ãƒ«ã¨ä¸­é–“ãƒ†ãƒ³ã‚½ãƒ«ã¸ã®ã™ã¹ã¦ã®å‚ç…§ã‚’æ­£ã—ãã‚¯ãƒªã‚¢ã—ã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚
> 
> ```
> import gc
> 
> gemma_model.cpu()
> del gemma_model
> torch.cuda.empty_cache()
> gc.collect()
> 
> ```
> 
> æŽ¨è«–ç’°å¢ƒã§ GPU ã‚’ä½¿ç”¨ã—ã¦ã„ã‚‹ä»–ã®ãƒ—ãƒ­ã‚»ã‚¹ãŒãªã„ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ãƒ—ãƒ­ã‚»ã‚¹ãŒå¤§é‡ã®ãƒ¡ãƒ¢ãƒªã‚’æ¶ˆè²»ã™ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚å‹¾é…ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä½¿ç”¨ã—ã¦ã€è¨ˆç®—ã‚³ã‚¹ãƒˆã‚’ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã¨äº¤æ›ã—ã¦ãã ã•ã„ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒãƒƒã‚¯ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ä¸­ã«ãƒ¢ãƒ‡ãƒ«ã®ä¸€éƒ¨ã‚’å†è¨ˆç®—ã™ã‚‹ã“ã¨ã§ãƒ¡ãƒ¢ãƒªã‚’ç¯€ç´„ã§ãã¾ã™ã€‚Veletin ãŒã‚³ãƒ¡ãƒ³ãƒˆã§è¿°ã¹ãŸã‚ˆã†ã«ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ max_length ã‚’å®Ÿé¨“ã—ã¦ãã ã•ã„ã€‚[ãƒ¢ãƒ‡ãƒ«ä¸¦åˆ—åŒ–](https://huggingface.co/docs/transformers/v4.15.0/parallelism) ã‚’è©¦ã™ã“ã¨ãŒã§ãã¾ã™ã€‚
> 
> 
> 
---
> ## Lorry Zouãƒˆãƒ”ãƒƒã‚¯ä½œæˆè€…
> 
> æŽ¨è«–å¾Œã« GPU 0 ã®ãƒ¡ãƒ¢ãƒªã®ã¿ãŒè§£æ”¾ã•ã‚Œã‚‹ã®ã¯ãªãœã§ã—ã‚‡ã†ã‹ã€‚ã‚‚ã—ã‹ã—ãŸã‚‰ã€æŽ¨è«–ä¸­ã«å®Ÿéš›ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã®ã¯ãƒ¢ãƒ‡ãƒ«ã® 1 ã¤ã ã‘ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚ã‚³ãƒ¼ãƒ‰:
> 
> `@torch.no_grad()
> 
> [@torch.cuda.amp.autocast](https://www.kaggle.com/torch.cuda.amp.autocast)()
> 
> def gemma_inference(df, model, device, batch_size=cfg.batch_size, max_length=cfg.max_length):
> 
>     a_win, b_win, tie = [], [], []
> 
> ```
> for start_idx in range(0, len(df), batch_size):
>     end_idx = min(start_idx + batch_size, len(df))
>     tmp = df.iloc[start_idx:end_idx]
>     input_ids = tmp["input_ids"].to_list()
>     attention_mask = tmp["attention_mask"].to_list()
>     inputs = pad_without_fast_tokenizer_warning(
>         gemma_tokenizer,
>         {"input_ids": input_ids, "attention_mask": attention_mask},
>         padding=True,
>         max_length=max_length,
>         pad_to_multiple_of=None,
>         return_tensors="pt",
>     )
>     outputs = model(**inputs.to(device))
>     proba = outputs.logits.softmax(-1).cpu()
> 
>     a_win.extend(proba[:, 0].tolist())
>     b_win.extend(proba[:, 1].tolist())
>     tie.extend(proba[:, 2].tolist())
> 
>     df["winner_model_a"] = a_win
>     df["winner_model_b"] = b_win
>     df["winner_tie"] = tie
>     return df` and
> 
> ```
> 
> with ThreadPoolExecutor(max_workers=2) as executor:
>     gemma_results = executor.map(gemma_inference, (gemma_sub_1, gemma_sub_2), (gemma_model_0, gemma_model_1), (device_0, device_1))
> 
> ã¾ãŸã€`batch_size=4` ã¨ `2` ã‚‚è©¦ã—ã¾ã—ãŸãŒã€é•ã„ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚
> 
> 
> 
> > ## Valentin Werner
> > 
> > å®Ÿéš›ã« `gc.collect()` ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ã‹ï¼ŸShelterW ãŒã‚³ãƒ¡ãƒ³ãƒˆã§èª¬æ˜Žã—ãŸã‚ˆã†ã«ã€`gc.collect()` ãŒå®Ÿè¡Œã•ã‚Œã‚‹ã¾ã§è§£æ”¾ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚Šã¾ã—ãŸã€‚
> > 
> > 
> > 
> > > ## Lorry Zouãƒˆãƒ”ãƒƒã‚¯ä½œæˆè€…
> > > 
> > > ã¯ã„ã€`gc.collect()` ã‚’ä½¿ç”¨ã—ã¦ã„ã¾ã™ãŒã€æ©Ÿèƒ½ã—ã¾ã›ã‚“ã€‚
> > > 
> > > gemma_model_0.to('cpu')
> > > del gemma_model_0
> > > gc.collect()
> > > gemma_model_1.to('cpu')
> > > del gemma_model_1
> > > gc.collect()
> > > with torch.no_grad():
> > >     torch.cuda.set_device('cuda:0')
> > >     torch.cuda.empty_cache()
> > >     torch.cuda.set_device('cuda:1')
> > >     torch.cuda.empty_cache()
> > > 
> > > 
> > > 
---
> ## ShelterW
> 
> Gemma2 ã¨ Llama3 ã®ã‚¢ãƒ³ã‚µãƒ³ãƒ–ãƒ«ã‚’ä½¿ç”¨ã—ã¦ã„ãŸã¨ãã€ã•ã‚‰ã«æ‚ªåŒ–ã—ã¾ã—ãŸã€‚
> 
> ```
> import torch
> import gc
> del proba, model_0, model_1, test, data, aug_data
> gc.collect()
> torch.cuda.empty_cache()
> 
> ```
> 
> 
> 
> > ## Lorry Zouãƒˆãƒ”ãƒƒã‚¯ä½œæˆè€…
> > 
> > ãƒ¡ãƒ¢ãƒªã«æ®‹ã£ã¦ã„ã‚‹ã‚‚ã®ãŒã‚ã‚Šã€å‰Šé™¤ã—å¿˜ã‚Œã¦ã„ã‚‹ã¨æ€ã„ã¾ã™â€¦ðŸ˜†
> > 
> > 
> > 
> > > ## Valentin Werner
> > > 
> > > ã“ã‚Œã«ã‚ˆã‚Šã€ä¸¡æ–¹ã® GPU ãŒ 300 MB æœªæº€ã«ãªã‚Šã¾ã™ã€‚ãã‚Œä»¥å¤–ã®å ´åˆã¯ã€`max_length` ã¨/ã¾ãŸã¯ `batch_size` ã‚’å°ã•ãã—ã¦ãã ã•ã„ã€‚
> > > 
> > > 
> > > 
> > ## Allen Wang
> > 
> > ã‚ãªãŸã¨åŒã˜å•é¡ŒãŒç™ºç”Ÿã—ã¦ã„ã¾ã™ã€‚è§£æ±ºç­–ã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ
> > 
> > 
> > 
---


